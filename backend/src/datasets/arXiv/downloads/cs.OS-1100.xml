<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acs.OS%26id_list%3D%26start%3D0%26max_results%3D1100" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:cs.OS&amp;id_list=&amp;start=0&amp;max_results=1100</title>
  <id>http://arxiv.org/api/tP/bA0XPSTtWny3T3AbFE0kKIlU</id>
  <updated>2025-05-27T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1089</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1100</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/cs/0612079v2</id>
    <updated>2015-04-12T14:14:00Z</updated>
    <published>2006-12-16T21:37:00Z</published>
    <title>Executing the same binary on several operating systems</title>
    <summary>  We notice a way to execute a binary file on Windows and ELF-based systems. It
can be used to create software installers and other applications not exceeding
64 kilo bytes.
</summary>
    <author>
      <name>Steffen Grønneberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The technique is outdated</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0612079v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612079v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.5064v1</id>
    <updated>2009-09-28T10:54:48Z</updated>
    <published>2009-09-28T10:54:48Z</published>
    <title>A Conceivable Origin of Machine Consciousness in the IDLE process</title>
    <summary>  In this short paper, we would like to call professional community's attention
to a daring idea that is surely unhelpful, but is exciting for programmers and
anyway conflicts with the trend of energy consumption in computer systems.
</summary>
    <author>
      <name>Norbert Bátfai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.5064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.5064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.7; C.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05798v1</id>
    <updated>2017-05-16T16:41:43Z</updated>
    <published>2017-05-16T16:41:43Z</published>
    <title>Comments on "Gang EDF Schedulability Analysis"</title>
    <summary>  This short report raises a correctness issue in the schedulability test
presented in Kato et al., "Gang EDF Scheduling of Parallel Task Systems", 30th
IEEE Real-Time Systems Symposium, 2009, pp. 459-468.
</summary>
    <author>
      <name>Pascal Richard</name>
    </author>
    <author>
      <name>Joël Goossens</name>
    </author>
    <author>
      <name>Shinpei Kato</name>
    </author>
    <link href="http://arxiv.org/abs/1705.05798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.01955v1</id>
    <updated>2019-03-05T17:40:38Z</updated>
    <published>2019-03-05T17:40:38Z</published>
    <title>The Lustre Storage Architecture</title>
    <summary>  This lengthy document often referred to as the "Lustre Book", contains a
detailed outline of Lustre file system architecture, as it was created between
2001 and 2005, in accordance with the requirements from various users. Now, in
2019, most features have been implemented, but some only recently, and some
along different lines of thought.
</summary>
    <author>
      <name>Peter Braam</name>
    </author>
    <link href="http://arxiv.org/abs/1903.01955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.6447v1</id>
    <updated>2012-10-24T07:56:48Z</updated>
    <published>2012-10-24T07:56:48Z</published>
    <title>Disk Scheduling: Selection of Algorithm</title>
    <summary>  The objective of this paper is to take some aspects of disk scheduling and
scheduling algorithms. The disk scheduling is discussed with a sneak peak in
general and selection of algorithm in particular.
</summary>
    <author>
      <name>S. Yashvir</name>
    </author>
    <author>
      <name>Om Prakash</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages; http://www.ijascse.in/publications-2012--2</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.6447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.6447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.4082v1</id>
    <updated>2008-09-24T06:10:39Z</updated>
    <published>2008-09-24T06:10:39Z</published>
    <title>Multiprocessor Global Scheduling on Frame-Based DVFS Systems</title>
    <summary>  In this ongoing work, we are interested in multiprocessor energy efficient
systems, where task durations are not known in advance, but are know
stochastically. More precisely, we consider global scheduling algorithms for
frame-based multiprocessor stochastic DVFS (Dynamic Voltage and Frequency
Scaling) systems. Moreover, we consider processors with a discrete set of
available frequencies.
</summary>
    <author>
      <name>Vandy Berten</name>
    </author>
    <author>
      <name>Joël Goossens</name>
    </author>
    <link href="http://arxiv.org/abs/0809.4082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.4082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.5238v1</id>
    <updated>2008-09-30T15:55:04Z</updated>
    <published>2008-09-30T15:55:04Z</published>
    <title>Mode Change Protocol for Multi-Mode Real-Time Systems upon Identical
  Multiprocessors</title>
    <summary>  In this paper, we propose a synchronous protocol without periodicity for
scheduling multi-mode real-time systems upon identical multiprocessor
platforms. Our proposal can be considered to be a multiprocessor extension of
the uniprocessor protocol called "Minimal Single Offset protocol".
</summary>
    <author>
      <name>Vincent Nélis</name>
    </author>
    <author>
      <name>Joël Goossens</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.5238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.5238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.3519v1</id>
    <updated>2009-08-25T12:46:56Z</updated>
    <published>2009-08-25T12:46:56Z</published>
    <title>Predictability of Fixed-Job Priority Schedulers on Heterogeneous
  Multiprocessor Real-Time Systems</title>
    <summary>  The multiprocessor Fixed-Job Priority (FJP) scheduling of real-time systems
is studied. An important property for the schedulability analysis, the
predictability (regardless to the execution times), is studied for
heterogeneous multiprocessor platforms. Our main contribution is to show that
any FJP schedulers are predictable on unrelated platforms. A convenient
consequence is the fact that any FJP schedulers are predictable on uniform
multiprocessors.
</summary>
    <author>
      <name>Liliana Cucu-Grosjean</name>
    </author>
    <author>
      <name>Joël Goossens</name>
    </author>
    <link href="http://arxiv.org/abs/0908.3519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.3519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.5525v1</id>
    <updated>2010-03-29T12:51:23Z</updated>
    <published>2010-03-29T12:51:23Z</published>
    <title>Searching publications on operating systems</title>
    <summary>  This note concerns a search for publications in which one can find statements
that explain the concept of an operating system, reasons for introducing
operating systems, a formalization of the concept of an operating system or
theory about operating systems based on such a formalization. It reports on the
way in which the search has been carried out and the outcome of the search. The
outcome includes not only what the search was meant for, but also some added
bonuses.
</summary>
    <author>
      <name>C. A. Middelburg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.5525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.5525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3687v1</id>
    <updated>2010-04-21T12:31:53Z</updated>
    <published>2010-04-21T12:31:53Z</published>
    <title>Scheduling Multi-Mode Real-Time Systems upon Uniform Multiprocessor
  Platforms</title>
    <summary>  In this paper, we address the scheduling problem of multi-mode real-time
systems upon uniform multiprocessor platforms. We propose two transition
protocols, specified together with their schedulability test, and provide the
reader with two distinct upper bounds for the length of the transient phases
during mode transitions, respectively for the cases where jobs priorities are
known and unknown beforehand.
</summary>
    <author>
      <name>Patrick Meumeu Yomsi</name>
    </author>
    <author>
      <name>Vincent Nelis</name>
    </author>
    <author>
      <name>Joël Goossens</name>
    </author>
    <link href="http://arxiv.org/abs/1004.3687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.0813v1</id>
    <updated>2010-06-04T09:03:39Z</updated>
    <published>2010-06-04T09:03:39Z</published>
    <title>On the definition of a theoretical concept of an operating system</title>
    <summary>  We dwell on how a definition of a theoretical concept of an operating system,
suitable to be incorporated in a mathematical theory of operating systems,
could look like. This is considered a valuable preparation for the development
of a mathematical theory of operating systems.
</summary>
    <author>
      <name>J. A. Bergstra</name>
    </author>
    <author>
      <name>C. A. Middelburg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.0813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.0813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.5793v1</id>
    <updated>2011-10-26T13:53:41Z</updated>
    <published>2011-10-26T13:53:41Z</published>
    <title>Sufficient FTP Schedulability Test for the Non-Cyclic Generalized
  Multiframe Task Model</title>
    <summary>  Our goal is to provide a sufficient schedulability test -ideally polynomial-
for the scheduling of Non-Cyclic Generalized Multiframe Task Model using
Fixed-Task-Priority schedulers. We report two first results: (i) we present and
prove correct the critical instant for the Non-Cyclic Generalized Multiframe
Task Model then (ii) we propose an algorithm which provides a sufficient (but
pseudo-polynomial) schedulability test.
</summary>
    <author>
      <name>Vandy Berten</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">U.L.B</arxiv:affiliation>
    </author>
    <author>
      <name>Joël Goossens</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">U.L.B</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1110.5793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.5793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1687v1</id>
    <updated>2012-05-08T13:12:27Z</updated>
    <published>2012-05-08T13:12:27Z</published>
    <title>Age Based User Interface in Mobile Operating System</title>
    <summary>  This paper proposes the creation of different interfaces in the mobile
operating system for different age groups. The different age groups identified
are kids, elderly people and all others. The motive behind creating different
interfaces is to make the smartphones of today's world usable to all age
groups.
</summary>
    <author>
      <name>Sumit Sharma</name>
    </author>
    <author>
      <name>Rohitt Sharma</name>
    </author>
    <author>
      <name>Paramjit Singh</name>
    </author>
    <author>
      <name>Aditya Mahajan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsea.2012.2215</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsea.2012.2215" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science, Engineering and
  Applications (IJCSEA) 2,2: 177-184 April 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1205.1687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.4929v1</id>
    <updated>2014-02-20T08:33:02Z</updated>
    <published>2014-02-20T08:33:02Z</published>
    <title>Formal Description of Components in Operating Systems</title>
    <summary>  The contemporary development of hardware components is a prerequisite for
increasing the concentration of computing power. System software is developing
at a much slower pace. To use available resources efficiently modeling is
required. Formalization of elements, present in the material, provides the
basis for modeling. Examples are presented to demonstrate the efficiency of the
concept.
</summary>
    <author>
      <name>Asen Petkov Iliev</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJITEE (ISSN: 2278 - 3075, Volume-3, Issue-4, September 2013) 96 -
  98</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.4929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.4929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.07694v1</id>
    <updated>2015-09-25T12:37:09Z</updated>
    <published>2015-09-25T12:37:09Z</published>
    <title>Folding a Tree into a Map</title>
    <summary>  Analysis of the retrieval architecture of the highly influential UNIX file
system (\cite{Ritchie}\cite{multicsfs}) provides insight into design methods,
constraints, and possible alternatives. The basic architecture can be
understood in terms of function composition and recursion by anyone with some
mathematical maturity. Expertise in operating system coding or in any
specialized "formal method" is not required.
</summary>
    <author>
      <name>Victor Yodaiken</name>
    </author>
    <link href="http://arxiv.org/abs/1509.07694v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.07694v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08154v1</id>
    <updated>2016-09-26T17:36:18Z</updated>
    <published>2016-09-26T17:36:18Z</published>
    <title>Implementing RBAC model in An Operating System Kernel</title>
    <summary>  In this paper, the implementation of an operating system oriented RBAC model
is discussed. Firstly, on the basis of RBAC96 model, a new RBAC model named OSR
is presented. Secondly, the OSR model is enforced in RFSOS kernel by the way of
integrating GFAC method and Capability mechanism together. All parts of the OSR
implementation are described in detail.
</summary>
    <author>
      <name>Zhiyong Shan</name>
    </author>
    <author>
      <name>Yu-fang Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Chinese</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.08154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.00402v1</id>
    <updated>2020-03-31T01:44:14Z</updated>
    <published>2020-03-31T01:44:14Z</published>
    <title>A File System For Write-Once Media</title>
    <summary>  A file system standard for use with write-once media such as digital compact
disks is proposed. The file system is designed to work with any operating
system and a variety of physical media. Although the implementation is simple,
it provides a a full-featured and high-performance alternative to conventional
file systems on traditional, multiple-write media such as magnetic disks.
</summary>
    <author>
      <name>Simson L. Garfinkel</name>
    </author>
    <author>
      <name>J. Spencer Love</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MIT Media Laboratory Tech Report, 1985</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.00402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.00402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.10707v1</id>
    <updated>2021-01-26T11:03:02Z</updated>
    <published>2021-01-26T11:03:02Z</published>
    <title>Enhancing Application Performance by Memory Partitioning in Android
  Platforms</title>
    <summary>  This paper suggests a new memory partitioning scheme that can enhance process
lifecycle, while avoiding Low Memory Killer and Out-of-Memory Killer operations
on mobile devices. Our proposed scheme offers the complete concept of virtual
memory nodes in operating systems of Android devices.
</summary>
    <author>
      <name>Geunsik Lim</name>
    </author>
    <author>
      <name>Changwoo Min</name>
    </author>
    <author>
      <name>Young Ik Eom</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCE.2013.6487055</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCE.2013.6487055" rel="related"/>
    <link href="http://arxiv.org/abs/2101.10707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.10707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.05762v1</id>
    <updated>2023-08-08T20:47:09Z</updated>
    <published>2023-08-08T20:47:09Z</published>
    <title>HotOS XIX Panel Report: Panel on Future of Reproduction and Replication
  of Systems Research</title>
    <summary>  At HotOS XIX (2023), we organized a panel to discuss the future of
reproducibility and replication in systems research. In this document, we
highlight the key points and themes that were discussed in the panel and
summarize the various opinions shared by both the panelists as well as the
HotOS attendees.
</summary>
    <author>
      <name>Roberta De Viti</name>
    </author>
    <author>
      <name>Solal Pirelli</name>
    </author>
    <author>
      <name>Vaastav Anand</name>
    </author>
    <link href="http://arxiv.org/abs/2308.05762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.05762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.05399v1</id>
    <updated>2023-10-09T04:38:51Z</updated>
    <published>2023-10-09T04:38:51Z</published>
    <title>Towards a debuggable kernel design</title>
    <summary>  This paper describes what it means for a kernel to be debuggable and proposes
a kernel design with debuggability in mind. We evaluate the proposed kernel
design by comparing the iterations required in cyclic debugging for different
classes of bugs in a vanilla monolithic kernel to a variant enhanced with our
design rules for debuggability. We discuss the trade offs involved in designing
a debuggable kernel.
</summary>
    <author>
      <name>Chandrika Parimoo</name>
    </author>
    <author>
      <name>Ashish Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.05399v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.05399v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.03488v1</id>
    <updated>2024-09-05T12:56:37Z</updated>
    <published>2024-09-05T12:56:37Z</published>
    <title>Head-First Memory Allocation on Best-Fit with Space-Fitting</title>
    <summary>  Although best-fit is known to be slow, it excels at optimizing memory space
utilization. Interestingly, by keeping the free memory region at the top of the
memory, the process of memory allocation and deallocation becomes approximately
34.86% faster while also maintaining external fragmentation at minimum.
</summary>
    <author>
      <name>Adam Noto Hakarsa</name>
    </author>
    <link href="http://arxiv.org/abs/2409.03488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.03488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9903014v1</id>
    <updated>1999-03-22T21:24:35Z</updated>
    <published>1999-03-22T21:24:35Z</published>
    <title>Perpetual Adaptation of Software to Hardware: An Extensible Architecture
  for Providing Code Optimization as a Central System Service</title>
    <summary>  We present an open architecture for just-in-time code generation and dynamic
code optimization that is flexible, customizable, and extensible. While
previous research has primarily investigated functional aspects of such a
system, architectural aspects have so far remained unexplored. In this paper,
we argue that these properties are important to generate optimal code for a
variety of hardware architectures and different processor generations within
processor families. These properties are also important to make system-level
code generation useful in practice.
</summary>
    <author>
      <name>Thomas Kistler</name>
    </author>
    <author>
      <name>Michael Franz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9903014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9903014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0111035v1</id>
    <updated>2001-11-10T03:30:11Z</updated>
    <published>2001-11-10T03:30:11Z</published>
    <title>Open Source Real Time Operating Systems Overview</title>
    <summary>  Modern control systems applications are often built on top of a real time
operating system (RTOS) which provides the necessary hardware abstraction as
well as scheduling, networking and other services. Several open source RTOS
solutions are publicly available, which is very attractive, both from an
economic (no licensing fees) as well as from a technical (control over the
source code) point of view. This contribution gives an overview of the RTLinux
and RTEMS systems (architecture, development environment, API etc.). Both
systems feature most popular CPUs, several APIs (including Posix), networking,
portability and optional commercial support. Some performance figures are
presented, focusing on interrupt latency and context switching delay.
</summary>
    <author>
      <name>Till Straumann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk at ICALEPCS 2001 Conference, Nov 2001, San Jose, USA, (WEBT001),
  3 pages, LaTex</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">eConf C011127 (2001) WEBT001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0111035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0111035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0309044v1</id>
    <updated>2003-09-23T13:57:01Z</updated>
    <published>2003-09-23T13:57:01Z</published>
    <title>The combinatorics of resource sharing</title>
    <summary>  We discuss general models of resource-sharing computations, with emphasis on
the combinatorial structures and concepts that underlie the various deadlock
models that have been proposed, the design of algorithms and deadlock-handling
policies, and concurrency issues. These structures are mostly graph-theoretic
in nature, or partially ordered sets for the establishment of priorities among
processes and acquisition orders on resources. We also discuss graph-coloring
concepts as they relate to resource sharing.
</summary>
    <author>
      <name>V. C. Barbosa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-1-4757-3609-0_2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-1-4757-3609-0_2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">R. Correa et alii (eds.), Models for Parallel and Distributed
  Computation, pp. 27-52. Kluwer Academic Publishers, Dordrecht, The
  Netherlands, 2002</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0309044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0309044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.3; D.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0403013v1</id>
    <updated>2004-03-11T04:01:51Z</updated>
    <published>2004-03-11T04:01:51Z</published>
    <title>Predictable Software -- A Shortcut to Dependable Computing ?</title>
    <summary>  Many dependability techniques expect certain behaviors from the underlying
subsystems and fail in chaotic ways if these expectations are not met. Under
expected circumstances, however, software tends to work quite well. This paper
suggests that, instead of fixing elusive bugs or rewriting software, we improve
the predictability of conditions faced by our programs. This approach might be
a cheaper and faster way to improve dependability of software. After
identifying some of the common triggers of unpredictability, the paper
describes three engineering principles that hold promise in combating
unpredictability, suggests a way to benchmark predictability, and outlines a
brief research agenda.
</summary>
    <author>
      <name>George Candea</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages; submitted to 11th ACM SIGOPS European Workshop</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">WIP Session, USENIX Technical Conference, Boston, MA, June 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0403013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0403013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410007v2</id>
    <updated>2005-01-09T19:07:05Z</updated>
    <published>2004-10-04T14:49:00Z</published>
    <title>A Shared Write-protected Root Filesystem for a Group of Networked
  Clients</title>
    <summary>  A method to boot a cluster of diskless network clients from a single
write-protected NFS root file system is shown. The problems encountered when
first implementing the setup and their solution are discussed. Finally, the
setup is briefly compared to using a kernel-embedded root file system.
</summary>
    <author>
      <name>Ignatios Souvatzis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 2nd European BSD Conference, 2002, Amsterdam, The
  Netherlands; v2: reformatted to help citation browser</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2nd European BSD Conference, 2002, Amsterdam,
  The Netherlands</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0410007v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410007v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4; D.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502027v1</id>
    <updated>2005-02-04T22:48:31Z</updated>
    <published>2005-02-04T22:48:31Z</published>
    <title>Markets are Dead, Long Live Markets</title>
    <summary>  Researchers have long proposed using economic approaches to resource
allocation in computer systems. However, few of these proposals became
operational, let alone commercial. Questions persist about the economic
approach regarding its assumptions, value, applicability, and relevance to
system design. The goal of this paper is to answer these questions. We find
that market-based resource allocation is useful, and more importantly, that
mechanism design and system design should be integrated to produce systems that
are both economically and computationally efficient.
</summary>
    <author>
      <name>Kevin Lai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fix rotation of figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0502027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.0; D.4.7; K.6.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0601068v1</id>
    <updated>2006-01-14T23:21:31Z</updated>
    <published>2006-01-14T23:21:31Z</published>
    <title>Checkbochs: Use Hardware to Check Software</title>
    <summary>  In this paper, we present a system called Checkbochs, a machine simulator
that checks rules about its guest operating system and applications at the
hardware level. The properties to be checked can be implemented as `plugins' in
the Checkbochs simulator. Some of the properties that were checked using
Checkbochs include null-pointer checks, format-string vulnerabilities,
user/kernel pointer checks, and race-conditions. On implementing these checks,
we were able to uncover previously-unknown bugs in widely used Linux
distributions. We also tested our tools on undergraduate coursework, and found
numerous bugs.
</summary>
    <author>
      <name>Sorav Bansal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0601068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0601068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611055v1</id>
    <updated>2006-11-14T10:11:34Z</updated>
    <published>2006-11-14T10:11:34Z</published>
    <title>A Low-Footprint Class Loading Mechanism for Embedded Java Virtual
  Machines</title>
    <summary>  This paper shows that it is possible to dramatically reduce the memory
consumption of classes loaded in an embedded Java virtual machine without
reducing its functionalities. We describe how to pack the constant pool by
deleting entries which are only used during the class loading process. We
present some benchmarks which demonstrate the efficiency of this mechanism. We
finally suggest some additional optimizations which can be applied if some
restrictions to the functionalities of the virtual machine can be tolerated.
</summary>
    <author>
      <name>Christophe Rippert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Alexandre Courbot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Gilles Grimaud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs, LIFL</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans 3rd ACM International Conference on the Principles and
  Practice of Programming in Java (2004)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0611055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.2748v2</id>
    <updated>2007-06-20T07:31:38Z</updated>
    <published>2007-06-19T09:44:36Z</published>
    <title>A Survey of Unix Init Schemes</title>
    <summary>  In most modern operating systems, init (as in "initialization") is the
program launched by the kernel at boot time. It runs as a daemon and typically
has PID 1. Init is responsible for spawning all other processes and scavenging
zombies. It is also responsible for reboot and shutdown operations. This
document describes existing solutions that implement the init process and/or
init scripts in Unix-like systems. These solutions range from the legacy and
still-in-use BSD and SystemV schemes, to recent and promising schemes from
Ubuntu, Apple, Sun and independent developers. Our goal is to highlight their
focus and compare their sets of features.
</summary>
    <author>
      <name>Yvan Royon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rhône-Alpes</arxiv:affiliation>
    </author>
    <author>
      <name>Stéphane Frénot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rhône-Alpes</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0706.2748v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.2748v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.4558v1</id>
    <updated>2007-09-28T09:04:27Z</updated>
    <published>2007-09-28T09:04:27Z</published>
    <title>Practical Multiwriter Lock-Free Queues for "Hard Real-Time" Systems
  without CAS</title>
    <summary>  FIFO queues with a single reader and writer can be insufficient for "hard
real-time" systems where interrupt handlers require wait-free guarantees when
writing to message queues. We present an algorithm which elegantly and
practically solves this problem on small processors that are often found in
embedded systems. The algorithm does not require special CPU instructions (such
as atomic CAS), and therefore is more robust than many existing methods that
suffer the ABA problem associated with swing pointers. The algorithm gives
"first-in, almost first-out" guarantees under pathological interrupt
conditions, which manifests as arbitrary "shoving" among nearly-simultaneous
arrivals at the end of the queue.
</summary>
    <author>
      <name>Jeremy Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages, Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/0709.4558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.4558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4635v1</id>
    <updated>2007-10-25T08:09:07Z</updated>
    <published>2007-10-25T08:09:07Z</published>
    <title>OS Debugging Method Using a Lightweight Virtual Machine Monitor</title>
    <summary>  Demands for implementing original OSs that can achieve high I/O performance
on PC/AT compatible hardware have recently been increasing, but conventional OS
debugging environments have not been able to simultaneously assure their
stability, be easily customized to new OSs and new I/O devices, and assure
efficient execution of I/O operations. We therefore developed a novel OS
debugging method using a lightweight virtual machine. We evaluated this
debugging method experimentally and confirmed that it can transfer data about
5.4 times as fast as the conventional virtual machine monitor.
</summary>
    <author>
      <name>Tadashi Takeuchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4746v1</id>
    <updated>2007-10-25T09:47:35Z</updated>
    <published>2007-10-25T09:47:35Z</published>
    <title>RTK-Spec TRON: A Simulation Model of an ITRON Based RTOS Kernel in
  SystemC</title>
    <summary>  This paper presents the methodology and the modeling constructs we have
developed to capture the real time aspects of RTOS simulation models in a
System Level Design Language (SLDL) like SystemC. We describe these constructs
and show how they are used to build a simulation model of an RTOS kernel
targeting the $\mu$-ITRON OS specification standard.
</summary>
    <author>
      <name>M. Abdelsalam Hassan</name>
    </author>
    <author>
      <name>Keishi Sakanushi</name>
    </author>
    <author>
      <name>Yoshinori Takeuchi</name>
    </author>
    <author>
      <name>Masaharu Imai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.2958v2</id>
    <updated>2008-03-10T16:10:50Z</updated>
    <published>2007-12-18T13:42:45Z</published>
    <title>Power-Aware Real-Time Scheduling upon Identical Multiprocessor Platforms</title>
    <summary>  In this paper, we address the power-aware scheduling of sporadic
constrained-deadline hard real-time tasks using dynamic voltage scaling upon
multiprocessor platforms. We propose two distinct algorithms. Our first
algorithm is an off-line speed determination mechanism which provides an
identical speed for each processor. That speed guarantees that all deadlines
are met if the jobs are scheduled using EDF. The second algorithm is an on-line
and adaptive speed adjustment mechanism which reduces the energy consumption
while the system is running.
</summary>
    <author>
      <name>Vincent Nélis</name>
    </author>
    <author>
      <name>Joël Goossens</name>
    </author>
    <author>
      <name>Nicolas Navet</name>
    </author>
    <author>
      <name>Raymond Devillers</name>
    </author>
    <author>
      <name>Dragomir Milojevic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The manuscript corresponds to the final version of SUTC 2008
  conference</arxiv:comment>
    <link href="http://arxiv.org/abs/0712.2958v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.2958v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.0200v2</id>
    <updated>2008-09-08T09:22:24Z</updated>
    <published>2008-05-02T09:32:03Z</published>
    <title>(m,k)-firm constraints and DBP scheduling: impact of the initial
  k-sequence and exact schedulability test</title>
    <summary>  In this paper we study the scheduling of (m,k)-firm synchronous periodic task
systems using the Distance Based Priority (DBP) scheduler. We first show three
phenomena: (i) choosing, for each task, the initial k-sequence 1^k is not
optimal, (ii) we can even start the scheduling from a (fictive) error state (in
regard to the initial k-sequence) and (iii) the period of feasible
DBP-schedules is not necessarily the task hyper-period. We then show that any
feasible DBP-schedule is periodic and we upper-bound the length of that period.
Lastly, based on our periodicity result we provide an exact schedulability
test.
</summary>
    <author>
      <name>Joël Goossens</name>
    </author>
    <link href="http://arxiv.org/abs/0805.0200v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.0200v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.3237v1</id>
    <updated>2008-05-21T09:38:15Z</updated>
    <published>2008-05-21T09:38:15Z</published>
    <title>Integrating Job Parallelism in Real-Time Scheduling Theory</title>
    <summary>  We investigate the global scheduling of sporadic, implicit deadline,
real-time task systems on multiprocessor platforms. We provide a task model
which integrates job parallelism. We prove that the time-complexity of the
feasibility problem of these systems is linear relatively to the number of
(sporadic) tasks for a fixed number of processors. We propose a scheduling
algorithm theoretically optimal (i.e., preemptions and migrations neglected).
Moreover, we provide an exact feasibility utilization bound. Lastly, we propose
a technique to limit the number of migrations and preemptions.
</summary>
    <author>
      <name>S. Collette</name>
    </author>
    <author>
      <name>L. Cucu</name>
    </author>
    <author>
      <name>J. Goossens</name>
    </author>
    <link href="http://arxiv.org/abs/0805.3237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.3237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.3933v1</id>
    <updated>2008-07-24T17:51:07Z</updated>
    <published>2008-07-24T17:51:07Z</published>
    <title>Interface Matching and Combining Techniques for Services Integration</title>
    <summary>  The development of many highly dynamic environments, like pervasive
environments, introduces the possibility to use geographically close-related
services. Dynamically integrating and unintegrating these services in running
applications is a key challenge for this use. In this article, we classify
service integration issues according to interfaces exported by services and
internal combining techniques. We also propose a contextual integration
service, IntegServ, and an interface, Integrable, for developing services.
</summary>
    <author>
      <name>Frédéric Le Mouël</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rhône-Alpes / CITI</arxiv:affiliation>
    </author>
    <author>
      <name>Noha Ibrahim</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rhône-Alpes / CITI</arxiv:affiliation>
    </author>
    <author>
      <name>Stéphane Frénot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rhône-Alpes / CITI</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans 3er Congreso Nacional de Ciencias de la Computacion
  (CNCC'2005) (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0807.3933v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.3933v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.0268v1</id>
    <updated>2009-06-01T12:10:10Z</updated>
    <published>2009-06-01T12:10:10Z</published>
    <title>MORA: an Energy-Aware Slack Reclamation Scheme for Scheduling Sporadic
  Real-Time Tasks upon Multiprocessor Platforms</title>
    <summary>  In this paper, we address the global and preemptive energy-aware scheduling
problem of sporadic constrained-deadline tasks on DVFS-identical multiprocessor
platforms. We propose an online slack reclamation scheme which profits from the
discrepancy between the worst- and actual-case execution time of the tasks by
slowing down the speed of the processors in order to save energy. Our algorithm
called MORA takes into account the application-specific consumption profile of
the tasks. We demonstrate that MORA does not jeopardize the system
schedulability and we show by performing simulations that it can save up to 32%
of energy (in average) compared to execution without using any energy-aware
algorithm.
</summary>
    <author>
      <name>Vincent Nelis</name>
    </author>
    <author>
      <name>Joel Goossens</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.0268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.0268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.5577v1</id>
    <updated>2009-10-29T08:38:32Z</updated>
    <published>2009-10-29T08:38:32Z</published>
    <title>On the stability of two-chunk file-sharing systems</title>
    <summary>  We consider five different peer-to-peer file sharing systems with two chunks,
with the aim of finding chunk selection algorithms that have provably stable
performance with any input rate and assuming non-altruistic peers who leave the
system immediately after downloading the second chunk. We show that many
algorithms that first looked promising lead to unstable or oscillating
behavior. However, we end up with a system with desirable properties. Most of
our rigorous results concern the corresponding deterministic large system
limits, but in two simplest cases we provide proofs for the stochastic systems
also.
</summary>
    <author>
      <name>Ilkka Norros</name>
    </author>
    <author>
      <name>Hannu Reittu</name>
    </author>
    <author>
      <name>Timo Eirola</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11134-011-9209-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11134-011-9209-2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Queueing Systems (2011) 67: 183</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0910.5577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.5577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60K25, 68M14" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.4062v1</id>
    <updated>2009-12-20T23:49:25Z</updated>
    <published>2009-12-20T23:49:25Z</published>
    <title>Process Description of COM Object Life Cycle</title>
    <summary>  The objective of this article is to provide for the reader a basic
description of all the steps involved in the COM object life-cycle process. COM
is a software technology and process performer. The first section briefly
introduces the Component Object Model (COM), considering the process of the COM
object life cycle as the baseline of all COM issues. The second part describes
in detail the basic steps of the process - client request, server location,
object creation, interaction, and disconnection. A brief description is given
for the components involved in each step. Finally, the third section provides a
brief conclusion summarizing all the process steps.
</summary>
    <author>
      <name>Emil Vassev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0912.4062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.4062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3727v1</id>
    <updated>2010-01-21T06:02:41Z</updated>
    <published>2010-01-21T06:02:41Z</published>
    <title>Fault Tolerance in Real Time Multiprocessors - Embedded Systems</title>
    <summary>  All real time tasks which are termed as critical tasks by nature have to
complete its execution before its deadline, even in presence of faults. The
most popularly used real time task assignment algorithms are First Fit (FF),
Best Fit (BF), Bin Packing (BP).The common task scheduling algorithms are Rate
Monotonic (RM), Earliest Deadline First (EDF) etc.All the current approaches
deal with either fault tolerance or criticality in real time. In this paper we
have proposed an integrated approach with a new algorithm, called SASA (Sorting
And Sequential Assignment) which maps the real time task assignment with task
schedule and fault tolerance
</summary>
    <author>
      <name>A. Christy Persya</name>
    </author>
    <author>
      <name>T. R. Gopalakrishnan Nair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Fourth Innovative Conference on Embedded Systems, Mobile
  Communication and Computing, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.3727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.1336v2</id>
    <updated>2010-03-09T13:02:01Z</updated>
    <published>2010-03-05T20:34:44Z</published>
    <title>FIFO anomaly is unbounded</title>
    <summary>  Virtual memory of computers is usually implemented by demand paging. For some
page replacement algorithms the number of page faults may increase as the
number of page frames increases. Belady, Nelson and Shedler constructed
reference strings for which page replacement algorithm FIFO produces near twice
more page faults in a larger memory than in a smaller one. They formulated the
conjecture that 2 is a general bound. We prove that this ratio can be
arbitrarily large.
</summary>
    <author>
      <name>Peter Fornai</name>
    </author>
    <author>
      <name>Antal Ivanyi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Acta Univ. Sapientiae, Informatica, 2,1 (2010) 80-89</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.1336v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.1336v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.2104v1</id>
    <updated>2010-06-10T18:13:36Z</updated>
    <published>2010-06-10T18:13:36Z</published>
    <title>Perbandingan Shell Unix</title>
    <summary>  Is it possible for an Information Technology [IT] product to be both mature
and state-of-theart at the same time? In the case of the UNIX system, the
answer is an unqualified "Yes." The UNIX system has continued to develop over
the past twenty-five years. In millions of installations running on nearly
every hardware platform made, the UNIX system has earned its reputation for
stability and scalability. Over the years, UNIX system suppliers have steadily
assimilated new technologies so that UNIX systems today provide more
functionality as any other operating system.
</summary>
    <author>
      <name>Spits Warnars H. L. H</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Widya, Vol 21,No. 230, pp. 9-15, November 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.2104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.2104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.2617v1</id>
    <updated>2010-06-14T07:17:14Z</updated>
    <published>2010-06-14T07:17:14Z</published>
    <title>Gang FTP scheduling of periodic and parallel rigid real-time tasks</title>
    <summary>  In this paper we consider the scheduling of periodic and parallel rigid
tasks. We provide (and prove correct) an exact schedulability test for Fixed
Task Priority (FTP) Gang scheduler sub-classes: Parallelism Monotonic, Idling,
Limited Gang, and Limited Slack Reclaiming. Additionally, we study the
predictability of our schedulers: we show that Gang FJP schedulers are not
predictable and we identify several sub-classes which are actually predictable.
Moreover, we extend the definition of rigid, moldable and malleable jobs to
recurrent tasks.
</summary>
    <author>
      <name>Joël Goossens</name>
    </author>
    <author>
      <name>Vandy Berten</name>
    </author>
    <link href="http://arxiv.org/abs/1006.2617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.2617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.1717v1</id>
    <updated>2011-03-09T07:25:00Z</updated>
    <published>2011-03-09T07:25:00Z</published>
    <title>Efficient and Playful Tools to Teach Unix to New Students</title>
    <summary>  Teaching Unix to new students is a common tasks in many higher schools. This
paper presents an approach to such course where the students progress
autonomously with the help of the teacher. The traditional textbook is
complemented with a wiki, and the main thread of the course is a game, in the
form of a treasure hunt. The course finishes with a lab exam, where students
have to perform practical manipulations similar to the ones performed during
the treasure hunt. The exam is graded fully automatically. This paper discusses
the motivations and advantages of the approach, and gives an overall view of
the tools we developed. The tools are available from the web, and open-source,
hence re-usable outside the Ensimag.
</summary>
    <author>
      <name>Matthieu Moy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">VERIMAG - IMAG</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ITiCSE, Darmstadt : Germany (2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.1717v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.1717v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.2110v1</id>
    <updated>2011-04-12T04:06:34Z</updated>
    <published>2011-04-12T04:06:34Z</published>
    <title>Deterministic Real-time Thread Scheduling</title>
    <summary>  Race condition is a timing sensitive problem. A significant source of timing
variation comes from nondeterministic hardware interactions such as cache
misses. While data race detectors and model checkers can check races, the
enormous state space of complex software makes it difficult to identify all of
the races and those residual implementation errors still remain a big
challenge. In this paper, we propose deterministic real-time scheduling methods
to address scheduling nondeterminism in uniprocessor systems. The main idea is
to use timing insensitive deterministic events, e.g, an instruction counter, in
conjunction with a real-time clock to schedule threads. By introducing the
concept of Worst Case Executable Instructions (WCEI), we guarantee both
determinism and real-time performance.
</summary>
    <author>
      <name>Heechul Yun</name>
    </author>
    <author>
      <name>Cheolgi Kim</name>
    </author>
    <author>
      <name>Lui Sha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RTAS11 Work-In-Progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.2110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.2110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.3523v1</id>
    <updated>2011-04-18T15:39:21Z</updated>
    <published>2011-04-18T15:39:21Z</published>
    <title>An Optimal Real-Time Scheduling Approach: From Multiprocessor to
  Uniprocessor</title>
    <summary>  An optimal solution to the problem of scheduling real-time tasks on a set of
identical processors is derived. The described approach is based on solving an
equivalent uniprocessor real-time scheduling problem. Although there are other
scheduling algorithms that achieve optimality, they usually impose prohibitive
preemption costs. Unlike these algorithms, it is observed through simulation
that the proposed approach produces no more than three preemptions points per
job.
</summary>
    <author>
      <name>Paul Regnier</name>
    </author>
    <author>
      <name>George Lima</name>
    </author>
    <author>
      <name>Ernesto Massa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages - rejected for publication by ECRTS 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.3523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.3523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="98B35, 68M20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.3075v1</id>
    <updated>2011-09-14T13:26:07Z</updated>
    <published>2011-09-14T13:26:07Z</published>
    <title>Design and Performance Evaluation of A New Proposed Fittest Job First
  Dynamic Round Robin(FJFDRR) Scheduling Algorithm</title>
    <summary>  In this paper, we have proposed a new variant of Round Robin scheduling
algorithm by executing the processes according to the new calculated Fit Factor
f and using the concept of dynamic time quantum. We have compared the
performance of our proposed Fittest Job First Dynamic Round Robin(FJFDRR)
algorithm with the Priority Based Static Round Robin(PBSRR) algorithm.
Experimental results show that our proposed algorithm performs better than
PBSRR in terms of reducing the number of context switches, average waiting time
and average turnaround time.
</summary>
    <author>
      <name>Rakesh Mohanty</name>
    </author>
    <author>
      <name>Manas Das</name>
    </author>
    <author>
      <name>M. Lakshmi Prasanna</name>
    </author>
    <author>
      <name> Sudhashree</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">05 Pages, 12 Figures, International Journal of Computer Information
  Systems Vol. 2, No. 2, February 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.3075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.3075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.6423v1</id>
    <updated>2012-05-29T17:10:29Z</updated>
    <published>2012-05-29T17:10:29Z</published>
    <title>Proposed Challenges And Areas of Concern in Operating System Research
  and Development</title>
    <summary>  Computers are a very important part of our lives and the major reason why
they have been such a success is because of the excellent graphical operating
systems that run on these powerful machines. As the computer hardware is
becoming more and more powerful, it is also vital to keep the software updated
in order to utilize the hardware of the system efficiently and make it faster
and smarter. This paper highlights some core issues that if dealt with in the
operating system level would make use of the full potential of the computer
hardware and provide an excellent user experience.
</summary>
    <author>
      <name>Plawan Kumar Rath</name>
    </author>
    <author>
      <name>G. N. Anil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages; International Journal for Computer Science Issues(IJCSI),
  Volume 9, Issue 2, March 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.6423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.6423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.6390v1</id>
    <updated>2012-08-31T06:31:48Z</updated>
    <published>2012-08-31T06:31:48Z</published>
    <title>Performance Evaluation of Flash File Systems</title>
    <summary>  Today, flash memory are strongly used in the embedded system domain. NAND
flash memories are the building block of main secondary storage systems. Such
memories present many benefits in terms of data density, I/O performance, shock
resistance and power consumption. Nevertheless, flash does not come without
constraints: the write / erase granularity asymmetry and the limited lifetime
bring the need for specific management. This can be done through the operating
system using dedicated Flash File Systems (FFSs). In this document, we present
general concepts about FFSs, and implementations example that are JFFS2, YAFFS2
and UBIFS, the most commonly used flash file systems. Then we give performance
evaluation results for these FFSs.
</summary>
    <author>
      <name>Pierre Olivier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Jalil Boukhobza</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Senn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Colloque du GDR SoC-SiP, Paris : France (2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.6390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.6390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.3502v1</id>
    <updated>2012-12-14T15:33:45Z</updated>
    <published>2012-12-14T15:33:45Z</published>
    <title>Adaptive Scheduling in Real-Time Systems Through Period Adjustment</title>
    <summary>  Real time system technology traditionally developed for safety critical
systems, has now been extended to support multimedia systems and virtual
reality. A large number of real-time application, related to multimedia and
adaptive control system, require more flexibility than classical real-time
theory usually permits. This paper proposes an efficient adaptive scheduling
framework in real-time systems based on period adjustment. Under this model
periodic task can change their execution rates based on their importance value
to keep the system underloaded. We propose Period_Adjust algorithm, which
consider the tasks whose periods are bounded as well as the tasks whose periods
are not bounded.
</summary>
    <author>
      <name>Shri Prakash Dwivedi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.3502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.3502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.1747v1</id>
    <updated>2013-02-07T13:52:09Z</updated>
    <published>2013-02-07T13:52:09Z</published>
    <title>Energy Minimization for Parallel Real-Time Systems with Malleable Jobs
  and Homogeneous Frequencies</title>
    <summary>  In this work, we investigate the potential utility of parallelization for
meeting real-time constraints and minimizing energy. We consider malleable Gang
scheduling of implicit-deadline sporadic tasks upon multiprocessors. We first
show the non-necessity of dynamic voltage/frequency regarding optimality of our
scheduling problem. We adapt the canonical schedule for DVFS multiprocessor
platforms and propose a polynomial-time optimal processor/frequency-selection
algorithm. We evaluate the performance of our algorithm via simulations using
parameters obtained from a hardware testbed implementation. Our algorithm has
up to a 60 watt decrease in power consumption over the optimal non-parallel
approach.
</summary>
    <author>
      <name>Nathan Fisher</name>
    </author>
    <author>
      <name>Joël Goossens</name>
    </author>
    <author>
      <name>Pradeep M. Hettiarachchi</name>
    </author>
    <author>
      <name>Antonio Paolillo</name>
    </author>
    <link href="http://arxiv.org/abs/1302.1747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.1747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5109v1</id>
    <updated>2013-02-20T15:50:29Z</updated>
    <published>2013-02-20T15:50:29Z</published>
    <title>Capturing Information Flows inside Android and Qemu Environments</title>
    <summary>  The smartphone market has grown so wide that it assumed a strategic
relevance. Today the most common smartphone OSs are Google's Android and
Apple's iOS. The former is particularly interesting due to its open source
nature, that allows everyone to deeply inspect every aspect of the OS. Android
source code is also bundled with an hardware emulator, based on the open source
software Qemu, that allows the user to run the Android OS without the need of a
physical device. We first present a procedure to extract information flows from
a generic system. We then focus on Android and Qemu architectures and their
logging infrastructures. Finally, we detail what happens inside an Android
device in a particular scenario: the system boot.
</summary>
    <author>
      <name>Marco Sironi</name>
    </author>
    <author>
      <name>Francesco Tisato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.5109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.3849v1</id>
    <updated>2013-05-16T15:54:12Z</updated>
    <published>2013-05-16T15:54:12Z</published>
    <title>On the periodic behavior of real-time schedulers on identical
  multiprocessor platforms</title>
    <summary>  This paper is proposing a general periodicity result concerning any
deterministic and memoryless scheduling algorithm (including
non-work-conserving algorithms), for any context, on identical multiprocessor
platforms. By context we mean the hardware architecture (uniprocessor,
multicore), as well as task constraints like critical sections, precedence
constraints, self-suspension, etc. Since the result is based only on the
releases and deadlines, it is independent from any other parameter. Note that
we do not claim that the given interval is minimal, but it is an upper bound
for any cycle of any feasible schedule provided by any deterministic and
memoryless scheduler.
</summary>
    <author>
      <name>Emmanuel Grolleau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIAS-ISAE/ENSMA</arxiv:affiliation>
    </author>
    <author>
      <name>Joël Goossens</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ULB</arxiv:affiliation>
    </author>
    <author>
      <name>Liliana Cucu-Grosjean</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1305.3849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.3849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.3322v1</id>
    <updated>2013-11-13T22:05:58Z</updated>
    <published>2013-11-13T22:05:58Z</published>
    <title>Impact of Limpware on HDFS: A Probabilistic Estimation</title>
    <summary>  With the advent of cloud computing, thousands of machines are connected and
managed collectively. This era is confronted with a new challenge: performance
variability, primarily caused by large-scale management issues such as hardware
failures, software bugs, and configuration mistakes. In our previous work we
highlighted one overlooked cause: limpware - hardware whose performance
degrades significantly compared to its specification. We showed that limpware
can cause severe impact in current scale-out systems. In this report, we
quantify how often these scenarios happen in Hadoop Distributed File System.
</summary>
    <author>
      <name>Thanh Do</name>
    </author>
    <author>
      <name>Haryadi S. Gunawi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures, detailed probability calculation for SOCC 13
  limplock paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.3322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.3322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4509v1</id>
    <updated>2013-12-16T20:32:52Z</updated>
    <published>2013-12-16T20:32:52Z</published>
    <title>Cache-aware static scheduling for hard real-time multicore systems based
  on communication affinities</title>
    <summary>  The growing need for continuous processing capabilities has led to the
development of multicore systems with a complex cache hierarchy. Such multicore
systems are generally designed for improving the performance in average case,
while hard real-time systems must consider worst-case scenarios. An open
challenge is therefore to efficiently schedule hard real-time tasks on a
multicore architecture. In this work, we propose a mathematical formulation for
computing a static scheduling that minimize L1 data cache misses between hard
real-time tasks on a multicore architecture using communication affinities.
</summary>
    <author>
      <name>Lilia Zaourar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIST</arxiv:affiliation>
    </author>
    <author>
      <name>Mathieu Jan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIST</arxiv:affiliation>
    </author>
    <author>
      <name>Maurice Pitel</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">34th IEEE Real-Time Systems Symposium (RTSS'13), WiP session,
  Vancouver : Canada (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1312.4509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.0631v1</id>
    <updated>2014-02-04T06:26:34Z</updated>
    <published>2014-02-04T06:26:34Z</published>
    <title>LWRP: Low Power Consumption Weighting Replacement Policy using Buffer
  Memory</title>
    <summary>  As the performance gap between memory and processors has increased, then it
leads to the poor performance. Efficient virtual memory can overcome this
problem. And the efficiency of virtual memory depends on the replacement policy
used for cache. In this paper, our algorithm not only based on the time to last
access and frequency index but, we also consider the power consumption. We show
that Low Power Consumption Weighting Replacement Policy (LWRP) has better
performance and low power consumption.
</summary>
    <author>
      <name>S. R. Bhalgama</name>
    </author>
    <author>
      <name>C. C. Kavar</name>
    </author>
    <author>
      <name>S. S. Parmar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V7P142</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V7P142" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures, Published with International Journal of Computer
  Trends and Technology (IJCTT)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCTT 7(3):147-150, January 2014. Published by Seventh Sense
  Research Group</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.0631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.0631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.0990v1</id>
    <updated>2014-08-02T19:56:24Z</updated>
    <published>2014-08-02T19:56:24Z</published>
    <title>Assessment of Response Time for New Multi Level Feedback Queue Scheduler</title>
    <summary>  Response time is one of the characteristics of scheduler, happens to be a
prominent attribute of any CPU scheduling algorithm. The proposed New Multi
Level Feedback Queue [NMLFQ] Scheduler is compared with dynamic, real time,
Dependent Activity Scheduling Algorithm (DASA) and Lockes Best Effort
Scheduling Algorithm (LBESA). We abbreviated beneficial result of NMLFQ
scheduler in comparison with dynamic best effort schedulers with respect to
response time.
</summary>
    <author>
      <name>M. V. Panduranga Rao</name>
    </author>
    <author>
      <name>K. C. Shet</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V13P124</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V13P124" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Trends and Technology (IJCTT),
  Volume 13, Number 3, Pages: 113-119, July 2014. ISSN:2231-2803</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.0990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.0990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.01509v1</id>
    <updated>2015-02-05T11:36:45Z</updated>
    <published>2015-02-05T11:36:45Z</published>
    <title>OS-level Failure Injection with SystemTap</title>
    <summary>  Failure injection in distributed systems has been an important issue to
experiment with robust, resilient distributed systems. In order to reproduce
real-life conditions, parts of the application must be killed without letting
the operating system close the existing network communications in a "clean"
way. When a process is simply killed, the OS closes them. SystemTap is a an
infrastructure that probes the Linux kernel's internal calls. If processes are
killed at kernel-level, they can be destroyed without letting the OS do
anything else. In this paper, we present a kernel-level failure injection
system based on SystemTap. We present how it can be used to implement
deterministic and probabilistic failure scenarios.
</summary>
    <author>
      <name>Camille Coti</name>
    </author>
    <author>
      <name>Nicolas Greneche</name>
    </author>
    <link href="http://arxiv.org/abs/1502.01509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.01509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.02517v2</id>
    <updated>2015-04-13T13:39:37Z</updated>
    <published>2015-04-09T23:17:01Z</published>
    <title>Survey of Operating Systems for the IoT Environment</title>
    <summary>  This paper is a comprehensive survey of the various operating systems
available for the Internet of Things environment. At first the paper introduces
the various aspects of the operating systems designed for the IoT environment
where resource constraint poses a huge problem for the operation of the general
OS designed for the various computing devices. The latter part of the paper
describes the various OS available for the resource constraint IoT environment
along with the various platforms each OS supports, the software development
kits available for the development of applications in the respective OS along
with the various protocols implemented in these OS for the purpose of
communication and networking.
</summary>
    <author>
      <name>Tuhin Borgohain</name>
    </author>
    <author>
      <name>Uday Kumar</name>
    </author>
    <author>
      <name>Sugata Sanyal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.02517v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02517v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01984v2</id>
    <updated>2016-03-10T11:04:54Z</updated>
    <published>2015-12-07T11:16:43Z</published>
    <title>Parallel and sequential reclaiming in multicore real-time global
  scheduling</title>
    <summary>  When integrating hard, soft and non-real-time tasks in general purpose
operating systems, it is necessary to provide temporal isolation so that the
timing properties of one task do not depend on the behaviour of the others.
However, strict budget enforcement can lead to inefficient use of the
computational resources in the presence of tasks with variable workload. Many
resource reclaiming algorithms have been proposed in the literature for single
processor scheduling, but not enough work exists for global scheduling in
multiprocessor systems. In this report, we propose two reclaiming algorithms
for multiprocessor global scheduling and we prove their correctness.
</summary>
    <author>
      <name>Luca Abeni</name>
    </author>
    <author>
      <name>Giuseppe Lipari</name>
    </author>
    <author>
      <name>Andrea Parri</name>
    </author>
    <author>
      <name>Youcheng Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1512.01984v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01984v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.06908v1</id>
    <updated>2015-12-21T23:35:42Z</updated>
    <published>2015-12-21T23:35:42Z</published>
    <title>Research on Scalability of Operating Systems on Multicore Processors</title>
    <summary>  Large number of cores and hardware resource sharing are two characteristics
on multicore processors, which bring new challenges for the design of operating
systems. How to locate and analyze the speedup restrictive factors in operating
systems, how to simulate and avoid the phenomenon that speedup decreases with
the number of cores because of lock contention (i.e., lock thrashing) and how
to avoid the contention of shared resources such as the last level cache are
key challenges for the operating system scalability research on multicore
systems.
</summary>
    <author>
      <name>Yan Cui</name>
    </author>
    <link href="http://arxiv.org/abs/1512.06908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.06908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05810v1</id>
    <updated>2016-05-19T05:03:00Z</updated>
    <published>2016-05-19T05:03:00Z</published>
    <title>The Design of the NetBSD I/O Subsystems</title>
    <summary>  This book describes the source code of the NetBSD Operating System Release
1.6 in SUN UltraSPARC 64-bit platform by annotating related excerpts from
references and user manuals on the NetBSD Operating System. The goal of this
book is to provide necessary information to understand the operation and the
implementation of I/O subsystems in the kernel as well as to design and
implement a new filesystem on the NetBSD platform.
</summary>
    <author>
      <name>SungWon Chung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This arXiv archival version is the same as the initial 2002 release
  of this publication except updates in preface and few corrections on typos
  and contact information</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00111v2</id>
    <updated>2016-06-02T05:32:46Z</updated>
    <published>2016-06-01T04:29:58Z</published>
    <title>It's Time: OS Mechanisms for Enforcing Asymmetric Temporal Integrity</title>
    <summary>  Mixed-criticality systems combine real-time components of different levels of
criticality, i.e. severity of failure, on the same processor, in order to
obtain good resource utilisation. They must guarantee deadlines of
highly-critical tasks at the expense of lower-criticality ones in the case of
overload. Present operating systems provide inadequate support for this kind of
system, which is of growing importance in avionics and other verticals. We
present an approach that provides the required asymmetric integrity and its
implementation in the high-assurance seL4 microkernel.
</summary>
    <author>
      <name>Anna Lyons</name>
    </author>
    <author>
      <name>Gernot Heiser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper submitted to OSDI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00111v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00111v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01198v1</id>
    <updated>2017-04-04T21:43:04Z</updated>
    <published>2017-04-04T21:43:04Z</published>
    <title>Tackling Diversity and Heterogeneity by Vertical Memory Management</title>
    <summary>  Existing memory management mechanisms used in commodity computing machines
typically adopt hardware based address interleaving and OS directed random
memory allocation to service generic application requests. These conventional
memory management mechanisms are challenged by contention at multiple memory
levels, a daunting variety of workload behaviors, and an increasingly
complicated memory hierarchy. Our ISCA-41 paper proposes vertical partitioning
to eliminate shared resource contention at multiple levels in the memory
hierarchy. Combined with horizontal memory management policies, our framework
supports a flexible policy space for tackling diverse application needs in
production environment and is suitable for future heterogeneous memory systems.
</summary>
    <author>
      <name>Lei Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1704.01198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02621v1</id>
    <updated>2017-05-26T16:39:55Z</updated>
    <published>2017-05-26T16:39:55Z</published>
    <title>Design and Implementation of Modified Fuzzy based CPU Scheduling
  Algorithm</title>
    <summary>  CPU Scheduling is the base of multiprogramming. Scheduling is a process which
decides order of task from a set of multiple tasks that are ready to execute.
There are number of CPU scheduling algorithms available, but it is very
difficult task to decide which one is better. This paper discusses the design
and implementation of modified fuzzy based CPU scheduling algorithm. This paper
present a new set of fuzzy rules. It demonstrates that scheduling done with new
priority improves average waiting time and average turnaround time.
</summary>
    <author>
      <name>Rajani Kumari</name>
    </author>
    <author>
      <name>Vivek Kumar Sharma</name>
    </author>
    <author>
      <name>Sandeep Kumar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/13612-1323 10.5120/13612-1323 10.5120/13612-1323
  10.5120/13612-1323</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/13612-1323" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.5120/13612-1323" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.5120/13612-1323" rel="related"/>
    <link title="doi" href="http://dx.doi.org/" rel="related"/>
    <link title="doi" href="http://dx.doi.org/" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.5120/13612-1323" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications, Volume 77, No 17,
  September 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.02621v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02621v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06450v1</id>
    <updated>2017-08-21T23:10:59Z</updated>
    <published>2017-08-21T23:10:59Z</published>
    <title>Entirely protecting operating systems against transient errors in space
  environment</title>
    <summary>  In this article, we propose a mainly-software hardening technique to totally
protect unmodified running operating systems on COTS hardware against transient
errors in heavily radiation - flooded environment like high altitude space. The
technique is currently being implemented in a hypervisor and allows to control
the upper layers of the software stack (operating system and applications). The
rest of the system, the hypervisor, will be protected by other means, thus
resulting in a completely protected system against transient errors. The
induced overhead turns around 200% but this is expected to decrease with future
improvements.
</summary>
    <author>
      <name>Mahoukpégo Parfait Tokponnon</name>
    </author>
    <author>
      <name>Marc Lobelle</name>
    </author>
    <author>
      <name>Eugene C. Ezin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 4 figures, Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.06450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01589v2</id>
    <updated>2018-06-11T14:25:54Z</updated>
    <published>2018-06-05T10:03:40Z</published>
    <title>Blocking time under basic priority inheritance: Polynomial bound and
  exact computation</title>
    <summary>  The Priority Inheritance Protocol (PIP) is arguably the best-known protocol
for resource sharing under real-time constraints. Its importance in modern
applications is undisputed. Nevertheless, because jobs may be blocked under PIP
for a variety of reasons, determining a job's maximum blocking time could be
difficult, and thus far no exact method has been proposed that does it.
Existing analysis methods are inefficient, inaccurate, and of limited
applicability. This article proposes a new characterization of the problem,
thus allowing a polynomial method for bounding the blocking time, and an exact,
optimally efficient method for blocking time computation under priority
inheritance that have a general applicability.
</summary>
    <author>
      <name>Paolo Torroni</name>
    </author>
    <author>
      <name>Zeynep Kiziltan</name>
    </author>
    <author>
      <name>Eugenio Faldella</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01589v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01589v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.05345v2</id>
    <updated>2018-10-15T23:38:53Z</updated>
    <published>2018-10-12T03:54:52Z</published>
    <title>Time Protection: the Missing OS Abstraction</title>
    <summary>  Timing channels enable data leakage that threatens the security of computer
systems, from cloud platforms to smartphones and browsers executing untrusted
third-party code. Preventing unauthorised information flow is a core duty of
the operating system, however, present OSes are unable to prevent timing
channels. We argue that OSes must provide time protection in addition to the
established memory protection. We examine the requirements of time protection,
present a design and its implementation in the seL4 microkernel, and evaluate
its efficacy as well as performance overhead on Arm and x86 processors.
</summary>
    <author>
      <name>Qian Ge</name>
    </author>
    <author>
      <name>Yuval Yarom</name>
    </author>
    <author>
      <name>Tom Chothia</name>
    </author>
    <author>
      <name>Gernot Heiser</name>
    </author>
    <link href="http://arxiv.org/abs/1810.05345v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.05345v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.08338v1</id>
    <updated>2019-01-24T10:44:31Z</updated>
    <published>2019-01-24T10:44:31Z</published>
    <title>Can We Prove Time Protection?</title>
    <summary>  Timing channels are a significant and growing security threat in computer
systems, with no established solution. We have recently argued that the OS must
provide time protection, in analogy to the established memory protection, to
protect applications from information leakage through timing channels. Based on
a recently-proposed implementation of time protection in the seL4 microkernel,
we investigate how such an implementation could be formally proved to prevent
timing channels. We postulate that this should be possible by reasoning about a
highly abstracted representation of the shared hardware resources that cause
timing channels.
</summary>
    <author>
      <name>Gernot Heiser</name>
    </author>
    <author>
      <name>Gerwin Klein</name>
    </author>
    <author>
      <name>Toby Murray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.08338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.08338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.01558v1</id>
    <updated>2019-10-03T15:48:44Z</updated>
    <published>2019-10-03T15:48:44Z</published>
    <title>SEUSS: Rapid serverless deployment using environment snapshots</title>
    <summary>  Modern FaaS systems perform well in the case of repeat executions when
function working sets stay small. However, these platforms are less effective
when applied to more complex, large-scale and dynamic workloads. In this paper,
we introduce SEUSS (serverless execution via unikernel snapshot stacks), a new
system-level approach for rapidly deploying serverless functions. Through our
approach, we demonstrate orders of magnitude improvements in function start
times and cacheability, which improves common re-execution paths while also
unlocking previously-unsupported large-scale bursty workloads.
</summary>
    <author>
      <name>James Cadden</name>
    </author>
    <author>
      <name>Thomas Unger</name>
    </author>
    <author>
      <name>Yara Awad</name>
    </author>
    <author>
      <name>Han Dong</name>
    </author>
    <author>
      <name>Orran Krieger</name>
    </author>
    <author>
      <name>Jonathan Appavoo</name>
    </author>
    <link href="http://arxiv.org/abs/1910.01558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.01558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.12821v1</id>
    <updated>2020-05-26T15:56:06Z</updated>
    <published>2020-05-26T15:56:06Z</published>
    <title>Study of Firecracker MicroVM</title>
    <summary>  Firecracker is a virtualization technology that makes use of Kernel Virtual
Machine (KVM). Firecracker belongs to a new virtualization class named the
micro-virtual machines (MicroVMs). Using Firecracker, we can launch lightweight
MicroVMs in non-virtualized environments in a fraction of a second, at the same
time offering the security and workload isolation provided by traditional VMs
and also the resource efficiency that comes along with containers \cite{b1}.
Firecracker aims to provide a slimmed-down MicroVM, comprised of approximately
50K lines of code in Rust and with a reduced attack surface for guest VMs. This
report will examine the internals of Firecracker and understand why Firecracker
is the next big thing going forward in virtualization and cloud computing.
</summary>
    <author>
      <name>Madhur Jain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.12821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.12821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.09487v1</id>
    <updated>2021-04-19T17:46:29Z</updated>
    <published>2021-04-19T17:46:29Z</published>
    <title>Android OS CASE STUDY</title>
    <summary>  Android is a mobile operating system based on a modified version of the Linux
kernel and other open source software, designed primarily for touchscreen
mobile devices such as smartphones and tablets. It is an operating system for
low powered devices that run on battery and are full of hardware like Global
Positioning System (GPS) receivers, cameras, light and orientation sensors,
Wi-Fi and LTE (4G telephony) connectivity and a touch screen. Like all
operating systems, Android enables applications to make use of the hardware
features through abstraction and provide a defined environment for
applications. The study includes following topic: Background And History
Android Architecture Kernel And StartUp Process Process Management Deadlock CPU
Scheduling Memory Management Storage Management I/O Battery Optimization
</summary>
    <author>
      <name>Mayank Goel</name>
    </author>
    <author>
      <name>Gourav Singal</name>
    </author>
    <link href="http://arxiv.org/abs/2104.09487v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.09487v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.11016v1</id>
    <updated>2021-09-22T20:09:25Z</updated>
    <published>2021-09-22T20:09:25Z</published>
    <title>Report on the "The Future of the Shell" Panel at HotOS 2021</title>
    <summary>  This document summarizes the challenges and possible research directions
around the shell and its ecosystem, collected during and after the HotOS21
Panel on the future of the shell. The goal is to create a snapshot of what a
number of researchers from various disciplines -- connected to the shell to
varying degrees -- think about its future. We hope that this document will
serve as a reference for future research on the shell and its ecosystem.
</summary>
    <author>
      <name>Michael Greenberg</name>
    </author>
    <author>
      <name>Konstantinos Kallas</name>
    </author>
    <author>
      <name>Nikos Vasilakis</name>
    </author>
    <author>
      <name>Stephen Kell</name>
    </author>
    <link href="http://arxiv.org/abs/2109.11016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.11016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.10106v1</id>
    <updated>2021-12-19T10:26:55Z</updated>
    <published>2021-12-19T10:26:55Z</published>
    <title>New Mechanism for Fast System Calls</title>
    <summary>  System calls have no place on the fast path of microsecond-scale systems.
However, kernel bypass prevents the OS from controlling and supervising access
to the hardware. In this paper we introduce the fastcall space, a new layer in
the traditional OS architecture, that hosts fastcalls. A fastcall implements
the fast path of a traditional kernel operation and can stay on the fast path,
because the transition to the fastcall space is $\approx\times 15$ faster than
to the kernel space. This way the OS does not give up the control over device
access, whereas the applications maintain their performance.
</summary>
    <author>
      <name>Till Miemietz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Barkhausen Institut, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Maksym Planeta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU Dresden, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Viktor Laurin Reusch</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Barkhausen Institut, Germany</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU Dresden, Germany</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2112.10106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.10106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.5080v1</id>
    <updated>2011-05-25T16:15:35Z</updated>
    <published>2011-05-25T16:15:35Z</published>
    <title>Scheduling of Hard Real-Time Multi-Thread Periodic Tasks</title>
    <summary>  In this paper we study the scheduling of parallel and real-time recurrent
tasks. Firstly, we propose a new parallel task model which allows recurrent
tasks to be composed of several threads, each thread requires a single
processor for execution and can be scheduled simultaneously. Secondly, we
define several kinds of real-time schedulers that can be applied to our
parallel task model. We distinguish between two scheduling classes:
hierarchical schedulers and global thread schedulers. We present and prove
correct an exact schedulability test for each class. Lastly, we also evaluate
the performance of our scheduling paradigm in comparison with Gang scheduling
by means of simulations.
</summary>
    <author>
      <name>Irina Iulia Lupu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">U.L.B.</arxiv:affiliation>
    </author>
    <author>
      <name>Joël Goossens</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">U.L.B.</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1105.5080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.5080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.05747v2</id>
    <updated>2020-02-11T08:51:14Z</updated>
    <published>2020-01-16T11:40:57Z</published>
    <title>On Schedulability Analysis of EDF Scheduling by Considering Suspension
  as Blocking</title>
    <summary>  During the execution of a job, it may suspend itself, i.e., its computation
ceases to process until certain activities are complete to be resumed. This
paper provides a counterexample of the schedulability analysis by Devi in
Euromicro Conference on Real-Time Systems (ECRTS) in 2003, which is the only
existing suspension-aware analysis specialized for uniprocessor systems when
preemptive earliest-deadline-first (EDF) is applied for scheduling dynamic
selfsuspending tasks.
</summary>
    <author>
      <name>Mario Günzel</name>
    </author>
    <author>
      <name>Jian-Jia Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2001.05747v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05747v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.07845v1</id>
    <updated>2022-01-19T20:07:24Z</updated>
    <published>2022-01-19T20:07:24Z</published>
    <title>How ISO C became unusable for operating systems development</title>
    <summary>  The C programming language was developed in the 1970s as a fairly
unconventional systems and operating systems development tool, but has, through
the course of the ISO Standards process, added many attributes of more
conventional programming languages and become less suitable for operating
systems development. Operating system programming continues to be done in
non-ISO dialects of C. The differences provide a glimpse of operating system
requirements for programming languages.
</summary>
    <author>
      <name>Victor Yodaiken</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3477113.3487274</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3477113.3487274" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PLOS '21: Proceedings of the 11th Workshop on Programming Languages
  and Operating Systems October 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.07845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.07845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="d.4, d.3" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4; D.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.10929v2</id>
    <updated>2022-05-30T11:36:09Z</updated>
    <published>2022-05-22T20:50:20Z</published>
    <title>rgpdOS: GDPR Enforcement By The Operating System</title>
    <summary>  The General Data Protection Regulation (GDPR) forces IT companies to comply
with a number of principles when dealing with European citizens' personal data.
Non-compliant companies are exposed to penalties which may represent up to 4%
of their turnover. Currently, it is very hard for companies driven by personal
data to make their applications GDPR-compliant, especially if those
applications were developed before the GDPR was established. We present rgpdOS,
a GDPR-aware operating system that aims to bring GDPR-compliance to every
application, while requiring minimal changes to application code.
</summary>
    <author>
      <name>Alain Tchana</name>
    </author>
    <author>
      <name>Raphael Colin</name>
    </author>
    <author>
      <name>Adrien Le Berre</name>
    </author>
    <author>
      <name>Vincent Berger</name>
    </author>
    <author>
      <name>Benoit Combemale</name>
    </author>
    <author>
      <name>Natacha Crooks</name>
    </author>
    <author>
      <name>Ludovic Pailler</name>
    </author>
    <link href="http://arxiv.org/abs/2205.10929v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.10929v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.12751v1</id>
    <updated>2022-06-25T23:56:45Z</updated>
    <published>2022-06-25T23:56:45Z</published>
    <title>Implementation of SquashFS Support in U-Boot</title>
    <summary>  U-Boot is a notorious bootloader and Open Source project. This work had as
objective adding support for the SquashFS filesystem to U-Boot and the support
developed was submitted as a contribution to the project. The bootloader is
responsible, in this context, for loading the kernel and the device tree blob
into RAM. It needs to be capable of reading a storage device's partition
formatted with a specific filesystem type. Adding this support allows U-Boot to
read from SquashFS partitions. The source code was submitted to U-Boot's
mailing list through a series of patches to be reviewed by one of the project's
maintainer. Once it gets merged, the support will be used and modified by
U-Boot's international community.
</summary>
    <author>
      <name>Mariana Villarim</name>
    </author>
    <author>
      <name>João Marcos Costa</name>
    </author>
    <author>
      <name>Diomadson Belfort</name>
    </author>
    <link href="http://arxiv.org/abs/2206.12751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.12751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.00898v1</id>
    <updated>2023-09-02T10:25:34Z</updated>
    <published>2023-09-02T10:25:34Z</published>
    <title>CoRD: Converged RDMA Dataplane for High-Performance Clouds</title>
    <summary>  High-performance networking is often characterized by kernel bypass which is
considered mandatory in high-performance parallel and distributed applications.
But kernel bypass comes at a price because it breaks the traditional OS
architecture, requiring applications to use special APIs and limiting the OS
control over existing network connections. We make the case, that kernel bypass
is not mandatory. Rather, high-performance networking relies on multiple
performance-improving techniques, with kernel bypass being the least effective.
CoRD removes kernel bypass from RDMA networks, enabling efficient OS-level
control over RDMA dataplane.
</summary>
    <author>
      <name>Maksym Planeta</name>
    </author>
    <author>
      <name>Jan Bierbaum</name>
    </author>
    <author>
      <name>Michael Roitzsch</name>
    </author>
    <author>
      <name>Hermann Härtig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.00898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.00898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.00428v3</id>
    <updated>2025-04-30T17:02:59Z</updated>
    <published>2023-09-30T16:40:16Z</published>
    <title>The First Principle of Big Memory Systems</title>
    <summary>  Persistence is the first principle of big memory systems. We comprehensively
analyze the vertical and horizontal extensions of existing memory hierarchy.
Networks are flattening traditional storage hierarchies. We present the
state-of-the-art studies upon the big memory systems, together with design
methodology and implementations. We discuss the full-stack and moving
persistence. In order to achieve cost efficiency and deliver high performance,
we present the speculative and deterministic persistence.
</summary>
    <author>
      <name>Yu Hua</name>
    </author>
    <link href="http://arxiv.org/abs/2310.00428v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.00428v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.02880v1</id>
    <updated>2023-10-04T15:17:30Z</updated>
    <published>2023-10-04T15:17:30Z</published>
    <title>Persistent Memory File Systems: A Survey</title>
    <summary>  Persistent Memory (PM) is non-volatile byte-addressable memory that offers
read and write latencies in the order of magnitude smaller than flash storage,
such as SSDs. This survey discusses how file systems address the most prominent
challenges in the implementation of file systems for Persistent Memory. First,
we discuss how the properties of Persistent Memory change file system design.
Second, we discuss work that aims to optimize small file I/O and the associated
meta-data resolution. Third, we address how existing Persistent Memory file
systems achieve (meta) data persistence and consistency.
</summary>
    <author>
      <name>Wiebe van Breukelen</name>
    </author>
    <author>
      <name>Animesh Trivedi</name>
    </author>
    <link href="http://arxiv.org/abs/2310.02880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.02880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.15153v1</id>
    <updated>2023-12-23T03:44:17Z</updated>
    <published>2023-12-23T03:44:17Z</published>
    <title>Design and Implementation Considerations for a Virtual File System Using
  an Inode Data Structure</title>
    <summary>  Virtual file systems are a tool to centralize and mobilize a file system that
could otherwise be complex and consist of multiple hierarchies, hard disks, and
more. In this paper, we discuss the design of Unix-based file systems and how
this type of file system layout using inode data structures and a disk emulator
can be implemented as a single-file virtual file system in Linux. We explore
the ways that virtual file systems are vulnerable to security attacks and
introduce straightforward solutions that can be implemented to help prevent or
mitigate the consequences of such attacks.
</summary>
    <author>
      <name>Qin Sun</name>
    </author>
    <author>
      <name>Grace McKenzie</name>
    </author>
    <author>
      <name>Guanqun Song</name>
    </author>
    <author>
      <name>Ting Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2312.15153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.15153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.13354v1</id>
    <updated>2024-01-24T10:30:39Z</updated>
    <published>2024-01-24T10:30:39Z</published>
    <title>Characterizing Network Requirements for GPU API Remoting in AI
  Applications</title>
    <summary>  GPU remoting is a promising technique for supporting AI applications.
Networking plays a key role in enabling remoting. However, for efficient
remoting, the network requirements in terms of latency and bandwidth are
unknown. In this paper, we take a GPU-centric approach to derive the minimum
latency and bandwidth requirements for GPU remoting, while ensuring no (or
little) performance degradation for AI applications. Our study including
theoretical model demonstrates that, with careful remoting design, unmodified
AI applications can run on the remoting setup using commodity networking
hardware without any overhead or even with better performance, with low network
demands.
</summary>
    <author>
      <name>Tianxia Wang</name>
    </author>
    <author>
      <name>Zhuofu Chen</name>
    </author>
    <author>
      <name>Xingda Wei</name>
    </author>
    <author>
      <name>Jinyu Gu</name>
    </author>
    <author>
      <name>Rong Chen</name>
    </author>
    <author>
      <name>Haibo Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2401.13354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.13354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.04238v1</id>
    <updated>2024-08-08T05:58:56Z</updated>
    <published>2024-08-08T05:58:56Z</published>
    <title>Crash Consistency in DRAM-NVM-Disk Hybrid Storage System</title>
    <summary>  NVM is used as a new hierarchy in the storage system, due to its intermediate
speed and capacity between DRAM, and its byte granularity. However, consistency
problems emerge when we attempt to put DRAM, NVM, and disk together as an
efficient whole. In this paper, we discuss the challenging consistency problems
faced by heterogeneous storage systems, and propose our solution to the
problems. The discussion is based on NVPC as a case study, but can be inspiring
and adaptive to all similar heterogeneous storage systems.
</summary>
    <author>
      <name>Guoyu Wang</name>
    </author>
    <author>
      <name>Xilong Che</name>
    </author>
    <author>
      <name>Haoyang Wei</name>
    </author>
    <author>
      <name>Chenju Pei</name>
    </author>
    <author>
      <name>Juncheng Hu</name>
    </author>
    <link href="http://arxiv.org/abs/2408.04238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.04238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.15704v1</id>
    <updated>2024-09-24T03:41:29Z</updated>
    <published>2024-09-24T03:41:29Z</published>
    <title>Assessing FIFO and Round Robin Scheduling:Effects on Data Pipeline
  Performance and Energy Usage</title>
    <summary>  In the case of compute-intensive machine learning, efficient operating system
scheduling is crucial for performance and energy efficiency. This paper
conducts a comparative study over FIFO(First-In-First-Out) and RR(Round-Robin)
scheduling policies with the application of real-time machine learning training
processes and data pipelines on Ubuntu-based systems. Knowing a few patterns of
CPU usage and energy consumption, we identify which policy (the exclusive or
the shared) provides higher performance and/or lower energy consumption for
typical modern workloads. Results of this study would help in providing better
operating system schedulers for modern systems like Ubuntu, working to improve
performance and reducing energy consumption in compute intensive workloads.
</summary>
    <author>
      <name>Malobika Roy Choudhury</name>
    </author>
    <author>
      <name>Akshat Mehrotra</name>
    </author>
    <link href="http://arxiv.org/abs/2409.15704v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.15704v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.18342v1</id>
    <updated>2024-09-26T23:35:23Z</updated>
    <published>2024-09-26T23:35:23Z</published>
    <title>Exploring Time-Space trade-offs for synchronized in Lilliput</title>
    <summary>  In the context of Project Lilliput, which attempts to reduce the size of
object header in the HotSpot Java Virtual Machine (JVM), we explore a curated
set of synchronization algorithms. Each of the algorithms could serve as a
potential replacement implementation for the "synchronized" construct in
HotSpot. Collectively, the algorithms illuminate trade-offs in space-time
properties. The key design decisions are where to locate synchronization
metadata (monitor fields), how to map from an object to those fields, and the
lifecycle of the monitor information. The reader is assumed to be familiar with
current HotSpot implementation of "synchronized" as well as the Compact Java
Monitors (CJM) design and Project Lilliput.
</summary>
    <author>
      <name>Dave Dice</name>
    </author>
    <author>
      <name>Alex Kogan</name>
    </author>
    <link href="http://arxiv.org/abs/2409.18342v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.18342v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.20221v1</id>
    <updated>2024-12-28T17:17:03Z</updated>
    <published>2024-12-28T17:17:03Z</published>
    <title>Revisiting Cache Freshness for Emerging Real-Time Applications</title>
    <summary>  Caching is widely used in industry to improve application performance by
reducing data-access latency and taking the load off the backend
infrastructure. TTLs have become the de-facto mechanism used to keep cached
data reasonably fresh (i.e., not too out of date with the backend). However,
the emergence of real-time applications requires tighter data freshness, which
is impractical to achieve with TTLs. We discuss why this is the case, and
propose a simple yet effective adaptive policy to achieve the desired
freshness.
</summary>
    <author>
      <name>Ziming Mao</name>
    </author>
    <author>
      <name>Rishabh Iyer</name>
    </author>
    <author>
      <name>Scott Shenker</name>
    </author>
    <author>
      <name>Ion Stoica</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3696348.3696858</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3696348.3696858" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">HotNets '24</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.20221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.20221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.00248v1</id>
    <updated>2024-12-31T03:35:03Z</updated>
    <published>2024-12-31T03:35:03Z</published>
    <title>Combining Type Checking and Formal Verification for Lightweight OS
  Correctness</title>
    <summary>  This paper reports our experience of providing lightweight correctness
guarantees to an open-source Rust OS, Theseus. First, we report new
developments in intralingual design that leverage Rust's type system to enforce
additional invariants at compile time, trusting the Rust compiler. Second, we
develop a hybrid approach that combines formal verification, type checking, and
informal reasoning, showing how the type system can assist in increasing the
scope of formally verified invariants. By slightly lessening the strength of
correctness guarantees, this hybrid approach substantially reduces the proof
effort. We share our experience in applying this approach to the memory
subsystem and the 10 Gb Ethernet driver of Theseus, demonstrate its utility,
and quantify its reduced proof effort.
</summary>
    <author>
      <name>Ramla Ijaz</name>
    </author>
    <author>
      <name>Kevin Boos</name>
    </author>
    <author>
      <name>Lin Zhong</name>
    </author>
    <link href="http://arxiv.org/abs/2501.00248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.00248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.06234v1</id>
    <updated>2025-01-08T05:01:24Z</updated>
    <published>2025-01-08T05:01:24Z</published>
    <title>Fast, Secure, Adaptable: LionsOS Design, Implementation and Performance</title>
    <summary>  We present LionsOS, an operating system for security- and safety-critical
embedded systems. LionsOS is based on the formally verified seL4 microkernel
and designed with verification in mind. It uses a static architecture and
features a highly modular design driven by strict separation of concerns and a
focus on simplicity. We demonstrate that LionsOS outperforms Linux.
</summary>
    <author>
      <name>Gernot Heiser</name>
    </author>
    <author>
      <name>Ivan Velickovic</name>
    </author>
    <author>
      <name>Peter Chubb</name>
    </author>
    <author>
      <name>Alwin Joshy</name>
    </author>
    <author>
      <name>Anuraag Ganesh</name>
    </author>
    <author>
      <name>Bill Nguyen</name>
    </author>
    <author>
      <name>Cheng Li</name>
    </author>
    <author>
      <name>Courtney Darville</name>
    </author>
    <author>
      <name>Guangtao Zhu</name>
    </author>
    <author>
      <name>James Archer</name>
    </author>
    <author>
      <name>Jingyao Zhou</name>
    </author>
    <author>
      <name>Krishnan Winter</name>
    </author>
    <author>
      <name>Lucy Parker</name>
    </author>
    <author>
      <name>Szymon Duchniewicz</name>
    </author>
    <author>
      <name>Tianyi Bai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.06234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.06234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.7; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.10217v1</id>
    <updated>2025-05-15T12:21:16Z</updated>
    <published>2025-05-15T12:21:16Z</published>
    <title>Enabling Syscall Intercept for RISC-V</title>
    <summary>  The European Union technological sovereignty strategy centers around the
RISC-V Instruction Set Architecture, with the European Processor Initiative
leading efforts to build production-ready processors. Focusing on realizing a
functional RISC-V ecosystem, the BZL initiative (www.bzl.es) is making an
effort to create a software stack along with the hardware. In this work, we
detail the efforts made in porting a widely used syscall interception library,
mainly used on AdHocFS (i.e., DAOS, GekkoFS), to RISC-V and how we overcame
some of the limitations encountered.
</summary>
    <author>
      <name>Petar Andrić</name>
    </author>
    <author>
      <name>Aaron Call</name>
    </author>
    <author>
      <name>Ramon Nou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RISC-V summit 2025 accepted</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.10217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.10217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.1316v1</id>
    <updated>2008-10-07T23:03:29Z</updated>
    <published>2008-10-07T23:03:29Z</published>
    <title>The meaning of concurrent programs</title>
    <summary>  The semantics of assignment and mutual exclusion in concurrent and
multi-core/multi-processor systems is presented with attention to low level
architectural features in an attempt to make the presentation realistic.
Recursive functions on event sequences are used to define state dependent
functions and variables in ordinary (non-formal-method) algebra.
</summary>
    <author>
      <name>Victor Yodaiken</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report on using recursive functions for the low level
  semantics of concurrent systems</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.1316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.1316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.2281v1</id>
    <updated>2014-05-08T07:09:55Z</updated>
    <published>2014-05-08T07:09:55Z</published>
    <title>Proceedings of the First Workshop on Resource Awareness and Adaptivity
  in Multi-Core Computing (Racing 2014)</title>
    <summary>  This volume contains the papers accepted at the 1st Workshop on Resource
Awareness and Adaptivity in Multi-Core Computing (Racing 2014), held in
Paderborn, Germany, May 29-30, 2014. Racing 2014 was co-located with the IEEE
European Test Symposium (ETS).
</summary>
    <author>
      <name>Frank Hannig</name>
    </author>
    <author>
      <name>Jürgen Teich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Website of the workshop: http://www12.cs.fau.de/racing2014/</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.2281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.2281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.3226v1</id>
    <updated>2014-10-13T08:51:40Z</updated>
    <published>2014-10-13T08:51:40Z</published>
    <title>Proceedings 2014 International Workshop on Advanced Intrusion Detection
  and Prevention</title>
    <summary>  This volume contains the proceedings of the 2014 International Advanced
Intrusion Detection and Prevention (AIDP'14) Workshop, held in Marrakesh,
Morocco, on the 5th of June 2014, in conjunction with the 29th IFIP TC-11 SEC
2014 International Conference. It includes a revised version of the papers
selected for presentation at the work- shop.
</summary>
    <author>
      <name>Joaquin Garcia-Alfaro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institut Mines-Télécom, Télécom SudParis</arxiv:affiliation>
    </author>
    <author>
      <name>Gürkan Gür</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Provus</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.165</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.165" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 165, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1410.3226v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.3226v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Security" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07353v1</id>
    <updated>2018-07-17T16:34:43Z</updated>
    <published>2018-07-17T16:34:43Z</published>
    <title>User Manual for the Apple CoreCapture Framework</title>
    <summary>  CoreCapture is Apple's primary logging and tracing framework for IEEE 802.11
on iOS and macOS. It allows users and developers to create comprehensive debug
output for analysis by Apple. In this manual, we provide an overview into the
concepts, show in detail how CoreCapture logs can be created on iOS and macOS,
and introduce the first CoreCapture dissector for Wireshark.
</summary>
    <author>
      <name>David Kreitschmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809006v1</id>
    <updated>1998-09-02T17:11:54Z</updated>
    <published>1998-09-02T17:11:54Z</published>
    <title>The Design and Architecture of the Microsoft Cluster Service -- A
  Practical Approach to High-Availability and Scalability</title>
    <summary>  Microsoft Cluster Service (MSCS) extends the Win-dows NT operating system to
support high-availability services. The goal is to offer an execution
environment where off-the-shelf server applications can continue to operate,
even in the presence of node failures. Later ver-sions of MSCS will provide
scalability via a node and application management system that allows
applications to scale to hundreds of nodes. This paper provides a de-tailed
description of the MSCS architecture and the de-sign decisions that have driven
the implementation of the service. The paper also describes how some major
appli-cations use the MSCS features, and describes features added to make it
easier to implement and manage fault-tolerant applications on MSCS.
</summary>
    <author>
      <name>Werner Vogels</name>
    </author>
    <author>
      <name>Dan Dumitriu</name>
    </author>
    <author>
      <name>Ken Birman</name>
    </author>
    <author>
      <name>Rod Gamache</name>
    </author>
    <author>
      <name>Mike Massa</name>
    </author>
    <author>
      <name>Rob Short</name>
    </author>
    <author>
      <name>John Vert</name>
    </author>
    <author>
      <name>Joe Barrera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Original document at:
  http://research.microsoft.com/~gray/MSCS_FTCS98.doc</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of FTCS'98, June 23-25, 1998 in Munich, Germany</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9809006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; C.5;D.4.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9812011v1</id>
    <updated>1998-12-10T16:49:04Z</updated>
    <published>1998-12-10T16:49:04Z</published>
    <title>A nested transaction mechanism for LOCUS</title>
    <summary>  A working implementation of nested transactions has been produced for LOCUS,
an integrated distributed operating system which provides a high degree of
network transparency. Several aspects of our mechanism are novel. First, the
mechanism allows a transaction to access objects directly without regard to the
location of the object. Second, processes running on behalf of a single
transaction may be located at many sites. Thus there is no need to invoke a new
transaction to perform processing or access objects at a remote site. Third,
unlike other environments, LOCUS allows replication of data objects at more
than one site in the network, and this capability is incorporated into the
transaction mechanism. If the copy of an object that is currently being
accessed becomes unavailable, it is possible to continue work by using another
one of the replicated copies. Finally, an efficient orphan removal algorithm is
presented, and the problem of providing continued operation during network
partitions is addressed in detail.
</summary>
    <author>
      <name>Erik T. Mueller</name>
    </author>
    <author>
      <name>Johanna D. Moore</name>
    </author>
    <author>
      <name>Gerald J. Popek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages. Appears in: Proceedings of the Ninth ACM Symposium on
  Operating Systems Principles (pp. 71-87). Operating Systems Review. Vol. 17,
  No. 5. New York: Association for Computing Machinery. 1983</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9812011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9812011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0003064v1</id>
    <updated>2000-03-15T19:20:53Z</updated>
    <published>2000-03-15T19:20:53Z</published>
    <title>A network file system over HTTP: remote access and modification of files
  and "files"</title>
    <summary>  The goal of the present HTTPFS project is to enable access to remote files,
directories, and other containers through an HTTP pipe. HTTPFS system permits
retrieval, creation and modification of these resources as if they were regular
files and directories on a local filesystem. The remote host can be any UNIX or
Win9x/WinNT box that is capable of running a Perl CGI script and accessible
either directly or via a web proxy or a gateway. HTTPFS runs entirely in user
space.
  The current implementation fully supports reading as well as creating,
writing, appending, and truncating of files on a remote HTTP host. HTTPFS
provides an isolation level for concurrent file access stronger than the one
mandated by POSIX file system semantics, closer to that of AFS. Both an API
with familiar open(), read(), write(), close(), etc. calls, and an interactive
interface, via the popular Midnight Commander file browser, are provided.
</summary>
    <author>
      <name>Oleg Kiselyov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This present document combines a paper and a Freenix Track talk
  presented at a 1999 USENIX Annual Technical Conference, June 6-11, 1999;
  Monterey, CA, USA; 6 HTML files. The paper alone appeared in Proc. FREENIX
  Track: 1999 USENIX Annual Technical Conference, June 6-11,1999; Monterey, CA,
  USA, pp. 75-80</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0003064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0003064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.3; D.4.4; E.5; C.2.2; C.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0403007v1</id>
    <updated>2004-03-06T02:52:17Z</updated>
    <published>2004-03-06T02:52:17Z</published>
    <title>End-User Effects of Microreboots in Three-Tiered Internet Systems</title>
    <summary>  Microreboots restart fine-grained components of software systems "with a
clean slate," and only take a fraction of the time needed for full system
reboot. Microreboots provide an application-generic recovery technique for
Internet services, which can be supported entirely in middleware and requires
no changes to the applications or any a priori knowledge of application
semantics.
  This paper investigates the effect of microreboots on end-users of an
eBay-like online auction application; we find that microreboots are nearly as
effective as full reboots, but are significantly less disruptive in terms of
downtime and lost work. In our experiments, microreboots reduced the number of
failed user requests by 65% and the perceived downtime by 78% compared to a
server process restart. We also show how to replace user-visible transient
failures with transparent call-retry, at the cost of a slight increase in
end-user-visible latency during recovery. Due to their low cost, microreboots
can be used aggressively, even when their necessity is less than certain, hence
adding to the reduced recovery time a reduction in the fault detection time,
which further improves availability.
</summary>
    <author>
      <name>George Candea</name>
    </author>
    <author>
      <name>Armando Fox</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0403007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0403007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0409034v1</id>
    <updated>2004-09-17T21:51:24Z</updated>
    <published>2004-09-17T21:51:24Z</published>
    <title>Securing Data in Storage: A Review of Current Research</title>
    <summary>  Protecting data from malicious computer users continues to grow in
importance. Whether preventing unauthorized access to personal photographs,
ensuring compliance with federal regulations, or ensuring the integrity of
corporate secrets, all applications require increased security to protect data
from talented intruders. Specifically, as more and more files are preserved on
disk the requirement to provide secure storage has increased in importance.
This paper presents a survey of techniques for securely storing data, including
theoretical approaches, prototype systems, and existing systems currently
available. Due to the wide variety of potential solutions available and the
variety of techniques to arrive at a particular solution, it is important to
review the entire field prior to selecting an implementation that satisfies
particular requirements. This paper provides an overview of the prominent
characteristics of several systems to provide a foundation for making an
informed decision. Initially, the paper establishes a set of criteria for
evaluating a storage solution based on confidentiality, integrity,
availability, and performance. Then, using these criteria, the paper explains
the relevant characteristics of select storage systems and provides a
comparison of the major differences.
</summary>
    <author>
      <name>Paul Stanton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 4 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0409034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0409034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.3;D.4.6;K.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0511010v1</id>
    <updated>2005-11-02T19:52:58Z</updated>
    <published>2005-11-02T19:52:58Z</published>
    <title>A Survey of Virtualization Techniques Focusing on Secure On-Demand
  Cluster Computing</title>
    <summary>  Virtualization, a technique once used to multiplex the resources of
high-priced mainframe hardware, is seeing a resurgence in applicability with
the increasing computing power of commodity computers. By inserting a layer of
software between the machine and traditional operating systems, this technology
allows access to a shared computing medium in a manner that is secure,
resource-controlled, and efficient. These properties are attractive in the
field of on-demand computing, where the fine-grained subdivision of resources
provided by virtualized systems allows potentially higher utilization of
computing resources.
  It this work, we survey a number of virtual machine systems with the goal of
finding an appropriate candidate to serve as the basis for the On-Demand Secure
Cluster Computing project at the National Center for Supercomputing
Applications. Contenders are reviewed on a number of desirable properties
including portability and security. We conclude with a comparison and
justification of our choice.
</summary>
    <author>
      <name>Nadir Kiyanclar</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0511010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0511010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701021v2</id>
    <updated>2013-07-05T14:09:11Z</updated>
    <published>2007-01-04T09:45:28Z</published>
    <title>The Unix KISS: A Case Study</title>
    <summary>  In this paper we show that the initial philosophy used in designing and
developing UNIX in early times has been forgotten due to "fast practices". We
question the leitmotif that microkernels, though being by design adherent to
the KISS principle, have a number of context switches higher than their
monolithic counterparts, running a test suite and verify the results with
standard statistical validation tests. We advocate a wiser distribution of
shared libraries by statistically analyzing the weight of each shared object in
a typical UNIX system, showing that the majority of shared libraries exist in a
common space for no real evidence of need. Finally we examine the UNIX heritage
with an historical point of view, noticing how habits swiftly replaced the
intents of the original authors, moving the focus from the earliest purpose of
is avoiding complications, keeping a system simple to use and maintain.
</summary>
    <author>
      <name>Franco Milicchio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Removed from arXiv and other sources</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0701021v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701021v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.4292v1</id>
    <updated>2008-01-28T14:30:34Z</updated>
    <published>2008-01-28T14:30:34Z</published>
    <title>Exact Feasibility Tests for Real-Time Scheduling of Periodic Tasks upon
  Multiprocessor Platforms</title>
    <summary>  In this paper we study the global scheduling of periodic task systems upon
multiprocessor platforms. We first show two very general properties which are
well-known for uniprocessor platforms and which remain for multiprocessor
platforms: (i) under few and not so restrictive assumptions, we show that
feasible schedules of periodic task systems are periodic from some point with a
period equal to the least common multiple of task periods and (ii) for the
specific case of synchronous periodic task systems, we show that feasible
schedules repeat from the origin. We then present our main result: we
characterize, for task-level fixed-priority schedulers and for asynchronous
constrained or arbitrary deadline periodic task models, upper bounds of the
first time instant where the schedule repeats. We show that job-level
fixed-priority schedulers are predictable upon unrelated multiprocessor
platforms. For task-level fixed-priority schedulers, based on the upper bounds
and the predictability property, we provide for asynchronous constrained or
arbitrary deadline periodic task sets, exact feasibility tests. Finally, for
the job-level fixed-priority EDF scheduler, for which such an upper bound
remains unknown, we provide an exact feasibility test as well.
</summary>
    <author>
      <name>Liliana Cucu</name>
    </author>
    <author>
      <name>Joël Goossens</name>
    </author>
    <link href="http://arxiv.org/abs/0801.4292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.4292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.4680v3</id>
    <updated>2008-06-10T07:13:01Z</updated>
    <published>2008-05-30T07:01:51Z</published>
    <title>Telex: Principled System Support for Write-Sharing in Collaborative
  Applications</title>
    <summary>  The Telex system is designed for sharing mutable data in a distributed
environment, particularly for collaborative applications. Users operate on
their local, persistent replica of shared documents; they can work disconnected
and suffer no network latency. The Telex approach to detect and correct
conflicts is application independent, based on an action-constraint graph (ACG)
that summarises the concurrency semantics of applications. The ACG is stored
efficiently in a multilog structure that eliminates contention and is optimised
for locality. Telex supports multiple applications and multi-document updates.
The Telex system clearly separates system logic (which includes replication,
views, undo, security, consistency, conflicts, and commitment) from application
logic. An example application is a shared calendar for managing multi-user
meetings; the system detects meeting conflicts and resolves them consistently.
</summary>
    <author>
      <name>Lamia Benmouffok</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt, LIP6</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Michel Busca</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt, LIP6</arxiv:affiliation>
    </author>
    <author>
      <name>Joan Manuel Marquès</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIP6, UOC</arxiv:affiliation>
    </author>
    <author>
      <name>Marc Shapiro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt, LIP6</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Sutra</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt, LIP6</arxiv:affiliation>
    </author>
    <author>
      <name>Georgios Tsoukalas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NTUA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0805.4680v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.4680v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.0132v1</id>
    <updated>2008-06-01T08:30:10Z</updated>
    <published>2008-06-01T08:30:10Z</published>
    <title>Control-theoretic dynamic voltage scaling for embedded controllers</title>
    <summary>  For microprocessors used in real-time embedded systems, minimizing power
consumption is difficult due to the timing constraints. Dynamic voltage scaling
(DVS) has been incorporated into modern microprocessors as a promising
technique for exploring the trade-off between energy consumption and system
performance. However, it remains a challenge to realize the potential of DVS in
unpredictable environments where the system workload cannot be accurately
known. Addressing system-level power-aware design for DVS-enabled embedded
controllers, this paper establishes an analytical model for the DVS system that
encompasses multiple real-time control tasks. From this model, a feedback
control based approach to power management is developed to reduce dynamic power
consumption while achieving good application performance. With this approach,
the unpredictability and variability of task execution times can be attacked.
Thanks to the use of feedback control theory, predictable performance of the
DVS system is achieved, which is favorable to real-time applications. Extensive
simulations are conducted to evaluate the performance of the proposed approach.
</summary>
    <author>
      <name>Feng Xia</name>
    </author>
    <author>
      <name>Yu-Chu Tian</name>
    </author>
    <author>
      <name>Youxian Sun</name>
    </author>
    <author>
      <name>Jinxiang Dong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in IET Computers and Digital Techniques.
  doi:10.1049/iet-cdt:20070112</arxiv:comment>
    <link href="http://arxiv.org/abs/0806.0132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.0132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.1; C.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.1381v1</id>
    <updated>2008-06-09T07:23:28Z</updated>
    <published>2008-06-09T07:23:28Z</published>
    <title>Feedback Scheduling: An Event-Driven Paradigm</title>
    <summary>  Embedded computing systems today increasingly feature resource constraints
and workload variability, which lead to uncertainty in resource availability.
This raises great challenges to software design and programming in multitasking
environments. In this paper, the emerging methodology of feedback scheduling is
introduced to address these challenges. As a closed-loop approach to resource
management, feedback scheduling promises to enhance the flexibility and
resource efficiency of various software programs through dynamically
distributing available resources among concurrent tasks based on feedback
information about the actual usage of the resources. With emphasis on the
behavioral design of feedback schedulers, we describe a general framework of
feedback scheduling in the context of real-time control applications. A simple
yet illustrative feedback scheduling algorithm is given. From a programming
perspective, we describe how to modify the implementation of control tasks to
facilitate the application of feedback scheduling. An event-driven paradigm
that combines time-triggered and event-triggered approaches is proposed for
programming of the feedback scheduler. Simulation results argue that the
proposed event-driven paradigm yields better performance than time-triggered
paradigm in dynamic environments where the workload varies irregularly and
unpredictably.
</summary>
    <author>
      <name>Feng Xia</name>
    </author>
    <author>
      <name>Guosong Tian</name>
    </author>
    <author>
      <name>Youxian Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SIGPLAN Notices, vol.42, no.12, pp. 7-14, Dec. 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0806.1381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.1381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.1; C.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.1768v1</id>
    <updated>2008-06-11T00:03:00Z</updated>
    <published>2008-06-11T00:03:00Z</published>
    <title>Local Read-Write Operations in Sensor Networks</title>
    <summary>  Designing protocols and formulating convenient programming units of
abstraction for sensor networks is challenging due to communication errors and
platform constraints. This paper investigates properties and implementation
reliability for a \emph{local read-write} abstraction. Local read-write is
inspired by the class of read-modify-write operations defined for shared-memory
multiprocessor architectures. The class of read-modify-write operations is
important in solving consensus and related synchronization problems for
concurrency control. Local read-write is shown to be an atomic abstraction for
synchronizing neighborhood states in sensor networks. The paper compares local
read-write to similar lightweight operations in wireless sensor networks, such
as read-all, write-all, and a transaction-based abstraction: for some
optimistic scenarios, local read-write is a more efficient neighborhood
operation. A partial implementation is described, which shows that three
outcomes characterize operation response: success, failure, and cancel. A
failure response indicates possible inconsistency for the operation result,
which is the result of a timeout event at the operation's initiator. The paper
presents experimental results on operation performance with different timeout
values and situations of no contention, with some tests also on various
neighborhood sizes.
</summary>
    <author>
      <name>Ted Herman</name>
    </author>
    <author>
      <name>Morten Mjelde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 16 figures (using pstricks)</arxiv:comment>
    <link href="http://arxiv.org/abs/0806.1768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.1768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4; D.1.3; C.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.0920v1</id>
    <updated>2008-08-06T20:33:56Z</updated>
    <published>2008-08-06T20:33:56Z</published>
    <title>A Distributed and Deterministic TDMA Algorithm for
  Write-All-With-Collision Model</title>
    <summary>  Several self-stabilizing time division multiple access (TDMA) algorithms are
proposed for sensor networks. In addition to providing a collision-free
communication service, such algorithms enable the transformation of programs
written in abstract models considered in distributed computing literature into
a model consistent with sensor networks, i.e., write all with collision (WAC)
model. Existing TDMA slot assignment algorithms have one or more of the
following properties: (i) compute slots using a randomized algorithm, (ii)
assume that the topology is known upfront, and/or (iii) assign slots
sequentially. If these algorithms are used to transform abstract programs into
programs in WAC model then the transformed programs are probabilistically
correct, do not allow the addition of new nodes, and/or converge in a
sequential fashion. In this paper, we propose a self-stabilizing deterministic
TDMA algorithm where a sensor is aware of only its neighbors. We show that the
slots are assigned to the sensors in a concurrent fashion and starting from
arbitrary initial states, the algorithm converges to states where
collision-free communication among the sensors is restored. Moreover, this
algorithm facilitates the transformation of abstract programs into programs in
WAC model that are deterministically correct.
</summary>
    <author>
      <name>Mahesh Arumugam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.0920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.0920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.1132v1</id>
    <updated>2008-09-06T04:38:34Z</updated>
    <published>2008-09-06T04:38:34Z</published>
    <title>Managing Varying Worst Case Execution Times on DVS Platforms</title>
    <summary>  Energy efficient real-time task scheduling attracted a lot of attention in
the past decade. Most of the time, deterministic execution lengths for tasks
were considered, but this model fits less and less with the reality, especially
with the increasing number of multimedia applications. It's why a lot of
research is starting to consider stochastic models, where execution times are
only known stochastically. However, authors consider that they have a pretty
much precise knowledge about the properties of the system, especially regarding
to the worst case execution time (or worst case execution cycles, WCEC).
  In this work, we try to relax this hypothesis, and assume that the WCEC can
vary. We propose miscellaneous methods to react to such a situation, and give
many simulation results attesting that with a small effort, we can provide very
good results, allowing to keep a low deadline miss rate as well as an energy
consumption similar to clairvoyant algorithms.
</summary>
    <author>
      <name>Vandy Berten</name>
    </author>
    <author>
      <name>Chi-Ju Chang</name>
    </author>
    <author>
      <name>Tei-Wei Kuo</name>
    </author>
    <link href="http://arxiv.org/abs/0809.1132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.1132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.5046v1</id>
    <updated>2009-10-27T17:31:02Z</updated>
    <published>2009-10-27T17:31:02Z</published>
    <title>Temporal Debugging using URDB</title>
    <summary>  A new style of temporal debugging is proposed. The new URDB debugger can
employ such techniques as temporal search for finding an underlying fault that
is causing a bug. This improves on the standard iterative debugging style,
which iteratively re-executes a program under debugger control in the search
for the underlying fault. URDB acts as a meta-debugger, with current support
for four widely used debuggers: gdb, MATLAB, python, and perl. Support for a
new debugger can be added in a few hours. Among its points of novelty are: (i)
the first reversible debuggers for MATLAB, python, and perl; (ii) support for
today's multi-core architectures; (iii) reversible debugging of multi-process
and distributed computations; and (iv) temporal search on changes in program
expressions. URDB gains its reversibility and temporal abilities through the
fast checkpoint-restart capability of DMTCP (Distributed MultiThreaded
CheckPointing). The recently enhanced DMTCP also adds ptrace support, enabling
one to freeze, migrate, and replicate debugging sessions.
</summary>
    <author>
      <name>Ana Maria Visan</name>
    </author>
    <author>
      <name>Artem Polyakov</name>
    </author>
    <author>
      <name>Praveen S. Solanki</name>
    </author>
    <author>
      <name>Kapil Arya</name>
    </author>
    <author>
      <name>Tyler Denniston</name>
    </author>
    <author>
      <name>Gene Cooperman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 3 figures, 5 tables; software at urdb.sourceforge.net</arxiv:comment>
    <link href="http://arxiv.org/abs/0910.5046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.5046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.5; D.4.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.0606v1</id>
    <updated>2009-12-03T09:06:59Z</updated>
    <published>2009-12-03T09:06:59Z</published>
    <title>A New Scheduling Algorithms For Real Time Tasks</title>
    <summary>  The main objective of this paper is to develop the two different ways in
which round robin architecture is modified and made suitable to be implemented
in real time and embedded systems. The scheduling algorithm plays a significant
role in the design of real time embedded systems. Simple round robin
architecture is not efficient to be implemented in embedded systems because of
higher context switch rate, larger waiting time and larger response time.
Missing of deadlines will degrade the system performance in soft real time
systems. The main objective of this paper is to develop the scheduling
algorithm which removes the drawbacks in simple round robin architecture. A
comparison with round robin architecture to the proposed architectures has been
made. It is observed that the proposed architectures solves the problems
encountered in round robin architecture in soft real time by decreasing the
number of context switches waiting time and response time thereby increasing
the system throughput.
</summary>
    <author>
      <name>C. Yaashuwanth</name>
    </author>
    <author>
      <name>Dr. R. Ramesh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS November 2009, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 6, No. 2, pp. 061-066, November 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.0606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.0606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.0926v2</id>
    <updated>2010-02-01T16:27:48Z</updated>
    <published>2009-12-04T20:10:12Z</published>
    <title>Deterministic Consistency: A Programming Model for Shared Memory
  Parallelism</title>
    <summary>  The difficulty of developing reliable parallel software is generating
interest in deterministic environments, where a given program and input can
yield only one possible result. Languages or type systems can enforce
determinism in new code, and runtime systems can impose synthetic schedules on
legacy parallel code. To parallelize existing serial code, however, we would
like a programming model that is naturally deterministic without language
restrictions or artificial scheduling. We propose "deterministic consistency",
a parallel programming model as easy to understand as the "parallel assignment"
construct in sequential languages such as Perl and JavaScript, where concurrent
threads always read their inputs before writing shared outputs. DC supports
common data- and task-parallel synchronization abstractions such as fork/join
and barriers, as well as non-hierarchical structures such as producer/consumer
pipelines and futures. A preliminary prototype suggests that software-only
implementations of DC can run applications written for popular parallel
environments such as OpenMP with low (&lt;10%) overhead for some applications.
</summary>
    <author>
      <name>Amittai Aviram</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yale University</arxiv:affiliation>
    </author>
    <author>
      <name>Bryan Ford</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yale University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0912.0926v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.0926v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.4; D.1.3; D.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.4115v3</id>
    <updated>2011-01-24T12:39:16Z</updated>
    <published>2010-01-25T08:09:27Z</published>
    <title>On the Design of an Optimal Multiprocessor Real-Time Scheduling
  Algorithm under Practical Considerations (Extended Version)</title>
    <summary>  This research addresses the multiprocessor scheduling problem of hard
real-time systems, and it especially focuses on optimal and global schedulers
when practical constraints are taken into account. First, we propose an
improvement of the optimal algorithm BF. We formally prove that our adaptation
is (i) optimal, i.e., it always generates a feasible schedule as long as such a
schedule exists, and (ii) valid, i.e., it complies with the all the
requirements. We also show that it outperforms BF by providing a computing
complexity of O(n), where n is the number of tasks to be scheduled. Next, we
propose a schedulability analysis which indicates a priori whether the
real-time application can be scheduled by our improvement of BF without missing
any deadline. This analysis is, to the best of our knowledge, the first such
test for multiprocessors that takes into account all the main overheads
generated by the Operating System.
</summary>
    <author>
      <name>Shelby Funk</name>
    </author>
    <author>
      <name>Vincent Nelis</name>
    </author>
    <author>
      <name>Joel Goossens</name>
    </author>
    <author>
      <name>Dragomir Milojevic</name>
    </author>
    <author>
      <name>Geoffrey Nelissen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.4115v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.4115v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3715v1</id>
    <updated>2010-04-21T14:24:13Z</updated>
    <published>2010-04-21T14:24:13Z</published>
    <title>Multi-Criteria Evaluation of Partitioning Schemes for Real-Time Systems</title>
    <summary>  In this paper we study the partitioning approach for multiprocessor real-time
scheduling. This approach seems to be the easiest since, once the partitioning
of the task set has been done, the problem reduces to well understood
uniprocessor issues. Meanwhile, there is no optimal and polynomial solution to
partition tasks on processors. In this paper we analyze partitioning algorithms
from several points of view such that for a given task set and specific
constraints (processor number, task set type, etc.) we should be able to
identify the best heuristic and the best schedulability test. We also analyze
the influence of the heuristics on the performance of the uniprocessor tests
and the impact of a specific task order on the schedulability. A study on
performance difference between Fixed Priority schedulers and EDF in the case of
partitioning scheduling is also considered.
</summary>
    <author>
      <name>Irina Lupu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">U.L.B</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Courbin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ECE</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent George</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ECE</arxiv:affiliation>
    </author>
    <author>
      <name>Joël Goossens</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">U.L.B</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Emerging Technologies and Factory
  Automation, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.3715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.3450v1</id>
    <updated>2010-05-19T14:17:56Z</updated>
    <published>2010-05-19T14:17:56Z</published>
    <title>Efficient System-Enforced Deterministic Parallelism</title>
    <summary>  Deterministic execution offers many benefits for debugging, fault tolerance,
and security. Running parallel programs deterministically is usually difficult
and costly, however - especially if we desire system-enforced determinism,
ensuring precise repeatability of arbitrarily buggy or malicious software.
Determinator is a novel operating system that enforces determinism on both
multithreaded and multi-process computations. Determinator's kernel provides
only single-threaded, "shared-nothing" address spaces interacting via
deterministic synchronization. An untrusted user-level runtime uses distributed
computing techniques to emulate familiar abstractions such as Unix processes,
file systems, and shared memory multithreading. The system runs parallel
applications deterministically both on multicore PCs and across nodes in a
cluster. Coarse-grained parallel benchmarks perform and scale comparably to -
sometimes better than - conventional systems, though determinism is costly for
fine-grained parallel applications.
</summary>
    <author>
      <name>Amittai Aviram</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yale University</arxiv:affiliation>
    </author>
    <author>
      <name>Shu-Chun Weng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yale University</arxiv:affiliation>
    </author>
    <author>
      <name>Sen Hu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yale University</arxiv:affiliation>
    </author>
    <author>
      <name>Bryan Ford</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yale University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 12 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.3450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.3450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.2637v1</id>
    <updated>2010-06-14T08:51:05Z</updated>
    <published>2010-06-14T08:51:05Z</published>
    <title>Semi-Partitioned Hard Real-Time Scheduling with Restricted Migrations
  upon Identical Multiprocessor Platforms</title>
    <summary>  Algorithms based on semi-partitioned scheduling have been proposed as a
viable alternative between the two extreme ones based on global and partitioned
scheduling. In particular, allowing migration to occur only for few tasks which
cannot be assigned to any individual processor, while most tasks are assigned
to specific processors, considerably reduces the runtime overhead compared to
global scheduling on the one hand, and improve both the schedulability and the
system utilization factor compared to partitioned scheduling on the other hand.
In this paper, we address the preemptive scheduling problem of hard real-time
systems composed of sporadic constrained-deadline tasks upon identical
multiprocessor platforms. We propose a new algorithm and a scheduling paradigm
based on the concept of semi-partitioned scheduling with restricted migrations
in which jobs are not allowed to migrate, but two subsequent jobs of a task can
be assigned to different processors by following a periodic strategy.
</summary>
    <author>
      <name>François Dorin</name>
    </author>
    <author>
      <name>Patrick Meumeu Yomsi</name>
    </author>
    <author>
      <name>Joël Goossens</name>
    </author>
    <author>
      <name>Pascal Richard</name>
    </author>
    <link href="http://arxiv.org/abs/1006.2637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.2637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.5845v1</id>
    <updated>2010-06-30T13:01:58Z</updated>
    <published>2010-06-30T13:01:58Z</published>
    <title>Dynamic and Transparent Analysis of Commodity Production Systems</title>
    <summary>  We propose a framework that provides a programming interface to perform
complex dynamic system-level analyses of deployed production systems. By
leveraging hardware support for virtualization available nowadays on all
commodity machines, our framework is completely transparent to the system under
analysis and it guarantees isolation of the analysis tools running on its top.
Thus, the internals of the kernel of the running system needs not to be
modified and the whole platform runs unaware of the framework. Moreover, errors
in the analysis tools do not affect the running system and the framework. This
is accomplished by installing a minimalistic virtual machine monitor and
migrating the system, as it runs, into a virtual machine. In order to
demonstrate the potentials of our framework we developed an interactive kernel
debugger, nicknamed HyperDbg. HyperDbg can be used to debug any critical kernel
component, and even to single step the execution of exception and interrupt
handlers.
</summary>
    <author>
      <name>Aristide Fattori</name>
    </author>
    <author>
      <name>Roberto Paleari</name>
    </author>
    <author>
      <name>Lorenzo Martignoni</name>
    </author>
    <author>
      <name>Mattia Monga</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1858996.1859085</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1858996.1859085" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, To appear in the 25th IEEE/ACM International Conference on
  Automated Software Engineering, Antwerp, Belgium, 20-24 September 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.5845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.5845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.5; D.4.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.1735v1</id>
    <updated>2010-11-08T09:07:54Z</updated>
    <published>2010-11-08T09:07:54Z</published>
    <title>Use of Data Mining in Scheduler Optimization</title>
    <summary>  The operating system's role in a computer system is to manage the various
resources. One of these resources is the Central Processing Unit. It is managed
by a component of the operating system called the CPU scheduler. Schedulers are
optimized for typical workloads expected to run on the platform. However, a
single scheduler may not be appropriate for all workloads. That is, a scheduler
may schedule a workload such that the completion time is minimized, but when
another type of workload is run on the platform, scheduling and therefore
completion time will not be optimal; a different scheduling algorithm, or a
different set of parameters, may work better. Several approaches to solving
this problem have been proposed. The objective of this survey is to summarize
the approaches based on data mining, which are available in the literature. In
addition to solutions that can be directly utilized for solving this problem,
we are interested in data mining research in related areas that have potential
for use in operating system scheduling. We also explain general technical
issues involved in scheduling in modern computers, including parallel
scheduling issues related to multi-core CPUs. We propose a taxonomy that
classifies the scheduling approaches we discuss into different categories.
</summary>
    <author>
      <name>George Anderson</name>
    </author>
    <author>
      <name>Tshilidzi Marwala</name>
    </author>
    <author>
      <name>Fulufhelo V. Nelwamondo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.1735v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.1735v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.4045v1</id>
    <updated>2010-12-18T00:51:57Z</updated>
    <published>2010-12-18T00:51:57Z</published>
    <title>Application of Global and One-Dimensional Local Optimization to
  Operating System Scheduler Tuning</title>
    <summary>  This paper describes a study of comparison of global and one-dimensional
local optimization methods to operating system scheduler tuning. The operating
system scheduler we use is the Linux 2.6.23 Completely Fair Scheduler (CFS)
running in simulator (LinSched). We have ported the Hackbench scheduler
benchmark to this simulator and use this as the workload. The global
optimization approach we use is Particle Swarm Optimization (PSO). We make use
of Response Surface Methodology (RSM) to specify optimal parameters for our PSO
implementation. The one-dimensional local optimization approach we use is the
Golden Section method. In order to use this approach, we convert the scheduler
tuning problem from one involving setting of three parameters to one involving
the manipulation of one parameter. Our results show that the global
optimization approach yields better response but the one- dimensional
optimization approach converges to a solution faster than the global
optimization approach.
</summary>
    <author>
      <name>George Anderson</name>
    </author>
    <author>
      <name>Tshilidzi Marwala</name>
    </author>
    <author>
      <name>Fulufhelo Vincent Nelwamondo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Twenty-First Annual Symposium of the Pattern
  Recognition Association of South Africa 22-23 November 2010 Stellenbosch,
  South Africa, pp. 7-11</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.4045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.4045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.5929v1</id>
    <updated>2010-12-29T12:41:13Z</updated>
    <published>2010-12-29T12:41:13Z</published>
    <title>Exact Schedulability Test for global-EDF Scheduling of Periodic Hard
  Real-Time Tasks on Identical Multiprocessors</title>
    <summary>  In this paper we consider the scheduling problem of hard real-time systems
composed of periodic constrained-deadline tasks upon identical multiprocessor
platforms. We assume that tasks are scheduled by using the global-EDF
scheduler. We establish an exact schedulability test for this scheduler by
exploiting on the one hand its predictability property and by providing on the
other hand a feasibility interval so that if it is possible to find a valid
schedule for all the jobs contained in this interval, then the whole system
will be stamped feasible. In addition, we show by means of a counterexample
that the feasibility interval, and thus the schedulability test, proposed by
Leung [Leung 1989] is incorrect and we show which arguments are actually
incorrect.
</summary>
    <author>
      <name>Joël Goossens</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Brussels University, U.L.B., Brussels, Belgium</arxiv:affiliation>
    </author>
    <author>
      <name>Patrick Meumeu Yomsi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">F.N.R.S, Belgium</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1012.5929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.5929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.1466v1</id>
    <updated>2011-01-06T10:28:58Z</updated>
    <published>2011-01-06T10:28:58Z</published>
    <title>Comparison of Loss ratios of different scheduling algorithms</title>
    <summary>  It is well known that in a firm real time system with a renewal arrival
process, exponential service times and independent and identically distributed
deadlines till the end of service of a job, the earliest deadline first (EDF)
scheduling policy has smaller loss ratio (expected fraction of jobs, not
completed) than any other service time independent scheduling policy, including
the first come first served (FCFS). Various modifications to the EDF and FCFS
policies have been proposed in the literature, with a view to improving
performance. In this article, we compare the loss ratios of these two policies
along with some of the said modifications, as well as their counterparts with
deterministic deadlines. The results include some formal inequalities and some
counter-examples to establish non-existence of an order. A few relations
involving loss ratios are posed as conjectures, and simulation results in
support of these are reported. These results lead to a complete picture of
dominance and non-dominance relations between pairs of scheduling policies, in
terms of loss ratios.
</summary>
    <author>
      <name>Sudipta Das</name>
    </author>
    <author>
      <name>Lawrence Jenkins</name>
    </author>
    <author>
      <name>Debasis Sengupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.1466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.1466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.2094v1</id>
    <updated>2011-02-10T12:15:58Z</updated>
    <published>2011-02-10T12:15:58Z</published>
    <title>Global Scheduling of Multi-Mode Real-Time Applications upon
  Multiprocessor Platforms</title>
    <summary>  Multi-mode real-time systems are those which support applications with
different modes of operation, where each mode is characterized by a specific
set of tasks. At run-time, such systems can, at any time, be requested to
switch from its current operating mode to another mode (called "new mode") by
replacing the current set of tasks with that of the new-mode. Thereby, ensuring
that all the timing requirements are met not only requires that a
schedulability test is performed on the tasks of each mode but also that (i) a
protocol for transitioning from one mode to another is specified and (ii) a
schedulability test for each transition is performed. We propose two distinct
protocols that manage the mode transitions upon uniform and identical
multiprocessor platforms at run-time, each specific to distinct task
requirements. For each protocol, we formally establish schedulability analyses
that indicate beforehand whether all the timing requirements will be met during
any mode transition of the system. This is performed assuming both
Fixed-Task-Priority and Fixed-Job-Priority schedulers.
</summary>
    <author>
      <name>Vincent Nelis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CISTER Research unit Polytechnic Institute of Porto</arxiv:affiliation>
    </author>
    <author>
      <name>Patrick Meumeu Yomsi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Björn Andersson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CISTER Research unit Polytechnic Institute of Porto</arxiv:affiliation>
    </author>
    <author>
      <name>Joël Goossens</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">U.L.B</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1102.2094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.2094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.2336v1</id>
    <updated>2011-03-11T18:36:38Z</updated>
    <published>2011-03-11T18:36:38Z</published>
    <title>Building XenoBuntu Linux Distribution for Teaching and Prototyping
  Real-Time Operating Systems</title>
    <summary>  This paper describes the realization of a new Linux distribution based on
Ubuntu Linux and Xenomai Real-Time framework. This realization is motivated by
the eminent need of real-time systems in modern computer science courses. The
majority of the technical choices are made after qualitative comparison. The
main goal of this distribution is to offer standard Operating Systems (OS) that
include Xenomai infrastructure and the essential tools to begin hard real-time
application development inside a convivial desktop environment. The released
live/installable DVD can be adopted to emulate several classic RTOS Application
Program Interfaces (APIs), directly use and understand real-time Linux in
convivial desktop environment and prototyping real-time embedded applications.
</summary>
    <author>
      <name>Nabil Litayem</name>
    </author>
    <author>
      <name>Ahmed Ben Achballah</name>
    </author>
    <author>
      <name>Slim Ben Saoud</name>
    </author>
    <link href="http://arxiv.org/abs/1103.2336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.2336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.3831v1</id>
    <updated>2011-03-20T06:45:16Z</updated>
    <published>2011-03-20T06:45:16Z</published>
    <title>A New Proposed Dynamic Quantum with Re-Adjusted Round Robin Scheduling
  Algorithm and Its Performance Analysis</title>
    <summary>  Scheduling is the central concept used frequently in Operating System. It
helps in choosing the processes for execution. Round Robin (RR) is one of the
most widely used CPU scheduling algorithm. But, its performance degrades with
respect to context switching, which is an overhead and it occurs during each
scheduling. Overall performance of the system depends on choice of an optimal
time quantum, so that context switching can be reduced. In this paper, we have
proposed a new variant of RR scheduling algorithm, known as Dynamic Quantum
with Readjusted Round Robin (DQRRR) algorithm. We have experimentally shown
that performance of DQRRR is better than RR by reducing number of context
switching, average waiting time and average turn around time.
</summary>
    <author>
      <name>H. S. Behera</name>
    </author>
    <author>
      <name>Rakesh Mohanty</name>
    </author>
    <author>
      <name>Debashree Nayak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">06 pages; International Journal of Computer Applications, Vol. 5, No.
  5, August 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.3831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.3831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.3832v1</id>
    <updated>2011-03-20T06:56:27Z</updated>
    <published>2011-03-20T06:56:27Z</published>
    <title>A New Dynamic Round Robin and SRTN Algorithm with Variable Original Time
  Slice and Intelligent Time Slice for Soft Real Time Systems</title>
    <summary>  The main objective of the paper is to improve the Round Robin (RR) algorithm
using dynamic ITS by coalescing it with Shortest Remaining Time Next (SRTN)
algorithm thus reducing the average waiting time, average turnaround time and
the number of context switches. The original time slice has been calculated for
each process based on its burst time.This is mostly suited for soft real time
systems where meeting of deadlines is desirable to increase its performance.
The advantage is that processes that are closer to their remaining completion
time will get more chances to execute and leave the ready queue. This will
reduce the number of processes in the ready queue by knocking out short jobs
relatively faster in a hope to reduce the average waiting time, turn around
time and number of context switches. This paper improves the algorithm [8] and
the experimental analysis shows that the proposed algorithm performs better
than algorithm [6] and [8] when the processes are having an increasing order,
decreasing order and random order of burst time.
</summary>
    <author>
      <name>H. S. Behera</name>
    </author>
    <author>
      <name>Simpi Patel</name>
    </author>
    <author>
      <name>Bijayalakshmi Panda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/2037-2648</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/2037-2648" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">07 pages; International Journal of Computer Applications, Vol 16, No.
  1(9) February 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.3832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.3832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.4786v1</id>
    <updated>2011-07-24T19:39:02Z</updated>
    <published>2011-07-24T19:39:02Z</published>
    <title>Towards Bridging IoT and Cloud Services: Proposing Smartphones as Mobile
  and Autonomic Service Gateways</title>
    <summary>  Computing is currently getting at the same time incredibly in the small with
sensors/actuators embedded in our every- day objects and also greatly in the
large with data and ser- vice clouds accessible anytime, anywhere. This
Internet of Things is physically closed to the user but suffers from weak
run-time execution environments. Cloud Environments provide powerful data
storage and computing power but can not be easily accessed and integrate the
final-user context- awareness. We consider smartphones are set to become the
universal interface between these two worlds. In this position paper, we
propose a middleware approach where smartphones provide service gateways to
bridge the gap between IoT services and Cloud services. Since smartphones are
mobile gateways, they should be able to (re)configure themself according to
their place, things discovered around, and their own resources such battery.
Several issues are discussed: collaborative event-based context management,
adaptive and opportunistic service deployment and invocation, multi-criteria
(user- and performance-oriented) optimization decision algorithm.
</summary>
    <author>
      <name>Roya Golchay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CITI Insa Lyon / Inria Grenoble Rhône-Alpes</arxiv:affiliation>
    </author>
    <author>
      <name>Frédéric Le Mouël</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CITI Insa Lyon / Inria Grenoble Rhône-Alpes</arxiv:affiliation>
    </author>
    <author>
      <name>Stéphane Frénot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CITI Insa Lyon / Inria Grenoble Rhône-Alpes</arxiv:affiliation>
    </author>
    <author>
      <name>Julien Ponge</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CITI Insa Lyon / Inria Grenoble Rhône-Alpes</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Position Paper</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">UbiMob'2011 (2011) 45--48</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1107.4786v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.4786v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.2638v1</id>
    <updated>2011-09-12T21:49:38Z</updated>
    <published>2011-09-12T21:49:38Z</published>
    <title>Light-weight Locks</title>
    <summary>  In this paper, we propose a new approach to building synchronization
primitives, dubbed "lwlocks" (short for light-weight locks). The primitives are
optimized for small memory footprint while maintaining efficient performance in
low contention scenarios. A read-write lwlock occupies 4 bytes, a mutex
occupies 4 bytes (2 if deadlock detection is not required), and a condition
variable occupies 4 bytes. The corresponding primitives of the popular pthread
library occupy 56 bytes, 40 bytes and 48 bytes respectively on the x86-64
platform. The API for lwlocks is similar to that of the pthread library but
covering only the most common use cases. Lwlocks allow explicit control of
queuing and scheduling decisions in contention situations and support
"asynchronous" or "deferred blocking" acquisition of locks. Asynchronous
locking helps in working around the constraints of lock-ordering which
otherwise limits concurrency. The small footprint of lwlocks enables the
construction of data structures with very fine-grained locking, which in turn
is crucial for lowering contention and supporting highly concurrent access to a
data structure. Currently, the Data Domain File System uses lwlocks for its
in-memory inode cache as well as in a generic doubly-linked concurrent list
which forms the building block for more sophisticated structures.
</summary>
    <author>
      <name>Nitin Garg</name>
    </author>
    <author>
      <name>Ed Zhu</name>
    </author>
    <author>
      <name>Fabiano C. Botelho</name>
    </author>
    <link href="http://arxiv.org/abs/1109.2638v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.2638v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.3076v1</id>
    <updated>2011-09-14T13:32:55Z</updated>
    <published>2011-09-14T13:32:55Z</published>
    <title>Comparative performance analysis of multi dynamic time quantum Round
  Robin(MDTQRR) algorithm with arrival time</title>
    <summary>  CPU being considered a primary computer resource, its scheduling is central
to operating-system design. A thorough performance evaluation of various
scheduling algorithms manifests that Round Robin Algorithm is considered as
optimal in time shared environment because the static time is equally shared
among the processes. We have proposed an efficient technique in the process
scheduling algorithm by using dynamic time quantum in Round Robin. Our approach
is based on the calculation of time quantum twice in single round robin cycle.
Taking into consideration the arrival time, we implement the algorithm.
Experimental analysis shows better performance of this improved algorithm over
the Round Robin algorithm and the Shortest Remaining Burst Round Robin
algorithm. It minimizes the overall number of context switches, average waiting
time and average turn-around time. Consequently the throughput and CPU
utilization is better.
</summary>
    <author>
      <name>H. S. Behera</name>
    </author>
    <author>
      <name>Rakesh Mohanty</name>
    </author>
    <author>
      <name>Sabyasachi Sahu</name>
    </author>
    <author>
      <name>Sourav Kumar Bhoi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 18 Figures, Indian Journal of Computer Science and
  Engineering vol. 2 no. 2 April-May 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.3076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.3076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.5251v1</id>
    <updated>2011-11-22T17:00:50Z</updated>
    <published>2011-11-22T17:00:50Z</published>
    <title>Evolution of a Modular Software Network</title>
    <summary>  "Evolution behaves like a tinkerer" (Francois Jacob, Science, 1977). Software
systems provide a unique opportunity to understand biological processes using
concepts from network theory. The Debian GNU/Linux operating system allows us
to explore the evolution of a complex network in a novel way. The modular
design detected during its growth is based on the reuse of existing code in
order to minimize costs during programming. The increase of modularity
experienced by the system over time has not counterbalanced the increase in
incompatibilities between software packages within modules. This negative
effect is far from being a failure of design. A random process of package
installation shows that the higher the modularity the larger the fraction of
packages working properly in a local computer. The decrease in the relative
number of conflicts between packages from different modules avoids a failure in
the functionality of one package spreading throughout the entire system. Some
potential analogies with the evolutionary and ecological processes determining
the structure of ecological networks of interacting species are discussed.
</summary>
    <author>
      <name>Miguel A. Fortuna</name>
    </author>
    <author>
      <name>Juan A. Bonachela</name>
    </author>
    <author>
      <name>Simon A. Levin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1073/pnas.1115960108</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1073/pnas.1115960108" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in PNAS</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.5251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.5251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.5348v1</id>
    <updated>2011-11-22T21:44:25Z</updated>
    <published>2011-11-22T21:44:25Z</published>
    <title>A New Round Robin Based Scheduling Algorithm for Operating Systems:
  Dynamic Quantum Using the Mean Average</title>
    <summary>  Round Robin, considered as the most widely adopted CPU scheduling algorithm,
undergoes severe problems directly related to quantum size. If time quantum
chosen is too large, the response time of the processes is considered too high.
On the other hand, if this quantum is too small, it increases the overhead of
the CPU. In this paper, we propose a new algorithm, called AN, based on a new
approach called dynamic-time-quantum; the idea of this approach is to make the
operating systems adjusts the time quantum according to the burst time of the
set of waiting processes in the ready queue. Based on the simulations and
experiments, we show that the new proposed algorithm solves the fixed time
quantum problem and increases the performance of Round Robin.
</summary>
    <author>
      <name>Abbas Noon</name>
    </author>
    <author>
      <name>Ali Kalakech</name>
    </author>
    <author>
      <name>Seifedine Kadry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 3, No. 1, 2011, 224-229</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1111.5348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.5348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68N25" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.1.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.4030v1</id>
    <updated>2012-02-17T22:33:12Z</updated>
    <published>2012-02-17T22:33:12Z</published>
    <title>AdSplit: Separating smartphone advertising from applications</title>
    <summary>  A wide variety of smartphone applications today rely on third-party
advertising services, which provide libraries that are linked into the hosting
application. This situation is undesirable for both the application author and
the advertiser. Advertising libraries require additional permissions, resulting
in additional permission requests to users. Likewise, a malicious application
could simulate the behavior of the advertising library, forging the user's
interaction and effectively stealing money from the advertiser. This paper
describes AdSplit, where we extended Android to allow an application and its
advertising to run as separate processes, under separate user-ids, eliminating
the need for applications to request permissions on behalf of their advertising
libraries.
  We also leverage mechanisms from Quire to allow the remote server to validate
the authenticity of client-side behavior. In this paper, we quantify the degree
of permission bloat caused by advertising, with a study of thousands of
downloaded apps. AdSplit automatically recompiles apps to extract their ad
services, and we measure minimal runtime overhead. We also observe that most ad
libraries just embed an HTML widget within and describe how AdSplit can be
designed with this in mind to avoid any need for ads to have native code.
</summary>
    <author>
      <name>Shashi Shekhar</name>
    </author>
    <author>
      <name>Michael Dietz</name>
    </author>
    <author>
      <name>Dan S. Wallach</name>
    </author>
    <link href="http://arxiv.org/abs/1202.4030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.4030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.0197v1</id>
    <updated>2012-04-01T11:05:34Z</updated>
    <published>2012-04-01T11:05:34Z</published>
    <title>Windows And Linux Operating Systems From A Security Perspective</title>
    <summary>  Operating systems are vital system software that, without them, humans would
not be able to manage and use computer systems. In essence, an operating system
is a collection of software programs whose role is to manage computer resources
and provide an interface for client applications to interact with the different
computer hardware. Most of the commercial operating systems available today on
the market have buggy code and they exhibit security flaws and vulnerabilities.
In effect, building a trusted operating system that can mostly resist attacks
and provide a secure computing environment to protect the important assets of a
computer is the goal of every operating system manufacturer. This paper deeply
investigates the various security features of the two most widespread and
successful operating systems, Microsoft Windows and Linux. The different
security features, designs, and components of the two systems are to be covered
elaborately, pin-pointing the key similarities and differences between them. In
due course, a head-to-head comparison is to be drawn for each security aspect,
exposing the advantage of one system over the other.
</summary>
    <author>
      <name>Youssef Bassil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org/; Journal of Global Research in Computer Science, Vol.
  3, No. 2, February 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.0197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.0197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.0124v1</id>
    <updated>2012-05-01T09:17:43Z</updated>
    <published>2012-05-01T09:17:43Z</published>
    <title>Schedulability Test for Soft Real-Time Systems under Multiprocessor
  Environment by using an Earliest Deadline First Scheduling Algorithm</title>
    <summary>  This paper deals with the study of Earliest Deadline First (EDF) which is an
optimal scheduling algorithm for uniprocessor real time systems use for
scheduling the periodic task in soft real-time multiprocessor systems. In hard
real-time systems, a significant disparity exists EDF-based schemes and RMA
scheduling (which is the only known way of optimally scheduling recurrent
real-time tasks on multiprocessors): on M processors, all known EDF variants
have utilization-based schedulability bounds of approximately M/2, while RMA
algorithms can fully utilize all processors. This is unfortunate because EDF
based algorithms entail lower scheduling and task migration overheads. In work
on hard real-time systems, it has been shown that this disparity in
Schedulability can be lessened by placing caps on per task utilizations. Our
main contribution is a new EDF based scheme that ensures bounded deadline
tardiness. In this scheme, per-task utilizations must be focused,but overall
utilization need not be stricted. Our scheme should enable a wide range of soft
real-time applications to be scheduled with no constraints on total
utilization. Also propose techniques and heuristics that can be used to reduce
tardiness as well as increase the efficiency of task.
</summary>
    <author>
      <name>Jagbeer Singh</name>
    </author>
    <author>
      <name>Satyendra Prasad Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.0124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.0124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.0396v1</id>
    <updated>2012-06-02T18:22:37Z</updated>
    <published>2012-06-02T18:22:37Z</published>
    <title>Energy-Aware Task Partitioning on Heterogeneous Multiprocessor Platforms</title>
    <summary>  Efficient task partitioning plays a crucial role in achieving high
performance at multiprocessor plat forms. This paper addresses the problem of
energy-aware static partitioning of periodic real-time tasks on heterogeneous
multiprocessor platforms. A Particle Swarm Optimization variant based on
Min-min technique for task partitioning is proposed. The proposed approach aims
to minimize the overall energy consumption, meanwhile avoid deadline
violations. An energy-aware cost function is proposed to be considered in the
proposed approach. Extensive simulations and comparisons are conducted in order
to validate the effectiveness of the proposed technique. The achieved results
demonstrate that the proposed partitioning scheme significantly surpasses
previous approaches in terms of both number of iterations and energy savings.
</summary>
    <author>
      <name>Elsayed Saad</name>
    </author>
    <author>
      <name>Medhat Awadalla</name>
    </author>
    <author>
      <name>Mohamed Shalan</name>
    </author>
    <author>
      <name>Abdullah Elewi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">the International Journal of Computer Science Issues (IJCSI), Vol.
  9, Issue 2, No. 1, 2012, pp. 176-183</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1206.0396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.0396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.6391v1</id>
    <updated>2012-08-31T06:32:38Z</updated>
    <published>2012-08-31T06:32:38Z</published>
    <title>On Benchmarking Embedded Linux Flash File Systems</title>
    <summary>  Due to its attractive characteristics in terms of performance, weight and
power consumption, NAND flash memory became the main non volatile memory (NVM)
in embedded systems. Those NVMs also present some specific
characteristics/constraints: good but asymmetric I/O performance, limited
lifetime, write/erase granularity asymmetry, etc. Those peculiarities are
either managed in hardware for flash disks (SSDs, SD cards, USB sticks, etc.)
or in software for raw embedded flash chips. When managed in software, flash
algorithms and structures are implemented in a specific flash file system
(FFS). In this paper, we present a performance study of the most widely used
FFSs in embedded Linux: JFFS2, UBIFS,and YAFFS. We show some very particular
behaviors and large performance disparities for tested FFS operations such as
mounting, copying, and searching file trees, compression, etc.
</summary>
    <author>
      <name>Pierre Olivier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Jalil Boukhobza</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Senn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Embed With Linux, Lorient : France (2012)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SIGBED Review 9(2) 43-47 9, 2 (2012) 43-47</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1208.6391v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.6391v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4600v1</id>
    <updated>2012-09-19T03:59:12Z</updated>
    <published>2012-09-19T03:59:12Z</published>
    <title>Classification Of Heterogeneous Operating System</title>
    <summary>  Operating system is a bridge between system and user. An operating system
(OS) is a software program that manages the hardware and software resources of
a computer. The OS performs basic tasks, such as controlling and allocating
memory, prioritizing the processing of instructions, controlling input and
output devices, facilitating networking, and managing files. It is difficult to
present a complete as well as deep account of operating systems developed till
date. So, this paper tries to overview only a subset of the available operating
systems and its different categories. OS are being developed by a large number
of academic and commercial organizations for the last several decades. This
paper, therefore, concentrates on the different categories of OS with special
emphasis to those that had deep impact on the evolution process. The aim of
this paper is to provide a brief timely commentary on the different categories
important operating systems available today.
</summary>
    <author>
      <name>Kamlesh Sharma</name>
    </author>
    <author>
      <name>T. V. Prasad</name>
    </author>
    <link href="http://arxiv.org/abs/1209.4600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.4840v1</id>
    <updated>2012-11-20T19:38:00Z</updated>
    <published>2012-11-20T19:38:00Z</published>
    <title>Multicore Dynamic Kernel Modules Attachment Technique for Kernel
  Performance Enhancement</title>
    <summary>  Traditional monolithic kernels dominated kernel structures for long time
along with small sized kernels,few hardware companies and limited kernel
functionalities. Monolithic kernel structure was not applicable when the number
of hardware companies increased and kernel services consumed by different users
for many purposes. One of the biggest disadvantages of the monolithic kernels
is the inflexibility due to the need to include all the available modules in
kernel compilation causing high time consuming. Lately, new kernel structure
was introduced through multicore operating systems. Unfortunately, many
multicore operating systems such as barrelfish and FOS are experimental. This
paper aims to simulate the performance of multicore hybrid kernels through
dynamic kernel module customized attachment/ deattachment for multicore
machines. In addition, this paper proposes a new technique for loading dynamic
kernel modules based on the user needs and machine capabilities.
</summary>
    <author>
      <name>Mohamed Farag</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsit.2012.4405</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsit.2012.4405" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, International Journal of Computer Science &amp; Information
  Technology (IJCSIT) Vol 4, No 4, August 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.4840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.4840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.6185v1</id>
    <updated>2012-11-27T02:36:10Z</updated>
    <published>2012-11-27T02:36:10Z</published>
    <title>Automatic Verification of Message-Based Device Drivers</title>
    <summary>  We develop a practical solution to the problem of automatic verification of
the interface between device drivers and the OS. Our solution relies on a
combination of improved driver architecture and verification tools. It supports
drivers written in C and can be implemented in any existing OS, which sets it
apart from previous proposals for verification-friendly drivers. Our
Linux-based evaluation shows that this methodology amplifies the power of
existing verification tools in detecting driver bugs, making it possible to
verify properties beyond the reach of traditional techniques.
</summary>
    <author>
      <name>Sidney Amani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NICTA and UNSW</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Chubb</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NICTA and UNSW</arxiv:affiliation>
    </author>
    <author>
      <name>Alastair F. Donaldson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Imperial College London</arxiv:affiliation>
    </author>
    <author>
      <name>Alexander Legg</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NICTA and UNSW</arxiv:affiliation>
    </author>
    <author>
      <name>Leonid Ryzhyk</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NICTA and UNSW</arxiv:affiliation>
    </author>
    <author>
      <name>Yanjin Zhu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NICTA and UNSW</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.102.3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.102.3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings SSV 2012, arXiv:1211.5873</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 102, 2012, pp. 4-17</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.6185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.6185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.4; B.4.2; D.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.1787v1</id>
    <updated>2012-12-08T12:56:49Z</updated>
    <published>2012-12-08T12:56:49Z</published>
    <title>A Generic Checkpoint-Restart Mechanism for Virtual Machines</title>
    <summary>  It is common today to deploy complex software inside a virtual machine (VM).
Snapshots provide rapid deployment, migration between hosts, dependability
(fault tolerance), and security (insulating a guest VM from the host). Yet, for
each virtual machine, the code for snapshots is laboriously developed on a
per-VM basis. This work demonstrates a generic checkpoint-restart mechanism for
virtual machines. The mechanism is based on a plugin on top of an unmodified
user-space checkpoint-restart package, DMTCP. Checkpoint-restart is
demonstrated for three virtual machines: Lguest, user-space QEMU, and KVM/QEMU.
The plugins for Lguest and KVM/QEMU require just 200 lines of code. The Lguest
kernel driver API is augmented by 40 lines of code. DMTCP checkpoints
user-space QEMU without any new code. KVM/QEMU, user-space QEMU, and DMTCP need
no modification. The design benefits from other DMTCP features and plugins.
Experiments demonstrate checkpoint and restart in 0.2 seconds using forked
checkpointing, mmap-based fast-restart, and incremental Btrfs-based snapshots.
</summary>
    <author>
      <name>Rohan Garg</name>
    </author>
    <author>
      <name>Komal Sodha</name>
    </author>
    <author>
      <name>Gene Cooperman</name>
    </author>
    <link href="http://arxiv.org/abs/1212.1787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.1787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.4800v1</id>
    <updated>2013-01-21T10:00:22Z</updated>
    <published>2013-01-21T10:00:22Z</published>
    <title>Schedulability Analysis of Distributed Real-Time Applications under
  Dependence and Several Latency Constraints</title>
    <summary>  This paper focuses on the analysis of real-time non preemptive multiprocessor
scheduling with precedence and several latency constraints. It aims to specify
a schedulability condition which enables a designer to check a priori -without
executing or simulating- if its scheduling of tasks will hold the precedences
between tasks as well as several latency constraints imposed on determined
pairs of tasks. It is shown that the required analysis is closely linked to the
topological structure of the application graph. More precisely, it depends on
the configuration of tasks paths subject to latency constraints. As a result of
the study, a sufficient schedulability condition is introduced for precedences
and latency constraints in the hardest configuration in term of complexity with
an optimal number of processors in term of applications parallelism. In
addition, the proposed conditions provides a practical lower bounds for general
cases. Performances results and comparisons with an optimal approach
demonstrate the effectiveness of the proposed approach.
</summary>
    <author>
      <name>Omar Kermia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/10145-4978</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/10145-4978" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures, Published with International Journal of Computer
  Applications (IJCA)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications 62(14):1-7, January
  2013. Published by Foundation of Computer Science, New York, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.4800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.4800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5502v1</id>
    <updated>2013-02-22T07:32:48Z</updated>
    <published>2013-02-22T07:32:48Z</published>
    <title>LFTL: A multi-threaded FTL for a Parallel IO Flash Card under Linux</title>
    <summary>  New PCI-e flash cards and SSDs supporting over 100,000 IOPs are now
available, with several usecases in the design of a high performance storage
system. By using an array of flash chips, arranged in multiple banks, large
capacities are achieved. Such multi-banked architecture allow parallel read,
write and erase operations. In a raw PCI-e flash card, such parallelism is
directly available to the software layer. In addition, the devices have
restrictions such as, pages within a block can only be written sequentially.
The devices also have larger minimum write sizes (greater than 4KB). Current
flash translation layers (FTLs) in Linux are not well suited for such devices
due to the high device speeds, architectural restrictions as well as other
factors such as high lock contention. We present a FTL for Linux that takes
into account the hardware restrictions, that also exploits the parallelism to
achieve high speeds. We also consider leveraging the parallelism for garbage
collection by scheduling the garbage collection activities on idle banks. We
propose and evaluate an adaptive method to vary the amount of garbage
collection according to the current I/O load on the device.
</summary>
    <author>
      <name> Srimugunthan</name>
    </author>
    <author>
      <name>K. Gopinath</name>
    </author>
    <author>
      <name>Giridhar Appaji Nag Yasa</name>
    </author>
    <link href="http://arxiv.org/abs/1302.5502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3557v1</id>
    <updated>2013-04-12T07:33:42Z</updated>
    <published>2013-04-12T07:33:42Z</published>
    <title>Survey of Server Virtualization</title>
    <summary>  Virtualization is a term that refers to the abstraction of computer
resources. The purpose of virtual computing environment is to improve resource
utilization by providing a unified integrated operating platform for users and
applications based on aggregation of heterogeneous and autonomous resources.
More recently, virtualization at all levels (system, storage, and network)
became important again as a way to improve system security, reliability and
availability, reduce costs, and provide greater flexibility. Virtualization has
rapidly become a go-to technology for increasing efficiency in the data center.
With virtualization technologies providing tremendous flexibility, even
disparate architectures may be deployed on a single machine without
interference This paper explains the basics of server virtualization and
addresses pros and cons of virtualization
</summary>
    <author>
      <name>Radhwan Y Ameen</name>
    </author>
    <author>
      <name>Asmaa Y. Hamo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages 14 figures. arXiv admin note: text overlap with
  arXiv:1010.3233 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.3557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3771v1</id>
    <updated>2013-04-13T04:04:08Z</updated>
    <published>2013-04-13T04:04:08Z</published>
    <title>Making I/O Virtualization Easy with Device Files</title>
    <summary>  Personal computers have diverse and fast-evolving I/O devices, making their
I/O virtualization different from that of servers and data centers. In this
paper, we present our recent endeavors in simplifying I/O virtualization for
personal computers. Our key insight is that many operating systems, including
Unix-like ones, abstract I/O devices as device files. There is a small and
stable set of operations on device files, therefore, I/O virtualization at the
device file boundary requires a one-time effort to support various I/O devices.
  We present devirtualization, our design of I/O virtualization at the device
file boundary and its implementation for Linux/x86 systems. We are able to
virtualize various GPUs, input devices, cameras, and audio devices with fewer
than 4900 LoC, of which only about 300 are specific to I/O device classes. Our
measurements show that devirtualized devices achieve interactive performance
indistinguishable from native ones by human users, even when running 3D HD
games.
</summary>
    <author>
      <name>Ardalan Amiri Sani</name>
    </author>
    <author>
      <name>Sreekumar Nair</name>
    </author>
    <author>
      <name>Lin Zhong</name>
    </author>
    <author>
      <name>Quinn Jacobson</name>
    </author>
    <link href="http://arxiv.org/abs/1304.3771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7001v1</id>
    <updated>2013-04-25T11:10:38Z</updated>
    <published>2013-04-25T11:10:38Z</published>
    <title>Network Control Systems RTAI framework A Review</title>
    <summary>  With the advancement in the automation industry, to perform complex remote
operations is required. Advancements in the networking technology has led to
the development of different architectures to implement control from a large
distance. In various control applications of the modern industry, the agents,
such as sensors, actuators, and controllers are basically geographically
distributed. For efficient working of a control application, all of the agents
have to exchange information through a communication media. At present, an
increasing number of distributed control systems are based on platforms made up
of conventional PCs running open-source real-time operating systems. Often,
these systems needed to have networked devices supporting synchronized
operations with respect to each node. A framework is studied that relies on
standard software and protocol as RTAI, EtherCAT, RTnet and IEEE 1588. RTAI and
its various protocols are studied in network control systems environment.
</summary>
    <author>
      <name>Deepika Bhatia</name>
    </author>
    <author>
      <name>Urmila Shrawankar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages: 4 Figures : 1</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Technologies (IJCSIT),Vol. 2(5) , 2011, 2380-2383</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.7001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.2553v1</id>
    <updated>2013-05-12T02:44:15Z</updated>
    <published>2013-05-12T02:44:15Z</published>
    <title>Practical Fine-grained Privilege Separation in Multithreaded
  Applications</title>
    <summary>  An inherent security limitation with the classic multithreaded programming
model is that all the threads share the same address space and, therefore, are
implicitly assumed to be mutually trusted. This assumption, however, does not
take into consideration of many modern multithreaded applications that involve
multiple principals which do not fully trust each other. It remains challenging
to retrofit the classic multithreaded programming model so that the security
and privilege separation in multi-principal applications can be resolved.
  This paper proposes ARBITER, a run-time system and a set of security
primitives, aimed at fine-grained and data-centric privilege separation in
multithreaded applications. While enforcing effective isolation among
principals, ARBITER still allows flexible sharing and communication between
threads so that the multithreaded programming paradigm can be preserved. To
realize controlled sharing in a fine-grained manner, we created a novel
abstraction named ARBITER Secure Memory Segment (ASMS) and corresponding OS
support. Programmers express security policies by labeling data and principals
via ARBITER's API following a unified model. We ported a widely-used, in-memory
database application (memcached) to ARBITER system, changing only around 100
LOC. Experiments indicate that only an average runtime overhead of 5.6% is
induced to this security enhanced version of application.
</summary>
    <author>
      <name>Jun Wang</name>
    </author>
    <author>
      <name>Xi Xiong</name>
    </author>
    <author>
      <name>Peng Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1305.2553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.2553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.3345v1</id>
    <updated>2013-05-15T02:53:19Z</updated>
    <published>2013-05-15T02:53:19Z</published>
    <title>Augmenting Operating Systems With the GPU</title>
    <summary>  The most popular heterogeneous many-core platform, the CPU+GPU combination,
has received relatively little attention in operating systems research. This
platform is already widely deployed: GPUs can be found, in some form, in most
desktop and laptop PCs. Used for more than just graphics processing, modern
GPUs have proved themselves versatile enough to be adapted to other
applications as well. Though GPUs have strengths that can be exploited in
systems software, this remains a largely untapped resource. We argue that
augmenting the OS kernel with GPU computing power opens the door to a number of
new opportunities. GPUs can be used to speed up some kernel functions, make
other scale better, and make it feasible to bring some computation-heavy
functionality into the kernel. We present our framework for using the GPU as a
co-processor from an OS kernel, and demonstrate a prototype in Linux.
</summary>
    <author>
      <name>Weibin Sun</name>
    </author>
    <author>
      <name>Robert Ricci</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, old white paper submitted for KGPU citation</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.3345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.3345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.4165v1</id>
    <updated>2013-07-16T05:21:34Z</updated>
    <published>2013-07-16T05:21:34Z</published>
    <title>A Comparative Study of CPU Scheduling Algorithms</title>
    <summary>  Developing CPU scheduling algorithms and understanding their impact in
practice can be difficult and time consuming due to the need to modify and test
operating system kernel code and measure the resulting performance on a
consistent workload of real applications. As processor is the important
resource, CPU scheduling becomes very important in accomplishing the operating
system (OS) design goals. The intention should be allowed as many as possible
running processes at all time in order to make best use of CPU. This paper
presents a state diagram that depicts the comparative study of various
scheduling algorithms for a single CPU and shows which algorithm is best for
the particular situation. Using this representation, it becomes much easier to
understand what is going on inside the system and why a different set of
processes is a candidate for the allocation of the CPU at different time. The
objective of the study is to analyze the high efficient CPU scheduler on design
of the high quality scheduling algorithms which suits the scheduling goals. Key
Words:-Scheduler, State Diagrams, CPU-Scheduling, Performance
</summary>
    <author>
      <name>Neetu Goel</name>
    </author>
    <author>
      <name>R. B. Garg</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Graphics &amp; Image Processing |Vol 2|issue
  4|November 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.4165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.4165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.4167v1</id>
    <updated>2013-07-16T05:25:16Z</updated>
    <published>2013-07-16T05:25:16Z</published>
    <title>An Optimum Multilevel Dynamic Round Robin Scheduling Algorithm</title>
    <summary>  The main objective of this paper is to improve the Round Robin scheduling
algorithm using the dynamic time slice concept. CPU scheduling becomes very
important in accomplishing the operating system (OS) design goals. The
intention should be allowed as many as possible running processes at all time
in order to make best use of CPU. CPU scheduling has strong effect on resource
utilization as well as overall performance of the system. Round Robin algorithm
performs optimally in time-shared systems, but it is not suitable for soft real
time systems, because it gives more number of context switches, larger waiting
time and larger response time. In this paper, a new CPU scheduling algorithm
called An Optimum Multilevel Dynamic Round Robin Scheduling Algorithm is
proposed, which calculates intelligent time slice and changes after every round
of execution. The suggested algorithm was evaluated on some CPU scheduling
objectives and it was observed that this algorithm gave good performance as
compared to the other existing CPU scheduling algorithms.
</summary>
    <author>
      <name>Neetu Goel</name>
    </author>
    <author>
      <name>R. B. Garg</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in National Conference on Information Communication &amp;
  Networks, Dated: April 6,2013 organized by Tecnia Institute of Advanced
  Studies</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.4167v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.4167v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.1199v1</id>
    <updated>2013-08-06T07:54:19Z</updated>
    <published>2013-08-06T07:54:19Z</published>
    <title>Intensional view of General Single Processor Operating Systems</title>
    <summary>  Operating systems are currently viewed ostensively. As a result they mean
different things to different people. The ostensive character makes it is hard
to understand OSes formally. An intensional view can enable better formal work,
and also offer constructive support for some important problems, e.g. OS
architecture. This work argues for an intensional view of operating systems. It
proposes to overcome the current ostensive view by defining an OS based on
formal models of computation, and also introduces some principles. Together
these are used to develop a framework of algorithms of single processor OS
structure using an approach similar to function level programming. In this
abridged paper we illustrate the essential approach, discuss some advantages
and limitations and point out some future possibilities.
</summary>
    <author>
      <name>Abhijat Vichare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 3 figures. Condensed and improved version of
  http://arxiv.org/abs/1112.4451 for submission to SOSP'13. arXiv admin note:
  substantial text overlap with arXiv:1112.4451</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.1199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.1199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.1714v1</id>
    <updated>2013-09-06T18:14:04Z</updated>
    <published>2013-09-06T18:14:04Z</published>
    <title>Flashmon V2: Monitoring Raw NAND Flash Memory I/O Requests on Embedded
  Linux</title>
    <summary>  This paper presents Flashmon version 2, a tool for monitoring embedded Linux
NAND flash memory I/O requests. It is designed for embedded boards based
devices containing raw flash chips. Flashmon is a kernel module and stands for
"flash monitor". It traces flash I/O by placing kernel probes at the NAND
driver level. It allows tracing at runtime the 3 main flash operations: page
reads / writes and block erasures. Flashmon is (1) generic as it was
successfully tested on the three most widely used flash file systems that are
JFFS2, UBIFS and YAFFS, and several NAND chip models. Moreover, it is (2) non
intrusive, (3) has a controllable memory footprint, and (4) exhibits a low
overhead (&lt;6%) on the traced system. Finally, it is (5) simple to integrate and
used as a standalone module or as a built-in function / module in existing
kernel sources. Monitoring flash memory operations allows a better
understanding of existing flash management systems by studying and analyzing
their behavior. Moreover it is useful in development phase for prototyping and
validating new solutions.
</summary>
    <author>
      <name>Pierre Olivier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Jalil Boukhobza</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Senn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EWiLi, the Embedded Operating Systems Workshop, Toulouse : France
  (2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.1714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.1714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.3096v1</id>
    <updated>2013-09-12T10:26:10Z</updated>
    <published>2013-09-12T10:26:10Z</published>
    <title>Simulation of an Optimum Multilevel Dynamic Round Robin Scheduling
  Algorithm</title>
    <summary>  CPU scheduling has valiant effect on resource utilization as well as overall
quality of the system. Round Robin algorithm performs optimally in time shared
systems, but it performs more number of context switches, larger waiting time
and larger response time. In order to simulate the behavior of various CPU
scheduling algorithms and to improve Round Robin scheduling algorithm using
dynamic time slice concept, in this paper we produce the implementation of new
CPU scheduling algorithm called An Optimum Multilevel Dynamic Round Robin
Scheduling (OMDRRS), which calculates intelligent time slice and warps after
every round of execution. The results display the robustness of this software,
especially for academic, research and experimental use, as well as proving the
desirability and efficiency of the probabilistic algorithm over the other
existing techniques and it is observed that this OMDRRS projects good
performance as compared to the other existing CPU scheduling algorithms.
</summary>
    <author>
      <name>Neetu Goel</name>
    </author>
    <author>
      <name>R. B. Garg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/13263-0743</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/13263-0743" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications, Aug 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.3096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.3096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.1588v1</id>
    <updated>2013-10-06T14:27:59Z</updated>
    <published>2013-10-06T14:27:59Z</published>
    <title>Impacting the bioscience progress by backporting software for Bio-Linux</title>
    <summary>  In year 2006 Bio-Linux with the work of Tim Booth and team gives its rising
and provide an operating system that was and still specialized in providing a
bioinformatic specific software environment for the working needs in this
corner of bioscience. It is shown that Bio-Linux is affected by a 2 year
release cycle and with this the final releases of Bio-Linux will not have the
latest bioinformatic software on board. The paper shows how to get around this
huge time gap and bring new software for Bio-Linux on board through a process
that is called backporting. A summary of within the work to this paper just
backported bioinformatic tools is given. A describtion of a workflow for
continuously integration of the newest bioinformatic tools gives an outlook to
further concrete planned developments and the influence of speeding up
scientific progress.
</summary>
    <author>
      <name>Sasa Paporovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,2 Figures, 1 Table and 1 notice</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.1588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.1588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.6298v1</id>
    <updated>2013-10-23T17:14:25Z</updated>
    <published>2013-10-23T17:14:25Z</published>
    <title>The Quest-V Separation Kernel for Mixed Criticality Systems</title>
    <summary>  Multi- and many-core processors are becoming increasingly popular in embedded
systems. Many of these processors now feature hardware virtualization
capabilities, such as the ARM Cortex A15, and x86 processors with Intel VT-x or
AMD-V support. Hardware virtualization offers opportunities to partition
physical resources, including processor cores, memory and I/O devices amongst
guest virtual machines. Mixed criticality systems and services can then
co-exist on the same platform in separate virtual machines. However,
traditional virtual machine systems are too expensive because of the costs of
trapping into hypervisors to multiplex and manage machine physical resources on
behalf of separate guests. For example, hypervisors are needed to schedule
separate VMs on physical processor cores. In this paper, we discuss the design
of the Quest-V separation kernel, that partitions services of different
criticalities in separate virtual machines, or sandboxes. Each sandbox
encapsulates a subset of machine physical resources that it manages without
requiring intervention of a hypervisor. Moreover, a hypervisor is not needed
for normal operation, except to bootstrap the system and establish
communication channels between sandboxes.
</summary>
    <author>
      <name>Ye Li</name>
    </author>
    <author>
      <name>Richard West</name>
    </author>
    <author>
      <name>Eric Missimer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.6298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.6298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.6301v1</id>
    <updated>2013-10-23T17:29:42Z</updated>
    <published>2013-10-23T17:29:42Z</published>
    <title>Predictable Migration and Communication in the Quest-V Multikernel</title>
    <summary>  Quest-V is a system we have been developing from the ground up, with
objectives focusing on safety, predictability and efficiency. It is designed to
work on emerging multicore processors with hardware virtualization support.
Quest-V is implemented as a "distributed system on a chip" and comprises
multiple sandbox kernels. Sandbox kernels are isolated from one another in
separate regions of physical memory, having access to a subset of processing
cores and I/O devices. This partitioning prevents system failures in one
sandbox affecting the operation of other sandboxes. Shared memory channels
managed by system monitors enable inter-sandbox communication.
  The distributed nature of Quest-V means each sandbox has a separate physical
clock, with all event timings being managed by per-core local timers. Each
sandbox is responsible for its own scheduling and I/O management, without
requiring intervention of a hypervisor.
  In this paper, we formulate bounds on inter-sandbox communication in the
absence of a global scheduler or global system clock. We also describe how
address space migration between sandboxes can be guaranteed without violating
service constraints. Experimental results on a working system show the
conditions under which Quest-V performs real-time communication and migration.
</summary>
    <author>
      <name>Ye Li</name>
    </author>
    <author>
      <name>Eric Missimer</name>
    </author>
    <author>
      <name>Richard West</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.6301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.6301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.6349v1</id>
    <updated>2013-10-23T17:35:03Z</updated>
    <published>2013-10-23T17:35:03Z</published>
    <title>Quest-V: A Virtualized Multikernel for Safety-Critical Real-Time Systems</title>
    <summary>  Modern processors are increasingly featuring multiple cores, as well as
support for hardware virtualization. While these processors are common in
desktop and server-class computing, they are less prevalent in embedded and
real-time systems. However, smartphones and tablet PCs are starting to feature
multicore processors with hardware virtualization. If the trend continues, it
is possible that future real-time systems will feature more sophisticated
processor architectures. Future automotive or avionics systems, for example,
could replace complex networks of uniprocessors with consolidated services on a
smaller number of multicore processors. Likewise, virtualization could be used
to isolate services and increase the availability of a system even when
failures occur.
  This paper investigates whether advances in modern processor technologies
offer new opportunities to rethink the design of real-time operating systems.
We describe some of the design principles behind Quest-V, which is being used
as an exploratory vehicle for real-time system design on multicore processors
with hardware virtualization capabilities. While not all embedded systems
should assume such features, a case can be made that more robust,
safety-critical systems can be built to use hardware virtualization without
incurring significant overheads.
</summary>
    <author>
      <name>Richard West</name>
    </author>
    <author>
      <name>Ye Li</name>
    </author>
    <author>
      <name>Eric Missimer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages. arXiv admin note: text overlap with arXiv:1112.5136,
  arXiv:1310.6301</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.6349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.6349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.3686v1</id>
    <updated>2013-11-13T10:53:02Z</updated>
    <published>2013-11-13T10:53:02Z</published>
    <title>Performance Evaluation of Java File Security System (JFSS)</title>
    <summary>  Security is a critical issue of the modern file and storage systems, it is
imperative to protect the stored data from unauthorized access. We have
developed a file security system named as Java File Security System (JFSS) [1]
that guarantee the security to files on the demand of all users. It has been
developed on Java platform. Java has been used as programming language in order
to provide portability, but it enforces some performance limitations. It is
developed in FUSE (File System in User space) [3]. Many efforts have been done
over the years for developing file systems in user space (FUSE). All have their
own merits and demerits. In this paper we have evaluated the performance of
Java File Security System (JFSS). Over and over again, the increased security
comes at the expense of user convenience, performance or compatibility with
other systems. JFSS system performance evaluations show that encryption
overheads are modest as compared to security.
</summary>
    <author>
      <name>Brijender Kahanwal</name>
    </author>
    <author>
      <name>Dr. Tejinder Pal Singh</name>
    </author>
    <author>
      <name>Dr. R. K. Tuteja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures, journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pelagia Research Library Advances in Applied Science Research,
  Vol. 2(6), pp. 254-260, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1311.3686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.3686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.3665v2</id>
    <updated>2014-05-05T16:05:35Z</updated>
    <published>2013-12-12T22:38:08Z</published>
    <title>Managing NymBoxes for Identity and Tracking Protection</title>
    <summary>  Despite the attempts of well-designed anonymous communication tools to
protect users from tracking or identification, flaws in surrounding software
(such as web browsers) and mistakes in configuration may leak the user's
identity. We introduce Nymix, an anonymity-centric operating system
architecture designed "top-to-bottom" to strengthen identity- and
tracking-protection. Nymix's core contribution is OS support for nym-browsing:
independent, parallel, and ephemeral web sessions. Each web session, or
pseudonym, runs in a unique virtual machine (VM) instance evolving from a
common base state with support for long-lived sessions which can be anonymously
stored to the cloud, avoiding de-anonymization despite potential confiscation
or theft. Nymix allows a user to safely browse the Web using various different
transports simultaneously through a pluggable communication model that supports
Tor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600
MB per nymbox and loads within 15 to 25 seconds.
</summary>
    <author>
      <name>David Isaac Wolinsky</name>
    </author>
    <author>
      <name>Bryan Ford</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 7 figure, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.3665v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.3665v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4931v1</id>
    <updated>2013-12-17T20:43:33Z</updated>
    <published>2013-12-17T20:43:33Z</published>
    <title>Rio: A System Solution for Sharing I/O between Mobile Systems</title>
    <summary>  Mobile systems are equipped with a diverse collection of I/O devices,
including cameras, microphones, sensors, and modems. There exist many novel use
cases for allowing an application on one mobile system to utilize I/O devices
from another. This paper presents Rio, an I/O sharing solution that supports
unmodified applications and exposes all the functionality of an I/O device for
sharing. Rio's design is common to many classes of I/O devices, thus
significantly reducing the engineering effort to support new I/O devices. Our
implementation of Rio on Android consists of 6700 total lines of code and
supports four I/O classes with fewer than 450 class-specific lines of code. Rio
also supports I/O sharing between mobile systems of different form factors,
including smartphones and tablets. We show that Rio achieves performance close
to that of local I/O for audio, sensors, and modems, but suffers noticeable
performance degradation for camera due to network throughput limitations
between the two systems, which is likely to be alleviated by emerging wireless
standards.
</summary>
    <author>
      <name>Ardalan Amiri Sani</name>
    </author>
    <author>
      <name>Kevin Boos</name>
    </author>
    <author>
      <name>Min Hong Yun</name>
    </author>
    <author>
      <name>Lin Zhong</name>
    </author>
    <link href="http://arxiv.org/abs/1312.4931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6650v2</id>
    <updated>2014-02-03T23:01:55Z</updated>
    <published>2013-12-23T19:19:40Z</published>
    <title>Transparent Checkpoint-Restart for Hardware-Accelerated 3D Graphics</title>
    <summary>  Providing fault-tolerance for long-running GPU-intensive jobs requires
application-specific solutions, and often involves saving the state of complex
data structures spread among many graphics libraries. This work describes a
mechanism for transparent GPU-independent checkpoint-restart of 3D graphics.
The approach is based on a record-prune-replay paradigm: all OpenGL calls
relevant to the graphics driver state are recorded; calls not relevant to the
internal driver state as of the last graphics frame prior to checkpoint are
discarded; and the remaining calls are replayed on restart. A previous approach
for OpenGL 1.5, based on a shadow device driver, required more than 78,000
lines of OpenGL-specific code. In contrast, the new approach, based on
record-prune-replay, is used to implement the same case in just 4,500 lines of
code. The speed of this approach varies between 80 per cent and nearly 100 per
cent of the speed of the native hardware acceleration for OpenGL 1.5, as
measured when running the ioquake3 game under Linux. This approach has also
been extended to demonstrate checkpointing of OpenGL 3.0 for the first time,
with a demonstration for PyMol, for molecular visualization.
</summary>
    <author>
      <name>Samaneh Kazemi Nafchi</name>
    </author>
    <author>
      <name>Rohan Garg</name>
    </author>
    <author>
      <name>Gene Cooperman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.6650v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6650v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.0334v1</id>
    <updated>2014-03-03T08:13:29Z</updated>
    <published>2014-03-03T08:13:29Z</published>
    <title>Design and Performance Evaluation of an Optimized Disk Scheduling
  Algorithm (ODSA)</title>
    <summary>  Management of disk scheduling is a very important aspect of operating system.
Performance of the disk scheduling completely depends on how efficient is the
scheduling algorithm to allocate services to the request in a better manner.
Many algorithms (FIFO, SSTF, SCAN, C-SCAN, LOOK, etc.) are developed in the
recent years in order to optimize the system disk I/O performance. By reducing
the average seek time and transfer time, we can improve the performance of disk
I/O operation. In our proposed algorithm, Optimize Disk Scheduling Algorithm
(ODSA) is taking less average seek time and transfer time as compare to other
disk scheduling algorithms (FIFO, SSTF, SCAN, C-SCAN, LOOK, etc.), which
enhances the efficiency of the disk performance in a better manner.
</summary>
    <author>
      <name>Sourav Kumar Bhoi</name>
    </author>
    <author>
      <name>Sanjaya Kumar Panda</name>
    </author>
    <author>
      <name>Imran Hossain Faruk</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/5010-7329 10.5120/5010-7329 10.5120/5010-7329 10.5120/5010-7329
  10.5120/5010-7329</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/5010-7329" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.5120/5010-7329" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.5120/5010-7329" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.5120/5010-7329" rel="related"/>
    <link title="doi" href="http://dx.doi.org/" rel="related"/>
    <link title="doi" href="http://dx.doi.org/" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.5120/5010-7329" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 26 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1403.0334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.0334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.0335v1</id>
    <updated>2014-03-03T08:27:29Z</updated>
    <published>2014-03-03T08:27:29Z</published>
    <title>A Group based Time Quantum Round Robin Algorithm using Min-Max Spread
  Measure</title>
    <summary>  Round Robin (RR) Scheduling is the basis of time sharing environment. It is
the combination of First Come First Served (FCFS) scheduling algorithm and
preemption among processes. It is basically used in a time sharing operating
system. It switches from one process to another process in a time interval. The
time interval or Time Quantum (TQ) is fixed for all available processes. So,
the larger process suffers from Context Switches (CS). To increase efficiency,
we have to select different TQ for processes. The main objective of RR is to
reduce the CS, maximize the utilization of CPU and minimize the turn around and
the waiting time. In this paper, we have considered different TQ for a group of
processes. It reduces CS as well as enhancing the performance of RR algorithm.
TQ can be calculated using min-max dispersion measure. Our experimental
analysis shows that Group Based Time Quantum (GBTQ) RR algorithm performs
better than existing RR algorithm with respect to Average Turn Around Time
(ATAT), Average Waiting Time (AWT) and CS.
</summary>
    <author>
      <name>Sanjaya Kumar Panda</name>
    </author>
    <author>
      <name>Debasis Dash</name>
    </author>
    <author>
      <name>Jitendra Kumar Rout</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/10667-5445</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/10667-5445" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 16 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1403.0335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.0335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.5976v1</id>
    <updated>2014-03-21T05:31:33Z</updated>
    <published>2014-03-21T05:31:33Z</published>
    <title>File System Design Approaches</title>
    <summary>  In this article, the file system development design approaches are discussed.
The selection of the file system design approach is done according to the needs
of the developers what are the needed requirements and specifications for the
new design. It allowed us to identify where our proposal fitted in with
relation to current and past file system development. Our experience with file
system development is limited so the research served to identify the different
techniques that can be used. The variety of file systems encountered show what
an active area of research file system development is. The file systems may be
from one of the two fundamental categories. In one category, the file system is
developed in user space and runs as a user process. Another file system may be
developed in the kernel space and runs as a privileged process. Another one is
the mixed approach in which we can take the advantages of both aforesaid
approaches. Each development option has its own pros and cons. In this article,
these design approaches are discussed.
</summary>
    <author>
      <name>Brijender Kahanwal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advances in Engineering Sciences,2014,
  Vol. 4(1), PP 16-20</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1403.5976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.5976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.0088v1</id>
    <updated>2014-04-01T00:40:01Z</updated>
    <published>2014-04-01T00:40:01Z</published>
    <title>Toward Parametric Timed Interfaces for Real-Time Components</title>
    <summary>  We propose here a framework to model real-time components consisting of
concurrent real-time tasks running on a single processor, using parametric
timed automata. Our framework is generic and modular, so as to be easily
adapted to different schedulers and more complex task models. We first perform
a parametric schedulability analysis of the components using the inverse
method. We show that the method unfortunately does not provide satisfactory
results when the task periods are consid- ered as parameters. After identifying
and explaining the problem, we present a solution adapting the model by making
use of the worst-case scenario in schedulability analysis. We show that the
analysis with the inverse method always converges on the modified model when
the system load is strictly less than 100%. Finally, we show how to use our
parametric analysis for the generation of timed interfaces in compositional
system design.
</summary>
    <author>
      <name>Youcheng Sun</name>
    </author>
    <author>
      <name>Giuseppe Lipari</name>
    </author>
    <author>
      <name>Étienne André</name>
    </author>
    <author>
      <name>Laurent Fribourg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.145.6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.145.6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings SynCoP 2014, arXiv:1403.7841</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 145, 2014, pp. 49-64</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.0088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.0088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5869v1</id>
    <updated>2014-04-23T15:55:50Z</updated>
    <published>2014-04-23T15:55:50Z</published>
    <title>An Effective Round Robin Algorithm using Min-Max Dispersion Measure</title>
    <summary>  Round Robin (RR) scheduling algorithm is a preemptive scheduling algorithm.
It is designed especially for time sharing Operating System (OS). In RR
scheduling algorithm the CPU switches between the processes when the static
Time Quantum (TQ) expires. RR scheduling algorithm is considered as the most
widely used scheduling algorithm in research because the TQ is equally shared
among the processes. In this paper a newly proposed variant of RR algorithm
called Min-Max Round Robin (MMRR) scheduling algorithm is presented. The idea
of this MMRR is to make the TQ repeatedly adjusted using Min-Max dispersion
measure in accordance with remaining CPU burst time. Our experimental analysis
shows that MMRR performs much better than RR algorithm in terms of average
turnaround time, average waiting time and number of context switches.
</summary>
    <author>
      <name>Sanjaya Kumar Panda</name>
    </author>
    <author>
      <name>Sourav Kumar Bhoi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 15 figures. International Journal on Computer Science and
  Engineering (IJCSE), 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.5869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.6087v1</id>
    <updated>2014-04-24T11:06:06Z</updated>
    <published>2014-04-24T11:06:06Z</published>
    <title>Enhancing CPU Performance using Subcontrary Mean Dynamic Round Robin
  (SMDRR) Scheduling Algorithm</title>
    <summary>  Round Robin (RR) Algorithm is considered as optimal in time shared
environment because the static time is equally shared among the processes. If
the time quantum taken is static then it undergoes degradation of the CPU
performance and leads to so many context switches. In this paper, we have
proposed a new effective dynamic RR algorithm SMDRR (Subcontrary Mean Dynamic
Round Robin) based on dynamic time quantum where we use the subcontrary mean or
harmonic mean to find the time quantum. The idea of this approach is to make
the time quantum repeatedly adjusted according to the burst time of the
currently running processes. Our experimental analysis shows that SMDRR
performs better than RR algorithm in terms of reducing the number of context
switches, average turnaround time and average waiting time.
</summary>
    <author>
      <name>Sourav Kumar Bhoi</name>
    </author>
    <author>
      <name>Sanjaya Kumar Panda</name>
    </author>
    <author>
      <name>Debashee Tarai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 13 figures. Journal of Global Research in Computer Science
  2011. arXiv admin note: text overlap with arXiv:1103.3832 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.6087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.6087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.2912v1</id>
    <updated>2014-05-12T16:41:23Z</updated>
    <published>2014-05-12T16:41:23Z</published>
    <title>Heterogeneity-aware Fault Tolerance using a Self-Organizing Runtime
  System</title>
    <summary>  Due to the diversity and implicit redundancy in terms of processing units and
compute kernels, off-the-shelf heterogeneous systems offer the opportunity to
detect and tolerate faults during task execution in hardware as well as in
software. To automatically leverage this diversity, we introduce an extension
of an online-learning runtime system that combines the benefits of the existing
performance-oriented task mapping with task duplication, a diversity-oriented
mapping strategy and heterogeneity-aware majority voter. This extension uses a
new metric to dynamically rate the remaining benefit of unreliable processing
units and a memory management mechanism for automatic data transfers and
checkpointing in the host and device memories.
</summary>
    <author>
      <name>Mario Kicherer</name>
    </author>
    <author>
      <name>Wolfgang Karl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.2912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.2912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.5651v1</id>
    <updated>2014-05-22T07:52:45Z</updated>
    <published>2014-05-22T07:52:45Z</published>
    <title>Hello rootKitty: A lightweight invariance-enforcing framework</title>
    <summary>  In monolithic operating systems, the kernel is the piece of code that
executes with the highest privileges and has control over all the software
running on a host. A successful attack against an operating system's kernel
means a total and complete compromise of the running system. These attacks
usually end with the installation of a rootkit, a stealthy piece of software
running with kernel privileges. When a rootkit is present, no guarantees can be
made about the correctness, privacy or isolation of the operating system.
  In this paper we present \emph{Hello rootKitty}, an invariance-enforcing
framework which takes advantage of current virtualization technology to protect
a guest operating system against rootkits. \emph{Hello rootKitty} uses the idea
of invariance to detect maliciously modified kernel data structures and restore
them to their original legitimate values. Our prototype has negligible
performance and memory overhead while effectively protecting commodity
operating systems from modern rootkits.
</summary>
    <author>
      <name>Francesco Gadaleta</name>
    </author>
    <author>
      <name>Nick Nikiforakis</name>
    </author>
    <author>
      <name>Yves Younan</name>
    </author>
    <author>
      <name>Wouter Joosen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, ISC Information Security Conference, Xi'an China, 2011,
  Springer</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.5651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7322v1</id>
    <updated>2014-05-28T18:28:45Z</updated>
    <published>2014-05-28T18:28:45Z</published>
    <title>Supporting Soft Real-Time Sporadic Task Systems on Heterogeneous
  Multiprocessors with No Utilization Loss</title>
    <summary>  Heterogeneous multicore architectures are becoming increasingly popular due
to their potential of achieving high performance and energy efficiency compared
to the homogeneous multicore architectures. In such systems, the real-time
scheduling problem becomes more challenging in that processors have different
speeds. A job executing on a processor with speed $x$ for $t$ time units
completes $(x \cdot t)$ units of execution. Prior research on heterogeneous
multiprocessor real-time scheduling has focused on hard real-time systems,
where, significant processing capacity may have to be sacrificed in the
worst-case to ensure that all deadlines are met. As meeting hard deadlines is
overkill for many soft real-time systems in practice, this paper shows that on
soft real-time heterogeneous multiprocessors, bounded response times can be
ensured for globally-scheduled sporadic task systems with no utilization loss.
A GEDF-based scheduling algorithm, namely GEDF-H, is presented and response
time bounds are established under both preemptive and non-preemptive GEDF-H
scheduling. Extensive experiments show that the magnitude of the derived
response time bound is reasonable, often smaller than three task periods. To
the best of our knowledge, this paper is the first to show that soft real-time
sporadic task systems can be supported on heterogeneous multiprocessors without
utilization loss, and with reasonable predicted response time.
</summary>
    <author>
      <name>Guangmo Tong</name>
    </author>
    <author>
      <name>Cong Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1405.7322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0122v1</id>
    <updated>2014-07-01T07:23:10Z</updated>
    <published>2014-07-01T07:23:10Z</published>
    <title>Effects of Hard Real-Time Constraints in Implementing the Myopic
  Scheduling Algorithm</title>
    <summary>  Myopic is a hard real-time process scheduling algorithm that selects a
suitable process based on a heuristic function from a subset (Window)of all
ready processes instead of choosing from all available processes, like original
heuristic scheduling algorithm. Performance of the algorithm significantly
depends on the chosen heuristic function that assigns weight to different
parameters like deadline, earliest starting time, processing time etc. and the
sizeof the Window since it considers only k processes from n processes (where,
k&lt;= n). This research evaluates the performance of the Myopic algorithm for
different parameters to demonstrate the merits and constraints of the
algorithm. A comparative performance of the impact of window size in
implementing the Myopic algorithm is presented and discussed through a set of
experiments.
</summary>
    <author>
      <name>Kazi Sakib</name>
    </author>
    <author>
      <name>M. S. Hasan</name>
    </author>
    <author>
      <name>M. A. Hossain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, Journal of Computer Science (JCS), Bangladesh, Vol. 1, No.
  2, December, 2007</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.0122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.3777v1</id>
    <updated>2014-11-14T02:30:08Z</updated>
    <published>2014-11-14T02:30:08Z</published>
    <title>Glider: A GPU Library Driver for Improved System Security</title>
    <summary>  Legacy device drivers implement both device resource management and
isolation. This results in a large code base with a wide high-level interface
making the driver vulnerable to security attacks. This is particularly
problematic for increasingly popular accelerators like GPUs that have large,
complex drivers. We solve this problem with library drivers, a new driver
architecture. A library driver implements resource management as an untrusted
library in the application process address space, and implements isolation as a
kernel module that is smaller and has a narrower lower-level interface (i.e.,
closer to hardware) than a legacy driver. We articulate a set of device and
platform hardware properties that are required to retrofit a legacy driver into
a library driver. To demonstrate the feasibility and superiority of library
drivers, we present Glider, a library driver implementation for two GPUs of
popular brands, Radeon and Intel. Glider reduces the TCB size and attack
surface by about 35% and 84% respectively for a Radeon HD 6450 GPU and by about
38% and 90% respectively for an Intel Ivy Bridge GPU. Moreover, it incurs no
performance cost. Indeed, Glider outperforms a legacy driver for applications
requiring intensive interactions with the device driver, such as applications
using the OpenGL immediate mode API.
</summary>
    <author>
      <name>Ardalan Amiri Sani</name>
    </author>
    <author>
      <name>Lin Zhong</name>
    </author>
    <author>
      <name>Dan S. Wallach</name>
    </author>
    <link href="http://arxiv.org/abs/1411.3777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.01370v1</id>
    <updated>2015-01-07T05:51:19Z</updated>
    <published>2015-01-07T05:51:19Z</published>
    <title>A Case Study: Task Scheduling Methodologies for High Speed Computing
  Systems</title>
    <summary>  High Speed computing meets ever increasing real-time computational demands
through the leveraging of flexibility and parallelism. The flexibility is
achieved when computing platform designed with heterogeneous resources to
support multifarious tasks of an application where as task scheduling brings
parallel processing. The efficient task scheduling is critical to obtain
optimized performance in heterogeneous computing Systems (HCS). In this paper,
we brought a review of various application scheduling models which provide
parallelism for homogeneous and heterogeneous computing systems. In this paper,
we made a review of various scheduling methodologies targeted to high speed
computing systems and also prepared summary chart. The comparative study of
scheduling methodologies for high speed computing systems has been carried out
based on the attributes of platform &amp; application as well. The attributes are
execution time, nature of task, task handling capability, type of host &amp;
computing platform. Finally a summary chart has been prepared and it
demonstrates that the need of developing scheduling methodologies for
Heterogeneous Reconfigurable Computing Systems (HRCS) which is an emerging high
speed computing platform for real time applications.
</summary>
    <author>
      <name>Mahendra Vucha</name>
    </author>
    <author>
      <name>Arvind Rajawat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijesa.2014.4401</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijesa.2014.4401" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.01370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.01370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02287v1</id>
    <updated>2015-02-08T19:09:36Z</updated>
    <published>2015-02-08T19:09:36Z</published>
    <title>Protecting Memory-Performance Critical Sections in Soft Real-Time
  Applications</title>
    <summary>  Soft real-time applications such as multimedia applications often show bursty
memory access patterns---regularly requiring a high memory bandwidth for a
short duration of time. Such a period is often critical for timely data
processing. Hence, we call it a memory-performance critical section.
Unfortunately, in multicore architecture, non-real-time applications on
different cores may also demand high memory bandwidth at the same time, which
can substantially increase the time spent on the memory performance critical
sections.
  In this paper, we present BWLOCK, user-level APIs and a memory bandwidth
control mechanism that can protect such memory performance critical sections of
soft real-time applications. BWLOCK provides simple lock like APIs to declare
memory-performance critical sections. If an application enters a
memory-performance critical section, the memory bandwidth control system then
dynamically limit other cores' memory access rates to protect memory
performance of the application until the critical section finishes.
  From case studies with real-world soft real-time applications, we found (1)
such memory-performance critical sections do exist and are often easy to
identify; and (2) applying BWLOCK for memory critical sections significantly
improve performance of the soft real-time applications at a small or no cost in
throughput of non real-time applications.
</summary>
    <author>
      <name>Heechul Yun</name>
    </author>
    <author>
      <name>Santosh Gondi</name>
    </author>
    <author>
      <name>Siddhartha Biswas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.02287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.06833v1</id>
    <updated>2015-04-26T14:44:00Z</updated>
    <published>2015-04-26T14:44:00Z</published>
    <title>Evaluating Dynamic File Striping For Lustre</title>
    <summary>  We define dynamic striping as the ability to assign different Lustre striping
characteristics to contiguous segments of a file as it grows. In this paper, we
evaluate the effects of dynamic striping using a watermark-based strategy where
the stripe count or width is increased once a file's size exceeds one of the
chosen watermarks. To measure the performance of this strategy we used a
modified version of the IOR benchmark, a netflow analysis workload, and the
blastn algorithm from NCBI BLAST. The results indicate that dynamic striping is
beneficial to tasks with unpredictable data file size and large sequential
reads, but are less conclusive for workloads with significant random read
phases.
</summary>
    <author>
      <name>Joel Reed</name>
    </author>
    <author>
      <name>Jeremy Archuleta</name>
    </author>
    <author>
      <name>Michael J. Brim</name>
    </author>
    <author>
      <name>Joshua Lothian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.06833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.07070v2</id>
    <updated>2016-05-30T13:35:25Z</updated>
    <published>2015-04-27T13:08:26Z</published>
    <title>Deterministically Deterring Timing Attacks in Deterland</title>
    <summary>  The massive parallelism and resource sharing embodying today's cloud business
model not only exacerbate the security challenge of timing channels, but also
undermine the viability of defenses based on resource partitioning. We propose
hypervisor-enforced timing mitigation to control timing channels in cloud
environments. This approach closes "reference clocks" internal to the cloud by
imposing a deterministic view of time on guest code, and uses timing mitigators
to pace I/O and rate-limit potential information leakage to external observers.
Our prototype hypervisor is the first system to mitigate timing-channel leakage
across full-scale existing operating systems such as Linux and applications in
arbitrary languages. Mitigation incurs a varying performance cost, depending on
workload and tunable leakage-limiting parameters, but this cost may be
justified for security-critical cloud applications and data.
</summary>
    <author>
      <name>Weiyi Wu</name>
    </author>
    <author>
      <name>Bryan Ford</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.07070v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07070v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.05269v1</id>
    <updated>2015-05-20T07:47:20Z</updated>
    <published>2015-05-20T07:47:20Z</published>
    <title>A Survey Report on Operating Systems for Tiny Networked Sensors</title>
    <summary>  Wireless sensor network (WSN) has attracted researchers worldwide to explore
the research opportunities, with application mainly in health monitoring,
industry automation, battlefields, home automation and environmental
monitoring. A WSN is highly resource constrained in terms of energy,
computation and memory. WSNs deployment ranges from the normal working
environment up to hostile and hazardous environment such as in volcano
monitoring and underground mines. These characteristics of WSNs hold additional
set of challenges in front of the operating system designer. The objective of
this survey is to highlight the features and weakness of the opearting system
available for WSNs, with the focus on the current application demands. The
paper also discusses the operating system design issues in terms of
architecture, programming model, scheduling and memory management and support
for real time applications.
</summary>
    <author>
      <name>Alok Ranjan</name>
    </author>
    <author>
      <name>H. B. Sahu</name>
    </author>
    <author>
      <name>Prasant Misra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, Submitted to Journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Advanced Research in Networking and Communication
  Engineering, Vol(1) issue 1, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1505.05269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.05269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.01449v4</id>
    <updated>2016-06-29T20:42:04Z</updated>
    <published>2015-06-04T02:11:27Z</published>
    <title>Defending against malicious peripherals with Cinch</title>
    <summary>  Malicious peripherals designed to attack their host computers are a growing
problem. Inexpensive and powerful peripherals that attach to plug-and-play
buses have made such attacks easy to mount. Making matters worse, commodity
operating systems lack coherent defenses, and users are often unaware of the
scope of the problem. We present Cinch, a pragmatic response to this threat.
Cinch uses virtualization to attach peripheral devices to a logically separate,
untrusted machine, and includes an interposition layer between the untrusted
machine and the protected one. This layer regulates interaction with devices
according to user-configured policies. Cinch integrates with existing OSes,
enforces policies that thwart real-world attacks, and has low overhead.
</summary>
    <author>
      <name>Sebastian Angel</name>
    </author>
    <author>
      <name>Riad S. Wahby</name>
    </author>
    <author>
      <name>Max Howald</name>
    </author>
    <author>
      <name>Joshua B. Leners</name>
    </author>
    <author>
      <name>Michael Spilo</name>
    </author>
    <author>
      <name>Zhen Sun</name>
    </author>
    <author>
      <name>Andrew J. Blumberg</name>
    </author>
    <author>
      <name>Michael Walfish</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. USENIX Security (2016), 397--414</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.01449v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.01449v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06356v2</id>
    <updated>2015-10-18T04:50:29Z</updated>
    <published>2015-08-26T03:43:38Z</published>
    <title>EOS: Automatic In-vivo Evolution of Kernel Policies for Better
  Performance</title>
    <summary>  Today's monolithic kernels often implement a small, fixed set of policies
such as disk I/O scheduling policies, while exposing many parameters to let
users select a policy or adjust the specific setting of the policy. Ideally,
the parameters exposed should be flexible enough for users to tune for good
performance, but in practice, users lack domain knowledge of the parameters and
are often stuck with bad, default parameter settings.
  We present EOS, a system that bridges the knowledge gap between kernel
developers and users by automatically evolving the policies and parameters in
vivo on users' real, production workloads. It provides a simple policy
specification API for kernel developers to programmatically describe how the
policies and parameters should be tuned, a policy cache to make in-vivo tuning
easy and fast by memorizing good parameter settings for past workloads, and a
hierarchical search engine to effectively search the parameter space.
Evaluation of EOS on four main Linux subsystems shows that it is easy to use
and effectively improves each subsystem's performance.
</summary>
    <author>
      <name>Yan Cui</name>
    </author>
    <author>
      <name>Quan Chen</name>
    </author>
    <author>
      <name>Junfeng Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, technique report</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.06356v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06356v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06367v2</id>
    <updated>2016-09-22T10:38:29Z</updated>
    <published>2015-08-26T05:20:50Z</published>
    <title>A Software-only Mechanism for Device Passthrough and Sharing</title>
    <summary>  Network processing elements in virtual machines, also known as Network
Function Virtualization (NFV) often face CPU bottlenecks at the virtualization
interface. Even highly optimized paravirtual device interfaces fall short of
the throughput requirements of modern devices. Passthrough devices, together
with SR-IOV support for multiple device virtual functions (VF) and IOMMU
support, mitigate this problem somewhat, by allowing a VM to directly control a
device partition bypassing the virtualization stack. However, device
passthrough requires high-end (expensive and power-hungry) hardware, places
scalability limits on consolidation ratios, and does not support efficient
switching between multiple VMs on the same host.
  We present a paravirtual interface that securely exposes an I/O device
directly to the guest OS running inside the VM, and yet allows that device to
be securely shared among multiple VMs and the host. Compared to the best-known
paravirtualization interfaces, our paravirtual interface supports up to 2x
higher throughput, and is closer in performance to device passthrough. Unlike
device passthrough however, we do not require SR-IOV or IOMMU support, and
allow fine-grained dynamic resource allocation, significantly higher
consolidation ratios, and seamless VM migration. Our security mechanism is
based on a novel approach called dynamic binary opcode subtraction.
</summary>
    <author>
      <name>Piyus Kedia</name>
    </author>
    <author>
      <name>Sorav Bansal</name>
    </author>
    <link href="http://arxiv.org/abs/1508.06367v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06367v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.02498v1</id>
    <updated>2015-11-08T15:44:59Z</updated>
    <published>2015-11-08T15:44:59Z</published>
    <title>Characteristic specific prioritized dynamic average burst round robin
  scheduling for uniprocessor and multiprocessor environment</title>
    <summary>  CPU scheduling is one of the most crucial operations performed by operating
systems. Different conventional algorithms like FCFS, SJF, Priority, and RR
(Round Robin) are available for CPU Scheduling. The effectiveness of Priority
and Round Robin scheduling algorithm completely depends on selection of
priority features of processes and on the choice of time quantum. In this paper
a new CPU scheduling algorithm has been proposed, named as CSPDABRR
(Characteristic specific Prioritized Dynamic Average Burst Round Robin), that
uses seven priority features for calculating priority of processes and uses
dynamic time quantum instead of static time quantum used in RR. The performance
of the proposed algorithm is experimentally compared with traditional RR and
Priority scheduling algorithm in both uni-processor and multi-processor
environment. The results of our approach presented in this paper demonstrate
improved performance in terms of average waiting time, average turnaround time,
and optimal priority feature.
</summary>
    <author>
      <name>Amar Ranjan Dash</name>
    </author>
    <author>
      <name>Sandipta Kumar Sahu</name>
    </author>
    <author>
      <name>Sanjay Kumar Samantra</name>
    </author>
    <author>
      <name>Sradhanjali Sabat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsea.2015.5501</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsea.2015.5501" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 Pages, 10 Figures, 18 Tables, 20 References, International Journal
  of Computer Science, Engineering and Applications (IJCSEA) Vol.5, No.4/5,
  October 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.02498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.02498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.00727v2</id>
    <updated>2015-12-03T10:38:11Z</updated>
    <published>2015-12-02T15:05:46Z</published>
    <title>TinyLFU: A Highly Efficient Cache Admission Policy</title>
    <summary>  This paper proposes to use a frequency based cache admission policy in order
to boost the effectiveness of caches subject to skewed access distributions.
Given a newly accessed item and an eviction candidate from the cache, our
scheme decides, based on the recent access history, whether it is worth
admitting the new item into the cache at the expense of the eviction candidate.
  Realizing this concept is enabled through a novel approximate LFU structure
called TinyLFU, which maintains an approximate representation of the access
frequency of a large sample of recently accessed items. TinyLFU is very compact
and light-weight as it builds upon Bloom filter theory.
  We study the properties of TinyLFU through simulations of both synthetic
workloads as well as multiple real traces from several sources. These
simulations demonstrate the performance boost obtained by enhancing various
replacement policies with the TinyLFU eviction policy. Also, a new combined
replacement and eviction policy scheme nicknamed W-TinyLFU is presented.
W-TinyLFU is demonstrated to obtain equal or better hit-ratios than other state
of the art replacement policies on these traces. It is the only scheme to
obtain such good results on all traces.
</summary>
    <author>
      <name>Gil Einziger</name>
    </author>
    <author>
      <name>Roy Friedman</name>
    </author>
    <author>
      <name>Ben Manes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A much earlier and shorter version of this work appeared in the
  Euromicro PDP 2014 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.00727v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.00727v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01978v1</id>
    <updated>2015-12-07T11:05:05Z</updated>
    <published>2015-12-07T11:05:05Z</published>
    <title>Real-Time scheduling: from hard to soft real-time systems</title>
    <summary>  Real-time systems are traditionally classified into hard real-time and soft
real-time: in the first category we have safety critical real-time systems
where missing a deadline can have catastrophic consequences, whereas in the
second class we find systems or which we need to optimise the Quality of
service provided to the user. However, the frontier between these two classes
is thinner than one may think, and many systems that were considered as hard
real-time in the past should now be reconsidered under a different light. In
this paper we shall first recall the fundamental notion of time-predictability
and criticality, in order to understand where the real-time deadlines that we
use in our theoretical models come from. We shall then introduce the model of a
soft real-time system and present one popular method for scheduling hard and
soft real-time tasks, the resource reservation framework. Finally, we shall
show how resource reservation techniques can be successfully applied to the
design of classical control systems, thus adding robustness to the system and
increasing resource utilisation and performance.
</summary>
    <author>
      <name>Giuseppe Lipari</name>
    </author>
    <author>
      <name>Luigi Palopoli</name>
    </author>
    <link href="http://arxiv.org/abs/1512.01978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.07351v1</id>
    <updated>2015-12-23T04:39:00Z</updated>
    <published>2015-12-23T04:39:00Z</published>
    <title>Energy-aware Fixed-Priority Multi-core Scheduling for Real-time Systems</title>
    <summary>  Multi-core processors are becoming more and more popular in embedded and
real-time systems. While fixed-priority scheduling with task-splitting in
real-time systems are widely applied, current approaches have not taken into
consideration energy-aware aspects such as dynamic voltage/frequency scheduling
(DVS). In this paper, we propose two strategies to apply dynamic voltage
scaling (DVS) to fixed-priority scheduling algorithms with task-splitting for
periodic real-time tasks on multi-core processors. The first strategy
determines voltage scales for each processor after scheduling (Static DVS),
which ensures all tasks meet the timing requirements on synchronization. The
second strategy adaptively determines the frequency of each task before
scheduling (Adaptive DVS) according to the total utilization of task-set and
number of cores available. The combination of frequency pre-allocation and
task-splitting makes it possible to maximize energy savings with DVS.
Simulation results show that it is possible to achieve significant energy
savings with DVS while preserving the schedulability requirements of real-time
schedulers for multi-core processors.
</summary>
    <author>
      <name>Yao Guo</name>
    </author>
    <author>
      <name>Junyang Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">conference extension</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.07351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.07351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.05851v1</id>
    <updated>2016-01-22T01:22:53Z</updated>
    <published>2016-01-22T01:22:53Z</published>
    <title>HyBIS: Windows Guest Protection through Advanced Memory Introspection</title>
    <summary>  Effectively protecting the Windows OS is a challenging task, since most
implementation details are not publicly known. Windows has always been the main
target of malwares that have exploited numerous bugs and vulnerabilities.
Recent trusted boot and additional integrity checks have rendered the Windows
OS less vulnerable to kernel-level rootkits. Nevertheless, guest Windows
Virtual Machines are becoming an increasingly interesting attack target. In
this work we introduce and analyze a novel Hypervisor-Based Introspection
System (HyBIS) we developed for protecting Windows OSes from malware and
rootkits. The HyBIS architecture is motivated and detailed, while targeted
experimental results show its effectiveness. Comparison with related work
highlights main HyBIS advantages such as: effective semantic introspection,
support for 64-bit architectures and for latest Windows (8.x and 10), advanced
malware disabling capabilities. We believe the research effort reported here
will pave the way to further advances in the security of Windows OSes.
</summary>
    <author>
      <name>Roberto di Pietro</name>
    </author>
    <author>
      <name>Federico Franzoni</name>
    </author>
    <author>
      <name>Flavio Lombardi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-58469-0_13</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-58469-0_13" rel="related"/>
    <link href="http://arxiv.org/abs/1601.05851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.05851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.05636v1</id>
    <updated>2016-03-17T19:43:36Z</updated>
    <published>2016-03-17T19:43:36Z</published>
    <title>An Implementation and Analysis of a Kernel Network Stack in Go with the
  CSP Style</title>
    <summary>  Modern operating system kernels are written in lower-level languages such as
C. Although the low-level functionalities of C are often useful within kernels,
they also give rise to several classes of bugs. Kernels written in higher level
languages avoid many of these potential problems, at the possible cost of
decreased performance. This research evaluates the advantages and disadvantages
of a kernel written in a higher level language. To do this, the network stack
subsystem of the kernel was implemented in Go with the Communicating Sequential
Processes (CSP) style. Go is a high-level programming language that supports
the CSP style, which recommends splitting large tasks into several smaller ones
running in independent "threads". Modules for the major networking protocols,
including Ethernet, ARP, IPv4, ICMP, UDP, and TCP, were implemented. In this
study, the implemented Go network stack, called GoNet, was compared to a
representative network stack written in C. The GoNet code is more readable and
generally performs better than that of its C stack counterparts. From this, it
can be concluded that Go with CSP style is a viable alternative to C for the
language of kernel implementations.
</summary>
    <author>
      <name>Harshal Sheth</name>
    </author>
    <author>
      <name>Aashish Welling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.05636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.05636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00362v1</id>
    <updated>2016-05-02T06:24:43Z</updated>
    <published>2016-05-02T06:24:43Z</published>
    <title>An optimized round robin cpu scheduling algorithm with dynamic time
  quantum</title>
    <summary>  CPU scheduling is one of the most crucial operations performed by operating
system. Different algorithms are available for CPU scheduling amongst them RR
(Round Robin) is considered as optimal in time shared environment. The
effectiveness of Round Robin completely depends on the choice of time quantum.
In this paper a new CPU scheduling algorithm has been proposed, named as DABRR
(Dynamic Average Burst Round Robin). That uses dynamic time quantum instead of
static time quantum used in RR. The performance of the proposed algorithm is
experimentally compared with traditional RR and some existing variants of RR.
The results of our approach presented in this paper demonstrate improved
performance in terms of average waiting time, average turnaround time, and
context switching.
</summary>
    <author>
      <name>Amar Ranjan Dash</name>
    </author>
    <author>
      <name>Sandipta kumar Sahu</name>
    </author>
    <author>
      <name>Sanjay Kumar Samantra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcseit.2015.5102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcseit.2015.5102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 7 figures, 16 Tables. arXiv admin note: text overlap with
  arXiv:1511.02498</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol. 5,No.1, February 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.00362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01168v1</id>
    <updated>2016-05-04T07:36:46Z</updated>
    <published>2016-05-04T07:36:46Z</published>
    <title>A Qualitative Comparison of MPSoC Mobile and Embedded Virtualization
  Techniques</title>
    <summary>  Virtualization is generally adopted in server and desktop environments to
provide for fault tolerance, resource management, and energy efficiency.
Virtualization enables parallel execution of multiple operating systems (OSs)
while sharing the hardware resources. Virtualization was previously not deemed
as feasible technology for mobile and embedded devices due to their limited
processing and memory resource. However, the enterprises are advocating Bring
Your Own Device (BYOD) applications that enable co-existence of heterogeneous
OSs on a single mobile device. Moreover, embedded device require virtualization
for logical isolation of secure and general purpose OSs on a single device. In
this paper, we investigate the processor architectures in the mobile and
embedded space while examining their formal visualizability. We also compare
the virtualization solutions enabling coexistence of multiple OSs in Multicore
Processor System-on-Chip (MPSoC) mobile and embedded systems. We advocate that
virtualization is necessary to manage resource in MPSoC designs and to enable
BYOD, security, and logical isolation use cases.
</summary>
    <author>
      <name>Junaid Shuja</name>
    </author>
    <author>
      <name>Abdullah Gani</name>
    </author>
    <author>
      <name>Sajjad A. Madani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference of Global Network for Innovative Technology
  (IGNITE-2014), Penang, Malaysia</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02635v1</id>
    <updated>2016-06-08T16:52:03Z</updated>
    <published>2016-06-08T16:52:03Z</published>
    <title>Feedback Scheduling for Energy-Efficient Real-Time Homogeneous
  Multiprocessor Systems</title>
    <summary>  Real-time scheduling algorithms proposed in the literature are often based on
worst-case estimates of task parameters. The performance of an open-loop scheme
can be degraded significantly if there are uncertainties in task parameters,
such as the execution times of the tasks. Therefore, to cope with such a
situation, a closed-loop scheme, where feedback is exploited to adjust the
system parameters, can be applied. We propose an optimal control framework that
takes advantage of feeding back information of finished tasks to solve a
real-time multiprocessor scheduling problem with uncertainty in task execution
times, with the objective of minimizing the total energy consumption.
Specifically, we propose a linear programming based algorithm to solve a
workload partitioning problem and adopt McNaughton's wrap around algorithm to
find the task execution order. The simulation results illustrate that our
feedback scheduling algorithm can save energy by as much as 40% compared to an
open-loop method for two processor models, i.e. a PowerPC 405LP and an XScale
processor.
</summary>
    <author>
      <name>Mason Thammawichai</name>
    </author>
    <author>
      <name>Eric C. Kerrigan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CDC.2016.7798501</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CDC.2016.7798501" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 55th IEEE Conference on Decision and Control, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.02635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05654v1</id>
    <updated>2016-08-19T16:23:24Z</updated>
    <published>2016-08-19T16:23:24Z</published>
    <title>POLYPATH: Supporting Multiple Tradeoffs for Interaction Latency</title>
    <summary>  Modern mobile systems use a single input-to-display path to serve all
applications. In meeting the visual goals of all applications, the path has a
latency inadequate for many important interactions. To accommodate the
different latency requirements and visual constraints by different
interactions, we present POLYPATH, a system design in which application
developers (and users) can choose from multiple path designs for their
application at any time. Because a POLYPATH system asks for two or more path
designs, we present a novel fast path design, called Presto. Presto reduces
latency by judiciously allowing frame drops and tearing.
  We report an Android 5-based prototype of POLYPATH with two path designs:
Android legacy and Presto. Using this prototype, we quantify the effectiveness,
overhead, and user experience of POLYPATH, especially Presto, through both
objective measurements and subjective user assessment. We show that Presto
reduces the latency of legacy touchscreen drawing applications by almost half;
and more importantly, this reduction is orthogonal to that of other popular
approaches and is achieved without any user-noticeable negative visual effect.
When combined with touch prediction, Presto is able to reduce the touch latency
below 10 ms, a remarkable achievement without any hardware support.
</summary>
    <author>
      <name>Min Hong Yun</name>
    </author>
    <author>
      <name>Songtao He</name>
    </author>
    <author>
      <name>Lin Zhong</name>
    </author>
    <link href="http://arxiv.org/abs/1608.05654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08051v3</id>
    <updated>2020-04-09T16:32:50Z</updated>
    <published>2016-08-13T22:14:47Z</published>
    <title>Duplication of Windows Services</title>
    <summary>  OS-level virtualization techniques virtualize system resources at the system
call interface, has the distinct advantage of smaller run-time resource
requirements as compared to HAL-level virtualization techniques, and thus forms
an important building block for virtualizing parallel and distributed
applications such as a HPC clusters. Because the Windows operating system puts
certain critical functionalities in privileged user-level system service
processes, a complete OS-level virtualization solution for the Windows platform
requires duplication of such Windows service as Remote Procedure Call Server
Service (RPCSS). As many implementation details of the Windows system services
are proprietary, duplicating Windows system services becomes the key technical
challenge for virtualizing the Windows platform at the OS level. Moreover, as a
core component of cloud computing, IIS web server-related services need to be
duplicated in containers (i.e., OS-level virtual machines), but so far there is
no such scheme. In this paper, we thoroughly identify all issues that affect
service duplication, and then propose the first known methodology to
systematically duplicate both system and ordinary Windows services. Our
experiments show that the methodology can duplicate a set of system and
ordinary services on different versions of Windows OS.
</summary>
    <author>
      <name>Zhiyong Shan</name>
    </author>
    <author>
      <name>Xin Wang</name>
    </author>
    <author>
      <name>Tzi-cker Chiueh</name>
    </author>
    <author>
      <name>Rajiv Bagai</name>
    </author>
    <link href="http://arxiv.org/abs/1608.08051v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08051v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00875v1</id>
    <updated>2016-09-03T23:09:11Z</updated>
    <published>2016-09-03T23:09:11Z</published>
    <title>Compatible and Usable Mandatory Access Control for Good-enough OS
  Security</title>
    <summary>  OS compromise is one of the most serious computer security problems today,
but still not being resolved. Although people proposed different kinds of
methods, they could not be accepted by most users who are non-expert due to the
lack of compatibility and usability. In this paper, we introduce a kind of new
mandatory access control model, named CUMAC, that aims to achieve good-enough
security, high compatibility and usability. It has two novel features. One is
access control based on tracing potential intrusion that can reduce false
negatives and facilitate security configuration, in order to improve both
compatibility and usability; the other is automatically figuring out all of the
compatibility exceptions that usually incurs incompatible problems. The
experiments performed on the prototype show that CUMAC can defense attacks from
network, mobile disk and local untrustable users while keeping good
compatibility and usability.
</summary>
    <author>
      <name>Zhiyong Shan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISECS.2009.29</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISECS.2009.29" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Electronic Commerce and Security, 2009. ISECS '09. Second
  International Symposium on</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04781v1</id>
    <updated>2016-09-15T19:18:12Z</updated>
    <published>2016-09-15T19:18:12Z</published>
    <title>Confining Windows Inter-Process Communications for OS-Level Virtual
  Machine</title>
    <summary>  As OS-level virtualization technology usually imposes little overhead on
virtual machine start-up and running, it provides an excellent choice for
building intrusion/fault tolerant applications that require redundancy and
frequent invocation. When developing Windows OS-level virtual machine, however,
people will inevitably face the challenge of confining Windows Inter-Process
Communications (IPC). As IPC on Windows platform is more complex than UNIX
style OS and most of the programs on Windows are not open-source, it is
difficult to discover all of the performed IPCs and confine them. In this
paper, we propose three general principles to confine IPC on Windows OS and a
novel IPC confinement mechanism based on the principles. With the mechanism,
for the first time from the literature, we successfully virtualized RPC System
Service (RPCSS) and Internet Information Server (IIS) on Feather-weight Virtual
Machine (FVM). Experimental results demonstrate that multiple IIS web server
instances can simultaneously run on single Windows OS with much less
performance overhead than other popular VM technology, offering a good basis
for constructing dependable system.
</summary>
    <author>
      <name>Zhiyong Shan</name>
    </author>
    <author>
      <name>Yang Yu</name>
    </author>
    <author>
      <name>Tzi-cker Chiueh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VDTS '09 Proceedings of the 1st EuroSys Workshop on Virtualization
  Technology for Dependable Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04785v1</id>
    <updated>2016-09-15T19:21:22Z</updated>
    <published>2016-09-15T19:21:22Z</published>
    <title>Virtualizing System and Ordinary Services in Windows-based OS-Level
  Virtual Machines</title>
    <summary>  OS-level virtualization incurs smaller start-up and run-time overhead than
HAL-based virtualization and thus forms an important building block for
developing fault-tolerant and intrusion-tolerant applications. A complete
implementation of OS-level virtualization on the Windows platform requires
virtualization of Windows services, such as system services like the Remote
Procedure Call Server Service (RPCSS), because they are essentially extensions
of the kernel. As Windows system services work very differently from their
counterparts on UNIX-style OS, i.e., daemons, and many of their implementation
details are proprietary, virtualizing Windows system services turned out to be
the most challenging technical barrier for OS-level virtualization for the
Windows platform. In this paper, we describe a general technique to virtualize
Windows services, and demonstrate its effectiveness by applying it to
successfully virtualize a set of important Windows system services and ordinary
services on different versions of Windows OS, including RPCSS, DcomLaunch, IIS
service group, Tlntsvr, MySQL, Apache2.2, CiSvc, ImapiService, etc.
</summary>
    <author>
      <name>Zhiyong Shan</name>
    </author>
    <author>
      <name>Tzi-cker Chiueh</name>
    </author>
    <author>
      <name>Xin Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SAC '11 Proceedings of the 2011 ACM Symposium on Applied Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08372v2</id>
    <updated>2016-09-28T09:17:05Z</updated>
    <published>2016-09-27T12:21:02Z</published>
    <title>An Evaluation of Coarse-Grained Locking for Multicore Microkernels</title>
    <summary>  The trade-off between coarse- and fine-grained locking is a well understood
issue in operating systems. Coarse-grained locking provides lower overhead
under low contention, fine-grained locking provides higher scalability under
contention, though at the expense of implementation complexity and re- duced
best-case performance.
  We revisit this trade-off in the context of microkernels and tightly-coupled
cores with shared caches and low inter-core migration latencies. We evaluate
performance on two architectures: x86 and ARM MPCore, in the former case also
utilising transactional memory (Intel TSX). Our thesis is that on such
hardware, a well-designed microkernel, with short system calls, can take
advantage of coarse-grained locking on modern hardware, avoid the run-time and
complexity cost of multiple locks, enable formal verification, and still
achieve scalability comparable to fine-grained locking.
</summary>
    <author>
      <name>Kevin Elphinstone</name>
    </author>
    <author>
      <name>Amirreza Zarrabi</name>
    </author>
    <author>
      <name>Adrian Danis</name>
    </author>
    <author>
      <name>Yanyan Shen</name>
    </author>
    <author>
      <name>Gernot Heiser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures, 28 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.08372v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08372v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.1; D.4.7; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06830v1</id>
    <updated>2016-12-20T20:14:35Z</updated>
    <published>2016-12-20T20:14:35Z</published>
    <title>CannyFS: Opportunistically Maximizing I/O Throughput Exploiting the
  Transactional Nature of Batch-Mode Data Processing</title>
    <summary>  We introduce a user mode file system, CannyFS, that hides latency by assuming
all I/O operations will succeed. The user mode process will in turn report
errors, allowing proper cleanup and a repeated attempt to take place. We
demonstrate benefits for the model tasks of extracting archives and removing
directory trees in a real-life HPC environment, giving typical reductions in
time use of over 80%.
  This approach can be considered a view of HPC jobs and their I/O activity as
transactions. In general, file systems lack clearly defined transaction
semantics. Over time, the competing trends to add cache and maintain data
integrity have resulted in different practical tradeoffs.
  High-performance computing is a special case where overall throughput demands
are high. Latency can also be high, with non-local storage. In addition, a
theoretically possible I/O error (like permission denied, loss of connection,
exceeding disk quota) will frequently warrant the resubmission of a full job or
task, rather than traditional error reporting or handling. Therefore,
opportunistically treating each I/O operation as successful, and part of a
larger transaction, can speed up some applications that do not leverage
asynchronous I/O.
</summary>
    <author>
      <name>Jessica Nettelblad</name>
    </author>
    <author>
      <name>Carl Nettelblad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, 1 table. Submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.06830v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06830v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06571v1</id>
    <updated>2017-03-20T02:47:57Z</updated>
    <published>2017-03-20T02:47:57Z</published>
    <title>Formalizing Memory Accesses and Interrupts</title>
    <summary>  The hardware/software boundary in modern heterogeneous multicore computers is
increasingly complex, and diverse across different platforms. A single memory
access by a core or DMA engine traverses multiple hardware translation and
caching steps, and the destination memory cell or register often appears at
different physical addresses for different cores. Interrupts pass through a
complex topology of interrupt controllers and remappers before delivery to one
or more cores, each with specific constraints on their configurations. System
software must not only correctly understand the specific hardware at hand, but
also configure it appropriately at runtime. We propose a formal model of
address spaces and resources in a system that allows us to express and verify
invariants of the system's runtime configuration, and illustrate (and motivate)
it with several real platforms we have encountered in the process of OS
implementation.
</summary>
    <author>
      <name>Reto Achermann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Systems Group, Department of Computer Science, ETH Zurich</arxiv:affiliation>
    </author>
    <author>
      <name>Lukas Humbel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Systems Group, Department of Computer Science, ETH Zurich</arxiv:affiliation>
    </author>
    <author>
      <name>David Cock</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Systems Group, Department of Computer Science, ETH Zurich</arxiv:affiliation>
    </author>
    <author>
      <name>Timothy Roscoe</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Systems Group, Department of Computer Science, ETH Zurich</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.244.4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.244.4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings MARS 2017, arXiv:1703.05812</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 244, 2017, pp. 66-116</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1703.06571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4; D.4.2; D.4.7; D.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07725v1</id>
    <updated>2017-03-22T16:07:11Z</updated>
    <published>2017-03-22T16:07:11Z</published>
    <title>Memos: Revisiting Hybrid Memory Management in Modern Operating System</title>
    <summary>  The emerging hybrid DRAM-NVM architecture is challenging the existing memory
management mechanism in operating system. In this paper, we introduce memos,
which can schedule memory resources over the entire memory hierarchy including
cache, channels, main memory comprising DRAM and NVM simultaneously. Powered by
our newly designed kernel-level monitoring module and page migration engine,
memos can dynamically optimize the data placement at the memory hierarchy in
terms of the on-line memory patterns, current resource utilization and feature
of memory medium. Our experimental results show that memos can achieve high
memory utilization, contributing to system throughput by 19.1% and QoS by 23.6%
on average. Moreover, memos can reduce the NVM side memory latency by 3~83.3%,
energy consumption by 25.1~99%, and benefit the NVM lifetime significantly (40X
improvement on average).
</summary>
    <author>
      <name>Lei Liu</name>
    </author>
    <author>
      <name>Mengyao Xie</name>
    </author>
    <author>
      <name>Hao Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1703.07725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.06932v1</id>
    <updated>2017-05-19T11:01:54Z</updated>
    <published>2017-05-19T11:01:54Z</published>
    <title>Look Mum, no VM Exits! (Almost)</title>
    <summary>  Multi-core CPUs are a standard component in many modern embedded systems.
Their virtualisation extensions enable the isolation of services, and gain
popularity to implement mixed-criticality or otherwise split systems. We
present Jailhouse, a Linux-based, OS-agnostic partitioning hypervisor that uses
novel architectural approaches to combine Linux, a powerful general-purpose
system, with strictly isolated special-purpose components. Our design goals
favour simplicity over features, establish a minimal code base, and minimise
hypervisor activity.
  Direct assignment of hardware to guests, together with a deferred
initialisation scheme, offloads any complex hardware handling and bootstrapping
issues from the hypervisor to the general purpose OS. The hypervisor
establishes isolated domains that directly access physical resources without
the need for emulation or paravirtualisation. This retains, with negligible
system overhead, Linux's feature-richness in uncritical parts, while frugal
safety and real-time critical workloads execute in isolated, safe domains.
</summary>
    <author>
      <name>Ralf Ramsauer</name>
    </author>
    <author>
      <name>Jan Kiszka</name>
    </author>
    <author>
      <name>Daniel Lohmann</name>
    </author>
    <author>
      <name>Wolfgang Mauerer</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 13th Workshop on Operating Systems Platforms
  for Embedded Real-Time Applications (OSPERT 2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1705.06932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.06932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.05405v1</id>
    <updated>2017-07-17T22:26:19Z</updated>
    <published>2017-07-17T22:26:19Z</published>
    <title>Study and Analysis of MAC/IPAD Lab Configuration</title>
    <summary>  This paper is about three virtualization modes: VMware, Parallels, and Boot
Camping. The trade off of their testing is the hardware requirements. The main
question is, among the three, which is the most suitable? The answer actually
varies from user to user. It depends on the user needs. Moreover, it is
necessary to consider its performance, graphics, efficiency and reliability,
and interoperability, and that is our major scope. In order to take the final
decision in choosing one of the modes it is important to run some tests, which
costs a lot in terms of money, complexity, and time consumption. Therefore, in
order to overcome this trade off, most of the research has been done through
online benchmarking and my own anticipation. The final solution was extracted
after comparing all previously mentioned above and after rigorous testing made
which will be introduced later in this document.
</summary>
    <author>
      <name>Ayman Noor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.05405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07780v1</id>
    <updated>2017-07-25T00:59:32Z</updated>
    <published>2017-07-25T00:59:32Z</published>
    <title>FluidMem: Memory as a Service for the Datacenter</title>
    <summary>  Disaggregating resources in data centers is an emerging trend. Recent work
has begun to explore memory disaggregation, but suffers limitations including
lack of consideration of the complexity of cloud-based deployment, including
heterogeneous hardware and APIs for cloud users and operators. In this paper,
we present FluidMem, a complete system to realize disaggregated memory in the
datacenter. Going beyond simply demonstrating remote memory is possible, we
create an entire Memory as a Service. We define the requirements of Memory as a
Service and build its implementation in Linux as FluidMem. We present a
performance analysis of FluidMem and demonstrate that it transparently supports
remote memory for standard applications such as MongoDB and genome sequencing
applications.
</summary>
    <author>
      <name>Blake Caldwell</name>
    </author>
    <author>
      <name>Youngbin Im</name>
    </author>
    <author>
      <name>Sangtae Ha</name>
    </author>
    <author>
      <name>Richard Han</name>
    </author>
    <author>
      <name>Eric Keller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">University of Colorado Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.07780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08514v1</id>
    <updated>2017-07-26T16:02:29Z</updated>
    <published>2017-07-26T16:02:29Z</published>
    <title>Analyzing IO Amplification in Linux File Systems</title>
    <summary>  We present the first systematic analysis of read, write, and space
amplification in Linux file systems. While many researchers are tackling write
amplification in key-value stores, IO amplification in file systems has been
largely unexplored. We analyze data and metadata operations on five widely-used
Linux file systems: ext2, ext4, XFS, btrfs, and F2FS. We find that data
operations result in significant write amplification (2-32X) and that metadata
operations have a large IO cost. For example, a single rename requires 648 KB
write IO in btrfs. We also find that small random reads result in read
amplification of 2-13X. Based on these observations, we present the CReWS
conjecture about the relationship between IO amplification, consistency, and
storage space utilization. We hope this paper spurs people to design future
file systems with less IO amplification, especially for non-volatile memory
technologies.
</summary>
    <author>
      <name>Jayashree Mohan</name>
    </author>
    <author>
      <name>Rohan Kadekodi</name>
    </author>
    <author>
      <name>Vijay Chidambaram</name>
    </author>
    <link href="http://arxiv.org/abs/1707.08514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02746v2</id>
    <updated>2017-09-25T16:10:02Z</updated>
    <published>2017-09-08T15:29:17Z</published>
    <title>FreeGuard: A Faster Secure Heap Allocator</title>
    <summary>  In spite of years of improvements to software security, heap-related attacks
still remain a severe threat. One reason is that many existing memory
allocators fall short in a variety of aspects. For instance,
performance-oriented allocators are designed with very limited countermeasures
against attacks, but secure allocators generally suffer from significant
performance overhead, e.g., running up to 10x slower. This paper, therefore,
introduces FreeGuard, a secure memory allocator that prevents or reduces a wide
range of heap-related attacks, such as heap overflows, heap over-reads,
use-after-frees, as well as double and invalid frees. FreeGuard has similar
performance to the default Linux allocator, with less than 2% overhead on
average, but provides significant improvement to security guarantees. FreeGuard
also addresses multiple implementation issues of existing secure allocators,
such as the issue of scalability. Experimental results demonstrate that
FreeGuard is very effective in defending against a variety of heap-related
attacks.
</summary>
    <author>
      <name>Sam Silvestro</name>
    </author>
    <author>
      <name>Hongyu Liu</name>
    </author>
    <author>
      <name>Corey Crosser</name>
    </author>
    <author>
      <name>Zhiqiang Lin</name>
    </author>
    <author>
      <name>Tongping Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3133956.3133957</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3133956.3133957" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures, to be published at CCS'17</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02746v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02746v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.10140v1</id>
    <updated>2017-09-28T19:30:05Z</updated>
    <published>2017-09-28T19:30:05Z</published>
    <title>Performance Evaluation of Container-based Virtualization for High
  Performance Computing Environments</title>
    <summary>  Virtualization technologies have evolved along with the development of
computational environments since virtualization offered needed features at that
time such as isolation, accountability, resource allocation, resource fair
sharing and so on. Novel processor technologies bring to commodity computers
the possibility to emulate diverse environments where a wide range of
computational scenarios can be run. Along with processors evolution, system
developers have created different virtualization mechanisms where each new
development enhanced the performance of previous virtualized environments.
Recently, operating system-based virtualization technologies captured the
attention of communities abroad (from industry to academy and research) because
their important improvements on performance area.
  In this paper, the features of three container-based operating systems
virtualization tools (LXC, Docker and Singularity) are presented. LXC, Docker,
Singularity and bare metal are put under test through a customized single node
HPL-Benchmark and a MPI-based application for the multi node testbed. Also the
disk I/O performance, Memory (RAM) performance, Network bandwidth and GPU
performance are tested for the COS technologies vs bare metal. Preliminary
results and conclusions around them are presented and discussed.
</summary>
    <author>
      <name>Carlos Arango</name>
    </author>
    <author>
      <name>Rémy Dernat</name>
    </author>
    <author>
      <name>John Sanabria</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: Container-based virtualization; Linux containers;
  Singularity-Containers; Docker; High performance computing</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.10140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.10140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03789v1</id>
    <updated>2017-10-10T19:10:40Z</updated>
    <published>2017-10-10T19:10:40Z</published>
    <title>The Case for a Single System Image for Personal Devices</title>
    <summary>  Computing technology has gotten cheaper and more powerful, allowing users to
have a growing number of personal computing devices at their disposal. While
this trend is beneficial for the user, it also creates a growing management
burden for the user. Each device must be managed independently and users must
repeat the same management tasks on the each device, such as updating software,
changing configurations, backup, and replicating data for availability. To
prevent the management burden from increasing with the number of devices, we
propose that all devices run a single system image called a personal computing
image. Personal computing images export a device-specific user interface on
each device, but provide a consistent view of application and operating state
across all devices. As a result, management tasks can be performed once on any
device and will be automatically propagated to all other devices belonging to
the user. We discuss evolutionary steps that can be taken to achieve personal
computing images for devices and elaborate on challenges that we believe
building such systems will face.
</summary>
    <author>
      <name>Beom Heyn Kim</name>
    </author>
    <author>
      <name>Eyal de Lara</name>
    </author>
    <author>
      <name>David Lie</name>
    </author>
    <link href="http://arxiv.org/abs/1710.03789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05040v1</id>
    <updated>2017-12-13T23:14:28Z</updated>
    <published>2017-12-13T23:14:28Z</published>
    <title>Reservation-Based Federated Scheduling for Parallel Real-Time Tasks</title>
    <summary>  This paper considers the scheduling of parallel real-time tasks with
arbitrary-deadlines. Each job of a parallel task is described as a directed
acyclic graph (DAG). In contrast to prior work in this area, where
decomposition-based scheduling algorithms are proposed based on the
DAG-structure and inter-task interference is analyzed as self-suspending
behavior, this paper generalizes the federated scheduling approach. We propose
a reservation-based algorithm, called reservation-based federated scheduling,
that dominates federated scheduling. We provide general constraints for the
design of such systems and prove that reservation-based federated scheduling
has a constant speedup factor with respect to any optimal DAG task scheduler.
Furthermore, the presented algorithm can be used in conjunction with any
scheduler and scheduling analysis suitable for ordinary arbitrary-deadline
sporadic task sets, i.e., without parallelism.
</summary>
    <author>
      <name>Niklas Ueter</name>
    </author>
    <author>
      <name>Georg von der Brüggen</name>
    </author>
    <author>
      <name>Jian-Jia Chen</name>
    </author>
    <author>
      <name>Jing Li</name>
    </author>
    <author>
      <name>Kunal Agrawal</name>
    </author>
    <link href="http://arxiv.org/abs/1712.05040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.06276v1</id>
    <updated>2017-12-18T07:32:19Z</updated>
    <published>2017-12-18T07:32:19Z</published>
    <title>Migrate when necessary: toward partitioned reclaiming for soft real-time
  tasks</title>
    <summary>  This paper presents a new strategy for scheduling soft real-time tasks on
multiple identical cores. The proposed approach is based on partitioned CPU
reservations and it uses a reclaiming mechanism to reduce the number of missed
deadlines. We introduce the possibility for a task to temporarily migrate to
another, less charged, CPU when it has exhausted the reserved bandwidth on its
allocated CPU. In addition, we propose a simple load balancing method to
decrease the number of deadlines missed by the tasks. The proposed algorithm
has been evaluated through simulations, showing its effectiveness (compared to
other multi-core reclaiming approaches) and comparing the performance of
different partitioning heuristics (Best Fit, Worst Fit and First Fit).
</summary>
    <author>
      <name>Houssam Eddine Zahaf</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRIStAL</arxiv:affiliation>
    </author>
    <author>
      <name>Giuseppe Lipari</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRIStAL</arxiv:affiliation>
    </author>
    <author>
      <name>Luca Abeni</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRIStAL</arxiv:affiliation>
    </author>
    <author>
      <name>Houssam-Eddine Zahaf</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRIStAL</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3139258.3139280</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3139258.3139280" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of International Conference on Real-Time Networks and
  Systems, 2017, 10, pp.1-24</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.06276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.06276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07759v2</id>
    <updated>2017-12-22T02:53:24Z</updated>
    <published>2017-12-21T01:26:15Z</published>
    <title>POSIX-based Operating System in the environment of NVM/SCM memory</title>
    <summary>  Modern Operating Systems are typically POSIX-compliant. The system calls are
the fundamental layer of interaction between user-space applications and the OS
kernel and its implementation of fundamental abstractions and primitives used
in modern computing. The next generation of NVM/SCM memory raises critical
questions about the efficiency of modern OS architecture. This paper
investigates how the POSIX API drives performance for a system with NVM/SCM
memory. We show that OS and metadata related system calls represent the most
important area of optimization. However, the synchronization related system
calls (poll(), futex(), wait4()) are the most time-consuming overhead that even
a RAMdisk platform fails to eliminate. Attempting to preserve the POSIX-based
approach will likely result in fundamental inefficiencies for any future
applications of NVM/SCM memory.
</summary>
    <author>
      <name>Vyacheslav Dubeyko</name>
    </author>
    <author>
      <name>Cyril Guyot</name>
    </author>
    <author>
      <name>Luis Cargnini</name>
    </author>
    <author>
      <name>Adam Manzanares</name>
    </author>
    <link href="http://arxiv.org/abs/1712.07759v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07759v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05637v2</id>
    <updated>2019-01-10T12:39:27Z</updated>
    <published>2018-01-17T12:28:30Z</published>
    <title>Elevating commodity storage with the SALSA host translation layer</title>
    <summary>  To satisfy increasing storage demands in both capacity and performance,
industry has turned to multiple storage technologies, including Flash SSDs and
SMR disks. These devices employ a translation layer that conceals the
idiosyncrasies of their mediums and enables random access. Device translation
layers are, however, inherently constrained: resources on the drive are scarce,
they cannot be adapted to application requirements, and lack visibility across
multiple devices. As a result, performance and durability of many storage
devices is severely degraded.
  In this paper, we present SALSA: a translation layer that executes on the
host and allows unmodified applications to better utilize commodity storage.
SALSA supports a wide range of single- and multi-device optimizations and,
because is implemented in software, can adapt to specific workloads. We
describe SALSA's design, and demonstrate its significant benefits using
microbenchmarks and case studies based on three applications: MySQL, the Swift
object store, and a video server.
</summary>
    <author>
      <name>Nikolas Ioannou</name>
    </author>
    <author>
      <name>Kornilios Kourtis</name>
    </author>
    <author>
      <name>Ioannis Koltsidas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MASCOTS.2018.00035</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MASCOTS.2018.00035" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 2018 IEEE 26th International Symposium on Modeling,
  Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.05637v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05637v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09250v3</id>
    <updated>2019-08-20T18:53:28Z</updated>
    <published>2018-01-28T17:09:14Z</published>
    <title>Virtual Breakpoints for x86/64</title>
    <summary>  Efficient, reliable trapping of execution in a program at the desired
location is a linchpin technique for dynamic malware analysis. The progression
of debuggers and malware is akin to a game of cat and mouse - each are
constantly in a state of trying to thwart one another. At the core of most
efficient debuggers today is a combination of virtual machines and traditional
binary modification breakpoints (int3). In this paper, we present a design for
Virtual Breakpoints. a modification to the x86 MMU which brings breakpoint
management into hardware alongside page tables. In this paper we demonstrate
the fundamental abstraction failures of current trapping methods, and design a
new mechanism from the hardware up. This design incorporates lessons learned
from 50 years of virtualization and debugger design to deliver fast, reliable
trapping without the pitfalls of traditional binary modification.
</summary>
    <author>
      <name>Gregory Michael Price</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages, Presented at BSides Las Vegas 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09250v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09250v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.10376v1</id>
    <updated>2018-02-28T11:51:26Z</updated>
    <published>2018-02-28T11:51:26Z</published>
    <title>Push Forward: Global Fixed-Priority Scheduling of Arbitrary-Deadline
  Sporadic Task Systems</title>
    <summary>  The sporadic task model is often used to analyze recurrent execution of
identical tasks in real-time systems. A sporadic task defines an infinite
sequence of task instances, also called jobs, that arrive under the minimum
inter-arrival time constraint. To ensure the system safety, timeliness has to
be guaranteed in addition to functional correctness, i.e., all jobs of all
tasks have to be finished before the job deadlines. We focus on analyzing
arbitrary-deadline task sets on a homogeneous (identical) multiprocessor system
under any given global fixed-priority scheduling approach and provide a series
of schedulability tests with different tradeoffs between their time complexity
and their accuracy. Under the arbitrary-deadline setting, the relative deadline
of a task can be longer than the minimum inter-arrival time of the jobs of the
task. We show that global deadline-monotonic (DM) scheduling has a speedup
bound of $3-1/M$ against any optimal scheduling algorithms, where $M$ is the
number of identical processors, and prove that this bound is asymptotically
tight.
</summary>
    <author>
      <name>Jian-Jia Chen</name>
    </author>
    <author>
      <name>Georg von der Brüggen</name>
    </author>
    <author>
      <name>Niklas Ueter</name>
    </author>
    <link href="http://arxiv.org/abs/1802.10376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01226v1</id>
    <updated>2018-04-04T03:26:13Z</updated>
    <published>2018-04-04T03:26:13Z</published>
    <title>iReplayer: In-situ and Identical Record-and-Replay for Multithreaded
  Applications</title>
    <summary>  Reproducing executions of multithreaded programs is very challenging due to
many intrinsic and external non-deterministic factors. Existing RnR systems
achieve significant progress in terms of performance overhead, but none targets
the in-situ setting, in which replay occurs within the same process as the
recording process. Also, most existing work cannot achieve identical replay,
which may prevent the reproduction of some errors.
  This paper presents iReplayer, which aims to identically replay multithreaded
programs in the original process (under the "in-situ" setting). The novel
in-situ and identical replay of iReplayer makes it more likely to reproduce
errors, and allows it to directly employ debugging mechanisms (e.g.
watchpoints) to aid failure diagnosis. Currently, iReplayer only incurs 3%
performance overhead on average, which allows it to be always enabled in the
production environment. iReplayer enables a range of possibilities, and this
paper presents three examples: two automatic tools for detecting buffer
overflows and use-after-free bugs, and one interactive debugging tool that is
integrated with GDB.
</summary>
    <author>
      <name>Hongyu Liu</name>
    </author>
    <author>
      <name>Sam Silvestro</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Chen Tian</name>
    </author>
    <author>
      <name>Tongping Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3192366.3192380</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3192366.3192380" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 5 figures, to be published at PLDI'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01226v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01226v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00680v2</id>
    <updated>2019-01-15T01:47:04Z</updated>
    <published>2018-06-02T18:05:28Z</published>
    <title>Datacenter RPCs can be General and Fast</title>
    <summary>  It is commonly believed that datacenter networking software must sacrifice
generality to attain high performance. The popularity of specialized
distributed systems designed specifically for niche technologies such as RDMA,
lossless networks, FPGAs, and programmable switches testifies to this belief.
In this paper, we show that such specialization is not necessary. eRPC is a new
general-purpose remote procedure call (RPC) library that offers performance
comparable to specialized systems, while running on commodity CPUs in
traditional datacenter networks based on either lossy Ethernet or lossless
fabrics. eRPC performs well in three key metrics: message rate for small
messages; bandwidth for large messages; and scalability to a large number of
nodes and CPU cores. It handles packet loss, congestion, and background request
execution. In microbenchmarks, one CPU core can handle up to 10 million small
RPCs per second, or send large messages at 75 Gbps. We port a production-grade
implementation of Raft state machine replication to eRPC without modifying the
core Raft source code. We achieve 5.5 microseconds of replication latency on
lossy Ethernet, which is faster than or comparable to specialized replication
systems that use programmable switches, FPGAs, or RDMA.
</summary>
    <author>
      <name>Anuj Kalia</name>
    </author>
    <author>
      <name>Michael Kaminsky</name>
    </author>
    <author>
      <name>David G. Andersen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated to NSDI 2019 version</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00680v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00680v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.11431v1</id>
    <updated>2018-06-29T14:19:12Z</updated>
    <published>2018-06-29T14:19:12Z</published>
    <title>Integrating Proactive Mode Changes in Mixed Criticality Systems</title>
    <summary>  In this work, we propose to integrate prediction algorithms to the scheduling
of mode changes under the Earliest-Deadline-First and Fixed-priority scheduling
in mixed-criticality real-time systems. The method proactively schedules a mode
change in the system based on state variables such as laxity, to the percentage
difference in the temporal distance between the completion time of the instance
of a task and its respective deadline, by the deadline (D) stipulated for the
task, in order to minimize deadline misses. The simulation model was validated
against an analytical model prior to the logical integration of the
Kalman-based prediction algorithm. Two study cases were presented, one covering
earliest-deadline first and the other the fixed-priority scheduling approach.
The results showed the gains in the adoption of the prediction approach for
both scheduling paradigms by presenting a significant reduction of the number
of missed deadlines for low-criticality tasks.
</summary>
    <author>
      <name>Flavio R Massaro Jr.</name>
    </author>
    <author>
      <name>Paulo S. Martins</name>
    </author>
    <author>
      <name>Edson L. Ursini</name>
    </author>
    <link href="http://arxiv.org/abs/1806.11431v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11431v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00017v1</id>
    <updated>2018-07-31T18:30:09Z</updated>
    <published>2018-07-31T18:30:09Z</published>
    <title>New Analysis Techniques for Supporting Hard Real-Time Sporadic DAG Task
  Systems on Multiprocessors</title>
    <summary>  The scheduling and schedulability analysis of real-time directed acyclic
graph (DAG) task systems have received much recent attention. The DAG model can
accurately represent intra-task parallelim and precedence constraints existing
in many application domains. Existing techniques show that analyzing the DAG
model is fundamentally more challenging compared to the ordinary sporadic task
model, due to the complex intra-DAG precedence constraints which may cause
rather pessimistic schedulability loss. However,such increased loss is
counter-intuitive because the DAG structure shall better exploit the
parallelism provided by the multiprocessor platform. Our observation is that
the intra-DAG precedence constraints, if not carefully considered by the
scheduling algorithm, may cause very unpredictable execution behaviors of
subtasks in a DAG and further cause pessimistic analysis. In this paper, we
present a set of novel scheduling and analysis techniques for better supporting
hard real-time sporadic DAG tasks on multiprocessors, through smartly defining
and analyzing the execution order of subtasks in each DAG. Evaluation
demonstrates that our developed utilization-based schedulability test is highly
efficient, which dramatically improves schedulability of existing
utilization-based tests by over 60% on average. Interestingly, when each DAG in
the system is an ordinary sporadic task, our test becomes identical to the
classical density test designed for the sporadic task model.
</summary>
    <author>
      <name>Zheng Dong</name>
    </author>
    <author>
      <name>Cong Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04239v1</id>
    <updated>2018-08-07T08:21:58Z</updated>
    <published>2018-08-07T08:21:58Z</published>
    <title>A Spin-based model checking for the simple concurrent program on a
  preemptive RTOS</title>
    <summary>  We adapt an existing preemptive scheduling model of RTOS kernel by eChronos
from machine-assisted proof to Spin-based model checker. The model we
constructed can be automatically verified rather than formulating proofs by
hand. Moreover, we look into the designs of a Linux-like real-time
kernel--Piko/RT and the specification of ARMv7-M architecture to reconstruct
the model, and use LTL to specify a simple concurrent
programs--consumer/producer problem during the development stage of the kernel.
We show that under the preemptive scheduling and the mechanism of ARMv7-M, the
program will not suffer from race condition, starvation, and deadlock.
</summary>
    <author>
      <name>Chen-Kai Lin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Jim</arxiv:affiliation>
    </author>
    <author>
      <name> Ching-Chun</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Jim</arxiv:affiliation>
    </author>
    <author>
      <name> Huang</name>
    </author>
    <author>
      <name>Bow-Yaw Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures, The 24th Workshop on Compiler Techniques and
  System Software for High-Performance and Embedded Computing, 2018, Chiayi,
  Taiwan</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.04239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08192v2</id>
    <updated>2023-04-17T05:50:07Z</updated>
    <published>2018-08-17T22:35:15Z</published>
    <title>Comparative Study of Virtual Machines and Containers for DevOps
  Developers</title>
    <summary>  This paper presents a comparative study of virtual machines (VMs) and
containers for DevOps developers. The study explores the benefits and drawbacks
of each technology in terms of their functionality, performance, security, and
resource utilization. The paper examines the underlying architecture of VMs and
containers, and how they differ from each other. The study includes a series of
experiments that compare the performance and resource utilization of VMs and
containers in different scenarios. The experiments evaluate factors such as
startup time, memory usage, disk I/O, network latency, scalability, and
administrative overhead. Finally, the paper provides recommendations for DevOps
developers on which technology to choose based on their specific requirements
and use cases. Overall, the study aims to provide a comprehensive understanding
of the strengths and limitations of VMs and containers, helping developers to
make informed decisions when choosing between them.
</summary>
    <author>
      <name>Saurabh Deochake</name>
    </author>
    <author>
      <name>Sumit Maheshwari</name>
    </author>
    <author>
      <name>Ridip De</name>
    </author>
    <author>
      <name>Anish Grover</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16:198:553 Course Rutgers CS</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08192v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08192v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10821v1</id>
    <updated>2018-08-30T15:20:56Z</updated>
    <published>2018-08-30T15:20:56Z</published>
    <title>Real-time Linux communications: an evaluation of the Linux communication
  stack for real-time robotic applications</title>
    <summary>  As robotics systems become more distributed, the communications between
different robot modules play a key role for the reliability of the overall
robot control. In this paper, we present a study of the Linux communication
stack meant for real-time robotic applications. We evaluate the real-time
performance of UDP based communications in Linux on multi-core embedded devices
as test platforms. We prove that, under an appropriate configuration, the Linux
kernel greatly enhances the determinism of communications using the UDP
protocol. Furthermore, we demonstrate that concurrent traffic disrupts the
bounded latencies and propose a solution by separating the real-time
application and the corresponding interrupt in a CPU.
</summary>
    <author>
      <name>Carlos San Vicente Gutiérrez</name>
    </author>
    <author>
      <name>Lander Usategui San Juan</name>
    </author>
    <author>
      <name>Irati Zamalloa Ugarte</name>
    </author>
    <author>
      <name>Víctor Mayoral Vilches</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04603v1</id>
    <updated>2018-10-10T15:49:52Z</updated>
    <published>2018-10-10T15:49:52Z</published>
    <title>Revitalizing Copybacks in Modern SSDs: Why and How</title>
    <summary>  For modern flash-based SSDs, the performance overhead of internal data
migrations is dominated by the data transfer time, not by the flash program
time as in old SSDs. In order to mitigate the performance impact of data
migrations, we propose rCopyback, a restricted version of copyback. Rcopyback
works like the original copyback except that only n consecutive copybacks are
allowed. By limiting the number of successive copybacks, it guarantees that no
data reliability problem occurs when data is internally migrated using
rCopyback. In order to take a full advantage of rCopyback, we developed a
rCopyback-aware FTL, rcFTL, which intelligently decides whether rCopyback
should be used or not by exploiting varying host workloads. Our evaluation
results show that rcFTL can improve the overall I/O throughput by 54% on
average over an existing FTL which does not use copybacks.
</summary>
    <author>
      <name>Duwon Hong</name>
    </author>
    <author>
      <name>Myungsuk Kim</name>
    </author>
    <author>
      <name>Jisung Park</name>
    </author>
    <author>
      <name>Myoungsoo Jung</name>
    </author>
    <author>
      <name>Jihong Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.04603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.05068v1</id>
    <updated>2018-10-11T15:05:44Z</updated>
    <published>2018-10-11T15:05:44Z</published>
    <title>T-Visor: A Hypervisor for Mixed Criticality Embedded Real-time System
  with Hardware Virtualization Support</title>
    <summary>  Recently, embedded systems have not only requirements for hard real-time
behavior and reliability, but also diversified functional demands, such as
network functions. To satisfy these requirements, virtualization using
hypervisors is promising for embedded systems. However, as most of existing
hypervisors are designed for general-purpose information processing systems,
they rely on large system stacks, so that they are not suitable for mixed
criticality embedded real-time systems. Even in hypervisors designed for
embedded systems, their schedulers do not consider the diversity of real-time
requirements and rapid change in scheduling theory.
  We present the design and implementation of T-Visor, a hypervisor specialized
for mixed criticality embedded real-time systems. T-Visor supports ARM
architecture and realizes full virtualization using ARM Virtualization
Extensions. To guarantee real-time behavior, T-Visor provides a flexible
scheduling framework so that developers can select the most suitable scheduling
algorithm for their systems. Our evaluation showed that it performed better
compared to Xen/ARM. From these results, we conclude that our design and
implementation are more suitable for embedded real-time systems than the
existing hypervisors.
</summary>
    <author>
      <name>Takumi Shimada</name>
    </author>
    <author>
      <name>Takeshi Yashiro</name>
    </author>
    <author>
      <name>Ken Sakamura</name>
    </author>
    <link href="http://arxiv.org/abs/1810.05068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.05068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.00757v1</id>
    <updated>2018-11-02T07:11:52Z</updated>
    <published>2018-11-02T07:11:52Z</published>
    <title>DurableFS: A File System for Persistent Memory</title>
    <summary>  With the availability of hybrid DRAM-NVRAM memory on the memory bus of CPUs,
a number of file systems on NVRAM have been designed and implemented. In this
paper we present the design and implementation of a file system on NVRAM called
DurableFS, which provides atomicity and durability of file operations to
applications. Due to the byte level random accessibility of memory, it is
possible to provide these guarantees without much overhead. We use standard
techniques like copy on write for data, and a redo log for metadata changes to
build an efficient file system which provides durability and atomicity
guarantees at the time a file is closed. Benchmarks on the implementation shows
that there is only a 7 %degradation in performance due to providing these
guarantees.
</summary>
    <author>
      <name>Chandan Kalita</name>
    </author>
    <author>
      <name>Gautam Barua</name>
    </author>
    <author>
      <name>Priya Sehgal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages, 3 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.00757v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.00757v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.09792v2</id>
    <updated>2018-12-01T04:45:49Z</updated>
    <published>2018-11-24T08:54:38Z</published>
    <title>MiniOS: an instructional platform for teaching operating systems labs</title>
    <summary>  Delivering hands-on practice laboratories for introductory courses on
operating systems is a difficult task. One of the main sources of the
difficulty is the sheer size and complexity of the operating systems software.
Consequently, some of the solutions adopted in the literature to teach
operating systems laboratory consider smaller and simpler systems, generally
referred to as instructional operating systems. This work continues in the same
direction and is threefold. First, it considers a simpler hardware platform.
Second, it argues that a minimal operating system is a viable option for
delivering laboratories. Third, it presents a laboratory teaching platform,
whereby students build a minimal operating system for an embedded hardware
platform. The proposed platform is called MiniOS. An important aspect of MiniOS
is that it is sufficiently supported with additional technical and pedagogic
material. Finally, the effectiveness of the proposed approach to teach
operating systems laboratories is illustrated through the experience of using
it to deliver laboratory projects in the Operating Systems course at the
University of Northern British Columbia. Finally, we discuss experimental
research in computing education and considered the qualitative results of this
work as part of a larger research endeavour.
</summary>
    <author>
      <name>Rafael Roman Otero</name>
    </author>
    <author>
      <name>Alex Aravind</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.09792v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.09792v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.01340v1</id>
    <updated>2019-01-04T23:10:05Z</updated>
    <published>2019-01-04T23:10:05Z</published>
    <title>File System in Data-Centric Computing</title>
    <summary>  The moving computation on the edge or near to data is the new trend that can
break the bandwidth wall and to unleash the power of next generation NVM or SCM
memory. File system is the important OS subsystem that plays the role of
mediator between the user-space application and storage device. The key goal of
the file system is to represent the file abstraction and to build the files'
namespace. In the current paradigm the file system needs to copy the metadata
and user data in the DRAM of the host with the goal to access and to modify the
user data on the host side. The DAX approach doesn't change the concept but to
build the way to bypass the page cache via the direct access to file's content
in persistent memory. Generally speaking, for the case of data-centric
computing, the file system needs to solve the opposite task not to copy data
into page cache but to deliver the processing activity near data on the storage
device side.
</summary>
    <author>
      <name>Viacheslav Dubeyko</name>
    </author>
    <link href="http://arxiv.org/abs/1901.01340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.01340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.06360v1</id>
    <updated>2019-01-18T18:01:38Z</updated>
    <published>2019-01-18T18:01:38Z</published>
    <title>Multiverse: Easy Conversion of Runtime Systems into OS Kernels via
  Automatic Hybridization</title>
    <summary>  The hybrid runtime (HRT) model offers a path towards high performance and
efficiency. By integrating the OS kernel, runtime, and application, an HRT
allows the runtime developer to leverage the full feature set of the hardware
and specialize OS services to the runtime's needs. However, conforming to the
HRT model currently requires a port of the runtime to the kernel level, for
example to the Nautilus kernel framework, and this requires knowledge of kernel
internals. In response, we developed Multiverse, a system that bridges the gap
between a built-from-scratch HRT and a legacy runtime system. Multiverse allows
unmodified applications and runtimes to be brought into the HRT model without
any porting effort whatsoever by splitting the execution of the application
between the domains of a legacy OS and an HRT environment. We describe the
design and implementation of Multiverse and illustrate its capabilities using
the massive, widely-used Racket runtime system.
</summary>
    <author>
      <name>Kyle C. Hale</name>
    </author>
    <author>
      <name>Conor Hetland</name>
    </author>
    <author>
      <name>Peter Dinda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICAC.2017.24</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICAC.2017.24" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in the Proceedings of the 14th International Conference on
  Autonomic Computing (ICAC '17)</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.06360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.06360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.00660v1</id>
    <updated>2019-02-02T07:22:25Z</updated>
    <published>2019-02-02T07:22:25Z</published>
    <title>Fine-Grain Checkpointing with In-Cache-Line Logging</title>
    <summary>  Non-Volatile Memory offers the possibility of implementing high-performance,
durable data structures. However, achieving performance comparable to
well-designed data structures in non-persistent (transient) memory is
difficult, primarily because of the cost of ensuring the order in which memory
writes reach NVM. Often, this requires flushing data to NVM and waiting a full
memory round-trip time.
  In this paper, we introduce two new techniques: Fine-Grained Checkpointing,
which ensures a consistent, quickly recoverable data structure in NVM after a
system failure, and In-Cache-Line Logging, an undo-logging technique that
enables recovery of earlier state without requiring cache-line flushes in the
normal case. We implemented these techniques in the Masstree data structure,
making it persistent and demonstrating the ease of applying them to a highly
optimized system and their low (5.9-15.4\%) runtime overhead cost.
</summary>
    <author>
      <name>Nachshon Cohen</name>
    </author>
    <author>
      <name>David T. Aksun</name>
    </author>
    <author>
      <name>Hillel Avni</name>
    </author>
    <author>
      <name>James R. Larus</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3297858.3304046</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3297858.3304046" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In 2019 Architectural Support for Programming Languages and Operating
  Systems (ASPLOS 19), April 13, 2019, Providence, RI, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.00660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.00660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.03383v1</id>
    <updated>2019-02-09T07:25:09Z</updated>
    <published>2019-02-09T07:25:09Z</published>
    <title>Cloud Programming Simplified: A Berkeley View on Serverless Computing</title>
    <summary>  Serverless cloud computing handles virtually all the system administration
operations needed to make it easier for programmers to use the cloud. It
provides an interface that greatly simplifies cloud programming, and represents
an evolution that parallels the transition from assembly language to high-level
programming languages. This paper gives a quick history of cloud computing,
including an accounting of the predictions of the 2009 Berkeley View of Cloud
Computing paper, explains the motivation for serverless computing, describes
applications that stretch the current limits of serverless, and then lists
obstacles and research opportunities required for serverless computing to
fulfill its full potential. Just as the 2009 paper identified challenges for
the cloud and predicted they would be addressed and that cloud use would
accelerate, we predict these issues are solvable and that serverless computing
will grow to dominate the future of cloud computing.
</summary>
    <author>
      <name>Eric Jonas</name>
    </author>
    <author>
      <name>Johann Schleier-Smith</name>
    </author>
    <author>
      <name>Vikram Sreekanti</name>
    </author>
    <author>
      <name>Chia-Che Tsai</name>
    </author>
    <author>
      <name>Anurag Khandelwal</name>
    </author>
    <author>
      <name>Qifan Pu</name>
    </author>
    <author>
      <name>Vaishaal Shankar</name>
    </author>
    <author>
      <name>Joao Carreira</name>
    </author>
    <author>
      <name>Karl Krauth</name>
    </author>
    <author>
      <name>Neeraja Yadwadkar</name>
    </author>
    <author>
      <name>Joseph E. Gonzalez</name>
    </author>
    <author>
      <name>Raluca Ada Popa</name>
    </author>
    <author>
      <name>Ion Stoica</name>
    </author>
    <author>
      <name>David A. Patterson</name>
    </author>
    <link href="http://arxiv.org/abs/1902.03383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.01950v2</id>
    <updated>2019-11-20T21:28:47Z</updated>
    <published>2019-03-05T17:35:03Z</published>
    <title>Pyronia: Intra-Process Access Control for IoT Applications</title>
    <summary>  Third-party code plays a critical role in IoT applications, which generate
and analyze highly privacy-sensitive data. Unlike traditional desktop and
server settings, IoT devices mostly run a dedicated, single application. As a
result, vulnerabilities in third-party libraries within a process pose a much
bigger threat than on traditional platforms.
  We present Pyronia, a fine-grained access control system for IoT applications
written in high-level languages. Pyronia exploits developers' coarse-grained
expectations about how imported third-party code operates to restrict access to
files, devices, and specific network destinations, at the granularity of
individual functions. To efficiently protect such sensitive OS resources,
Pyronia combines three techniques: system call interposition, stack inspection,
and memory domains. This design avoids the need for application refactoring, or
unintuitive data flow analysis, while enforcing the developer's access policy
at run time. Our Pyronia prototype for Python runs on a custom Linux kernel,
and incurs moderate performance overhead on unmodified Python applications.
</summary>
    <author>
      <name>Marcela S. Melara</name>
    </author>
    <author>
      <name>David H. Liu</name>
    </author>
    <author>
      <name>Michael J. Freedman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.01950v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01950v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.04075v1</id>
    <updated>2019-03-10T23:34:14Z</updated>
    <published>2019-03-10T23:34:14Z</published>
    <title>Nature of System Calls in CPU-centric Computing Paradigm</title>
    <summary>  Modern operating systems are typically POSIX-compliant with major system
calls specified decades ago. The next generation of non-volatile memory (NVM)
technologies raise concerns about the efficiency of the traditional POSIX-based
systems. As one step toward building high performance NVM systems, we explore
the potential dependencies between system call performance and major hardware
components (e.g., CPU, memory, storage) under typical user cases (e.g.,
software compilation, installation, web browser, office suite) in this paper.
We build histograms for the most frequent and time-consuming system calls with
the goal to understand the nature of distribution on different platforms. We
find that there is a strong dependency between the system call performance and
the CPU architecture. On the other hand, the type of persistent storage plays a
less important role in affecting the performance.
</summary>
    <author>
      <name>Viacheslav Dubeyko</name>
    </author>
    <author>
      <name>Om Rameshwar Gatla</name>
    </author>
    <author>
      <name>Mai Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1903.04075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.04075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.06889v1</id>
    <updated>2019-03-16T07:17:47Z</updated>
    <published>2019-03-16T07:17:47Z</published>
    <title>MultiK: A Framework for Orchestrating Multiple Specialized Kernels</title>
    <summary>  We present, MultiK, a Linux-based framework 1 that reduces the attack surface
for operating system kernels by reducing code bloat. MultiK "orchestrates"
multiple kernels that are specialized for individual applications in a
transparent manner. This framework is flexible to accommodate different kernel
code reduction techniques and, most importantly, run the specialized kernels
with near-zero additional runtime overheads. MultiK avoids the overheads of
virtualization and runs natively on the system. For instance, an Apache
instance is shown to run on a kernel that has (a) 93.68% of its code reduced,
(b) 19 of 23 known kernel vulnerabilities eliminated and (c) with negligible
performance overheads (0.19%). MultiK is a framework that can integrate with
existing code reduction and OS security techniques. We demonstrate this by
using D-KUT and S-KUT -- two methods to profile and eliminate unwanted kernel
code. The whole process is transparent to the user applications because MultiK
does not require a recompilation of the application.
</summary>
    <author>
      <name>Hsuan-Chi Kuo</name>
    </author>
    <author>
      <name>Akshith Gunasekaran</name>
    </author>
    <author>
      <name>Yeongjin Jang</name>
    </author>
    <author>
      <name>Sibin Mohan</name>
    </author>
    <author>
      <name>Rakesh B. Bobba</name>
    </author>
    <author>
      <name>David Lie</name>
    </author>
    <author>
      <name>Jesse Walker</name>
    </author>
    <link href="http://arxiv.org/abs/1903.06889v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.06889v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.09310v4</id>
    <updated>2019-05-20T16:35:52Z</updated>
    <published>2019-03-22T13:08:13Z</published>
    <title>A WCET-aware cache coloring technique for reducing interference in
  real-time systems</title>
    <summary>  The predictability of a system is the condition to give saferbound on worst
case execution timeof real-time tasks which are running on it. Commercial
off-the-shelf(COTS) processors are in-creasingly used in embedded systems and
contain shared cache memory. This component hasa hard predictable behavior
because its state depends of theexecution history of the systems.To increase
predictability of COTS component we use cache coloring, a technique widely
usedto partition cache memory. Our main contribution is a WCET aware heuristic
which parti-tion task according to the needs of each task. Our experiments are
made with CPLEX an ILPsolver with random tasks set generated running on
preemptive system scheduled with earliestdeadline first(EDF).
</summary>
    <author>
      <name>Fabien Bouquillon</name>
    </author>
    <author>
      <name>Clément Ballabriga</name>
    </author>
    <author>
      <name>Giuseppe Lipari</name>
    </author>
    <author>
      <name>Smail Niar</name>
    </author>
    <link href="http://arxiv.org/abs/1903.09310v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.09310v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.10818v2</id>
    <updated>2019-07-11T23:15:30Z</updated>
    <published>2019-05-26T15:38:30Z</published>
    <title>Avoiding Scalability Collapse by Restricting Concurrency</title>
    <summary>  Saturated locks often degrade the performance of a multithreaded application,
leading to a so-called scalability collapse problem. This problem arises when a
growing number of threads circulating through a saturated lock causes the
overall application performance to fade or even drop abruptly. This problem is
particularly (but not solely) acute on oversubscribed systems (systems with
more threads than available hardware cores). In this paper, we introduce GCR
(generic concurrency restriction), a mechanism that aims to avoid the
scalability collapse. GCR, designed as a generic, lock-agnostic wrapper,
intercepts lock acquisition calls, and decides when threads would be allowed to
proceed with the acquisition of the underlying lock. Furthermore, we present
GCR-NUMA, a non-uniform memory access (NUMA)-aware extension of GCR, that
strives to ensure that threads allowed to acquire the lock are those that run
on the same socket. The extensive evaluation that includes more than two dozen
locks, three machines and three benchmarks shows that GCR brings substantial
speedup (in many cases, up to three orders of magnitude) in case of contention
and growing thread counts, while introducing nearly negligible slowdown when
the underlying lock is not contended. GCR-NUMA brings even larger performance
gains starting at even lighter lock contention.
</summary>
    <author>
      <name>Dave Dice</name>
    </author>
    <author>
      <name>Alex Kogan</name>
    </author>
    <link href="http://arxiv.org/abs/1905.10818v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.10818v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.00271v1</id>
    <updated>2019-06-29T19:59:21Z</updated>
    <published>2019-06-29T19:59:21Z</published>
    <title>HTS: A Hardware Task Scheduler for Heterogeneous Systems</title>
    <summary>  As the Moore's scaling era comes to an end, application specific hardware
accelerators appear as an attractive way to improve the performance and power
efficiency of our computing systems. A massively heterogeneous system with a
large number of hardware accelerators along with multiple general purpose CPUs
is a promising direction, but pose several challenges in terms of the run-time
scheduling of tasks on the accelerators and design granularity of accelerators.
This paper addresses these challenges by developing an example heterogeneous
system to enable multiple applications to share the available accelerators. We
propose to design accelerators at a lower abstraction to enable applications to
be broken down into tasks that can be mapped on several accelerators. We
observe that several real-life workloads can be broken down into common
primitives that are shared across many workloads. Finally, we propose and
design a hardware task scheduler inspired by the hardware schedulers in
out-of-order superscalar processors to efficiently utilize the accelerators in
the system by scheduling tasks in out-of-order and even speculatively. We
evaluate the proposed system on both real-life and synthetic benchmarks based
on Digital Signal Processing~(DSP) applications. Compared to executing the
benchmark on a system with sequential scheduling, proposed scheduler achieves
up to 12x improvement in performance.
</summary>
    <author>
      <name>Kartik Hegde</name>
    </author>
    <author>
      <name>Abhishek Srivastava</name>
    </author>
    <author>
      <name>Rohit Agrawal</name>
    </author>
    <link href="http://arxiv.org/abs/1907.00271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.00271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.03356v1</id>
    <updated>2019-07-07T22:26:02Z</updated>
    <published>2019-07-07T22:26:02Z</published>
    <title>Reproducible Execution of POSIX Programs with DiOS</title>
    <summary>  In this paper, we describe DiOS, a lightweight model operating system which
can be used to execute programs that make use of POSIX APIs. Such executions
are fully reproducible: running the same program with the same inputs twice
will result in two exactly identical instruction traces, even if the program
uses threads for parallelism.
  DiOS is implemented almost entirely in portable C and C++: although its
primary platform is DiVM, a verification-oriented virtual machine, it can be
configured to also run in KLEE, a symbolic executor. Finally, it can be
compiled into machine code to serve as a user-mode kernel.
  Additionally, DiOS is modular and extensible. Its various components can be
combined to match both the capabilities of the underlying platform and to
provide services required by a particular program. New components can be added
to cover additional system calls or APIs.
  The experimental evaluation has two parts. DiOS is first evaluated as a
component of a program verification platform based on DiVM. In the second part,
we consider its portability and modularity by combining it with the symbolic
executor KLEE.
</summary>
    <author>
      <name>Petr Ročkai</name>
    </author>
    <author>
      <name>Zuzana Baranová</name>
    </author>
    <author>
      <name>Jan Mrázek</name>
    </author>
    <author>
      <name>Katarína Kejstová</name>
    </author>
    <author>
      <name>Jiří Barnat</name>
    </author>
    <link href="http://arxiv.org/abs/1907.03356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.03356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.11825v1</id>
    <updated>2019-07-27T01:36:43Z</updated>
    <published>2019-07-27T01:36:43Z</published>
    <title>SSDFS: Towards LFS Flash-Friendly File System without GC operation</title>
    <summary>  Solid state drives have a number of interesting characteristics. However,
there are numerous file system and storage design issues for SSDs that impact
the performance and device endurance. Many flash-oriented and flash-friendly
file systems introduce significant write amplification issue and GC overhead
that results in shorter SSD lifetime and necessity to use the NAND flash
overprovisioning. SSDFS file system introduces several authentic concepts and
mechanisms: logical segment, logical extent, segment's PEBs pool,
Main/Diff/Journal areas in the PEB's log, Diff-On-Write approach, PEBs
migration scheme, hot/warm data self-migration, segment bitmap, hybrid b-tree,
shared dictionary b-tree, shared extents b-tree. Combination of all suggested
concepts are able: (1) manage write amplification in smart way, (2) decrease GC
overhead, (3) prolong SSD lifetime, and (4) provide predictable file system's
performance.
</summary>
    <author>
      <name>Viacheslav Dubeyko</name>
    </author>
    <link href="http://arxiv.org/abs/1907.11825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.11825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.06807v2</id>
    <updated>2020-03-23T20:52:02Z</updated>
    <published>2019-08-19T13:55:44Z</published>
    <title>Boomerang: Real-Time I/O Meets Legacy Systems</title>
    <summary>  This paper presents Boomerang, an I/O system that integrates a legacy
non-real-time OS with one that is customized for timing-sensitive tasks. A
relatively small RTOS benefits from the pre-existing libraries, drivers and
services of the legacy system. Additionally, timing-critical tasks are isolated
from less critical tasks by securely partitioning machine resources among the
separate OSes. Boomerang guarantees end-to-end processing delays on input data
that requires outputs to be generated within specific time bounds.
  We show how to construct composable task pipelines in Boomerang that combine
functionality spanning a custom RTOS and a legacy Linux system. By dedicating
time-critical I/O to the RTOS, we ensure that complementary services provided
by Linux are sufficiently predictable to meet end-to-end service guarantees.
While Boomerang benefits from spatial isolation, it also outperforms a
standalone Linux system using deadline-based CPU reservations for pipeline
tasks. We also show how Boomerang outperforms a virtualized system called ACRN,
designed for automotive systems.
</summary>
    <author>
      <name>Ahmad Golchin</name>
    </author>
    <author>
      <name>Soham Sinha</name>
    </author>
    <author>
      <name>Richard West</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is now accepted for publication in Proceedings of the 26th
  IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),
  Sydney, Australia, April 21-24, 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.06807v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.06807v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.0; D.4.7; D.4.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.10740v1</id>
    <updated>2019-08-28T14:15:21Z</updated>
    <published>2019-08-28T14:15:21Z</published>
    <title>Kernel/User-level Collaborative Persistent Memory File System with
  Efficiency and Protection</title>
    <summary>  Emerging high performance non-volatile memories recall the importance of
efficient file system design. To avoid the virtual file system (VFS) and
syscall overhead as in these kernel-based file systems, recent works deploy
file systems directly in user level. Unfortunately, a userlevel file system can
easily be corrupted by a buggy program with misused pointers, and is hard to
scale on multi-core platforms which incorporates a centralized coordination
service. In this paper, we propose KucoFS, a Kernel and user-level
collaborative file system. It consists of two parts: a user-level library with
direct-access interfaces, and a kernel thread, which performs metadata updates
and enforces write protection by toggling the permission bits in the page
table. Hence, KucoFS achieves both direct-access of user-level designs and
fine-grained write protection of kernel-level ones. We further explore its
scalability to multicores: For metadata scalability, KucoFS rebalances the
pathname resolution overhead between the kernel and userspace, by adopting the
index offloading technique. For data access efficiency, it coordinates the data
allocation between kernel and userspace, and uses range-lock write and
lock-free read to improve concurrency. Experiments on Optane DC persistent
memory show that KucoFS significantly outperforms existing file systems and
shows better scalability.
</summary>
    <author>
      <name>Youmin Chen</name>
    </author>
    <author>
      <name>Youyou Lu</name>
    </author>
    <author>
      <name>Bohong Zhu</name>
    </author>
    <author>
      <name>Jiwu Shu</name>
    </author>
    <link href="http://arxiv.org/abs/1908.10740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.04949v1</id>
    <updated>2019-10-11T02:45:20Z</updated>
    <published>2019-10-11T02:45:20Z</published>
    <title>Enabling Failure-resilient Intermittent Systems Without Runtime
  Checkpointing</title>
    <summary>  Self-powered intermittent systems typically adopt runtime checkpointing as a
means to accumulate computation progress across power cycles and recover system
status from power failures. However, existing approaches based on the
checkpointing paradigm normally require system suspension and/or logging at
runtime. This paper presents a design which overcomes the drawbacks of
checkpointing-based approaches, to enable failure-resilient intermittent
systems. Our design allows accumulative execution and instant system recovery
under frequent power failures while enforcing the serializability of concurrent
task execution to improve computation progress and ensuring data consistency
without system suspension during runtime, by leveraging the characteristics of
data accessed in hybrid memory. We integrated the design into FreeRTOS running
on a Texas Instruments device. Experimental results show that our design can
still accumulate progress when the power source is too weak for
checkpointing-based approaches to make progress, and improves the computation
progress by up to 43% under a relatively strong power source, while reducing
the recovery time by at least 90%.
</summary>
    <author>
      <name>Wei-Ming Chen</name>
    </author>
    <author>
      <name> Tei-Wei-Kuo</name>
    </author>
    <author>
      <name>Pi-Cheng Hsiu</name>
    </author>
    <link href="http://arxiv.org/abs/1910.04949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.04949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.13056v1</id>
    <updated>2019-10-29T02:56:07Z</updated>
    <published>2019-10-29T02:56:07Z</published>
    <title>Disaggregation and the Application</title>
    <summary>  This paper examines disaggregated data center architectures from the
perspective of the applications that would run on these data centers, and
challenges the abstractions that have been proposed to date. In particular, we
argue that operating systems for disaggregated data centers should not abstract
disaggregated hardware resources, such as memory, compute, and storage away
from applications, but should instead give them information about, and control
over, these resources. To this end, we propose additional OS abstractions and
interfaces for disaggregation and show how they can improve data transfer in
data parallel frameworks and speed up failure recovery in replicated,
fault-tolerant applications. This paper studies the technical challenges in
providing applications with this additional functionality and advances several
preliminary proposals to overcome these challenges.
</summary>
    <author>
      <name>Sebastian Angel</name>
    </author>
    <author>
      <name>Mihir Nanavati</name>
    </author>
    <author>
      <name>Siddhartha Sen</name>
    </author>
    <link href="http://arxiv.org/abs/1910.13056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.13056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.08367v1</id>
    <updated>2019-11-19T15:56:46Z</updated>
    <published>2019-11-19T15:56:46Z</published>
    <title>Cichlid: Explicit physical memory management for large machines</title>
    <summary>  In this paper, we rethink how an OS supports virtual memory. Classical VM is
an opaque abstraction of RAM, backed by demand paging. However, most systems
today (from phones to data-centers) do not page, and indeed may require the
performance benefits of non-paged physical memory, precise NUMA allocation,
etc. Moreover, MMU hardware is now useful for other purposes, such as detecting
page access or providing large page translation. Accordingly, the venerable VM
abstraction in OSes like Windows and Linux has acquired a plethora of extra
APIs to poke at the policy behind the illusion of a virtual address space.
  Instead, we present Cichlid, a memory system which inverts this model.
Applications explicitly manage their physical RAM of different types, and
directly (though safely) program the translation hardware. Cichlid is
implemented in Barrelfish, requires no virtualization support, and outperforms
VMM-based approaches for all but the smallest working sets. We show that
Cichlid enables use-cases for virtual memory not possible in Linux today, and
other use-cases are simple to program and significantly faster.
</summary>
    <author>
      <name>Simon Gerber</name>
    </author>
    <author>
      <name>Gerd Zellweger</name>
    </author>
    <author>
      <name>Reto Achermann</name>
    </author>
    <author>
      <name>Moritz Hoffmann</name>
    </author>
    <author>
      <name>Kornilios Kourtis</name>
    </author>
    <author>
      <name>Timothy Roscoe</name>
    </author>
    <author>
      <name>Dejan Milojicic</name>
    </author>
    <link href="http://arxiv.org/abs/1911.08367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.08367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.08773v1</id>
    <updated>2019-11-20T08:57:09Z</updated>
    <published>2019-11-20T08:57:09Z</published>
    <title>CleanQ: a lightweight, uniform, formally specified interface for
  intra-machine data transfer</title>
    <summary>  We present CleanQ, a high-performance operating-system interface for
descriptor-based data transfer with rigorous formal semantics, based on a
simple, formally-verified notion of ownership transfer, with a fast reference
implementation. CleanQ aims to replace the current proliferation of similar,
but subtly diverse, and loosely specified, descriptor-based interfaces in OS
kernels and device drivers. CleanQ has strict semantics that not only clarify
both the implementation of the interface for different hardware devices and
software usecases, but also enable composition of modules as in more
heavyweight frameworks like Unix streams. We motivate CleanQ by showing that
loose specifications derived from implementation lead to security and
correctness bugs in production systems that a clean, formal, and
easilyunderstandable abstraction helps eliminate. We further demonstrate by
experiment that there is negligible performance cost for a clean design: we
show overheads in the tens of cycles for operations, and comparable end-to-end
performance to the highly-tuned Virtio and DPDK implementations on Linux.
</summary>
    <author>
      <name>Roni Haecki</name>
    </author>
    <author>
      <name>Lukas Humbel</name>
    </author>
    <author>
      <name>Reto Achermann</name>
    </author>
    <author>
      <name>David Cock</name>
    </author>
    <author>
      <name>Daniel Schwyn</name>
    </author>
    <author>
      <name>Timothy Roscoe</name>
    </author>
    <link href="http://arxiv.org/abs/1911.08773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.08773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.11937v2</id>
    <updated>2020-03-14T14:58:13Z</updated>
    <published>2019-11-27T03:52:34Z</published>
    <title>Period Adaptation for Continuous Security Monitoring in Multicore
  Real-Time Systems</title>
    <summary>  We propose a design-time framework (named HYDRA-C) for integrating security
tasks into partitioned real-time systems (RTS) running on multicore platforms.
Our goal is to opportunistically execute security monitoring mechanisms in a
'continuous' manner -- i.e., as often as possible, across cores, to ensure that
security tasks run with as few interruptions as possible. Our framework will
allow designers to integrate security mechanisms without perturbing existing
real-time (RT) task properties or execution order. We demonstrate the framework
using a proof-of-concept implementation with intrusion detection mechanisms as
security tasks. We develop and use both, (a) a custom intrusion detection
system (IDS), as well as (b) Tripwire -- an open source data integrity checking
tool. These are implemented on a realistic rover platform designed using an ARM
multicore chip. We compare the performance of HYDRA-C with a state-of-the-art
RT security integration approach for multicore-based RTS and find that our
method can, on average, detect intrusions 19.05% faster without impacting the
performance of RT tasks.
</summary>
    <author>
      <name>Monowar Hasan</name>
    </author>
    <author>
      <name>Sibin Mohan</name>
    </author>
    <author>
      <name>Rodolfo Pellizzoni</name>
    </author>
    <author>
      <name>Rakesh B. Bobba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication DATE 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.11937v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11937v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.01161v1</id>
    <updated>2019-11-30T09:12:12Z</updated>
    <published>2019-11-30T09:12:12Z</published>
    <title>Exact Polynomial Time Algorithm for the Response Time Analysis of
  Harmonic Tasks with Constrained Release Jitter</title>
    <summary>  In some important application areas of hard real-time systems, preemptive
sporadic tasks with harmonic periods and constraint deadlines running upon a
uni-processor platform play an important role. We propose a new algorithm for
determining the exact worst-case response time for a task that has a lower
computational complexity (linear in the number of tasks) than the known
algorithm developed for the same system class. We also allow the task
executions to start delayed due to release jitter if they are within certain
value ranges. For checking if these constraints are met we define a constraint
programming problem that has a special structure and can be solved with
heuristic components in a time that is linear in the task number. If the check
determines the admissibility of the jitter values, the linear time algorithm
can be used to determine the worst-case response time also for jitter-aware
systems.
</summary>
    <author>
      <name>Thi Huyen Chau Nguyen</name>
    </author>
    <author>
      <name>Werner Grass</name>
    </author>
    <author>
      <name>Klaus Jansen</name>
    </author>
    <link href="http://arxiv.org/abs/1912.01161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.01161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.10959v2</id>
    <updated>2020-02-27T19:48:02Z</updated>
    <published>2019-12-23T16:36:23Z</published>
    <title>Virtual Gang based Scheduling of Real-Time Tasks on Multicore Platforms</title>
    <summary>  We propose a virtual-gang based parallel real-time task scheduling approach
for multicore platforms. Our approach is based on the notion of a virtual-gang,
which is a group of parallel real-time tasks that are statically linked and
scheduled together by a gang scheduler. We present a light-weight intra-gang
synchronization framework, called RTG-Sync, and virtual gang formation
algorithms that provide strong temporal isolation and high real-time
schedulability in scheduling real-time tasks on multicore. We evaluate our
approach both analytically, with generated tasksets against state-of-the-art
approaches, and empirically with a case-study involving real-world workloads on
a real embedded multicore platform. The results show that our approach provides
simple but powerful compositional analysis framework, achieves better analytic
schedulability, especially when the effect of interference is considered, and
is a practical solution for COTS multicore platforms.
</summary>
    <author>
      <name>Waqar Ali</name>
    </author>
    <author>
      <name>Rodolfo Pellizzoni</name>
    </author>
    <author>
      <name>Heechul Yun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.10959v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.10959v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.07515v1</id>
    <updated>2020-02-18T12:26:24Z</updated>
    <published>2020-02-18T12:26:24Z</published>
    <title>Characterizing Synchronous Writes in Stable Memory Devices</title>
    <summary>  Distributed algorithms that operate in the fail-recovery model rely on the
state stored in stable memory to guarantee the irreversibility of operations
even in the presence of failures. The performance of these algorithms lean
heavily on the performance of stable memory. Current storage technologies have
a defined performance profile: data is accessed in blocks of hundreds or
thousands of bytes, random access to these blocks is expensive and sequential
access is somewhat better. File system implementations hide some of the
performance limitations of the underlying storage devices using buffers and
caches. However, fail-recovery distributed algorithms bypass some of these
techniques and perform synchronous writes to be able to tolerate a failure
during the write itself. Assuming the distributed system designer is able to
buffer the algorithm's writes, we ask how buffer size and latency complement
each other. In this paper we start to answer this question by characterizing
the performance (throughput and latency) of typical stable memory devices using
a representative set of current file systems.
</summary>
    <author>
      <name>William B. Mingardi</name>
    </author>
    <author>
      <name>Gustavo M. D. Vieira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5753/wperformance.2019.6458</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5753/wperformance.2019.6458" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">WPerformance '19: Proceedings of the XVIII Computer and
  Communication Systems Workshop, SBC, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2002.07515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.07515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.05025v2</id>
    <updated>2020-05-01T21:24:00Z</updated>
    <published>2020-03-10T22:51:16Z</published>
    <title>Fissile Locks</title>
    <summary>  Classic test-and-test (TS) mutual exclusion locks are simple, and enjoy high
performance and low latency of ownership transfer under light or no contention.
However, they do not scale gracefully under high contention and do not provide
any admission order guarantees. Such concerns led to the development of
scalable queue-based locks, such as a recent Compact NUMA-aware (CNA) lock, a
variant of another popular queue-based MCS lock. CNA scales well under load and
provides certain admission guarantees, but has more complicated lock handover
operations than TS and incurs higher latencies at low contention. We propose
Fissile locks, which capture the most desirable properties of both TS and CNA.
A Fissile lock consists of two underlying locks: a TS lock, which serves as a
fast path, and a CNA lock, which serves as a slow path. The key feature of
Fissile locks is the ability of threads on the fast path to bypass threads
enqueued on the slow path, and acquire the lock with less overhead than CNA.
Bypass is bounded (by a tunable parameter) to avoid starvation and ensure
long-term fairness. The result is a highly scalable NUMA-aware lock with
progress guarantees that performs like TS at low contention and like CNA at
high contention.
</summary>
    <author>
      <name>Dave Dice</name>
    </author>
    <author>
      <name>Alex Kogan</name>
    </author>
    <link href="http://arxiv.org/abs/2003.05025v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.05025v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.05444v1</id>
    <updated>2020-03-11T05:00:53Z</updated>
    <published>2020-03-11T05:00:53Z</published>
    <title>Demand-based Scheduling of Mixed-Criticality Sporadic Tasks on One
  Processor</title>
    <summary>  Strategies that artificially tighten high-criticality task deadlines in
low-criticality behaviors have been successfully employed for scheduling
mixed-criticality systems. Although efficient scheduling algorithms have been
developed for implicit deadline task systems, the same is not true for more
general sporadic tasks. In this paper we develop a new demand-based
schedulability test for such general mixed-criticality task systems, in which
we collectively bound the low- and high-criticality demand of tasks. We show
that the new test strictly dominates the only other known demand-based test for
such systems. We also propose a new deadline tightening strategy based on this
test, and show through simulations that the strategy significantly outperforms
all known scheduling algorithms for a variety of sporadic task systems.
</summary>
    <author>
      <name>Arvind Easwaran</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/RTSS.2013.16</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/RTSS.2013.16" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">\copyright 2013 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Real-Time Systems Symposium (RTSS), Vancouver, Canada, 2013,
  pages 78-87</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.05444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.05444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.04760v1</id>
    <updated>2020-04-09T18:04:25Z</updated>
    <published>2020-04-09T18:04:25Z</published>
    <title>Efficient Kernel Object Management for Tiered Memory Systems with KLOC</title>
    <summary>  Software-controlled heterogeneous memory systems have the potential to
improve performance, efficiency, and cost tradeoffs in emerging systems.
Delivering on this promise requires an efficient operating system (OS)
mechanisms and policies for data management. Unfortunately, modern OSes do not
support efficient tiering of data between heterogeneous memories. While this
problem is known (and is being studied) for application-level data pages, the
question of how best to tier OS kernel objects has largely been ignored. We
show that careful kernel object management is vital to the performance of
software-controlled tiered memory systems. We find that the state-of-the-art OS
page management research leaves considerable performance on the table by
overlooking how best to tier, migrate, and manage kernel objects like inodes,
dentry caches, journal blocks, network socket buffers, etc., associated with
the filesystem and networking stack. In response, we characterize hotness,
reuse, and liveness properties of kernel objects to develop appropriate
tiering/migration mechanisms and policies. We evaluate our proposal using a
real-system emulation framework on large-scale workloads like RocksDB, Redis,
Cassandra, and Spark and achieve 1.4X to 4X higher throughput compared to the
prior art.
</summary>
    <author>
      <name>Sudarsun Kannan</name>
    </author>
    <author>
      <name>Yujie Ren</name>
    </author>
    <author>
      <name>Abhishek Bhatacharjee</name>
    </author>
    <link href="http://arxiv.org/abs/2004.04760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.04760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.04846v1</id>
    <updated>2020-04-09T23:07:44Z</updated>
    <published>2020-04-09T23:07:44Z</published>
    <title>$μ$Tiles: Efficient Intra-Process Privilege Enforcement of Memory
  Regions</title>
    <summary>  With the alarming rate of security advisories and privacy concerns on
connected devices, there is an urgent need for strong isolation guarantees in
resource-constrained devices that demand very lightweight solutions. However,
the status quo is that Unix-like operating systems do not offer privilege
separation inside a process. Lack of practical fine-grained
compartmentalization inside a shared address space leads to private data
leakage through applications' untrusted dependencies and compromised threads.
To this end, we propose $\mu$Tiles, a lightweight kernel abstraction and set of
security primitives based on mutual distrust for intra-process privilege
separation, memory protection, and secure multithreading. $\mu$Tiles takes
advantage of hardware support for virtual memory tagging (e.g., ARM memory
domains) to achieve significant performance gain while eliminating various
hardware limitations. Our results (based on OpenSSL, the Apache HTTP server,
and LevelDB) show that $\mu$Tiles is extremely lightweight (adds $\approx 10KB$
to kernel image) for IoT use cases. It adds negligible runtime overhead
($\approx 0.5\%-3.5\%$) and is easy to integrate with existing applications for
providing strong privilege separation.
</summary>
    <author>
      <name>Zahra Tarkhani</name>
    </author>
    <author>
      <name>Anil Madhavapeddy</name>
    </author>
    <link href="http://arxiv.org/abs/2004.04846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.04846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.09619v1</id>
    <updated>2020-04-20T20:35:12Z</updated>
    <published>2020-04-20T20:35:12Z</published>
    <title>Vilamb: Low Overhead Asynchronous Redundancy for Direct Access NVM</title>
    <summary>  Vilamb provides efficient asynchronous systemredundancy for direct access
(DAX) non-volatile memory (NVM) storage. Production storage deployments often
use system-redundancy in form of page checksums and cross-page parity.
State-of-the-art solutions for maintaining system-redundancy for DAX NVM either
incur a high performance overhead or require specialized hardware. The Vilamb
user-space library maintains system-redundancy with low overhead by delaying
and amortizing the system-redundancy updates over multiple data writes. As a
result, Vilamb provides 3--5x the throughput of the state-of-the-art software
solution at high operation rates. For applications that need system-redundancy
with high performance, and can tolerate some delaying of data redundancy,
Vilamb provides a tunable knob between performance and quicker redundancy. Even
with the delayed coverage, Vilamb increases the mean time to data loss due to
firmware-induced corruptions by up to two orders of magnitude in comparison to
maintaining no system-redundancy.
</summary>
    <author>
      <name>Rajat Kateja</name>
    </author>
    <author>
      <name>Andy Pavlo</name>
    </author>
    <author>
      <name>Gregory R. Ganger</name>
    </author>
    <link href="http://arxiv.org/abs/2004.09619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.09619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.01498v1</id>
    <updated>2020-05-04T14:04:43Z</updated>
    <published>2020-05-04T14:04:43Z</published>
    <title>Dim Silicon and the Case for Improved DVFS Policies</title>
    <summary>  Due to thermal and power supply limits, modern Intel CPUs reduce their
frequency when AVX2 and AVX-512 instructions are executed. As the CPUs wait for
670{\mu}s before increasing the frequency again, the performance of some
heterogeneous workloads is reduced. In this paper, we describe parallels
between this situation and dynamic power management as well as between the
policy implemented by these CPUs and fixed-timeout device shutdown policies. We
show that the policy implemented by Intel CPUs is not optimal and describe
potential better policies. In particular, we present a mechanism to classify
applications based on their likeliness to cause frequency reduction. Our
approach takes either the resulting classification information or information
provided by the application and generates hints for the DVFS policy. We show
that faster frequency changes based on these hints are able to improve
performance for a web server using the OpenSSL library.
</summary>
    <author>
      <name>Mathias Gottschlag</name>
    </author>
    <author>
      <name>Yussuf Khalil</name>
    </author>
    <author>
      <name>Frank Bellosa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.01498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.01498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.02547v2</id>
    <updated>2021-07-21T21:36:31Z</updated>
    <published>2020-05-06T01:07:29Z</published>
    <title>On Failure Diagnosis of the Storage Stack</title>
    <summary>  Diagnosing storage system failures is challenging even for professionals. One
example is the "When Solid State Drives Are Not That Solid" incident occurred
at Algolia data center, where Samsung SSDs were mistakenly blamed for failures
caused by a Linux kernel bug. With the system complexity keeps increasing, such
obscure failures will likely occur more often. As one step to address the
challenge, we present our on-going efforts called X-Ray. Different from
traditional methods that focus on either the software or the hardware, X-Ray
leverages virtualization to collects events across layers, and correlates them
to generate a correlation tree. Moreover, by applying simple rules, X-Ray can
highlight critical nodes automatically. Preliminary results based on 5 failure
cases shows that X-Ray can effectively narrow down the search space for
failures.
</summary>
    <author>
      <name>Duo Zhang</name>
    </author>
    <author>
      <name>Om Rameshwar Gatla</name>
    </author>
    <author>
      <name>Runzhou Han</name>
    </author>
    <author>
      <name>Mai Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/2005.02547v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.02547v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.00380v1</id>
    <updated>2020-05-30T22:39:46Z</updated>
    <published>2020-05-30T22:39:46Z</published>
    <title>Memory virtualization in virtualized systems: segmentation is better
  than paging</title>
    <summary>  The utilization of paging for virtual machine (VM) memory management is the
root cause of memory virtualization overhead. This paper shows that paging is
not necessary in the hypervisor. In fact, memory fragmentation, which explains
paging utilization, is not an issue in virtualized datacenters thanks to VM
memory demand patterns. Our solution Compromis, a novel Memory Management Unit,
uses direct segment for VM memory management combined with paging for VM's
processes. The paper presents a systematic methodology for implementing
Compromis in the hardware, the hypervisor and the datacenter scheduler.
Evaluation results show that Compromis outperforms the two popular memory
virtualization solutions: shadow paging and Extended Page Table by up to 30%
and 370% respectively.
</summary>
    <author>
      <name>Boris Teabe</name>
    </author>
    <author>
      <name>Peterson Yuhala</name>
    </author>
    <author>
      <name>Alain Tchana</name>
    </author>
    <author>
      <name>Fabien Hermenier</name>
    </author>
    <author>
      <name>Daniel Hagimont</name>
    </author>
    <author>
      <name>Gilles Muller</name>
    </author>
    <link href="http://arxiv.org/abs/2006.00380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.00380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.08966v2</id>
    <updated>2020-06-22T13:11:11Z</updated>
    <published>2020-06-16T07:45:22Z</published>
    <title>FastDrain: Removing Page Victimization Overheads in NVMe Storage Stack</title>
    <summary>  Host-side page victimizations can easily overflow the SSD internal buffer,
which interferes I/O services of diverse user applications thereby degrading
user-level experiences. To address this, we propose FastDrain, a co-design of
OS kernel and flash firmware to avoid the buffer overflow, caused by page
victimizations. Specifically, FastDrain can detect a triggering point where a
near-future page victimization introduces an overflow of the SSD internal
buffer. Our new flash firmware then speculatively scrubs the buffer space to
accommodate the requests caused by the page victimization. In parallel, our new
OS kernel design controls the traffic of page victimizations by considering the
target device buffer status, which can further reduce the risk of buffer
overflow. To secure more buffer spaces, we also design a latency-aware FTL,
which dumps the dirty data only to the fast flash pages. Our evaluation results
reveal that FastDrain reduces the 99th response time of user applications by
84%, compared to a conventional system.
</summary>
    <author>
      <name>Jie Zhang</name>
    </author>
    <author>
      <name>Miryeong Kwon</name>
    </author>
    <author>
      <name>Sanghyun Han</name>
    </author>
    <author>
      <name>Nam Sung Kim</name>
    </author>
    <author>
      <name>Mahmut Kandemir</name>
    </author>
    <author>
      <name>Myoungsoo Jung</name>
    </author>
    <link href="http://arxiv.org/abs/2006.08966v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.08966v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.00706v1</id>
    <updated>2020-07-01T19:13:26Z</updated>
    <published>2020-07-01T19:13:26Z</published>
    <title>DPCP-p: A Distributed Locking Protocol for Parallel Real-Time Tasks</title>
    <summary>  Real-time scheduling and locking protocols are fundamental facilities to
construct time-critical systems. For parallel real-time tasks, predictable
locking protocols are required when concurrent sub-jobs mutually exclusive
access to shared resources. This paper for the first time studies the
distributed synchronization framework of parallel real-time tasks, where both
tasks and global resources are partitioned to designated processors, and
requests to each global resource are conducted on the processor on which the
resource is partitioned. We extend the Distributed Priority Ceiling Protocol
(DPCP) for parallel tasks under federated scheduling, with which we proved that
a request can be blocked by at most one lower-priority request. We develop task
and resource partitioning heuristics and propose analysis techniques to safely
bound the task response times. Numerical evaluation (with heavy tasks on 8-,
16-, and 32-core processors) indicates that the proposed methods improve the
schedulability significantly compared to the state-of-the-art locking protocols
under federated scheduling.
</summary>
    <author>
      <name>Maolin Yang</name>
    </author>
    <author>
      <name>Zewei Chen</name>
    </author>
    <author>
      <name>Xu Jiang</name>
    </author>
    <author>
      <name>Nan Guan</name>
    </author>
    <author>
      <name>Hang Lei</name>
    </author>
    <link href="http://arxiv.org/abs/2007.00706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.00706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.08302v1</id>
    <updated>2020-07-16T12:43:15Z</updated>
    <published>2020-07-16T12:43:15Z</published>
    <title>Scheduling of Real-Time Tasks with Multiple Critical Sections in
  Multiprocessor Systems</title>
    <summary>  The performance of multiprocessor synchronization and locking protocols is a
key factor to utilize the computation power of multiprocessor systems under
real-time constraints. While multiple protocols have been developed in the past
decades, their performance highly depends on the task partition and
prioritization. The recently proposed Dependency Graph Approach showed its
advantages and attracted a lot of interest. It is, however, restricted to task
sets where each task has at most one critical section. In this paper, we remove
this restriction and demonstrate how to utilize algorithms for the classical
job shop scheduling problem to construct a dependency graph for tasks with
multiple critical sections. To show the applicability, we discuss the
implementation in Litmus^{RT} and report the overheads. Moreover, we provide
extensive numerical evaluations under different configurations, which in many
situations show significant improvement compared to the state-of-the-art.
</summary>
    <author>
      <name>Jian-Jia Chen</name>
    </author>
    <author>
      <name>Junjie Shi</name>
    </author>
    <author>
      <name>Georg von der Brüggen</name>
    </author>
    <author>
      <name>Niklas Ueter</name>
    </author>
    <link href="http://arxiv.org/abs/2007.08302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.08302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.11112v1</id>
    <updated>2020-07-21T22:01:00Z</updated>
    <published>2020-07-21T22:01:00Z</published>
    <title>DBOS: A Proposal for a Data-Centric Operating System</title>
    <summary>  Current operating systems are complex systems that were designed before
today's computing environments. This makes it difficult for them to meet the
scalability, heterogeneity, availability, and security challenges in current
cloud and parallel computing environments. To address these problems, we
propose a radically new OS design based on data-centric architecture: all
operating system state should be represented uniformly as database tables, and
operations on this state should be made via queries from otherwise stateless
tasks. This design makes it easy to scale and evolve the OS without
whole-system refactoring, inspect and debug system state, upgrade components
without downtime, manage decisions using machine learning, and implement
sophisticated security features. We discuss how a database OS (DBOS) can
improve the programmability and performance of many of today's most important
applications and propose a plan for the development of a DBOS proof of concept.
</summary>
    <author>
      <name>Michael Cafarella</name>
    </author>
    <author>
      <name>David DeWitt</name>
    </author>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Christos Kozyrakis</name>
    </author>
    <author>
      <name>Tim Kraska</name>
    </author>
    <author>
      <name>Michael Stonebraker</name>
    </author>
    <author>
      <name>Matei Zaharia</name>
    </author>
    <link href="http://arxiv.org/abs/2007.11112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.11112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.12112v1</id>
    <updated>2020-07-23T16:33:23Z</updated>
    <published>2020-07-23T16:33:23Z</published>
    <title>HeRTA: Heaviside Real-Time Analysis</title>
    <summary>  We investigate the mathematical properties of event bound functions as they
are used in the worst-case response time analysis and utilization tests. We
figure out the differences and similarities between the two approaches. Based
on this analysis, we derive a more general form do describe events and event
bounds. This new unified approach gives clear new insights in the investigation
of real-time systems, simplifies the models and will support algebraic proofs
in future work. In the end, we present a unified analysis which allows the
algebraic definition of any scheduler. Introducing such functions to the
real-time scheduling theory will lead two a more systematic way to integrate
new concepts and applications to the theory. Last but not least, we show how
the response time analysis in dynamic scheduling can be improved.
</summary>
    <author>
      <name>Frank Slomka</name>
    </author>
    <author>
      <name>Mohammadreza Sadeghi</name>
    </author>
    <link href="http://arxiv.org/abs/2007.12112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.12112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.08770v2</id>
    <updated>2021-05-23T18:41:00Z</updated>
    <published>2021-05-18T18:35:40Z</published>
    <title>Lightweight Robust Size Aware Cache Management</title>
    <summary>  Modern key-value stores, object stores, Internet proxy caches, as well as
Content Delivery Networks (CDN) often manage objects of diverse sizes, e.g.,
blobs, video files of different lengths, images with varying resolution, and
small documents. In such workloads, size-aware cache policies outperform
size-oblivious algorithms. Unfortunately, existing size-aware algorithms tend
to be overly complicated and computationally~expensive.
  Our work follows a more approachable pattern; we extend the prevalent
(size-oblivious) TinyLFU cache admission policy to handle variable sized items.
Implementing our approach inside two popular caching libraries only requires
minor changes. We show that our algorithms yield competitive or better
hit-ratios and byte hit-ratios compared to the state of the art size-aware
algorithms such as AdaptSize, LHD, LRB, and GDSF. Further, a runtime comparison
indicates that our implementation is faster by up to x3 compared to the best
alternative, i.e., it imposes much lower CPU overhead.
</summary>
    <author>
      <name>Gil Einziger</name>
    </author>
    <author>
      <name>Ohad Eytan</name>
    </author>
    <author>
      <name>Roy Friedman</name>
    </author>
    <author>
      <name>Benjamin Manes</name>
    </author>
    <link href="http://arxiv.org/abs/2105.08770v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.08770v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.14651v2</id>
    <updated>2022-10-27T22:31:58Z</updated>
    <published>2021-06-23T23:44:27Z</published>
    <title>MAGE: Nearly Zero-Cost Virtual Memory for Secure Computation</title>
    <summary>  Secure Computation (SC) is a family of cryptographic primitives for computing
on encrypted data in single-party and multi-party settings. SC is being
increasingly adopted by industry for a variety of applications. A significant
obstacle to using SC for practical applications is the memory overhead of the
underlying cryptography. We develop MAGE, an execution engine for SC that
efficiently runs SC computations that do not fit in memory. We observe that,
due to their intended security guarantees, SC schemes are inherently oblivious
-- their memory access patterns are independent of the input data. Using this
property, MAGE calculates the memory access pattern ahead of time and uses it
to produce a memory management plan. This formulation of memory management,
which we call memory programming, is a generalization of paging that allows
MAGE to provide a highly efficient virtual memory abstraction for SC. MAGE
outperforms the OS virtual memory system by up to an order of magnitude, and in
many cases, runs SC computations that do not fit in memory at nearly the same
speed as if the underlying machines had unbounded physical memory to fit the
entire computation.
</summary>
    <author>
      <name>Sam Kumar</name>
    </author>
    <author>
      <name>David E. Culler</name>
    </author>
    <author>
      <name>Raluca Ada Popa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages; Accepted to OSDI 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.14651v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.14651v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.11666v1</id>
    <updated>2021-09-23T22:10:26Z</updated>
    <published>2021-09-23T22:10:26Z</published>
    <title>SLO beyond the Hardware Isolation Limits</title>
    <summary>  Performance isolation is a keystone for SLO guarantees with shared resources
in cloud and datacenter environments. To meet SLO requirements, the state of
the art relies on hardware QoS support (e.g., Intel RDT) to allocate shared
resources such as last-level caches and memory bandwidth for co-located
latency-critical applications. As a result, the number of latency-critical
applications that can be deployed on a physical machine is bounded by the
hardware allocation capability. Unfortunately, such hardware capability is very
limited. For example, Intel Xeon E5 v3 processors support at most four
partitions for last-level caches, i.e., at most four applications can have
dedicated resource allocation. This paper discusses the feasibility and
unexplored challenges of providing SLO guarantees beyond the limits of hardware
capability. We present CoCo to show the feasibility and the benefits. CoCo
schedules applications to time-share interference-free partitions as a
transparent software layer. Our evaluation shows that CoCo outperforms
non-partitioned and round-robin approaches by up to 9x and 1.2x.
</summary>
    <author>
      <name>Haoran Qiu</name>
    </author>
    <author>
      <name>Yongzhou Chen</name>
    </author>
    <author>
      <name>Tianyin Xu</name>
    </author>
    <author>
      <name>Zbigniew T. Kalbarczyk</name>
    </author>
    <author>
      <name>Ravishankar K. Iyer</name>
    </author>
    <link href="http://arxiv.org/abs/2109.11666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.11666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.08303v2</id>
    <updated>2022-03-15T15:50:04Z</updated>
    <published>2021-10-15T18:22:10Z</published>
    <title>Minimum Viable Device Drivers for ARM TrustZone</title>
    <summary>  While TrustZone can isolate IO hardware, it lacks drivers for modern IO
devices. Rather than porting drivers, we propose a novel approach to deriving
minimum viable drivers: developers exercise a full driver and record the
driver/device interactions; the processed recordings, dubbed driverlets, are
replayed in the TEE at run time to access IO devices.
  Driverlets address two key challenges: correctness and expressiveness, for
which they build on a key construct called interaction template. The
interaction template ensures faithful reproduction of recorded IO jobs (albeit
on new IO data); it accepts dynamic input values; it tolerates nondeterministic
device behaviors.
  We demonstrate driverlets on a series of sophisticated devices, making them
accessible to TrustZone for the first time to our knowledge. Our experiments
show that driverlets are secure, easy to build, and incur acceptable overhead
(1.4x -2.7x compared to native drivers). Driverlets fill a critical gap in the
TrustZone TEE, realizing its long-promised vision of secure IO.
</summary>
    <author>
      <name>Liwei Guo</name>
    </author>
    <author>
      <name>Felix Xiaozhu Lin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3492321.3519565</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3492321.3519565" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Eurosys 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.08303v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.08303v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.09725v1</id>
    <updated>2021-11-18T14:35:14Z</updated>
    <published>2021-11-18T14:35:14Z</published>
    <title>EDF-Like Scheduling for Self-Suspending Real-Time Tasks</title>
    <summary>  In real-time systems, schedulability tests are utilized to provide timing
guarantees. However, for self-suspending task sets, current suspension-aware
schedulability tests are limited to Task-Level Fixed-Priority~(TFP) scheduling
or Earliest-Deadline-First~(EDF) with constrained-deadline task systems. In
this work we provide a unifying schedulability test for the uniprocessor
version of Global EDF-Like (GEL) schedulers and arbitrary-deadline task sets. A
large body of existing scheduling algorithms can be considered as EDF-Like,
such as EDF, First-In-First-Out~(FIFO), Earliest-Quasi-Deadline-First~(EQDF)
and Suspension-Aware EDF~(SAEDF). Therefore, the unifying schedulability test
is applicable to those algorithms. Moreover, the schedulability test can be
applied to TFP scheduling as well.
  Our analysis is the first suspension-aware schedulability test applicable to
arbitrary-deadline sporadic real-time task systems under Job-Level
Fixed-Priority (JFP) scheduling, such as EDF. Moreover, it is the first
unifying suspension-aware schedulability test framework that covers a wide
range of scheduling algorithms. Through numerical simulations, we show that the
schedulability test outperforms the state of the art for EDF under
constrained-deadline scenarios. Moreover, we demonstrate the performance of
different configurations under EQDF and SAEDF.
</summary>
    <author>
      <name>Mario Günzel</name>
    </author>
    <author>
      <name>Kuan-Hsun Chen</name>
    </author>
    <author>
      <name>Jian-Jia Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2111.09725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.09725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.06810v1</id>
    <updated>2021-12-13T17:13:00Z</updated>
    <published>2021-12-13T17:13:00Z</published>
    <title>Bento and the Art of Repeated Research</title>
    <summary>  Bento provides a new approach to developing file systems, with safety and
high-velocity development in mind. This is achieved by using Rust, a modern and
memory-safe systems programming language, and by providing a framework to run a
single file system implementation in kernel space with the VFS or in user space
with FUSE. In this paper, the benchmarking experiments from the Bento paper are
repeated. We fail to exactly reproduce the results of the Bento paper, but more
or less find the same patterns albeit with more outlying results. Additionally
we unsuccessfully run a standardized test suite, and expand the set of
experiments with latency benchmarks and throughput benchmarks using a RAM block
device. The latency benchmarks show that ext4 with journaling consistently
outperforms Bento-fs and the RAM throughput benchmarks show no additional
consistent performance pattern. During this experimentation, a set of 12 bugs
was encountered and analyzed. We find that the ratio of memory related bugs is
lower than other systems programming projects that use C as opposed to Rust,
thus supporting the claims of the Bento framework.
</summary>
    <author>
      <name>Peter-Jan Gootzen</name>
    </author>
    <author>
      <name>Animesh Trivedi</name>
    </author>
    <link href="http://arxiv.org/abs/2112.06810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.06810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.07010v1</id>
    <updated>2021-12-13T21:04:26Z</updated>
    <published>2021-12-13T21:04:26Z</published>
    <title>Slowing Down for Performance and Energy: An OS-Centric Study in Network
  Driven Workloads</title>
    <summary>  This paper studies three fundamental aspects of an OS that impact the
performance and energy efficiency of network processing: 1) batching, 2)
processor energy settings, and 3) the logic and instructions of the OS
networking paths. A network device's interrupt delay feature is used to induce
batching and processor frequency is manipulated to control the speed of
instruction execution. A baremetal library OS is used to explore OS path
specialization. This study shows how careful use of batching and interrupt
delay results in 2X energy and performance improvements across different
workloads. Surprisingly, we find polling can be made energy efficient and can
result in gains up to 11X over baseline Linux. We developed a methodology and a
set of tools to collect system data in order to understand how energy is
impacted at a fine-grained granularity. This paper identifies a number of other
novel findings that have implications in OS design for networked applications
and suggests a path forward to consider energy as a focal point of systems
research.
</summary>
    <author>
      <name>Han Dong</name>
    </author>
    <author>
      <name>Sanjay Arora</name>
    </author>
    <author>
      <name>Yara Awad</name>
    </author>
    <author>
      <name>Tommy Unger</name>
    </author>
    <author>
      <name>Orran Krieger</name>
    </author>
    <author>
      <name>Jonathan Appavoo</name>
    </author>
    <link href="http://arxiv.org/abs/2112.07010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.07010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.09615v2</id>
    <updated>2022-10-12T05:50:01Z</updated>
    <published>2022-03-17T21:06:15Z</published>
    <title>Canvas: Isolated and Adaptive Swapping for Multi-Applications on Remote
  Memory</title>
    <summary>  Remote memory techniques for datacenter applications have recently gained a
great deal of popularity. Existing remote memory techniques focus on the
efficiency of a single application setting only. However, when multiple
applications co-run on a remote-memory system, significant interference could
occur, resulting in unexpected slowdowns even if the same amounts of physical
resources are granted to each application. This slowdown stems from massive
sharing in applications' swap data paths. Canvas is a redesigned swap system
that fully isolates swap paths for remote-memory applications. Canvas allows
each application to possess its dedicated swap partition, swap cache,
prefetcher, and RDMA bandwidth. Swap isolation lays a foundation for adaptive
optimization techniques based on each application's own access patterns and
needs. We develop three such techniques: (1) adaptive swap entry allocation,
(2) semantics-aware prefetching, and (3) two-dimensional RDMA scheduling. A
thorough evaluation with a set of widely-deployed applications demonstrates
that Canvas minimizes performance variation and dramatically reduces
performance degradation.
</summary>
    <author>
      <name>Chenxi Wang</name>
    </author>
    <author>
      <name>Yifan Qiao</name>
    </author>
    <author>
      <name>Haoran Ma</name>
    </author>
    <author>
      <name>Shi Liu</name>
    </author>
    <author>
      <name>Yiying Zhang</name>
    </author>
    <author>
      <name>Wenguang Chen</name>
    </author>
    <author>
      <name>Ravi Netravali</name>
    </author>
    <author>
      <name>Miryung Kim</name>
    </author>
    <author>
      <name>Guoqing Harry Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2203.09615v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.09615v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.10225v3</id>
    <updated>2022-09-17T01:52:44Z</updated>
    <published>2022-03-19T02:49:55Z</published>
    <title>No Provisioned Concurrency: Fast RDMA-codesigned Remote Fork for
  Serverless Computing</title>
    <summary>  Serverless platforms essentially face a tradeoff between container startup
time and provisioned concurrency (i.e., cached instances), which is further
exaggerated by the frequent need for remote container initialization. This
paper presents MITOSIS, an operating system primitive that provides fast remote
fork, which exploits a deep codesign of the OS kernel with RDMA. By leveraging
the fast remote read capability of RDMA and partial state transfer across
serverless containers, MITOSIS bridges the performance gap between local and
remote container initialization. MITOSIS is the first to fork over 10,000 new
containers from one instance across multiple machines within a second, while
allowing the new containers to efficiently transfer the pre-materialized states
of the forked one. We have implemented MITOSIS on Linux and integrated it with
FN, a popular serverless platform. Under load spikes in real-world serverless
workloads, MITOSIS reduces the function tail latency by 89% with orders of
magnitude lower memory usage. For serverless workflow that requires state
transfer, MITOSIS improves its execution time by 86%.
</summary>
    <author>
      <name>Xingda Wei</name>
    </author>
    <author>
      <name>Fangming Lu</name>
    </author>
    <author>
      <name>Tianxia Wang</name>
    </author>
    <author>
      <name>Jinyu Gu</name>
    </author>
    <author>
      <name>Yuhan Yang</name>
    </author>
    <author>
      <name>Rong Chen</name>
    </author>
    <author>
      <name>Haibo Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in OSDI'23</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.10225v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.10225v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.03289v1</id>
    <updated>2022-04-07T08:38:37Z</updated>
    <published>2022-04-07T08:38:37Z</published>
    <title>Persistent Memory Objects: Fast and Easy Crash Consistency for
  Persistent Memory</title>
    <summary>  DIMM-compatible persistent memory unites memory and storage. Prior works
utilize persistent memory either by combining the filesystem with direct access
on memory mapped files or by managing it as a collection of objects while
abolishing the POSIX abstraction. In contrast, we propose retaining the POSIX
abstraction and extending it to provide support for persistent memory, using
Persistent Memory Objects (PMOs). In this work, we design and implement PMOs, a
crash-consistent abstraction for managing persistent memory. We introduce
psync, a single system call, that a programmer can use to specify crash
consistency points in their code, without needing to orchestrate durability
explicitly. When rendering data crash consistent, our design incurs a overhead
of $\approx 25\%$ and $\approx 21\%$ for parallel workloads and FileBench,
respectively, compared to a system without crash consistency. Compared to
NOVA-Fortis, our design provides a speedup of $\approx 1.67\times$ and $\approx
3\times$ for the two set of benchmarks, respectively.
</summary>
    <author>
      <name>Derrick Greenspan</name>
    </author>
    <author>
      <name>Naveed Ul Mustafa</name>
    </author>
    <author>
      <name>Zoran Kolega</name>
    </author>
    <author>
      <name>Mark Heinrich</name>
    </author>
    <author>
      <name>Yan Solihin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.03289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.03289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.06066v1</id>
    <updated>2022-04-12T20:03:47Z</updated>
    <published>2022-04-12T20:03:47Z</published>
    <title>Finding and Analyzing Crash-Consistency Bugs in Persistent-Memory File
  Systems</title>
    <summary>  We present a study of crash-consistency bugs in persistent-memory (PM) file
systems and analyze their implications for file-system design and testing crash
consistency. We develop FlyTrap, a framework to test PM file systems for
crash-consistency bugs. FlyTrap discovered 18 new bugs across four PM file
systems; the bugs have been confirmed by developers and many have been already
fixed. The discovered bugs have serious consequences such as breaking the
atomicity of rename or making the file system unmountable. We present a
detailed study of the bugs we found and discuss some important lessons from
these observations. For instance, one of our findings is that many of the bugs
are due to logic errors, rather than errors in using flushes or fences; this
has important applications for future work on testing PM file systems. Another
key finding is that many bugs arise from attempts to improve efficiency by
performing metadata updates in-place and that recovery code that deals with
rebuilding in-DRAM state is a significant source of bugs. These observations
have important implications for designing and testing PM file systems. Our code
is available at https://github.com/utsaslab/flytrap .
</summary>
    <author>
      <name>Hayley LeBlanc</name>
    </author>
    <author>
      <name>Shankara Pailoor</name>
    </author>
    <author>
      <name>Isil Dillig</name>
    </author>
    <author>
      <name>James Bornholt</name>
    </author>
    <author>
      <name>Vijay Chidambaram</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.06066v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.06066v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.2786v1</id>
    <updated>2007-05-19T00:02:24Z</updated>
    <published>2007-05-19T00:02:24Z</published>
    <title>Virtualization: A double-edged sword</title>
    <summary>  Virtualization became recently a hot topic once again, after being dormant
for more than twenty years. In the meantime, it has been almost forgotten, that
virtual machines are not so perfect isolating environments as it seems, when
looking at the principles. These lessons were already learnt earlier when the
first virtualized systems have been exposed to real life usage.
  Contemporary virtualization software enables instant creation and destruction
of virtual machines on a host, live migration from one host to another,
execution history manipulation, etc. These features are very useful in
practice, but also causing headaches among security specialists, especially in
current hostile network environments.
  In the present contribution we discuss the principles, potential benefits and
risks of virtualization in a deja vu perspective, related to previous
experiences with virtualization in the mainframe era.
</summary>
    <author>
      <name>Joachim J. Wlodarz</name>
    </author>
    <link href="http://arxiv.org/abs/0705.2786v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.2786v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.2365v1</id>
    <updated>2008-03-16T18:24:13Z</updated>
    <published>2008-03-16T18:24:13Z</published>
    <title>SAFIUS - A secure and accountable filesystem over untrusted storage</title>
    <summary>  We describe SAFIUS, a secure accountable file system that resides over an
untrusted storage. SAFIUS provides strong security guarantees like
confidentiality, integrity, prevention from rollback attacks, and
accountability. SAFIUS also enables read/write sharing of data and provides the
standard UNIX-like interface for applications. To achieve accountability with
good performance, it uses asynchronous signatures; to reduce the space required
for storing these signatures, a novel signature pruning mechanism is used.
SAFIUS has been implemented on a GNU/Linux based system modifying OpenGFS.
Preliminary performance studies show that SAFIUS has a tolerable overhead for
providing secure storage: while it has an overhead of about 50% of OpenGFS in
data intensive workloads (due to the overhead of performing
encryption/decryption in software), it is comparable (or better in some cases)
to OpenGFS in metadata intensive workloads.
</summary>
    <author>
      <name>V Sriram</name>
    </author>
    <author>
      <name>Ganesh Narayan</name>
    </author>
    <author>
      <name>K Gopinath</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SISW.2007.7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SISW.2007.7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11pt, 12 pages, 16 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Fourth International IEEE Security in Storage Workshop, 2007 -
  SISW '07. Publication Date: 27-27 Sept. 2007 On page(s): 34-45</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.2365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.2365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6; D.4.2; C.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.3632v1</id>
    <updated>2008-03-25T20:52:17Z</updated>
    <published>2008-03-25T20:52:17Z</published>
    <title>Void Traversal for Guaranteed Delivery in Geometric Routing</title>
    <summary>  Geometric routing algorithms like GFG (GPSR) are lightweight, scalable
algorithms that can be used to route in resource-constrained ad hoc wireless
networks. However, such algorithms run on planar graphs only. To efficiently
construct a planar graph, they require a unit-disk graph. To make the topology
unit-disk, the maximum link length in the network has to be selected
conservatively. In practical setting this leads to the designs where the node
density is rather high. Moreover, the network diameter of a planar subgraph is
greater than the original graph, which leads to longer routes. To remedy this
problem, we propose a void traversal algorithm that works on arbitrary
geometric graphs. We describe how to use this algorithm for geometric routing
with guaranteed delivery and compare its performance with GFG.
</summary>
    <author>
      <name>Mikhail Nesterenko</name>
    </author>
    <author>
      <name>Adnan Vora</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MAHSS.2005.1542862</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MAHSS.2005.1542862" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 2nd IEEE International Conference on Mobile Ad-hoc and Sensor
  Systems (MASS 2005), Washington, DC, November, 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.3632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.3632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.2; C.2.1; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.4308v2</id>
    <updated>2008-04-07T07:55:38Z</updated>
    <published>2008-03-30T09:26:38Z</published>
    <title>Discrete Frequency Selection of Frame-Based Stochastic Real-Time Tasks</title>
    <summary>  Energy-efficient real-time task scheduling has been actively explored in the
past decade. Different from the past work, this paper considers schedulability
conditions for stochastic real-time tasks. A schedulability condition is first
presented for frame-based stochastic real-time tasks, and several algorithms
are also examined to check the schedulability of a given strategy. An approach
is then proposed based on the schedulability condition to adapt a
continuous-speed-based method to a discrete-speed system. The approach is able
to stay as close as possible to the continuous-speed-based method, but still
guaranteeing the schedulability. It is shown by simulations that the energy
saving can be more than 20% for some system configurations
</summary>
    <author>
      <name>Vandy Berten</name>
    </author>
    <author>
      <name>Chi-Ju Chang</name>
    </author>
    <author>
      <name>Tei-Wei Kuo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0803.4308v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.4308v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1736v1</id>
    <updated>2011-05-09T17:29:03Z</updated>
    <published>2011-05-09T17:29:03Z</published>
    <title>Priority Based Dynamic Round Robin (PBDRR) Algorithm with Intelligent
  Time Slice for Soft Real Time Systems</title>
    <summary>  In this paper, a new variant of Round Robin (RR) algorithm is proposed which
is suitable for soft real time systems. RR algorithm performs optimally in
timeshared systems, but it is not suitable for soft real time systems. Because
it gives more number of context switches, larger waiting time and larger
response time. We have proposed a novel algorithm, known as Priority Based
Dynamic Round Robin Algorithm(PBDRR),which calculates intelligent time slice
for individual processes and changes after every round of execution. The
proposed scheduling algorithm is developed by taking dynamic time quantum
concept into account. Our experimental results show that our proposed algorithm
performs better than algorithm in [8] in terms of reducing the number of
context switches, average waiting time and average turnaround time.
</summary>
    <author>
      <name>Rakesh Mohanty</name>
    </author>
    <author>
      <name>H. S. Behera</name>
    </author>
    <author>
      <name>Khusbu Patwari</name>
    </author>
    <author>
      <name>Monisha Dash</name>
    </author>
    <author>
      <name>M. Lakshmi Prasanna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Computer Science and
  Applications(IJACSA), Vol. 2 No. 2, February 2011 2011, 46-50</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1105.1736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1811v1</id>
    <updated>2011-05-09T22:26:15Z</updated>
    <published>2011-05-09T22:26:15Z</published>
    <title>User Mode Memory Page Allocation: A Silver Bullet For Memory Allocation?</title>
    <summary>  This paper proposes a novel solution: the elimination of paged virtual memory
and partial outsourcing of memory page allocation and manipulation from the
operating system kernel into the individual process' user space - a user mode
page allocator - which allows an application to have direct, bare metal access
to the page mappings used by the hardware Memory Management Unit (MMU) for its
part of the overall address space. A user mode page allocator based emulation
of the mmap() abstraction layer of dlmalloc is then benchmarked against the
traditional kernel mode implemented mmap() in a series of synthetic Monte-Carlo
and real world application settings. Given the superb synthetic and positive
real world results from the profiling conducted, this paper proposes that with
proper operating system and API support one could gain a further order higher
performance again while keeping allocator performance invariant to the amount
of memory being allocated or freed i.e. a 100x performance improvement or more
in some common use cases. It is rare that through a simple and easy to
implement API and operating system structure change one can gain a Silver
Bullet with the potential for a second one.
</summary>
    <author>
      <name>Niall Douglas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages. Rejected from ISMM11</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.1811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.5055v2</id>
    <updated>2011-09-07T13:42:51Z</updated>
    <published>2011-05-25T15:02:01Z</published>
    <title>A faster exact multiprocessor schedulability test for sporadic tasks</title>
    <summary>  Baker and Cirinei introduced an exact but naive algorithm, based on solving a
state reachability problem in a finite automaton, to check whether sets of
sporadic hard real-time tasks are schedulable on identical multiprocessor
platforms. However, the algorithm suffered from poor performance due to the
exponential size of the automaton relative to the size of the task set. In this
paper, we successfully apply techniques developed by the formal verification
community, specifically antichain algorithms, by defining and proving the
correctness of a simulation relation on Baker and Cirinei's automaton. We show
our improved algorithm yields dramatically improved performance for the
schedulability test and opens for many further improvements.
</summary>
    <author>
      <name>Markus Lindström</name>
    </author>
    <author>
      <name>Gilles Geeraerts</name>
    </author>
    <author>
      <name>Joël Goossens</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.5055v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.5055v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.4451v2</id>
    <updated>2012-02-17T20:40:00Z</updated>
    <published>2011-12-19T20:07:54Z</published>
    <title>What is an OS?</title>
    <summary>  While the engineering of operating systems is well understood, their formal
structure and properties are not. The latter needs a clear definition of the
purpose of an OS and an identification of the core. In this paper I offer
definitions of the OS, processes and files, and present a few useful
principles. The principles allow us to identify work like closure and
continuation algorithms, in programming languages that is useful for the OS
problem. The definitions and principles should yield a symbolic, albeit
semiquantitative, framework that encompasses practice. Towards that end I
specialise the definitions to describe conventional OSes and identify the core
operations for a single computer OS that can be used to express their
algorithms. The assumptions underlying the algorithms offer the design space
framework. The paging and segmentation algorithms for conventional OSes are
extracted from the framework as a check. Among the insights the emerge is that
an OS is a constructive proof of equivalence between models of computation.
Clear and useful definitions and principles are the first step towards a fully
quantitative structure of an OS.
</summary>
    <author>
      <name>Abhijat Vichare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Major changes: Improvised the discussion of the implicit assumptions,
  added a sketch of a theory of an OS, and added a figure. Comments welcome. 32
  pages, 5 figures. Submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.4451v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.4451v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.1039v1</id>
    <updated>2012-10-03T09:15:19Z</updated>
    <published>2012-10-03T09:15:19Z</published>
    <title>JooFlux: Hijacking Java 7 InvokeDynamic To Support Live Code
  Modifications</title>
    <summary>  Changing functional and non-functional software implementation at runtime is
useful and even sometimes critical both in development and production
environments. JooFlux is a JVM agent that allows both the dynamic replacement
of method implementations and the application of aspect advices. It works by
doing bytecode transformation to take advantage of the new invokedynamic
instruction added in Java SE 7 to help implementing dynamic languages for the
JVM. JooFlux can be managed using a JMX agent so as to operate dynamic
modifications at runtime, without resorting to a dedicated domain-specific
language. We compared JooFlux with existing AOP platforms and dynamic
languages. Results demonstrate that JooFlux performances are close to the Java
ones --- with most of the time a marginal overhead, and sometimes a gain ---
where AOP platforms and dynamic languages present significant overheads. This
paves the way for interesting future evolutions and applications of JooFlux.
</summary>
    <author>
      <name>Julien Ponge</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CITI</arxiv:affiliation>
    </author>
    <author>
      <name>Frédéric Le Mouël</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CITI</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1210.1039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.1039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.1316v1</id>
    <updated>2013-06-06T06:57:57Z</updated>
    <published>2013-06-06T06:57:57Z</published>
    <title>Partitioned scheduling of multimode multiprocessor real-time systems
  with temporal isolation</title>
    <summary>  We consider the partitioned scheduling problem of multimode real-time systems
upon identical multiprocessor platforms. During the execution of a multimode
system, the system can change from one mode to another such that the current
task set is replaced with a new one. In this paper, we consider a synchronous
transition protocol in order to take into account mode-independent tasks, i.e.,
tasks of which the execution pattern must not be jeopardized by the mode
changes. We propose two methods for handling mode changes in partitioned
scheduling. The first method is offline/optimal and computes a static
allocation of tasks schedulable and respecting both tasks and transition
deadlines (if any). The second approach is subject to a sufficient condition in
order to ensure online First Fit based allocation to satisfy the timing
constraints.
</summary>
    <author>
      <name>Joël Goossens</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ULB</arxiv:affiliation>
    </author>
    <author>
      <name>Pascal Richard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIAS/Ensma and Université de Poitiers</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1306.1316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.1316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.02258v1</id>
    <updated>2017-11-07T02:09:09Z</updated>
    <published>2017-11-07T02:09:09Z</published>
    <title>Barrier Enabled IO Stack for Flash Storage</title>
    <summary>  This work is dedicated to eliminating the overhead of guaranteeing the
storage order in modern IO stack. The existing block device adopts
prohibitively expensive resort in ensuring the storage order among write
requests: interleaving successive write requests with transfer and flush.
Exploiting the cache barrier command for the Flash storage, we overhaul the IO
scheduler, the dispatch module and the filesystem so that these layers are
orchestrated to preserve the ordering condition imposed by the application can
be delivered to the storage. Key ingredients of Barrier Enabled IO stack are
Epoch based IO scheduling, Order Preserving Dispatch, and Dual Mode Journaling.
Barrier enabled IO stack successfully eliminates the root cause of excessive
overhead in enforcing the storage order. Dual Mode Journaling in BarrierFS
dedicates the separate threads to effectively decouple the control plane and
data plane of the journal commit. We implement Barrier Enabled IO Stack in
server as well as in mobile platform. SQLite performance increases by 270% and
75%, in server and in smartphone, respectively. Relaxing the durability of a
transaction, SQLite performance and MySQL performance increases as much as by
73X and by 43X, respectively, in server storage.
</summary>
    <author>
      <name>Youjip Won</name>
    </author>
    <author>
      <name>Jaemin Jung</name>
    </author>
    <author>
      <name>Gyeongyeol Choi</name>
    </author>
    <author>
      <name>Joontaek Oh</name>
    </author>
    <author>
      <name>Seongbae Son</name>
    </author>
    <author>
      <name>Jooyoung Hwang</name>
    </author>
    <author>
      <name>Sangyeun Cho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 15 figures, 71 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.02258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.08139v1</id>
    <updated>2017-11-22T05:47:15Z</updated>
    <published>2017-11-22T05:47:15Z</published>
    <title>Implementation of an Android Framework for USB storage access without
  root rights</title>
    <summary>  This bachelor thesis describes the implementation of an Android framework to
access mass storage devices over the USB interface of a smartphone. First the
basics of USB (i.e. interfaces, endpoints and USB On the go) and accessing USB
devices via the official Android API are discussed. Next the USB mass storage
class is explained, which was de- signed by the USB-IF to access mobile mass
storage like USB pen drives or external HDDs. For communication with mass
storage devices, most important are the bulk-only transfer and the SCSI
transparent command set. Furthermore file systems, for accessing directo- ries
and files, are described. This thesis focuses on the FAT32 file system from
Microsoft, because it is the most commonly used file system on such devices.
After the theory part it is time to look at the implementation of the
framework. In this section, the first concern is the purpose in general. Then
the architecture of the framework and the actual implementation are presented.
Important parts are discussed in detail. The thesis finishes with an overview
of the test results on various Android devices, a short conclusion and an
outlook to future developments. Moreover the current status of the developed
framework is visualized.
</summary>
    <author>
      <name>Magnus Jahnen</name>
    </author>
    <link href="http://arxiv.org/abs/1711.08139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.08139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02514v1</id>
    <updated>2018-05-04T07:36:09Z</updated>
    <published>2018-05-04T07:36:09Z</published>
    <title>An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM
  Memory Architecture</title>
    <summary>  With the emergence of Non-Volatile Memories (NVMs) and their shortcomings
such as limited endurance and high power consumption in write requests, several
studies have suggested hybrid memory architecture employing both Dynamic Random
Access Memory (DRAM) and NVM in a memory system. By conducting a comprehensive
experiments, we have observed that such studies lack to consider very important
aspects of hybrid memories including the effect of: a) data migrations on
performance, b) data migrations on power, and c) the granularity of data
migration. This paper presents an efficient data migration scheme at the
Operating System level in a hybrid DRAMNVM memory architecture. In the proposed
scheme, two Least Recently Used (LRU) queues, one for DRAM section and one for
NVM section, are used for the sake of data migration. With careful
characterization of the workloads obtained from PARSEC benchmark suite, the
proposed scheme prevents unnecessary migrations and only allows migrations
which benefits the system in terms of power and performance. The experimental
results show that the proposed scheme can reduce the power consumption up to
79% compared to DRAM-only memory and up to 48% compared to the state-of-the art
techniques.
</summary>
    <author>
      <name>Reza Salkhordeh</name>
    </author>
    <author>
      <name>Hossein Asadi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">DATE 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.02514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.07723v1</id>
    <updated>2018-12-19T01:18:05Z</updated>
    <published>2018-12-19T01:18:05Z</published>
    <title>Modeling Processor Idle Times in MPSoC Platforms to Enable Integrated
  DPM, DVFS, and Task Scheduling Subject to a Hard Deadline</title>
    <summary>  Energy efficiency is one of the most critical design criteria for modern
embedded systems such as multiprocessor system-on-chips (MPSoCs). Dynamic
voltage and frequency scaling (DVFS) and dynamic power management (DPM) are two
major techniques for reducing energy consumption in such embedded systems.
Furthermore, MPSoCs are becoming more popular for many real-time applications.
One of the challenges of integrating DPM with DVFS and task scheduling of
real-time applications on MPSoCs is the modeling of idle intervals on these
platforms. In this paper, we present a novel approach for modeling idle
intervals in MPSoC platforms which leads to a mixed integer linear programming
(MILP) formulation integrating DPM, DVFS, and task scheduling of periodic task
graphs subject to a hard deadline. We also present a heuristic approach for
solving the MILP and compare its results with those obtained from solving the
MILP.
</summary>
    <author>
      <name>Amirhossein Esmaili</name>
    </author>
    <author>
      <name>Mahdi Nazemi</name>
    </author>
    <author>
      <name>Massoud Pedram</name>
    </author>
    <link href="http://arxiv.org/abs/1812.07723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.07723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.00834v1</id>
    <updated>2019-06-03T14:32:03Z</updated>
    <published>2019-06-03T14:32:03Z</published>
    <title>Cache Contention on Multicore Systems: An Ontology-based Approach</title>
    <summary>  Multicore processors have proved to be the right choice for both desktop and
server systems because it can support high performance with an acceptable
budget expenditure. In this work, we have compared several works in cache
contention and found that such works have identified several techniques for
cache contention other than cache size including FSB, Memory Controller and
prefetching hardware. We found that Distributed Intensity Online (DIO) is a
very promising cache contention algorithm since it can achieve up to 2% from
the optimal technique. Moreover, we propose a new framework for cache
contention based on resource ontologies. In which ontologies instances will be
used for communication between diverse processes instead of grasping schedules
based on hardware.
</summary>
    <author>
      <name>Maruthi Rohit Ayyagari</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V67I5P110</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V67I5P110" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures. International Journal of Engineering Trends and
  Technology 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.00834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.07124v1</id>
    <updated>2019-06-17T16:49:56Z</updated>
    <published>2019-06-17T16:49:56Z</published>
    <title>Slicing the IO execution with ReLayTracer</title>
    <summary>  Analyzing IO performance anomalies is a crucial task in various computing
environments, ranging from large-scale cloud applications to desktop
applications. However, the IO stack of modern operating systems is complicated,
making it hard to understand the performance anomalies with existing tools.
Kernel IO executions are frequently interrupted by internal kernel activities,
requiring a sophisticated IO profile tool to deal with the noises. Furthermore,
complicated interactions of concurrent IO requests cause different sources of
tail latencies in kernel IO stack. As a consequence, developers want to know
fine-grained latency profile across IO layers, which may differ in each IO
requests. To meet the requirements, this paper suggests ReLayTracer, a
per-request, per-layer IO profiler. ReLayTracer enables a detailed analysis to
identify root causes of IO performance anomalies by providing per-layer latency
distributions of each IO request, hardware performance behavior, and time spent
by kernel activities such as an interrupt.
</summary>
    <author>
      <name>Ganguk Lee</name>
    </author>
    <author>
      <name>Yeaseul Park</name>
    </author>
    <author>
      <name>Jeongseob Ahn</name>
    </author>
    <author>
      <name>Youngjin Kwon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.07124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.07124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.09799v2</id>
    <updated>2019-06-26T15:24:10Z</updated>
    <published>2019-06-24T09:11:18Z</published>
    <title>On The Performance of ARM TrustZone</title>
    <summary>  The TrustZone technology, available in the vast majority of recent ARM
processors, allows the execution of code inside a so-called secure world. It
effectively provides hardware-isolated areas of the processor for sensitive
data and code, i.e., a trusted execution environment (TEE). The OP-TEE
framework provides a collection of toolchain, open-source libraries and secure
kernel specifically geared to develop applications for TrustZone. This paper
presents an in-depth performance- and energy-wise study of TrustZone using the
OP-TEE framework, including secure storage and the cost of switching between
secure and unsecure worlds, using emulated and hardware measurements.
</summary>
    <author>
      <name>Julien Amacher</name>
    </author>
    <author>
      <name>Valerio Schiavoni</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-22496-7_9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-22496-7_9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, extended version for version appeared in IFIP DAIS 2019.
  European Commission Project: LEGaTO - Low Energy Toolset for Heterogeneous
  Computing (EC-H2020-780681)</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.09799v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.09799v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.07820v2</id>
    <updated>2020-03-01T20:05:26Z</updated>
    <published>2019-09-16T02:09:58Z</published>
    <title>Data Centers Job Scheduling with Deep Reinforcement Learning</title>
    <summary>  Efficient job scheduling on data centers under heterogeneous complexity is
crucial but challenging since it involves the allocation of multi-dimensional
resources over time and space. To adapt the complex computing environment in
data centers, we proposed an innovative Advantage Actor-Critic (A2C) deep
reinforcement learning based approach called A2cScheduler for job scheduling.
A2cScheduler consists of two agents, one of which, dubbed the actor, is
responsible for learning the scheduling policy automatically and the other one,
the critic, reduces the estimation error. Unlike previous policy gradient
approaches, A2cScheduler is designed to reduce the gradient estimation variance
and to update parameters efficiently. We show that the A2cScheduler can achieve
competitive scheduling performance using both simulated workloads and real data
collected from an academic data center.
</summary>
    <author>
      <name>Sisheng Liang</name>
    </author>
    <author>
      <name>Zhou Yang</name>
    </author>
    <author>
      <name>Fang Jin</name>
    </author>
    <author>
      <name>Yong Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.07820v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.07820v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.10123v1</id>
    <updated>2019-09-23T02:03:13Z</updated>
    <published>2019-09-23T02:03:13Z</published>
    <title>SplitFS: Reducing Software Overhead in File Systems for Persistent
  Memory</title>
    <summary>  We present SplitFS, a file system for persistent memory (PM) that reduces
software overhead significantly compared to state-of-the-art PM file systems.
SplitFS presents a novel split of responsibilities between a user-space library
file system and an existing kernel PM file system. The user-space library file
system handles data operations by intercepting POSIX calls, memory-mapping the
underlying file, and serving the read and overwrites using processor loads and
stores. Metadata operations are handled by the kernel PM file system (ext4
DAX). SplitFS introduces a new primitive termed relink to efficiently support
file appends and atomic data operations. SplitFS provides three consistency
modes, which different applications can choose from, without interfering with
each other. SplitFS reduces software overhead by up-to 4x compared to the NOVA
PM file system, and 17x compared to ext4-DAX. On a number of micro-benchmarks
and applications such as the LevelDB key-value store running the YCSB
benchmark, SplitFS increases application performance by up to 2x compared to
ext4 DAX and NOVA while providing similar consistency guarantees.
</summary>
    <author>
      <name>Rohan Kadekodi</name>
    </author>
    <author>
      <name>Se Kwon Lee</name>
    </author>
    <author>
      <name>Sanidhya Kashyap</name>
    </author>
    <author>
      <name>Taesoo Kim</name>
    </author>
    <author>
      <name>Aasheesh Kolli</name>
    </author>
    <author>
      <name>Vijay Chidambaram</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3341301.3359631</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3341301.3359631" rel="related"/>
    <link href="http://arxiv.org/abs/1909.10123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.10123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.10377v1</id>
    <updated>2019-09-23T14:10:16Z</updated>
    <published>2019-09-23T14:10:16Z</published>
    <title>SIVSHM: Secure Inter-VM Shared Memory</title>
    <summary>  With wide spread acceptance of virtualization, virtual machines (VMs) find
their presence in various applications such as Network Address Translation
(NAT) servers, firewall servers and MapReduce applications. Typically, in these
applications a data manager collects data from the external world and
distributes it to multiple workers for further processing. Currently, data
managers distribute data with workers either using inter-VM shared memory
(IVSHMEM) or network communication. IVSHMEM provides better data distribution
throughput sacrificing security as all untrusted workers have full access to
the shared memory region and network communication provides better security at
the cost of throughput. Secondly, IVSHMEM uses a central distributor to
exchange eventfd - a file descriptor to an event queue of length one, which is
used for inter-VM signaling. This central distributor becomes a bottleneck and
increases boot time of VMs. Secure Inter-VM Shared Memory (SIVSHM) provided
both security and better throughout by segmenting inter-VM shared memory, so
that each worker has access to segment that belong only to it, thereby enabling
security without sacrificing throughput. SIVSHM boots VMs in 30% less time
compared to IVSHMEM by eliminating central distributor from its architecture
and enabling direct exchange of eventfds amongst VMs.
</summary>
    <author>
      <name>Shesha Sreenivasamurthy</name>
    </author>
    <author>
      <name>Ethan Miller</name>
    </author>
    <link href="http://arxiv.org/abs/1909.10377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.10377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.02145v1</id>
    <updated>2020-08-05T14:07:24Z</updated>
    <published>2020-08-05T14:07:24Z</published>
    <title>Interprocess Communication in FreeBSD 11: Performance Analysis</title>
    <summary>  Interprocess communication, IPC, is one of the most fundamental functions of
a modern operating system, playing an essential role in the fabric of
contemporary applications. This report conducts an investigation in FreeBSD of
the real world performance considerations behind two of the most common IPC
mechanisms; pipes and sockets. A simple benchmark provides a fair sense of
effective bandwidth for each, and analysis using DTrace, hardware performance
counters and the operating system's source code is presented. We note that
pipes outperform sockets by 63% on average across all configurations, and
further that the size of userspace transmission buffers has a profound effect
on performance - larger buffers are beneficial up to a point (~32-64 KiB) after
which performance collapses as a result of devastating cache exhaustion. A deep
scrutiny of the probe effects at play is also presented, justifying the
validity of conclusions drawn from these experiments.
</summary>
    <author>
      <name>A. H. Bell-Thomas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.02145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.02145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.03563v1</id>
    <updated>2020-08-08T17:41:15Z</updated>
    <published>2020-08-08T17:41:15Z</published>
    <title>eXpOS: A Simple Pedagogical Operating System for Undergraduate
  Instruction</title>
    <summary>  An operating system project suitable for undergraduate computing/electrical
sciences students is presented. The project can be used as a course project in
a one semester course, or as a self-study project for motivated students. The
course is organized such that a student with a basic background in programming
and computer organization can follow the implementation road map available
online, and build the OS from scratch on her personal machine/laptop, with
minimal instructional supervision. The student is provided with a simulated
abstract machine, an application interface specification, specification and
design of the OS, and a step by step project implementation road map. The
functionalities of the OS include multitasking, virtual memory, semaphores,
shared memory, an elementary file system, interrupt driven disk and console
I/O, and a limited multi-user support. The final stage of the project involves
porting the OS to a two-core machine. An independent one semester compiler
design project, where the student builds a compiler for a tiny object oriented
programming language that generates target code that can be loaded and executed
by the OS is also briefly discussed.
</summary>
    <author>
      <name>K. Murali Krishnan</name>
    </author>
    <link href="http://arxiv.org/abs/2008.03563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.03563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.00466v1</id>
    <updated>2020-09-01T14:24:20Z</updated>
    <published>2020-09-01T14:24:20Z</published>
    <title>toki: A Build- and Test-Platform for Prototyping and Evaluating
  Operating System Concepts in Real-Time Environments</title>
    <summary>  Typically, even low-level operating system concepts, such as resource sharing
strategies and predictability measures, are evaluated with Linux on PC
hardware. This leaves a large gap to real industrial applications. Hence, the
direct transfer of the results might be difficult. As a solution, we present
toki, a prototyping and evaluation platform based on FreeRTOS and several
open-source libraries. toki comes with a unified build- and test-environment
based on Yocto and Qemu, which makes it well suited for rapid prototyping. With
its architecture chosen similar to production industrial systems, toki provides
the ground work to implement early prototypes of real-time systems research
results, up to technology readiness level 7, with little effort.
</summary>
    <author>
      <name>Oliver Horst</name>
    </author>
    <author>
      <name>Uwe Baumgarten</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in proceedings of the Open Demo Session of Real-Time Systems
  (RTSS@Work) held in conjunction with the 40th IEEE Real-Time Systems
  Symposium (RTSS)</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.00466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.00466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.00506v1</id>
    <updated>2020-09-01T15:08:31Z</updated>
    <published>2020-09-01T15:08:31Z</published>
    <title>Quantifying the Latency and Possible Throughput of External Interrupts
  on Cyber-Physical Systems</title>
    <summary>  An important characteristic of cyber-physical systems is their capability to
respond, in-time, to events from their physical environment. However, to the
best of our knowledge there exists no benchmark for assessing and comparing the
interrupt handling performance of different software stacks. Hence, we present
a flexible evaluation method for measuring the interrupt latency and throughput
on ARMv8-A based platforms. We define and validate seven test-cases that stress
individual parts of the overall process and combine them to three benchmark
functions that provoke the minimal and maximal interrupt latency, and maximal
interrupt throughput.
</summary>
    <author>
      <name>Oliver Horst</name>
    </author>
    <author>
      <name>Johannes Wiesböck</name>
    </author>
    <author>
      <name>Raphael Wild</name>
    </author>
    <author>
      <name>Uwe Baumgarten</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in proceedings of the 3rd Workshop on Benchmarking
  Cyber-Physical Systems and Internet of Things (CPS-IoTBench) held in
  conjunction with the 26th Annual International Conference on Mobile Computing
  and Networking (MobiCom)</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.00506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.00506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.02737v1</id>
    <updated>2020-09-06T13:56:40Z</updated>
    <published>2020-09-06T13:56:40Z</published>
    <title>Secure Memory Management on Modern Hardware</title>
    <summary>  Almost all modern hardware, from phone SoCs to high-end servers with
accelerators, contain memory translation and protection hardware like IOMMUs,
firewalls, and lookup tables which make it impossible to reason about, and
enforce protection and isolation based solely on the processor's MMUs. This has
led to numerous bugs and security vulnerabilities in today's system software.
  In this paper we regain the ability to reason about and enforce access
control using the proven concept of a reference monitor mediating accesses to
memory resources. We present a fine-grained, realistic memory protection model
that makes this traditional concept applicable today, and bring system software
in line with the complexity of modern, heterogeneous hardware.
  Our design is applicable to any operating system, regardless of architecture.
We show that it not only enforces the integrity properties of a system, but
does so with no inherent performance overhead and it is even amenable to
automation through code generation from trusted hardware specifications.
</summary>
    <author>
      <name>Reto Achermann</name>
    </author>
    <author>
      <name>Nora Hossle</name>
    </author>
    <author>
      <name>Lukas Humbel</name>
    </author>
    <author>
      <name>Daniel Schwyn</name>
    </author>
    <author>
      <name>David Cock</name>
    </author>
    <author>
      <name>Timothy Roscoe</name>
    </author>
    <link href="http://arxiv.org/abs/2009.02737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.02737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4; D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.09104v1</id>
    <updated>2020-09-18T22:07:17Z</updated>
    <published>2020-09-18T22:07:17Z</published>
    <title>Akita: A CPU scheduler for virtualized Clouds</title>
    <summary>  Clouds inherit CPU scheduling policies of operating systems. These policies
enforce fairness while leveraging best-effort mechanisms to enhance
responsiveness of all schedulable entities, irrespective of their service level
objectives (SLOs). This leads to unpredictable performance that forces cloud
providers to enforce strict reservation and isolation policies to prevent
high-criticality services (e.g., Memcached) from being impacted by
low-criticality ones (e.g., logging), which results in low utilization.
  In this paper, we present Akita, a hypervisor CPU scheduler that delivers
predictable performance at high utilization. Akita allows virtual machines
(VMs) to be categorized into high- and low-criticality VMs. Akita provides
strong guarantees on the ability of cloud providers to meet SLOs of
high-criticality VMs, by temporarily slowing down low-criticality VMs if
necessary. Akita, therefore, allows the co-existence of high and
low-criticality VMs on the same physical machine, leading to higher
utilization. The effectiveness of Akita is demonstrated by a prototype
implementation in the Xen hypervisor. We present experimental results that show
the many advantages of adopting Akita as the hypervisor CPU scheduler. In
particular, we show that high-criticality Memcached VMs are able to deliver
predictable performance despite being co-located with low-criticality CPU-bound
VMs.
</summary>
    <author>
      <name>Esmail Asyabi</name>
    </author>
    <author>
      <name>Azer Bestavros</name>
    </author>
    <author>
      <name>Renato Mancuso</name>
    </author>
    <author>
      <name>Richard West</name>
    </author>
    <author>
      <name>Erfan Sharafzadeh</name>
    </author>
    <link href="http://arxiv.org/abs/2009.09104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.07833v1</id>
    <updated>2020-10-15T15:52:25Z</updated>
    <published>2020-10-15T15:52:25Z</published>
    <title>PIMOD: A Tool for Configuring Single-Board Computer Operating System
  Images</title>
    <summary>  Computer systems used in the field of humanitarian technology are often based
on general-purpose single-board computers, such as Raspberry Pis. While these
systems offer great flexibility for developers and users, configuration and
deployment either introduces overhead by executing scripts on multiple devices
or requires deeper technical understanding when building operating system
images for such small computers from scratch. In this paper, we present PIMOD,
a software tool for configuring operating system images for single-board
computer systems. We propose a simple yet comprehensive configuration language.
In a configuration profile, called Pifile, a small set of commands is used to
describe the configuration of an operating system image. Virtualization
techniques are used during the execution of the profile in order to be
distribution and platform independent. Commands can be issued in the guest
operating system, providing access to the distribution specific tools, e.g., to
configure hardware parameters. The implementation of PIMOD is made public under
a free and open source license. PIMOD is evaluated in terms of user benefits,
performance compared to on-system configuration, and applicability across
different hardware platforms and operating systems.
</summary>
    <author>
      <name>Jonas Höchst</name>
    </author>
    <author>
      <name>Alvar Penning</name>
    </author>
    <author>
      <name>Patrick Lampe</name>
    </author>
    <author>
      <name>Bernd Freisleben</name>
    </author>
    <link href="http://arxiv.org/abs/2010.07833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.07833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.08741v1</id>
    <updated>2020-10-17T08:34:08Z</updated>
    <published>2020-10-17T08:34:08Z</published>
    <title>Stage Lookup: Accelerating Path Lookup using Directory Shortcuts</title>
    <summary>  The lookup procedure in Linux costs a significant portion of file accessing
time as the virtual file system (VFS) traverses the file path components one
after another. The lookup procedure becomes more time consuming when
applications frequently access files, especially those with small sizes. We
propose Stage Lookup, which dynamically caches popular directories to speed up
lookup procedures and further reduce file accessing latency. The core of Stage
Lookup is to cache popular dentries as shortcuts, so that path walks do not
bother to traverse directory trees from the root. Furthermore, Stage Lookup
enriches backward path walks as it treats the directory tree in a VFS as an
undirected map. We implement a Stage Lookup prototype and integrate it into
Linux Kernel v3.14. Our extensive performance evaluation studies show that
Stage Lookup offers up to 46.9% performance gain compared to ordinary path
lookup schemes. Furthermore, Stage Lookup shows smaller performance overheads
in rename and chmod operations compared to the original method of the kernel.
</summary>
    <author>
      <name>Yanliang Zou</name>
    </author>
    <author>
      <name>Tongliang Deng</name>
    </author>
    <author>
      <name>Jian Zhang</name>
    </author>
    <author>
      <name>Chen Chen</name>
    </author>
    <author>
      <name>Shu Yin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.08741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.08741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.13594v1</id>
    <updated>2020-10-26T14:07:06Z</updated>
    <published>2020-10-26T14:07:06Z</published>
    <title>Disaggregated Accelerator Management System for Cloud Data Centers</title>
    <summary>  A conventional data center that consists of monolithic-servers is confronted
with limitations including lack of operational flexibility, low resource
utilization, low maintainability, etc. Resource disaggregation is a promising
solution to address the above issues. We propose a concept of disaggregated
cloud data center architecture called Flow-in-Cloud (FiC) that enables an
existing cluster computer system to expand an accelerator pool through a
high-speed network. FlowOS-RM manages the entire pool resources, and deploys a
user job on a dynamically constructed slice according to a user request. This
slice consists of compute nodes and accelerators where each accelerator is
attached to the corresponding compute node. This paper demonstrates the
feasibility of FiC in a proof of concept experiment running a distributed deep
learning application on the prototype system. The result successfully warrants
the applicability of the proposed system.
</summary>
    <author>
      <name>Ryousei Takano</name>
    </author>
    <author>
      <name>Kuniyasu Suzaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEICE Transactions on Information and Systems, 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.13594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.13594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.01024v3</id>
    <updated>2021-08-04T18:08:18Z</updated>
    <published>2020-11-02T15:48:20Z</published>
    <title>Efficient Data Management with a Flexible Address Space</title>
    <summary>  Data management applications store their data using structured files in which
data are usually sorted to serve indexing and queries. However, in-place
insertions and removals of data are not naturally supported in a file's address
space. To avoid repeatedly rewriting existing data in a sorted file to admit
changes in place, applications usually employ extra layers of indirections,
such as mapping tables and logs, to admit changes out of place. However, this
approach leads to increased access cost and excessive complexity.
  This paper presents a novel storage engine that provides a flexible address
space, where in-place updates of arbitrary-sized data, such as insertions and
removals, can be performed efficiently. With this mechanism, applications can
manage sorted data in a linear address space with minimal complexity. Extensive
evaluations show that a key-value store built on top of it can achieve high
performance and efficiency with a simple implementation.
</summary>
    <author>
      <name>Chen Chen</name>
    </author>
    <author>
      <name>Wenshao Zhong</name>
    </author>
    <author>
      <name>Xingbo Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages incl. references; 13 figures; 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.01024v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.01024v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.3; E.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.12922v1</id>
    <updated>2021-02-25T15:22:38Z</updated>
    <published>2021-02-25T15:22:38Z</published>
    <title>BPF for storage: an exokernel-inspired approach</title>
    <summary>  The overhead of the kernel storage path accounts for half of the access
latency for new NVMe storage devices. We explore using BPF to reduce this
overhead, by injecting user-defined functions deep in the kernel's I/O
processing stack. When issuing a series of dependent I/O requests, this
approach can increase IOPS by over 2.5$\times$ and cut latency by half, by
bypassing kernel layers and avoiding user-kernel boundary crossings. However,
we must avoid losing important properties when bypassing the file system and
block layer such as the safety guarantees of the file system and translation
between physical blocks addresses and file offsets. We sketch potential
solutions to these problems, inspired by exokernel file systems from the late
90s, whose time, we believe, has finally come!
</summary>
    <author>
      <name>Yu Jian Wu</name>
    </author>
    <author>
      <name>Hongyi Wang</name>
    </author>
    <author>
      <name>Yuhong Zhong</name>
    </author>
    <author>
      <name>Asaf Cidon</name>
    </author>
    <author>
      <name>Ryan Stutsman</name>
    </author>
    <author>
      <name>Amy Tai</name>
    </author>
    <author>
      <name>Junfeng Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2102.12922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.12922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.10477v1</id>
    <updated>2022-01-25T17:21:58Z</updated>
    <published>2022-01-25T17:21:58Z</published>
    <title>SOL: Safe On-Node Learning in Cloud Platforms</title>
    <summary>  Cloud platforms run many software agents on each server node. These agents
manage all aspects of node operation, and in some cases frequently collect data
and make decisions. Unfortunately, their behavior is typically based on
pre-defined static heuristics or offline analysis; they do not leverage on-node
machine learning (ML). In this paper, we first characterize the spectrum of
node agents in Azure, and identify the classes of agents that are most likely
to benefit from on-node ML. We then propose SOL, an extensible framework for
designing ML-based agents that are safe and robust to the range of failure
conditions that occur in production. SOL provides a simple API to agent
developers and manages the scheduling and running of the agent-specific
functions they write. We illustrate the use of SOL by implementing three
ML-based agents that manage CPU cores, node power, and memory placement. Our
experiments show that (1) ML substantially improves our agents, and (2) SOL
ensures that agents operate safely under a variety of failure conditions. We
conclude that ML-based agents show significant potential and that SOL can help
build them.
</summary>
    <author>
      <name>Yawen Wang</name>
    </author>
    <author>
      <name>Daniel Crankshaw</name>
    </author>
    <author>
      <name>Neeraja J. Yadwadkar</name>
    </author>
    <author>
      <name>Daniel Berger</name>
    </author>
    <author>
      <name>Christos Kozyrakis</name>
    </author>
    <author>
      <name>Ricardo Bianchini</name>
    </author>
    <link href="http://arxiv.org/abs/2201.10477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.10477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.07314v1</id>
    <updated>2022-05-15T15:38:59Z</updated>
    <published>2022-05-15T15:38:59Z</published>
    <title>Dynamic Ready Queue Based Process Priority Scheduling Algorithm</title>
    <summary>  CPU scheduling is the reason behind the performance of multiprocessing and in
time-shared operating systems. Different scheduling criteria are used to
evaluate Central Processing Unit Scheduling algorithms which are based on
different properties of the system. Round Robin is known to be the most
recurrent pre-emptive algorithm used in an environment where processes are
allotted a unit of time and multiprocessing operating systems. In this paper, a
reformed variation of the Round Robin algorithm has been introduced to minimise
the completion time, turnaround time, waiting time and number of context
switches that results in the better performance of the system. The proposed
work consists of calculation of priority on the basis of the difference between
time spent in ready upto the moment and arrival time of the process, to ease up
the burden on the ready queue. We have also evaluated the performance of the
proposed approach on different datasets and measured the different scheduling
criteria.
</summary>
    <author>
      <name>Raghav Dalmia</name>
    </author>
    <author>
      <name>Aryaman Sinha</name>
    </author>
    <author>
      <name>Ruchi Verma</name>
    </author>
    <author>
      <name>P. K. Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 Figures, 5 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.07314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.07314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.12270v1</id>
    <updated>2022-05-24T15:22:16Z</updated>
    <published>2022-05-24T15:22:16Z</published>
    <title>The Next-Generation OS Process Abstraction</title>
    <summary>  Operating Systems are built upon a set of abstractions to provide resource
management and programming APIs for common functionality, such as
synchronization, communication, protection, and I/O. The process abstraction is
the bridge across these two aspects; unsurprisingly, research efforts pay
particular attention to the process abstraction, aiming at enhancing security,
improving performance, and supporting hardware innovations. However, given the
intrinsic difficulties to implement modifications at the OS level, recent
endeavors have not yet been widely adopted in production-oriented OSes. Still,
we believe the current hardware evolution and new application requirements
provide favorable conditions to change this trend. This paper evaluates recent
research on OS process features identifying potential evolution paths. We
derive a set of relevant process characteristics, and propose how to extend
them as to benefit OSes and applications.
</summary>
    <author>
      <name>Rodrigo Siqueira</name>
    </author>
    <author>
      <name>Nelson Lago</name>
    </author>
    <author>
      <name>Fabio Kon</name>
    </author>
    <author>
      <name>Dejan Milojičić</name>
    </author>
    <link href="http://arxiv.org/abs/2205.12270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.12270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.01547v1</id>
    <updated>2022-06-03T12:54:55Z</updated>
    <published>2022-06-03T12:54:55Z</published>
    <title>Understanding NVMe Zoned Namespace (ZNS) Flash SSD Storage Devices</title>
    <summary>  The standardization of NVMe Zoned Namespaces (ZNS) in the NVMe 2.0
specification presents a unique new addition to storage devices. Unlike
traditional SSDs, where the flash media management idiosyncrasies are hidden
behind a flash translation layer (FTL) inside the device, ZNS devices push
certain operations regarding data placement and garbage collection out from the
device to the host. This allows the host to achieve more optimal data placement
and predictable garbage collection overheads, along with lower device write
amplification. Thus, additionally increasing flash media lifetime. As a result,
ZNS devices are gaining significant attention in the research community.
  However, with the current software stack there are numerous ways of
integrating ZNS devices into a host system. In this work, we begin to
systematically analyze the integration options, report on the current software
support for ZNS devices in the Linux Kernel, and provide an initial set of
performance measurements. Our main findings show that larger I/O sizes are
required to saturate the ZNS device bandwidth, and configuration of the I/O
scheduler can provide workload dependent performance gains, requiring careful
consideration of ZNS integration and configuration depending on the application
workload and its access patterns. Our dataset and code are available at https:
//github.com/nicktehrany/ZNS-Study.
</summary>
    <author>
      <name>Nick Tehrany</name>
    </author>
    <author>
      <name>Animesh Trivedi</name>
    </author>
    <link href="http://arxiv.org/abs/2206.01547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.01547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.13367v1</id>
    <updated>2022-06-27T15:28:44Z</updated>
    <published>2022-06-27T15:28:44Z</published>
    <title>Multilevel Bidirectional Cache Filter</title>
    <summary>  Modern caches are often required to handle a massive amount of data, which
exceeds the amount of available memory; thus, hybrid caches, specifically
DRAM/SSD combination, become more and more prevalent. In such environments, in
addition to the classical hit-ratio target, saving writes to the second-level
cache is a dominant factor to avoid write amplification and wear out, two
notorious phenomena of SSD.
  This paper presents BiDiFilter, a novel multilevel caching scheme that
controls demotions and promotions between cache levels using a frequency sketch
filter. Further, it splits the higher cache level into two areas to keep the
most recent and the most frequent items close to the user.
  We conduct an extensive evaluation over real-world traces, comparing to
previous multilevel policies. We show that using our mechanism yields an x10
saving of writes in almost all cases and often improving latencies by up to
20%.
</summary>
    <author>
      <name>Ohad Eytan</name>
    </author>
    <author>
      <name>Roy Friedman</name>
    </author>
    <link href="http://arxiv.org/abs/2206.13367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.13367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.03432v1</id>
    <updated>2022-10-07T10:03:55Z</updated>
    <published>2022-10-07T10:03:55Z</published>
    <title>Femto-Containers: Lightweight Virtualization and Fault Isolation For
  Small Software Functions on Low-Power IoT Microcontrollers</title>
    <summary>  Low-power operating system runtimes used on IoT microcontrollers typically
provide rudimentary APIs, basic connectivity and, sometimes, a (secure)
firmware update mechanism. In contrast, on less constrained hardware, networked
software has entered the age of serverless, microservices and agility. With a
view to bridge this gap, in the paper we design Femto-Containers, a new
middleware runtime which can be embedded on heterogeneous low-power IoT
devices. Femto-Containers enable the secure deployment, execution and isolation
of small virtual software functions on low-power IoT devices, over the network.
We implement Femto-Containers, and provide integration in RIOT, a popular open
source IoT operating system. We then evaluate the performance of our
implementation, which was formally verified for fault-isolation, guaranteeing
that RIOT is shielded from logic loaded and executed in a Femto-Container. Our
experiments on various popular microcontroller architectures (Arm Cortex-M,
ESP32 and RISC-V) show that Femto-Containers offer an attractive trade-off in
terms of memory footprint overhead, energy consumption, and security
</summary>
    <author>
      <name>Koen Zandberg</name>
    </author>
    <author>
      <name>Emmanuel Baccelli</name>
    </author>
    <author>
      <name>Shenghao Yuan</name>
    </author>
    <author>
      <name>Frédéric Besson</name>
    </author>
    <author>
      <name>Jean-Pierre Talpin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2106.12553</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">23rd ACM/IFIP International Middleware Conference (MIDDLEWARE
  2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2210.03432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.03432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.04951v1</id>
    <updated>2022-10-10T18:41:56Z</updated>
    <published>2022-10-10T18:41:56Z</published>
    <title>Ecovisor: A Virtual Energy System for Carbon-Efficient Applications</title>
    <summary>  Cloud platforms' rapid growth is raising significant concerns about their
carbon emissions. To reduce emissions, future cloud platforms will need to
increase their reliance on renewable energy sources, such as solar and wind,
which have zero emissions but are highly unreliable. Unfortunately, today's
energy systems effectively mask this unreliability in hardware, which prevents
applications from optimizing their carbon-efficiency, or work done per kilogram
of carbon emitted. To address this problem, we design an "ecovisor", which
virtualizes the energy system and exposes software-defined control of it to
applications. An ecovisor enables each application to handle clean energy's
unreliability in software based on its own specific requirements. We implement
a small-scale ecovisor prototype that virtualizes a physical energy system to
enable software-based application-level i) visibility into variable grid
carbon-intensity and renewable generation and ii) control of server power usage
and battery charging/discharging. We evaluate the ecovisor approach by showing
how multiple applications can concurrently exercise their virtual energy system
in different ways to better optimize carbon-efficiency based on their specific
requirements compared to a general system-wide policy.
</summary>
    <author>
      <name>Abel Souza</name>
    </author>
    <author>
      <name>Noman Bashir</name>
    </author>
    <author>
      <name>Jorge Murillo</name>
    </author>
    <author>
      <name>Walid Hanafy</name>
    </author>
    <author>
      <name>Qianlin Liang</name>
    </author>
    <author>
      <name>David Irwin</name>
    </author>
    <author>
      <name>Prashant Shenoy</name>
    </author>
    <link href="http://arxiv.org/abs/2210.04951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.04951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.02177v1</id>
    <updated>2022-11-03T23:10:21Z</updated>
    <published>2022-11-03T23:10:21Z</published>
    <title>MUSTACHE: Multi-Step-Ahead Predictions for Cache Eviction</title>
    <summary>  In this work, we propose MUSTACHE, a new page cache replacement algorithm
whose logic is learned from observed memory access requests rather than fixed
like existing policies. We formulate the page request prediction problem as a
categorical time series forecasting task. Then, our method queries the learned
page request forecaster to obtain the next $k$ predicted page memory references
to better approximate the optimal B\'el\'ady's replacement algorithm. We
implement several forecasting techniques using advanced deep learning
architectures and integrate the best-performing one into an existing
open-source cache simulator. Experiments run on benchmark datasets show that
MUSTACHE outperforms the best page replacement heuristic (i.e., exact LRU),
improving the cache hit ratio by 1.9% and reducing the number of reads/writes
required to handle cache misses by 18.4% and 10.3%.
</summary>
    <author>
      <name>Gabriele Tolomei</name>
    </author>
    <author>
      <name>Lorenzo Takanen</name>
    </author>
    <author>
      <name>Fabio Pinelli</name>
    </author>
    <link href="http://arxiv.org/abs/2211.02177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.02177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.08056v1</id>
    <updated>2022-11-15T11:15:46Z</updated>
    <published>2022-11-15T11:15:46Z</published>
    <title>MeSHwA: The case for a Memory-Safe Software and Hardware Architecture
  for Serverless Computing</title>
    <summary>  Motivated by developer productivity, serverless computing, and microservices
have become the de facto development model in the cloud. Microservices
decompose monolithic applications into separate functional units deployed
individually. This deployment model, however, costs CSPs a large infrastructure
tax of more than 25%. To overcome these limitations, CSPs shift workloads to
Infrastructure Processing Units (IPUs) like Amazon's Nitro or, complementary,
innovate by building on memory-safe languages and novel software abstractions.
  Based on these trends, we hypothesize a \arch providing a general-purpose
runtime environment to specialize functionality when needed and strongly
isolate components. To achieve this goal, we investigate building a single
address space OS or a multi-application library OS, possible hardware
implications, and demonstrate their capabilities, drawbacks and requirements.
The goal is to bring the advantages to all application workloads including
legacy and memory-unsafe applications, and analyze how hardware may improve the
efficiency and security.
</summary>
    <author>
      <name>Anjo Vahldiek-Oberwagner</name>
    </author>
    <author>
      <name>Mona Vij</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop On Resource Disaggregation and Serverless Computing (WORDS)</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.08056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.08056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.07376v1</id>
    <updated>2022-12-13T04:09:44Z</updated>
    <published>2022-12-13T04:09:44Z</published>
    <title>Automated Cache for Container Executables</title>
    <summary>  Linux container technologies such as Docker and Singularity offer
encapsulated environments for easy execution of software. In high performance
computing, this is especially important for evolving and complex software
stacks with conflicting dependencies that must co-exist. Singularity Registry
HPC ("shpc") was created as an effort to install containers in this environment
as modules, seamlessly allowing for typically hidden executables inside
containers to be presented to the user as commands, and as such significantly
simplifying the user experience. A remaining challenge, however, is deriving
the list of important executables in the container. In this work, we present
new automation and methods that allow for not only discovering new containers
in large community sets, but also deriving container entries with important
executables. With this work we have added over 8,000 containers from the
BioContainers community that can be maintained and updated by the software
automation over time. All software is publicly available on the GitHub
platform, and can be beneficial to container registries and infrastructure
providers for automatically generating container modules to lower the usage
entry barrier and improve user experience.
</summary>
    <author>
      <name>Vanessa Sochat</name>
    </author>
    <author>
      <name>Matthieu Muffato</name>
    </author>
    <author>
      <name>Audrey Stott</name>
    </author>
    <author>
      <name>Marco De La Pierre</name>
    </author>
    <author>
      <name>Georgia Stuart</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.07376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.07376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.12671v1</id>
    <updated>2022-12-24T06:36:36Z</updated>
    <published>2022-12-24T06:36:36Z</published>
    <title>MProtect: Operating System Memory Management without Access</title>
    <summary>  Modern operating systems (OSes) have unfettered access to application data,
assuming that applications trust them. This assumption, however, is problematic
under many scenarios where either the OS provider is not trustworthy or the OS
can be compromised due to its large attack surface. Our investigation began
with the hypothesis that unfettered access to memory is not fundamentally
necessary for the OS to perform its own job, including managing the memory. The
result is a system called MProtect that leverages a small piece of software
running at a higher privilege level than the OS. MProtect protects the entire
user space of a process, requires only a small modification to the OS, and
supports major architectures such as ARM, x86 and RISC-V. Unlike prior works
that resorted to nested virtualization, which is often undesirable in mobile
and embedded systems, MProtect mediates how the OS accesses the memory and
handles exceptions. We report an implementation of MProtect called MGuard with
ARMv8/Linux and evaluate its performance with both macro and microbenchmarks.
We show MGuard has a runtime TCB 2~3 times smaller than related systems and
enjoys competitive performance while supporting legitimate OS access to the
user space.
</summary>
    <author>
      <name>Caihua Li</name>
    </author>
    <author>
      <name>Seung-seob Lee</name>
    </author>
    <author>
      <name>Min Hong Yun</name>
    </author>
    <author>
      <name>Lin Zhong</name>
    </author>
    <link href="http://arxiv.org/abs/2212.12671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.12671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.11886v1</id>
    <updated>2023-01-27T17:57:37Z</updated>
    <published>2023-01-27T17:57:37Z</published>
    <title>A Learned Cache Eviction Framework with Minimal Overhead</title>
    <summary>  Recent work shows the effectiveness of Machine Learning (ML) to reduce cache
miss ratios by making better eviction decisions than heuristics. However,
state-of-the-art ML caches require many predictions to make an eviction
decision, making them impractical for high-throughput caching systems. This
paper introduces Machine learning At the Tail (MAT), a framework to build
efficient ML-based caching systems by integrating an ML module with a
traditional cache system based on a heuristic algorithm. MAT treats the
heuristic algorithm as a filter to receive high-quality samples to train an ML
model and likely candidate objects for evictions. We evaluate MAT on 8
production workloads, spanning storage, in-memory caching, and CDNs. The
simulation experiments show MAT reduces the number of costly ML
predictions-per-eviction from 63 to 2, while achieving comparable miss ratios
to the state-of-the-art ML cache system. We compare a MAT prototype system with
an LRU-based caching system in the same setting and show that they achieve
similar request rates.
</summary>
    <author>
      <name>Dongsheng Yang</name>
    </author>
    <author>
      <name>Daniel S. Berger</name>
    </author>
    <author>
      <name>Kai Li</name>
    </author>
    <author>
      <name>Wyatt Lloyd</name>
    </author>
    <link href="http://arxiv.org/abs/2301.11886v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.11886v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.14417v2</id>
    <updated>2023-03-01T03:40:50Z</updated>
    <published>2023-02-28T08:50:03Z</published>
    <title>Protected Data Plane OS Using Memory Protection Keys and Lightweight
  Activation</title>
    <summary>  Increasing data center network speed coupled with application requirements
for high throughput and low latencies have raised the efficiency bar for
network stacks. To reduce substantial kernel overhead in network processing,
recent proposals bypass the kernel or implement the stack as user space OS
service -- both with performance isolation, security, and resource efficiency
trade-offs. We present Tardis, a new network stack architecture that combines
the performance and resource efficiency benefits of kernel-bypass and the
security and performance enforcement of in-kernel stacks. Tardis runs the OS
I/O stack in user-level threads that share both address spaces and kernel
threads with applications, avoiding almost all kernel context switch and
cross-core communication overheads. To provide sufficient protection, Tardis
leverages x86 protection keys (MPK) extension to isolate the I/O stack from
application code. And to enforce timely scheduling of network processing and
fine-grained performance isolation, Tardis implements lightweight scheduler
activations with preemption timers.
</summary>
    <author>
      <name>Yihan Yang</name>
    </author>
    <author>
      <name>Zhuobin Huang</name>
    </author>
    <author>
      <name>Antoine Kaufmann</name>
    </author>
    <author>
      <name>Jialin Li</name>
    </author>
    <link href="http://arxiv.org/abs/2302.14417v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.14417v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.11186v2</id>
    <updated>2023-03-23T20:27:09Z</updated>
    <published>2023-03-20T15:17:21Z</published>
    <title>Shedding Light on Static Partitioning Hypervisors for Arm-based
  Mixed-Criticality Systems</title>
    <summary>  In this paper, we aim to understand the properties and guarantees of static
partitioning hypervisors (SPH) for Arm-based mixed-criticality systems (MCS).
To this end, we performed a comprehensive empirical evaluation of popular
open-source SPH, i.e., Jailhouse, Xen (Dom0-less), Bao, and seL4 CAmkES VMM,
focusing on two key requirements of modern MCS: real-time and safety. The goal
of this study is twofold. Firstly, to empower industrial practitioners with
hard data to reason about the different trade-offs of SPH. Secondly, we aim to
raise awareness of the research and open-source communities to the still open
problems in SPH by unveiling new insights regarding lingering weaknesses. All
artifacts will be open-sourced to enable independent validation of results and
encourage further exploration on SPH.
</summary>
    <author>
      <name>José Martins</name>
    </author>
    <author>
      <name>Sandro Pinto</name>
    </author>
    <link href="http://arxiv.org/abs/2303.11186v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.11186v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.01721v1</id>
    <updated>2023-04-04T11:30:24Z</updated>
    <published>2023-04-04T11:30:24Z</published>
    <title>Virtio-FPGA: a virtualization solution for SoC-attached FPGAs</title>
    <summary>  Recently, FPGA accelerators have risen in popularity as they present a
suitable way of satisfying the high-computation and low-power demands of real
time applications. The modern electric transportation systems (such as
aircraft, road vehicles) can greatly profit from embedded FPGAs, which
incorporate both high-performance and flexibility features into a single SoC.
At the same time, the virtualization of FPGA resources aims to reinforce these
systems with strong isolation, consolidation and security. In this paper, we
present a novel virtualization framework aimed for SoC-attached FPGA devices,
in a Linux and QEMU/KVM setup. We use Virtio as a means to enable the
configuration of FPGA resources from guest systems in an efficient way. Also,
we employ the Linux VFIO and Device Tree Overlays technologies in order to
render the FPGA resources dynamically accessible to guest systems. The ability
to dynamically configure and utilize the FPGA resources from a virtualization
environment is described in details. The evaluation procedure of the solution
is presented and the virtualization overhead is benchmarked as minimal (around
10%) when accessing the FPGA devices from guest systems.
</summary>
    <author>
      <name>Anna Panagopoulou</name>
    </author>
    <author>
      <name>Michele Paolino</name>
    </author>
    <author>
      <name>Daniel Raho</name>
    </author>
    <link href="http://arxiv.org/abs/2304.01721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.01721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.02244v1</id>
    <updated>2023-05-03T16:29:39Z</updated>
    <published>2023-05-03T16:29:39Z</published>
    <title>NVMM cache design: Logging vs. Paging</title>
    <summary>  Modern NVMM is closing the gap between DRAM and persistent storage, both in
terms of performance and features. Having both byte addressability and
persistence on the same device gives NVMM an unprecedented set of features,
leading to the following question: How should we design an NVMM-based caching
system to fully exploit its potential? We build two caching mechanisms, NVPages
and NVLog, based on two radically different design approaches. NVPages stores
memory pages in NVMM, similar to the Linux page cache (LPC). NVLog uses NVMM to
store a log of pending write operations to be submitted to the LPC, while it
ensures reads with a small DRAM cache. Our study shows and quantifies
advantages and flaws for both designs.
</summary>
    <author>
      <name>Rémi Dulong</name>
    </author>
    <author>
      <name>Quentin Acher</name>
    </author>
    <author>
      <name>Baptiste Lepers</name>
    </author>
    <author>
      <name>Valerio Schiavoni</name>
    </author>
    <author>
      <name>Pascal Felber</name>
    </author>
    <author>
      <name>Gaël Thomas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures, presented for NVMW'23: 14th Annual Non-Volatile
  Memories Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.02244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.02244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M15" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.08345v1</id>
    <updated>2023-06-14T08:28:41Z</updated>
    <published>2023-06-14T08:28:41Z</published>
    <title>SWAM: Revisiting Swap and OOMK for Improving Application Responsiveness
  on Mobile Devices</title>
    <summary>  Existing memory reclamation policies on mobile devices may be no longer valid
because they have negative effects on the response time of running
applications. In this paper, we propose SWAM, a new integrated memory
management technique that complements the shortcomings of both the swapping and
killing mechanism in mobile devices and improves the application
responsiveness. SWAM consists of (1) Adaptive Swap that performs swapping
adaptively into memory or storage device while managing the swap space
dynamically, (2) OOM Cleaner that reclaims shared object pages in the swap
space to secure available memory and storage space, and (3) EOOM Killer that
terminates processes in the worst case while prioritizing the lowest
initialization cost applications as victim processes first. Experimental
results demonstrate that SWAM significantly reduces the number of applications
killed by OOMK (6.5x lower), and improves application launch time (36% faster)
and response time (41% faster), compared to the conventional schemes.
</summary>
    <author>
      <name>Geunsik Lim</name>
    </author>
    <author>
      <name>Donghyun Kang</name>
    </author>
    <author>
      <name>MyungJoo Ham</name>
    </author>
    <author>
      <name>Young Ik Eom</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3570361.3592518</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3570361.3592518" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 13 figures, MobiCom 2023 (29th Annual International
  Conference On Mobile Computing And Networking)</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.08345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.08345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.10503v1</id>
    <updated>2023-06-18T08:55:36Z</updated>
    <published>2023-06-18T08:55:36Z</published>
    <title>A Survey on User-Space Storage and Its Implementations</title>
    <summary>  The storage stack in the traditional operating system is primarily optimized
towards improving the CPU utilization and hiding the long I/O latency imposed
by the slow I/O devices such as hard disk drivers (HDDs). However, the emerging
storage media experience significant technique shifts in the past decade, which
exhibit high bandwidth and low latency. These high-performance storage devices,
unfortunately, suffer from the huge overheads imposed by the system software
including the long storage stack and the frequent context switch between the
user and kernel modes. Many researchers have investigated huge efforts in
addressing this challenge by constructing a direct software path between a user
process and the underlying storage devices. We revisit such novel designs in
the prior work and present a survey in this paper. Specifically, we classify
the former research into three categories according to their commonalities. We
then present the designs of each category based on the timeline and analyze
their uniqueness and contributions. This paper also reviews the applications
that exploit the characteristics of theses designs. Given that the user-space
storage is a growing research field, we believe this paper can be an
inspiration for future researchers, who are interested in the user-space
storage system designs.
</summary>
    <author>
      <name>Junzhe Li</name>
    </author>
    <author>
      <name>Xiurui Pan</name>
    </author>
    <author>
      <name>Shushu Yi</name>
    </author>
    <author>
      <name>Jie Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2306.10503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.10503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.15076v1</id>
    <updated>2023-06-26T21:24:12Z</updated>
    <published>2023-06-26T21:24:12Z</published>
    <title>Agile Development of Linux Schedulers with Ekiben</title>
    <summary>  Kernel task scheduling is important for application performance, adaptability
to new hardware, and complex user requirements. However, developing, testing,
and debugging new scheduling algorithms in Linux, the most widely used cloud
operating system, is slow and difficult. We developed Ekiben, a framework for
high velocity development of Linux kernel schedulers. Ekiben schedulers are
written in safe Rust, and the system supports live upgrade of new scheduling
policies into the kernel, userspace debugging, and bidirectional communication
with applications. A scheduler implemented with Ekiben achieved near identical
performance (within 1% on average) to the default Linux scheduler CFS on a wide
range of benchmarks. Ekiben is also able to support a range of research
schedulers, specifically the Shinjuku scheduler, a locality aware scheduler,
and the Arachne core arbiter, with good performance.
</summary>
    <author>
      <name>Samantha Miller</name>
    </author>
    <author>
      <name>Anirudh Kumar</name>
    </author>
    <author>
      <name>Tanay Vakharia</name>
    </author>
    <author>
      <name>Tom Anderson</name>
    </author>
    <author>
      <name>Ang Chen</name>
    </author>
    <author>
      <name>Danyang Zhuo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures, submitted to Eurosys 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.15076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.15076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.00950v1</id>
    <updated>2023-07-03T11:49:08Z</updated>
    <published>2023-07-03T11:49:08Z</published>
    <title>Energy-aware Time- and Event-triggered KVM Nodes</title>
    <summary>  Industries are considering the adoption of cloud and edge computing for
real-time applications due to current improvements in network latencies and the
advent of Fog and Edge computing. Current cloud paradigms are not designed for
real-time applications, as they neither provide low latencies/jitter nor the
guarantees and determinism required by real-time applications. Experts estimate
that data centers use 1% of global electricity for powering the equipment, and
in turn, for dealing with the produced heat. Hence, energy consumption is a
crucial metric in cloud technologies. Applying energy conservation techniques
is not straightforward due to the increased scheduling overheads and
application execution times. Inspired by slot shifting, we propose an algorithm
to support energy-aware time-triggered execution of periodic real-time VMs
while still providing the ability to execute aperiodic real-time and
best-effort VMs in the slack of the time-triggered ones. The algorithm
considers energy reduction techniques based on dynamic power management and
dynamic voltage and frequency scaling. We implement our algorithm as an
extension to the Linux kernel scheduler (for use with the KVM hypervisor) and
evaluate it on a server-grade Intel Xeon node.
</summary>
    <author>
      <name>Isser Kadusale</name>
    </author>
    <author>
      <name>Gautam Gala</name>
    </author>
    <author>
      <name>Gerhard Fohler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in Real-time Cloud (RT-Cloud) 2023 workshop co-hosted with
  35th Euromicro conference on Real-time systems (ECRTS)</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.00950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.00950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68N25" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.04095v1</id>
    <updated>2023-07-09T04:35:20Z</updated>
    <published>2023-07-09T04:35:20Z</published>
    <title>Understanding Persistent-Memory Related Issues in the Linux Kernel</title>
    <summary>  Persistent memory (PM) technologies have inspired a wide range of PM-based
system optimizations. However, building correct PM-based systems is difficult
due to the unique characteristics of PM hardware. To better understand the
challenges as well as the opportunities to address them, this paper presents a
comprehensive study of PM-related issues in the Linux kernel. By analyzing
1,553 PM-related kernel patches in-depth and conducting experiments on
reproducibility and tool extension, we derive multiple insights in terms of PM
patch categories, PM bug patterns, consequences, fix strategies, triggering
conditions, and remedy solutions. We hope our results could contribute to the
development of robust PM-based storage systems
</summary>
    <author>
      <name>Om Rameshwar Gatla</name>
    </author>
    <author>
      <name>Duo Zhang</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <author>
      <name>Mai Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM TRANSACTIONS ON STORAGE(TOS'23)</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.04095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.04095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.11860v1</id>
    <updated>2023-07-21T19:00:25Z</updated>
    <published>2023-07-21T19:00:25Z</published>
    <title>Understanding (Un)Written Contracts of NVMe ZNS Devices with zns-tools</title>
    <summary>  Operational and performance characteristics of flash SSDs have long been
associated with a set of Unwritten Contracts due to their hidden, complex
internals and lack of control from the host software stack. These unwritten
contracts govern how data should be stored, accessed, and garbage collected.
The emergence of Zoned Namespace (ZNS) flash devices with their open and
standardized interface allows us to write these unwritten contracts for the
storage stack. However, even with a standardized storage-host interface, due to
the lack of appropriate end-to-end operational data collection tools, the
quantification and reasoning of such contracts remain a challenge. In this
paper, we propose zns.tools, an open-source framework for end-to-end event and
metadata collection, analysis, and visualization for the ZNS SSDs contract
analysis. We showcase how zns.tools can be used to understand how the
combination of RocksDB with the F2FS file system interacts with the underlying
storage. Our tools are available openly at
\url{https://github.com/stonet-research/zns-tools}.
</summary>
    <author>
      <name>Nick Tehrany</name>
    </author>
    <author>
      <name>Krijn Doekemeijer</name>
    </author>
    <author>
      <name>Animesh Trivedi</name>
    </author>
    <link href="http://arxiv.org/abs/2307.11860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.11860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.01983v2</id>
    <updated>2023-08-15T04:31:28Z</updated>
    <published>2023-08-03T18:33:56Z</published>
    <title>Unleashing Unprivileged eBPF Potential with Dynamic Sandboxing</title>
    <summary>  For safety reasons, unprivileged users today have only limited ways to
customize the kernel through the extended Berkeley Packet Filter (eBPF). This
is unfortunate, especially since the eBPF framework itself has seen an increase
in scope over the years. We propose SandBPF, a software-based kernel isolation
technique that dynamically sandboxes eBPF programs to allow unprivileged users
to safely extend the kernel, unleashing eBPF's full potential. Our early
proof-of-concept shows that SandBPF can effectively prevent exploits missed by
eBPF's native safety mechanism (i.e., static verification) while incurring
0%-10% overhead on web server benchmarks.
</summary>
    <author>
      <name>Soo Yee Lim</name>
    </author>
    <author>
      <name>Xueyuan Han</name>
    </author>
    <author>
      <name>Thomas Pasquier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, to appear in the 1st SIGCOMM Workshop on eBPF and
  Kernel Extensions</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.01983v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.01983v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.10062v7</id>
    <updated>2024-12-17T14:30:30Z</updated>
    <published>2023-08-19T15:55:37Z</published>
    <title>Revitalising the Single Batch Environment: A 'Quest' to Achieve Fairness
  and Efficiency</title>
    <summary>  In the realm of computer systems, efficient utilisation of the CPU (Central
Processing Unit) has always been a paramount concern. Researchers and engineers
have long sought ways to optimise process execution on the CPU, leading to the
emergence of CPU scheduling as a field of study. This research proposes a novel
algorithm for batch processing that operates on a preemptive model, dynamically
assigning priorities based on a robust ratio, employing a dynamic time slice,
and utilising periodic sorting technique to achieve fairness. By engineering
this responsive and fair model, the proposed algorithm strikes a delicate
balance between efficiency and fairness, providing an optimised solution for
batch scheduling while ensuring system responsiveness.
</summary>
    <author>
      <name>Supriya Manna</name>
    </author>
    <author>
      <name>Krishna Siva Prasad Mudigonda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/1206212X.2024.2380660</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/1206212X.2024.2380660" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computers and Applications, pp. 1-15,
  2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2308.10062v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.10062v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.03370v1</id>
    <updated>2023-10-05T08:04:15Z</updated>
    <published>2023-10-05T08:04:15Z</published>
    <title>Motivating Next-Generation OS Physical Memory Management for
  Terabyte-Scale NVMMs</title>
    <summary>  Software managed byte-addressable hybrid memory systems consisting of DRAMs
and NVMMs offer a lot of flexibility to design efficient large scale data
processing applications. Operating systems (OS) play an important role in
enabling the applications to realize the integrated benefits of DRAMs' low
access latency and NVMMs' large capacity along with its persistent
characteristics. In this paper, we comprehensively analyze the performance of
conventional OS physical memory management subsystems that were designed only
based on the DRAM memory characteristics in the context of modern hybrid
byte-addressable memory systems.
  To study the impact of high access latency and large capacity of NVMMs on
physical memory management, we perform an extensive evaluation on Linux with
Intel's Optane NVMM. We observe that the core memory management functionalities
such as page allocation are negatively impacted by high NVMM media latency,
while functionalities such as conventional fragmentation management are
rendered inadequate. We also demonstrate that certain traditional memory
management functionalities are affected by neither aspects of modern NVMMs. We
conclusively motivate the need to overhaul fundamental aspects of traditional
OS physical memory management in order to fully exploit terabyte-scale NVMMs.
</summary>
    <author>
      <name>Shivank Garg</name>
    </author>
    <author>
      <name>Aravinda Prasad</name>
    </author>
    <author>
      <name>Debadatta Mishra</name>
    </author>
    <author>
      <name>Sreenivas Subramoney</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 24 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.03370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.03370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.12554v1</id>
    <updated>2023-10-19T08:03:09Z</updated>
    <published>2023-10-19T08:03:09Z</published>
    <title>GMEM: Generalized Memory Management for Peripheral Devices</title>
    <summary>  This paper presents GMEM, generalized memory management, for peripheral
devices. GMEM provides OS support for centralized memory management of both CPU
and devices. GMEM provides a high-level interface that decouples MMU-specific
functions. Device drivers can thus attach themselves to a process's address
space and let the OS take charge of their memory management. This eliminates
the need for device drivers to "reinvent the wheel" and allows them to benefit
from general memory optimizations integrated by GMEM. Furthermore, GMEM
internally coordinates all attached devices within each virtual address space.
This drastically improves user-level programmability, since programmers can use
a single address space within their program, even when operating across the CPU
and multiple devices. A case study on device drivers demonstrates these
benefits. A GMEM-based IOMMU driver eliminates around seven hundred lines of
code and obtains 54% higher network receive throughput utilizing 32% less CPU
compared to the state-of-the-art. In addition, the GMEM-based driver of a
simulated GPU takes less than 70 lines of code, excluding its MMU functions.
</summary>
    <author>
      <name>Weixi Zhu</name>
    </author>
    <author>
      <name>Alan L. Cox</name>
    </author>
    <author>
      <name>Scott Rixner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Finished before Weixi left Rice and submitted to ASPLOS'23</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.12554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.12554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.14741v1</id>
    <updated>2023-10-23T09:20:51Z</updated>
    <published>2023-10-23T09:20:51Z</published>
    <title>Adaptive CPU Resource Allocation for Emulator in Kernel-based Virtual
  Machine</title>
    <summary>  The technologies of heterogeneous multi-core architectures, co-location, and
virtualization can be used to reduce server power consumption and improve
system utilization, which are three important technologies for data centers.
This article explores the scheduling strategy of Emulator threads within
virtual machine processes in a scenario of co-location of multiple virtual
machines on heterogeneous multi-core architectures. In this co-location
scenario, the scheduling strategy for Emulator threads significantly affects
the performance of virtual machines. This article focuses on this thread for
the first time in the relevant field. This article found that the scheduling
latency metric can well indicate the running status of the vCPU threads and
Emulator threads in the virtualization environment, and applied this metric to
the design of the scheduling strategy. This article designed an Emulator thread
scheduler based on heuristic rules, which, in coordination with the host
operating system's scheduler, dynamically adjusts the scheduling scope of
Emulator threads to improve the overall performance of virtual machines. The
article found that in real application scenarios, the scheduler effectively
improved the performance of applications within virtual machines, with a
maximum performance improvement of 40.7%.
</summary>
    <author>
      <name>Yecheng Yang</name>
    </author>
    <author>
      <name>Pu Pang</name>
    </author>
    <author>
      <name>Jiawen Wang</name>
    </author>
    <author>
      <name>Quan Chen</name>
    </author>
    <author>
      <name>Minyi Guo</name>
    </author>
    <link href="http://arxiv.org/abs/2310.14741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.14741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.17046v1</id>
    <updated>2023-10-25T22:52:37Z</updated>
    <published>2023-10-25T22:52:37Z</published>
    <title>Proving the Absence of Microarchitectural Timing Channels</title>
    <summary>  Microarchitectural timing channels are a major threat to computer security. A
set of OS mechanisms called time protection was recently proposed as a
principled way of preventing information leakage through such channels and
prototyped in the seL4 microkernel. We formalise time protection and the
underlying hardware mechanisms in a way that allows linking them to the
information-flow proofs that showed the absence of storage channels in seL4.
</summary>
    <author>
      <name>Scott Buckley</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UNSW Sydney</arxiv:affiliation>
    </author>
    <author>
      <name>Robert Sison</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UNSW Sydney</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Melbourne</arxiv:affiliation>
    </author>
    <author>
      <name>Nils Wistoff</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ETH Zürich</arxiv:affiliation>
    </author>
    <author>
      <name>Curtis Millar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UNSW Sydney</arxiv:affiliation>
    </author>
    <author>
      <name>Toby Murray</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Melbourne</arxiv:affiliation>
    </author>
    <author>
      <name>Gerwin Klein</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Proofcraft</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UNSW Sydney</arxiv:affiliation>
    </author>
    <author>
      <name>Gernot Heiser</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UNSW Sydney</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Scott Buckley and Robert Sison were joint lead authors</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.17046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.17046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6; D.2.4; F.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.19825v1</id>
    <updated>2023-10-27T19:19:07Z</updated>
    <published>2023-10-27T19:19:07Z</published>
    <title>A Survey of the Security Challenges and Requirements for IoT Operating
  Systems</title>
    <summary>  The Internet of Things (IoT) is becoming an integral part of our modern lives
as we converge towards a world surrounded by ubiquitous connectivity. The
inherent complexity presented by the vast IoT ecosystem ends up in an
insufficient understanding of individual system components and their
interactions, leading to numerous security challenges. In order to create a
secure IoT platform from the ground up, there is a need for a unifying
operating system (OS) that can act as a cornerstone regulating the development
of stable and secure solutions. In this paper, we present a classification of
the security challenges stemming from the manifold aspects of IoT development.
We also specify security requirements to direct the secure development of an
unifying IoT OS to resolve many of those ensuing challenges. Survey of several
modern IoT OSs confirm that while the developers of the OSs have taken many
alternative approaches to implement security, we are far from engineering an
adequately secure and unified architecture. More broadly, the study presented
in this paper can help address the growing need for a secure and unified
platform to base IoT development on and assure the safe, secure, and reliable
operation of IoT in critical domains.
</summary>
    <author>
      <name>Alvi Jawad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.19825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.19825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.09032v1</id>
    <updated>2023-11-15T15:25:08Z</updated>
    <published>2023-11-15T15:25:08Z</published>
    <title>Nahida: In-Band Distributed Tracing with eBPF</title>
    <summary>  Microservices are commonly used in modern cloud-native applications to
achieve agility. However, the complexity of service dependencies in large-scale
microservices systems can lead to anomaly propagation, making fault
troubleshooting a challenge. To address this issue, distributed tracing systems
have been proposed to trace complete request execution paths, enabling
developers to troubleshoot anomalous services. However, existing distributed
tracing systems have limitations such as invasive instrumentation, trace loss,
or inaccurate trace correlation. To overcome these limitations, we propose a
new tracing system based on eBPF (extended Berkeley Packet Filter), named
Nahida, that can track complete requests in the kernel without intrusion,
regardless of programming language or implementation. Our evaluation results
show that Nahida can track over 92% of requests with stable accuracy, even
under the high concurrency of user requests, while the state-of-the-art
non-invasive approaches can not track any of the requests. Importantly, Nahida
can track requests served by a multi-threaded application that none of the
existing invasive tracing systems can handle by instrumenting tracing codes
into libraries. Moreover, the overhead introduced by Nahida is negligible,
increasing service latency by only 1.55%-2.1%. Overall, Nahida provides an
effective and non-invasive solution for distributed tracing.
</summary>
    <author>
      <name>Wanqi Yang</name>
    </author>
    <author>
      <name>Pengfei Chen</name>
    </author>
    <author>
      <name>Kai Liu</name>
    </author>
    <author>
      <name>Huxing Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2311.09032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.09032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.13333v2</id>
    <updated>2023-11-23T10:09:31Z</updated>
    <published>2023-11-22T11:54:42Z</published>
    <title>Trace-enabled Timing Model Synthesis for ROS2-based Autonomous
  Applications</title>
    <summary>  Autonomous applications are typically developed over Robot Operating System
2.0 (ROS2) even in time-critical systems like automotive. Recent years have
seen increased interest in developing model-based timing analysis and schedule
optimization approaches for ROS2-based applications. To complement these
approaches, we propose a tracing and measurement framework to obtain timing
models of ROS2-based applications. It offers a tracer based on extended
Berkeley Packet Filter (eBPF) that probes different functions in ROS2
middleware and reads their arguments or return values to reason about the data
flow in applications. It combines event traces from ROS2 and the operating
system to generate a directed acyclic graph showing ROS2 callbacks, precedence
relations between them, and their timing attributes. While being compatible
with existing analyses, we also show how to model (i)~message synchronization,
e.g., in sensor fusion, and (ii)~service requests from multiple clients, e.g.,
in motion planning. Considering that, in real-world scenarios, the application
code might be confidential and formal models are unavailable, our framework
still enables the application of existing analysis and optimization techniques.
</summary>
    <author>
      <name>Hazem Abaza</name>
    </author>
    <author>
      <name>Debayan Roy</name>
    </author>
    <author>
      <name>Shiqing Fan</name>
    </author>
    <author>
      <name>Selma Saidi</name>
    </author>
    <author>
      <name>Antonios Motakis</name>
    </author>
    <link href="http://arxiv.org/abs/2311.13333v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.13333v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.17329v1</id>
    <updated>2023-11-29T03:03:21Z</updated>
    <published>2023-11-29T03:03:21Z</published>
    <title>Cascade: A Platform for Delay-Sensitive Edge Intelligence</title>
    <summary>  Interactive intelligent computing applications are increasingly prevalent,
creating a need for AI/ML platforms optimized to reduce per-event latency while
maintaining high throughput and efficient resource management. Yet many
intelligent applications run on AI/ML platforms that optimize for high
throughput even at the cost of high tail-latency. Cascade is a new AI/ML
hosting platform intended to untangle this puzzle. Innovations include a
legacy-friendly storage layer that moves data with minimal copying and a "fast
path" that collocates data and computation to maximize responsiveness. Our
evaluation shows that Cascade reduces latency by orders of magnitude with no
loss of throughput.
</summary>
    <author>
      <name>Weijia Song</name>
    </author>
    <author>
      <name>Thiago Garrett</name>
    </author>
    <author>
      <name>Yuting Yang</name>
    </author>
    <author>
      <name>Mingzhao Liu</name>
    </author>
    <author>
      <name>Edward Tremel</name>
    </author>
    <author>
      <name>Lorenzo Rosa</name>
    </author>
    <author>
      <name>Andrea Merlina</name>
    </author>
    <author>
      <name>Roman Vitenberg</name>
    </author>
    <author>
      <name>Ken Birman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 12 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.17329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.17329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.00647v1</id>
    <updated>2023-12-01T15:18:35Z</updated>
    <published>2023-12-01T15:18:35Z</published>
    <title>MaxMem: Colocation and Performance for Big Data Applications on Tiered
  Main Memory Servers</title>
    <summary>  We present MaxMem, a tiered main memory management system that aims to
maximize Big Data application colocation and performance. MaxMem uses an
application-agnostic and lightweight memory occupancy control mechanism based
on fast memory miss ratios to provide application QoS under increasing
colocation. By relying on memory access sampling and binning to quickly
identify per-process memory heat gradients, MaxMem maximizes performance for
many applications sharing tiered main memory simultaneously. MaxMem is designed
as a user-space memory manager to be easily modifiable and extensible, without
complex kernel code development. On a system with tiered main memory consisting
of DRAM and Intel Optane persistent memory modules, our evaluation confirms
that MaxMem provides 11% and 38% better throughput and up to 80% and an order
of magnitude lower 99th percentile latency than HeMem and Linux AutoNUMA,
respectively, with a Big Data key-value store in dynamic colocation scenarios.
</summary>
    <author>
      <name>Amanda Raybuck</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The University of Texas at Austin</arxiv:affiliation>
    </author>
    <author>
      <name>Wei Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Microsoft</arxiv:affiliation>
    </author>
    <author>
      <name>Kayvan Mansoorshahi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The University of Texas at Austin</arxiv:affiliation>
    </author>
    <author>
      <name>Aditya K. Kamath</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Washington</arxiv:affiliation>
    </author>
    <author>
      <name>Mattan Erez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The University of Texas at Austin</arxiv:affiliation>
    </author>
    <author>
      <name>Simon Peter</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Washington</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.00647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.00647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.01436v1</id>
    <updated>2023-12-03T16:03:08Z</updated>
    <published>2023-12-03T16:03:08Z</published>
    <title>Robust Resource Partitioning Approach for ARINC 653 RTOS</title>
    <summary>  Modern airborne operating systems implement the concept of robust time and
resource partitioning imposed by the standards for aerospace and
airborne-embedded software systems, such as ARINC 653. While these standards do
provide a considerable amount of design choices in regards to resource
partitioning on the architectural and API levels, such as isolated memory
spaces between the application partitions, predefined resource configuration,
and unidirectional ports with limited queue and message sizes for
inter-partition communication, they do not specify how an operating system
should implement them in software. Furthermore, they often tend to set the
minimal level of the required guarantees, for example, in terms of memory
permissions, and disregard the hardware state of the art, which presently can
provide considerably stronger guarantees at no extra cost. In the paper we
present an architecture of robust resource partitioning for ARINC 653 real-time
operating systems based on completely static MMU configuration. The
architecture was implemented on different types of airborne hardware, including
platforms with TLB-based and page table-based MMU. Key benefits of the proposed
approach include minimised run-time overhead and simpler verification of the
memory subsystem.
</summary>
    <author>
      <name>Vitaly Cheptsov</name>
    </author>
    <author>
      <name>Alexey Khoroshilov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, submitted to Ivannikov ISP RAS Open Conference
  2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.01436v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.01436v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.07813v1</id>
    <updated>2023-12-13T00:23:22Z</updated>
    <published>2023-12-13T00:23:22Z</published>
    <title>On a Foundation Model for Operating Systems</title>
    <summary>  This paper lays down the research agenda for a domain-specific foundation
model for operating systems (OSes). Our case for a foundation model revolves
around the observations that several OS components such as CPU, memory, and
network subsystems are interrelated and that OS traces offer the ideal dataset
for a foundation model to grasp the intricacies of diverse OS components and
their behavior in varying environments and workloads. We discuss a wide range
of possibilities that then arise, from employing foundation models as policy
agents to utilizing them as generators and predictors to assist traditional OS
control algorithms. Our hope is that this paper spurs further research into OS
foundation models and creating the next generation of operating systems for the
evolving computing landscape.
</summary>
    <author>
      <name>Divyanshu Saxena</name>
    </author>
    <author>
      <name>Nihal Sharma</name>
    </author>
    <author>
      <name>Donghyun Kim</name>
    </author>
    <author>
      <name>Rohit Dwivedula</name>
    </author>
    <author>
      <name>Jiayi Chen</name>
    </author>
    <author>
      <name>Chenxi Yang</name>
    </author>
    <author>
      <name>Sriram Ravula</name>
    </author>
    <author>
      <name>Zichao Hu</name>
    </author>
    <author>
      <name>Aditya Akella</name>
    </author>
    <author>
      <name>Sebastian Angel</name>
    </author>
    <author>
      <name>Joydeep Biswas</name>
    </author>
    <author>
      <name>Swarat Chaudhuri</name>
    </author>
    <author>
      <name>Isil Dillig</name>
    </author>
    <author>
      <name>Alex Dimakis</name>
    </author>
    <author>
      <name>P. Brighten Godfrey</name>
    </author>
    <author>
      <name>Daehyeok Kim</name>
    </author>
    <author>
      <name>Chris Rossbach</name>
    </author>
    <author>
      <name>Gang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning for Systems Workshop at 37th NeurIPS Conference,
  2023, New Orleans, LA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.07813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.07813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.05641v1</id>
    <updated>2024-01-11T03:30:50Z</updated>
    <published>2024-01-11T03:30:50Z</published>
    <title>When eBPF Meets Machine Learning: On-the-fly OS Kernel
  Compartmentalization</title>
    <summary>  Compartmentalization effectively prevents initial corruption from turning
into a successful attack. This paper presents O2C, a pioneering system designed
to enforce OS kernel compartmentalization on the fly. It not only provides
immediate remediation for sudden threats but also maintains consistent system
availability through the enforcement process.
  O2C is empowered by the newest advancements of the eBPF ecosystem which
allows to instrument eBPF programs that perform enforcement actions into the
kernel at runtime. O2C takes the lead in embedding a machine learning model
into eBPF programs, addressing unique challenges in on-the-fly
compartmentalization. Our comprehensive evaluation shows that O2C effectively
confines damage within the compartment. Further, we validate that decision tree
is optimally suited for O2C owing to its advantages in processing tabular data,
its explainable nature, and its compliance with the eBPF ecosystem. Last but
not least, O2C is lightweight, showing negligible overhead and excellent
sacalability system-wide.
</summary>
    <author>
      <name>Zicheng Wang</name>
    </author>
    <author>
      <name>Tiejin Chen</name>
    </author>
    <author>
      <name>Qinrun Dai</name>
    </author>
    <author>
      <name>Yueqi Chen</name>
    </author>
    <author>
      <name>Hua Wei</name>
    </author>
    <author>
      <name>Qingkai Zeng</name>
    </author>
    <link href="http://arxiv.org/abs/2401.05641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.05641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.08908v1</id>
    <updated>2024-01-17T01:32:45Z</updated>
    <published>2024-01-17T01:32:45Z</published>
    <title>Herding LLaMaS: Using LLMs as an OS Module</title>
    <summary>  Computer systems are becoming increasingly heterogeneous with the emergence
of new memory technologies and compute devices. GPUs alongside CPUs have become
commonplace and CXL is poised to be a mainstay of cloud systems. The operating
system is responsible for managing these hardware resources, requiring
modification every time a new device is released. Years of research and
development are sunk into tuning the OS for high performance with each new
heterogeneous device. With the recent explosion in memory technologies and
domain-specific accelerators, it would be beneficial to have an OS that could
provide high performance for new devices without significant effort.
  We propose LLaMaS which can adapt to new devices easily. LLaMaS uses Large
Language Models (LLMs) to extract the useful features of new devices from their
textual description and uses these features to make operating system decisions
at runtime. Adding support to LLaMaS for a new device is as simple as
describing the system and new device properties in plaintext.
  LLaMaS reduces the burden on system administrators to enable easy integration
of new devices into production systems.
  Preliminary evaluation using ChatGPT shows that LLMs are capable of
extracting device features from text and make correct OS decisions based on
those features.
</summary>
    <author>
      <name>Aditya K Kamath</name>
    </author>
    <author>
      <name>Sujay Yadalam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ASPLOS 2023, Wild and Crazy Ideas session</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.08908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.08908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.06790v1</id>
    <updated>2024-03-11T15:08:11Z</updated>
    <published>2024-03-11T15:08:11Z</published>
    <title>Next4: Snapshots in Ext4 File System</title>
    <summary>  The growing value of data as a strategic asset has given rise to the
necessity of implementing reliable backup and recovery solutions in the most
efficient and cost-effective manner. The data backup methods available today on
linux are not effective enough, because while running, most of them block I/Os
to guarantee data integrity. We propose and implement Next4 - file system based
snapshot feature in Ext4 which creates an instant image of the file system, to
provide incremental versions of data, enabling reliable backup and data
recovery. In our design, the snapshot feature is implemented by efficiently
infusing the copy-on-write strategy in the write-in-place, extent based Ext4
file system, without affecting its basic structure. Each snapshot is an
incremental backup of the data within the system. What distinguishes Next4 is
the way that the data is backed up, improving both space utilization as well as
performance.
</summary>
    <author>
      <name>Aditya Dani</name>
    </author>
    <author>
      <name>Shardul Mangade</name>
    </author>
    <author>
      <name>Piyush Nimbalkar</name>
    </author>
    <author>
      <name>Harshad Shirwadkar</name>
    </author>
    <link href="http://arxiv.org/abs/2403.06790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.06790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.04036v1</id>
    <updated>2024-05-07T06:09:53Z</updated>
    <published>2024-05-07T06:09:53Z</published>
    <title>uTNT: Unikernels for Efficient and Flexible Internet Probing</title>
    <summary>  The last twenty years have seen the development and popularity of network
measurement infrastructures. Internet measurement platforms have become common
and have demonstrated their relevance in Internet understanding and security
observation. However, despite their popularity, those platforms lack of
flexibility and reactivity, as they are usually used for longitudinal
measurements. As a consequence, they may miss detecting events that are
security or Internet-related. During the same period, operating systems have
evolved to virtual machines (VMs) as self-contained units for running
applications, with the recent rise of unikernels, ultra-lightweight VMs
tailored for specific applications, eliminating the need for a host OS. In this
paper, we advocate that measurement infrastructures could take advantage of
unikernels to become more flexible and efficient. We propose uTNT, a
proof-of-concept unikernel-based implementation of TNT, a traceroute extension
able to reveal MPLS tunnels. This paper documents the full toolchain for
porting TNT into a unikernel and evaluates uTNT performance with respect to
more traditional approaches. The paper also discusses a use case in which uTNT
could find a suitable usage. uTNT source code is publicly available on Gitlab.
</summary>
    <author>
      <name>Maxime Letemple</name>
    </author>
    <author>
      <name>Gaulthier Gain</name>
    </author>
    <author>
      <name>Sami Ben Mariem</name>
    </author>
    <author>
      <name>Laurent Mathy</name>
    </author>
    <author>
      <name>Benoit Donnet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 11 figures, IFIP extended-abstract</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.04036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.04036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.09213v1</id>
    <updated>2024-05-15T09:43:47Z</updated>
    <published>2024-05-15T09:43:47Z</published>
    <title>Potential of WebAssembly for Embedded Systems</title>
    <summary>  Application virtual machines provide strong isolation properties and are
established in the context of software portability. Those opportunities make
them interesting for scalable and secure IoT deployments. WebAssembly is an
application virtual machine with origins in web browsers, that is getting
rapidly adopted in other domains. The strong and steadily growing ecosystem
makes WebAssembly an interesting candidate for Embedded Systems. This position
paper discusses the usage of WebAssembly in Embedded Systems. After introducing
the basic concepts of WebAssembly and existing runtime environments, we give an
overview of the challenges for the efficient usage of WebAssembly in Embedded
Systems. The paper concludes with a real world case study that demonstrates the
viability, before giving an outlook on open issues and upcoming work.
</summary>
    <author>
      <name>Stefan Wallentowitz</name>
    </author>
    <author>
      <name>Bastian Kersting</name>
    </author>
    <author>
      <name>Dan Mihai Dumitriu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MECO55406.2022.9797106</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MECO55406.2022.9797106" rel="related"/>
    <link href="http://arxiv.org/abs/2405.09213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.09213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.09649v1</id>
    <updated>2024-06-14T00:51:36Z</updated>
    <published>2024-06-14T00:51:36Z</published>
    <title>SquirrelFS: using the Rust compiler to check file-system crash
  consistency</title>
    <summary>  This work introduces a new approach to building crash-safe file systems for
persistent memory. We exploit the fact that Rust's typestate pattern allows
compile-time enforcement of a specific order of operations. We introduce a
novel crash-consistency mechanism, Synchronous Soft Updates, that boils down
crash safety to enforcing ordering among updates to file-system metadata. We
employ this approach to build SquirrelFS, a new file system with
crash-consistency guarantees that are checked at compile time. SquirrelFS
avoids the need for separate proofs, instead incorporating correctness
guarantees into the typestate itself. Compiling SquirrelFS only takes tens of
seconds; successful compilation indicates crash consistency, while an error
provides a starting point for fixing the bug. We evaluate SquirrelFS against
state of the art file systems such as NOVA and WineFS, and find that SquirrelFS
achieves similar or better performance on a wide range of benchmarks and
applications.
</summary>
    <author>
      <name>Hayley LeBlanc</name>
    </author>
    <author>
      <name>Nathan Taylor</name>
    </author>
    <author>
      <name>James Bornholt</name>
    </author>
    <author>
      <name>Vijay Chidambaram</name>
    </author>
    <link href="http://arxiv.org/abs/2406.09649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.09649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.10098v1</id>
    <updated>2024-07-14T06:36:19Z</updated>
    <published>2024-07-14T06:36:19Z</published>
    <title>Accelerator-as-a-Service in Public Clouds: An Intra-Host Traffic
  Management View for Performance Isolation in the Wild</title>
    <summary>  I/O devices in public clouds have integrated increasing numbers of hardware
accelerators, e.g., AWS Nitro, Azure FPGA and Nvidia BlueField. However, such
specialized compute (1) is not explicitly accessible to cloud users with
performance guarantee, (2) cannot be leveraged simultaneously by both providers
and users, unlike general-purpose compute (e.g., CPUs). Through ten
observations, we present that the fundamental difficulty of democratizing
accelerators is insufficient performance isolation support. The key obstacles
to enforcing accelerator isolation are (1) too many unknown traffic patterns in
public clouds and (2) too many possible contention sources in the datapath. In
this work, instead of scheduling such complex traffic on-the-fly and augmenting
isolation support on each system component, we propose to model traffic as
network flows and proactively re-shape the traffic to avoid unpredictable
contention. We discuss the implications of our findings on the design of future
I/O management stacks and device interfaces.
</summary>
    <author>
      <name>Jiechen Zhao</name>
    </author>
    <author>
      <name>Ran Shu</name>
    </author>
    <author>
      <name>Katie Lim</name>
    </author>
    <author>
      <name>Zewen Fan</name>
    </author>
    <author>
      <name>Thomas Anderson</name>
    </author>
    <author>
      <name>Mingyu Gao</name>
    </author>
    <author>
      <name>Natalie Enright Jerger</name>
    </author>
    <link href="http://arxiv.org/abs/2407.10098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.10098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.02911v2</id>
    <updated>2025-03-03T03:08:56Z</updated>
    <published>2024-08-06T02:51:22Z</published>
    <title>Boosting File Systems Elegantly: A Transparent NVM Write-ahead Log for
  Disk File Systems</title>
    <summary>  We propose NVLog, an NVM-based write-ahead log for disk file systems,
designed to transparently harness the high performance of NVM within the legacy
storage stack. NVLog provides on-demand byte-granularity sync absorption,
reserving the fast DRAM path for asynchronous operations, meanwhile occupying
NVM space only temporarily. To accomplish this, we designed a highly efficient
log structure, developed mechanisms to address heterogeneous crash consistency,
optimized for small writes, and implemented robust crash recovery and garbage
collection methods. Compared to previous solutions, NVLog is lighter, more
stable, and delivers higher performance, all while leveraging the mature kernel
software stack and avoiding data migration overhead. Experimental results
demonstrate that NVLog can accelerate disk file systems by up to 15.09x and
outperform NOVA and SPFS in various scenarios by up to 3.72x and 324.11x,
respectively.
</summary>
    <author>
      <name>Guoyu Wang</name>
    </author>
    <author>
      <name>Xilong Che</name>
    </author>
    <author>
      <name>Haoyang Wei</name>
    </author>
    <author>
      <name>Shuo Chen</name>
    </author>
    <author>
      <name>Puyi He</name>
    </author>
    <author>
      <name>Juncheng Hu</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 23rd USENIX Conference on File and Storage
  Technologies (FAST '25) (2025) Article 2 19-34</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2408.02911v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.02911v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.04856v1</id>
    <updated>2024-08-09T04:22:50Z</updated>
    <published>2024-08-09T04:22:50Z</published>
    <title>Wasm-bpf: Streamlining eBPF Deployment in Cloud Environments with
  WebAssembly</title>
    <summary>  The extended Berkeley Packet Filter (eBPF) is extensively utilized for
observability and performance analysis in cloud-native environments. However,
deploying eBPF programs across a heterogeneous cloud environment presents
challenges, including compatibility issues across different kernel versions,
operating systems, runtimes, and architectures. Traditional deployment methods,
such as standalone containers or tightly integrated core applications, are
cumbersome and inefficient, particularly when dynamic plugin management is
required. To address these challenges, we introduce Wasm-bpf, a lightweight
runtime on WebAssembly and the WebAssembly System Interface (WASI). Leveraging
Wasm platform independence and WASI standardized system interface, with
enhanced relocation for different architectures, Wasm-bpf ensures
cross-platform compatibility for eBPF programs. It simplifies deployment by
integrating with container toolchains, allowing eBPF programs to be packaged as
Wasm modules that can be easily managed within cloud environments.
Additionally, Wasm-bpf supports dynamic plugin management in WebAssembly. Our
implementation and evaluation demonstrate that Wasm-bpf introduces minimal
overhead compared to native eBPF implementations while simplifying the
deployment process.
</summary>
    <author>
      <name>Yusheng Zheng</name>
    </author>
    <author>
      <name>Tong Yu</name>
    </author>
    <author>
      <name>Yiwei Yang</name>
    </author>
    <author>
      <name>Andrew Quinn</name>
    </author>
    <link href="http://arxiv.org/abs/2408.04856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.04856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.13772v2</id>
    <updated>2024-08-27T13:18:42Z</updated>
    <published>2024-08-25T08:44:45Z</published>
    <title>FRAP: A Flexible Resource Accessing Protocol for Multiprocessor
  Real-Time Systems</title>
    <summary>  Fully-partitioned fixed-priority scheduling (FP-FPS) multiprocessor systems
are widely found in real-time applications, where spin-based protocols are
often deployed to manage the mutually exclusive access of shared resources.
Unfortunately, existing approaches either enforce rigid spin priority rules for
resource accessing or carry significant pessimism in the schedulability
analysis, imposing substantial blocking time regardless of task execution
urgency or resource over-provisioning. This paper proposes FRAP, a spin-based
flexible resource accessing protocol for FP-FPS systems. A task under FRAP can
spin at any priority within a range for accessing a resource, allowing flexible
and fine-grained resource control with predictable worst-case behaviour. Under
flexible spinning, we demonstrate that the existing analysis techniques can
lead to incorrect timing bounds and present a novel MCMF (minimum cost maximum
flow)-based blocking analysis, providing predictability guarantee for FRAP. A
spin priority assignment is reported that fully exploits flexible spinning to
reduce the blocking time of tasks with high urgency, enhancing the performance
of FRAP. Experimental results show that FRAP outperforms the existing
spin-based protocols in schedulability by 15.20%-32.73% on average, up to
65.85%.
</summary>
    <author>
      <name>Shuai Zhao</name>
    </author>
    <author>
      <name>Hanzhi Xu</name>
    </author>
    <author>
      <name>Nan Chen</name>
    </author>
    <author>
      <name>Ruoxian Su</name>
    </author>
    <author>
      <name>Wanli Chang</name>
    </author>
    <link href="http://arxiv.org/abs/2408.13772v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.13772v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.01580v1</id>
    <updated>2024-09-03T03:28:42Z</updated>
    <published>2024-09-03T03:28:42Z</published>
    <title>Foreactor: Exploiting Storage I/O Parallelism with Explicit Speculation</title>
    <summary>  We introduce explicit speculation, a variant of I/O speculation technique
where I/O system calls can be parallelized under the guidance of explicit
application code knowledge. We propose a formal abstraction -- the foreaction
graph -- which describes the exact pattern of I/O system calls in an
application function as well as any necessary computation associated to produce
their argument values. I/O system calls can be issued ahead of time if the
graph says it is safe and beneficial to do so. With explicit speculation,
serial applications can exploit storage I/O parallelism without involving
expensive prediction or checkpointing mechanisms.
  Based on explicit speculation, we implement Foreactor, a library framework
that allows application developers to concretize foreaction graphs and enable
concurrent I/O with little or no modification to application source code.
Experimental results show that Foreactor is able to improve the performance of
both synthetic benchmarks and real applications by significant amounts
(29%-50%).
</summary>
    <author>
      <name>Guanzhou Hu</name>
    </author>
    <author>
      <name>Andrea Arpaci-Dusseau</name>
    </author>
    <author>
      <name>Remzi Arpaci-Dusseau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.01580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.01580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.11220v1</id>
    <updated>2024-09-17T14:13:03Z</updated>
    <published>2024-09-17T14:13:03Z</published>
    <title>eBPF-mm: Userspace-guided memory management in Linux with eBPF</title>
    <summary>  We leverage eBPF in order to implement custom policies in the Linux memory
subsystem. Inspired by CBMM, we create a mechanism that provides the kernel
with hints regarding the benefit of promoting a page to a specific size. We
introduce a new hook point in Linux page fault handling path for eBPF programs,
providing them the necessary context to determine the page size to be used. We
then develop a framework that allows users to define profiles for their
applications and load them into the kernel. A profile consists of memory
regions of interest and their expected benefit from being backed by 4KB, 64KB
and 2MB pages. In our evaluation, we profiled our workloads to identify hot
memory regions using DAMON.
</summary>
    <author>
      <name>Konstantinos Mores</name>
    </author>
    <author>
      <name>Stratos Psomadakis</name>
    </author>
    <author>
      <name>Georgios Goumas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SRC@MICRO'24</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.11220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.11220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.14381v2</id>
    <updated>2024-10-22T11:27:06Z</updated>
    <published>2024-10-18T11:08:15Z</published>
    <title>Optimizing over FP/EDF Execution Times: Known Results and Open Problems</title>
    <summary>  In many use cases the execution time of tasks is unknown and can be chosen by
the designer to increase or decrease the application features depending on the
availability of processing capacity. If the application has real-time
constraints, such as deadlines, then the necessary and sufficient
schedulability test must allow the execution times to be left unspecified. By
doing so, the designer can then perform optimization of the execution times by
picking the schedulable values that minimize any given cost.
  In this paper, we review existing results on the formulation of both the
Fixed Priority and Earliest Deadline First exact schedulability constraints.
The reviewed formulations are expressed by a combination of linear constraints,
which enables then optimization routines.
</summary>
    <author>
      <name>Enrico Bini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at OPERA 2024 (https://opera24.di.unito.it/) This work is
  partially supported by the project "Trustworthy Cyber-Physical Pipelines",
  funded by the MAECI Italy-Sweden co-operation id. PGR02086, and the spoke
  "FutureHPC and BigData" of the ICSC - Centro Nazionale di Ricerca in
  High-Performance Computing, Big Data and Quantum Computing funded by European
  Union -NextGenerationEU</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.14381v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.14381v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18308v1</id>
    <updated>2024-10-23T22:35:55Z</updated>
    <published>2024-10-23T22:35:55Z</published>
    <title>Exact schedulability test for sporadic mixed-criticality real-time
  systems using antichains and oracles</title>
    <summary>  This work addresses the problem of exact schedulability assessment in
uniprocessor mixed-criticality real-time systems with sporadic task sets. We
model the problem by means of a finite automaton that has to be explored in
order to check for schedulability. To mitigate the state explosion problem, we
provide a generic algorithm which is parameterised by several techniques called
oracles and simulation relations. These techniques leverage results from the
scheduling literature as "plug-ins" that make the algorithm more efficient in
practice. Our approach achieves up to a 99.998% reduction in the search space
required for exact schedulability testing, making it practical for a range of
task sets, up to 8 tasks or maximum periods of 350. This method enables to
challenge the pessimism of an existing schedulability test and to derive a new
dynamic-priority scheduler, demonstrating its good performance. This is the
full version of an RTNS 2024 paper.
</summary>
    <author>
      <name>Simon Picard</name>
    </author>
    <author>
      <name>Antonio Paolillo</name>
    </author>
    <author>
      <name>Gilles Geeraerts</name>
    </author>
    <author>
      <name>Joël Goossens</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3696355.3699702</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3696355.3699702" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In the proceedings of 32nd International Conference on Real-Time
  Networks and Systems, RTNS24, ACM, 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.23661v1</id>
    <updated>2024-10-31T06:21:44Z</updated>
    <published>2024-10-31T06:21:44Z</published>
    <title>Microsecond-scale Dynamic Validation of Idempotency for GPU Kernels</title>
    <summary>  We discovered that a GPU kernel can have both idempotent and non-idempotent
instances depending on the input. These kernels, called
conditionally-idempotent, are prevalent in real-world GPU applications (490 out
of 547 from six applications). Consequently, prior work that classifies GPU
kernels as either idempotent or non-idempotent can severely compromise the
correctness or efficiency of idempotence-based systems. This paper presents
PICKER, the first system for instance-level idempotency validation. PICKER
dynamically validates the idempotency of GPU kernel instances before their
execution, by utilizing their launch arguments. Several optimizations are
proposed to significantly reduce validation latency to microsecond-scale.
Evaluations using representative GPU applications (547 kernels and 18,217
instances in total) show that PICKER can identify idempotent instances with no
false positives and a false-negative rate of 18.54%, and can complete the
validation within 5 us for all instances. Furthermore, by integrating PICKER, a
fault-tolerant system can reduce the checkpoint cost to less than 4% and a
scheduling system can reduce the preemption latency by 84.2%.
</summary>
    <author>
      <name>Mingcong Han</name>
    </author>
    <author>
      <name>Weihang Shen</name>
    </author>
    <author>
      <name>Guanwen Peng</name>
    </author>
    <author>
      <name>Rong Chen</name>
    </author>
    <author>
      <name>Haibo Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2410.23661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.23661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.08938v1</id>
    <updated>2024-12-12T04:59:25Z</updated>
    <published>2024-12-12T04:59:25Z</published>
    <title>Mercury: QoS-Aware Tiered Memory System</title>
    <summary>  Memory tiering has received wide adoption in recent years as an effective
solution to address the increasing memory demands of memory-intensive
workloads. However, existing tiered memory systems often fail to meet
service-level objectives (SLOs) when multiple applications share the system
because they lack Quality-of-Service (QoS) support. Consequently, applications
suffer severe performance drops due to local memory contention and memory
bandwidth interference.
  In this paper, we present Mercury, a QoS-aware tiered memory system that
ensures predictable performance for coexisting memory-intensive applications
with different SLOs. Mercury enables per-tier page reclamation for
application-level resource management and uses a proactive admission control
algorithm to satisfy SLOs via per-tier memory capacity allocation and intra-
and inter-tier bandwidth interference mitigation. It reacts to dynamic
requirement changes via real-time adaptation. Extensive evaluations show that
Mercury improves application performance by up to 53.4% and 20.3% compared to
TPP and Colloid, respectively.
</summary>
    <author>
      <name>Jiaheng Lu</name>
    </author>
    <author>
      <name>Yiwen Zhang</name>
    </author>
    <author>
      <name>Hasan Al Maruf</name>
    </author>
    <author>
      <name>Minseo Park</name>
    </author>
    <author>
      <name>Yunxuan Tang</name>
    </author>
    <author>
      <name>Fan Lai</name>
    </author>
    <author>
      <name>Mosharaf Chowdhury</name>
    </author>
    <link href="http://arxiv.org/abs/2412.08938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.08938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.12491v1</id>
    <updated>2024-12-17T02:46:09Z</updated>
    <published>2024-12-17T02:46:09Z</published>
    <title>Optimizing System Memory Bandwidth with Micron CXL Memory Expansion
  Modules on Intel Xeon 6 Processors</title>
    <summary>  High-Performance Computing (HPC) and Artificial Intelligence (AI) workloads
typically demand substantial memory bandwidth and, to a degree, memory
capacity. CXL memory expansion modules, also known as CXL "type-3" devices,
enable enhancements in both memory capacity and bandwidth for server systems by
utilizing the CXL protocol which runs over the PCIe interfaces of the
processor. This paper discusses experimental findings on achieving increased
memory bandwidth for HPC and AI workloads using Micron's CXL modules. This is
the first study that presents real data experiments utilizing eight CXL E3.S
(x8) Micron CZ122 devices on the Intel Xeon 6 processor 6900P (previously
codenamed Granite Rapids AP) featuring 128 cores, alongside Micron DDR-5 memory
operating at 6400 MT/s on each of the CPU's 12 DRAM channels. The eight CXL
memories were set up as a unified NUMA configuration, employing software-based
page level interleaving mechanism, available in Linux kernel v6.9+, between
DDR5 and CXL memory nodes to improve overall system bandwidth. Memory expansion
via CXL boosts read-only bandwidth by 24% and mixed read/write bandwidth by up
to 39%. Across HPC and AI workloads, the geometric mean of performance speedups
is 24%.
</summary>
    <author>
      <name>Rohit Sehgal</name>
    </author>
    <author>
      <name>Vishal Tanna</name>
    </author>
    <author>
      <name>Vinicius Petrucci</name>
    </author>
    <author>
      <name>Anil Godbole</name>
    </author>
    <link href="http://arxiv.org/abs/2412.12491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.12491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.00994v2</id>
    <updated>2025-01-06T06:36:01Z</updated>
    <published>2025-01-02T01:11:18Z</published>
    <title>Exploiting Application-to-Architecture Dependencies for Designing
  Scalable OS</title>
    <summary>  With the advent of hundreds of cores on a chip to accelerate applications,
the operating system (OS) needs to exploit the existing parallelism provided by
the underlying hardware resources to determine the right amount of processes to
be mapped on the multi-core systems. However, the existing OS is not scalable
and is oblivious to applications. We address these issues by adopting a
multi-layer network representation of the dynamic application-to
OS-to-architecture dependencies, namely the NetworkedOS. We adopt a
compile-time analysis and construct a network representing the dependencies
between dynamic instructions translated from the applications and the kernel
and services. We propose an overlapping partitioning scheme to detect the
clusters or processes that can potentially run in parallel to be mapped onto
cores while reducing the number of messages transferred. At run time, processes
are mapped onto the multi-core systems, taking into consideration the process
affinity. Our experimental results indicate that NetworkedOS achieves
performance improvement as high as 7.11x compared to Linux running on a
128-core system and 2.01x to Barrelfish running on a 64-core system.
</summary>
    <author>
      <name>Yao Xiao</name>
    </author>
    <author>
      <name>Nikos Kanakaris</name>
    </author>
    <author>
      <name>Anzhe Cheng</name>
    </author>
    <author>
      <name>Chenzhong Yin</name>
    </author>
    <author>
      <name>Nesreen K. Ahmed</name>
    </author>
    <author>
      <name>Shahin Nazarian</name>
    </author>
    <author>
      <name>Andrei Irimia</name>
    </author>
    <author>
      <name>Paul Bogdan</name>
    </author>
    <link href="http://arxiv.org/abs/2501.00994v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.00994v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.10138v2</id>
    <updated>2025-04-23T10:48:52Z</updated>
    <published>2025-01-17T12:01:28Z</published>
    <title>The NIC should be part of the OS</title>
    <summary>  The network interface adapter (NIC) is a critical component of a cloud server
occupying a unique position. Not only is network performance vital to efficient
operation of the machine, but unlike compute accelerators like GPUs, the
network subsystem must react to unpredictable events like the arrival of a
network packet and communicate with the appropriate application end point with
minimal latency.
  Current approaches to server stacks navigate a trade-off between flexibility,
efficiency, and performance: the fastest kernel-bypass approaches dedicate
cores to applications, busy-wait on receive queues, etc. while more flexible
approaches appropriate to more dynamic workload mixes incur much greater
software overhead on the data path.
  However, we reject this trade-off, which we ascribe to an arbitrary (and
sub-optimal) split in system state between the OS and the NIC. Instead, by
exploiting the properties of cache-coherent interconnects and integrating the
NIC closely with the OS kernel, we can achieve something surprising:
performance for RPC workloads better than the fastest kernelbypass approaches
without sacrificing the robustness and dynamic adaptation of kernel-based
network subsystems.
</summary>
    <author>
      <name>Pengcheng Xu</name>
    </author>
    <author>
      <name>Timothy Roscoe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3713082.3730388</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3713082.3730388" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Camera ready for HotOS'25</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.10138v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.10138v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.05413v1</id>
    <updated>2025-02-08T02:43:10Z</updated>
    <published>2025-02-08T02:43:10Z</published>
    <title>XPUTimer: Anomaly Diagnostics for Divergent LLM Training in GPU Clusters
  of Thousand-Plus Scale</title>
    <summary>  The rapid proliferation of large language models has driven the need for
efficient GPU training clusters. However, ensuring high-performance training in
these clusters is challenging due to the complexity of software-hardware
interactions and the frequent occurrence of training anomalies. Since existing
diagnostic tools are narrowly tailored to specific issues, there are gaps in
their ability to address anomalies spanning the entire training stack. In
response, we introduce XPUTimer, a real-time diagnostic framework designed for
distributed LLM training at scale. XPUTimer first integrates a lightweight
tracing daemon to monitor key code segments with minimal overhead.
Additionally, it features a diagnostic engine that employs novel intra-kernel
tracing and holistic aggregated metrics to efficiently identify and resolve
anomalies. Deployment of XPUTimer across 6,000 GPUs over eight months
demonstrated significant improvements across the training stack, validating its
effectiveness in real-world scenarios.
</summary>
    <author>
      <name>Weihao Cui</name>
    </author>
    <author>
      <name>Ji Zhang</name>
    </author>
    <author>
      <name>Han Zhao</name>
    </author>
    <author>
      <name>Chao Liu</name>
    </author>
    <author>
      <name>Wenhao Zhang</name>
    </author>
    <author>
      <name>Jian Sha</name>
    </author>
    <author>
      <name>Quan Chen</name>
    </author>
    <author>
      <name>Bingsheng He</name>
    </author>
    <author>
      <name>Minyi Guo</name>
    </author>
    <link href="http://arxiv.org/abs/2502.05413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.05413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.13163v2</id>
    <updated>2025-02-20T09:52:47Z</updated>
    <published>2025-02-17T02:53:02Z</published>
    <title>A Survey of Fuzzing Open-Source Operating Systems</title>
    <summary>  Vulnerabilities in open-source operating systems (OSs) pose substantial
security risks to software systems, making their detection crucial. While
fuzzing has been an effective vulnerability detection technique in various
domains, OS fuzzing (OSF) faces unique challenges due to OS complexity and
multi-layered interaction, and has not been comprehensively reviewed.
Therefore, this work systematically surveys the state-of-the-art OSF
techniques, categorizes them based on the general fuzzing process, and
investigates challenges specific to kernel, file system, driver, and hypervisor
fuzzing. Finally, future research directions for OSF are discussed. GitHub:
https://github.com/pghk13/Survey-OSF.
</summary>
    <author>
      <name>Kun Hu</name>
    </author>
    <author>
      <name>Qicai Chen</name>
    </author>
    <author>
      <name>Zilong Lu</name>
    </author>
    <author>
      <name>Wenzhuo Zhang</name>
    </author>
    <author>
      <name>Bihuan Chen</name>
    </author>
    <author>
      <name>You Lu</name>
    </author>
    <author>
      <name>Haowen Jiang</name>
    </author>
    <author>
      <name>Bingkun Sun</name>
    </author>
    <author>
      <name>Xin Peng</name>
    </author>
    <author>
      <name>Wenyun Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">45 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.13163v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.13163v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.14987v1</id>
    <updated>2025-02-20T19:22:23Z</updated>
    <published>2025-02-20T19:22:23Z</published>
    <title>Taming and Controlling Performance and Energy Trade-offs Automatically
  in Network Applications</title>
    <summary>  In this paper, we demonstrate that a server running a single
latency-sensitive application can be treated as a black box to reduce energy
consumption while meeting an SLA target. We find that when the mean offered
load is stable, one can find the "sweet spot" settings in packet batching (via
interrupt coalescing) and controlling the processing rate (DVFS) that
represents optimal trade-offs in the interactions of the software stack and
hardware with the arrival rate and composition of requests currently being
served. Trying a few combinations of settings on the live system, an example
Bayesian optimizer can find settings that reduce the energy consumption to meet
a desired tail latency for the current load.
  This research demonstrates that: 1) without software changes, dramatic energy
savings (up to 60%) can be achieved across diverse hardware systems if one
controls batching and processing rate, 2) specialized research OSes that have
been developed for performance can achieve more than 2x better energy
efficiency than general-purpose OSes, and 3) a controller, agnostic to the
application and system, can easily find energy-efficient settings for the
offered load that meets SLA objectives.
</summary>
    <author>
      <name>Han Dong</name>
    </author>
    <author>
      <name>Yara Awad</name>
    </author>
    <author>
      <name>Sanjay Arora</name>
    </author>
    <author>
      <name>Orran Krieger</name>
    </author>
    <author>
      <name>Jonathan Appavoo</name>
    </author>
    <link href="http://arxiv.org/abs/2502.14987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.14987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.5.0; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.03777v1</id>
    <updated>2025-03-04T20:08:03Z</updated>
    <published>2025-03-04T20:08:03Z</published>
    <title>FlexInfer: Breaking Memory Constraint via Flexible and Efficient
  Offloading for On-Device LLM Inference</title>
    <summary>  Large Language Models (LLMs) face challenges for on-device inference due to
high memory demands. Traditional methods to reduce memory usage often
compromise performance and lack adaptability. We propose FlexInfer, an
optimized offloading framework for on-device inference, addressing these issues
with techniques like asynchronous prefetching, balanced memory locking, and
flexible tensor preservation. These strategies enhance memory efficiency and
mitigate I/O bottlenecks, ensuring high performance within user-specified
resource constraints. Experiments demonstrate that FlexInfer significantly
improves throughput under limited resources, achieving up to 12.5 times better
performance than existing methods and facilitating the deployment of large
models on resource-constrained devices.
</summary>
    <author>
      <name>Hongchao Du</name>
    </author>
    <author>
      <name>Shangyu Wu</name>
    </author>
    <author>
      <name>Arina Kharlamova</name>
    </author>
    <author>
      <name>Nan Guan</name>
    </author>
    <author>
      <name>Chun Jason Xue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures, to be published in EuroMLSys '25</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.03777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.03777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.06421v1</id>
    <updated>2025-03-09T03:33:15Z</updated>
    <published>2025-03-09T03:33:15Z</published>
    <title>Efficient Function-as-a-Service for Large Language Models with TIDAL</title>
    <summary>  Large Language Model (LLM) applications have emerged as a prominent use case
for Function-as-a-Service (FaaS) due to their high computational demands and
sporadic invocation patterns. However, serving LLM functions within FaaS
frameworks faces significant GPU-side cold start. A fundamental approach
involves leveraging a template with function state saved on GPUs to bypass the
cold start for new invocations. Yet, this approach struggles with the high GPU
footprint, dynamic initialization behaviors, and lazy GPU kernel loading
inherent in LLM functions, primarily due to a lack of insight into the
underlying execution details. In this paper, we introduce TIDAL, an optimized
FaaS framework for LLM applications that achieves fast startups by tracing
fine-grained execution paths. By utilizing the traced execution details, TIDAL
generates adaptive function templates, effectively breaking startup barriers
for LLM functions. Extensive evaluations demonstrate that TIDAL reduces cold
start latency by $1.79\times\text{\textasciitilde}2.11\times$ and improves the
$95\%$-ile time-to-first-token by $76.0\%$, surpassing state-of-the-art
methods.
</summary>
    <author>
      <name>Weihao Cui</name>
    </author>
    <author>
      <name>Ziyi Xu</name>
    </author>
    <author>
      <name>Han Zhao</name>
    </author>
    <author>
      <name>Quan Chen</name>
    </author>
    <author>
      <name>Zijun Li</name>
    </author>
    <author>
      <name>Bingsheng He</name>
    </author>
    <author>
      <name>Minyi Guo</name>
    </author>
    <link href="http://arxiv.org/abs/2503.06421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.06421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.09663v1</id>
    <updated>2025-03-12T15:50:16Z</updated>
    <published>2025-03-12T15:50:16Z</published>
    <title>BYOS: Knowledge-driven Large Language Models Bring Your Own Operating
  System More Excellent</title>
    <summary>  Kernel configurations play an important role in the performance of Operating
System (OS). However, with the rapid iteration of OS, finding the proper
configurations that meet specific requirements can be challenging, which can be
primarily attributed to the default kernel provided by vendors does not take
the requirements of specific workloads into account, and the heavyweight tuning
process cannot catch up with the rapid evolving pace of the kernel. To address
these challenges, we propose BYOS, a novel framework powered by Large Language
Models (LLMs) to customize kernel configurations for diverse user requirements.
By integrating OS-oriented Dual-layer Knowledge Graph (OD-KG) and corresponding
reasoning strategy, BYOS enhanced the LLM's understanding of the
characteristics and capabilities of OS, thus enabling customized,
cost-effective, and convenient generation of kernel configurations. Experiments
show that the kernels configured by BYOS outperform the default
vendor-configured kernels by 7.1% to 155.4%, demonstrating the effectiveness
and efficiency of BYOS in customizing kernel configurations. Our code is
available at https://github.com/LHY-24/BYOS.
</summary>
    <author>
      <name>Hongyu Lin</name>
    </author>
    <author>
      <name>Yuchen Li</name>
    </author>
    <author>
      <name>Haoran Luo</name>
    </author>
    <author>
      <name>Kaichun Yao</name>
    </author>
    <author>
      <name>Libo Zhang</name>
    </author>
    <author>
      <name>Mingjie Xing</name>
    </author>
    <author>
      <name>Yanjun Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2503.09663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.09663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.18191v1</id>
    <updated>2025-03-23T20:18:16Z</updated>
    <published>2025-03-23T20:18:16Z</published>
    <title>Enabling the Write-Back Page Cache with Strong Consistency in
  Distributed Userspace File Systems</title>
    <summary>  The large-scale, multi-tenant nature of cloud computing requires distributed
file systems that offer stability, adaptability, and compatibility. FUSE-based
distributed file systems have emerged as a popular solution for the cloud,
offering fast deployment, fault isolation, and POSIX compliance. However,
FUSE's performance limitations, particularly its inability to reconcile page
caching with strong consistency in distributed environments, remain a
persistent problem. Existing approaches either sacrifice consistency for
performance or rely on inefficient caching, limiting their practicality.
  To this end, we present DistFUSE, the first FUSE-based distributed file
system that relies on a write-back kernel-based page cache for performance and
provides strong consistency. DistFUSE achieves this by offloading userspace
lock management to the kernel driver, allowing coordinated access to the
kernel's page cache across nodes. This design eliminates blind local cache
updates and ensures cluster-wide consistency without compromising performance.
Our evaluation shows DistFUSE improves throughput by up to 75% compared to
baseline approaches.
</summary>
    <author>
      <name>Haoyu Li</name>
    </author>
    <author>
      <name>Jingkai Fu</name>
    </author>
    <author>
      <name>Qing Li</name>
    </author>
    <author>
      <name>Windsor Hsu</name>
    </author>
    <author>
      <name>Asaf Cidon</name>
    </author>
    <link href="http://arxiv.org/abs/2503.18191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.18191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.23068v1</id>
    <updated>2025-03-29T13:02:00Z</updated>
    <published>2025-03-29T13:02:00Z</published>
    <title>Linux for Everyone: Can Standardization Drive Mainstream Adoption?</title>
    <summary>  Despite its technical superiority and flexibility, Linux remains a niche OS
in the consumer markets. Because fragmentation stems from diverse
distributions, it lacks the standardized experience, which discourages
mainstream adoption. This foundational paper explores whether a balanced
approach to standardization can bridge this gap without compromising Linux's
core philosophy of freedom and openness. We analyze historical attempts at
unification, such as Flatpak, Wayland, and Snap, identifying reasons for their
limited success. Using case studies and statistical insights, we understand how
fragmentation affects developers, designers, management users, and gaming
users. The paper proposes a standardized yet modular Linux ecosystem ensuring
adaptability for new users and flexibility for power users. Rather than giving
a technical solution, this paper discusses the feasibility of a unified Linux
experience by providing the groundwork for structured standardization. We aim
to inspire future research as well for positioning Linux as a viable
alternative to Windows and MacOS without sacrificing its open--source nature.
</summary>
    <author>
      <name>Rohit J Nandha</name>
    </author>
    <author>
      <name>Ronak D Patel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, both the authors have contributed equally</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.23068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.23068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.23952v1</id>
    <updated>2025-03-31T11:11:16Z</updated>
    <published>2025-03-31T11:11:16Z</published>
    <title>HeteroPod: XPU-Accelerated Infrastructure Offloading for Commodity
  Cloud-Native Applications</title>
    <summary>  Cloud-native systems increasingly rely on infrastructure services (e.g.,
service meshes, monitoring agents), which compete for resources with user
applications, degrading performance and scalability. We propose HeteroPod, a
new abstraction that offloads these services to Data Processing Units (DPUs) to
enforce strict isolation while reducing host resource contention and
operational costs. To realize HeteroPod, we introduce HeteroNet, a cross-PU
(XPU) network system featuring: (1) split network namespace, a unified network
abstraction for processes spanning CPU and DPU, and (2) elastic and efficient
XPU networking, a communication mechanism achieving shared-memory performance
without pinned resource overhead and polling costs. By leveraging HeteroNet and
the compositional nature of cloud-native workloads, HeteroPod can optimally
offload infrastructure containers to DPUs. We implement HeteroNet based on
Linux, and implement a cloud-native system called HeteroK8s based on
Kubernetes. We evaluate the systems using NVIDIA Bluefield-2 DPUs and CXL-based
DPUs (simulated with real CXL memory devices). The results show that HeteroK8s
effectively supports complex (unmodified) commodity cloud-native applications
(up to 1 million LoC) and provides up to 31.9x better latency and 64x less
resource consumption (compared with kernel-bypass design), 60% better
end-to-end latency, and 55% higher scalability compared with SOTA systems.
</summary>
    <author>
      <name>Bicheng Yang</name>
    </author>
    <author>
      <name>Jingkai He</name>
    </author>
    <author>
      <name>Dong Du</name>
    </author>
    <author>
      <name>Yubin Xia</name>
    </author>
    <author>
      <name>Haibo Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2503.23952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.23952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.04874v1</id>
    <updated>2025-04-07T09:28:54Z</updated>
    <published>2025-04-07T09:28:54Z</published>
    <title>Futureproof Static Memory Planning</title>
    <summary>  The NP-complete combinatorial optimization task of assigning offsets to a set
of buffers with known sizes and lifetimes so as to minimize total memory usage
is called dynamic storage allocation (DSA). Existing DSA implementations bypass
the theoretical state-of-the-art algorithms in favor of either fast but
wasteful heuristics, or memory-efficient approaches that do not scale beyond
one thousand buffers. The "AI memory wall", combined with deep neural networks'
static architecture, has reignited interest in DSA. We present idealloc, a
low-fragmentation, high-performance DSA implementation designed for
million-buffer instances. Evaluated on a novel suite of particularly hard
benchmarks from several domains, idealloc ranks first against four production
implementations in terms of a joint effectiveness/robustness criterion.
</summary>
    <author>
      <name>Christos Lamprakos</name>
    </author>
    <author>
      <name>Panagiotis Xanthopoulos</name>
    </author>
    <author>
      <name>Manolis Katsaragakis</name>
    </author>
    <author>
      <name>Sotirios Xydis</name>
    </author>
    <author>
      <name>Dimitrios Soudris</name>
    </author>
    <author>
      <name>Francky Catthoor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ACM TOPLAS</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.04874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.04874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06151v2</id>
    <updated>2025-05-13T21:03:41Z</updated>
    <published>2025-04-08T15:46:52Z</published>
    <title>Zerrow: True Zero-Copy Arrow Pipelines in Bauplan</title>
    <summary>  Bauplan is a FaaS-based lakehouse specifically built for data pipelines: its
execution engine uses Apache Arrow for data passing between the nodes in the
DAG. While Arrow is known as the "zero copy format", in practice, limited Linux
kernel support for shared memory makes it difficult to avoid copying entirely.
In this work, we introduce several new techniques to eliminate nearly all
copying from pipelines: in particular, we implement a new kernel module that
performs de-anonymization, thus eliminating a copy to intermediate data. We
conclude by sharing our preliminary evaluation on different workloads types, as
well as discussing our plan for future improvements.
</summary>
    <author>
      <name>Yifan Dai</name>
    </author>
    <author>
      <name>Jacopo Tagliabue</name>
    </author>
    <author>
      <name>Andrea Arpaci-Dusseau</name>
    </author>
    <author>
      <name>Remzi Arpaci-Dusseau</name>
    </author>
    <author>
      <name>Tyler R. Caraza-Harter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pre-print conf submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.06151v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06151v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.14489v2</id>
    <updated>2025-04-22T15:19:48Z</updated>
    <published>2025-04-20T04:46:34Z</published>
    <title>Optimizing SLO-oriented LLM Serving with PD-Multiplexing</title>
    <summary>  Modern LLM services demand high throughput and stringent SLO guarantees
across two distinct inference phases-prefill and decode-and complex multi-turn
workflows. However, current systems face a fundamental tradeoff: out-of-place
compute partition enables per-phase SLO attainment, while in-place memory
sharing maximizes throughput via KV cache reuse. Moreover, existing in-place
compute partition also encounters low utilization and high overhead due to
phase-coupling design. We present Drift, a new LLM serving framework that
resolves this tension via PD multiplexing, enabling in-place and
phase-decoupled compute partition. Drift leverages low-level GPU partitioning
techniques to multiplex prefill and decode phases spatially and adaptively on
shared GPUs, while preserving in-place memory sharing. To fully leverage the
multiplexing capability, Drift introduces an adaptive gang scheduling
mechanism, a contention-free modeling method, and a SLO-aware dispatching
policy. Evaluation shows that Drift achieves an average $5.1\times$ throughput
improvement (up to $17.5\times$) over state-of-the-art baselines, while
consistently meeting SLO targets under complex LLM workloads.
</summary>
    <author>
      <name>Weihao Cui</name>
    </author>
    <author>
      <name>Yukang Chen</name>
    </author>
    <author>
      <name>Han Zhao</name>
    </author>
    <author>
      <name>Ziyi Xu</name>
    </author>
    <author>
      <name>Quan Chen</name>
    </author>
    <author>
      <name>Xusheng Chen</name>
    </author>
    <author>
      <name>Yangjie Zhou</name>
    </author>
    <author>
      <name>Shixuan Sun</name>
    </author>
    <author>
      <name>Minyi Guo</name>
    </author>
    <link href="http://arxiv.org/abs/2504.14489v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.14489v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.16251v2</id>
    <updated>2025-05-14T19:58:34Z</updated>
    <published>2025-04-22T20:28:44Z</published>
    <title>Adaptive and Efficient Dynamic Memory Management for Hardware Enclaves</title>
    <summary>  The second version of Intel Software Guard Extensions (Intel SGX), or SGX2,
adds dynamic management of enclave memory and threads. The first version
required the address space and thread counts to be fixed before execution. The
Enclave Dynamic Memory Management (EDMM) feature of SGX2 has the potential to
lower launch times and overall execution time. Despite reducing the enclave
loading time by 28--93%, straightforward EDMM adoption strategies actually slow
execution time down by as much as 58%. Using the Gramine library OS as a
representative enclave runtime environment, this paper shows how to recover
EDMM performance. The paper explains how implementing mutual distrust between
the OS and enclave increases the cost of modifying page mappings. The paper
then describes and evaluates a series of optimizations on application
benchmarks, showing that these optimizations effectively eliminate the
overheads of EDMM while retaining EDMM's performance and flexibility gains.
</summary>
    <author>
      <name>Vijay Dhanraj</name>
    </author>
    <author>
      <name>Harpreet Singh Chwarla</name>
    </author>
    <author>
      <name>Tao Zhang</name>
    </author>
    <author>
      <name>Daniel Manila</name>
    </author>
    <author>
      <name>Eric Thomas Schneider</name>
    </author>
    <author>
      <name>Erica Fu</name>
    </author>
    <author>
      <name>Mona Vij</name>
    </author>
    <author>
      <name>Chia-Che Tsai</name>
    </author>
    <author>
      <name>Donald E. Porter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.16251v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.16251v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.17984v1</id>
    <updated>2025-04-24T23:46:28Z</updated>
    <published>2025-04-24T23:46:28Z</published>
    <title>A Journey of Modern OS Construction From boot to DOOM</title>
    <summary>  VOS is a first-of-its-kind instructional OS that: (1) Runs on commodity,
portable hardware. (2) Showcases modern features, including per-app address
spaces, threading, commodity filesystems, USB, DMA, multicore, self-hosted
debugging, and a window manager. (3) Supports rich applications such as 2D/3D
games, music and video players, and a blockchain miner. Unlike traditional
instructional systems, VOS emphasizes strong motivation for building
systems-supporting engaging, media-rich apps that go beyond basic terminal
programs. To achieve this, we design VOS to strike a careful balance between
essential OS complexity and overall simplicity. Our method, which we call
inverse engineering, breaks down a full-featured OS into a set of incremental,
self-contained prototypes. Each prototype introduces a minimal set of OS
mechanisms, driven by the needs of specific apps. The construction process
(i.e., forward engineering) then progressively enables these apps by bringing
up one mechanism at a time. VOS makes it accessible for a wider audience to
experience building a software system that is self-contained and usable in
everyday scenarios.
</summary>
    <author>
      <name>Wonkyo Choe</name>
    </author>
    <author>
      <name>Rongxiang Wang</name>
    </author>
    <author>
      <name>Afsara Benazir</name>
    </author>
    <author>
      <name>Felix Xiaozhu Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2504.17984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.17984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.18714v1</id>
    <updated>2025-04-25T21:56:15Z</updated>
    <published>2025-04-25T21:56:15Z</published>
    <title>From Good to Great: Improving Memory Tiering Performance Through
  Parameter Tuning</title>
    <summary>  Memory tiering systems achieve memory scaling by adding multiple tiers of
memory wherein different tiers have different access latencies and bandwidth.
For maximum performance, frequently accessed (hot) data must be placed close to
the host in faster tiers and infrequently accessed (cold) data can be placed in
farther slower memory tiers. Existing tiering solutions employ heuristics and
pre-configured thresholds to make data placement and migration decisions.
Unfortunately, these systems fail to adapt to different workloads and the
underlying hardware, so perform sub-optimally.
  In this paper, we improve performance of memory tiering by using application
behavior knowledge to set various parameters (knobs) in existing tiering
systems. To do so, we leverage Bayesian Optimization to discover the good
performing configurations that capture the application behavior and the
underlying hardware characteristics. We find that Bayesian Optimization is able
to learn workload behaviors and set the parameter values that result in good
performance. We evaluate this approach with existing tiering systems, HeMem and
HMSDK. Our evaluation reveals that configuring the parameter values correctly
can improve performance by 2x over the same systems with default configurations
and 1.56x over state-of-the-art tiering system.
</summary>
    <author>
      <name>Konstantinos Kanellis</name>
    </author>
    <author>
      <name>Sujay Yadalam</name>
    </author>
    <author>
      <name>Fanchao Chen</name>
    </author>
    <author>
      <name>Michael Swift</name>
    </author>
    <author>
      <name>Shivaram Venkataraman</name>
    </author>
    <link href="http://arxiv.org/abs/2504.18714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.18714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.19662v1</id>
    <updated>2025-04-28T10:21:13Z</updated>
    <published>2025-04-28T10:21:13Z</published>
    <title>Ariel OS: An Embedded Rust Operating System for Networked Sensors &amp;
  Multi-Core Microcontrollers</title>
    <summary>  Large swaths of low-level system software building blocks originally
implemented in C/C++ are currently being swapped for equivalent rewrites in
Rust, a relatively more secure and dependable programming language. So far,
however, no embedded OS in Rust supports multicore preemptive scheduling on
microcontrollers. In this paper, we thus fill this gap with a new operating
system: Ariel OS. We describe its design, we provide the source code of its
implementation, and we perform micro-benchmarks on the main 32-bit
microcontroller architectures: ARM Cortex-M, RISC-V and Espressif Xtensa. We
show how our scheduler takes advantage of several cores, while incurring only
small overhead on single-core hardware. As such, Ariel OS provides a convenient
embedded software platform for small networked devices, for both research and
industry practitioners.
</summary>
    <author>
      <name>Elena Frank</name>
    </author>
    <author>
      <name>Kaspar Schleiser</name>
    </author>
    <author>
      <name>Romain Fouquet</name>
    </author>
    <author>
      <name>Koen Zandberg</name>
    </author>
    <author>
      <name>Christian Amsüss</name>
    </author>
    <author>
      <name>Emmanuel Baccelli</name>
    </author>
    <link href="http://arxiv.org/abs/2504.19662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.19662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.21394v1</id>
    <updated>2025-04-30T07:48:52Z</updated>
    <published>2025-04-30T07:48:52Z</published>
    <title>Concurrency Testing in the Linux Kernel via eBPF</title>
    <summary>  Concurrency is vital for our critical software to meet modern performance
requirements, yet concurrency bugs are notoriously difficult to detect and
reproduce. Controlled Concurrency Testing (CCT) can make bugs easier to expose
by enabling control over thread interleavings and systematically exploring the
interleaving space through scheduling algorithms. However, existing CCT
solutions for kernel code are heavyweight, leading to significant performance,
maintainability and extensibility issues. In this work, we introduce LACE, a
lightweight CCT framework for kernel code empowered by eBPF. Without hypervisor
modification, LACE features a custom scheduler tailored for CCT algorithms to
serialize non-determistic thread execution into a controlled ordering. LACE
also provides a mechanism to safely inject scheduling points into the kernel
for fine-grained control. Furthermore, LACE employs a two-phase mutation
strategy to integrate the scheduler with a concurrency fuzzer, allowing for
automated exploration of both the input and schedule space. In our evaluation,
LACE achieves 38\% more branches, 57\% overhead reduction and 11.4$\times$
speed-up in bug exposure compared to the state-of-the-art kernel concurrency
fuzzers. Our qualitative analysis also demonstrates the extensibility and
maintainability of LACE. Furthermore, LACE discovers eight previously unknown
bugs in the Linux kernel, with six confirmed by developers.
</summary>
    <author>
      <name>Jiacheng Xu</name>
    </author>
    <author>
      <name>Dylan Wolff</name>
    </author>
    <author>
      <name>Xing Yi Han</name>
    </author>
    <author>
      <name>Jialin Li</name>
    </author>
    <author>
      <name>Abhik Roychoudhury</name>
    </author>
    <link href="http://arxiv.org/abs/2504.21394v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.21394v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.06546v1</id>
    <updated>2025-05-10T07:16:10Z</updated>
    <published>2025-05-10T07:16:10Z</published>
    <title>Work in Progress: Middleware-Transparent Callback Enforcement in
  Commoditized Component-Oriented Real-time Systems</title>
    <summary>  Real-time scheduling in commoditized component-oriented real-time systems,
such as ROS 2 systems on Linux, has been studied under nested scheduling: OS
thread scheduling and middleware layer scheduling (e.g., ROS 2 Executor).
However, by establishing a persistent one-to-one correspondence between
callbacks and OS threads, we can ignore the middleware layer and directly apply
OS scheduling parameters (e.g., scheduling policy, priority, and affinity) to
individual callbacks. We propose a middleware model that enables this idea and
implements CallbackIsolatedExecutor as a novel ROS 2 Executor. We demonstrate
that the costs (user-kernel switches, context switches, and memory usage) of
CallbackIsolatedExecutor remain lower than those of the MultiThreadedExecutor,
regardless of the number of callbacks. Additionally, the cost of
CallbackIsolatedExecutor relative to SingleThreadedExecutor stays within a
fixed ratio (1.4x for inter-process and 5x for intra-process communication).
Future ROS 2 real-time scheduling research can avoid nested scheduling,
ignoring the existence of the middleware layer.
</summary>
    <author>
      <name>Takahiro Ishikawa-Aso</name>
    </author>
    <author>
      <name>Atsushi Yano</name>
    </author>
    <author>
      <name>Takuya Azumi</name>
    </author>
    <author>
      <name>Shinpei Kato</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/RTAS65571.2025.00017</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/RTAS65571.2025.00017" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures. Accepted for IEEE RTAS 2025; this is the
  author-accepted manuscript. Final version in IEEE Xplore:
  https://doi.org/10.1109/RTAS65571.2025.00017</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. IEEE Real-Time and Embedded Technology and Applications
  Symposium (RTAS), 2025, pp. 78--81</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2505.06546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3; D.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.06645v1</id>
    <updated>2025-05-10T13:44:03Z</updated>
    <published>2025-05-10T13:44:03Z</published>
    <title>RTOS Architectures that Solve the Diminishing Bandwidth Problem</title>
    <summary>  The Diminishing Bandwidth Problem is a long standing, previously
unidentified, extensibility problem of current real-time operating systems
characterized by a superficial dependency between the number of tasks in a
system and the maximum bandwidth associated with a peripheral device. In the
worst case, this diabolical deficiency will continue to decrease the maximum
bandwidth of a peripheral device as more tasks are added to the application. If
this is not taken into account, a previously functional application may
experience data loss if more tasks are added to it in order to, for example,
implement new features. Three novel RTOS architectures that solve the
Diminishing Bandwidth Problem are specified and discussed: the Defer Structure
RTOS Architecture, the Barriers and Requests RTOS Architecture, and the
Strictly Atomic RTOS Architecture. Finally, two hardware solutions to the
Diminishing Bandwidth Problem are also presented.
</summary>
    <author>
      <name>Mazen Arakji</name>
    </author>
    <link href="http://arxiv.org/abs/2505.06645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.06780v2</id>
    <updated>2025-05-13T02:40:38Z</updated>
    <published>2025-05-10T23:29:35Z</published>
    <title>Work-in-Progress: Multi-Deadline DAG Scheduling Model for Autonomous
  Driving Systems</title>
    <summary>  Autoware is an autonomous driving system implemented on Robot Operation
System (ROS) 2, where an end-to-end timing guarantee is crucial to ensure
safety. However, existing ROS 2 cause-effect chain models for analyzing
end-to-end latency struggle to accurately represent the complexities of
Autoware, particularly regarding sync callbacks, queue consumption patterns,
and feedback loops. To address these problems, we propose a new scheduling
model that decomposes the end-to-end timing constraints of Autoware into local
relative deadlines for each sub-DAG. This multi-deadline DAG scheduling model
avoids the need for complex analysis of data flows through queues and loops,
while ensuring that all callbacks receive data within correct intervals.
Furthermore, we extend the Global Earliest Deadline First (GEDF) algorithm for
the proposed model and evaluate its effectiveness using a synthetic workload
derived from Autoware.
</summary>
    <author>
      <name>Atsushi Yano</name>
    </author>
    <author>
      <name>Takuya Azumi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/RTSS62706.2024.00049</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/RTSS62706.2024.00049" rel="related"/>
    <link href="http://arxiv.org/abs/2505.06780v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06780v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0302033v1</id>
    <updated>2003-02-24T22:03:30Z</updated>
    <published>2003-02-24T22:03:30Z</published>
    <title>Experimental Software Schedulability Estimation For Varied Processor
  Frequencies</title>
    <summary>  This paper describes a new approach to experimentally estimate the
application schedulability for various processor frequencies. We use additional
workload generated by an artificial high priority routine to simulate the
frequency decrease of a processor. Then we estimate the schedulability of
applications at different frequencies. The results of such estimation can be
used to determine the frequencies and control algorithms of dynamic voltage
scaling/dynamic frequency scaling (DVS/DFS) implementations. The paper presents
a general problem description, the proposed schedulability estimation method,
its analysis and evaluation.
</summary>
    <author>
      <name>Sampsa Fabritius</name>
    </author>
    <author>
      <name>Raimondas Lencevicius</name>
    </author>
    <author>
      <name>Edu Metz</name>
    </author>
    <author>
      <name>Alexander Ran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, published in the Proceedings of the Symposium on
  Software Engineering at 21th IASTED International Multi-Conference on Applied
  Informatics (AI 2003)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0302033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0302033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.8;D.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502012v1</id>
    <updated>2005-02-02T04:43:33Z</updated>
    <published>2005-02-02T04:43:33Z</published>
    <title>Sequential File Programming Patterns and Performance with .NET</title>
    <summary>  Programming patterns for sequential file access in the .NET Framework are
described and the performance is measured. The default behavior provides
excellent performance on a single disk - 50 MBps both reading and writing.
Using large request sizes and doing file pre-allocation when possible have
quantifiable benefits. When one considers disk arrays, .NET unbuffered IO
delivers 800 MBps on a 16-disk array, but buffered IO delivers about 12% of
that performance. Consequently, high-performance file and database utilities
are still forced to use unbuffered IO for maximum sequential performance. The
report is accompanied by downloadable source code that demonstrates the
concepts and code that was used to obtain these measurements.
</summary>
    <author>
      <name>Peter Kukol</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0502012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504051v1</id>
    <updated>2005-04-13T16:37:59Z</updated>
    <published>2005-04-13T16:37:59Z</published>
    <title>A Scalable Stream-Oriented Framework for Cluster Applications</title>
    <summary>  This paper presents a stream-oriented architecture for structuring cluster
applications. Clusters that run applications based on this architecture can
scale to tenths of thousands of nodes with significantly less performance loss
or reliability problems. Our architecture exploits the stream nature of the
data flow and reduces congestion through load balancing, hides latency behind
data pushes and transparently handles node failures. In our ongoing work, we
are developing an implementation for this architecture and we are able to run
simple data mining applications on a cluster simulator.
</summary>
    <author>
      <name>Tassos S. Argyros</name>
    </author>
    <author>
      <name>David R. Cheriton</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0507073v1</id>
    <updated>2005-07-29T19:33:13Z</updated>
    <published>2005-07-29T19:33:13Z</published>
    <title>Software Performance Analysis</title>
    <summary>  The key to speeding up applications is often understanding where the elapsed
time is spent, and why. This document reviews in depth the full array of
performance analysis tools and techniques available on Linux for this task,
from the traditional tools like gcov and gprof, to the more advanced tools
still under development like oprofile and the Linux Trace Toolkit. The focus is
more on the underlying data collection and processing algorithms, and their
overhead and precision, than on the cosmetic details of the graphical user
interface frontends.
</summary>
    <author>
      <name>Michel R. Dagenais</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dept. of Computer Engineering, Ecole Polytechnique, Montreal, Canada</arxiv:affiliation>
    </author>
    <author>
      <name>Karim Yaghmour</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Opersys, Montreal, Canada</arxiv:affiliation>
    </author>
    <author>
      <name>Charles Levert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Ericsson Research, Montreal, Canada</arxiv:affiliation>
    </author>
    <author>
      <name>Makan Pourzandi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Ericsson Research, Montreal, Canada</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/cs/0507073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0507073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0508063v1</id>
    <updated>2005-08-12T16:13:32Z</updated>
    <published>2005-08-12T16:13:32Z</published>
    <title>Disks, Partitions, Volumes and RAID Performance with the Linux Operating
  System</title>
    <summary>  Block devices in computer operating systems typically correspond to disks or
disk partitions, and are used to store files in a filesystem. Disks are not the
only real or virtual device which adhere to the block accessible stream of
bytes block device model. Files, remote devices, or even RAM may be used as a
virtual disks. This article examines several common combinations of block
device layers used as virtual disks in the Linux operating system: disk
partitions, loopback files, software RAID, Logical Volume Manager, and Network
Block Devices. It measures their relative performance using different
filesystems: Ext2, Ext3, ReiserFS, JFS, XFS,NFS.
</summary>
    <author>
      <name>Michel R. Dagenais</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dept. of Computer Engineering, Ecole Polytechnique, Montreal, Canada</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/cs/0508063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0508063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.0372v1</id>
    <updated>2008-10-02T09:41:52Z</updated>
    <published>2008-10-02T09:41:52Z</published>
    <title>Optimizing Binary Code Produced by Valgrind (Project Report on Virtual
  Execution Environments Course - AVExe)</title>
    <summary>  Valgrind is a widely used framework for dynamic binary instrumentation and
its mostly known by its memcheck tool. Valgrind's code generation module is far
from producing optimal code. In addition it has many backends for different CPU
architectures, which difficults code optimization in an architecture
independent way. Our work focused on identifying sub-optimal code produced by
Valgrind and optimizing it.
</summary>
    <author>
      <name>Filipe Cabecinhas</name>
    </author>
    <author>
      <name>Nuno Lopes</name>
    </author>
    <author>
      <name>Renato Crisostomo</name>
    </author>
    <author>
      <name>Luis Veiga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report from INESC-ID Lisboa describing optimizations to
  code generation of the Valgring execution environment. Work developed in the
  context of a Virtual Execution Environments course (AVExe) at IST/Technical
  university of Lisbon</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.0372v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.0372v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.1610v1</id>
    <updated>2009-02-10T09:14:59Z</updated>
    <published>2009-02-10T09:14:59Z</published>
    <title>Package upgrades in FOSS distributions: details and challenges</title>
    <summary>  The upgrade problems faced by Free and Open Source Software distributions
have characteristics not easily found elsewhere. We describe the structure of
packages and their role in the upgrade process. We show that state of the art
package managers have shortcomings inhibiting their ability to cope with
frequent upgrade failures. We survey current countermeasures to such failures,
argue that they are not satisfactory, and sketch alternative solutions.
</summary>
    <author>
      <name>Roberto Di Cosmo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PPS</arxiv:affiliation>
    </author>
    <author>
      <name>Stefano Zacchiroli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PPS</arxiv:affiliation>
    </author>
    <author>
      <name>Paulo Trezentos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1490283.1490292</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1490283.1490292" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Workshop On Hot Topics In Software Upgrades
  Proceedings of the 1st International Workshop on Hot Topics in Software
  Upgrades, Nashville, Tennessee : \'Etats-Unis d'Am\'erique (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0902.1610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.1610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1763v1</id>
    <updated>2009-09-09T18:09:06Z</updated>
    <published>2009-09-09T18:09:06Z</published>
    <title>Remembrance: The Unbearable Sentience of Being Digital</title>
    <summary>  We introduce a world vision in which data is endowed with memory. In this
data-centric systems paradigm, data items can be enabled to retain all or some
of their previous values. We call this ability "remembrance" and posit that it
empowers significant leaps in the security, availability, and general
operational dimensions of systems. With the explosion in cheap, fast memories
and storage, large-scale remembrance will soon become practical. Here, we
introduce and explore the advantages of such a paradigm and the challenges in
making it a reality.
</summary>
    <author>
      <name>Ragib Hasan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Illinois</arxiv:affiliation>
    </author>
    <author>
      <name>Radu Sion</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stony Brook University</arxiv:affiliation>
    </author>
    <author>
      <name>Marianne Winslett</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Illinois</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.0196v1</id>
    <updated>2010-01-01T00:34:59Z</updated>
    <published>2010-01-01T00:34:59Z</published>
    <title>A distributed file system for a wide-area high performance computing
  infrastructure</title>
    <summary>  We describe our work in implementing a wide-area distributed file system for
the NSF TeraGrid. The system, called XUFS, allows private distributed name
spaces to be created for transparent access to personal files across over 9000
computer nodes. XUFS builds on many principles from prior distributed file
systems research, but extends key design goals to support the workflow of
computational science researchers. Specifically, XUFS supports file access from
the desktop to the wide-area network seamlessly, survives transient
disconnected operations robustly, and demonstrates comparable or better
throughput than some current high performance file systems on the wide-area
network.
</summary>
    <author>
      <name>Edward Walker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, Proceedings of Third USENIX Workshop on Real, Large
  Distributed Systems, Seattle, Nov 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.0196v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.0196v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4; D.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.4411v1</id>
    <updated>2010-10-21T10:12:59Z</updated>
    <published>2010-10-21T10:12:59Z</published>
    <title>Revisiting deadlock prevention: a probabilistic approach</title>
    <summary>  We revisit the deadlock-prevention problem by focusing on priority digraphs
instead of the traditional wait-for digraphs. This has allowed us to formulate
deadlock prevention in terms of prohibiting the occurrence of directed cycles
even in the most general of wait models (the so-called AND-OR model, in which
prohibiting wait-for directed cycles is generally overly restrictive). For a
particular case in which the priority digraphs are somewhat simplified, we
introduce a Las Vegas probabilistic mechanism for resource granting and analyze
its key aspects in detail.
</summary>
    <author>
      <name>Fabiano de S. Oliveira</name>
    </author>
    <author>
      <name>Valmir C. Barbosa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/net.21537</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/net.21537" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Networks 63 (2014), 203-210</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1010.4411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.4411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.2766v1</id>
    <updated>2011-06-14T17:26:21Z</updated>
    <published>2011-06-14T17:26:21Z</published>
    <title>Supporting Parallelism in Server-based Multiprocessor Systems</title>
    <summary>  Developing an efficient server-based real-time scheduling solution that
supports dynamic task-level parallelism is now relevant to even the desktop and
embedded domains and no longer only to the high performance computing market
niche. This paper proposes a novel approach that combines the constant
bandwidth server abstraction with a work-stealing load balancing scheme which,
while ensuring isolation among tasks, enables a task to be executed on more
than one processor at a given time instant.
</summary>
    <author>
      <name>Luís Nogueira</name>
    </author>
    <author>
      <name>Luís Miguel Pinho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WiP Session of the 31st IEEE Real-Time Systems Symposium</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.2766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.2766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.2992v1</id>
    <updated>2011-06-15T15:18:27Z</updated>
    <published>2011-06-15T15:18:27Z</published>
    <title>A Characterization of the SPARC T3-4 System</title>
    <summary>  This technical report covers a set of experiments on the 64-core SPARC T3-4
system, comparing it to two similar AMD and Intel systems. Key characteristics
as maximum integer and floating point arithmetic throughput are measured as
well as memory throughput, showing the scalability of the SPARC T3-4 system.
The performance of POSIX threads primitives is characterized and compared in
detail, such as thread creation and mutex synchronization. Scalability tests
with a fine grained multithreaded runtime are performed, showing problems with
atomic CAS operations on such physically highly parallel systems.
</summary>
    <author>
      <name>Michiel W. van Tol</name>
    </author>
    <link href="http://arxiv.org/abs/1106.2992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.2992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.2649v1</id>
    <updated>2013-01-12T05:31:57Z</updated>
    <published>2013-01-12T05:31:57Z</published>
    <title>Dynamic Transparent General Purpose Process Migration For Linux</title>
    <summary>  Process migration refers to the act of transferring a process in the middle
of its execution from one machine to another in a network. In this paper, we
proposed a process migration framework for Linux OS. It is a multilayer
architecture to confine every functionality independent section of the system
in separate layer. This architecture is capable of supporting diverse
applications due to generic user space interface and dynamic structure that can
be modified according to demands.
</summary>
    <author>
      <name>Amirreza Zarrabi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijgca.2012.3402</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijgca.2012.3402" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Grid Computing &amp; Applications (IJGCA)
  Vol.3, No.4, December 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.2649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.2649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.2881v1</id>
    <updated>2013-08-13T14:37:51Z</updated>
    <published>2013-08-13T14:37:51Z</published>
    <title>Opacity of Memory Management in Software Transactional Memory</title>
    <summary>  Opacity of Transactional Memory is proposed to be established by incremental
validation. Quiescence in terms of epoch-based memory reclamation is applied to
deal with doomed transactions causing memory access violations. This method
unfortunately involves increased memory consumption and does not cover
reclamations outside of transactions. This paper introduces a different method
which combines incremental validation with elements of sandboxing to solve
these issues.
</summary>
    <author>
      <name>Holger Machens</name>
    </author>
    <author>
      <name>Volker Turau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: transactional memory, opacity, privatization, memory
  reclamation</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.2881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.2881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65Y05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.2913v1</id>
    <updated>2014-05-12T16:41:49Z</updated>
    <published>2014-05-12T16:41:49Z</published>
    <title>Resource-Aware Replication on Heterogeneous Multicores: Challenges and
  Opportunities</title>
    <summary>  Decreasing hardware feature sizes and increasing heterogeneity in multicore
hardware require software that can adapt to these platforms' properties. We
implemented ROMAIN, an OS service providing redundant multithreading on top of
the FIASCO.OC microkernel to address the increasing unreliability of hardware.
In this paper we review challenges and opportunities for ROMAIN to adapt to
such multicore platforms in order to decrease execution overhead, resource
requirements, and vulnerability against faults.
</summary>
    <author>
      <name>Björn Döbel</name>
    </author>
    <author>
      <name>Robert Muschner</name>
    </author>
    <author>
      <name>Hermann Härtig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.2913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.2913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.1133v1</id>
    <updated>2014-06-04T18:18:37Z</updated>
    <published>2014-06-04T18:18:37Z</published>
    <title>Timing Analysis for DAG-based and GFP Scheduled Tasks</title>
    <summary>  Modern embedded systems have made the transition from single-core to
multi-core architectures, providing performance improvement via parallelism
rather than higher clock frequencies. DAGs are considered among the most
generic task models in the real-time domain and are well suited to exploit this
parallelism. In this paper we provide a schedulability test using response-time
analysis exploiting exploring and bounding the self interference of a DAG task.
Additionally we bound the interference a high priority task has on lower
priority ones.
</summary>
    <author>
      <name>José Marinho</name>
    </author>
    <author>
      <name>Stefan M. Petters</name>
    </author>
    <link href="http://arxiv.org/abs/1406.1133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.1133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.4245v1</id>
    <updated>2014-07-16T10:03:51Z</updated>
    <published>2014-07-16T10:03:51Z</published>
    <title>Security of OS-level virtualization technologies: Technical report</title>
    <summary>  The need for flexible, low-overhead virtualization is evident on many fronts
ranging from high-density cloud servers to mobile devices. During the past
decade OS-level virtualization has emerged as a new, efficient approach for
virtualization, with implementations in multiple different Unix-based systems.
Despite its popularity, there has been no systematic study of OS-level
virtualization from the point of view of security. In this report, we conduct a
comparative study of several OS-level virtualization systems, discuss their
security and identify some gaps in current solutions.
</summary>
    <author>
      <name>Elena Reshetova</name>
    </author>
    <author>
      <name>Janne Karhunen</name>
    </author>
    <author>
      <name>Thomas Nyman</name>
    </author>
    <author>
      <name>N. Asokan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.4245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.4245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.4715v1</id>
    <updated>2014-08-20T16:39:15Z</updated>
    <published>2014-08-20T16:39:15Z</published>
    <title>Making FPGAs Accessible to Scientists and Engineers as Domain Expert
  Software Programmers with LabVIEW</title>
    <summary>  In this paper we present a graphical programming framework, LabVIEW, and
associated language and libraries, as well as programming techniques and
patterns that we have found useful in making FPGAs accessible to scientists and
engineers as domain expert software programmers.
</summary>
    <author>
      <name>Hugo A. Andrade</name>
    </author>
    <author>
      <name>Simon Hogg</name>
    </author>
    <author>
      <name>Stephan Ahrends</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at First International Workshop on FPGAs for Software
  Programmers (FSP 2014) (arXiv:1408.4423)</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.4715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.4715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04169v1</id>
    <updated>2015-11-13T06:27:39Z</updated>
    <published>2015-11-13T06:27:39Z</published>
    <title>Specifying a Realistic File System</title>
    <summary>  We present the most interesting elements of the correctness specification of
BilbyFs, a performant Linux flash file system. The BilbyFs specification
supports asynchronous writes, a feature that has been overlooked by several
file system verification projects, and has been used to verify the correctness
of BilbyFs's fsync() C implementation. It makes use of nondeterminism to be
concise and is shallowly-embedded in higher-order logic.
</summary>
    <author>
      <name>Sidney Amani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NICTA and University of New South Wales, Australia</arxiv:affiliation>
    </author>
    <author>
      <name>Toby Murray</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NICTA and University of New South Wales, Australia</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.196.1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.196.1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings MARS 2015, arXiv:1511.02528</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 196, 2015, pp. 1-9</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.04169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03635v1</id>
    <updated>2016-03-11T14:10:18Z</updated>
    <published>2016-03-11T14:10:18Z</published>
    <title>Powering the Internet of Things with RIOT: Why? How? What is RIOT?</title>
    <summary>  The crucial importance of software platforms was highlighted by recent events
both at the political level (e.g. renewed calls for digital data and operating
system "sovereignty", following E. Snowden's revelations) and at the business
level (e.g. Android generated a new industry worth tens of billions of euros
yearly). In the Internet of Things, which is expected to generate business at
very large scale, but also to threaten even more individual privacy, such
aspects will be exacerbated. The need for an operating system like RIOT stems
from this context, and this short article outlines RIOT's main non-technical
aspects, as well as its key technical characteristics.
</summary>
    <author>
      <name>Emmanuel Baccelli</name>
    </author>
    <author>
      <name>Kaspar Schleiser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.03635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09334v1</id>
    <updated>2017-08-29T10:24:40Z</updated>
    <published>2017-08-29T10:24:40Z</published>
    <title>Tug-of-War: Observations on Unified Content Handling</title>
    <summary>  Modern applications and Operating Systems vary greatly with respect to how
they register and identify different types of content. These discrepancies lead
to exploits and inconsistencies in user experience. In this paper, we highlight
the issues arising in the modern content handling ecosystem, and examine how
the operating system can be used to achieve unified and consistent content
identification.
</summary>
    <author>
      <name>Theofilos Petsios</name>
    </author>
    <author>
      <name>Adrian Tang</name>
    </author>
    <author>
      <name>Dimitris Mitropoulos</name>
    </author>
    <author>
      <name>Salvatore Stolfo</name>
    </author>
    <author>
      <name>Angelos D. Keromytis</name>
    </author>
    <author>
      <name>Suman Jana</name>
    </author>
    <link href="http://arxiv.org/abs/1708.09334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08705v1</id>
    <updated>2017-10-24T11:14:02Z</updated>
    <published>2017-10-24T11:14:02Z</published>
    <title>Tails &amp; Tor and other tools for Safeguarding Online Activities</title>
    <summary>  There are not many known ways to break Tor anonymity, and they require an
enormous amount of computational power. Controlling both entrance and exit
nodes allows an attacker to compromise client IP with enough pattern analysis.
If an .onion or public website does not use SSL, information will not be
encrypted once it reaches the exit node. Tor has been successfully broken by
Carnegie Mellon, however they will not answer questions nor confirm their
method. This research paper investigates Tails &amp; Tor and other tools for
Safeguarding Online Activities.
</summary>
    <author>
      <name>Stephanie Abraham</name>
    </author>
    <author>
      <name>Tyler Silva</name>
    </author>
    <author>
      <name>Robert Decourcy</name>
    </author>
    <author>
      <name>Jim Cardon</name>
    </author>
    <link href="http://arxiv.org/abs/1710.08705v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08705v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04309v1</id>
    <updated>2018-10-10T00:35:05Z</updated>
    <published>2018-10-10T00:35:05Z</published>
    <title>Formalising Filesystems in the ACL2 Theorem Prover: an Application to
  FAT32</title>
    <summary>  In this work, we present an approach towards constructing executable
specifications of existing filesystems and verifying their functional
properties in a theorem proving environment. We detail an application of this
approach to the FAT32 filesystem.
  We also detail the methodology used to build up this type of executable
specification through a series of models which incrementally add features of
the target filesystem. This methodology has the benefit of allowing the
verification effort to start from simple models which encapsulate features
common to many filesystems and which are thus suitable for reuse.
</summary>
    <author>
      <name>Mihir Parang Mehta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UT Austin</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.280.2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.280.2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings ACL2 2018, arXiv:1810.03762</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 280, 2018, pp. 18-29</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1810.04309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.02064v5</id>
    <updated>2021-11-24T12:51:43Z</updated>
    <published>2019-07-02T21:04:47Z</published>
    <title>Accelerator-level Parallelism</title>
    <summary>  Future applications demand more performance, but technology advances have
been faltering. A promising approach to further improve computer system
performance under energy constraints is to employ hardware accelerators.
Already today, mobile systems concurrently employ multiple accelerators in what
we call accelerator-level parallelism (ALP). To spread the benefits of ALP more
broadly, we charge computer scientists to develop the science needed to best
achieve the performance and cost goals of ALP hardware and software.
</summary>
    <author>
      <name>Mark D. Hill</name>
    </author>
    <author>
      <name>Vijay Janapa Reddi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, &amp; 7 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.02064v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.02064v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.00580v1</id>
    <updated>2019-12-02T05:05:39Z</updated>
    <published>2019-12-02T05:05:39Z</published>
    <title>Multi-version Indexing in Flash-based Key-Value Stores</title>
    <summary>  Maintaining multiple versions of data is popular in key-value stores since it
increases concurrency and improves performance. However, designing a
multi-version key-value store entails several challenges, such as additional
capacity for storing extra versions and an indexing mechanism for mapping
versions of a key to their values. We present SkimpyFTL, a FTL-integrated
multi-version key-value store that exploits the remap-on-write property of
flash-based SSDs for multi-versioning and provides a tradeoff between memory
capacity and lookup latency for indexing.
</summary>
    <author>
      <name>Pulkit A. Misra</name>
    </author>
    <author>
      <name>Jeffrey S. Chase</name>
    </author>
    <author>
      <name>Johannes Gehrke</name>
    </author>
    <author>
      <name>Alvin R. Lebeck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.00580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.00580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.08885v1</id>
    <updated>2021-01-21T23:17:42Z</updated>
    <published>2021-01-21T23:17:42Z</published>
    <title>User-Aware Power Management for Mobile Devices</title>
    <summary>  The power management techniques to extend battery lifespan is becoming
increasingly important due to longer user applications' running time in mobile
devices. Even when users do not use any applications, battery lifespan
decreases continually. It occurs because of service daemons of mobile platform
and network-based data synchronization operations. In this paper, we propose a
new power management system that recognizes the idle time of the device to
reduce the battery consumption of mobile devices.
</summary>
    <author>
      <name>Geunsik Lim</name>
    </author>
    <author>
      <name>Changwoo Min</name>
    </author>
    <author>
      <name>Dong Hyun Kang</name>
    </author>
    <author>
      <name>Young Ik Eom</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/GCCE.2013.6664780</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/GCCE.2013.6664780" rel="related"/>
    <link href="http://arxiv.org/abs/2101.08885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.08885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.07092v1</id>
    <updated>2021-03-12T05:14:46Z</updated>
    <published>2021-03-12T05:14:46Z</published>
    <title>Performance Exploration of Virtualization Systems</title>
    <summary>  Virtualization has gained astonishing popularity in recent decades. It is
applied in several application domains, including mainframes, personal
computers, data centers, and embedded systems. While the benefits of
virtualization are no longer to be demonstrated, it often comes at the price of
performance degradation compared to native execution. In this work, we conduct
a comparative study on the performance outcome of VMWare, KVM, and Docker
against compute-intensive, IO-intensive, and system benchmarks. The experiments
reveal that containers are the way-to-go for the fast execution of
applications. It also shows that VMWare and KVM perform similarly on most of
the benchmarks.
</summary>
    <author>
      <name>Joel Mandebi Mbongue</name>
    </author>
    <author>
      <name>Danielle Tchuinkou Kwadjo</name>
    </author>
    <author>
      <name>Christophe Bobda</name>
    </author>
    <link href="http://arxiv.org/abs/2103.07092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.07092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.04183v1</id>
    <updated>2021-08-09T17:12:10Z</updated>
    <published>2021-08-09T17:12:10Z</published>
    <title>Understanding Fuchsia Security</title>
    <summary>  Fuchsia is a new open-source operating system created at Google that is
currently under active development. The core architectural principles guiding
the design and development of the OS include high system modularity and a
specific focus on security and privacy. This paper analyzes the architecture
and the software model of Fuchsia, giving a specific focus on the core security
mechanisms of this new operating system.
</summary>
    <author>
      <name>Francesco Pagano</name>
    </author>
    <author>
      <name>Luca Verderame</name>
    </author>
    <author>
      <name>Alessio Merlo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.22667/JOWUA.2021.09.30.047</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.22667/JOWUA.2021.09.30.047" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Wireless Mobile Networks, Ubiquitous Computing, and
  Dependable Applications, September 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2108.04183v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.04183v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.10357v1</id>
    <updated>2021-10-20T03:22:38Z</updated>
    <published>2021-10-20T03:22:38Z</published>
    <title>Fast Bitmap Fit: A CPU Cache Line friendly memory allocator for single
  object allocations</title>
    <summary>  Applications making excessive use of single-object based data structures
(such as linked lists, trees, etc...) can see a drop in efficiency over a
period of time due to the randomization of nodes in memory. This slow down is
due to the ineffective use of the CPU's L1/L2 cache. We present a novel
approach for mitigating this by presenting the design of a single-object memory
allocator that preserves memory locality across randomly ordered memory
allocations and deallocations.
</summary>
    <author>
      <name>Dhruv Matani</name>
    </author>
    <author>
      <name>Gaurav Menghani</name>
    </author>
    <link href="http://arxiv.org/abs/2110.10357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.10357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.01947v1</id>
    <updated>2021-10-22T11:05:34Z</updated>
    <published>2021-10-22T11:05:34Z</published>
    <title>An Evaluation of WebAssembly and eBPF as Offloading Mechanisms in the
  Context of Computational Storage</title>
    <summary>  As the volume of data that needs to be processed continues to increase, we
also see renewed interests in near-data processing in the form of computational
storage, with eBPF (extended Berkeley Packet Filter) being proposed as a
vehicle for computation offloading. However, discussions in this regard have so
far ignored viable alternatives, and no convincing analysis has been provided.
As such, we qualitatively and quantitatively evaluated eBPF against
WebAssembly, a seemingly similar technology, in the context of computation
offloading. This report presents our findings.
</summary>
    <author>
      <name>Wenjun Huang</name>
    </author>
    <author>
      <name>Marcus Paradies</name>
    </author>
    <link href="http://arxiv.org/abs/2111.01947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.01947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.04384v1</id>
    <updated>2021-12-07T17:07:53Z</updated>
    <published>2021-12-07T17:07:53Z</published>
    <title>Reproducing software environments: a prerequisite for reproducible
  research</title>
    <summary>  As software has become an integral part of scientific workflows, reproducible
research practices must take it into account. In what way? Archiving source
code is a necessary but insufficient condition. The ability to redeploy
software environments, which at first sight may be viewed as a technical
detail, is in fact a requirement. This article explores tools and methods to
achieve this goal.
</summary>
    <author>
      <name>Ludovic Courtès</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SED</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.48556/SIF.1024.18.15</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.48556/SIF.1024.18.15" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">1024 : Bulletin de la Soci{\'e}t{\'e} Informatique de France,
  Soci{\'e}t{\'e} Informatique de France, 2021, pp.15-22</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2112.04384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.04384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.3230v2</id>
    <updated>2008-05-08T02:55:57Z</updated>
    <published>2008-03-21T21:28:16Z</published>
    <title>A Type System for Data-Flow Integrity on Windows Vista</title>
    <summary>  The Windows Vista operating system implements an interesting model of
multi-level integrity. We observe that in this model, trusted code can be
blamed for any information-flow attack; thus, it is possible to eliminate such
attacks by static analysis of trusted code. We formalize this model by
designing a type system that can efficiently enforce data-flow integrity on
Windows Vista. Typechecking guarantees that objects whose contents are
statically trusted never contain untrusted values, regardless of what untrusted
code runs in the environment. Some of Windows Vista's runtime access checks are
necessary for soundness; others are redundant and can be optimized away.
</summary>
    <author>
      <name>Avik Chaudhuri</name>
    </author>
    <author>
      <name>Prasad Naldurg</name>
    </author>
    <author>
      <name>Sriram Rajamani</name>
    </author>
    <link href="http://arxiv.org/abs/0803.3230v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.3230v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6; D.2.4; F.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.03724v1</id>
    <updated>2019-06-09T22:15:19Z</updated>
    <published>2019-06-09T22:15:19Z</published>
    <title>Neural Heterogeneous Scheduler</title>
    <summary>  Access to parallel and distributed computation has enabled researchers and
developers to improve algorithms and performance in many applications. Recent
research has focused on next generation special purpose systems with multiple
kinds of coprocessors, known as heterogeneous system-on-chips (SoC). In this
paper, we introduce a method to intelligently schedule--and learn to
schedule--a stream of tasks to available processing elements in such a system.
We use deep reinforcement learning enabling complex sequential decision making
and empirically show that our reinforcement learning system provides for a
viable, better alternative to conventional scheduling heuristics with respect
to minimizing execution time.
</summary>
    <author>
      <name>Tegg Taekyong Sung</name>
    </author>
    <author>
      <name>Valliappa Chockalingam</name>
    </author>
    <author>
      <name>Alex Yahja</name>
    </author>
    <author>
      <name>Bo Ryu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages. The first two authors contributed equally. ICML 2019
  Real-world Sequential Decision Making Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.03724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.09600v1</id>
    <updated>2019-09-20T16:26:25Z</updated>
    <published>2019-09-20T16:26:25Z</published>
    <title>Multiprocessor Real-Time Locking Protocols: A Systematic Review</title>
    <summary>  We systematically survey the literature on analytically sound multiprocessor
real-time locking protocols from 1988 until 2018, covering the following
topics: progress mechanisms that prevent the lock-holder preemption problem,
spin-lock protocols, binary semaphore protocols, independence-preserving (or
fully preemptive) locking protocols, reader-writer and k-exclusion
synchronization, support for nested critical sections, and implementation and
system-integration aspects. A special focus is placed on the
suspension-oblivious and suspension-aware analysis approaches for semaphore
protocols, their respective notions of priority inversion, optimality criteria,
lower bounds on maximum priority-inversion blocking, and matching
asymptotically optimal locking protocols.
</summary>
    <author>
      <name>Björn B. Brandenburg</name>
    </author>
    <link href="http://arxiv.org/abs/1909.09600v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.09600v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.02455v3</id>
    <updated>2021-05-20T15:45:04Z</updated>
    <published>2020-11-03T17:40:36Z</published>
    <title>Hints and Principles for Computer System Design</title>
    <summary>  This new long version of my 1983 paper suggests the goals you might have for
your system -- Simple, Timely, Efficient, Adaptable, Dependable, Yummy (STEADY)
-- and techniques for achieving them -- Approximate, Incremental, Divide &amp;
Conquer (AID). It also gives some principles for system design that are more
than just hints, and many examples of how to apply the ideas.
</summary>
    <author>
      <name>Butler Lampson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">There is also a short version of this paper, about half the length of
  this one</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.02455v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.02455v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.03863v4</id>
    <updated>2022-01-05T23:57:48Z</updated>
    <published>2021-02-07T17:46:25Z</published>
    <title>Hemlock : Compact and Scalable Mutual Exclusion</title>
    <summary>  We present Hemlock, a novel mutual exclusion locking algorithm that is
extremely compact, requiring just one word per thread plus one word per lock,
but which still provides local spinning in most circumstances, high throughput
under contention, and low latency in the uncontended case. Hemlock is
context-free -- not requiring any information to be passed from a lock
operation to the corresponding unlock -- and FIFO. The performance of Hemlock
is competitive with and often better than the best scalable spin locks.
</summary>
    <author>
      <name>Dave Dice</name>
    </author>
    <author>
      <name>Alex Kogan</name>
    </author>
    <link href="http://arxiv.org/abs/2102.03863v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.03863v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.06972v1</id>
    <updated>2021-02-13T18:12:34Z</updated>
    <published>2021-02-13T18:12:34Z</published>
    <title>BPFContain: Fixing the Soft Underbelly of Container Security</title>
    <summary>  Linux containers currently provide limited isolation guarantees. While
containers separate namespaces and partition resources, the patchwork of
mechanisms used to ensure separation cannot guarantee consistent security
semantics. Even worse, attempts to ensure complete coverage results in a
mishmash of policies that are difficult to understand or audit. Here we present
BPFContain, a new container confinement mechanism designed to integrate with
existing container management systems. BPFContain combines a simple yet
flexible policy language with an eBPF-based implementation that allows for
deployment on virtually any Linux system running a recent kernel. In this
paper, we present BPFContain's policy language, describe its current
implementation as integrated into docker, and present benchmarks comparing it
with current container confinement technologies.
</summary>
    <author>
      <name>William Findlay</name>
    </author>
    <author>
      <name>David Barrera</name>
    </author>
    <author>
      <name>Anil Somayaji</name>
    </author>
    <link href="http://arxiv.org/abs/2102.06972v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.06972v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.11198v1</id>
    <updated>2021-02-22T17:24:08Z</updated>
    <published>2021-02-22T17:24:08Z</published>
    <title>Reading from External Memory</title>
    <summary>  Modern external memory is represented by several device classes. At present,
HDD, SATA SSD and NVMe SSD are widely used. Recently ultra-low latency SSD such
as Intel Optane became available on the market. Each of these types exhibits
it's own pattern for throughput, latency and parallelism. To achieve the
highest performance one has to pick an appropriate I/O interface provided by
the operating system. In this work we present a detailed overview and
evaluation of modern storage reading performance with regard to available Linux
synchronous and asynchronous interfaces. While throughout this work we aim for
the highest throughput we also measure latency and CPU usage. We provide this
report in hope the detailed results could be interesting to both researchers
and practitioners.
</summary>
    <author>
      <name>Ruslan Savchenko</name>
    </author>
    <link href="http://arxiv.org/abs/2102.11198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.11198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.07480v2</id>
    <updated>2022-10-07T02:42:58Z</updated>
    <published>2022-05-16T07:04:11Z</published>
    <title>Analyzing FreeRTOS Scheduling Behaviors with the Spin Model Checker</title>
    <summary>  FreeRTOS is a real-time operating system with configurable scheduling
policies. Its portability and configurability make FreeRTOS one of the most
popular real-time operating systems for embedded devices. We formally analyze
the FreeRTOS scheduler on ARM Cortex-M4 processor in this work. Specifically,
we build a formal model for the FreeRTOS ARM Cortex-M4 port and apply model
checking to find errors in our models for FreeRTOS example applications.
Intriguingly, several errors are found in our application models under
different scheduling policies. In order to confirm our findings, we modify
application programs distributed by FreeRTOS and reproduce assertion failures
on the STM32F429I-DISC1 board.
</summary>
    <author>
      <name>Chen-Kai Lin</name>
    </author>
    <author>
      <name>Bow-Yaw Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2205.07480v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.07480v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.01843v1</id>
    <updated>2022-09-05T09:06:39Z</updated>
    <published>2022-09-05T09:06:39Z</published>
    <title>RunPHI: Enabling Mixed-criticality Containers via Partitioning
  Hypervisors in Industry 4.0</title>
    <summary>  Orchestration systems are becoming a key component to automatically manage
distributed computing resources in many fields with criticality requirements
like Industry 4.0 (I4.0). However, they are mainly linked to OS-level
virtualization, which is known to suffer from reduced isolation. In this paper,
we propose RunPHI with the aim of integrating partitioning hypervisors, as a
solution for assuring strong isolation, with OS-level orchestration systems.
The purpose is to enable container orchestration in mixed-criticality systems
with isolation requirements through partitioned containers.
</summary>
    <author>
      <name>Marco Barletta</name>
    </author>
    <author>
      <name>Marcello Cinque</name>
    </author>
    <author>
      <name>Luigi De Simone</name>
    </author>
    <author>
      <name>Raffaele Della Corte</name>
    </author>
    <author>
      <name>Giorgio Farina</name>
    </author>
    <author>
      <name>Daniele Ottaviano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, accepted for publication in Proc. ISSREW, 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.01843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.01843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.03734v1</id>
    <updated>2023-01-10T00:43:32Z</updated>
    <published>2023-01-10T00:43:32Z</published>
    <title>Exoshuffle-CloudSort</title>
    <summary>  We present Exoshuffle-CloudSort, a sorting application running on top of Ray
using the Exoshuffle architecture. Exoshuffle-CloudSort runs on Amazon EC2,
with input and output data stored on Amazon S3. Using 40 i4i.4xlarge workers,
Exoshuffle-CloudSort completes the 100 TB CloudSort Benchmark (Indy category)
in 5378 seconds, with an average total cost of $97.
</summary>
    <author>
      <name>Frank Sifei Luan</name>
    </author>
    <author>
      <name>Stephanie Wang</name>
    </author>
    <author>
      <name>Samyukta Yagati</name>
    </author>
    <author>
      <name>Sean Kim</name>
    </author>
    <author>
      <name>Kenneth Lien</name>
    </author>
    <author>
      <name>Isaac Ong</name>
    </author>
    <author>
      <name>Tony Hong</name>
    </author>
    <author>
      <name>SangBin Cho</name>
    </author>
    <author>
      <name>Eric Liang</name>
    </author>
    <author>
      <name>Ion Stoica</name>
    </author>
    <link href="http://arxiv.org/abs/2301.03734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.03734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.03588v1</id>
    <updated>2023-11-06T22:43:46Z</updated>
    <published>2023-11-06T22:43:46Z</published>
    <title>Pinky: A Modern Malware-oriented Dynamic Information Retrieval Tool</title>
    <summary>  We present here a reverse engineering tool that can be used for information
retrieval and anti-malware techniques. Our main contribution is the design and
implementation of an instrumentation framework aimed at providing insight on
the emulation process. Sample emulation is achieved via translation of the
binary code to an intermediate representation followed by compilation and
execution. The design makes this a versatile tool that can be used for multiple
task such as information retrieval, reverse engineering, debugging, and
integration with anti-malware products.
</summary>
    <author>
      <name>Paul Irofti</name>
    </author>
    <link href="http://arxiv.org/abs/2311.03588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.03588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.14171v1</id>
    <updated>2023-11-23T19:22:49Z</updated>
    <published>2023-11-23T19:22:49Z</published>
    <title>OpenMP behavior in low resource and high stress mobile environment</title>
    <summary>  This paper investigates the use of OpenMP for parallel post processing in
obejct detection on personal Android devices, where resources like
computational power, memory, and battery are limited. Specifically, it explores
various configurations of thread count, CPU affinity, and chunk size on a Redmi
Note 10 Pro with an ARM Cortex A76 CPU. The study finds that using four threads
offers a maximum post processing speedup of 2.3x but increases overall
inference time by 2.7x. A balanced configuration of two threads achieves a 1.8x
speedup in post processing and a 2% improvement in overall program performance.
</summary>
    <author>
      <name>Kaijun Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.14171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.14171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS, cs.PF, cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.03235v1</id>
    <updated>2024-01-06T15:08:45Z</updated>
    <published>2024-01-06T15:08:45Z</published>
    <title>RAID Organizations for Improved Reliability and Performance: A Not
  Entirely Unbiased Tutorial (1st revision)</title>
    <summary>  RAID proposal advocated replacing large disks with arrays of PC disks, but as
the capacity of small disks increased 100-fold in 1990s the production of large
disks was discontinued. Storage dependability is increased via replication or
erasure coding. Cloud storage providers store multiple copies of data obviating
for need for further redundancy. Varitaions of RAID based on local recovery
codes, partial MDS reduce recovery cost. NAND flash Solid State Disks - SSDs
have low latency and high bandwidth, are more reliable, consume less power and
have a lower TCO than Hard Disk Drives, which are more viable for hyperscalers.
</summary>
    <author>
      <name>Alexander Thomasian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ACM Computing Surveys. arXiv admin note: substantial
  text overlap with arXiv:2306.08763</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.03235v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.03235v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.03245v1</id>
    <updated>2024-04-04T07:02:32Z</updated>
    <published>2024-04-04T07:02:32Z</published>
    <title>Memory Sharing with CXL: Hardware and Software Design Approaches</title>
    <summary>  Compute Express Link (CXL) is a rapidly emerging coherent interconnect
standard that provides opportunities for memory pooling and sharing. Memory
sharing is a well-established software feature that improves memory utilization
by avoiding unnecessary data movement. In this paper, we discuss multiple
approaches to enable memory sharing with different generations of CXL protocol
(i.e., CXL 2.0 and CXL 3.0) considering the challenges with each of the
architectures from the device hardware and software viewpoint.
</summary>
    <author>
      <name>Sunita Jain</name>
    </author>
    <author>
      <name>Nagaradhesh Yeleswarapu</name>
    </author>
    <author>
      <name>Hasan Al Maruf</name>
    </author>
    <author>
      <name>Rita Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 3rd Workshop on Heterogeneous Composable and
  Disaggregated Systems (HCDS 2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.03245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.03245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.06085v1</id>
    <updated>2024-05-09T20:18:21Z</updated>
    <published>2024-05-09T20:18:21Z</published>
    <title>Zero-consistency root emulation for unprivileged container image build</title>
    <summary>  Do Linux distribution package managers need the privileged operations they
request to actually happen? Apparently not, at least for building container
images for HPC applications. We use this observation to implement a root
emulation mode using a Linux seccomp filter that intercepts some privileged
system calls, does nothing, and returns success to the calling program. This
approach provides no consistency whatsoever but appears sufficient to build all
Dockerfiles we examined, simplifying fully-unprivileged workflows needed for
HPC application containers.
</summary>
    <author>
      <name>Reid Priedhorsky</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Los Alamos National Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Jennings</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Los Alamos National Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>Megan Phinney</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.06085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.06085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.16254v1</id>
    <updated>2024-11-25T10:15:38Z</updated>
    <published>2024-11-25T10:15:38Z</published>
    <title>Asynchronous I/O -- With Great Power Comes Great Responsibility</title>
    <summary>  The performance of storage hardware has improved vastly recently, leaving the
traditional I/O stack incapable of exploiting these gains due to increasingly
large relative overheads. Newer asynchronous I/O APIs, such as io_uring, have
significantly improved performance by reducing such overheads, but exhibit
limited adoption in practice. In this paper, we discuss the complexities that
the usage of these contemporary I/O APIs introduces to applications, which we
believe are mostly responsible for their low adoption rate. Finally, we share
implications and trade offs made by architectures that may be used to integrate
asynchronous I/O into DB applications.
</summary>
    <author>
      <name>Constantin Pestka</name>
    </author>
    <author>
      <name>Marcus Paradies</name>
    </author>
    <author>
      <name>Matthias Pohl</name>
    </author>
    <link href="http://arxiv.org/abs/2411.16254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.16254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08872v1</id>
    <updated>2025-03-11T20:36:49Z</updated>
    <published>2025-03-11T20:36:49Z</published>
    <title>Meta-Reinforcement Learning with Discrete World Models for Adaptive Load
  Balancing</title>
    <summary>  We integrate a meta-reinforcement learning algorithm with the DreamerV3
architecture to improve load balancing in operating systems. This approach
enables rapid adaptation to dynamic workloads with minimal retraining,
outperforming the Advantage Actor-Critic (A2C) algorithm in standard and
adaptive trials. It demonstrates robust resilience to catastrophic forgetting,
maintaining high performance under varying workload distributions and sizes.
These findings have important implications for optimizing resource management
and performance in modern operating systems. By addressing the challenges posed
by dynamic and heterogeneous workloads, our approach advances the adaptability
and efficiency of reinforcement learning in real-world system management tasks.
</summary>
    <author>
      <name>Cameron Redovian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, to be published in ACMSE 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.08872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.8; D.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406005v3</id>
    <updated>2004-10-12T05:43:58Z</updated>
    <published>2004-06-02T21:54:50Z</published>
    <title>Microreboot -- A Technique for Cheap Recovery</title>
    <summary>  A significant fraction of software failures in large-scale Internet systems
are cured by rebooting, even when the exact failure causes are unknown.
However, rebooting can be expensive, causing nontrivial service disruption or
downtime even when clusters and failover are employed. In this work we separate
process recovery from data recovery to enable microrebooting -- a fine-grain
technique for surgically recovering faulty application components, without
disturbing the rest of the application.
  We evaluate microrebooting in an Internet auction system running on an
application server. Microreboots recover most of the same failures as full
reboots, but do so an order of magnitude faster and result in an order of
magnitude savings in lost work. This cheap form of recovery engenders a new
approach to high availability: microreboots can be employed at the slightest
hint of failure, prior to node failover in multi-node clusters, even when
mistakes in failure detection are likely; failure and recovery can be masked
from end users through transparent call-level retries; and systems can be
rejuvenated by parts, without ever being shut down.
</summary>
    <author>
      <name>George Candea</name>
    </author>
    <author>
      <name>Shinichi Kawamoto</name>
    </author>
    <author>
      <name>Yuichi Fujiki</name>
    </author>
    <author>
      <name>Greg Friedman</name>
    </author>
    <author>
      <name>Armando Fox</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 6th Symposium on Operating Systems Design and Implementation
  (OSDI), San Francisco, CA, Dec 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0406005v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406005v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0411080v1</id>
    <updated>2004-11-23T16:52:26Z</updated>
    <published>2004-11-23T16:52:26Z</published>
    <title>Modeling the input history of programs for improved instruction-memory
  performance</title>
    <summary>  When a program is loaded into memory for execution, the relative position of
its basic blocks is crucial, since loading basic blocks that are unlikely to be
executed first places them high in the instruction-memory hierarchy only to be
dislodged as the execution goes on. In this paper we study the use of Bayesian
networks as models of the input history of a program. The main point is the
creation of a probabilistic model that persists as the program is run on
different inputs and at each new input refines its own parameters in order to
reflect the program's input history more accurately. As the model is thus
tuned, it causes basic blocks to be reordered so that, upon arrival of the next
input for execution, loading the basic blocks into memory automatically takes
into account the input history of the program. We report on extensive
experiments, whose results demonstrate the efficacy of the overall approach in
progressively lowering the execution times of a program on identical inputs
placed randomly in a sequence of varied inputs. We provide results on selected
SPEC CINT2000 programs and also evaluate our approach as compared to the gcc
level-3 optimization and to Pettis-Hansen reordering.
</summary>
    <author>
      <name>C. A. G. Assis</name>
    </author>
    <author>
      <name>E. S. T. Fernandes</name>
    </author>
    <author>
      <name>V. C. Barbosa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/comjnl/bxl044</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/comjnl/bxl044" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Journal 49 (2006), 744-761</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0411080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0411080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.3329v1</id>
    <updated>2010-02-17T17:52:17Z</updated>
    <published>2010-02-17T17:52:17Z</published>
    <title>A new model for virtual machine migration in virtualized cluster server
  based on Fuzzy Decision Making</title>
    <summary>  In this paper, we show that performance of the virtualized cluster servers
could be improved through intelligent decision over migration time of Virtual
Machines across heterogeneous physical nodes of a cluster server. The cluster
serves a variety range of services from Web Service to File Service. Some of
them are CPU-Intensive while others are RAM-Intensive and so on. Virtualization
has many advantages such as less hardware cost, cooling cost, more
manageability. One of the key benefits is better load balancing by using of VM
migration between hosts. To migrate, we must know which virtual machine needs
to be migrated and when this relocation has to be done and, moreover, which
host must be destined. To relocate VMs from overloaded servers to underloaded
ones, we need to sort nodes from the highest volume to the lowest. There are
some models to finding the most overloaded node, but they have some
shortcomings. The focus of this paper is to present a new method to migrate VMs
between cluster nodes using TOPSIS algorithm - one of the most efficient Multi
Criteria Decision Making techniques- to make more effective decision over whole
active servers of the Cluster and find the most loaded serversTo evaluate the
performance improvement resulted from this model, we used cluster Response time
and Unbalanced Factor.
</summary>
    <author>
      <name>M. Tarighi</name>
    </author>
    <author>
      <name>S. A. Motamedi</name>
    </author>
    <author>
      <name>S. Sharifian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Telecommunications,Volume 1, Issue 1, pp40-51, February
  2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.3329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.3329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4088v1</id>
    <updated>2010-03-22T06:52:12Z</updated>
    <published>2010-03-22T06:52:12Z</published>
    <title>Proficient Pair of Replacement Algorithms on L1 and L2 Cache for Merge
  Sort</title>
    <summary>  Memory hierarchy is used to compete the processors speed. Cache memory is the
fast memory which is used to conduit the speed difference of memory and
processor. The access patterns of Level 1 cache (L1) and Level 2 cache (L2) are
different, when CPU not gets the desired data in L1 then it accesses L2. Thus
the replacement algorithm which works efficiently on L1 may not be as efficient
on L2. Similarly various applications such as Matrix Multiplication, Web, Fast
Fourier Transform (FFT) etc will have varying access pattern. Thus same
replacement algorithm for all types of application may not be efficient. This
paper works for getting an efficient pair of replacement algorithm on L1 and L2
for the algorithm Merge Sort. With the memory reference string of Merge Sort,
we have analyzed the behavior of various existing replacement algorithms on L1.
The existing replacement algorithms which are taken into consideration are:
Least Recently Used (LRU), Least Frequently Used (LFU) and First In First Out
(FIFO). After Analyzing the memory reference pattern of Merge Sort, we have
proposed a Partition Based Replacement algorithm (PBR_L1)) on L1 Cache.
Furthermore we have analyzed various pairs of algorithms on L1 and L2
respectively, resulting in finding a suitable pair of replacement algorithms.
Simulation on L1 shows, among the considered existing replacement algorithms
FIFO is performing better than others. While the proposed replacement algorithm
PBR_L1 is working about 1.7% to 44 % better than FIFO for various cache sizes.
</summary>
    <author>
      <name>Richa Gupta</name>
    </author>
    <author>
      <name>Sanjiv Tokekar</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 2, Issue 3, March 2010,
  https://sites.google.com/site/journalofcomputing/</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.5303v2</id>
    <updated>2010-07-25T15:40:38Z</updated>
    <published>2010-03-27T14:44:01Z</published>
    <title>Determinating Timing Channels in Compute Clouds</title>
    <summary>  Timing side-channels represent an insidious security challenge for cloud
computing, because: (a) massive parallelism in the cloud makes timing channels
pervasive and hard to control; (b) timing channels enable one customer to steal
information from another without leaving a trail or raising alarms; (c) only
the cloud provider can feasibly detect and report such attacks, but the
provider's incentives are not to; and (d) resource partitioning schemes for
timing channel control undermine statistical sharing efficiency, and, with it,
the cloud computing business model. We propose a new approach to timing channel
control, using provider-enforced deterministic execution instead of resource
partitioning to eliminate timing channels within a shared cloud domain.
Provider-enforced determinism prevents execution timing from affecting the
results of a compute task, however large or parallel, ensuring that a task's
outputs leak no timing information apart from explicit timing inputs and total
compute duration. Experiments with a prototype OS for deterministic cloud
computing suggest that such an approach may be practical and efficient. The OS
supports deterministic versions of familiar APIs such as processes, threads,
shared memory, and file systems, and runs coarse-grained parallel tasks as
efficiently and scalably as current timing channel-ridden systems.
</summary>
    <author>
      <name>Amittai Aviram</name>
    </author>
    <author>
      <name>Sen Hu</name>
    </author>
    <author>
      <name>Bryan Ford</name>
    </author>
    <author>
      <name>Ramakrishna Gummadi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.5303v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.5303v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.2831v2</id>
    <updated>2010-12-20T22:16:29Z</updated>
    <published>2010-12-13T18:47:10Z</published>
    <title>Sesame: Self-Constructive System Energy Modeling for Battery-Powered
  Mobile Systems</title>
    <summary>  System energy models are important for energy optimization and management in
mobile systems. However, existing system energy models are built in lab with
the help from a second computer. Not only are they labor-intensive; but also
they will not adequately account for the great diversity in the hardware and
usage of mobile systems. Moreover, existing system energy models are intended
for energy estimation for time intervals of one second or longer; they do not
provide the required rate for fine-grain use such as per-application energy
accounting.
  In this work, we study a self-modeling paradigm in which a mobile system
automatically generates its energy model without any external assistance. Our
solution, Se-same, leverages the possibility of self power measurement through
the smart battery interface and employs a suite of novel techniques to achieve
accuracy and rate much higher than that of the smart battery interface.
  We report the implementation and evaluation of Se-same on a laptop and a
smartphone. The experiment results show that Sesame generates system energy
models of 95% accuracy at one estimation per second and 88% accuracy at one
estimation per 10ms, without any external assistance. A five-day field studies
with four laptop and four smartphones users further demonstrate the
effectiveness, efficiency, and noninvasiveness of Sesame.
</summary>
    <author>
      <name>Mian Dong</name>
    </author>
    <author>
      <name>Lin Zhong</name>
    </author>
    <link href="http://arxiv.org/abs/1012.2831v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.2831v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.3452v1</id>
    <updated>2010-12-15T20:38:40Z</updated>
    <published>2010-12-15T20:38:40Z</published>
    <title>Customer Appeasement Scheduling</title>
    <summary>  Almost all of the current process scheduling algorithms which are used in
modern operating systems (OS) have their roots in the classical scheduling
paradigms which were developed during the 1970's. But modern computers have
different types of software loads and user demands. We think it is important to
run what the user wants at the current moment. A user can be a human, sitting
in front of a desktop machine, or it can be another machine sending a request
to a server through a network connection. We think that OS should become
intelligent to distinguish between different processes and allocate resources,
including CPU, to those processes which need them most. In this work, as a
first step to make the OS aware of the current state of the system, we consider
process dependencies and interprocess communications. We are developing a
model, which considers the need to satisfy interactive users and other possible
remote users or customers, by making scheduling decisions based on process
dependencies and interprocess communications. Our simple proof of concept
implementation and experiments show the effectiveness of this approach in the
real world applications. Our implementation does not require any change in the
software applications nor any special kind of configuration in the system,
Moreover, it does not require any additional information about CPU needs of
applications nor other resource requirements. Our experiments show significant
performance improvement for real world applications. For example, almost
constant average response time for Mysql data base server and constant frame
rate for mplayer under different simulated load values.
</summary>
    <author>
      <name>Mohammad R Nikseresht</name>
    </author>
    <author>
      <name>Anil Somayaji</name>
    </author>
    <author>
      <name>Anil Maheshwari</name>
    </author>
    <link href="http://arxiv.org/abs/1012.3452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.3452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.5695v1</id>
    <updated>2010-12-28T04:35:02Z</updated>
    <published>2010-12-28T04:35:02Z</published>
    <title>Dynamic Scheduling of Skippable Periodic Tasks with Energy Efficiency in
  Weakly Hard Real-Time System</title>
    <summary>  Energy consumption is a critical design issue in real-time systems,
especially in battery- operated systems. Maintaining high performance, while
extending the battery life between charges is an interesting challenge for
system designers. Dynamic Voltage Scaling (DVS) allows a processor to
dynamically change speed and voltage at run time, thereby saving energy by
spreading run cycles into idle time. Knowing when to use full power and when
not, requires the cooperation of the operating system scheduler. Usually,
higher processor voltage and frequency leads to higher system throughput while
energy reduction can be obtained using lower voltage and frequency. Instead of
lowering processor voltage and frequency as much as possible, energy efficient
real-time scheduling adjusts voltage and frequency according to some
optimization criteria, such as low energy consumption or high throughput, while
it meets the timing constraints of the real-time tasks. As the quantity and
functional complexity of battery powered portable devices continues to raise,
energy efficient design of such devices has become increasingly important. Many
real-time scheduling algorithms have been developed recently to reduce energy
consumption in the portable devices that use DVS capable processors. Three
algorithms namely Red Tasks Only (RTO), Blue When Possible (BWP) and Red as
Late as Possible (RLP) are proposed in the literature to schedule the real-time
tasks in Weakly-hard real-time systems. This paper proposes optimal slack
management algorithms to make the above existing weakly hard real-time
scheduling algorithms energy efficient using DVS and DPD techniques.
</summary>
    <author>
      <name>Santhi Baskaran</name>
    </author>
    <author>
      <name>P. Thambidurai</name>
    </author>
    <link href="http://arxiv.org/abs/1012.5695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.5695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.2348v1</id>
    <updated>2011-03-11T19:12:15Z</updated>
    <published>2011-03-11T19:12:15Z</published>
    <title>Transparent Programming of Heterogeneous Smartphones for Sensing</title>
    <summary>  Sensing on smartphones is known to be power-hungry. It has been shown that
this problem can be solved by adding an ultra low-power processor to execute
simple, frequent sensor data processing. While very effective in saving energy,
this resulting heterogeneous, distributed architecture poses a significant
challenge to application development.
  We present Reflex, a suite of runtime and compilation techniques to conceal
the heterogeneous, distributed nature from developers. The Reflex automatically
transforms the developer's code for distributed execution with the help of the
Reflex runtime. To create a unified system illusion, Reflex features a novel
software distributed shared memory (DSM) design that leverages the extreme
architectural asymmetry between the low-power processor and the powerful
central processor to achieve both energy efficiency and performance.
  We report a complete realization of Reflex for heterogeneous smartphones with
Maemo/Linux as the central kernel. Using a tri-processor hardware prototype and
sensing applications reported in recent literature, we evaluate the Reflex
realization for programming transparency, energy efficiency, and performance.
We show that Reflex supports a programming style that is very close to
contemporary smartphone programming. It allows existing sensing applications to
be ported with minor source code changes. Reflex reduces the system power in
sensing by up to 83%, and its runtime system only consumes 10% local memory on
a typical ultra-low power processor.
</summary>
    <author>
      <name>Felix Xiaozhu Lin</name>
    </author>
    <author>
      <name>Zhen Wang</name>
    </author>
    <author>
      <name>Robert LiKamWa</name>
    </author>
    <author>
      <name>Lin Zhong</name>
    </author>
    <link href="http://arxiv.org/abs/1103.2348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.2348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.4623v1</id>
    <updated>2011-10-20T19:43:58Z</updated>
    <published>2011-10-20T19:43:58Z</published>
    <title>Efficient Synchronization Primitives for GPUs</title>
    <summary>  In this paper, we revisit the design of synchronization
primitives---specifically barriers, mutexes, and semaphores---and how they
apply to the GPU. Previous implementations are insufficient due to the
discrepancies in hardware and programming model of the GPU and CPU. We create
new implementations in CUDA and analyze the performance of spinning on the GPU,
as well as a method of sleeping on the GPU, by running a set of memory-system
benchmarks on two of the most common GPUs in use, the Tesla- and Fermi-class
GPUs from NVIDIA. From our results we define higher-level principles that are
valid for generic many-core processors, the most important of which is to limit
the number of atomic accesses required for a synchronization operation because
atomic accesses are slower than regular memory accesses. We use the results of
the benchmarks to critique existing synchronization algorithms and guide our
new implementations, and then define an abstraction of GPUs to classify any GPU
based on the behavior of the memory system. We use this abstraction to create
suitable implementations of the primitives specifically targeting the GPU, and
analyze the performance of these algorithms on Tesla and Fermi. We then predict
performance on future GPUs based on characteristics of the abstraction. We also
examine the roles of spin waiting and sleep waiting in each primitive and how
their performance varies based on the machine abstraction, then give a set of
guidelines for when each strategy is useful based on the characteristics of the
GPU and expected contention.
</summary>
    <author>
      <name>Jeff A. Stuart</name>
    </author>
    <author>
      <name>John D. Owens</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages with appendix, several figures, plans to submit to CompSci
  conference in early 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.4623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.4623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.1; I.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.1591v1</id>
    <updated>2012-07-06T11:48:24Z</updated>
    <published>2012-07-06T11:48:24Z</published>
    <title>A Secure Dynamic Job Scheduling on Smart Grid using RSA Algorithm</title>
    <summary>  Grid computing is a computation methodology using group of clusters connected
over high-speed networks that involves coordinating and sharing computational
power, data storage and network resources. Integrating a set of clusters of
workstations into one large computing environment can improve the availability
of computing power. The goal of scheduling is to achieve highest possible
system throughput and to match the application need with the available
computing resources. A secure scheduling model is presented, that performs job
grouping activity at runtime. In a Grid environment, security is necessary
because grid is a dynamic environment and participates are independent bodies
with different policies, objectives and requirements. Authentication should be
verified for Grid resource owners as well as resource requesters before they
are allowed to join in scheduling activities. In order to achieve secure
resource and job scheduling including minimum processing time and maximum
resource utilization, A Secure Resource by using RSA algorithm on Networking
and Job Scheduling model with Job Grouping strategy(JGS) in Grid Computing has
been proposed. The result shows significant improvement in the processing time
of jobs and resource utilization as compared to dynamic job grouping (DJG)
based scheduling on smart grids (SG).
</summary>
    <author>
      <name>P. Radha Krishna Reddy</name>
    </author>
    <author>
      <name>Ashim Roy</name>
    </author>
    <author>
      <name>G. Sireesha</name>
    </author>
    <author>
      <name>Ismatha Begum</name>
    </author>
    <author>
      <name>S. Siva Ramaiah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published; 2277128x 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.1591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.1591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.6406v1</id>
    <updated>2012-08-31T06:53:14Z</updated>
    <published>2012-08-31T06:53:14Z</published>
    <title>Building Resilient Cloud Over Unreliable Commodity Infrastructure</title>
    <summary>  Cloud Computing has emerged as a successful computing paradigm for
efficiently utilizing managed compute infrastructure such as high speed
rack-mounted servers, connected with high speed networking, and reliable
storage. Usually such infrastructure is dedicated, physically secured and has
reliable power and networking infrastructure. However, much of our idle compute
capacity is present in unmanaged infrastructure like idle desktops, lab
machines, physically distant server machines, and laptops. We present a scheme
to utilize this idle compute capacity on a best-effort basis and provide high
availability even in face of failure of individual components or facilities.
  We run virtual machines on the commodity infrastructure and present a cloud
interface to our end users. The primary challenge is to maintain availability
in the presence of node failures, network failures, and power failures. We run
multiple copies of a Virtual Machine (VM) redundantly on geographically
dispersed physical machines to achieve availability. If one of the running
copies of a VM fails, we seamlessly switchover to another running copy. We use
Virtual Machine Record/Replay capability to implement this redundancy and
switchover. In current progress, we have implemented VM Record/Replay for
uniprocessor machines over Linux/KVM and are currently working on VM
Record/Replay on shared-memory multiprocessor machines. We report initial
experimental results based on our implementation.
</summary>
    <author>
      <name>Piyus Kedia</name>
    </author>
    <author>
      <name>Sorav Bansal</name>
    </author>
    <author>
      <name>Deepak Deshpande</name>
    </author>
    <author>
      <name>Sreekanth Iyer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CCEM.2012.6354601</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CCEM.2012.6354601" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Oral presentation at IEEE "Cloud Computing for Emerging Markets",
  Oct. 11-12, 2012, Bangalore, India</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.6406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.6406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.6428v1</id>
    <updated>2012-08-31T09:04:05Z</updated>
    <published>2012-08-31T09:04:05Z</published>
    <title>A Hardware Time Manager Implementation for the Xenomai Real-Time Kernel
  of Embedded Linux</title>
    <summary>  Nowadays, the use of embedded operating systems in different embedded
projects is subject to a tremendous growth. Embedded Linux is becoming one of
those most popular EOSs due to its modularity, efficiency, reliability, and
cost. One way to make it hard real-time is to include a real-time kernel like
Xenomai. One of the key characteristics of a Real-Time Operating System (RTOS)
is its ability to meet execution time deadlines deterministically. So, the more
precise and flexible the time management can be, the better it can handle
efficiently the determinism for different embedded applications. RTOS time
precision is characterized by a specific periodic interrupt service controlled
by a software time manager. The smaller the period of the interrupt, the better
the precision of the RTOS, the more it overloads the CPU, and though reduces
the overall efficiency of the RTOS. In this paper, we propose to drastically
reduce these overheads by migrating the time management service of Xenomai into
a configurable hardware component to relieve the CPU. The hardware component is
implemented in a Field Programmable Gate Array coupled to the CPU. This work
was achieved in a Master degree project where students could apprehend many
fields of embedded systems: RTOS programming, hardware design, performance
evaluation, etc.
</summary>
    <author>
      <name>Pierre Olivier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Jalil Boukhobza</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Embed With Linux (EWiLi) workshop, Lorient : France (2012)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SIGBED Review 9(2) 38-42 9, 2 (2012) 38-42</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1208.6428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.6428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.4839v1</id>
    <updated>2012-11-20T19:33:08Z</updated>
    <published>2012-11-20T19:33:08Z</published>
    <title>An Insight View of Kernel Visual Debugger in System Boot up</title>
    <summary>  For many years, developers could not figure out the mystery of OS kernels.
The main source of this mystery is the interaction between operating systems
and hardware while system's boot up and kernel initialization. In addition,
many operating system kernels differ in their behavior toward many situations.
For instance, kernels act differently in racing conditions, kernel
initialization and process scheduling. For such operations, kernel debuggers
were designed to help in tracing kernel behavior and solving many kernel bugs.
The importance of kernel debuggers is not limited to kernel code tracing but
also, they can be used in verification and performance comparisons. However,
developers had to be aware of debugger commands thus introducing some
difficulties to non-expert programmers. Later, several visual kernel debuggers
were presented to make it easier for programmers to trace their kernel code and
analyze kernel behavior. Nowadays, several kernel debuggers exist for solving
this mystery but only very few support line-by-line debugging at run-time. In
this paper, a generic approach for operating system source code debugging in
graphical mode with line-by-line tracing support is proposed. In the context of
this approach, system boot up and evaluation of two operating system schedulers
from several points of views will be discussed.
</summary>
    <author>
      <name>Mohamed Farag</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsit.2012.4510</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsit.2012.4510" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, International Journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 4, No 5, October 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.4839v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.4839v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.6067v1</id>
    <updated>2013-04-22T19:29:05Z</updated>
    <published>2013-04-22T19:29:05Z</published>
    <title>Invasive Computing - Common Terms and Granularity of Invasion</title>
    <summary>  Future MPSoCs with 1000 or more processor cores on a chip require new means
for resource-aware programming in order to deal with increasing imperfections
such as process variation, fault rates, aging effects, and power as well as
thermal problems. On the other hand, predictable program executions are
threatened if not impossible if no proper means of resource isolation and
exclusive use may be established on demand. In view of these problems and
menaces, invasive computing enables an application programmer to claim for
processing resources and spread computations to claimed processors dynamically
at certain points of the program execution.
  Such decisions may be depending on the degree of application parallelism and
the state of the underlying resources such as utilization, load, and
temperature, but also with the goal to provide predictable program execution on
MPSoCs by claiming processing resources exclusively as the default and thus
eliminating interferences and creating the necessary isolation between multiple
concurrently running applications. For achieving this goal, invasive computing
introduces new programming constructs for resource-aware programming that
meanwhile, for testing purpose, have been embedded into the parallel computing
language X10 as developed by IBM using a library-based approach.
  This paper presents major ideas and common terms of invasive computing as
investigated by the DFG Transregional Collaborative Research Centre TR89.
Moreoever, a reflection is given on the granularity of resources that may be
requested by invasive programs.
</summary>
    <author>
      <name>Jürgen Teich</name>
    </author>
    <author>
      <name>Wolfgang Schröder-Preikschat</name>
    </author>
    <author>
      <name>Andreas Herkersdorf</name>
    </author>
    <link href="http://arxiv.org/abs/1304.6067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.6067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.1810v1</id>
    <updated>2013-12-06T09:18:04Z</updated>
    <published>2013-12-06T09:18:04Z</published>
    <title>File System - A Component of Operating System</title>
    <summary>  The file system provides the mechanism for online storage and access to file
contents, including data and programs. This paper covers the high-level details
of file systems, as well as related topics such as the disk cache, the file
system interface to the kernel, and the user-level APIs that use the features
of the file system. It will give you a thorough understanding of how a file
system works in general. The main component of the operating system is the file
system. It is used to create, manipulate, store, and retrieve data. At the
highest level, a file system is a way to manage information on a secondary
storage medium. There are so many layers under and above the file system. All
the layers are to be fully described here. This paper will give the explanatory
knowledge of the file system designers and the researchers in the area. The
complete path from the user process to secondary storage device is to be
mentioned. File system is the area where the researchers are doing lot of job
and there is always a need to do more work. The work is going on for the
efficient, secure, energy saving techniques for the file systems. As we know
that the hardware is going to be fast in performance and low-priced day by day.
The software is not built to comeback with the hardware technology. So there is
a need to do research in this area to bridge the technology gap.
</summary>
    <author>
      <name>Brijender Kahanwal</name>
    </author>
    <author>
      <name>Tejinder Pal Singh</name>
    </author>
    <author>
      <name>Ruchira Bhargava</name>
    </author>
    <author>
      <name>Girish Pal Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Asian Journal of Computer Science and Information Technology 2(5),
  pp. 124-128, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1312.1810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.1810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.1822v1</id>
    <updated>2013-12-06T10:20:38Z</updated>
    <published>2013-12-06T10:20:38Z</published>
    <title>Towards the Framework of the File Systems Performance Evaluation
  Techniques and the Taxonomy of Replay Traces</title>
    <summary>  This is the era of High Performance Computing (HPC). There is a great demand
of the best performance evaluation techniques for the file and storage systems.
The task of evaluation is both necessary and hard. It gives in depth analysis
of the target system and that becomes the decision points for the users. That
is also helpful for the inventors or developers to find out the bottleneck in
their systems. In this paper many performance evaluation techniques are
described for file and storage system evaluation and the main stress is given
on the important one that is replay traces. A survey has been done for the
performance evaluation techniques used by the researchers and on the replay
traces. And the taxonomy of the replay traces is described. The some of the
popular replay traces are just like, Tracefs [1], //Trace [2], Replayfs [3] and
VFS Interceptor [12]. At last we have concluded all the features that must be
considered when we are going to develop the new tool for the replay traces. The
complete work of this paper shows that the storage system developers must care
about all the techniques which can be used for the performance evaluation of
the file systems. So they can develop highly efficient future file and storage
systems.
</summary>
    <author>
      <name>Brijender Kahanwal</name>
    </author>
    <author>
      <name>Tejinder Pal Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Research in Computer Science,
  2(6) pp. 224-229, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1312.1822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.1822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.3938v3</id>
    <updated>2014-01-30T21:39:39Z</updated>
    <published>2013-12-13T20:53:39Z</published>
    <title>Transparent Checkpoint-Restart over InfiniBand</title>
    <summary>  InfiniBand is widely used for low-latency, high-throughput cluster computing.
Saving the state of the InfiniBand network as part of distributed checkpointing
has been a long-standing challenge for researchers. Because of a lack of a
solution, typical MPI implementations have included custom checkpoint-restart
services that "tear down" the network, checkpoint each node as if the node were
a standalone computer, and then re-connect the network again. We present the
first example of transparent, system-initiated checkpoint-restart that directly
supports InfiniBand. The new approach is independent of any particular Linux
kernel, thus simplifying the current practice of using a kernel-based module,
such as BLCR. This direct approach results in checkpoints that are found to be
faster than with the use of a checkpoint-restart service. The generality of
this approach is shown not only by checkpointing an MPI computation, but also a
native UPC computation (Berkeley Unified Parallel C), which does not use MPI.
Scalability is shown by checkpointing 2,048 MPI processes across 128 nodes
(with 16 cores per node). In addition, a cost-effective debugging approach is
also enabled, in which a checkpoint image from an InfiniBand-based production
cluster is copied to a local Ethernet-based cluster, where it can be restarted
and an interactive debugger can be attached to it. This work is based on a
plugin that extends the DMTCP (Distributed MultiThreaded CheckPointing)
checkpoint-restart package.
</summary>
    <author>
      <name>Jiajun Cao</name>
    </author>
    <author>
      <name>Gregory Kerr</name>
    </author>
    <author>
      <name>Kapil Arya</name>
    </author>
    <author>
      <name>Gene Cooperman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 2 figures, 9 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.3938v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.3938v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.5010v1</id>
    <updated>2014-03-20T00:34:00Z</updated>
    <published>2014-03-20T00:34:00Z</published>
    <title>Task &amp; Resource Self-adaptive Embedded Real-time Operating System
  Microkernel for Wireless Sensor Nodes</title>
    <summary>  Wireless Sensor Networks (WSNs) are used in many application fields, such as
military, healthcare, environment surveillance, etc. The WSN OS based on
event-driven model doesn't support real-time and multi-task application types
and the OSs based on thread-driven model consume much energy because of
frequent context switch. Due to the high-dense and large-scale deployment of
sensor nodes, it is very difficult to collect sensor nodes to update their
software. Furthermore, the sensor nodes are vulnerable to security attacks
because of the characteristics of broadcast communication and unattended
application. This paper presents a task and resource self-adaptive embedded
real-time microkernel, which proposes hybrid programming model and offers a
two-level scheduling strategy to support real-time multi-task correspondingly.
A communication scheme, which takes the "tuple" space and "IN/OUT" primitives
from "LINDA", is proposed to support some collaborative and distributed tasks.
In addition, this kernel implements a run-time over-the-air updating mechanism
and provides a security policy to avoid the attacks and ensure the reliable
operation of nodes. The performance evaluation is proposed and the experiential
results show this kernel is task-oriented and resource-aware and can be used
for the applications of event-driven and real-time multi-task.
</summary>
    <author>
      <name>Kexing Xing</name>
    </author>
    <author>
      <name>Decheng Zuo</name>
    </author>
    <author>
      <name>Haiying Zhou</name>
    </author>
    <author>
      <name>Hou Kun-Mean</name>
    </author>
    <link href="http://arxiv.org/abs/1403.5010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.5010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1637v1</id>
    <updated>2014-04-06T23:53:01Z</updated>
    <published>2014-04-06T23:53:01Z</published>
    <title>An Enhanced Multi-Pager Environment Support for Second Generation
  Microkernels</title>
    <summary>  The main objective of this paper is to present a mechanism of enhanced paging
support for the second generation microkernels in the form of explicit support
of multi-pager environment for the tasks running in the system. Proposed
mechanism is based on the intra-kernel high granularity pagers assignments per
virtual address space, which allow efficient and simple dispatching of page
faults to the appropriate pagers. The paging is one of the major features of
the virtual memory, which is extensively used by advanced operating systems to
provide an illusion of elastic memory. Original and present second generation
microkernels provide only limited, inflexible and unnatural support for paging.
Furthermore, facilities provided by current solutions for multi-pager support
on the runtime level introduce an overhead in terms of mode switches and thread
context switches which can be significantly reduced. Limited paging support
limits the attractiveness of the second generation microkernel based systems
use in real-life applications, in which processes usually have concurrent
servicing of multiple paging servers. The purpose of this paper is to present a
facilities for the efficient and flexible support of multi-pager environments
for the second generation microkernels. A comparison of the proposed solution
to the present architecture L4 + L4Re has been made and overhead of the page
fault handling critical path has been evaluated. Proposed solution is simple
enough and provides a natural and flexible support of multi-pager environments
for second generation microkernels in efficient way. It introduces a third less
overhead in terms of the mode switches and thread context switches in
comparison to the present L4 + L4Re solution implemented in the Fiasco.OC.
</summary>
    <author>
      <name>Yauhen Klimiankou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, Vol. 12 No. 1 JAN 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.1637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.3463v1</id>
    <updated>2014-10-13T14:26:28Z</updated>
    <published>2014-10-13T14:26:28Z</published>
    <title>Mining Block I/O Traces for Cache Preloading with Sparse Temporal
  Non-parametric Mixture of Multivariate Poisson</title>
    <summary>  Existing caching strategies, in the storage domain, though well suited to
exploit short range spatio-temporal patterns, are unable to leverage long-range
motifs for improving hitrates. Motivated by this, we investigate novel Bayesian
non-parametric modeling(BNP) techniques for count vectors, to capture long
range correlations for cache preloading, by mining Block I/O traces. Such
traces comprise of a sequence of memory accesses that can be aggregated into
high-dimensional sparse correlated count vector sequences.
  While there are several state of the art BNP algorithms for clustering and
their temporal extensions for prediction, there has been no work on exploring
these for correlated count vectors. Our first contribution addresses this gap
by proposing a DP based mixture model of Multivariate Poisson (DP-MMVP) and its
temporal extension(HMM-DP-MMVP) that captures the full covariance structure of
multivariate count data. However, modeling full covariance structure for count
vectors is computationally expensive, particularly for high dimensional data.
Hence, we exploit sparsity in our count vectors, and as our main contribution,
introduce the Sparse DP mixture of multivariate Poisson(Sparse-DP-MMVP),
generalizing our DP-MMVP mixture model, also leading to more efficient
inference. We then discuss a temporal extension to our model for cache
preloading.
  We take the first step towards mining historical data, to capture long range
patterns in storage traces for cache preloading. Experimentally, we show a
dramatic improvement in hitrates on benchmark traces and lay the groundwork for
further research in storage domain to reduce latencies using data mining
techniques to capture long range motifs.
</summary>
    <author>
      <name>Lavanya Sita Tekumalla</name>
    </author>
    <author>
      <name>Chiranjib Bhattacharyya</name>
    </author>
    <link href="http://arxiv.org/abs/1410.3463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.3463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.07084v3</id>
    <updated>2015-09-14T21:25:55Z</updated>
    <published>2015-01-28T12:38:20Z</published>
    <title>k2U: A General Framework from k-Point Effective Schedulability Analysis
  to Utilization-Based Tests</title>
    <summary>  To deal with a large variety of workloads in different application domains in
real-time embedded systems, a number of expressive task models have been
developed. For each individual task model, researchers tend to develop
different types of techniques for deriving schedulability tests with different
computation complexity and performance. In this paper, we present a general
schedulability analysis framework, namely the k2U framework, that can be
potentially applied to analyze a large set of real-time task models under any
fixed-priority scheduling algorithm, on both uniprocessor and multiprocessor
scheduling. The key to k2U is a k-point effective schedulability test, which
can be viewed as a "blackbox" interface. For any task model, if a corresponding
k-point effective schedulability test can be constructed, then a sufficient
utilization-based test can be automatically derived. We show the generality of
k2U by applying it to different task models, which results in new and improved
tests compared to the state-of-the-art.
  Analogously, a similar concept by testing only k points with a different
formulation has been studied by us in another framework, called k2Q, which
provides quadratic bounds or utilization bounds based on a different
formulation of schedulability test. With the quadratic and hyperbolic forms,
k2Q and k2U frameworks can be used to provide many quantitive features to be
measured, like the total utilization bounds, speed-up factors, etc., not only
for uniprocessor scheduling but also for multiprocessor scheduling. These
frameworks can be viewed as a "blackbox" interface for schedulability tests and
response-time analysis.
</summary>
    <author>
      <name>Jian-Jia Chen</name>
    </author>
    <author>
      <name>Wen-Hung Huang</name>
    </author>
    <author>
      <name>Cong Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1501.07084v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.07084v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.04640v2</id>
    <updated>2015-04-29T20:05:56Z</updated>
    <published>2015-04-17T21:13:07Z</published>
    <title>The Influence of Malloc Placement on TSX Hardware Transactional Memory</title>
    <summary>  The hardware transactional memory (HTM) implementation in Intel's i7-4770
"Haswell" processor tracks the transactional read-set in the L1 (level-1), L2
(level-2) and L3 (level-3) caches and the write-set in the L1 cache.
Displacement or eviction of read-set entries from the cache hierarchy or
write-set entries from the L1 results in abort. We show that the placement
policies of dynamic storage allocators -- such as those found in common
"malloc" implementations -- can influence the L1 conflict miss rate in the L1.
Conflict misses -- sometimes called mapping misses -- arise because of less
than ideal associativity and represent imbalanced distribution of active memory
blocks over the set of available L1 indices. Under transactional execution
conflict misses may manifest as aborts, representing wasted or futile effort
instead of a simple stall as would occur in normal execution mode.
  Furthermore, when HTM is used for transactional lock elision (TLE),
persistent aborts arising from conflict misses can force the offending thread
through the so-called "slow path". The slow path is undesirable as the thread
must acquire the lock and run the critical section in normal execution mode,
precluding the concurrent execution of threads in the "fast path" that monitor
that same lock and run their critical sections in transactional mode. For a
given lock, multiple threads can concurrently use the transactional fast path,
but at most one thread can use the non-transactional slow path at any given
time. Threads in the slow path preclude safe concurrent fast path execution.
Aborts rising from placement policies and L1 index imbalance can thus result in
loss of concurrency and reduced aggregate throughput.
</summary>
    <author>
      <name>Dave Dice</name>
    </author>
    <author>
      <name>Tim Harris</name>
    </author>
    <author>
      <name>Alex Kogan</name>
    </author>
    <author>
      <name>Yossi Lev</name>
    </author>
    <link href="http://arxiv.org/abs/1504.04640v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.04640v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.07481v1</id>
    <updated>2015-04-28T14:05:59Z</updated>
    <published>2015-04-28T14:05:59Z</published>
    <title>Improving Block-level Efficiency with scsi-mq</title>
    <summary>  Current generation solid-state storage devices are exposing a new bottlenecks
in the SCSI and block layers of the Linux kernel, where IO throughput is
limited by lock contention, inefficient interrupt handling, and poor memory
locality. To address these limitations, the Linux kernel block layer underwent
a major rewrite with the blk-mq project to move from a single request queue to
a multi-queue model. The Linux SCSI subsystem rework to make use of this new
model, known as scsi-mq, has been merged into the Linux kernel and work is
underway for dm-multipath support in the upcoming Linux 4.0 kernel. These
pieces were necessary to make use of the multi-queue block layer in a Lustre
parallel filesystem with high availability requirements. We undertook adding
support of the 3.18 kernel to Lustre with scsi-mq and dm-multipath patches to
evaluate the potential of these efficiency improvements. In this paper we
evaluate the block-level performance of scsi-mq with backing storage hardware
representative of a HPC-targerted Lustre filesystem. Our findings show that
SCSI write request latency is reduced by as much as 13.6%. Additionally, when
profiling the CPU usage of our prototype Lustre filesystem, we found that CPU
idle time increased by a factor of 7 with Linux 3.18 and blk-mq as compared to
a standard 2.6.32 Linux kernel. Our findings demonstrate increased efficiency
of the multi-queue block layer even with disk-based caching storage arrays used
in existing parallel filesystems.
</summary>
    <author>
      <name>Blake Caldwell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.07481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.07566v1</id>
    <updated>2015-06-24T21:14:27Z</updated>
    <published>2015-06-24T21:14:27Z</published>
    <title>Optimize Unsynchronized Garbage Collection in an SSD Array</title>
    <summary>  Solid state disks (SSDs) have advanced to outperform traditional hard drives
significantly in both random reads and writes. However, heavy random writes
trigger fre- quent garbage collection and decrease the performance of SSDs. In
an SSD array, garbage collection of individ- ual SSDs is not synchronized,
leading to underutilization of some of the SSDs.
  We propose a software solution to tackle the unsyn- chronized garbage
collection in an SSD array installed in a host bus adaptor (HBA), where
individual SSDs are exposed to an operating system. We maintain a long I/O
queue for each SSD and flush dirty pages intelligently to fill the long I/O
queues so that we hide the performance imbalance among SSDs even when there are
few parallel application writes. We further define a policy of select- ing
dirty pages to flush and a policy of taking out stale flush requests to reduce
the amount of data written to SSDs. We evaluate our solution in a real system.
Experi- ments show that our solution fully utilizes all SSDs in an array under
random write-heavy workloads. It improves I/O throughput by up to 62% under
random workloads of mixed reads and writes when SSDs are under active garbage
collection. It causes little extra data writeback and increases the cache hit
rate.
</summary>
    <author>
      <name>Da Zheng</name>
    </author>
    <author>
      <name>Randal Burns</name>
    </author>
    <author>
      <name>Alexander S. Szalay</name>
    </author>
    <link href="http://arxiv.org/abs/1506.07566v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.07566v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02552v1</id>
    <updated>2015-10-09T02:58:40Z</updated>
    <published>2015-10-09T02:58:40Z</published>
    <title>Multitasking Programming of OBDH Satellite Based On PC-104</title>
    <summary>  On Board Data Handling (OBDH) has functions to monitor, control, acquire,
analyze, take a decision, and execute the command. OBDH should organize the
task between sub system. OBDH like a heart which has a vital function. Because
the function is seriously important therefore designing and implementing the
OBDH should be carefully, in order to have a good reliability. Many OBDHs have
been made to support the satellite mission using primitive programming. In
handling the data from various input, OBDH should always be available to all
sub systems, when the tasks are many, it is not easy to program using primitive
programming. Sometimes the data become corrupt because the data which come to
the OBDH is in the same time. Therefore it is required to have a way to handle
the data safely and also easy in programming perspective. In this research,
OBDH is programmed using multi tasking programming perspective has been
created. The Operating System (OS) has been implemented so that can run the
tasks simultaneously. The OS is prepared by configuring the Linux Kernel for
the specific processor, creating Root File System (RFS), installing the
BusyBox. In order to do the above method, preparing the environment in our
machine has been done, they are installing the Cross Tool Chain, U-Boot,
GNU-Linux Kernel Source etc. After that, programming using c code with
multitasking programming can be implemented. By using above method, it is found
that programming is easier and the corruption data because of reentrancy can be
minimized. Keywords- Operating System, PC-104, Kernel, C Programming
</summary>
    <author>
      <name>Haryono Haryono</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of advanced studies in Computer Science and
  Engineering IJASCSE Volume 4, Issue 8, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1510.02552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.05567v2</id>
    <updated>2015-11-12T12:16:11Z</updated>
    <published>2015-10-19T16:26:47Z</published>
    <title>Energy-Efficient Scheduling for Homogeneous Multiprocessor Systems</title>
    <summary>  We present a number of novel algorithms, based on mathematical optimization
formulations, in order to solve a homogeneous multiprocessor scheduling
problem, while minimizing the total energy consumption. In particular, for a
system with a discrete speed set, we propose solving a tractable linear
program. Our formulations are based on a fluid model and a global scheduling
scheme, i.e. tasks are allowed to migrate between processors. The new methods
are compared with three global energy/feasibility optimal workload allocation
formulations. Simulation results illustrate that our methods achieve both
feasibility and energy optimality and outperform existing methods for
constrained deadline tasksets. Specifically, the results provided by our
algorithm can achieve up to an 80% saving compared to an algorithm without a
frequency scaling scheme and up to 70% saving compared to a constant frequency
scaling scheme for some simulated tasksets. Another benefit is that our
algorithms can solve the scheduling problem in one step instead of using a
recursive scheme. Moreover, our formulations can solve a more general class of
scheduling problems, i.e. any periodic real-time taskset with arbitrary
deadline. Lastly, our algorithms can be applied to both online and offline
scheduling schemes.
</summary>
    <author>
      <name>Mason Thammawichai</name>
    </author>
    <author>
      <name>Eric C. Kerrigan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected typos: definition of J_i in Section 2.1; (3b)-(3c);
  definition of \Phi_A and \Phi_D in paragraph after (6b). Previous equations
  were correct only for special case of p_i=d_i</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.05567v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.05567v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.07654v3</id>
    <updated>2016-03-12T18:13:31Z</updated>
    <published>2015-12-23T22:41:04Z</published>
    <title>Mixed-Criticality Scheduling with I/O</title>
    <summary>  This paper addresses the problem of scheduling tasks with different
criticality levels in the presence of I/O requests. In mixed-criticality
scheduling, higher criticality tasks are given precedence over those of lower
criticality when it is impossible to guarantee the schedulability of all tasks.
While mixed-criticality scheduling has gained attention in recent years, most
approaches typically assume a periodic task model. This assumption does not
always hold in practice, especially for real-time and embedded systems that
perform I/O. For example, many tasks block on I/O requests until devices signal
their completion via interrupts; both the arrival of interrupts and the waking
of blocked tasks can be aperiodic. In our prior work, we developed a scheduling
technique in the Quest real-time operating system, which integrates the
time-budgeted management of I/O operations with Sporadic Server scheduling of
tasks. This paper extends our previous scheduling approach with support for
mixed-criticality tasks and I/O requests on the same processing core. Results
show the effective schedulability of different task sets in the presence of I/O
requests is superior in our approach compared to traditional methods that
manage I/O using techniques such as Sporadic Servers.
</summary>
    <author>
      <name>Eric Missimer</name>
    </author>
    <author>
      <name>Katherine Zhao</name>
    </author>
    <author>
      <name>Richard West</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Second version has replaced simulation experiments with real machine
  experiments, third version fixed minor error in Equation 5 (missing a plus
  sign)</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.07654v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.07654v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01378v5</id>
    <updated>2017-10-26T03:27:40Z</updated>
    <published>2016-04-05T19:33:27Z</published>
    <title>Isolate First, Then Share: a New OS Architecture for Datacenter
  Computing</title>
    <summary>  This paper presents the "isolate first, then share" OS model in which the
processor cores, memory, and devices are divided up between disparate OS
instances and a new abstraction, subOS, is proposed to encapsulate an OS
instance that can be created, destroyed, and resized on-the-fly. The intuition
is that this avoids shared kernel states between applications, which in turn
reduces performance loss caused by contention. We decompose the OS into the
supervisor and several subOSes running at the same privilege level: a subOS
directly manages physical resources, while the supervisor can create, destroy,
resize a subOS on-the-fly. The supervisor and subOSes have few state sharing,
but fast inter-subOS communication mechanisms are provided on demand.
  We present the first implementation, RainForest, which supports unmodified
Linux binaries. Our comprehensive evaluation shows RainForest outperforms Linux
with four different kernels, LXC, and Xen in terms of worst-case and average
performance most of time when running a large number of benchmarks. The source
code is available soon.
</summary>
    <author>
      <name>Gang Lu</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <author>
      <name>Chongkang Tan</name>
    </author>
    <author>
      <name>Xinlong Lin</name>
    </author>
    <author>
      <name>Defei Kong</name>
    </author>
    <author>
      <name>Chen Zheng</name>
    </author>
    <author>
      <name>Fei Tang</name>
    </author>
    <author>
      <name>Cheng Huang</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Tianshu Hao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 13 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01378v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01378v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.0; D.4.6; D.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02171v1</id>
    <updated>2016-04-07T20:38:51Z</updated>
    <published>2016-04-07T20:38:51Z</published>
    <title>Aware: Controlling App Access to I/O Devices on Mobile Platforms</title>
    <summary>  Smartphones' cameras, microphones, and device displays enable users to
capture and view memorable moments of their lives. However, adversaries can
trick users into authorizing malicious apps that exploit weaknesses in current
mobile platforms to misuse such on-board I/O devices to stealthily capture
photos, videos, and screen content without the users' consent. Contemporary
mobile operating systems fail to prevent such misuse of I/O devices by
authorized apps due to lack of binding between users' interactions and accesses
to I/O devices performed by these apps. In this paper, we propose Aware, a
security framework for authorizing app requests to perform operations using I/O
devices, which binds app requests with user intentions to make all uses of
certain I/O devices explicit. We evaluate our defense mechanisms through
laboratory-based experimentation and a user study, involving 74 human subjects,
whose ability to identify undesired operations targeting I/O devices increased
significantly. Without Aware, only 18% of the participants were able to
identify attacks from tested RAT apps. Aware systematically blocks all the
attacks in absence of user consent and supports users in identifying 82% of
social-engineering attacks tested to hijack approved requests, including some
more sophisticated forms of social engineering not yet present in available
RATs. Aware introduces only 4.79% maximum performance overhead over operations
targeting I/O devices. Aware shows that a combination of system defenses and
user interface can significantly strengthen defenses for controlling the use of
on-board I/O devices.
</summary>
    <author>
      <name>Giuseppe Petracca</name>
    </author>
    <author>
      <name>Ahmad Atamli</name>
    </author>
    <author>
      <name>Yuqiong Sun</name>
    </author>
    <author>
      <name>Jens Grossklags</name>
    </author>
    <author>
      <name>Trent Jaeger</name>
    </author>
    <link href="http://arxiv.org/abs/1604.02171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.08129v1</id>
    <updated>2016-10-26T00:37:34Z</updated>
    <published>2016-10-26T00:37:34Z</published>
    <title>Memshare: a Dynamic Multi-tenant Memory Key-value Cache</title>
    <summary>  Web application performance is heavily reliant on the hit rate of
memory-based caches. Current DRAM-based web caches statically partition their
memory across multiple applications sharing the cache. This causes under
utilization of memory which negatively impacts cache hit rates. We present
Memshare, a novel web memory cache that dynamically manages memory across
applications. Memshare provides a resource sharing model that guarantees
private memory to different applications while dynamically allocating the
remaining shared memory to optimize overall hit rate. Today's high cost of DRAM
storage and the availability of high performance CPU and memory bandwidth, make
web caches memory capacity bound. Memshare's log-structured design allows it to
provide significantly higher hit rates and dynamically partition memory among
applications at the expense of increased CPU and memory bandwidth consumption.
In addition, Memshare allows applications to use their own eviction policy for
their objects, independent of other applications. We implemented Memshare and
ran it on a week-long trace from a commercial memcached provider. We
demonstrate that Memshare increases the combined hit rate of the applications
in the trace by an 6.1% (from 84.7% hit rate to 90.8% hit rate) and reduces the
total number of misses by 39.7% without affecting system throughput or latency.
Even for single-tenant applications, Memshare increases the average hit rate of
the current state-of-the-art memory cache by an additional 2.7% on our
real-world trace.
</summary>
    <author>
      <name>Asaf Cidon</name>
    </author>
    <author>
      <name>Daniel Rushton</name>
    </author>
    <author>
      <name>Stephen M. Rumble</name>
    </author>
    <author>
      <name>Ryan Stutsman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.08129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.08129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07862v2</id>
    <updated>2019-04-29T04:32:16Z</updated>
    <published>2016-11-23T16:23:40Z</published>
    <title>Browsix: Bridging the Gap Between Unix and the Browser</title>
    <summary>  Applications written to run on conventional operating systems typically
depend on OS abstractions like processes, pipes, signals, sockets, and a shared
file system. Porting these applications to the web currently requires extensive
rewriting or hosting significant portions of code server-side because browsers
present a nontraditional runtime environment that lacks OS functionality.
  This paper presents Browsix, a framework that bridges the considerable gap
between conventional operating systems and the browser, enabling unmodified
programs expecting a Unix-like environment to run directly in the browser.
Browsix comprises two core parts: (1) a JavaScript-only system that makes core
Unix features (including pipes, concurrent processes, signals, sockets, and a
shared file system) available to web applications; and (2) extended JavaScript
runtimes for C, C++, Go, and Node.js that support running programs written in
these languages as processes in the browser. Browsix supports running a POSIX
shell, making it straightforward to connect applications together via pipes.
  We illustrate Browsix's capabilities via case studies that demonstrate how it
eases porting legacy applications to the browser and enables new functionality.
We demonstrate a Browsix-enabled LaTeX editor that operates by executing
unmodified versions of pdfLaTeX and BibTeX. This browser-only LaTeX editor can
render documents in seconds, making it fast enough to be practical. We further
demonstrate how Browsix lets us port a client-server application to run
entirely in the browser for disconnected operation. Creating these applications
required less than 50 lines of glue code and no code modifications,
demonstrating how easily Browsix can be used to build sophisticated web
applications from existing parts without modification.
</summary>
    <author>
      <name>Bobby Powers</name>
    </author>
    <author>
      <name>John Vilk</name>
    </author>
    <author>
      <name>Emery D. Berger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3037697.3037727</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3037697.3037727" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final version published at
  https://dl.acm.org/citation.cfm?doid=3037697.3037727</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ASPLOS 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.07862v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07862v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02588v1</id>
    <updated>2017-02-08T19:21:13Z</updated>
    <published>2017-02-08T19:21:13Z</published>
    <title>Flashield: a Key-value Cache that Minimizes Writes to Flash</title>
    <summary>  As its price per bit drops, SSD is increasingly becoming the default storage
medium for cloud application databases. However, it has not become the
preferred storage medium for key-value caches, even though SSD offers more than
10x lower price per bit and sufficient performance compared to DRAM. This is
because key-value caches need to frequently insert, update and evict small
objects. This causes excessive writes and erasures on flash storage, since
flash only supports writes and erasures of large chunks of data. These
excessive writes and erasures significantly shorten the lifetime of flash,
rendering it impractical to use for key-value caches. We present Flashield, a
hybrid key-value cache that uses DRAM as a "filter" to minimize writes to SSD.
Flashield performs light-weight machine learning profiling to predict which
objects are likely to be read frequently before getting updated; these objects,
which are prime candidates to be stored on SSD, are written to SSD in large
chunks sequentially. In order to efficiently utilize the cache's available
memory, we design a novel in-memory index for the variable-sized objects stored
on flash that requires only 4 bytes per object in DRAM. We describe Flashield's
design and implementation and, we evaluate it on a real-world cache trace.
Compared to state-of-the-art systems that suffer a write amplification of 2.5x
or more, Flashield maintains a median write amplification of 0.5x without any
loss of hit rate or throughput.
</summary>
    <author>
      <name>Assaf Eisenman</name>
    </author>
    <author>
      <name>Asaf Cidon</name>
    </author>
    <author>
      <name>Evgenya Pergament</name>
    </author>
    <author>
      <name>Or Haimovich</name>
    </author>
    <author>
      <name>Ryan Stutsman</name>
    </author>
    <author>
      <name>Mohammad Alizadeh</name>
    </author>
    <author>
      <name>Sachin Katti</name>
    </author>
    <link href="http://arxiv.org/abs/1702.02588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08876v1</id>
    <updated>2017-04-28T11:17:51Z</updated>
    <published>2017-04-28T11:17:51Z</published>
    <title>Mixed-criticality Scheduling with Dynamic Redistribution of Shared Cache</title>
    <summary>  The design of mixed-criticality systems often involvespainful tradeoffs
between safety guarantees and performance.However, the use of more detailed
architectural modelsin the design and analysis of scheduling arrangements for
mixedcriticalitysystems can provide greater confidence in the analysis,but also
opportunities for better performance. Motivated by thisview, we propose an
extension of Vestal 19s model for mixedcriticalitymulticore systems that (i)
accounts for the per-taskpartitioning of the last-level cache and (ii) supports
the dynamicreassignment, for better schedulability, of cache portions
initiallyreserved for lower-criticality tasks to the higher-criticalitytasks,
when the system switches to high-criticality mode. Tothis model, we apply
partitioned EDF scheduling with Ekbergand Yi 19s deadline-scaling technique.
Our schedulability analysisand scalefactor calculation is cognisant of the
cache resourcesassigned to each task, by using WCET estimates that take
intoaccount these resources. It is hence able to leverage the
dynamicreconfiguration of the cache partitioning, at mode change, forbetter
performance, in terms of provable schedulability. We alsopropose heuristics for
partitioning the cache in low- and highcriticalitymode, that promote
schedulability. Our experimentswith synthetic task sets, indicate tangible
improvements inschedulability compared to a baseline cache-aware
arrangementwhere there is no redistribution of cache resources from low-
tohigh-criticality tasks in the event of a mode change.
</summary>
    <author>
      <name>Muhammad Ali Awan</name>
    </author>
    <author>
      <name>Konstantinos Bletsas</name>
    </author>
    <author>
      <name>Pedro F. Souto</name>
    </author>
    <author>
      <name>Benny Akesson</name>
    </author>
    <author>
      <name>Eduardo Tovar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECRTS 2017, 26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03591v1</id>
    <updated>2017-05-10T02:30:06Z</updated>
    <published>2017-05-10T02:30:06Z</published>
    <title>IOTune: A G-states Driver for Elastic Performance of Block Storage</title>
    <summary>  Imagining a disk which provides baseline performance at a relatively low
price during low-load periods, but when workloads demand more resources, the
disk performance is automatically promoted in situ and in real time. In a
hardware era, this is hardly achievable. However, this imagined disk is
becoming reality due to the technical advances of software-defined storage,
which enable volume performance to be adjusted on the fly. We propose IOTune, a
resource management middleware which employs software-defined storage
primitives to implement G-states of virtual block devices. G-states enable
virtual block devices to serve at multiple performance gears, getting rid of
conflicts between immutable resource reservation and dynamic resource demands,
and always achieving resource right-provisioning for workloads. Accompanying
G-states, we also propose a new block storage pricing policy for cloud
providers. Our case study for applying G-states to cloud block storage verifies
the effectiveness of the IOTune framework. Trace-replay based evaluations
demonstrate that storage volumes with G-states adapt to workload fluctuations.
For tenants, G-states enable volumes to provide much better QoS with a same
cost of ownership, comparing with static IOPS provisioning and the I/O credit
mechanism. G-states also reduce I/O tail latencies by one to two orders of
magnitude. From the standpoint of cloud providers, G-states promote storage
utilization, creating values and benefiting competitiveness. G-states supported
by IOTune provide a new paradigm for storage resource management and pricing in
multi-tenant clouds.
</summary>
    <author>
      <name>Tao Lu</name>
    </author>
    <author>
      <name>Ping Huang</name>
    </author>
    <author>
      <name>Xubin He</name>
    </author>
    <author>
      <name>Matthew Welch</name>
    </author>
    <author>
      <name>Steven Gonzales</name>
    </author>
    <author>
      <name>Ming Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.03591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.06965v2</id>
    <updated>2017-05-24T20:48:00Z</updated>
    <published>2017-05-19T12:48:50Z</published>
    <title>GPU System Calls</title>
    <summary>  GPUs are becoming first-class compute citizens and are being tasked to
perform increasingly complex work. Modern GPUs increasingly support
programmability- enhancing features such as shared virtual memory and hardware
cache coherence, enabling them to run a wider variety of programs. But a key
aspect of general-purpose programming where GPUs are still found lacking is the
ability to invoke system calls. We explore how to directly invoke generic
system calls in GPU programs. We examine how system calls should be meshed with
prevailing GPGPU programming models where thousands of threads are organized in
a hierarchy of execution groups: Should a system call be invoked at the
individual GPU task, or at different execution group levels? What are
reasonable ordering semantics for GPU system calls across these hierarchy of
execution groups? To study these questions, we implemented GENESYS -- a
mechanism to allow GPU pro- grams to invoke system calls in the Linux operating
system. Numerous subtle changes to Linux were necessary, as the existing kernel
assumes that only CPUs invoke system calls. We analyze the performance of
GENESYS using micro-benchmarks and three applications that exercise the
filesystem, networking, and memory allocation subsystems of the kernel. We
conclude by analyzing the suitability of all of Linux's system calls for the
GPU.
</summary>
    <author>
      <name>Ján Veselý</name>
    </author>
    <author>
      <name>Arkaprava Basu</name>
    </author>
    <author>
      <name>Abhishek Bhattacharjee</name>
    </author>
    <author>
      <name>Gabriel Loh</name>
    </author>
    <author>
      <name>Mark Oskin</name>
    </author>
    <author>
      <name>Steven K. Reinhardt</name>
    </author>
    <link href="http://arxiv.org/abs/1705.06965v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.06965v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.4; D.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07161v1</id>
    <updated>2017-07-22T13:29:01Z</updated>
    <published>2017-07-22T13:29:01Z</published>
    <title>Optimizations of Management Algorithms for Multi-Level Memory Hierarchy</title>
    <summary>  In the near future the SCM is predicted to modify the form of new programs,
the access form to storage, and the way that storage devices themselves are
built. Therefore, a combination between the SCM and a designated Memory
Allocation Manager (MAM) that will allow the programmer to manually control the
different memories in the memory hierarchy will be likely to achieve a new
level of performance for memory-aware data structures. Although the manual MAM
seems to be the optimal approach for multi-level memory hierarchy management,
this technique is still very far from being realistic, and the chances that it
would be implemented in current codes using High Performance Computing (HPC)
platforms is quite low. This premise means that the most reasonable way to
introduce the SCM into any usable and popular memory system would be by
implementing an automated version of the MAM using the fundamentals of paging
algorithms, as used for two-level memory hierarchy. Our hypothesis is that
achieving appropriate transferability between memory levels may be possible
using ideas of algorithms employed in current virtual memory systems, and that
the adaptation of those algorithms from a two-level memory hierarchy to an
N-level memory hierarchy is possible. In order to reach the conclusion that our
hypothesis is correct, we investigated various paging algorithms, and found the
ones that could be adapted successfully from two-level memory hierarchy to an
N-level memory hierarchy. We discovered that using an adaptation of the Aging
paging algorithm to an N-level memory hierarchy results in the best
performances in terms of Hit/Miss ratio. In order to verify our hypothesis we
build a simulator called "DeMemory simulator" for analyzing our algorithms as
well as for other algorithms that will be devised in the future.
</summary>
    <author>
      <name>Gal Oren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Master's Thesis, Diss. The Open University (2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.07161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.07880v1</id>
    <updated>2018-01-24T07:11:22Z</updated>
    <published>2018-01-24T07:11:22Z</published>
    <title>vLibOS: Babysitting OS Evolution with a Virtualized Library OS</title>
    <summary>  Many applications have service requirements that are not easily met by
existing operating systems. Real-time and security-critical tasks, for example,
often require custom OSes to meet their needs. However, development of special
purpose OSes is a time-consuming and difficult exercise. Drivers, libraries and
applications have to be written from scratch or ported from existing sources.
Many researchers have tackled this problem by developing ways to extend
existing systems with application-specific services. However, it is often
difficult to ensure an adequate degree of separation between legacy and new
services, especially when security and timing requirements are at stake.
Virtualization, for example, supports logical isolation of separate guest
services, but suffers from inadequate temporal isolation of time-critical code
required for real-time systems. This paper presents vLibOS, a master-slave
paradigm for new systems, whose services are built on legacy code that is
temporally and spatially isolated in separate VM domains. Existing OSes are
treated as sandboxed libraries, providing legacy services that are requested by
inter-VM calls, which execute with the time budget of the caller. We evaluate a
real-time implementation of vLibOS. Empirical results show that vLibOS achieves
as much as a 50\% reduction in performance slowdown for real-time threads, when
competing for a shared memory bus with a Linux VM.
</summary>
    <author>
      <name>Ying Ye</name>
    </author>
    <author>
      <name>Zhuoqun Cheng</name>
    </author>
    <author>
      <name>Soham Sinha</name>
    </author>
    <author>
      <name>Richard West</name>
    </author>
    <link href="http://arxiv.org/abs/1801.07880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.07880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.11265v1</id>
    <updated>2018-04-30T15:08:54Z</updated>
    <published>2018-04-30T15:08:54Z</published>
    <title>Mosaic: An Application-Transparent Hardware-Software Cooperative Memory
  Manager for GPUs</title>
    <summary>  Modern GPUs face a trade-off on how the page size used for memory management
affects address translation and demand paging. Support for multiple page sizes
can help relax the page size trade-off so that address translation and demand
paging optimizations work together synergistically. However, existing page
coalescing and splintering policies require costly base page migrations that
undermine the benefits multiple page sizes provide. In this paper, we observe
that GPGPU applications present an opportunity to support multiple page sizes
without costly data migration, as the applications perform most of their memory
allocation en masse (i.e., they allocate a large number of base pages at once).
We show that this en masse allocation allows us to create intelligent memory
allocation policies which ensure that base pages that are contiguous in virtual
memory are allocated to contiguous physical memory pages. As a result,
coalescing and splintering operations no longer need to migrate base pages.
  We introduce Mosaic, a GPU memory manager that provides
application-transparent support for multiple page sizes. Mosaic uses base pages
to transfer data over the system I/O bus, and allocates physical memory in a
way that (1) preserves base page contiguity and (2) ensures that a large page
frame contains pages from only a single memory protection domain. This
mechanism allows the TLB to use large pages, reducing address translation
overhead. During data transfer, this mechanism enables the GPU to transfer only
the base pages that are needed by the application over the system I/O bus,
keeping demand paging overhead low.
</summary>
    <author>
      <name>Rachata Ausavarungnirun</name>
    </author>
    <author>
      <name>Joshua Landgraf</name>
    </author>
    <author>
      <name>Vance Miller</name>
    </author>
    <author>
      <name>Saugata Ghose</name>
    </author>
    <author>
      <name>Jayneel Gandhi</name>
    </author>
    <author>
      <name>Christopher J. Rossbach</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <link href="http://arxiv.org/abs/1804.11265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.11265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01147v1</id>
    <updated>2018-06-04T14:29:41Z</updated>
    <published>2018-06-04T14:29:41Z</published>
    <title>Minimizing Event-Handling Latencies in Secure Virtual Machines</title>
    <summary>  Virtualization, after having found widespread adoption in the server and
desktop arena, is poised to change the architecture of embedded systems as
well. The benefits afforded by virtualization - enhanced isolation,
manageability, flexibility, and security - could be instrumental for developers
of embedded systems as an answer to the rampant increase in complexity.
  While mature desktop and server solutions exist, they cannot be easily reused
on embedded systems because of markedly different requirements. Unfortunately,
optimizations aimed at throughput, important for servers, often compromise on
aspects like predictable real-time behavior, which are crucial to many embedded
systems. In a similar vein, the requirements for small trusted computing bases,
lightweight inter-VM communication, and small footprints are often not
accommodated. This observation suggests that virtual machines for embedded
systems should be constructed from scratch with particular attention paid to
the specific requirements.
  In this paper, we set out with a virtual machine designed for
security-conscious workloads and describe the steps necessary to achieve good
event-handling latencies. That evolution is possible because the underlying
microkernel is well suited to satisfy real-time requirements. As the guest
system we chose Linux with the PREEMPT_RT configuration, which itself was
developed in an effort to bring down event-handling latencies in a general
purpose system. Our results indicate that the increase of event-handling
latencies of a guest running in a virtual machine does not, compared to native
execution, exceed a factor of two.
</summary>
    <author>
      <name>Janis Danisevskis</name>
    </author>
    <author>
      <name>Michael Peter</name>
    </author>
    <author>
      <name>Jan Nordholz</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07480v1</id>
    <updated>2018-06-19T21:59:59Z</updated>
    <published>2018-06-19T21:59:59Z</published>
    <title>LazyFP: Leaking FPU Register State using Microarchitectural
  Side-Channels</title>
    <summary>  Modern processors utilize an increasingly large register set to facilitate
efficient floating point and SIMD computation. This large register set is a
burden for operating systems, as its content needs to be saved and restored
when the operating system context switches between tasks. As an optimization,
the operating system can defer the context switch of the FPU and SIMD register
set until the first instruction is executed that needs access to these
registers. Meanwhile, the old content is left in place with the hope that the
current task might not use these registers at all. This optimization is
commonly called lazy FPU context switching. To make it possible, a processor
offers the ability to toggle the availability of instructions utilizing
floating point and SIMD registers. If the instructions are turned off, any
attempt of executing them will generate a fault.
  In this paper, we present an attack that exploits lazy FPU context switching
and allows an adversary to recover the FPU and SIMD register set of arbitrary
processes or VMs. The attack works on processors that transiently execute FPU
or SIMD instructions that follow an instruction generating the fault indicating
the first use of FPU or SIMD instructions. On operating systems using lazy FPU
context switching, the FPU and SIMD register content of other processes or
virtual machines can then be reconstructed via cache side effects.
  With SIMD registers not only being used for cryptographic computation, but
also increasingly for simple operations, such as copying memory, we argue that
lazy FPU context switching is a dangerous optimization that needs to be turned
off in all operating systems, if there is a chance that they run on affected
processors.
</summary>
    <author>
      <name>Julian Stecklina</name>
    </author>
    <author>
      <name>Thomas Prescher</name>
    </author>
    <link href="http://arxiv.org/abs/1806.07480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10097v2</id>
    <updated>2019-01-03T20:36:43Z</updated>
    <published>2018-08-30T03:09:40Z</published>
    <title>Profiling and Improving the Duty-Cycling Performance of Linux-based IoT
  Devices</title>
    <summary>  Minimizing the energy consumption of Linux-based devices is an essential step
towards their wide deployment in various IoT scenarios. Energy saving methods
such as duty-cycling aim to address this constraint by limiting the amount of
time the device is powered on. In this work we study and improve the amount of
time a Linux-based IoT device is powered on to accomplish its tasks. We analyze
the processes of system boot up and shutdown on two platforms, the Raspberry Pi
3 and Raspberry Pi Zero Wireless, and enhance duty-cycling performance by
identifying and disabling time-consuming or unnecessary units initialized in
the userspace. We also study whether SD card speed and SD card capacity
utilization affect boot up duration and energy consumption. In addition, we
propose 'Pallex', a parallel execution framework built on top of the 'systemd
init' system to run a user application concurrently with userspace
initialization. We validate the performance impact of Pallex when applied to
various IoT application scenarios: (i) capturing an image, (ii) capturing and
encrypting an image, (iii) capturing and classifying an image using the the
k-nearest neighbor algorithm, and (iv) capturing images and sending them to a
cloud server. Our results show that system lifetime is increased by 18.3%,
16.8%, 13.9% and 30.2%, for these application scenarios, respectively.
</summary>
    <author>
      <name>Immanuel Amirtharaj</name>
    </author>
    <author>
      <name>Tai Groot</name>
    </author>
    <author>
      <name>Behnam Dezfouli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SCU's IoT Research Lab</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.10097v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10097v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.01139v1</id>
    <updated>2018-10-02T09:44:32Z</updated>
    <published>2018-10-02T09:44:32Z</published>
    <title>Platform-Agnostic Steal-Time Measurement in a Guest Operating System</title>
    <summary>  Steal time is a key performance metric for applications executed in a
virtualized environment. Steal time measures the amount of time the processor
is preempted by code outside the virtualized environment. This, in turn, allows
to compute accurately the execution time of an application inside a virtual
machine (i.e. it eliminates the time the virtual machine is suspended).
Unfortunately, this metric is only available in particular scenarios in which
the host and the guest OS are tightly coupled. Typical examples are the Xen
hypervisor and Linux-based guest OSes. In contrast, in scenarios where the
steal time is not available inside the virtualized environment, performance
measurements are, most often, incorrect.
  In this paper, we introduce a novel and platform agnostic approach to
calculate this steal time within the virtualized environment and without the
cooperation of the host OS. The theoretical execution time of a deterministic
microbenchmark is compared to its execution time in a virtualized environment.
When factoring in the virtual machine load, this solution -as simple as it is-
can compute the steal time. The preliminary results show that we are able to
compute the load of the physical processor within the virtual machine with high
accuracy.
</summary>
    <author>
      <name>Javier Verdu</name>
    </author>
    <author>
      <name>Juan Jose Costa</name>
    </author>
    <author>
      <name>Beatriz Otero</name>
    </author>
    <author>
      <name>Eva Rodriguez</name>
    </author>
    <author>
      <name>Alex Pajuelo</name>
    </author>
    <author>
      <name>Ramon Canal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures, technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.01139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.01139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.01553v3</id>
    <updated>2019-07-10T15:52:16Z</updated>
    <published>2018-10-03T00:56:10Z</published>
    <title>BRAVO -- Biased Locking for Reader-Writer Locks</title>
    <summary>  Designers of modern reader-writer locks confront a difficult trade-off
related to reader scalability. Locks that have a compact memory representation
for active readers will typically suffer under high intensity read-dominated
workloads when the "reader indicator"' state is updated frequently by a diverse
set of threads, causing cache invalidation and coherence traffic. Other
designs, such as cohort reader-writer locks, use distributed reader indicators,
one per NUMA node. This improves reader-reader scalability, but also increases
the size of each lock instance. We propose a simple transformation BRAVO, that
augments any existing reader-writer lock, adding just two integer fields to the
lock instance. Readers make their presence known to writers by hashing their
thread's identity with the lock address, forming an index into a visible
readers table. Readers attempt to install the lock address into that element in
the table, making their existence known to potential writers. All locks and
threads in an address space can share the visible readers table. Updates by
readers tend to be diffused over the table, resulting in a NUMA-friendly
design. Crucially, readers of the same lock tend to write to different
locations in the array, reducing coherence traffic. Specifically, BRAVO allows
a simple compact lock to be augmented so as to provide scalable concurrent
reading but with only a modest increase in footprint.
</summary>
    <author>
      <name>David Dice</name>
    </author>
    <author>
      <name>Alex Kogan</name>
    </author>
    <link href="http://arxiv.org/abs/1810.01553v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.01553v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.01573v4</id>
    <updated>2019-07-10T21:59:00Z</updated>
    <published>2018-10-03T03:27:51Z</published>
    <title>TWA -- Ticket Locks Augmented with a Waiting Array</title>
    <summary>  The classic ticket lock consists of ticket and grant fields. Arriving threads
atomically fetch-and-increment ticket and then wait for grant to become equal
to the value returned by the fetch-and-increment primitive, at which point the
thread holds the lock. The corresponding unlock operation simply increments
grant. This simple design has short code paths and fast handover (transfer of
ownership) under light contention, but may suffer degraded scalability under
high contention when multiple threads busy wait on the grant field -- so-called
global spinning. We propose a variation on ticket locks where long-term waiting
threads wait on locations in a waiting array instead of busy waiting on the
grant field. The single waiting array is shared among all locks. Short-term
waiting is accomplished in the usual manner on the grant field. The resulting
algorithm, TWA, improves on ticket locks by limiting the number of threads
spinning on the grant field at any given time, reducing the number of remote
caches requiring invalidation from the store that releases the lock. In turn,
this accelerates handover, and since the lock is held throughout the handover
operation, scalability improves. Under light or no contention, TWA yields
performance comparable to the classic ticket lock, avoiding the complexity and
extra accesses incurred by MCS locks in the handover path, but providing
performance above or beyond that of MCS at high contention.
</summary>
    <author>
      <name>Dave Dice</name>
    </author>
    <author>
      <name>Alex Kogan</name>
    </author>
    <link href="http://arxiv.org/abs/1810.01573v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.01573v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.02904v1</id>
    <updated>2018-10-05T23:04:23Z</updated>
    <published>2018-10-05T23:04:23Z</published>
    <title>Finding Crash-Consistency Bugs with Bounded Black-Box Crash Testing</title>
    <summary>  We present a new approach to testing file-system crash consistency: bounded
black-box crash testing (B3). B3 tests the file system in a black-box manner
using workloads of file-system operations. Since the space of possible
workloads is infinite, B3 bounds this space based on parameters such as the
number of file-system operations or which operations to include, and
exhaustively generates workloads within this bounded space. Each workload is
tested on the target file system by simulating power-loss crashes while the
workload is being executed, and checking if the file system recovers to a
correct state after each crash. B3 builds upon insights derived from our study
of crash-consistency bugs reported in Linux file systems in the last five
years. We observed that most reported bugs can be reproduced using small
workloads of three or fewer file-system operations on a newly-created file
system, and that all reported bugs result from crashes after fsync() related
system calls. We build two tools, CrashMonkey and ACE, to demonstrate the
effectiveness of this approach. Our tools are able to find 24 out of the 26
crash-consistency bugs reported in the last five years. Our tools also revealed
10 new crash-consistency bugs in widely-used, mature Linux file systems, seven
of which existed in the kernel since 2014. Our tools also found a
crash-consistency bug in a verified file system, FSCQ. The new bugs result in
severe consequences like broken rename atomicity and loss of persisted files.
</summary>
    <author>
      <name>Jayashree Mohan</name>
    </author>
    <author>
      <name>Ashlie Martinez</name>
    </author>
    <author>
      <name>Soujanya Ponnapalli</name>
    </author>
    <author>
      <name>Pandian Raju</name>
    </author>
    <author>
      <name>Vijay Chidambaram</name>
    </author>
    <link href="http://arxiv.org/abs/1810.02904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.02904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.05600v2</id>
    <updated>2019-03-01T05:13:22Z</updated>
    <published>2018-10-12T16:42:49Z</published>
    <title>Compact NUMA-Aware Locks</title>
    <summary>  Modern multi-socket architectures exhibit non-uniform memory access (NUMA)
behavior, where access by a core to data cached locally on a socket is much
faster than access to data cached on a remote socket. Prior work offers several
efficient NUMA-aware locks that exploit this behavior by keeping the lock
ownership on the same socket, thus reducing remote cache misses and
inter-socket communication. Virtually all those locks, however, are
hierarchical in their nature, thus requiring space proportional to the number
of sockets. The increased memory cost renders NUMA-aware locks unsuitable for
systems that are conscious to space requirements of their synchronization
constructs, with the Linux kernel being the chief example.
  In this work, we present a compact NUMA-aware lock that requires only one
word of memory, regardless of the number of sockets in the underlying machine.
The new lock is a variant of an efficient (NUMA-oblivious) MCS lock, and
inherits its performant features, such as local spinning and a single atomic
instruction in the acquisition path. Unlike MCS, the new lock organizes waiting
threads in two queues, one composed of threads running on the same socket as
the current lock holder, and another composed of threads running on a different
socket(s).
  We integrated the new lock in the Linux kernel's qspinlock, one of the major
synchronization constructs in the kernel. Our evaluation using both user-space
and kernel benchmarks shows that the new lock has a single-thread performance
of MCS, but significantly outperforms the latter under contention, achieving a
similar level of performance when compared to other, state-of-the-art
NUMA-aware locks that require substantially more space.
</summary>
    <author>
      <name>Dave Dice</name>
    </author>
    <author>
      <name>Alex Kogan</name>
    </author>
    <link href="http://arxiv.org/abs/1810.05600v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.05600v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.1; D.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.05000v3</id>
    <updated>2019-06-05T19:27:33Z</updated>
    <published>2018-11-12T21:00:23Z</published>
    <title>Transkernel: Bridging Monolithic Kernels to Peripheral Cores</title>
    <summary>  Smart devices see a large number of ephemeral tasks driven by background
activities. In order to execute such a task, the OS kernel wakes up the
platform beforehand and puts it back to sleep afterwards. In doing so, the
kernel operates various IO devices and orchestrates their power state
transitions. Such kernel executions are inefficient as they mismatch typical
CPU hardware. They are better off running on a low-power, microcontroller-like
core, i.e., peripheral core, relieving CPU from the inefficiency.
  We therefore present a new OS structure, in which a lightweight virtual
executor called transkernel offloads specific phases from a monolithic kernel.
The transkernel translates stateful kernel execution through cross-ISA, dynamic
binary translation (DBT); it emulates a small set of stateless kernel services
behind a narrow, stable binary interface; it specializes for hot paths; it
exploits ISA similarities for lowering DBT cost.
  Through an ARM-based prototype, we demonstrate transkernel's feasibility and
benefit. We show that while cross-ISA DBT is typically used under the
assumption of efficiency loss, it can enable efficiency gain, even on
off-the-shelf hardware.
</summary>
    <author>
      <name>Liwei Guo</name>
    </author>
    <author>
      <name>Shuang Zhai</name>
    </author>
    <author>
      <name>Yi Qiao</name>
    </author>
    <author>
      <name>Felix Xiaozhu Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The camera-ready version of this paper, will appear at USENIX ATC'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.05000v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.05000v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.00825v1</id>
    <updated>2019-01-03T17:27:14Z</updated>
    <published>2019-01-03T17:27:14Z</published>
    <title>XOS: An Application-Defined Operating System for Data Center Servers</title>
    <summary>  Rapid growth of datacenter (DC) scale, urgency of cost control, increasing
workload diversity, and huge software investment protection place unprecedented
demands on the operating system (OS) efficiency, scalability, performance
isolation, and backward-compatibility. The traditional OSes are not built to
work with deep-hierarchy software stacks, large numbers of cores, tail latency
guarantee, and increasingly rich variety of applications seen in modern DCs,
and thus they struggle to meet the demands of such workloads.
  This paper presents XOS, an application-defined OS for modern DC servers. Our
design moves resource management out of the OS kernel, supports customizable
kernel subsystems in user space, and enables elastic partitioning of hardware
resources. Specifically, XOS leverages modern hardware support for
virtualization to move resource management functionality out of the
conventional kernel and into user space, which lets applications achieve near
bare-metal performance. We implement XOS on top of Linux to provide backward
compatibility. XOS speeds up a set of DC workloads by up to 1.6X over our
baseline Linux on a 24-core server, and outperforms the state-of-the-art Dune
by up to 3.3X in terms of virtual memory management. In addition, XOS
demonstrates good scalability and strong performance isolation.
</summary>
    <author>
      <name>Chen Zheng</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Sally A. McKee</name>
    </author>
    <author>
      <name>Lixin Zhang</name>
    </author>
    <author>
      <name>Hainan Ye</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in IEEE BigData 2018. 10 pages, 6 figures, 3
  tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.00825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.00825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.0; D.4.7; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.01222v1</id>
    <updated>2019-01-04T17:31:56Z</updated>
    <published>2019-01-04T17:31:56Z</published>
    <title>Efficient, Dynamic Multi-tenant Edge Computation in EdgeOS</title>
    <summary>  In the future, computing will be immersed in the world around us -- from
augmented reality to autonomous vehicles to the Internet of Things. Many of
these smart devices will offer services that respond in real time to their
physical surroundings, requiring complex processing with strict performance
guarantees. Edge clouds promise a pervasive computational infrastructure a
short network hop away from end devices, but today's operating systems are a
poor fit to meet the goals of scalable isolation, dense multi-tenancy, and
predictable performance required by these emerging applications. In this paper
we present EdgeOS, a micro-kernel based operating system that meets these goals
by blending recent advances in real-time systems and network function
virtualization. EdgeOS introduces a Featherweight Process model that offers
lightweight isolation and supports extreme scalability even under high churn.
Our architecture provides efficient communication mechanisms, and low-overhead
per-client isolation. To achieve high performance networking, EdgeOS employs
kernel bypass paired with the isolation properties of Featherweight Processes.
We have evaluated our EdgeOS prototype for running high scale network
middleboxes using the Click software router and endpoint applications using
memcached. EdgeOS reduces startup latency by 170X compared to Linux processes
and over five orders of magnitude compared to containers, while providing three
orders of magnitude latency improvement when running 300 to 1000 edge-cloud
memcached instances on one server.
</summary>
    <author>
      <name>Yuxin Ren</name>
    </author>
    <author>
      <name>Vlad Nitu</name>
    </author>
    <author>
      <name>Guyue Liu</name>
    </author>
    <author>
      <name>Gabriel Parmer</name>
    </author>
    <author>
      <name>Timothy Wood</name>
    </author>
    <author>
      <name>Alain Tchana</name>
    </author>
    <author>
      <name>Riley Kennedy</name>
    </author>
    <link href="http://arxiv.org/abs/1901.01222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.01222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.02450v1</id>
    <updated>2019-01-08T08:37:23Z</updated>
    <published>2019-01-08T08:37:23Z</published>
    <title>A C-DAG task model for scheduling complex real-time tasks on
  heterogeneous platforms: preemption matters</title>
    <summary>  Recent commercial hardware platforms for embedded real-time systems feature
heterogeneous processing units and computing accelerators on the same
System-on-Chip. When designing complex real-time application for such
architectures, the designer needs to make a number of difficult choices: on
which processor should a certain task be implemented? Should a component be
implemented in parallel or sequentially? These choices may have a great impact
on feasibility, as the difference in the processor internal architectures
impact on the tasks' execution time and preemption cost. To help the designer
explore the wide space of design choices and tune the scheduling parameters, in
this paper we propose a novel real-time application model, called C-DAG,
specifically conceived for heterogeneous platforms. A C-DAG allows to specify
alternative implementations of the same component of an application for
different processing engines to be selected off-line, as well as conditional
branches to model if-then-else statements to be selected at run-time. We also
propose a schedulability analysis for the C-DAG model and a heuristic
allocation algorithm so that all deadlines are respected. Our analysis takes
into account the cost of preempting a task, which can be non-negligible on
certain processors. We demonstrate the effectiveness of our approach on a large
set of synthetic experiments by comparing with state of the art algorithms in
the literature.
</summary>
    <author>
      <name>Houssam-Eddine Zahaf</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UNIMORE</arxiv:affiliation>
    </author>
    <author>
      <name>Nicola Capodieci</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UNIMORE</arxiv:affiliation>
    </author>
    <author>
      <name>Roberto Cavicchioli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UNIMORE</arxiv:affiliation>
    </author>
    <author>
      <name>Marko Bertogna</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UNIMORE</arxiv:affiliation>
    </author>
    <author>
      <name>Giuseppe Lipari</name>
    </author>
    <link href="http://arxiv.org/abs/1901.02450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.02450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.07732v1</id>
    <updated>2019-01-23T05:41:42Z</updated>
    <published>2019-01-23T05:41:42Z</published>
    <title>PINPOINT: Efficient and Effective Resource Isolation for Mobile Security
  and Privacy</title>
    <summary>  Virtualization is frequently used to isolate untrusted processes and control
their access to sensitive resources. However, isolation usually carries a price
in terms of less resource sharing and reduced inter-process communication. In
an open architecture such as Android, this price and its impact on performance,
usability, and transparency must be carefully considered. Although previous
efforts in developing general-purpose isolation solutions have shown that some
of these negative side effects can be mitigated, doing so involves overcoming
significant design challenges by incorporating numerous additional platform
complexities not directly related to improved security. Thus, the general
purpose solutions become inefficient and burdensome if the end-user has only
specific security goals. In this paper, we present PINPOINT, a resource
isolation strategy that forgoes general-purpose solutions in favor of a
"building block" approach that addresses specific end-user security goals.
PINPOINT embodies the concept of Linux Namespace lightweight isolation, but
does so in the Android Framework by guiding the security designer towards
isolation points that are contextually close to the resource(s) that need to be
isolated. This strategy allows the rest of the Framework to function fully as
intended, transparently. We demonstrate our strategy with a case study on
Android System Services, and show four applications of PINPOINTed system
services functioning with unmodified market apps. Our evaluation results show
that practical security and privacy advantages can be gained using our
approach, without inducing the problematic side-effects that other
general-purpose designs must address.
</summary>
    <author>
      <name>Paul Ratazzi</name>
    </author>
    <author>
      <name>Ashok Bommisetti</name>
    </author>
    <author>
      <name>Nian Ji</name>
    </author>
    <author>
      <name>Wenliang Du</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Mobile Security Technologies (MoST) Workshop, May 21, 2015,
  http://www.ieee-security.org/TC/SPW2015/MoST/</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.07732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.07732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.09347v1</id>
    <updated>2019-03-22T03:46:22Z</updated>
    <published>2019-03-22T03:46:22Z</published>
    <title>Understanding and taming SSD read performance variability: HDFS case
  study</title>
    <summary>  In this paper we analyze the influence that lower layers (file system, OS,
SSD) have on HDFS' ability to extract maximum performance from SSDs on the read
path. We uncover and analyze three surprising performance slowdowns induced by
lower layers that result in HDFS read throughput loss. First, intrinsic
slowdown affects reads from every new file system extent for a variable amount
of time. Second, temporal slowdown appears temporarily and periodically and is
workload-agnostic. Third, in permanent slowdown, some files can individually
and permanently become slower after a period of time. We analyze the impact of
these slowdowns on HDFS and show significant throughput loss. Individually,
each of the slowdowns can cause a read throughput loss of 10-15%. However,
their effect is cumulative. When all slowdowns happen concurrently, read
throughput drops by as much as 30%. We further analyze mitigation techniques
and show that two of the three slowdowns could be addressed via increased IO
request parallelism in the lower layers. Unfortunately, HDFS cannot
automatically adapt to use such additional parallelism. Our results point to a
need for adaptability in storage stacks. The reason is that an access pattern
that maximizes performance in the common case is not necessarily the same one
that can mask performance fluctuations.
</summary>
    <author>
      <name>María F. Borge</name>
    </author>
    <author>
      <name>Florin Dinu</name>
    </author>
    <author>
      <name>Willy Zwaenepoel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.09347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.09347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.12014v1</id>
    <updated>2019-07-28T05:06:24Z</updated>
    <published>2019-07-28T05:06:24Z</published>
    <title>The Preliminary Evaluation of a Hypervisor-based Virtualization
  Mechanism for Intel Optane DC Persistent Memory Module</title>
    <summary>  Non-volatile memory (NVM) technologies, being accessible in the same manner
as DRAM, are considered indispensable for expanding main memory capacities.
Intel Optane DCPMM is a long-awaited product that drastically increases main
memory capacities. However, a substantial performance gap exists between DRAM
and DCPMM. In our experiments, the read/write latencies of DCPMM were 400% and
407% higher than those of DRAM, respectively. The read/write bandwidths were
37% and 8% of those of DRAM. This performance gap in main memory presents a new
challenge to researchers; we need a new system software technology supporting
emerging hybrid memory architecture. In this paper, we present RAMinate, a
hypervisor-based virtualization mechanism for hybrid memory systems, and a key
technology to address the performance gap in main memory systems. It provides
great flexibility in memory management and maximizes the performance of virtual
machines (VMs) by dynamically optimizing memory mappings. Through experiments,
we confirmed that even though a VM has only 1% of DRAM in its RAM, the
performance degradation of the VM was drastically alleviated by memory mapping
optimization. The elapsed time to finish the build of Linux Kernel in the VM
was 557 seconds, which was only 13% increase from the 100% DRAM case (i.e., 495
seconds). When the optimization mechanism was disabled, the elapsed time
increased to 624 seconds (i.e. 26% increase from the 100% DRAM case).
</summary>
    <author>
      <name>Takahiro Hirofuchi</name>
    </author>
    <author>
      <name>Ryousei Takano</name>
    </author>
    <link href="http://arxiv.org/abs/1907.12014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.12014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4; B.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01167v1</id>
    <updated>2019-08-03T13:16:51Z</updated>
    <published>2019-08-03T13:16:51Z</published>
    <title>An Optimized Disk Scheduling Algorithm With Bad-Sector Management</title>
    <summary>  In high performance computing, researchers try to optimize the CPU Scheduling
algorithms, for faster and efficient working of computers. But a process needs
both CPU bound and I/O bound for completion of its execution. With
modernization of computers the speed of processor, hard-disk, and I/O devices
increases gradually. Still the data access speed of hard-disk is much less than
the speed of the processor. So when processor receives a data from secondary
memory it executes immediately and again it have to wait for receiving another
data. So the slowness of the hard-disk becomes a bottleneck in the performance
of processor. Researchers try to develop and optimize the traditional disk
scheduling algorithms for faster data transfer to and from secondary data
storage devices. In this paper we try to evolve an optimized scheduling
algorithm by reducing the seek time, the rotational latency, and the data
transfer time in runtime. This algorithm has the feature to manage the
bad-sectors of the hard-disk. It also attempts to reduce power consumption and
heat reduction by minimizing bad sector reading time.
</summary>
    <author>
      <name>Amar Ranjan Dash</name>
    </author>
    <author>
      <name>Sandipta Kumar Sahu</name>
    </author>
    <author>
      <name>B Kewal</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5012/ijcsea.2019.9301</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5012/ijcsea.2019.9301" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 21 figures, 3 table, International Journal of Computer
  Science, Engineering and Applications (IJCSEA)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science, Engineering and
  Applications (IJCSEA), AIRCC, 2019, Vol. 9(3), pp 1-21, DOI
  :10.5012/ijcsea.2019.9301</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1908.01167v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01167v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.06211v2</id>
    <updated>2021-05-31T15:53:06Z</updated>
    <published>2019-08-17T00:24:07Z</published>
    <title>PAStime: Progress-aware Scheduling for Time-critical Computing</title>
    <summary>  Over-estimation of worst-case execution times (WCETs) of real-time tasks
leads to poor resource utilization. In a mixed-criticality system (MCS), the
over-provisioning of CPU time to accommodate the WCETs of highly critical tasks
may lead to degraded service for less critical tasks. In this paper, we present
PAStime, a novel approach to monitor and adapt the runtime progress of highly
time-critical applications, to allow for improved service to lower criticality
tasks. In PAStime, CPU time is allocated to time-critical tasks according to
the delays they experience as they progress through their control flow graphs.
This ensures that as much time as possible is made available to improve the
Quality-of-Service of less critical tasks, while high-criticality tasks are
compensated after their delays.
  In this paper, we integrate PAStime with Adaptive Mixed-criticality (AMC)
scheduling. The LO-mode budget of a high-criticality task is adjusted according
to the delay observed at execution checkpoints. This is the first
implementation of AMC in the scheduling framework Using LITMUS-RT, which is
extended with our PAStime runtime policy and tested with real-time Linux
applications such as object classification and detection. We observe in our
experimental evaluation that AMC-PAStime significantly improves the utilization
of the low-criticality tasks while guaranteeing service to high-criticality
tasks.
</summary>
    <author>
      <name>Soham Sinha</name>
    </author>
    <author>
      <name>Richard West</name>
    </author>
    <author>
      <name>Ahmad Golchin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/LIPIcs.ECRTS.2020.3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/LIPIcs.ECRTS.2020.3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.06211v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.06211v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3; D.4.7; D.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.08707v1</id>
    <updated>2019-08-23T08:00:38Z</updated>
    <published>2019-08-23T08:00:38Z</published>
    <title>A Least-Privilege Memory Protection Model for Modern Hardware</title>
    <summary>  We present a new least-privilege-based model of addressing on which to base
memory management functionality in an OS for modern computers like phones or
server-based accelerators. Existing software assumptions do not account for
heterogeneous cores with different views of the address space, leading to the
related problems of numerous security bugs in memory management code (for
example programming IOMMUs), and an inability of mainstream OSes to securely
manage the complete set of hardware resources on, say, a phone System-on-Chip.
  Our new work is based on a recent formal model of address translation
hardware which views the machine as a configurable network of address spaces.
We refine this to capture existing address translation hardware from modern
SoCs and accelerators at a sufficiently fine granularity to model minimal
rights both to access memory and configure translation hardware. We then build
an executable specification in Haskell, which expresses the model and metadata
structures in terms of partitioned capabilities. Finally, we show a fully
functional implementation of the model in C created by extending the capability
system of the Barrelfish research OS.
  Our evaluation shows that our unoptimized implementation has comparable (and
in some cases) better performance than the Linux virtual memory system, despite
both capturing all the functionality of modern hardware addressing and enabling
least-privilege, decentralized authority to access physical memory and devices.
</summary>
    <author>
      <name>Reto Achermann</name>
    </author>
    <author>
      <name>Nora Hossle</name>
    </author>
    <author>
      <name>Lukas Humbel</name>
    </author>
    <author>
      <name>Daniel Schwyn</name>
    </author>
    <author>
      <name>David Cock</name>
    </author>
    <author>
      <name>Timothy Roscoe</name>
    </author>
    <link href="http://arxiv.org/abs/1908.08707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.08707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.11143v3</id>
    <updated>2020-01-20T12:10:51Z</updated>
    <published>2019-08-29T10:25:37Z</published>
    <title>SGX-LKL: Securing the Host OS Interface for Trusted Execution</title>
    <summary>  Hardware support for trusted execution in modern CPUs enables tenants to
shield their data processing workloads in otherwise untrusted cloud
environments. Runtime systems for the trusted execution must rely on an
interface to the untrusted host OS to use external resources such as storage,
network, and other functions. Attackers may exploit this interface to leak data
or corrupt the computation.
  We describe SGX-LKL, a system for running Linux binaries inside of Intel SGX
enclaves that only exposes a minimal, protected and oblivious host interface:
the interface is (i) minimal because SGX-LKL uses a complete library OS inside
the enclave, including file system and network stacks, which requires a host
interface with only 7 calls; (ii) protected because SGX-LKL transparently
encrypts and integrity-protects all data passed via low-level I/O operations;
and (iii) oblivious because SGX-LKL performs host operations independently of
the application workload. For oblivious disk I/O, SGX-LKL uses an encrypted
ext4 file system with shuffled disk blocks. We show that SGX-LKL protects
TensorFlow training with a 21% overhead.
</summary>
    <author>
      <name>Christian Priebe</name>
    </author>
    <author>
      <name>Divya Muthukumaran</name>
    </author>
    <author>
      <name>Joshua Lind</name>
    </author>
    <author>
      <name>Huanzhou Zhu</name>
    </author>
    <author>
      <name>Shujie Cui</name>
    </author>
    <author>
      <name>Vasily A. Sartakov</name>
    </author>
    <author>
      <name>Peter Pietzuch</name>
    </author>
    <link href="http://arxiv.org/abs/1908.11143v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.11143v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.01642v1</id>
    <updated>2019-10-03T09:28:35Z</updated>
    <published>2019-10-03T09:28:35Z</published>
    <title>APEX: Adaptive Ext4 File System for Enhanced Data Recoverability in Edge
  Devices</title>
    <summary>  Recently Edge Computing paradigm has gained significant popularity both in
industry and academia. With its increased usage in real-life scenarios,
security, privacy and integrity of data in such environments have become
critical. Malicious deletion of mission-critical data due to ransomware,
trojans and viruses has been a huge menace and recovering such lost data is an
active field of research. As most of Edge computing devices have compute and
storage limitations, difficult constraints arise in providing an optimal scheme
for data protection. These devices mostly use Linux/Unix based operating
systems. Hence, this work focuses on extending the Ext4 file system to APEX
(Adaptive Ext4): a file system based on novel on-the-fly learning model that
provides an Adaptive Recover-ability Aware file allocation platform for
efficient post-deletion data recovery and therefore maintaining data integrity.
Our recovery model and its lightweight implementation allow significant
improvement in recover-ability of lost data with lower compute, space, time,
and cost overheads compared to other methods. We demonstrate the effectiveness
of APEX through a case study of overwriting surveillance videos by CryPy
malware on Raspberry-Pi based Edge deployment and show 678% and 32% higher
recovery than Ext4 and current state-of-the-art File Systems. We also evaluate
the overhead characteristics and experimentally show that they are lower than
other related works.
</summary>
    <author>
      <name>Shreshth Tuli</name>
    </author>
    <author>
      <name>Shikhar Tuli</name>
    </author>
    <author>
      <name>Udit Jain</name>
    </author>
    <author>
      <name>Rajkumar Buyya</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 11th IEEE International Conference on Cloud
  Computing, Sydney, Australia, December 11-13, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.01642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.01642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.05398v2</id>
    <updated>2019-11-08T06:36:11Z</updated>
    <published>2019-10-11T20:26:14Z</published>
    <title>Mitosis: Transparently Self-Replicating Page-Tables for Large-Memory
  Machines</title>
    <summary>  Multi-socket machines with 1-100 TBs of physical memory are becoming
prevalent. Applications running on multi-socket machines suffer non-uniform
bandwidth and latency when accessing physical memory. Decades of research have
focused on data allocation and placement policies in NUMA settings, but there
have been no studies on the question of how to place page-tables amongst
sockets. We make the case for explicit page-table allocation policies and show
that page-table placement is becoming crucial to overall performance. We
propose Mitosis to mitigate NUMA effects on page-table walks by transparently
replicating and migrating page-tables across sockets without application
changes. This reduces the frequency of accesses to remote NUMA nodes when
performing page-table walks. Mitosis uses two components: (i) a mechanism to
enable efficient page-table replication and migration; and (ii) policies for
processes to efficiently manage and control page-table replication and
migration. We implement Mitosis in Linux and evaluate its benefits on real
hardware. Mitosis improves performance for large-scale multi-socket workloads
by up to 1.34x by replicating page-tables across sockets. Moreover, it improves
performance by up to 3.24x in cases when the OS migrates a process across
sockets by enabling cross-socket page-table migration.
</summary>
    <author>
      <name>Reto Achermann</name>
    </author>
    <author>
      <name>Ashish Panwar</name>
    </author>
    <author>
      <name>Abhishek Bhattacharjee</name>
    </author>
    <author>
      <name>Timothy Roscoe</name>
    </author>
    <author>
      <name>Jayneel Gandhi</name>
    </author>
    <link href="http://arxiv.org/abs/1910.05398v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.05398v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.06998v1</id>
    <updated>2019-12-15T08:09:31Z</updated>
    <published>2019-12-15T08:09:31Z</published>
    <title>Faster than Flash: An In-Depth Study of System Challenges for Emerging
  Ultra-Low Latency SSDs</title>
    <summary>  Emerging storage systems with new flash exhibit ultra-low latency (ULL) that
can address performance disparities between DRAM and conventional solid state
drives (SSDs) in the memory hierarchy. Considering the advanced low-latency
characteristics, different types of I/O completion methods (polling/hybrid) and
storage stack architecture (SPDK) are proposed. While these new techniques are
expected to take costly software interventions off the critical path in
ULL-applied systems, unfortunately no study exists to quantitatively analyze
system-level characteristics and challenges of combining such newly-introduced
techniques with real ULL SSDs. In this work, we comprehensively perform
empirical evaluations with 800GB ULL SSD prototypes and characterize ULL
behaviors by considering a wide range of I/O path parameters, such as different
queues and access patterns. We then analyze the efficiencies and challenges of
the polled-mode and hybrid polling I/O completion methods (added into Linux
kernels 4.4 and 4.10, respectively) and compare them with the efficiencies of a
conventional interrupt-based I/O path. In addition, we revisit the common
expectations of SPDK by examining all the system resources and parameters.
Finally, we demonstrate the challenges of ULL SSDs in a real SPDK-enabled
server-client system. Based on the performance behaviors that this study
uncovers, we also discuss several system implications, which are required to
take a full advantage of ULL SSD in the future.
</summary>
    <author>
      <name>Sungjoon Koh</name>
    </author>
    <author>
      <name>Junhyeok Jang</name>
    </author>
    <author>
      <name>Changrim Lee</name>
    </author>
    <author>
      <name>Miryeong Kwon</name>
    </author>
    <author>
      <name>Jie Zhang</name>
    </author>
    <author>
      <name>Myoungsoo Jung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 23 figures, 2019 IEEE International Symposium on Workload
  Characterization</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.06998v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.06998v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.08928v1</id>
    <updated>2020-02-20T18:25:42Z</updated>
    <published>2020-02-20T18:25:42Z</published>
    <title>LibrettOS: A Dynamically Adaptable Multiserver-Library OS</title>
    <summary>  We present LibrettOS, an OS design that fuses two paradigms to simultaneously
address issues of isolation, performance, compatibility, failure
recoverability, and run-time upgrades. LibrettOS acts as a microkernel OS that
runs servers in an isolated manner. LibrettOS can also act as a library OS
when, for better performance, selected applications are granted exclusive
access to virtual hardware resources such as storage and networking.
Furthermore, applications can switch between the two OS modes with no
interruption at run-time. LibrettOS has a uniquely distinguishing advantage in
that, the two paradigms seamlessly coexist in the same OS, enabling users to
simultaneously exploit their respective strengths (i.e., greater isolation,
high performance). Systems code, such as device drivers, network stacks, and
file systems remain identical in the two modes, enabling dynamic mode switching
and reducing development and maintenance costs.
  To illustrate these design principles, we implemented a prototype of
LibrettOS using rump kernels, allowing us to reuse existent, hardened NetBSD
device drivers and a large ecosystem of POSIX/BSD-compatible applications. We
use hardware (VM) virtualization to strongly isolate different rump kernel
instances from each other. Because the original rumprun unikernel targeted a
much simpler model for uniprocessor systems, we redesigned it to support
multicore systems. Unlike kernel-bypass libraries such as DPDK, applications
need not be modified to benefit from direct hardware access. LibrettOS also
supports indirect access through a network server that we have developed.
Applications remain uninterrupted even when network components fail or need to
be upgraded. Finally, to efficiently use hardware resources, applications can
dynamically switch between the indirect and direct modes based on their I/O
load at run-time.
  [full abstract is in the paper]
</summary>
    <author>
      <name>Ruslan Nikolaev</name>
    </author>
    <author>
      <name>Mincheol Sung</name>
    </author>
    <author>
      <name>Binoy Ravindran</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3381052.3381316</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3381052.3381316" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution
  Environments (VEE '20), March 17, 2020, Lausanne, Switzerland</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.08928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.08928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.05160v1</id>
    <updated>2020-03-11T08:38:34Z</updated>
    <published>2020-03-11T08:38:34Z</published>
    <title>Efficient Schedulability Test for Dynamic-Priority Scheduling of
  Mixed-Criticality Real-Time Systems</title>
    <summary>  Systems in many safety-critical application domains are subject to
certification requirements. In such a system, there are typically different
applications providing functionalities that have varying degrees of
criticality. Consequently, the certification requirements for functionalities
at these different criticality levels are also varying, with very high levels
of assurance required for a highly critical functionality, whereas relatively
low levels of assurance required for a less critical functionality. Considering
the timing assurance given to various applications in the form of guaranteed
budgets within deadlines, a theory of real-time scheduling for such
multi-criticality systems has been under development in the recent past. In
particular, an algorithm called Earliest Deadline First with Virtual Deadlines
(EDF-VD) has shown a lot of promise for systems with two criticality levels,
especially in terms of practical performance demonstrated through experiment
results. In this paper we design a new schedulability test for EDF-VD that
extend these performance benefits to multi-criticality systems. We propose a
new test based on demand bound functions and also present a novel virtual
deadline assignment strategy. Through extensive experiments we show that the
proposed technique significantly outperforms existing strategies for a variety
of generic real-time systems.
</summary>
    <author>
      <name>Xiaozhe Gu</name>
    </author>
    <author>
      <name>Arvind Easwaran</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3105922</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3105922" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Publication rights licensed to ACM</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Embedded Computing Systems, Volume 17, Issue
  1, Pages 24:1-24:24, November 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.05160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.05160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.05445v1</id>
    <updated>2020-03-11T05:11:33Z</updated>
    <published>2020-03-11T05:11:33Z</published>
    <title>Utilization Difference Based Partitioned Scheduling of Mixed-Criticality
  Systems</title>
    <summary>  Mixed-Criticality (MC) systems consolidate multiple functionalities with
different criticalities onto a single hardware platform. Such systems improve
the overall resource utilization while guaranteeing resources to critical
tasks. In this paper, we focus on the problem of partitioned multiprocessor MC
scheduling, in particular the problem of designing efficient partitioning
strategies. We develop two new partitioning strategies based on the principle
of evenly distributing the difference between total high-critical utilization
and total low-critical utilization for the critical tasks among all processors.
By balancing this difference, we are able to reduce the pessimism in
uniprocessor MC schedulability tests that are applied on each processor, thus
improving overall schedulability. To evaluate the schedulability performance of
the proposed strategies, we compare them against existing partitioned
algorithms using extensive experiments. We show that the proposed strategies
are effective with both dynamic-priority Earliest Deadline First with Virtual
Deadlines (EDF-VD) and fixed-priority Adaptive Mixed-Criticality (AMC)
algorithms. Specifically, our results show that the proposed strategies improve
schedulability by as much as 28.1% and 36.2% for implicit and
constrained-deadline task systems respectively.
</summary>
    <author>
      <name>Saravanan Ramanathan</name>
    </author>
    <author>
      <name>Arvind Easwaran</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23919/DATE.2017.7926989</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23919/DATE.2017.7926989" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">\copyright 2017 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Design, Automation &amp; Test in Europe Conference &amp; Exhibition
  (DATE), 2017, Lausanne, 2017, pages 238-243</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.05445v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.05445v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.02400v1</id>
    <updated>2020-04-06T04:39:58Z</updated>
    <published>2020-04-06T04:39:58Z</published>
    <title>Resource Efficient Isolation Mechanisms in Mixed-Criticality Scheduling</title>
    <summary>  Mixed-criticality real-time scheduling has been developed to improve resource
utilization while guaranteeing safe execution of critical applications. These
studies use optimistic resource reservation for all the applications to improve
utilization, but prioritize critical applications when the reservations become
insufficient at runtime. Many of them however share an impractical assumption
that all the critical applications will simultaneously demand additional
resources. As a consequence, they under-utilize resources by penalizing all the
low-criticality applications. In this paper we overcome this shortcoming using
a novel mechanism that comprises a parameter to model the expected number of
critical applications simultaneously demanding more resources, and an execution
strategy based on the parameter to improve resource utilization. Since most
mixed-criticality systems in practice are component-based, we design our
mechanism such that the component boundaries provide the isolation necessary to
support the execution of low-criticality applications, and at the same time
protect the critical ones. We also develop schedulability tests for the
proposed mechanism under both a flat as well as a hierarchical scheduling
framework. Finally, through simulations, we compare the performance of the
proposed approach with existing studies in terms of schedulability and the
capability to support low-criticality applications.
</summary>
    <author>
      <name>Xiaozhe Gu</name>
    </author>
    <author>
      <name>Arvind Easwaran</name>
    </author>
    <author>
      <name>Kieu-My Phan</name>
    </author>
    <author>
      <name>Insik Shin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ECRTS.2015.9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ECRTS.2015.9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">\copyright 2015 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Euromicro Conference on Real-Time Systems (ECRTS), Lund, 2015, pp.
  13-24</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2004.02400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.02400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.02439v1</id>
    <updated>2020-04-06T07:24:40Z</updated>
    <published>2020-04-06T07:24:40Z</published>
    <title>Optimal Virtual Cluster-based Multiprocessor Scheduling</title>
    <summary>  Scheduling of constrained deadline sporadic task systems on multiprocessor
platforms is an area which has received much attention in the recent past. It
is widely believed that finding an optimal scheduler is hard, and therefore
most studies have focused on developing algorithms with good processor
utilization bounds. These algorithms can be broadly classified into two
categories: partitioned scheduling in which tasks are statically assigned to
individual processors, and global scheduling in which each task is allowed to
execute on any processor in the platform. In this paper we consider a third,
more general, approach called cluster-based scheduling. In this approach each
task is statically assigned to a processor cluster, tasks in each cluster are
globally scheduled among themselves, and clusters in turn are scheduled on the
multiprocessor platform. We develop techniques to support such cluster-based
scheduling algorithms, and also consider properties that minimize total
processor utilization of individual clusters. In the last part of this paper,
we develop new virtual cluster-based scheduling algorithms. For implicit
deadline sporadic task systems, we develop an optimal scheduling algorithm that
is neither Pfair nor ERfair. We also show that the processor utilization bound
of US-EDF{m/(2m-1)} can be improved by using virtual clustering. Since neither
partitioned nor global strategies dominate over the other, cluster-based
scheduling is a natural direction for research towards achieving improved
processor utilization bounds.
</summary>
    <author>
      <name>Arvind Easwaran</name>
    </author>
    <author>
      <name>Insik Shin</name>
    </author>
    <author>
      <name>Insup Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11241-009-9073-x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11241-009-9073-x" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a post-peer-review, pre-copyedit version of an article
  published in Springer Real-Time Systems journal. The final authenticated
  version is available online at: https://doi.org/10.1007/s11241-009-9073-x</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Springer Real-Time Systems, Volume 43, Pages 25-59, July 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2004.02439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.02439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.05524v1</id>
    <updated>2020-04-12T01:59:20Z</updated>
    <published>2020-04-12T01:59:20Z</published>
    <title>Accelerating Filesystem Checking and Repair with pFSCK</title>
    <summary>  File system checking and recovery (C/R) tools play a pivotal role in
increasing the reliability of storage software, identifying and correcting file
system inconsistencies. However, with increasing disk capacity and data
content, file system C/R tools notoriously suffer from long runtimes. We posit
that current file system checkers fail to exploit CPU parallelism and high
throughput offered by modern storage devices. To overcome these challenges, we
propose pFSCK, a tool that redesigns C/R to enable fine-grained parallelism at
the granularity of inodes without impacting the correctness of C/R's
functionality. To accelerate C/R, pFSCK first employs data parallelism by
identifying functional operations in each stage of the checker and isolating
dependent operation and their shared data structures. However, fully isolating
shared structures is infeasible, consequently requiring serialization that
limits scalability. To reduce the impact of synchronization bottlenecks and
exploit CPU parallelism, pFSCK designs pipeline parallelism allowing multiple
stages of C/R to run simultaneously without impacting correctness. To realize
efficient pipeline parallelism for different file system data configurations,
pFSCK provides techniques for ordering updates to global data structures,
efficient per-thread I/O cache management, and dynamic thread placement across
different passes of a C/R. Finally, pFSCK designs a resource-aware scheduler
aimed towards reducing the impact of C/R on other applications sharing CPUs and
the file system. Evaluation of pFSCK shows more than 2.6x gains of e2fsck and
more than 1.8x over XFS's checker that provides coarse-grained parallelism.
</summary>
    <author>
      <name>David Domingo</name>
    </author>
    <author>
      <name>Kyle Stratton</name>
    </author>
    <author>
      <name>Sudarsun Kannan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.05524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.05524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.06354v1</id>
    <updated>2020-04-14T08:35:33Z</updated>
    <published>2020-04-14T08:35:33Z</published>
    <title>A Linux Kernel Scheduler Extension for Multi-core Systems</title>
    <summary>  The Linux kernel is mostly designed for multi-programed environments, but
high-performance applications have other requirements. Such applications are
run standalone, and usually rely on runtime systems to distribute the
application's workload on worker threads, one per core. However, due to current
OSes limitations, it is not feasible to track whether workers are actually
running or blocked due to, for instance, a requested resource. For I/O
intensive applications, this leads to a significant performance degradation
given that the core of a blocked thread becomes idle until it is able to run
again. In this paper, we present the proof-of-concept of a Linux kernel
extension denoted User-Monitored Threads (UMT) which tackles this problem. Our
extension allows a user-space process to be notified of when the selected
threads become blocked or unblocked, making it possible for a runtime to
schedule additional work on the idle core. We implemented the extension on the
Linux Kernel 5.1 and adapted the Nanos6 runtime of the OmpSs-2 programming
model to take advantage of it. The whole prototype was tested on two
applications which, on the tested hardware and the appropriate conditions,
reported speedups of almost 2x.
</summary>
    <author>
      <name>Aleix Roca</name>
    </author>
    <author>
      <name>Samuel Rodríguez</name>
    </author>
    <author>
      <name>Albert Segura</name>
    </author>
    <author>
      <name>Kevin Marquet</name>
    </author>
    <author>
      <name>Vicenç Beltran</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HiPC.2019.00050</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HiPC.2019.00050" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.06354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.06354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.1; D.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.09723v3</id>
    <updated>2021-02-08T20:07:02Z</updated>
    <published>2020-05-19T19:46:01Z</published>
    <title>High Velocity Kernel File Systems with Bento</title>
    <summary>  High development velocity is critical for modern systems. This is especially
true for Linux file systems which are seeing increased pressure from new
storage devices and new demands on storage systems. However, high velocity
Linux kernel development is challenging due to the ease of introducing bugs,
the difficulty of testing and debugging, and the lack of support for
redeployment without service disruption. Existing approaches to high-velocity
development of file systems for Linux have major downsides, such as the high
performance penalty for FUSE file systems, slowing the deployment cycle for new
file system functionality.
  We propose Bento, a framework for high velocity development of Linux kernel
file systems. It enables file systems written in safe Rust to be installed in
the Linux kernel, with errors largely sandboxed to the file system. Bento file
systems can be replaced with no disruption to running applications, allowing
daily or weekly upgrades in a cloud server setting. Bento also supports
userspace debugging. We implement a simple file system using Bento and show
that it performs similarly to VFS-native ext4 on a variety of benchmarks and
outperforms a FUSE version by 7x on 'git clone'. We also show that we can
dynamically add file provenance tracking to a running kernel file system with
only 15ms of service interruption.
</summary>
    <author>
      <name>Samantha Miller</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Washington</arxiv:affiliation>
    </author>
    <author>
      <name>Kaiyuan Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Washington</arxiv:affiliation>
    </author>
    <author>
      <name>Mengqi Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Washington</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Jennings</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Washington</arxiv:affiliation>
    </author>
    <author>
      <name>Ang Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Rice University</arxiv:affiliation>
    </author>
    <author>
      <name>Danyang Zhuo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Duke University</arxiv:affiliation>
    </author>
    <author>
      <name>Tom Anderson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Washington</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures, to be published in FAST 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.09723v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.09723v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.12144v1</id>
    <updated>2020-06-22T11:12:44Z</updated>
    <published>2020-06-22T11:12:44Z</published>
    <title>Scalable Range Locks for Scalable Address Spaces and Beyond</title>
    <summary>  Range locks are a synchronization construct designed to provide concurrent
access to multiple threads (or processes) to disjoint parts of a shared
resource. Originally conceived in the file system context, range locks are
gaining increasing interest in the Linux kernel community seeking to alleviate
bottlenecks in the virtual memory management subsystem. The existing
implementation of range locks in the kernel, however, uses an internal spin
lock to protect the underlying tree structure that keeps track of acquired and
requested ranges. This spin lock becomes a point of contention on its own when
the range lock is frequently acquired. Furthermore, where and exactly how
specific (refined) ranges can be locked remains an open question.
  In this paper, we make two independent, but related contributions. First, we
propose an alternative approach for building range locks based on linked lists.
The lists are easy to maintain in a lock-less fashion, and in fact, our range
locks do not use any internal locks in the common case. Second, we show how the
range of the lock can be refined in the mprotect operation through a
speculative mechanism. This refinement, in turn, allows concurrent execution of
mprotect operations on non-overlapping memory regions. We implement our new
algorithms and demonstrate their effectiveness in user-space and kernel-space,
achieving up to 9$\times$ speedup compared to the stock version of the Linux
kernel. Beyond the virtual memory management subsystem, we discuss other
applications of range locks in parallel software. As a concrete example, we
show how range locks can be used to facilitate the design of scalable
concurrent data structures, such as skip lists.
</summary>
    <author>
      <name>Alex Kogan</name>
    </author>
    <author>
      <name>Dave Dice</name>
    </author>
    <author>
      <name>Shady Issa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3342195.3387533</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3342195.3387533" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 9 figures, Eurosys 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.12144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.12144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.3; D.4.1; D.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.05136v1</id>
    <updated>2020-07-10T02:03:52Z</updated>
    <published>2020-07-10T02:03:52Z</published>
    <title>LINTS^RT: A Learning-driven Testbed for Intelligent Scheduling in
  Embedded Systems</title>
    <summary>  Due to the increasing complexity seen in both workloads and hardware
resources in state-of-the-art embedded systems, developing efficient real-time
schedulers and the corresponding schedulability tests becomes rather
challenging. Although close to optimal schedulability performance can be
achieved for supporting simple system models in practice, adding any small
complexity element into the problem context such as non-preemption or resource
heterogeneity would cause significant pessimism, which may not be eliminated by
any existing scheduling technique. In this paper, we present LINTS^RT, a
learning-based testbed for intelligent real-time scheduling, which has the
potential to handle various complexities seen in practice. The design of
LINTS^RT is fundamentally motivated by AlphaGo Zero for playing the board game
Go, and specifically addresses several critical challenges due to the real-time
scheduling context. We first present a clean design of LINTS^RT for supporting
the basic case: scheduling sporadic workloads on a homogeneous multiprocessor,
and then demonstrate how to easily extend the framework to handle further
complexities such as non-preemption and resource heterogeneity. Both
application and OS-level implementation and evaluation demonstrate that
LINTS^RT is able to achieve significantly higher runtime schedulability under
different settings compared to perhaps the most commonly applied schedulers,
global EDF, and RM. To our knowledge, this work is the first attempt to design
and implement an extensible learning-based testbed for autonomously making
real-time scheduling decisions.
</summary>
    <author>
      <name>Zelun Kong</name>
    </author>
    <author>
      <name>Yaswanth Yadlapalli</name>
    </author>
    <author>
      <name>Soroush Bateni</name>
    </author>
    <author>
      <name>Junfeng Guo</name>
    </author>
    <author>
      <name>Cong Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.05136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.05136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.08062v1</id>
    <updated>2021-01-20T10:54:59Z</updated>
    <published>2021-01-20T10:54:59Z</published>
    <title>Thread Evolution Kit for Optimizing Thread Operations on CE/IoT Devices</title>
    <summary>  Most modern operating systems have adopted the one-to-one thread model to
support fast execution of threads in both multi-core and single-core systems.
This thread model, which maps the kernel-space and user-space threads in a
one-to-one manner, supports quick thread creation and termination in
high-performance server environments. However, the performance of time-critical
threads is degraded when multiple threads are being run in low-end CE devices
with limited system resources. When a CE device runs many threads to support
diverse application functionalities, low-level hardware specifications often
lead to significant resource contention among the threads trying to obtain
system resources. As a result, the operating system encounters challenges, such
as excessive thread context switching overhead, execution delay of
time-critical threads, and a lack of virtual memory for thread stacks. This
paper proposes a state-of-the-art Thread Evolution Kit (TEK) that consists of
three primary components: a CPU Mediator, Stack Tuner, and Enhanced Thread
Identifier. From the experiment, we can see that the proposed scheme
significantly improves user responsiveness (7x faster) under high CPU
contention compared to the traditional thread model. Also, TEK solves the
segmentation fault problem that frequently occurs when a CE application
increases the number of threads during its execution.
</summary>
    <author>
      <name>Geunsik Lim</name>
    </author>
    <author>
      <name>Donghyun Kang</name>
    </author>
    <author>
      <name>Young Ik Eom</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCE.2020.3033328</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCE.2020.3033328" rel="related"/>
    <link href="http://arxiv.org/abs/2101.08062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.08062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.06366v2</id>
    <updated>2022-06-20T12:42:32Z</updated>
    <published>2021-04-13T17:23:21Z</published>
    <title>Supporting Multiprocessor Resource Synchronization Protocols in RTEMS</title>
    <summary>  When considering recurrent tasks in real-time systems, concurrent accesses to
shared resources, can cause race conditions or data corruptions. Such a problem
has been extensively studied since the 1990s, and numerous resource
synchronization protocols have been developed for both uni-processor and
multiprocessor real-time systems, with the assumption that the implementation
overheads are negligible. However, in practice, the implementation overheads
may impact the performance of different protocols depending upon the practiced
scenarios, e.g., resources are accessed locally or remotely, and tasks spin or
suspend themselves when the requested resources are not available. In this
paper, to show the applicability of different protocols in real-world systems,
we detail the implementation of several state-of-the-art multiprocessor
resource synchronization protocols in RTEMS. To study the impact of the
implementation overheads, we deploy these implemented protocols on a real
platform with synthetic task set. The measured results illustrate that the
developed resource synchronization protocols in RTEMS are comparable to the
existed protocol, i.e., MrsP.
</summary>
    <author>
      <name>Junjie Shi</name>
    </author>
    <author>
      <name>Jan Duy Thien Pham</name>
    </author>
    <author>
      <name>Malte Münch</name>
    </author>
    <author>
      <name>Jan Viktor Hafemeister</name>
    </author>
    <author>
      <name>Jian-Jia Chen</name>
    </author>
    <author>
      <name>Kuan-Hsun Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures, presented in 16th annual workshop on Operating
  Systems Platforms for Embedded Real-Time applications (OSPERT'22)</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.06366v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.06366v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12721v1</id>
    <updated>2021-04-26T17:07:07Z</updated>
    <published>2021-04-26T17:07:07Z</published>
    <title>Unikraft: Fast, Specialized Unikernels the Easy Way</title>
    <summary>  Unikernels are famous for providing excellent performance in terms of boot
times, throughput and memory consumption, to name a few metrics. However, they
are infamous for making it hard and extremely time consuming to extract such
performance, and for needing significant engineering effort in order to port
applications to them. We introduce Unikraft, a novel micro-library OS that (1)
fully modularizes OS primitives so that it is easy to customize the unikernel
and include only relevant components and (2) exposes a set of composable,
performance-oriented APIs in order to make it easy for developers to obtain
high performance.
  Our evaluation using off-the-shelf applications such as nginx, SQLite, and
Redis shows that running them on Unikraft results in a 1.7x-2.7x performance
improvement compared to Linux guests. In addition, Unikraft images for these
apps are around 1MB, require less than 10MB of RAM to run, and boot in around
1ms on top of the VMM time (total boot time 3ms-40ms). Unikraft is a Linux
Foundation open source project and can be found at www.unikraft.org.
</summary>
    <author>
      <name>Simon Kuenzer</name>
    </author>
    <author>
      <name>Vlad-Andrei Bădoiu</name>
    </author>
    <author>
      <name>Hugo Lefeuvre</name>
    </author>
    <author>
      <name>Sharan Santhanam</name>
    </author>
    <author>
      <name>Alexander Jung</name>
    </author>
    <author>
      <name>Gaulthier Gain</name>
    </author>
    <author>
      <name>Cyril Soldani</name>
    </author>
    <author>
      <name>Costin Lupu</name>
    </author>
    <author>
      <name>Ştefan Teodorescu</name>
    </author>
    <author>
      <name>Costi Răducanu</name>
    </author>
    <author>
      <name>Cristian Banu</name>
    </author>
    <author>
      <name>Laurent Mathy</name>
    </author>
    <author>
      <name>Răzvan Deaconescu</name>
    </author>
    <author>
      <name>Costin Raiciu</name>
    </author>
    <author>
      <name>Felipe Huici</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3447786.3456248</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3447786.3456248" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 22 figures, 7 tables, conference proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.12721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.14354v3</id>
    <updated>2021-10-12T01:19:49Z</updated>
    <published>2021-04-28T15:46:02Z</published>
    <title>SoCRATES: System-on-Chip Resource Adaptive Scheduling using Deep
  Reinforcement Learning</title>
    <summary>  Deep Reinforcement Learning (DRL) is being increasingly applied to the
problem of resource allocation for emerging System-on-Chip (SoC) applications,
and has shown remarkable promises. In this paper, we introduce SoCRATES (SoC
Resource AdapTivE Scheduler), an extremely efficient DRL-based SoC scheduler
which maps a wide range of hierarchical jobs to heterogeneous resources within
SoC using the Eclectic Interaction Matching (EIM) technique. It is noted that
the majority of SoC resource management approaches have been targeting makespan
minimization with fixed number of jobs in the system. In contrast, SoCRATES
aims at minimizing average latency in a steady-state condition while assigning
tasks in the ready queue to heterogeneous resources (processing elements). We
first show that the latency-minimization-driven SoC applications operate
high-frequency job workload and distributed/parallel job execution. We then
demonstrate SoCRATES successfully addresses the challenge of concurrent
observations caused by the task dependency inherent in the latency minimization
objective. Extensive tests show that SoCRATES outperforms other existing neural
and non-neural schedulers with as high as 38% gain in latency reduction under a
variety of job types and incoming rates. The resulting model is also compact in
size and has very favorable energy consumption behaviors, making it highly
practical for deployment in future SoC systems with built-in neural
accelerator.
</summary>
    <author>
      <name>Tegg Taekyong Sung</name>
    </author>
    <author>
      <name>Bo Ryu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted for publication by 20th IEEE
  International Conference on Machine Learning and Applications (ICMLA 2021).
  The copyright is with the IEEE</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.14354v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.14354v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.11554v2</id>
    <updated>2022-01-26T01:45:24Z</updated>
    <published>2021-11-22T21:59:50Z</published>
    <title>KML: Using Machine Learning to Improve Storage Systems</title>
    <summary>  Operating systems include many heuristic algorithms designed to improve
overall storage performance and throughput. Because such heuristics cannot work
well for all conditions and workloads, system designers resorted to exposing
numerous tunable parameters to users -- thus burdening users with continually
optimizing their own storage systems and applications. Storage systems are
usually responsible for most latency in I/O-heavy applications, so even a small
latency improvement can be significant. Machine learning (ML) techniques
promise to learn patterns, generalize from them, and enable optimal solutions
that adapt to changing workloads. We propose that ML solutions become a
first-class component in OSs and replace manual heuristics to optimize storage
systems dynamically. In this paper, we describe our proposed ML architecture,
called KML. We developed a prototype KML architecture and applied it to two
case studies: optimizing readahead and NFS read-size values. Our experiments
show that KML consumes less than 4KB of dynamic kernel memory, has a CPU
overhead smaller than 0.2%, and yet can learn patterns and improve I/O
throughput by as much as 2.3x and 15x for two case studies -- even for complex,
never-seen-before, concurrently running mixed workloads on different storage
devices.
</summary>
    <author>
      <name>Ibrahim Umit Akgun</name>
    </author>
    <author>
      <name>Ali Selman Aydin</name>
    </author>
    <author>
      <name>Andrew Burford</name>
    </author>
    <author>
      <name>Michael McNeill</name>
    </author>
    <author>
      <name>Michael Arkhangelskiy</name>
    </author>
    <author>
      <name>Aadil Shaikh</name>
    </author>
    <author>
      <name>Lukas Velikov</name>
    </author>
    <author>
      <name>Erez Zadok</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.11554v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.11554v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.15240v2</id>
    <updated>2022-07-09T11:21:18Z</updated>
    <published>2021-11-30T09:49:32Z</published>
    <title>Verifying and Optimizing Compact NUMA-Aware Locks on Weak Memory Models</title>
    <summary>  Developing concurrent software is challenging, especially if it has to run on
modern architectures with Weak Memory Models (WMMs) such as ARMv8, Power, or
RISC-V. For the sake of performance, WMMs allow hardware and compilers to
aggressively reorder memory accesses. To guarantee correctness, developers have
to carefully place memory barriers in the code to enforce ordering among
critical memory operations.
  While WMM architectures are growing in popularity, identifying the necessary
and sufficient barriers of complex synchronization primitives is notoriously
difficult. Unfortunately, publications often consider barriers to be just
implementation details and omit them. In this technical note, we report our
efforts in verifying the correctness of the Compact NUMA-Aware (CNA) lock
algorithm on WMMs. The CNA lock is of special interest because it has been
proposed as a new slowpath for Linux qspinlock, the main spinlock in Linux.
Besides determining a correct and efficient set of barriers for the original
CNA algorithm on WMMs, we investigate the correctness of Linux qspinlock and
the latest Linux CNA patch (v15) on the memory models LKMM, ARMv8, and Power.
Surprisingly, we have found that Linux qspinlock and, consequently, Linux CNA
are incorrect according to LKMM, but are still correct when compiled to ARMv8
or Power.
</summary>
    <author>
      <name>Antonio Paolillo</name>
    </author>
    <author>
      <name>Hernán Ponce-de-León</name>
    </author>
    <author>
      <name>Thomas Haas</name>
    </author>
    <author>
      <name>Diogo Behrens</name>
    </author>
    <author>
      <name>Rafael Chehab</name>
    </author>
    <author>
      <name>Ming Fu</name>
    </author>
    <author>
      <name>Roland Meyer</name>
    </author>
    <link href="http://arxiv.org/abs/2111.15240v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.15240v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.06566v3</id>
    <updated>2022-01-14T09:12:31Z</updated>
    <published>2021-12-13T11:19:01Z</published>
    <title>FlexOS: Towards Flexible OS Isolation</title>
    <summary>  At design time, modern operating systems are locked in a specific safety and
isolation strategy that mixes one or more hardware/software protection
mechanisms (e.g. user/kernel separation); revisiting these choices after
deployment requires a major refactoring effort. This rigid approach shows its
limits given the wide variety of modern applications' safety/performance
requirements, when new hardware isolation mechanisms are rolled out, or when
existing ones break.
  We present FlexOS, a novel OS allowing users to easily specialize the safety
and isolation strategy of an OS at compilation/deployment time instead of
design time. This modular LibOS is composed of fine-grained components that can
be isolated via a range of hardware protection mechanisms with various data
sharing strategies and additional software hardening. The OS ships with an
exploration technique helping the user navigate the vast safety/performance
design space it unlocks. We implement a prototype of the system and
demonstrate, for several applications (Redis/Nginx/SQLite), FlexOS' vast
configuration space as well as the efficiency of the exploration technique: we
evaluate 80 FlexOS configurations for Redis and show how that space can be
probabilistically subset to the 5 safest ones under a given performance budget.
We also show that, under equivalent configurations, FlexOS performs similarly
or better than several baselines/competitors.
</summary>
    <author>
      <name>Hugo Lefeuvre</name>
    </author>
    <author>
      <name>Vlad-Andrei Bădoiu</name>
    </author>
    <author>
      <name>Alexander Jung</name>
    </author>
    <author>
      <name>Stefan Teodorescu</name>
    </author>
    <author>
      <name>Sebastian Rauch</name>
    </author>
    <author>
      <name>Felipe Huici</name>
    </author>
    <author>
      <name>Costin Raiciu</name>
    </author>
    <author>
      <name>Pierre Olivier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Artifact Evaluation Repository:
  https://github.com/project-flexos/asplos22-ae</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.06566v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.06566v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.00241v4</id>
    <updated>2022-10-21T22:02:53Z</updated>
    <published>2022-03-01T05:32:52Z</published>
    <title>Pond: CXL-Based Memory Pooling Systems for Cloud Platforms</title>
    <summary>  Public cloud providers seek to meet stringent performance requirements and
low hardware cost. A key driver of performance and cost is main memory. Memory
pooling promises to improve DRAM utilization and thereby reduce costs. However,
pooling is challenging under cloud performance requirements. This paper
proposes Pond, the first memory pooling system that both meets cloud
performance goals and significantly reduces DRAM cost. Pond builds on the
Compute Express Link (CXL) standard for load/store access to pool memory and
two key insights. First, our analysis of cloud production traces shows that
pooling across 8-16 sockets is enough to achieve most of the benefits. This
enables a small-pool design with low access latency. Second, it is possible to
create machine learning models that can accurately predict how much local and
pool memory to allocate to a virtual machine (VM) to resemble same-NUMA-node
memory performance. Our evaluation with 158 workloads shows that Pond reduces
DRAM costs by 7% with performance within 1-5% of same-NUMA-node VM allocations.
</summary>
    <author>
      <name>Huaicheng Li</name>
    </author>
    <author>
      <name>Daniel S. Berger</name>
    </author>
    <author>
      <name>Stanko Novakovic</name>
    </author>
    <author>
      <name>Lisa Hsu</name>
    </author>
    <author>
      <name>Dan Ernst</name>
    </author>
    <author>
      <name>Pantea Zardoshti</name>
    </author>
    <author>
      <name>Monish Shah</name>
    </author>
    <author>
      <name>Samir Rajadnya</name>
    </author>
    <author>
      <name>Scott Lee</name>
    </author>
    <author>
      <name>Ishwar Agarwal</name>
    </author>
    <author>
      <name>Mark D. Hill</name>
    </author>
    <author>
      <name>Marcus Fontoura</name>
    </author>
    <author>
      <name>Ricardo Bianchini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Update affiliations</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.00241v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.00241v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1815v1</id>
    <updated>2011-05-09T22:39:46Z</updated>
    <published>2011-05-09T22:39:46Z</published>
    <title>User Mode Memory Page Management: An old idea applied anew to the memory
  wall problem</title>
    <summary>  It is often said that one of the biggest limitations on computer performance
is memory bandwidth (i.e."the memory wall problem"). In this position paper, I
argue that if historical trends in computing evolution (where growth in
available capacity is exponential and reduction in its access latencies is
linear) continue as they have, then this view is wrong - in fact we ought to be
concentrating on reducing whole system memory access latencies wherever
possible, and by "whole system" I mean that we ought to look at how software
can be unnecessarily wasteful with memory bandwidth due to legacy design
decisions. To this end I conduct a feasibility study to determine whether we
ought to virtualise the MMU for each application process such that it has
direct access to its own MMU page tables and the memory allocated to a process
is managed exclusively by the process and not the kernel. I find under typical
conditions that nearly scale invariant performance to memory allocation size is
possible such that hundreds of megabytes of memory can be allocated, relocated,
swapped and deallocated in almost the same time as kilobytes (e.g. allocating
8Mb is 10x quicker under this experimental allocator than a conventional
allocator, and resizing a 128Kb block to 256Kb block is 4.5x faster). I find
that first time page access latencies are improved tenfold; moreover, because
the kernel page fault handler is never called, the lack of cache pollution
improves whole application memory access latencies increasing performance by up
to 2x. Finally, I try binary patching existing applications to use the
experimental allocation technique, finding almost universal performance
improvements without having to recompile these applications to make better use
of the new facilities.
</summary>
    <author>
      <name>Niall Douglas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages. Rejected from MSPC11</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.1815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; D.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.5136v1</id>
    <updated>2011-12-21T19:02:12Z</updated>
    <published>2011-12-21T19:02:12Z</published>
    <title>Quest-V: A Virtualized Multikernel for High-Confidence Systems</title>
    <summary>  This paper outlines the design of `Quest-V', which is implemented as a
collection of separate kernels operating together as a distributed system on a
chip. Quest-V uses virtualization techniques to isolate kernels and prevent
local faults from affecting remote kernels. This leads to a high-confidence
multikernel approach, where failures of system subcomponents do not render the
entire system inoperable. A virtual machine monitor for each kernel keeps track
of shadow page table mappings that control immutable memory access
capabilities. This ensures a level of security and fault tolerance in
situations where a service in one kernel fails, or is corrupted by a malicious
attack. Communication is supported between kernels using shared memory regions
for message passing. Similarly, device driver data structures are shareable
between kernels to avoid the need for complex I/O virtualization, or
communication with a dedicated kernel responsible for I/O. In Quest-V, device
interrupts are delivered directly to a kernel, rather than via a monitor that
determines the destination. Apart from bootstrapping each kernel, handling
faults and managing shadow page tables, the monitors are not needed. This
differs from conventional virtual machine systems in which a central monitor,
or hypervisor, is responsible for scheduling and management of host resources
amongst a set of guest kernels. In this paper we show how Quest-V can implement
novel fault isolation and recovery techniques that are not possible with
conventional systems. We also show how the costs of using virtualization for
isolation of system services does not add undue overheads to the overall system
performance.
</summary>
    <author>
      <name>Ye Li</name>
    </author>
    <author>
      <name>Matthew Danish</name>
    </author>
    <author>
      <name>Richard West</name>
    </author>
    <link href="http://arxiv.org/abs/1112.5136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.5136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.08495v1</id>
    <updated>2017-11-14T21:28:19Z</updated>
    <published>2017-11-14T21:28:19Z</published>
    <title>Seamless Resources Sharing in Wearable Networks by Application Function
  Virtualization</title>
    <summary>  The prevalence of smart wearable devices is increasing exponentially and we
are witnessing a wide variety of fascinating new services that leverage the
capabilities of these wearables. Wearables are truly changing the way mobile
computing is deployed and mobile applications are being developed. It is
possible to leverage the capabilities such as connectivity, processing, and
sensing of wearable devices in an adaptive manner for efficient resource usage
and information accuracy within the personal area network. We show that
application developers are not yet taking advantage of these cross-device
capabilities, however, instead using wearables as passive sensors or simple end
displays to provide notifications to the user. We thus design AFV (Application
Function Virtualization), an architecture enabling automated dynamic function
virtualization and scheduling across devices in a personal area network,
simplifying the development of the apps that are adaptive to context changes.
AFV provides a simple set of APIs hiding complex architectural tasks from app
developers whilst continuously monitoring the user, device and network context,
to enable the adaptive invocation of functions across devices. We show the
feasibility of our design by implementing AFV on Android, and the benefits for
the user in terms of resource efficiency, especially in saving energy
consumption, and quality of experience with multiple use cases.
</summary>
    <author>
      <name>Harini Kolamunna</name>
    </author>
    <author>
      <name>Kanchana Thilakarathna</name>
    </author>
    <author>
      <name>Diego Perino</name>
    </author>
    <author>
      <name>Dwight Makaroff</name>
    </author>
    <author>
      <name>Aruna Seneviratne</name>
    </author>
    <link href="http://arxiv.org/abs/1711.08495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.08495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.02892v1</id>
    <updated>2018-09-08T22:53:27Z</updated>
    <published>2018-09-08T22:53:27Z</published>
    <title>Dependency Graph Approach for Multiprocessor Real-Time Synchronization</title>
    <summary>  Over the years, many multiprocessor locking protocols have been designed and
analyzed. However, the performance of these protocols highly depends on how the
tasks are partitioned and prioritized and how the resources are shared locally
and globally. This paper answers a few fundamental questions when real-time
tasks share resources in multiprocessor systems. We explore the fundamental
difficulty of the multiprocessor synchronization problem and show that a very
simplified version of this problem is ${\mathcal NP}$-hard in the strong sense
regardless of the number of processors and the underlying scheduling paradigm.
Therefore, the allowance of preemption or migration does not reduce the
computational complexity. For the positive side, we develop a dependency-graph
approach, that is specifically useful for frame-based real-time tasks, in which
all tasks have the same period and release their jobs always at the same time.
We present a series of algorithms with speedup factors between $2$ and $3$
under semi-partitioned scheduling. We further explore methodologies and
tradeoffs of preemptive against non-preemptive scheduling algorithms and
partitioned against semi-partitioned scheduling algorithms. The approach is
extended to periodic tasks under certain conditions.
</summary>
    <author>
      <name>Jian-Jia Chen</name>
    </author>
    <author>
      <name>Georg von der Brüggen</name>
    </author>
    <author>
      <name>Junjie Shi</name>
    </author>
    <author>
      <name>Niklas Uete</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted and to appear in IEEE Real-Time System Symposium (RTSS) 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.02892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.02892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.04658v1</id>
    <updated>2019-09-09T14:49:00Z</updated>
    <published>2019-09-09T14:49:00Z</published>
    <title>The Study of Dynamic Caching via State Transition Field -- the Case of
  Time-Invariant Popularity</title>
    <summary>  This two-part paper investigates cache replacement schemes with the objective
of developing a general model to unify the analysis of various replacement
schemes and illustrate their features. To achieve this goal, we study the
dynamic process of caching in the vector space and introduce the concept of
state transition field (STF) to model and characterize replacement schemes. In
the first part of this work, we consider the case of time-invariant content
popularity based on the independent reference model (IRM). In such case, we
demonstrate that the resulting STFs are static, and each replacement scheme
leads to a unique STF. The STF determines the expected trace of the dynamic
change in the cache state distribution, as a result of content requests and
replacements, from any initial point. Moreover, given the replacement scheme,
the STF is only determined by the content popularity. Using four example
schemes including random replacement (RR) and least recently used (LRU), we
show that the STF can be used to analyze replacement schemes such as finding
their steady states, highlighting their differences, and revealing insights
regarding the impact of knowledge of content popularity. Based on the above
results, STF is shown to be useful for characterizing and illustrating
replacement schemes. Extensive numeric results are presented to demonstrate
analytical STFs and STFs from simulations for the considered example
replacement schemes.
</summary>
    <author>
      <name>Jie Gao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name>Lian Zhao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name> Xuemin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name> Shen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.04658v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.04658v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.04659v1</id>
    <updated>2019-09-09T14:59:45Z</updated>
    <published>2019-09-09T14:59:45Z</published>
    <title>The Study of Dynamic Caching via State Transition Field -- the Case of
  Time-Varying Popularity</title>
    <summary>  In the second part of this two-part paper, we extend the study of dynamic
caching via state transition field (STF) to the case of time-varying content
popularity. The objective of this part is to investigate the impact of
time-varying content popularity on the STF and how such impact accumulates to
affect the performance of a replacement scheme. Unlike the case in the first
part, the STF is no longer static over time, and we introduce instantaneous STF
to model it. Moreover, we demonstrate that many metrics, such as instantaneous
state caching probability and average cache hit probability over an arbitrary
sequence of requests, can be found using the instantaneous STF. As a steady
state may not exist under time-varying content popularity, we characterize the
performance of replacement schemes based on how the instantaneous STF of a
replacement scheme after a content request impacts on its cache hit probability
at the next request. From this characterization, insights regarding the
relations between the pattern of change in the content popularity, the
knowledge of content popularity exploited by the replacement schemes, and the
effectiveness of these schemes under time-varying popularity are revealed. In
the simulations, different patterns of time-varying popularity, including the
shot noise model, are experimented. The effectiveness of example replacement
schemes under time-varying popularity is demonstrated, and the numerical
results support the observations from the analytic results.
</summary>
    <author>
      <name>Jie Gao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name>Lian Zhao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name> Xuemin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name> Shen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.04659v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.04659v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.11644v2</id>
    <updated>2019-10-01T18:15:48Z</updated>
    <published>2019-09-25T17:44:55Z</published>
    <title>An Improvement Over Threads Communications on Multi-Core Processors</title>
    <summary>  Multicore is an integrated circuit chip that uses two or more computational
engines (cores) places in a single processor. This new approach is used to
split the computational work of a threaded application and spread it over
multiple execution cores, so that the computer system can benefits from a
better performance and better responsiveness of the system. A thread is a unit
of execution inside a process that is created and maintained to execute a set
of actions/ instructions. Threads can be implemented differently from an
operating system to another, but the operating system is in most cases
responsible to schedule the execution of different threads. Multi-threading
improving efficiency of processor performance with a cost-effective memory
system. In this paper, we explore one approach to improve communications for
multithreaded. Pre-send is a software Controlled data forwarding technique that
sends data to destination's cache before it is needed, eliminating cache misses
in the destination's cache as well as reducing the coherence traffic on the
bus. we show how we could improve the overall system performance by addition of
these architecture optimizations to multi-core processors.
</summary>
    <author>
      <name>Reza Fotohi</name>
    </author>
    <author>
      <name>Mehdi Effatparvar</name>
    </author>
    <author>
      <name>Fateme Sarkohaki</name>
    </author>
    <author>
      <name>Shahram Behzad</name>
    </author>
    <author>
      <name>Jaber Hoseini balov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2012, Volume 6, Issue 12, pp 379-384</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1909.11644v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.11644v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.04698v1</id>
    <updated>2020-01-14T10:19:15Z</updated>
    <published>2020-01-14T10:19:15Z</published>
    <title>Online Scheduling with Makespan Minimization: State of the Art Results,
  Research Challenges and Open Problems</title>
    <summary>  Online scheduling has been a well studied and challenging research problem
over the last five decades since the pioneering work of Graham with immense
practical significance in various applications such as interactive parallel
processing, routing in communication networks, distributed data management,
client-server communications, traffic management in transportation, industrial
manufacturing and production. In this problem, a sequence of jobs is received
one by one in order by the scheduler for scheduling over a number of machines.
On arrival of a job, the scheduler assigns the job irrevocably to a machine
before the availability of the next job with an objective to minimize the
completion time of the scheduled jobs. This paper highlights the state of the
art contributions for online scheduling of a sequence of independent jobs on
identical and uniform related machines with a special focus on preemptive and
non-preemptive processing formats by considering makespan minimization as the
optimality criterion. We present the fundamental aspects of online scheduling
from a beginner's perspective along with a background of general scheduling
framework. Important competitive analysis results obtained by well-known
deterministic and randomized online scheduling algorithms in the literature are
presented along with research challenges and open problems. Two of the emerging
recent trends such as resource augmentation and semi-online scheduling are
discussed as a motivation for future research work.
</summary>
    <author>
      <name>Debasis Dwibedy</name>
    </author>
    <author>
      <name>Rakesh Mohanty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 13 Tables, Submitted to Computer Science Review</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.04698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.04698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.07450v1</id>
    <updated>2020-01-21T11:42:17Z</updated>
    <published>2020-01-21T11:42:17Z</published>
    <title>Occlum: Secure and Efficient Multitasking Inside a Single Enclave of
  Intel SGX</title>
    <summary>  Intel Software Guard Extensions (SGX) enables user-level code to create
private memory regions called enclaves, whose code and data are protected by
the CPU from software and hardware attacks outside the enclaves. Recent work
introduces library operating systems (LibOSes) to SGX so that legacy
applications can run inside enclaves with few or even no modifications. As
virtually any non-trivial application demands multiple processes, it is
essential for LibOSes to support multitasking. However, none of the existing
SGX LibOSes support multitasking both securely and efficiently.
  This paper presents Occlum, a system that enables secure and efficient
multitasking on SGX. We implement the LibOS processes as SFI-Isolated Processes
(SIPs). SFI is a software instrumentation technique for sandboxing untrusted
modules (called domains). We design a novel SFI scheme named MPX-based,
Multi-Domain SFI (MMDSFI) and leverage MMDSFI to enforce the isolation of SIPs.
We also design an independent verifier to ensure the security guarantees of
MMDSFI. With SIPs safely sharing the single address space of an enclave, the
LibOS can implement multitasking efficiently. The Occlum LibOS outperforms the
state-of-the-art SGX LibOS on multitasking-heavy workloads by up to 6,600X on
micro-benchmarks and up to 500X on application benchmarks.
</summary>
    <author>
      <name>Youren Shen</name>
    </author>
    <author>
      <name>Hongliang Tian</name>
    </author>
    <author>
      <name>Yu Chen</name>
    </author>
    <author>
      <name>Kang Chen</name>
    </author>
    <author>
      <name>Runji Wang</name>
    </author>
    <author>
      <name>Yi Xu</name>
    </author>
    <author>
      <name>Yubin Xia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3373376.3378469</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3373376.3378469" rel="related"/>
    <link href="http://arxiv.org/abs/2001.07450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.07450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.06988v2</id>
    <updated>2020-10-23T12:21:28Z</updated>
    <published>2020-09-15T11:15:25Z</published>
    <title>MigrOS: Transparent Operating Systems Live Migration Support for
  Containerised RDMA-applications</title>
    <summary>  Major data centre providers are introducing RDMA-based networks for their
tenants, as well as for operating the underlying infrastructure. In comparison
to traditional socket-based network stacks, RDMA-based networks offer higher
throughput, lower latency and reduced CPU overhead. However, transparent
checkpoint and migration operations become much more difficult. The key reason
is that the OS is removed from the critical path of communication. As a result,
some of the communication state itself resides in the NIC hardware and is no
more under the direct control of the OS. This control includes especially the
support for virtualisation of communication which is needed for live migration
of communication partners. In this paper, we propose the basic principles
required to implement a migration-capable RDMA-based network. We recommend some
changes at the software level and small changes at the hardware level. As a
proof of concept, we integrate the proposed changes into SoftRoCE, an
open-source kernel-level implementation of the RoCE protocol. We claim that
these changes introduce no runtime overhead when migration does not happen.
Finally, we develop a proof-of-concept implementation for migrating
containerised applications that use RDMA-based networks.
</summary>
    <author>
      <name>Maksym Planeta</name>
    </author>
    <author>
      <name>Jan Bierbaum</name>
    </author>
    <author>
      <name>Leo Sahaya Daphne Antony</name>
    </author>
    <author>
      <name>Torsten Hoefler</name>
    </author>
    <author>
      <name>Hermann Härtig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 13 figures, 4 tables, 1 listing</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.06988v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.06988v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.09206v1</id>
    <updated>2020-09-19T10:23:15Z</updated>
    <published>2020-09-19T10:23:15Z</published>
    <title>DEAP Cache: Deep Eviction Admission and Prefetching for Cache</title>
    <summary>  Recent approaches for learning policies to improve caching, target just one
out of the prefetching, admission and eviction processes. In contrast, we
propose an end to end pipeline to learn all three policies using machine
learning. We also take inspiration from the success of pretraining on large
corpora to learn specialized embeddings for the task. We model prefetching as a
sequence prediction task based on past misses. Following previous works
suggesting that frequency and recency are the two orthogonal fundamental
attributes for caching, we use an online reinforcement learning technique to
learn the optimal policy distribution between two orthogonal eviction
strategies based on them. While previous approaches used the past as an
indicator of the future, we instead explicitly model the future frequency and
recency in a multi-task fashion with prefetching, leveraging the abilities of
deep networks to capture futuristic trends and use them for learning eviction
and admission. We also model the distribution of the data in an online fashion
using Kernel Density Estimation in our approach, to deal with the problem of
caching non-stationary data. We present our approach as a "proof of concept" of
learning all three components of cache strategies using machine learning and
leave improving practical deployment for future work.
</summary>
    <author>
      <name>Ayush Mangal</name>
    </author>
    <author>
      <name>Jitesh Jain</name>
    </author>
    <author>
      <name>Keerat Kaur Guliani</name>
    </author>
    <author>
      <name>Omkar Bhalerao</name>
    </author>
    <link href="http://arxiv.org/abs/2009.09206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.12092v1</id>
    <updated>2020-11-24T13:54:55Z</updated>
    <published>2020-11-24T13:54:55Z</published>
    <title>Leveraging Architectural Support of Three Page Sizes with Trident</title>
    <summary>  Large pages are commonly deployed to reduce address translation overheads for
big-memory workloads. Modern x86-64 processors from Intel and AMD support two
large page sizes -- 1GB and 2MB. However, previous works on large pages have
primarily focused on 2MB pages, partly due to lack of substantial evidence on
the profitability of 1GB pages to real-world applications. We argue that in
fact, inadequate system software support is responsible for a decade of
underutilized hardware support for 1GB pages.
  Through extensive experimentation on a real system, we demonstrate that 1GB
pages can improve performance over 2MB pages, and when used in tandem with 2MB
pages for an important set of applications; the support for the latter is
crucial but missing in current systems. Our design and implementation of
\trident{} in Linux fully exploit hardware supported large pages by dynamically
and transparently allocating 1GB, 2MB, and 4KB pages as deemed suitable.
\trident{} speeds up eight memory-intensive applications by {$18\%$}, on
average, over Linux's use of 2MB pages. We also propose \tridentpv{}, an
extension to \trident{} that effectively virtualizes 1GB pages via copy-less
promotion and compaction in the guest OS. Overall, this paper shows that even
GB-sized pages have considerable practical significance with adequate software
enablement, in turn motivating architects to continue investing/innovating in
large pages.
</summary>
    <author>
      <name>Venkat Sri Sai Ram</name>
    </author>
    <author>
      <name>Ashish Panwar</name>
    </author>
    <author>
      <name>Arkaprava Basu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 16 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.12092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.12092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.13848v1</id>
    <updated>2021-07-29T09:29:37Z</updated>
    <published>2021-07-29T09:29:37Z</published>
    <title>Revisiting Swapping in User-space with Lightweight Threading</title>
    <summary>  Memory-intensive applications, such as in-memory databases, caching systems
and key-value stores, are increasingly demanding larger main memory to fit
their working sets. Conventional swapping can enlarge the memory capacity by
paging out inactive pages to disks. However, the heavy I/O stack makes the
traditional kernel-based swapping suffers from several critical performance
issues.
  In this paper, we redesign the swapping system and propose LightSwap, an
high-performance user-space swapping scheme that supports paging with both
local SSDs and remote memories. First, to avoids kernel-involving, a novel page
fault handling mechanism is proposed to handle page faults in user-space and
further eliminates the heavy I/O stack with the help of user-space I/O drivers.
Second, we co-design Lightswap with light weight thread (LWT) to improve system
throughput and make it be transparent to user applications. Finally, we propose
a try-catch framework in Lightswap to deal with paging errors which are
exacerbated by the scaling in process technology.
  We implement Lightswap in our production-level system and evaluate it with
YCSB workloads running on memcached. Results show that Ligthswap reduces the
page faults handling latency by 3--5 times, and improves the throughput of
memcached by more than 40% compared with the stat-of-art swapping systems.
</summary>
    <author>
      <name>Kan Zhong</name>
    </author>
    <author>
      <name>Wenlin Cui</name>
    </author>
    <author>
      <name>Youyou Lu</name>
    </author>
    <author>
      <name>Quanzhang Liu</name>
    </author>
    <author>
      <name>Xiaodan Yan</name>
    </author>
    <author>
      <name>Qizhao Yuan</name>
    </author>
    <author>
      <name>Siwei Luo</name>
    </author>
    <author>
      <name>Keji Huang</name>
    </author>
    <link href="http://arxiv.org/abs/2107.13848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.13848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.09652v1</id>
    <updated>2022-01-24T13:17:51Z</updated>
    <published>2022-01-24T13:17:51Z</published>
    <title>DuVisor: a User-level Hypervisor Through Delegated Virtualization</title>
    <summary>  Today's mainstream virtualization systems comprise of two cooperative
components: a kernel-resident driver that accesses virtualization hardware and
a user-level helper process that provides VM management and I/O virtualization.
However, this virtualization architecture has intrinsic issues in both security
(a large attack surface) and performance. While there is a long thread of work
trying to minimize the kernel-resident driver by offloading functions to user
mode, they face a fundamental tradeoff between security and performance: more
offloading may reduce the kernel attack surface, yet increase the runtime ring
crossings between the helper process and the driver, and thus more performance
cost.
  This paper explores a new design called delegated virtualization, which
completely separates the control plane (the kernel driver) from the data plane
(the helper process) and thus eliminates the kernel driver from runtime
intervention. The resulting user-level hypervisor, called DuVisor, can handle
all VM operations without trapping into the kernel once the kernel driver has
done the initialization. DuVisor retrofits existing hardware virtualization
support with a new delegated virtualization extension to directly handle VM
exits, configure virtualization registers, manage the stage-2 page table and
virtual devices in user mode. We have implemented the hardware extension on an
open-source RISC-V CPU and built a Rust-based hypervisor atop the hardware.
Evaluation on FireSim shows that DuVisor outperforms KVM by up to 47.96\% in a
variety of real-world applications and significantly reduces the attack
surface.
</summary>
    <author>
      <name>Jiahao Chen</name>
    </author>
    <author>
      <name>Dingji Li</name>
    </author>
    <author>
      <name>Zeyu Mi</name>
    </author>
    <author>
      <name>Yuxuan Liu</name>
    </author>
    <author>
      <name>Binyu Zang</name>
    </author>
    <author>
      <name>Haibing Guan</name>
    </author>
    <author>
      <name>Haibo Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.09652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.09652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.05732v2</id>
    <updated>2022-06-24T09:55:17Z</updated>
    <published>2022-02-11T16:08:43Z</published>
    <title>CAP-VMs: Capability-Based Isolation and Sharing for Microservices</title>
    <summary>  Cloud stacks must isolate application components, while permitting efficient
data sharing between components deployed on the same physical host.
Traditionally, the MMU enforces isolation and permits sharing at page
granularity. MMU approaches, however, lead to cloud stacks with large TCBs in
kernel space, and page granularity requires inefficient OS interfaces for data
sharing. Forthcoming CPUs with hardware support for memory capabilities offer
new opportunities to implement isolation and sharing at a finer granularity.
  We describe cVMs, a new VM-like abstraction that uses memory capabilities to
isolate application components while supporting efficient data sharing, all
without mandating application code to be capability-aware. cVMs share a single
virtual address space safely, each having only capabilities to access its own
memory. A cVM may include a library OS, thus minimizing its dependency on the
cloud environment. cVMs efficiently exchange data through two capability-based
primitives assisted by a small trusted monitor: (i) an asynchronous read-write
interface to buffers shared between cVMs; and (ii) a call interface to transfer
control between cVMs. Using these two primitives, we build more expressive
mechanisms for efficient cross-cVM communication. Our prototype implementation
using CHERI RISC-V capabilities shows that cVMs isolate services (Redis and
Python) with low overhead while improving data sharing.
</summary>
    <author>
      <name>Vasily A. Sartakov</name>
    </author>
    <author>
      <name>Lluís Vilanova</name>
    </author>
    <author>
      <name>David Eyers</name>
    </author>
    <author>
      <name>Takahiro Shinagawa</name>
    </author>
    <author>
      <name>Peter Pietzuch</name>
    </author>
    <link href="http://arxiv.org/abs/2202.05732v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.05732v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.09365v1</id>
    <updated>2022-02-18T12:50:30Z</updated>
    <published>2022-02-18T12:50:30Z</published>
    <title>Migration-Based Synchronization</title>
    <summary>  A fundamental challenge in multi- and many-core systems is the correct
execution of concurrent access to shared data. A common drawback from existing
synchronization mechanisms is the loss of data locality as the shared data is
transferred between the accessing cores. In real-time systems, this is
especially important as knowledge about data access times is crucial to
establish bounds on execution times and guarantee the meeting of deadlines.We
propose in this paper a refinement of our previously sketched approach of
Migration-Based Synchronization (MBS) as well as its first practical
implementation. The core concept of MBS is the replacement of data migration
with control-flow migration to achieve synchronized memory accesses with
guaranteed data locality. This leads to both shorter and more predictable
execution times for critical sections. As MBS can be used as a substitute for
classical locks, it can be employed in legacy applications without code
alterations.We further examine how the gained data locality improves the
results of worst-case timing analyses and results in tighter bounds on
execution and response time. We reason about the similarity of MBS to existing
synchronization approaches and how it enables us to reuse existing analysis
techniques.Finally, we evaluate our prototype implementation, showing that MBS
can exploit data locality with similar overheads as traditional locking
mechanisms.
</summary>
    <author>
      <name>Stefan Reif</name>
    </author>
    <author>
      <name>Phillip Raffeck</name>
    </author>
    <author>
      <name>Luis Gerhorst</name>
    </author>
    <author>
      <name>Wolfgang Schröder-Preikschat</name>
    </author>
    <author>
      <name>Timo Hönig</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SBESC53686.2021.9628358</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SBESC53686.2021.9628358" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SBESC'21: Proceedings of the XI Brazilian Symposium on Computing
  Systems Engineering. 2021. IEEE, Pages 1-8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2202.09365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.09365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.00789v2</id>
    <updated>2023-06-22T19:13:54Z</updated>
    <published>2022-06-01T22:45:12Z</published>
    <title>Unikernel Linux (UKL)</title>
    <summary>  This paper presents Unikernel Linux (UKL), a path toward integrating
unikernel optimization techniques in Linux, a general purpose operating system.
UKL adds a configuration option to Linux allowing for a single, optimized
process to link with the kernel directly, and run at supervisor privilege. This
UKL process does not require application source code modification, only a
re-link with our, slightly modified, Linux kernel and glibc. Unmodified
applications show modest performance gains out of the box, and developers can
further optimize applications for more significant gains (e.g. 26% throughput
improvement for Redis). UKL retains support for co-running multiple user level
processes capable of communicating with the UKL process using standard IPC. UKL
preserves Linux's battle-tested codebase, community, and ecosystem of tools,
applications, and hardware support. UKL runs both on bare-metal and virtual
servers and supports multi-core execution. The changes to the Linux kernel are
modest (1250 LOC).
</summary>
    <author>
      <name>Ali Raza</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Boston University</arxiv:affiliation>
    </author>
    <author>
      <name>Thomas Unger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Boston University</arxiv:affiliation>
    </author>
    <author>
      <name>Matthew Boyd</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIT CSAIL</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Munson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Boston University</arxiv:affiliation>
    </author>
    <author>
      <name>Parul Sohal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Boston University</arxiv:affiliation>
    </author>
    <author>
      <name>Ulrich Drepper</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Red Hat</arxiv:affiliation>
    </author>
    <author>
      <name>Richard Jones</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Red Hat</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Bristot de Oliveira</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Red Hat</arxiv:affiliation>
    </author>
    <author>
      <name>Larry Woodman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Red Hat</arxiv:affiliation>
    </author>
    <author>
      <name>Renato Mancuso</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Boston University</arxiv:affiliation>
    </author>
    <author>
      <name>Jonathan Appavoo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Boston University</arxiv:affiliation>
    </author>
    <author>
      <name>Orran Krieger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Boston University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3552326.3587458</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3552326.3587458" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Added more results in the evaluation section. Improved overall
  writing and added diagrams to explain the architecture</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Eighteenth European Conference on Computer
  Systems (EuroSys 23), May 2023, Pages 590 - 605</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2206.00789v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.00789v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.07688v1</id>
    <updated>2022-07-15T18:17:48Z</updated>
    <published>2022-07-15T18:17:48Z</published>
    <title>3PO: Programmed Far-Memory Prefetching for Oblivious Applications</title>
    <summary>  Using memory located on remote machines, or far memory, as a swap space is a
promising approach to meet the increasing memory demands of modern datacenter
applications. Operating systems have long relied on prefetchers to mask the
increased latency of fetching pages from swap space to main memory.
Unfortunately, with traditional prefetching heuristics, performance still
degrades when applications use far memory. In this paper we propose a new
prefetching technique for far-memory applications. We focus our efforts on
memory-intensive, oblivious applications whose memory access patterns are
independent of their inputs, such as matrix multiplication. For this class of
applications we observe that we can perfectly prefetch pages without relying on
heuristics. However, prefetching perfectly without requiring significant
application modifications is challenging.
  In this paper we describe the design and implementation of 3PO, a system that
provides pre-planned prefetching for general oblivious applications. We
demonstrate that 3PO can accelerate applications, e.g., running them 30-150%
faster than with Linux's prefetcher with 20% local memory. We also use 3PO to
understand the fundamental software overheads of prefetching in a paging-based
system, and the minimum performance penalty that they impose when we run
applications under constrained local memory.
</summary>
    <author>
      <name>Christopher Branner-Augmon</name>
    </author>
    <author>
      <name>Narek Galstyan</name>
    </author>
    <author>
      <name>Sam Kumar</name>
    </author>
    <author>
      <name>Emmanuel Amaro</name>
    </author>
    <author>
      <name>Amy Ousterhout</name>
    </author>
    <author>
      <name>Aurojit Panda</name>
    </author>
    <author>
      <name>Sylvia Ratnasamy</name>
    </author>
    <author>
      <name>Scott Shenker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.07688v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.07688v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.01709v2</id>
    <updated>2022-09-07T03:57:32Z</updated>
    <published>2022-09-04T23:38:25Z</published>
    <title>SFS: Smart OS Scheduling for Serverless Functions</title>
    <summary>  Serverless computing enables a new way of building and scaling cloud
applications by allowing developers to write fine-grained serverless or cloud
functions. The execution duration of a cloud function is typically
short-ranging from a few milliseconds to hundreds of seconds. However, due to
resource contentions caused by public clouds' deep consolidation, the function
execution duration may get significantly prolonged and fail to accurately
account for the function's true resource usage. We observe that the function
duration can be highly unpredictable with huge amplification of more than 50x
for an open-source FaaS platform (OpenLambda). Our experiments show that the OS
scheduling policy of cloud functions' host server can have a crucial impact on
performance. The default Linux scheduler, CFS (Completely Fair Scheduler),
being oblivious to workloads, frequently context-switches short functions,
causing a turnaround time that is much longer than their service time.
  We propose SFS (Smart Function Scheduler),which works entirely in the user
space and carefully orchestrates existing Linux FIFO and CFS schedulers to
approximate Shortest Remaining Time First (SRTF). SFS uses two-level scheduling
that seamlessly combines a new FILTER policy with Linux CFS, to trade off
increased duration of long functions for significant performance improvement
for short functions. We implement {\proj} in the Linux user space and port it
to OpenLambda. Evaluation results show that SFS significantly improves short
functions' duration with a small impact on relatively longer functions,
compared to CFS.
</summary>
    <author>
      <name>Yuqi Fu</name>
    </author>
    <author>
      <name>Li Liu</name>
    </author>
    <author>
      <name>Haoliang Wang</name>
    </author>
    <author>
      <name>Yue Cheng</name>
    </author>
    <author>
      <name>Songqing Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SFS has been accepted to appear in SC'22 and nominated as a best
  student paper award finalist</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.01709v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.01709v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.08826v1</id>
    <updated>2022-09-19T08:16:09Z</updated>
    <published>2022-09-19T08:16:09Z</published>
    <title>Rapid Recovery of Program Execution Under Power Failures for Embedded
  Systems with NVM</title>
    <summary>  After power is switched on, recovering the interrupted program from the
initial state can cause negative impact. Some programs are even unrecoverable.
To rapid recovery of program execution under power failures, the execution
states of checkpoints are backed up by NVM under power failures for embedded
systems with NVM. However, frequent checkpoints will shorten the lifetime of
the NVM and incur significant write overhead. In this paper, the technique of
checkpoint setting triggered by function calls is proposed to reduce the write
on NVM. The evaluation results show an average of 99.8% and 80.5$% reduction on
NVM backup size for stack backup, compared to the log-based method and
step-based method. In order to better achieve this, we also propose
pseudo-function calls to increase backup points to reduce recovery costs, and
exponential incremental call-based backup methods to reduce backup costs in the
loop. To further avoid the content on NVM is cluttered and out of NVM, a method
to clean the contents on the NVM that are useless for restoration is proposed.
Based on aforementioned problems and techniques, the recovery technology is
proposed, and the case is used to analyze how to recover rapidly under
different power failures.
</summary>
    <author>
      <name>Min Jia</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">East China Normal University School of Computer Science and Technology, China</arxiv:affiliation>
    </author>
    <author>
      <name>Edwin Hsing. -M. Sha</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">East China Normal University School of Computer Science and Technology, China</arxiv:affiliation>
    </author>
    <author>
      <name>Qingfeng Zhuge</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">East China Normal University School of Computer Science and Technology, China</arxiv:affiliation>
    </author>
    <author>
      <name>Rui Xu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">East China Normal University School of Computer Science and Technology, China</arxiv:affiliation>
    </author>
    <author>
      <name>Shouzhen Gu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">East China Normal University Software Engineering Institute, China</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted for publication to Microprocessors and
  Microsystems in March 15, 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.08826v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.08826v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.13111v1</id>
    <updated>2022-09-27T01:56:19Z</updated>
    <published>2022-09-27T01:56:19Z</published>
    <title>HMM-V: Heterogeneous Memory Management for Virtualization</title>
    <summary>  The memory demand of virtual machines (VMs) is increasing, while DRAM has
limited capacity and high power consumption. Non-volatile memory (NVM) is an
alternative to DRAM, but it has high latency and low bandwidth. We observe that
the VM with heterogeneous memory may incur up to a $1.5\times$ slowdown
compared to a DRAM VM, if not managed well. However, none of the
state-of-the-art heterogeneous memory management designs are customized for
virtualization on a real system.
  In this paper, we propose HMM-V, a Heterogeneous Memory Management system for
Virtualization. HMM-V automatically determines page hotness and migrates pages
between DRAM and NVM to achieve performance close to the DRAM system. First,
HMM-V tracks memory accesses through page table manipulation, but reduces the
cost by leveraging Intel page-modification logging (PML) and a multi-level
queue. Second, HMM-V quantifies the ``temperature'' of page and determines the
hot set with bucket-sorting. HMM-V then efficiently migrates pages with minimal
access pause and handles dirty pages with the assistance of PML. Finally, HMM-V
provides pooling management to balance precious DRAM across multiple VMs to
maximize utilization and overall performance. HMM-V is implemented on a real
system with Intel Optane DC persistent memory. The four-VM co-running results
show that HMM-V outperforms NUMA balancing and hardware management (Intel
Optane memory mode) by $51\%$ and $31\%$, respectively.
</summary>
    <author>
      <name>Sai sha</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Peking University</arxiv:affiliation>
    </author>
    <author>
      <name>Chuandong Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Peking University</arxiv:affiliation>
    </author>
    <author>
      <name>Yingwei Luo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Peking University</arxiv:affiliation>
    </author>
    <author>
      <name>Xiaolin Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Peking University</arxiv:affiliation>
    </author>
    <author>
      <name>Zhenlin Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Michigan Technological University</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2209.13111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.13111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.08818v1</id>
    <updated>2022-10-17T08:08:09Z</updated>
    <published>2022-10-17T08:08:09Z</published>
    <title>The Digital Foundation Platform -- A Multi-layered SOA Architecture for
  Intelligent Connected Vehicle Operating System</title>
    <summary>  Legacy AD/ADAS development from OEMs centers around developing functions on
ECUs using services provided by AUTOSAR Classic Platform (CP) to meet
automotive-grade and mass-production requirements. The AUTOSAR CP couples
hardware and software components statically and encounters challenges to
provide sufficient capacities for the processing of high-level intelligent
driving functions, whereas the new platform, AUTOSAR Adaptive Platform (AP) is
designed to support dynamically communication and provide richer services and
function abstractions for those resource-intensive (memory, CPU) applications.
Yet for both platforms, application development and the supporting system
software are still closely coupled together, and this makes application
development and the enhancement less scalable and flexible, resulting in longer
development cycles and slower time-to-market. This paper presents a
multi-layered, service-oriented intelligent driving operating system foundation
(we named it as Digital Foundation Platform) that provides abstractions for
easier adoption of heterogeneous computing hardware. It features a multi-layer
SOA software architecture with each layer providing adaptive service API at
north-bound for application developers. The proposed Digital Foundation
Platform (DFP) has significant advantages of decoupling hardware, operating
system core, middle-ware, functional software and application software
development. It provides SOA at multiple layers and enables application
developers from OEMs, to customize and develop new applications or enhance
existing applications with new features, either in autonomous domain or
intelligent cockpit domain, with great agility, and less code through
re-usability, and thus reduce the time-to-market.
</summary>
    <author>
      <name>David Yu</name>
    </author>
    <author>
      <name>Andy Xiao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4271/2022-01-0107</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4271/2022-01-0107" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WCX SAE World Congress Experience 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.08818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.08818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.08934v1</id>
    <updated>2022-10-17T10:55:15Z</updated>
    <published>2022-10-17T10:55:15Z</published>
    <title>RIO: Order-Preserving and CPU-Efficient Remote Storage Access</title>
    <summary>  Modern NVMe SSDs and RDMA networks provide dramatically higher bandwidth and
concurrency. Existing networked storage systems (e.g., NVMe over Fabrics) fail
to fully exploit these new devices due to inefficient storage ordering
guarantees. Severe synchronous execution for storage order in these systems
stalls the CPU and I/O devices and lowers the CPU and I/O performance
efficiency of the storage system.
  We present Rio, a new approach to the storage order of remote storage access.
The key insight in Rio is that the layered design of the software stack, along
with the concurrent and asynchronous network and storage devices, makes the
storage stack conceptually similar to the CPU pipeline. Inspired by the CPU
pipeline that executes out-of-order and commits in-order, Rio introduces the
I/O pipeline that allows internal out-of-order and asynchronous execution for
ordered write requests while offering intact external storage order to
applications. Together with merging consecutive ordered requests, these design
decisions make for write throughput and CPU efficiency close to that of
orderless requests.
  We implement Rio in Linux NVMe over RDMA stack, and further build a file
system named RioFS atop Rio. Evaluations show that Rio outperforms Linux NVMe
over RDMA and a state-of-the-art storage stack named Horae by two orders of
magnitude and 4.9 times on average in terms of throughput of ordered write
requests, respectively. RioFS increases the throughput of RocksDB by 1.9 times
and 1.5 times on average, against Ext4 and HoraeFS, respectively.
</summary>
    <author>
      <name>Xiaojian Liao</name>
    </author>
    <author>
      <name>Zhe Yang</name>
    </author>
    <author>
      <name>Jiwu Shu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3552326.3567495</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3552326.3567495" rel="related"/>
    <link href="http://arxiv.org/abs/2210.08934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.08934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.11185v5</id>
    <updated>2023-08-18T16:44:47Z</updated>
    <published>2022-10-20T11:57:36Z</published>
    <title>Cutting-plane algorithms for preemptive uniprocessor real-time
  scheduling problems</title>
    <summary>  Fixed-point iteration algorithms like RTA (response time analysis) and QPA
(quick processor-demand analysis) are arguably the most popular ways of solving
schedulability problems for preemptive uniprocessor FP (fixed-priority) and EDF
(earliest-deadline-first) systems. Several IP (integer program) formulations
have also been proposed for these problems, but it is unclear whether the
algorithms for solving these formulations are related to RTA and QPA. By
discovering connections between the problems and the algorithms, we show that
RTA and QPA are, in fact, suboptimal cutting-plane algorithms for specific IP
formulations of FP and EDF schedulability, where optimality is defined with
respect to convergence rate. We propose optimal cutting-plane algorithms for
these IP formulations. We compare the new algorithms with RTA and QPA on large
collections of synthetic systems to gauge the improvement in convergence rates
and running times.
</summary>
    <author>
      <name>Abhishek Singh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11241-023-09408-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11241-023-09408-y" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">45 pages, 5 figures. Changes in v2: new terms like CP-KERN are added
  to explain ideas more clearly; models include release jitter. Changes in v3:
  typos are fixed; evaluation section is modified so that it is in sync with
  public code. Changes in v4: algorithm, theoretical and empirical analyses are
  improved. Changes in v5: minor structural changes, acknowledgements added</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.11185v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.11185v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3; F.2.1; G.1.6; I.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.00072v1</id>
    <updated>2022-12-30T23:37:39Z</updated>
    <published>2022-12-30T23:37:39Z</published>
    <title>LeaFTL: A Learning-Based Flash Translation Layer for Solid-State Drives</title>
    <summary>  In modern solid-state drives (SSDs), the indexing of flash pages is a
critical component in their storage controllers. It not only affects the data
access performance, but also determines the efficiency of the precious
in-device DRAM resource. A variety of address mapping schemes and optimization
techniques have been proposed. However, most of them were developed with
human-driven heuristics. They cannot automatically capture diverse data access
patterns at runtime in SSD controllers, which leaves a large room for
improvement. In this paper, we present a learning-based flash translation layer
(FTL), named LeaFTL, which learns the address mapping to tolerate dynamic data
access patterns via linear regression at runtime. By grouping a large set of
mapping entries into a learned segment, it significantly reduces the memory
footprint of the address mapping table, which further benefits the data caching
in SSD controllers. LeaFTL also employs various optimization techniques,
including out-of-band metadata verification to tolerate mispredictions,
optimized flash allocation, and dynamic compaction of learned index segments.
We implement LeaFTL with an SSD simulator and evaluate it with various storage
workloads. LeaFTL saves the memory consumption of the mapping table by 2.9x on
average and improves the storage performance by 1.4x on average, in comparison
with state-of-the-art FTL schemes.
</summary>
    <author>
      <name>Jinghan Sun</name>
    </author>
    <author>
      <name>Shaobo Li</name>
    </author>
    <author>
      <name>Yunxin Sun</name>
    </author>
    <author>
      <name>Chao Sun</name>
    </author>
    <author>
      <name>Dejan Vucinic</name>
    </author>
    <author>
      <name>Jian Huang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3575693.3575744</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3575693.3575744" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted at the 28th Conference on Architectural
  Support for Programming Languages and Operating Systems (ASPLOS 2023)</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.00072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.00072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.04546v1</id>
    <updated>2023-01-11T16:15:17Z</updated>
    <published>2023-01-11T16:15:17Z</published>
    <title>From MMU to MPU: adaptation of the Pip kernel to constrained devices</title>
    <summary>  This article presents a hardware-based memory isolation solution for
constrained devices. Existing solutions target high-end embedded systems
(typically ARM Cortex-A with a Memory Management Unit, MMU) such as seL4 or Pip
(formally verified kernels) or target low-end devices such as ACES, MINION,
TrustLite, EwoK but with limited flexibility by proposing a single level of
isolation. Our approach consists in adapting Pip to inherit its flexibility
(multiple levels of isolation) but using the Memory Protection Unit (MPU)
instead of the MMU since the MPU is commonly available on constrained embedded
systems (typically ARMv7 Cortex-M4 or ARMv8 Cortex-M33 and similar devices).
This paper describes our design of Pip-MPU (Pip's variant based on the MPU) and
the rationale behind our choices. We validate our proposal with an
implementation on an nRF52840 development kit and we perform various
evaluations such as memory footprint, CPU cycles and energy consumption. We
demonstrate that although our prototyped Pip-MPU causes a 16% overhead on both
performance and energy consumption, it can reduce the attack surface of the
accessible application memory from 100% down to 2% and the privileged
operations by 99%. Pip-MPU takes less than 10 kB of Flash (6 kB for its core
components) and 550 B of RAM.
</summary>
    <author>
      <name>Nicolas Dejon</name>
    </author>
    <author>
      <name>Chrystel Gaber</name>
    </author>
    <author>
      <name>Gilles Grimaud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3rd International Conference on Internet of Things &amp; Embedded Systems
  (IoTE 2022), 23-23 December 2022, Sydney (Australia)</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.04546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.04546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.09002v1</id>
    <updated>2023-02-17T17:13:35Z</updated>
    <published>2023-02-17T17:13:35Z</published>
    <title>Virtualization of Tiny Embedded Systems with a robust real-time capable
  and extensible Stack Virtual Machine REXAVM supporting Material-integrated
  Intelligent Systems and Tiny Machine Learning</title>
    <summary>  In the past decades, there has been a significant increase in sensor density
and sensor deployment, driven by a significant miniaturization and decrease in
size down to the chip level, addressing ubiquitous computing, edge computing,
as well as distributed sensor networks. Material-integrated and intelligent
systems (MIIS) provide the next integration and application level, but they
create new challenges and introduce hard constraints (resources, energy supply,
communication, resilience, and security). Commonly, low-resource systems are
statically programmed processors with application-specific software or
application-specific hardware (FPGA). This work demonstrates the need for and
solution to virtualization in such low-resource and constrained systems towards
resilient distributed sensor and cyber-physical networks using a unified
low-resource, customizable, and real-time capable embedded and extensible stack
virtual machine (REXAVM) that can be implemented and cooperate in both software
and hardware. In a holistic architecture approach, the VM specifically
addresses digital signal processing and tiny machine learning. The REXAVM is
highly customizable through the use of VM program code generators at compile
time and incremental code processing at run time. The VM uses an integrated,
highly efficient just-in-time compiler to create Bytecode from text code. This
paper shows and evaluates the suitability of the proposed VM architecture for
operationally equivalent software and hardware (FPGA) implementations. Specific
components supporting tiny ML and DSP using fixed-point arithmetic with respect
to efficiency and accuracy are discussed. An extended use-case section
demonstrates the usability of the introduced VM architecture for a broad range
of applications.
</summary>
    <author>
      <name>Stefan Bosse</name>
    </author>
    <author>
      <name>Sarah Bornemann</name>
    </author>
    <author>
      <name>Björn Lüssem</name>
    </author>
    <link href="http://arxiv.org/abs/2302.09002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.09002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.10366v1</id>
    <updated>2023-02-20T23:54:04Z</updated>
    <published>2023-02-20T23:54:04Z</published>
    <title>Programmable System Call Security with eBPF</title>
    <summary>  System call filtering is a widely used security mechanism for protecting a
shared OS kernel against untrusted user applications. However, existing system
call filtering techniques either are too expensive due to the context switch
overhead imposed by userspace agents, or lack sufficient programmability to
express advanced policies. Seccomp, Linux's system call filtering module, is
widely used by modern container technologies, mobile apps, and system
management services. Despite the adoption of the classic BPF language (cBPF),
security policies in Seccomp are mostly limited to static allow lists,
primarily because cBPF does not support stateful policies. Consequently, many
essential security features cannot be expressed precisely and/or require kernel
modifications.
  In this paper, we present a programmable system call filtering mechanism,
which enables more advanced security policies to be expressed by leveraging the
extended BPF language (eBPF). More specifically, we create a new Seccomp eBPF
program type, exposing, modifying or creating new eBPF helper functions to
safely manage filter state, access kernel and user state, and utilize
synchronization primitives. Importantly, our system integrates with existing
kernel privilege and capability mechanisms, enabling unprivileged users to
install advanced filters safely. Our evaluation shows that our eBPF-based
filtering can enhance existing policies (e.g., reducing the attack surface of
early execution phase by up to 55.4% for temporal specialization), mitigate
real-world vulnerabilities, and accelerate filters.
</summary>
    <author>
      <name>Jinghao Jia</name>
    </author>
    <author>
      <name>YiFei Zhu</name>
    </author>
    <author>
      <name>Dan Williams</name>
    </author>
    <author>
      <name>Andrea Arcangeli</name>
    </author>
    <author>
      <name>Claudio Canella</name>
    </author>
    <author>
      <name>Hubertus Franke</name>
    </author>
    <author>
      <name>Tobin Feldman-Fitzthum</name>
    </author>
    <author>
      <name>Dimitrios Skarlatos</name>
    </author>
    <author>
      <name>Daniel Gruss</name>
    </author>
    <author>
      <name>Tianyin Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2302.10366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.10366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.09725v1</id>
    <updated>2023-03-17T01:52:02Z</updated>
    <published>2023-03-17T01:52:02Z</published>
    <title>Policy/mechanism separation in the Warehouse-Scale OS</title>
    <summary>  "As many of us know from bitter experience, the policies provided in extant
operating systems, which are claimed to work well and behave fairly 'on the
average', often fail to do so in the special cases important to us" [Wulf et
al. 1974]. Written in 1974, these words motivated moving policy decisions into
user-space. Today, as warehouse-scale computers (WSCs) have become ubiquitous,
it is time to move policy decisions away from individual servers altogether.
Built-in policies are complex and often exhibit bad performance at scale.
Meanwhile, the highly-controlled WSC setting presents opportunities to improve
performance and predictability.
  We propose moving all policy decisions from the OS kernel to the cluster
manager (CM), in a new paradigm we call Grape CM. In this design, the role of
the kernel is reduced to monitoring, sending metrics to the CM, and executing
policy decisions made by the CM. The CM uses metrics from all kernels across
the WSC to make informed policy choices, sending commands back to each kernel
in the cluster. We claim that Grape CM will improve performance, transparency,
and simplicity. Our initial experiments show how the CM can identify the
optimal set of huge pages for any workload or improve memcached latency by 15%.
</summary>
    <author>
      <name>Mark Mansi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Wisconsin-Madison</arxiv:affiliation>
    </author>
    <author>
      <name>Michael M. Swift</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Wisconsin-Madison</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures, 2 code listings</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.09725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.09725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.7; D.4.2; D.4.8; C.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.10204v2</id>
    <updated>2023-03-25T22:22:19Z</updated>
    <published>2023-03-17T18:48:50Z</published>
    <title>ESP32: QEMU Emulation within a Docker Container</title>
    <summary>  The ESP32 is a popular microcontroller from Espressif that can be used in
many embedded applications. Robotic joints, smart car chargers, beer vat
agitators and automated bread mixers are a few examples where this
system-on-a-chip excels. It is cheap to buy and has a number of vendors
providing low-cost development board kits that come with the microcontroller
and many external connection points with peripherals. There is a large software
ecosystem for the ESP32. Espressif maintains an SDK containing many C-language
sample projects providing a starting point for a huge variety of software
services and I/O needs. Third party projects provide additional sample code as
well as support for other programming languages. For example, MicroPython is a
mature project with sample code and officially supported by Espressif. The SDK
provides tools to not just build an application but also merge a flash image,
flash to the microcontroller and monitor the output. Is it possible to build
the ESP32 load and emulate on another host OS? This paper explores the QEMU
emulator and its ability to emulate the ethernet interface for the guest OS.
Additionally, we look into the concept of containerizing the entire emulator
and ESP32 load package such that a microcontroller flash image can successfully
run with a one-step deployment of a Docker container.
</summary>
    <author>
      <name>Michael Howard</name>
    </author>
    <author>
      <name>R. Bruce Irvin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages and 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.10204v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.10204v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.03864v2</id>
    <updated>2023-10-11T07:50:28Z</updated>
    <published>2023-04-07T23:25:48Z</published>
    <title>SGDP: A Stream-Graph Neural Network Based Data Prefetcher</title>
    <summary>  Data prefetching is important for storage system optimization and access
performance improvement. Traditional prefetchers work well for mining access
patterns of sequential logical block address (LBA) but cannot handle complex
non-sequential patterns that commonly exist in real-world applications. The
state-of-the-art (SOTA) learning-based prefetchers cover more LBA accesses.
However, they do not adequately consider the spatial interdependencies between
LBA deltas, which leads to limited performance and robustness. This paper
proposes a novel Stream-Graph neural network-based Data Prefetcher (SGDP).
Specifically, SGDP models LBA delta streams using a weighted directed graph
structure to represent interactive relations among LBA deltas and further
extracts hybrid features by graph neural networks for data prefetching. We
conduct extensive experiments on eight real-world datasets. Empirical results
verify that SGDP outperforms the SOTA methods in terms of the hit ratio by
6.21%, the effective prefetching ratio by 7.00%, and speeds up inference time
by 3.13X on average. Besides, we generalize SGDP to different variants by
different stream constructions, further expanding its application scenarios and
demonstrating its robustness. SGDP offers a novel data prefetching solution and
has been verified in commercial hybrid storage systems in the experimental
phase. Our codes and appendix are available at
https://github.com/yyysjz1997/SGDP/.
</summary>
    <author>
      <name>Yiyuan Yang</name>
    </author>
    <author>
      <name>Rongshang Li</name>
    </author>
    <author>
      <name>Qiquan Shi</name>
    </author>
    <author>
      <name>Xijun Li</name>
    </author>
    <author>
      <name>Gang Hu</name>
    </author>
    <author>
      <name>Xing Li</name>
    </author>
    <author>
      <name>Mingxuan Yuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by International Joint Conference on Neural Networks (IJCNN
  2023)</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.03864v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.03864v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.17222v2</id>
    <updated>2023-07-07T15:23:13Z</updated>
    <published>2023-05-26T19:30:48Z</published>
    <title>Karma: Resource Allocation for Dynamic Demands</title>
    <summary>  We consider the problem of fair resource allocation in a system where user
demands are dynamic, that is, where user demands vary over time. Our key
observation is that the classical max-min fairness algorithm for resource
allocation provides many desirable properties (e.g., Pareto efficiency,
strategy-proofness, and fairness), but only under the strong assumption of user
demands being static over time. For the realistic case of dynamic user demands,
the max-min fairness algorithm loses one or more of these properties.
  We present Karma, a new resource allocation mechanism for dynamic user
demands. The key technical contribution in Karma is a credit-based resource
allocation algorithm: in each quantum, users donate their unused resources and
are assigned credits when other users borrow these resources; Karma carefully
orchestrates the exchange of credits across users (based on their instantaneous
demands, donated resources and borrowed resources), and performs prioritized
resource allocation based on users' credits. We theoretically establish Karma
guarantees related to Pareto efficiency, strategy-proofness, and fairness for
dynamic user demands. Empirical evaluations over production workloads show that
these properties translate well into practice: Karma is able to reduce
disparity in performance across users to a bare minimum while maintaining
Pareto-optimal system-wide performance.
</summary>
    <author>
      <name>Midhul Vuppalapati</name>
    </author>
    <author>
      <name>Giannis Fikioris</name>
    </author>
    <author>
      <name>Rachit Agarwal</name>
    </author>
    <author>
      <name>Asaf Cidon</name>
    </author>
    <author>
      <name>Anurag Khandelwal</name>
    </author>
    <author>
      <name>Eva Tardos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of paper accepted to USENIX OSDI 2023 with proofs of
  theoretical guarantees</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.17222v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.17222v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.16271v2</id>
    <updated>2023-07-27T13:10:40Z</updated>
    <published>2023-06-28T14:52:51Z</published>
    <title>Joint Time-and Event-Triggered Scheduling in the Linux Kernel</title>
    <summary>  There is increasing interest in using Linux in the real-time domain due to
the emergence of cloud and edge computing, the need to decrease costs, and the
growing number of complex functional and non-functional requirements of
real-time applications. Linux presents a valuable opportunity as it has rich
hardware support, an open-source development model, a well-established
programming environment, and avoids vendor lock-in. Although Linux was
initially developed as a general-purpose operating system, some real-time
capabilities have been added to the kernel over many years to increase its
predictability and reduce its scheduling latency. Unfortunately, Linux
currently has no support for time-triggered (TT) scheduling, which is widely
used in the safety-critical domain for its determinism, low run-time scheduling
latency, and strong isolation properties. We present an enhancement of the
Linux scheduler as a new low-overhead TT scheduling class to support offline
table-driven scheduling of tasks on multicore Linux nodes. Inspired by the Slot
shifting algorithm, we complement the new scheduling class with a low overhead
slot shifting manager running on a non-time-triggered core to provide
guaranteed execution time to real-time aperiodic tasks by using the slack of
the time-triggered tasks and avoiding high-overhead table regeneration for
adding new periodic tasks. Furthermore, we evaluate our implementation on
server-grade hardware with Intel Xeon Scalable Processor.
</summary>
    <author>
      <name>Gautam Gala</name>
    </author>
    <author>
      <name>Isser Kadusale</name>
    </author>
    <author>
      <name>Gerhard Fohler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the 17th annual workshop on Operating Systems Platforms
  for Embedded Real-Time applications (OSPERT) workshop 2023 co-hosted with
  35th Euromicro conference on Real-time systems. OSPERT proceedings:
  https://www.ecrts.org/wp-content/uploads/2023/07/ospert23-proceedings.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.16271v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.16271v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68N25" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.11866v1</id>
    <updated>2023-07-21T19:14:40Z</updated>
    <published>2023-07-21T19:14:40Z</published>
    <title>A Survey on the Integration of NAND Flash Storage in the Design of File
  Systems and the Host Storage Software Stack</title>
    <summary>  With the ever-increasing amount of data generate in the world, estimated to
reach over 200 Zettabytes by 2025, pressure on efficient data storage systems
is intensifying. The shift from HDD to flash-based SSD provides one of the most
fundamental shifts in storage technology, increasing performance capabilities
significantly. However, flash storage comes with different characteristics than
prior HDD storage technology. Therefore, storage software was unsuitable for
leveraging the capabilities of flash storage. As a result, a plethora of
storage applications have been design to better integrate with flash storage
and align with flash characteristics.
  In this literature study we evaluate the effect the introduction of flash
storage has had on the design of file systems, which providing one of the most
essential mechanisms for managing persistent storage. We analyze the mechanisms
for effectively managing flash storage, managing overheads of introduced design
requirements, and leverage the capabilities of flash storage. Numerous methods
have been adopted in file systems, however prominently revolve around similar
design decisions, adhering to the flash hardware constrains, and limiting
software intervention. Future design of storage software remains prominent with
the constant growth in flash-based storage devices and interfaces, providing an
increasing possibility to enhance flash integration in the host storage
software stack.
</summary>
    <author>
      <name>Nick Tehrany</name>
    </author>
    <author>
      <name>Krijn Doekemeijer</name>
    </author>
    <author>
      <name>Animesh Trivedi</name>
    </author>
    <link href="http://arxiv.org/abs/2307.11866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.11866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.15996v1</id>
    <updated>2023-09-27T20:21:37Z</updated>
    <published>2023-09-27T20:21:37Z</published>
    <title>Loupe: Driving the Development of OS Compatibility Layers</title>
    <summary>  Supporting mainstream applications is fundamental for a new OS to have
impact. It is generally achieved by developing a layer of compatibility
allowing applications developed for a mainstream OS like Linux to run
unmodified on the new OS. Building such a layer, as we show, results in large
engineering inefficiencies due to the lack of efficient methods to precisely
measure the OS features required by a set of applications.
  We propose Loupe, a novel method based on dynamic analysis that determines
the OS features that need to be implemented in a prototype OS to bring support
for a target set of applications and workloads. Loupe guides and boosts OS
developers as they build compatibility layers, prioritizing which features to
implement in order to quickly support many applications as early as possible.
We apply our methodology to 100+ applications and several OSes currently under
development, demonstrating high engineering effort savings vs. existing
approaches: for example, for the 62 applications supported by the OSv kernel,
we show that using Loupe, would have required implementing only 37 system calls
vs. 92 for the non-systematic process followed by OSv developers.
  We study our measurements and extract novel key insights. Overall, we show
that the burden of building compatibility layers is significantly less than
what previous works suggest: in some cases, only as few as 20% of system calls
reported by static analysis, and 50% of those reported by naive dynamic
analysis need an implementation for an application to successfully run standard
benchmarks.
</summary>
    <author>
      <name>Hugo Lefeuvre</name>
    </author>
    <author>
      <name>Gaulthier Gain</name>
    </author>
    <author>
      <name>Vlad-Andrei Bădoiu</name>
    </author>
    <author>
      <name>Daniel Dinca</name>
    </author>
    <author>
      <name>Vlad-Radu Schiller</name>
    </author>
    <author>
      <name>Costin Raiciu</name>
    </author>
    <author>
      <name>Felipe Huici</name>
    </author>
    <author>
      <name>Pierre Olivier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to appear at ASPLOS'24
  (https://www.asplos-conference.org/asplos2024/)</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.15996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.15996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.00933v2</id>
    <updated>2024-01-18T15:16:32Z</updated>
    <published>2023-10-02T06:56:29Z</published>
    <title>Case Study: Securing MMU-less Linux Using CHERI</title>
    <summary>  MMU-less Linux variant lacks security because it does not have protection or
isolation mechanisms. It also does not use MPUs as they do not fit with its
software model because of the design drawbacks of MPUs (\ie coarse-grained
protection with fixed number of protected regions). We secure the existing
MMU-less Linux version of the RISC-V port using CHERI. CHERI is a
hardware-software capability-based system that extends the ISA, toolchain,
programming languages, operating systems, and applications in order to provide
complete pointer and memory safety. We believe that CHERI could provide
significant security guarantees for high-end dynamic MMU-less embedded systems
at lower costs, compared to MMUs and MPUs, by: 1) building the entire software
stack in pure-capability CHERI C mode which provides complete spatial memory
safety at the kernel and user-level, 2) isolating user programs as separate
ELFs, each with its own CHERI-based capability table; this provides spatial
memory safety similar to what the MMU offers (\ie user programs cannot access
each other's memory), 3) isolating user programs from the kernel as the kernel
has its own capability table from the users and vice versa, and 4)
compartmentalising kernel modules using CompartOS' linkage-based
compartmentalisation. This offers a new security front that is not possible
using the current MMU-based Linux, where vulnerable/malicious kernel modules
(\eg device drivers) executing in the kernel space would not compromise or take
down the entire system. These are the four main contributions of this paper,
presenting novel CHERI-based mechanisms to secure MMU-less embedded Linux.
</summary>
    <author>
      <name>Hesham Almatary</name>
    </author>
    <author>
      <name>Alfredo Mazzinghi</name>
    </author>
    <author>
      <name>Robert N. M. Watson</name>
    </author>
    <link href="http://arxiv.org/abs/2310.00933v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.00933v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.07227v2</id>
    <updated>2024-09-27T22:50:23Z</updated>
    <published>2023-11-13T10:48:36Z</published>
    <title>CARTOS: A Charging-Aware Real-Time Operating System for Intermittent
  Batteryless Devices</title>
    <summary>  This paper presents CARTOS, a charging-aware real-time operating system
designed to enhance the functionality of intermittently-powered batteryless
devices (IPDs) for various Internet of Things (IoT) applications. While IPDs
offer significant advantages such as extended lifespan and operability in
extreme environments, they pose unique challenges, including the need to ensure
forward progress of program execution amidst variable energy availability and
maintaining reliable real-time time behavior during power disruptions. To
address these challenges, CARTOS introduces a mixed-preemption scheduling model
that classifies tasks into computational and peripheral tasks, and ensures
their efficient and timely execution by adopting just-in-time checkpointing for
divisible computation tasks and uninterrupted execution for indivisible
peripheral tasks. CARTOS also supports processing chains of tasks with
precedence constraints and adapts its scheduling in response to environmental
changes to offer continuous execution under diverse conditions. CARTOS is
implemented with new APIs and components added to FreeRTOS but is designed for
portability to other embedded RTOSs. Through real hardware experiments and
simulations, CARTOS exhibits superior performance over state-of-the-art
methods, demonstrating that it can serve as a practical platform for developing
resilient, real-time sensing applications on IPDs.
</summary>
    <author>
      <name>Mohsen Karimi</name>
    </author>
    <author>
      <name>Yidi Wang</name>
    </author>
    <author>
      <name>Youngbin Kim</name>
    </author>
    <author>
      <name>Yoojin Lim</name>
    </author>
    <author>
      <name>Hyoseung Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2311.07227v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.07227v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.07923v2</id>
    <updated>2023-12-08T00:39:26Z</updated>
    <published>2023-11-14T05:49:16Z</published>
    <title>bpftime: userspace eBPF Runtime for Uprobe, Syscall and Kernel-User
  Interactions</title>
    <summary>  In kernel-centric operations, the uprobe component of eBPF frequently
encounters performance bottlenecks, largely attributed to the overheads borne
by context switches. Transitioning eBPF operations to user space bypasses these
hindrances, thereby optimizing performance. This also enhances configurability
and obviates the necessity for root access or privileges for kernel eBPF,
subsequently minimizing the kernel attack surface. This paper introduces
bpftime, a novel user-space eBPF runtime, which leverages binary rewriting to
implement uprobe and syscall hook capabilities. Through bpftime, userspace
uprobes achieve a 10x speed enhancement compared to their kernel counterparts
without requiring dual context switches. Additionally, this runtime facilitates
the programmatic hooking of syscalls within a process, both safely and
efficiently. Bpftime can be seamlessly attached to any running process,
limiting the need for either a restart or manual recompilation. Our
implementation also extends to interprocess eBPF Maps within shared memory,
catering to summary aggregation or control plane communication requirements.
Compatibility with existing eBPF toolchains such as clang and libbpf is
maintained, not only simplifying the development of user-space eBPF without
necessitating any modifications but also supporting CO-RE through BTF. Through
bpftime, we not only enhance uprobe performance but also extend the versatility
and user-friendliness of eBPF runtime in user space, paving the way for more
efficient and secure kernel operations.
</summary>
    <author>
      <name>Yusheng Zheng</name>
    </author>
    <author>
      <name>Tong Yu</name>
    </author>
    <author>
      <name>Yiwei Yang</name>
    </author>
    <author>
      <name>Yanpeng Hu</name>
    </author>
    <author>
      <name>Xiaozheng Lai</name>
    </author>
    <author>
      <name>Andrew Quinn</name>
    </author>
    <link href="http://arxiv.org/abs/2311.07923v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.07923v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.10275v2</id>
    <updated>2023-11-30T04:14:30Z</updated>
    <published>2023-11-17T01:44:14Z</published>
    <title>Telescope: Telemetry at Terabyte Scale</title>
    <summary>  Data-hungry applications that require terabytes of memory have become
widespread in recent years. To meet the memory needs of these applications,
data centers are embracing tiered memory architectures with near and far memory
tiers. Precise, efficient, and timely identification of hot and cold data and
their placement in appropriate tiers is critical for performance in such
systems. Unfortunately, the existing state-of-the-art telemetry techniques for
hot and cold data detection are ineffective at the terabyte scale.
  We propose Telescope, a novel technique that profiles different levels of the
application's page table tree for fast and efficient identification of hot and
cold data. Telescope is based on the observation that, for a memory- and
TLB-intensive workload, higher levels of a page table tree are also frequently
accessed during a hardware page table walk. Hence, the hotness of the higher
levels of the page table tree essentially captures the hotness of its subtrees
or address space sub-regions at a coarser granularity. We exploit this insight
to quickly converge on even a few megabytes of hot data and efficiently
identify several gigabytes of cold data in terabyte-scale applications.
Importantly, such a technique can seamlessly scale to petabyte-scale
applications.
  Telescope's telemetry achieves 90%+ precision and recall at just 0.009%
single CPU utilization for microbenchmarks with a 5 TB memory footprint. Memory
tiering based on Telescope results in 5.6% to 34% throughput improvement for
real-world benchmarks with a 1-2 TB memory footprint compared to other
state-of-the-art telemetry techniques.
</summary>
    <author>
      <name>Alan Nair</name>
    </author>
    <author>
      <name>Sandeep Kumar</name>
    </author>
    <author>
      <name>Aravinda Prasad</name>
    </author>
    <author>
      <name>Andy Rudoff</name>
    </author>
    <author>
      <name>Sreenivas Subramoney</name>
    </author>
    <link href="http://arxiv.org/abs/2311.10275v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.10275v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.03858v4</id>
    <updated>2025-03-27T17:57:41Z</updated>
    <published>2023-12-06T19:11:15Z</published>
    <title>Empowering WebAssembly with Thin Kernel Interfaces</title>
    <summary>  Wasm is gaining popularity outside the Web as a well-specified low-level
binary format with ISA portability, low memory footprint and polyglot
targetability, enabling efficient in-process sandboxing of untrusted code.
Despite these advantages, Wasm adoption for new domains is often hindered by
the lack of many standard system interfaces which precludes reusability of
existing software and slows ecosystem growth.
  This paper proposes thin kernel interfaces for Wasm, which directly expose OS
userspace syscalls without breaking intra-process sandboxing, enabling a new
class of virtualization with Wasm as a universal binary format. By virtualizing
the bottom layer of userspace, kernel interfaces enable effortless application
ISA portability, compiler backend reusability, and armor programs with Wasm's
built-in control flow integrity and arbitrary code execution protection.
Furthermore, existing capability-based APIs for Wasm, such as WASI, can be
implemented as a Wasm module over kernel interfaces, improving reuse,
robustness, and portability through better layering. We present an
implementation of this concept for two kernels -- Linux and Zephyr -- by
extending a modern Wasm engine and evaluate our system's performance on a
number of sophisticated applications which can run for the first time on Wasm.
</summary>
    <author>
      <name>Arjun Ramesh</name>
    </author>
    <author>
      <name>Tianshu Huang</name>
    </author>
    <author>
      <name>Ben L. Titzer</name>
    </author>
    <author>
      <name>Anthony Rowe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3689031.3717470</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3689031.3717470" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work is published at EuroSys 2025, Rotterdam, Netherlands (March
  30 - April 3) 14 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Twentieth European Conference on Computer Systems (EuroSys 2025)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2312.03858v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.03858v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.06808v1</id>
    <updated>2023-12-11T19:45:01Z</updated>
    <published>2023-12-11T19:45:01Z</published>
    <title>BPF-oF: Storage Function Pushdown Over the Network</title>
    <summary>  Storage disaggregation, wherein storage is accessed over the network, is
popular because it allows applications to independently scale storage capacity
and bandwidth based on dynamic application demand. However, the added network
processing introduced by disaggregation can consume significant CPU resources.
In many storage systems, logical storage operations (e.g., lookups,
aggregations) involve a series of simple but dependent I/O access patterns.
Therefore, one way to reduce the network processing overhead is to execute
dependent series of I/O accesses at the remote storage server, reducing the
back-and-forth communication between the storage layer and the application. We
refer to this approach as \emph{remote-storage pushdown}. We present BPF-oF, a
new remote-storage pushdown protocol built on top of NVMe-oF, which enables
applications to safely push custom eBPF storage functions to a remote storage
server.
  The main challenge in integrating BPF-oF with storage systems is preserving
the benefits of their client-based in-memory caches. We address this challenge
by designing novel caching techniques for storage pushdown, including splitting
queries into separate in-memory and remote-storage phases and periodically
refreshing the client cache with sampled accesses from the remote storage
device. We demonstrate the utility of BPF-oF by integrating it with three
storage systems, including RocksDB, a popular persistent key-value store that
has no existing storage pushdown capability. We show BPF-oF provides
significant speedups in all three systems when accessed over the network, for
example improving RocksDB's throughput by up to 2.8$\times$ and tail latency by
up to 2.6$\times$.
</summary>
    <author>
      <name>Ioannis Zarkadas</name>
    </author>
    <author>
      <name>Tal Zussman</name>
    </author>
    <author>
      <name>Jeremy Carin</name>
    </author>
    <author>
      <name>Sheng Jiang</name>
    </author>
    <author>
      <name>Yuhong Zhong</name>
    </author>
    <author>
      <name>Jonas Pfefferle</name>
    </author>
    <author>
      <name>Hubertus Franke</name>
    </author>
    <author>
      <name>Junfeng Yang</name>
    </author>
    <author>
      <name>Kostis Kaffes</name>
    </author>
    <author>
      <name>Ryan Stutsman</name>
    </author>
    <author>
      <name>Asaf Cidon</name>
    </author>
    <link href="http://arxiv.org/abs/2312.06808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.06808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.03523v1</id>
    <updated>2024-01-07T15:50:17Z</updated>
    <published>2024-01-07T15:50:17Z</published>
    <title>Characterizing Physical Memory Fragmentation</title>
    <summary>  External fragmentation of physical memory occurs when adjacent differently
sized regions of allocated physical memory are freed at different times,
causing free memory to be physically discontiguous. It can significantly
degrade system performance and efficiency, such as reducing the ability to use
huge pages, a critical optimization on modern large-memory system. For decades
system developers have sought to avoid and mitigate fragmentation, but few
prior studies quantify and characterize it in production settings.
  Moreover, prior work often artificially fragments physical memory to create
more realistic performance evaluations, but their fragmentation methodologies
are ad hoc and unvalidated. Out of 13 papers, we found 11 different
methodologies, some of which were subsequently found inadequate. The importance
of addressing fragmentation necessitates a validated and principled
methodology.
  Our work fills these gaps in knowledge and methodology. We conduct a study of
memory fragmentation in production by observing 248 machines in the Computer
Sciences Department at University of Wisconsin - Madison for a week. We
identify six key memory usage patterns, and find that Linux's file cache and
page reclamation systems are major contributors to fragmentation because they
often obliviously break up contiguous memory. Finally, we create and\'uril, a
tool to artificially fragment memory during experimental research evaluations.
While and\'uril ultimately fails as a scientific tool, we discuss its design
ideas, merits, and failings in hope that they may inspire future research.
</summary>
    <author>
      <name>Mark Mansi</name>
    </author>
    <author>
      <name>Michael M. Swift</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.03523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.03523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.09688v1</id>
    <updated>2024-02-15T03:53:33Z</updated>
    <published>2024-02-15T03:53:33Z</published>
    <title>A System-Level Dynamic Binary Translator using Automatically-Learned
  Translation Rules</title>
    <summary>  System-level emulators have been used extensively for system design,
debugging and evaluation. They work by providing a system-level virtual machine
to support a guest operating system (OS) running on a platform with the same or
different native OS that uses the same or different instruction-set
architecture. For such system-level emulation, dynamic binary translation (DBT)
is one of the core technologies. A recently proposed learning-based DBT
approach has shown a significantly improved performance with a higher quality
of translated code using automatically learned translation rules. However, it
has only been applied to user-level emulation, and not yet to system-level
emulation. In this paper, we explore the feasibility of applying this approach
to improve system-level emulation, and use QEMU to build a prototype. ... To
achieve better performance, we leverage several optimizations that include
coordination overhead reduction to reduce the overhead of each coordination,
and coordination elimination and code scheduling to reduce the coordination
frequency. Experimental results show that it can achieve an average of 1.36X
speedup over QEMU 6.1 with negligible coordination overhead in the system
emulation mode using SPEC CINT2006 as application benchmarks and 1.15X on
real-world applications.
</summary>
    <author>
      <name>Jinhu Jiang</name>
    </author>
    <author>
      <name>Chaoyi Liang</name>
    </author>
    <author>
      <name>Rongchao Dong</name>
    </author>
    <author>
      <name>Zhaohui Yang</name>
    </author>
    <author>
      <name>Zhongjun Zhou</name>
    </author>
    <author>
      <name>Wenwen Wang</name>
    </author>
    <author>
      <name>Pen-Chung Yew</name>
    </author>
    <author>
      <name>Weihua Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 19 figures, to be published in International Symposium on
  Code Generation and Optimization (CGO) 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.09688v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.09688v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.11805v1</id>
    <updated>2024-03-18T14:03:23Z</updated>
    <published>2024-03-18T14:03:23Z</published>
    <title>LLM as a System Service on Mobile Devices</title>
    <summary>  Being more powerful and intrusive into user-device interactions, LLMs are
eager for on-device execution to better preserve user privacy. In this work, we
propose a new paradigm of mobile AI: LLM as a system service on mobile devices
(LLMaaS). Unlike traditional DNNs that execute in a stateless manner, such a
system service is stateful: LLMs execution often needs to maintain persistent
states (mainly KV cache) across multiple invocations. To minimize the LLM
context switching overhead under tight device memory budget, this work presents
LLMS, which decouples the memory management of app and LLM contexts with a key
idea of fine-grained, chunk-wise, globally-optimized KV cache compression and
swapping. By fully leveraging KV cache's unique characteristics, it proposes
three novel techniques: (1) Tolerance-Aware Compression: it compresses chunks
based on their measured accuracy tolerance to compression. (2) IO-Recompute
Pipelined Loading: it introduces recompute to swapping-in for acceleration. (3)
Chunk Lifecycle Management: it optimizes the memory activities of chunks with
an ahead-of-time swapping-out and an LCTRU (Least Compression-Tolerable and
Recently-Used) queue based eviction. In evaluations conducted on
well-established traces and various edge devices, \sys reduces context
switching latency by up to 2 orders of magnitude when compared to competitive
baseline solutions.
</summary>
    <author>
      <name>Wangsong Yin</name>
    </author>
    <author>
      <name>Mengwei Xu</name>
    </author>
    <author>
      <name>Yuanchun Li</name>
    </author>
    <author>
      <name>Xuanzhe Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.11805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.11805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.16971v4</id>
    <updated>2025-05-11T20:23:45Z</updated>
    <published>2024-03-25T17:32:23Z</published>
    <title>AIOS: LLM Agent Operating System</title>
    <summary>  LLM-based intelligent agents face significant deployment challenges,
particularly related to resource management. Allowing unrestricted access to
LLM or tool resources can lead to inefficient or even potentially harmful
resource allocation and utilization for agents. Furthermore, the absence of
proper scheduling and resource management mechanisms in current agent designs
hinders concurrent processing and limits overall system efficiency. As the
diversity and complexity of agents continue to grow, addressing these resource
management issues becomes increasingly critical to LLM-based agent systems. To
address these challenges, this paper proposes the architecture of AIOS
(LLM-based AI Agent Operating System) under the context of managing LLM-based
agents. It introduces a novel architecture for serving LLM-based agents by
isolating resources and LLM-specific services from agent applications into an
AIOS kernel. This AIOS kernel provides fundamental services (e.g., scheduling,
context management, memory management, storage management, access control) and
efficient management of resources (e.g., LLM and external tools) for runtime
agents. To enhance usability, AIOS also includes an AIOS-Agent SDK, a
comprehensive suite of APIs designed for utilizing functionalities provided by
the AIOS kernel. Experimental results demonstrate that using AIOS can achieve
up to 2.1x faster execution for serving agents built by various agent
frameworks. The source code is available at
https://github.com/agiresearch/AIOS.
</summary>
    <author>
      <name>Kai Mei</name>
    </author>
    <author>
      <name>Xi Zhu</name>
    </author>
    <author>
      <name>Wujiang Xu</name>
    </author>
    <author>
      <name>Wenyue Hua</name>
    </author>
    <author>
      <name>Mingyu Jin</name>
    </author>
    <author>
      <name>Zelong Li</name>
    </author>
    <author>
      <name>Shuyuan Xu</name>
    </author>
    <author>
      <name>Ruosong Ye</name>
    </author>
    <author>
      <name>Yingqiang Ge</name>
    </author>
    <author>
      <name>Yongfeng Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2403.16971v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.16971v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.00507v3</id>
    <updated>2025-05-06T16:32:38Z</updated>
    <published>2024-03-31T00:29:55Z</published>
    <title>THEMIS: Time, Heterogeneity, and Energy Minded Scheduling for Fair
  Multi-Tenant Use in FPGAs</title>
    <summary>  Using correct design metrics and understanding the limitations of the
underlying technology is critical to developing effective scheduling
algorithms. Unfortunately, existing scheduling techniques used \emph{incorrect}
metrics and had \emph{unrealistic} assumptions for fair scheduling of
multi-tenant FPGAs where each tenant is aimed to share approximately the same
number of resources both spatially and temporally.
  This paper introduces an enhanced fair scheduling algorithm for multi-tenant
FPGA use, addressing previous metric and assumption issues, with three specific
improvements claimed First, our method ensures spatiotemporal fairness by
considering both spatial and temporal aspects, addressing the limitation of
prior work that assumed uniform task latency. Second, we incorporate energy
considerations into fairness by adjusting scheduling intervals and accounting
for energy overhead, thereby balancing energy efficiency with fairness. Third,
we acknowledge overlooked aspects of FPGA multi-tenancy, including
heterogeneous regions and the constraints on dynamically merging/splitting
partially reconfigurable regions. We develop and evaluate our improved fair
scheduling algorithm with these three enhancements. Inspired by the Greek
goddess of law and personification of justice, we name our fair scheduling
solution THEMIS: \underline{T}ime, \underline{H}eterogeneity, and
\underline{E}nergy \underline{Mi}nded \underline{S}cheduling.
  We used the Xilinx Zedboard XC7Z020 to quantify our approach's savings.
Compared to previous algorithms, our improved scheduling algorithm enhances
fairness between 24.2--98.4\% and allows a trade-off between 55.3$\times$ in
energy vs. 69.3$\times$ in fairness. The paper thus informs cloud providers
about future scheduling optimizations for fairness with related challenges and
opportunities.
</summary>
    <author>
      <name>Emre Karabulut</name>
    </author>
    <author>
      <name>Arsalan Ali Malik</name>
    </author>
    <author>
      <name>Amro Awad</name>
    </author>
    <author>
      <name>Aydin Aysu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TC.2025.3566874</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TC.2025.3566874" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages, 8 Figures, 3 Tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Computers Early Access May 2025</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2404.00507v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.00507v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.01363v1</id>
    <updated>2024-04-01T17:32:22Z</updated>
    <published>2024-04-01T17:32:22Z</published>
    <title>AIOps Solutions for Incident Management: Technical Guidelines and A
  Comprehensive Literature Review</title>
    <summary>  The management of modern IT systems poses unique challenges, necessitating
scalability, reliability, and efficiency in handling extensive data streams.
Traditional methods, reliant on manual tasks and rule-based approaches, prove
inefficient for the substantial data volumes and alerts generated by IT
systems. Artificial Intelligence for Operating Systems (AIOps) has emerged as a
solution, leveraging advanced analytics like machine learning and big data to
enhance incident management. AIOps detects and predicts incidents, identifies
root causes, and automates healing actions, improving quality and reducing
operational costs. However, despite its potential, the AIOps domain is still in
its early stages, decentralized across multiple sectors, and lacking
standardized conventions. Research and industrial contributions are distributed
without consistent frameworks for data management, target problems,
implementation details, requirements, and capabilities. This study proposes an
AIOps terminology and taxonomy, establishing a structured incident management
procedure and providing guidelines for constructing an AIOps framework. The
research also categorizes contributions based on criteria such as incident
management tasks, application areas, data sources, and technical approaches.
The goal is to provide a comprehensive review of technical and research aspects
in AIOps for incident management, aiming to structure knowledge, identify gaps,
and establish a foundation for future developments in the field.
</summary>
    <author>
      <name>Youcef Remil</name>
    </author>
    <author>
      <name>Anes Bendimerad</name>
    </author>
    <author>
      <name>Romain Mathonat</name>
    </author>
    <author>
      <name>Mehdi Kaytoue</name>
    </author>
    <link href="http://arxiv.org/abs/2404.01363v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.01363v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.13886v1</id>
    <updated>2024-04-22T05:23:25Z</updated>
    <published>2024-04-22T05:23:25Z</published>
    <title>Taming Server Memory TCO with Multiple Software-Defined Compressed Tiers</title>
    <summary>  Memory accounts for 33 - 50% of the total cost of ownership (TCO) in modern
data centers. We propose a novel solution to tame memory TCO through the novel
creation and judicious management of multiple software-defined compressed
memory tiers.
  As opposed to the state-of-the-art solutions that employ a 2-Tier solution, a
single compressed tier along with DRAM, we define multiple compressed tiers
implemented through a combination of different compression algorithms, memory
allocators for compressed objects, and backing media to store compressed
objects. These compressed memory tiers represent distinct points in the access
latency, data compressibility, and unit memory usage cost spectrum, allowing
rich and flexible trade-offs between memory TCO savings and application
performance impact. A key advantage with ntier is that it enables aggressive
memory TCO saving opportunities by placing warm data in low latency compressed
tiers with a reasonable performance impact while simultaneously placing cold
data in the best memory TCO saving tiers. We believe our work represents an
important server system configuration and optimization capability to achieve
the best SLA-aware performance per dollar for applications hosted in production
data center environments.
  We present a comprehensive and rigorous analytical cost model for performance
and TCO trade-off based on continuous monitoring of the application's data
access profile. Guided by this model, our placement model takes informed
actions to dynamically manage the placement and migration of application data
across multiple software-defined compressed tiers. On real-world benchmarks,
our solution increases memory TCO savings by 22% - 40% percentage points while
maintaining performance parity or improves performance by 2% - 10% percentage
points while maintaining memory TCO parity compared to state-of-the-art 2-Tier
solutions.
</summary>
    <author>
      <name>Sandeep Kumar</name>
    </author>
    <author>
      <name>Aravinda Prasad</name>
    </author>
    <author>
      <name>Sreenivas Subramoney</name>
    </author>
    <link href="http://arxiv.org/abs/2404.13886v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.13886v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.01225v1</id>
    <updated>2024-06-03T11:47:55Z</updated>
    <published>2024-06-03T11:47:55Z</published>
    <title>SVFF: An Automated Framework for SR-IOV Virtual Function Management in
  FPGA Accelerated Virtualized Environments</title>
    <summary>  FPGA accelerator devices have emerged as a powerful platform for implementing
high-performance and scalable solutions in a wide range of industries,
leveraging their reconfigurability and virtualization capabilities.
Virtualization, in particular, offers several benefits including improved
security by resource isolation and sharing, and SR-IOV is the main solution for
enabling it on FPGAs.
  This paper introduces the SR-IOV Virtual Function Framework (SVFF), a
solution that aims to simplify and enhance the management of Virtual Functions
(VFs) on PCIe-attached FPGA devices in Linux and QEMU/KVM environments, solving
the lack of SR-IOV re-configuration support on guests. The framework leverages
the SR-IOV support in the Xilinx Queue-based Direct Memory Access (QDMA) to
automate the creation, attachment, detachment, and reconfiguration of VFs to
different Virtual Machines (VMs). A novel pause functionality for the VFIO
device has been implemented in QEMU to enable the detachment of VFs from the
host without detaching them from the guest, making reconfiguration of VFs
transparent for guests that already have a VF attached to them without any
performance loss. The proposed solution offers the ability to automatically and
seamlessly assign a set of VFs to different VMs and adjust the configuration on
the fly. Thanks to the pause functionality, it also offers the ability to
attach additional VFs to new VMs without affecting devices already attached to
other VMs.
</summary>
    <author>
      <name>Stefano Cirici</name>
    </author>
    <author>
      <name>Michele Paolino</name>
    </author>
    <author>
      <name>Daniel Raho</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CITS58301.2023.10188786</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CITS58301.2023.10188786" rel="related"/>
    <link href="http://arxiv.org/abs/2406.01225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.01225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.15776v1</id>
    <updated>2024-06-22T07:53:55Z</updated>
    <published>2024-06-22T07:53:55Z</published>
    <title>Simulation of high-performance memory allocators</title>
    <summary>  For the last thirty years, a large variety of memory allocators have been
proposed. Since performance, memory usage and energy consumption of each memory
allocator differs, software engineers often face difficult choices in selecting
the most suitable approach for their applications. To this end, custom
allocators are developed from scratch, which is a difficult and error-prone
process. This issue has special impact in the field of portable consumer
embedded systems, that must execute a limited amount of multimedia
applications, demanding high performance and extensive memory usage at a low
energy consumption. This paper presents a flexible and efficient simulator to
study Dynamic Memory Managers (DMMs), a composition of one or more memory
allocators. This novel approach allows programmers to simulate custom and
general DMMs, which can be composed without incurring any additional runtime
overhead or additional programming cost. We show that this infrastructure
simplifies DMM construction, mainly because the target application does not
need to be compiled every time a new DMM must be evaluated and because we
propose a structured method to search and build DMMs in an object-oriented
fashion. Within a search procedure, the system designer can choose the "best"
allocator by simulation for a particular target application and embedded
system. In our evaluation, we show that our scheme delivers better performance,
less memory usage and less energy consumption than single memory allocators.
</summary>
    <author>
      <name>José L. Risco-Martín</name>
    </author>
    <author>
      <name>J. Manuel Colmenar</name>
    </author>
    <author>
      <name>David Atienza</name>
    </author>
    <author>
      <name>J. Ignacio Hidalgo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.micpro.2011.08.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.micpro.2011.08.003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2403.04414</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Microprocessors and Microsystems, 35(8), pp. 755-765, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2406.15776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.15776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.18980v1</id>
    <updated>2024-06-27T08:19:39Z</updated>
    <published>2024-06-27T08:19:39Z</published>
    <title>E-Mapper: Energy-Efficient Resource Allocation for Traditional Operating
  Systems on Heterogeneous Processors</title>
    <summary>  Energy efficiency has become a key concern in modern computing. Major
processor vendors now offer heterogeneous architectures that combine powerful
cores with energy-efficient ones, such as Intel P/E systems, Apple M1 chips,
and Samsungs Exyno's CPUs. However, apart from simple cost-based thread
allocation strategies, today's OS schedulers do not fully exploit these
systems' potential for adaptive energy-efficient computing. This is, in part,
due to missing application-level interfaces to pass information about
task-level energy consumption and application-level elasticity. This paper
presents E-Mapper, a novel resource management approach integrated into Linux
for improved execution on heterogeneous processors. In E-Mapper, we base
resource allocation decisions on high-level application descriptions that user
can attach to programs or that the system can learn automatically at runtime.
Our approach supports various programming models including OpenMP, Intel TBB,
and TensorFlow. Crucially, E-Mapper leverages this information to extend beyond
existing thread-to-core allocation strategies by actively managing application
configurations through a novel uniform application-resource manager interface.
By doing so, E-Mapper achieves substantial enhancements in both performance and
energy efficiency, particularly in multi-application scenarios. On an Intel
Raptor Lake and an Arm big.LITTLE system, E-Mapper reduces the application
execution on average by 20 % with an average reduction in energy consumption of
34 %. We argue that our solution marks a crucial step toward creating a generic
approach for sustainable and efficient computing across different processor
architectures.
</summary>
    <author>
      <name>Till Smejkal</name>
    </author>
    <author>
      <name>Robert Khasanov</name>
    </author>
    <author>
      <name>Jeronimo Castrillon</name>
    </author>
    <author>
      <name>Hermann Härtig</name>
    </author>
    <link href="http://arxiv.org/abs/2406.18980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.18980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.02764v2</id>
    <updated>2025-05-20T23:44:59Z</updated>
    <published>2024-07-03T02:42:59Z</published>
    <title>Data-driven Software-based Power Estimation for Embedded Devices</title>
    <summary>  Energy measurement of computer devices, which are widely used in the Internet
of Things (IoT), is an important yet challenging task. Most of these IoT
devices lack ready-to-use hardware or software for power measurement. In this
paper, we propose an easy-to-use approach to derive a software-based energy
estimation model with external low-end power meters based on data-driven
analysis. Our solution is demonstrated with a Jetson Nano board and Ruideng
UM25C USB power meter. Various machine learning methods combined with our smart
data collection &amp; profiling method and physical measurement are explored.
Periodic Long-duration measurements are utilized in the experiments to derive
and validate power models, allowing more accurate power readings from the
low-end power meter. Benchmarks were used to evaluate the derived
software-power model for the Jetson Nano board and Raspberry Pi. The results
show that 92\% accuracy can be achieved by the software-based power estimation
compared to measurement. A kernel module that can collect running traces of
utilization and frequencies needed is developed, together with the power model
derived, for power prediction for programs running in a real environment. Our
cost-effective method facilitates accurate instantaneous power estimation,
which low-end power meters cannot directly provide.
</summary>
    <author>
      <name>Haoyu Wang</name>
    </author>
    <author>
      <name>Xinyi Li</name>
    </author>
    <author>
      <name>Ti Zhou</name>
    </author>
    <author>
      <name>Man Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2407.02764v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.02764v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.14567v2</id>
    <updated>2024-12-22T13:04:08Z</updated>
    <published>2024-07-19T05:29:34Z</published>
    <title>Integrating Artificial Intelligence into Operating Systems: A
  Comprehensive Survey on Techniques, Applications, and Future Directions</title>
    <summary>  In the era of the Internet of Everything, operating systems (OSs) face
unprecedented challenges posed by an evolving application landscape and
increasingly heterogeneous hardware ecosystems. This shift toward increasingly
dynamic and unpredictable operational contexts presents significant challenges
for both OS developers and users. Against this backdrop, the fusion of
Artificial Intelligence (AI) with Operating Systems emerges as a critical
frontier for innovation. This survey delves into the intricate interplay
between AI and OSs, illustrating how existing OS mechanisms combined with AI
significantly elevate the performance, security, and efficiency of modern
operating systems. We investigate a range of AI methodologies applied to
optimize core OS functionalities and clarify the correlation to related
studies. Our analysis touches on the existing hurdles and prospective avenues
in this interdisciplinary domain, underscoring the imperative for robust and
seamless integration of AI capabilities into OS architectures.
  Through an examination of illustrative case studies and cutting-edge
developments, we offer a thorough review of the current status of AI-OS
integration, accentuating its pivotal role in steering the evolution of
advanced computing paradigms. We also envision the promising prospects of
Intelligent Operating Systems, debating how groundbreaking OS designs will
usher in novel possibilities and highlight the central role that AI will assume
in propelling these next-generation systems forward. This forward-thinking
outlook illuminates the profound influence of AI on the foundational elements
of computing, heralding the advent of a new epoch characterized by intelligent,
self-adapting, and highly adaptive software ecosystems.
</summary>
    <author>
      <name>Yifan Zhang</name>
    </author>
    <author>
      <name>Xinkui Zhao</name>
    </author>
    <author>
      <name>Ziying Li</name>
    </author>
    <author>
      <name>Jianwei Yin</name>
    </author>
    <author>
      <name>Lufei Zhang</name>
    </author>
    <author>
      <name>Zuoning Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">47 pages,12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.14567v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.14567v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.17351v2</id>
    <updated>2024-10-20T19:41:12Z</updated>
    <published>2024-08-30T15:30:00Z</published>
    <title>Tide: A Split OS Architecture for Control Plane Offloading</title>
    <summary>  The end of Moore's Law is driving cloud providers to offload virtualization
and the network data plane to SmartNICs to improve compute efficiency. Even
though individual OS control plane tasks consume up to 5% of cycles across the
fleet, they remain on the host CPU because they are tightly intertwined with OS
mechanisms. Moreover, offloading puts the slow PCIe interconnect in the
critical path of OS decisions.
  We propose Tide, a new split OS architecture that separates OS control plane
policies from mechanisms and offloads the control plane policies onto a
SmartNIC. Tide has a new host-SmartNIC communication API, state synchronization
mechanism, and communication mechanisms that overcome the PCIe bottleneck, even
for $\mu$s-scale workloads. Tide frees up host compute for applications and
unlocks new optimization opportunities, including machine learning-driven
policies, scheduling on the network I/O path, and reducing on-host
interference. We demonstrate that Tide enables OS control planes that are
competitive with on-host performance for the most difficult $\mu$s-scale
workloads. Tide outperforms on-host control planes for memory management
(saving 16 host cores), Stubby network RPCs (saving 8 cores), and GCE virtual
machine management (11.2% performance improvement).
</summary>
    <author>
      <name>Jack Tigar Humphries</name>
    </author>
    <author>
      <name>Neel Natu</name>
    </author>
    <author>
      <name>Kostis Kaffes</name>
    </author>
    <author>
      <name>Stanko Novaković</name>
    </author>
    <author>
      <name>Paul Turner</name>
    </author>
    <author>
      <name>Hank Levy</name>
    </author>
    <author>
      <name>David Culler</name>
    </author>
    <author>
      <name>Christos Kozyrakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">About 11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.17351v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.17351v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.10946v1</id>
    <updated>2024-09-17T07:28:56Z</updated>
    <published>2024-09-17T07:28:56Z</published>
    <title>Skip TLB flushes for reused pages within mmap's</title>
    <summary>  Memory access efficiency is significantly enhanced by caching recent address
translations in the CPUs' Translation Lookaside Buffers (TLBs). However, since
the operating system is not aware of which core is using a particular mapping,
it flushes TLB entries across all cores where the application runs whenever
addresses are unmapped, ensuring security and consistency. These TLB flushes,
known as TLB shootdowns, are costly and create a performance and scalability
bottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,
particularly during mmap-munmap cycles and page cache evictions. Often, the
same physical pages are reassigned to the same process post-eviction,
presenting an opportunity for the operating system to reduce the frequency of
TLB shootdowns. We demonstrate, that by slightly extending the mmap function,
TLB shootdowns for these "recycled pages" can be avoided.
  Therefore we introduce and implement the "fast page recycling" (FPR) feature
within the mmap system call. FPR-mmaps maintain security by only triggering TLB
shootdowns when a page exits its recycling cycle and is allocated to a
different process. To ensure consistency when FPR-mmap pointers are used, we
made minor adjustments to virtual memory management to avoid the ABA problem.
Unlike previous methods to mitigate shootdown effects, our approach does not
require any hardware modifications and operates transparently within the
existing Linux virtual memory framework.
  Our evaluations across a variety of CPU, memory, and storage setups,
including persistent memory and Optane SSDs, demonstrate that FPR delivers
notable performance gains, with improvements of up to 28% in real-world
applications and 92% in micro-benchmarks. Additionally, we show that TLB
shootdowns are a significant source of bottlenecks, previously misattributed to
other components of the Linux kernel.
</summary>
    <author>
      <name>Frederic Schimmelpfennig</name>
    </author>
    <author>
      <name>André Brinkmann</name>
    </author>
    <author>
      <name>Hossein Asadi</name>
    </author>
    <author>
      <name>Reza Salkhordeh</name>
    </author>
    <link href="http://arxiv.org/abs/2409.10946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.10946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.11271v1</id>
    <updated>2024-09-17T15:21:47Z</updated>
    <published>2024-09-17T15:21:47Z</published>
    <title>Analysis of Synchronization Mechanisms in Operating Systems</title>
    <summary>  This research analyzed the performance and consistency of four
synchronization mechanisms-reentrant locks, semaphores, synchronized methods,
and synchronized blocks-across three operating systems: macOS, Windows, and
Linux. Synchronization ensures that concurrent processes or threads access
shared resources safely, and efficient synchronization is vital for maintaining
system performance and reliability. The study aimed to identify the
synchronization mechanism that balances efficiency, measured by execution time,
and consistency, assessed by variance and standard deviation, across platforms.
The initial hypothesis proposed that mutex-based mechanisms, specifically
synchronized methods and blocks, would be the most efficient due to their
simplicity. However, empirical results showed that reentrant locks had the
lowest average execution time (14.67ms), making them the most efficient
mechanism, but with the highest variability (standard deviation of 1.15). In
contrast, synchronized methods, blocks, and semaphores exhibited higher average
execution times (16.33ms for methods and 16.67ms for blocks) but with greater
consistency (variance of 0.33). The findings indicated that while reentrant
locks were faster, they were more platform-dependent, whereas mutex-based
mechanisms provided more predictable performance across all operating systems.
The use of virtual machines for Windows and Linux was a limitation, potentially
affecting the results. Future research should include native testing and
explore additional synchronization mechanisms and higher concurrency levels.
These insights help developers and system designers optimize synchronization
strategies for either performance or stability, depending on the application's
requirements.
</summary>
    <author>
      <name>Oluwatoyin Kode</name>
    </author>
    <author>
      <name>Temitope Oyemade</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was submitted to the 2nd International Conference on
  Computer Science and Software Engineering (CSSE 2024). It contains 19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.11271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.11271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.14317v1</id>
    <updated>2024-09-22T05:11:16Z</updated>
    <published>2024-09-22T05:11:16Z</published>
    <title>Dissecting CXL Memory Performance at Scale: Analysis, Modeling, and
  Optimization</title>
    <summary>  We present SupMario, a characterization framework designed to thoroughly
analyze, model, and optimize CXL memory performance. SupMario is based on
extensive evaluation of 265 workloads spanning 4 real CXL devices within 7
memory latency configurations across 4 processor platforms. SupMario uncovers
many key insights, including detailed workload performance at sub-us memory
latencies (140-410 ns), CXL tail latencies, CPU tolerance to CXL latencies, CXL
performance root-cause analysis and precise performance prediction models. In
particular, SupMario performance models rely solely on 12 CPU performance
counters and accurately fit over 99% and 91%-94% workloads with a 10%
misprediction target for NUMA and CXL memory, respectively. We demonstrate the
practical utility of SupMario characterization findings, models, and insights
by applying them to popular CXL memory management schemes, such as page
interleaving and tiering policies, to identify system inefficiencies during
runtime. We introduce a novel ``bestshot'' page interleaving policy and a
regulated page tiering policy (Alto) tailored for memory bandwidth- and
latency-sensitive workloads. In bandwidth bound scenarios, our ``best-shot''
interleaving, guided by our novel performance prediction model, achieves
close-to optimal scenarios by exploiting the aggregate system and CXL/NUMA
memory bandwidth. For latency sensitive workloads, Alto, driven by our key
insight of utilizing ``amortized'' memory latency to regulate unnecessary page
migrations, achieves up to 177% improvement over state-of-the-art memory
tiering systems like TPP, as demonstrated through extensive evaluation with 8
real-world applications.
</summary>
    <author>
      <name>Jinshu Liu</name>
    </author>
    <author>
      <name>Hamid Hadian</name>
    </author>
    <author>
      <name>Hanchen Xu</name>
    </author>
    <author>
      <name>Daniel S. Berger</name>
    </author>
    <author>
      <name>Huaicheng Li</name>
    </author>
    <link href="http://arxiv.org/abs/2409.14317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.14317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.19434v3</id>
    <updated>2025-05-21T00:02:06Z</updated>
    <published>2024-09-28T18:44:39Z</published>
    <title>Energy-Efficient Computation with DVFS using Deep Reinforcement Learning
  for Multi-Task Systems in Edge Computing</title>
    <summary>  Finding an optimal energy-efficient policy that is adaptable to underlying
edge devices while meeting deadlines for tasks has always been challenging.
This research studies generalized systems with multi-task, multi-deadline
scenarios with reinforcement learning-based DVFS for energy saving for periodic
soft real-time applications on edge devices. This work addresses the limitation
of previous work that models a periodic system as a single task and
single-deadline scenario, which is too simplified to cope with complex
situations. The method encodes time series data in the Linux kernel into
information that is easy to interpret for reinforcement learning, allowing the
system to generate DVFS policies to adapt system patterns based on the general
workload. For encoding, we present two different methods for comparison. Both
methods use only one performance counter: system utilization, and the kernel
only needs minimal information from the userspace. Our method is implemented on
Jetson Nano Board (2GB) and is tested with three fixed multitask workloads,
which are three, five, and eight tasks in the workload, respectively. For
randomness and generalization, we also designed a random workload generator to
build different multitask workloads to test. Based on the test results, our
method could save 3%-10% power compared to Linux built-in governors.
</summary>
    <author>
      <name>Xinyi Li</name>
    </author>
    <author>
      <name>Ti Zhou</name>
    </author>
    <author>
      <name>Haoyu Wang</name>
    </author>
    <author>
      <name>Man Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2409.19434v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.19434v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.00026v2</id>
    <updated>2024-10-03T10:23:05Z</updated>
    <published>2024-09-16T13:28:07Z</published>
    <title>The eBPF Runtime in the Linux Kernel</title>
    <summary>  Extended Berkeley Packet Filter (eBPF) is a runtime that enables users to
load programs into the operating system (OS) kernel, like Linux or Windows, and
execute them safely and efficiently at designated kernel hooks. Each program
passes through a verifier that reasons about the safety guarantees for
execution. Hosting a safe virtual machine runtime within the kernel makes it
dynamically programmable. Unlike the popular approach of bypassing or
completely replacing the kernel, eBPF gives users the flexibility to modify the
kernel on the fly, rapidly experiment and iterate, and deploy solutions to
achieve their workload-specific needs, while working in concert with the
kernel.
  In this paper, we present the first comprehensive description of the design
and implementation of the eBPF runtime in the Linux kernel. We argue that eBPF
today provides a mature and safe programming environment for the kernel. It has
seen wide adoption since its inception and is increasingly being used not just
to extend, but program entire components of the kernel, while preserving its
runtime integrity. We outline the compelling advantages it offers for
real-world production usage, and illustrate current use cases. Finally, we
identify its key challenges, and discuss possible future directions.
</summary>
    <author>
      <name>Bolaji Gbadamosi</name>
    </author>
    <author>
      <name>Luigi Leonardi</name>
    </author>
    <author>
      <name>Tobias Pulls</name>
    </author>
    <author>
      <name>Toke Høiland-Jørgensen</name>
    </author>
    <author>
      <name>Simone Ferlin-Reiter</name>
    </author>
    <author>
      <name>Simo Sorce</name>
    </author>
    <author>
      <name>Anna Brunström</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.00026v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.00026v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.01528v1</id>
    <updated>2024-10-02T13:20:21Z</updated>
    <published>2024-10-02T13:20:21Z</published>
    <title>Global Scheduling of Weakly-Hard Real-Time Tasks using Job-Level
  Priority Classes</title>
    <summary>  Real-time systems are intrinsic components of many pivotal applications, such
as self-driving vehicles, aerospace and defense systems. The trend in these
applications is to incorporate multiple tasks onto fewer, more powerful
hardware platforms, e.g., multi-core systems, mainly for reducing cost and
power consumption. Many real-time tasks, like control tasks, can tolerate
occasional deadline misses due to robust algorithms. These tasks can be modeled
using the weakly-hard model. Literature shows that leveraging the weakly-hard
model can relax the over-provisioning associated with designed real-time
systems. However, a wide-range of the research focuses on single-core
platforms. Therefore, we strive to extend the state-of-the-art of scheduling
weakly-hard real-time tasks to multi-core platforms. We present a global
job-level fixed priority scheduling algorithm together with its schedulability
analysis. The scheduling algorithm leverages the tolerable continuous deadline
misses to assigning priorities to jobs. The proposed analysis extends the
Response Time Analysis (RTA) for global scheduling to test the schedulability
of tasks. Hence, our analysis scales with the number of tasks and number of
cores because, unlike literature, it depends neither on Integer Linear
Programming nor reachability trees. Schedulability analyses show that the
schedulability ratio is improved by 40% comparing to the global Rate Monotonic
(RM) scheduling and up to 60% more than the global EDF scheduling, which are
the state-of-the-art schedulers on the RTEMS real-time operating system. Our
evaluation on industrial embedded multi-core platform running RTEMS shows that
the scheduling overhead of our proposal does not exceed 60 Nanosecond.
</summary>
    <author>
      <name>V. Gabriel Moyano</name>
    </author>
    <author>
      <name>Zain A. H. Hammadeh</name>
    </author>
    <author>
      <name>Selma Saidi</name>
    </author>
    <author>
      <name>Daniel Lüdtke</name>
    </author>
    <link href="http://arxiv.org/abs/2410.01528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.01528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.15894v1</id>
    <updated>2024-10-21T11:14:55Z</updated>
    <published>2024-10-21T11:14:55Z</published>
    <title>Transparent and Efficient Live Migration across Heterogeneous Hosts with
  Wharf</title>
    <summary>  Live migration allows a user to move a running application from one machine
(a source) to another (a destination) without restarting it. The technique has
proven useful for diverse tasks including load balancing, managing system
updates, improving data locality, and improving system resilience.
Unfortunately, current live migration solutions fail to meet today's computing
needs. First, most techniques do not support heterogeneous source and
destination hosts, as they require the two machines to have the same
instruction set architecture (ISA) or use the same operating system (OS), which
hampers numerous live migration usecases. Second, many techniques are not
transparent, as they require that applications be written in a specific
high-level language or call specific library functions, which imposes barriers
to entry for many users. We present a new lightweight abstraction, called a
vessel, that supports transparent heterogeneous live migration. A vessel
maintains a machine-independent encoding of a process's state, using
WebAssembly abstractions, allowing it to be executed on nearly-arbitrary ISAs.
A vessel virtualizes all of its OS state, using the WebAssembly System
Interface (WASI), allowing it to execute on nearly arbitrary OS. We introduce
docks and software systems that execute and migrate vessels. Docks face two key
challenges: First, maintaining a machine-independent encoding at all points in
a process is extremely expensive. So, docks instead ensure that a vessel is
guaranteed to eventually reach a machine-independent point and delay the
initiation of vessel migration until the vessel reaches such a point. Second, a
dock may receive a vessel migration that originates from a dock executing on a
different OS.
</summary>
    <author>
      <name>Yiwei Yang</name>
    </author>
    <author>
      <name>Aibo Hu</name>
    </author>
    <author>
      <name>Yusheng Zheng</name>
    </author>
    <author>
      <name>Brian Zhao</name>
    </author>
    <author>
      <name>Xinqi Zhang</name>
    </author>
    <author>
      <name>Andrew Quinn</name>
    </author>
    <link href="http://arxiv.org/abs/2410.15894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.15894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.17928v1</id>
    <updated>2024-10-23T14:47:12Z</updated>
    <published>2024-10-23T14:47:12Z</published>
    <title>SJMalloc: the security-conscious, fast, thread-safe and memory-efficient
  heap allocator</title>
    <summary>  Heap-based exploits that leverage memory management errors continue to pose a
significant threat to application security. The root cause of these
vulnerabilities are the memory management errors within the applications,
however various hardened allocator designs have been proposed as mitigation. A
common feature of these designs is the strategic decision to store heap
metadata separately from the application data in use, thereby reducing the risk
of metadata corruption leading to security breaches. Despite their potential
benefits, hardened allocators have not been widely adopted in real-world
applications. The primary barrier to their adoption is the performance
overheads they introduce. These overheads can negatively impact the efficiency
and speed of applications, which is a critical consideration for developers and
system administrators. Having learned from previous implementations, we
developed SJMalloc, a general-purpose, high-performance allocator that
addresses these concerns. SJMalloc stores its metadata out-of-band, away from
the application's data on the heap. This design choice not only enhances
security but also improves performance. Across a variety of real-world
workloads, SJMalloc demonstrates a ~6% performance improvement compared to
GLibcs allocator, while using only ~5% more memory. Furthermore, SJMalloc
successfully passes the generic elements of the GLibc malloc testsuite and can
thus be used as a drop-in replacement for the standard allocator, offering an
easy upgrade path for enhanced security and performance without requiring
changes to existing applications.
</summary>
    <author>
      <name>Stephan Bauroth</name>
    </author>
    <link href="http://arxiv.org/abs/2410.17928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.17928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.00572v1</id>
    <updated>2024-11-01T13:29:33Z</updated>
    <published>2024-11-01T13:29:33Z</published>
    <title>Enhancing Adaptive Mixed-Criticality Scheduling with Deep Reinforcement
  Learning</title>
    <summary>  Adaptive Mixed-Criticality (AMC) is a fixed-priority preemptive scheduling
algorithm for mixed-criticality hard real-time systems. It dominates many other
scheduling algorithms for mixed-criticality systems, but does so at the cost of
occasionally dropping jobs of less important/critical tasks, when low-priority
jobs overrun their time budgets. In this paper we enhance AMC with a deep
reinforcement learning (DRL) approach based on a Deep-Q Network. The DRL agent
is trained off-line, and at run-time adjusts the low-criticality budgets of
tasks to avoid budget overruns, while ensuring that no job misses its deadline
if it does not overrun its budget. We have implemented and evaluated this
approach by simulating realistic workloads from the automotive domain. The
results show that the agent is able to reduce budget overruns by at least up to
50%, even when the budget of each task is chosen based on sampling the
distribution of its execution time. To the best of our knowledge, this is the
first use of DRL in AMC reported in the literature.
</summary>
    <author>
      <name>Bruno Mendes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Informatics Engineering</arxiv:affiliation>
    </author>
    <author>
      <name>Pedro F. Souto</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Informatics Engineering</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CISTER Research Centre</arxiv:affiliation>
    </author>
    <author>
      <name>Pedro C. Diniz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CISTER Research Centre</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version submitted to RTNS 2024, on 17/08/2024 (with some typos fixed)</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.00572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.00572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.7; D.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.06980v1</id>
    <updated>2024-11-11T13:38:49Z</updated>
    <published>2024-11-11T13:38:49Z</published>
    <title>xNVMe: Unleashing Storage Hardware-Software Co-design</title>
    <summary>  NVMe SSD hardware has witnessed widespread deployment as commodity and
enterprise hardware due to its high performance and rich feature set. Despite
the open specifications of various NVMe protocols by the NVMe Express group and
NVMe being of software abstractions to program the underlying hardware. The
myriad storage I/O paths such as POSIX storage API, ad-hoc OS mechanisms, and
userspace I/O libraries have different syntax and semantics that complicate
software development and stand in the way of mass adoption and evolution of the
NVMe ecosystem. To unify the diverse I/O storage paths, we built xNVMe that
exposes a single message-passing API to support both asynchronous and
synchronous communication with NVMe devices. xNVMe provides various command
sets to support diverse storage I/O paths in different OS (e.g., Linux,
FreeBSD, Windows, and MacOS) and userspace libraries (e.g., SPDK) with minimal
overhead. xNVMe is an Open Source project and has gained traction amongst
various industry stakeholders. In this paper, we elaborate on the lessons that
we have learned in the project during its evolution. We also provide some
ongoing and future work planned for the project. We hope the database and
storage systems community can join in the effort to both extend xNVMe and
leverage it as a building block for innovative co-design of storage systems on
modern NVMe hardware.
</summary>
    <author>
      <name>Simon A. F. Lund</name>
    </author>
    <author>
      <name>Vivek Shah</name>
    </author>
    <link href="http://arxiv.org/abs/2411.06980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.06980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.08300v4</id>
    <updated>2025-02-13T19:57:26Z</updated>
    <published>2024-11-13T02:41:17Z</published>
    <title>EDM: An Ultra-Low Latency Ethernet Fabric for Memory Disaggregation</title>
    <summary>  Achieving low remote memory access latency remains the primary challenge in
realizing memory disaggregation over Ethernet within the datacenters. We
present EDM that attempts to overcome this challenge using two key ideas.
First, while existing network protocols for remote memory access over the
Ethernet, such as TCP/IP and RDMA, are implemented on top of the MAC layer, EDM
takes a radical approach by implementing the entire network protocol stack for
remote memory access within the Physical layer (PHY) of the Ethernet. This
overcomes fundamental latency and bandwidth overheads imposed by the MAC layer,
especially for small memory messages. Second, EDM implements a centralized,
fast, in-network scheduler for memory traffic within the PHY of the Ethernet
switch. Inspired by the classic Parallel Iterative Matching (PIM) algorithm,
the scheduler dynamically reserves bandwidth between compute and memory nodes
by creating virtual circuits in the PHY, thus eliminating queuing delay and
layer 2 packet processing delay at the switch for memory traffic, while
maintaining high bandwidth utilization. Our FPGA testbed demonstrates that
EDM's network fabric incurs a latency of only $\sim$300 ns for remote memory
access in an unloaded network, which is an order of magnitude lower than
state-of-the-art Ethernet-based solutions such as RoCEv2 and comparable to
emerging PCIe-based solutions such as CXL. Larger-scale network simulations
indicate that even at high network loads, EDM's average latency remains within
1.3$\times$ its unloaded latency.
</summary>
    <author>
      <name>Weigao Su</name>
    </author>
    <author>
      <name>Vishal Shrivastav</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3669940.3707221</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3669940.3707221" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ASPLOS 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.08300v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.08300v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.12893v1</id>
    <updated>2024-11-19T22:18:31Z</updated>
    <published>2024-11-19T22:18:31Z</published>
    <title>Fast and Efficient Memory Reclamation For Serverless MicroVMs</title>
    <summary>  Resource elasticity is one of the key defining characteristics of the
Function-as-a-Service (FaaS) serverless computing paradigm. In order to provide
strong multi-tenant isolation, FaaS providers commonly sandbox functions inside
virtual machines (VMs or microVMs). While compute resources assigned to
VM-sandboxed functions can be seamlessly adjusted on the fly, memory elasticity
remains challenging, especially when scaling down. State-of-the-art mechanisms
for VM memory elasticity suffer from increased reclaim latency when memory
needs to be released, compounded by CPU and memory bandwidth overheads. We
identify the obliviousness of the Linux memory manager to the virtually
hotplugged memory as the key issue hindering hot-unplug performance, and design
HotMem, a novel approach for fast and efficient VM memory hot(un)plug,
targeting VM-sandboxed serverless functions. Our key insight is that by
segregating virtually hotplugged memory regions from regular VM memory, we are
able to bound the lifetimes of allocations within these regions thus enabling
their fast and efficient reclamation. We implement HotMem in Linux v6.6 and our
evaluation shows that it is an order of magnitude faster than state-of-practice
to reclaim VM memory, while achieving the same P99 function latency with a
model that statically over-provisions VMs.
</summary>
    <author>
      <name>Orestis Lagkas Nikolos</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Technical University Of Athens</arxiv:affiliation>
    </author>
    <author>
      <name>Chloe Alverti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Illinois Urbana-Champaign</arxiv:affiliation>
    </author>
    <author>
      <name>Stratos Psomadakis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Technical University Of Athens</arxiv:affiliation>
    </author>
    <author>
      <name>Georgios Goumas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Technical University Of Athens</arxiv:affiliation>
    </author>
    <author>
      <name>Nectarios Koziris</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Technical University Of Athens</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2411.12893v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.12893v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.18104v1</id>
    <updated>2024-12-24T02:26:04Z</updated>
    <published>2024-12-24T02:26:04Z</published>
    <title>Interference-free Operating System: A 6 Years' Experience in Mitigating
  Cross-Core Interference in Linux</title>
    <summary>  Real-time operating systems employ spatial and temporal isolation to
guarantee predictability and schedulability of real-time systems on multi-core
processors. Any unbounded and uncontrolled cross-core performance interference
poses a significant threat to system time safety. However, the current Linux
kernel has a number of interference issues and represents a primary source of
interference. Unfortunately, existing research does not systematically and
deeply explore the cross-core performance interference issue within the OS
itself.
  This paper presents our industry practice for mitigating cross-core
performance interference in Linux over the past 6 years. We have fixed dozens
of interference issues in different Linux subsystems. Compared to the version
without our improvements, our enhancements reduce the worst-case jitter by a
factor of 8.7, resulting in a maximum 11.5x improvement over system
schedulability. For the worst-case latency in the Core Flight System and the
Robot Operating System 2, we achieve a 1.6x and 1.64x reduction over RT-Linux.
Based on our development experience, we summarize the lessons we learned and
offer our suggestions to system developers for systematically eliminating
cross-core interference from the following aspects: task management, resource
management, and concurrency management. Most of our modifications have been
merged into Linux upstream and released in commercial distributions.
</summary>
    <author>
      <name>Zhaomeng Deng</name>
    </author>
    <author>
      <name>Ziqi Zhang</name>
    </author>
    <author>
      <name>Ding Li</name>
    </author>
    <author>
      <name>Yao Guo</name>
    </author>
    <author>
      <name>Yunfeng Ye</name>
    </author>
    <author>
      <name>Yuxin Ren</name>
    </author>
    <author>
      <name>Ning Jia</name>
    </author>
    <author>
      <name>Xinwei Hu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/RTSS62706.2024.00034</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/RTSS62706.2024.00034" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 10 figures, published in RTSS 2024</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2024 IEEE Real-Time Systems Symposium (RTSS), York, United
  Kingdom, 2024, pp. 308-321</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2412.18104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.18104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.00068v1</id>
    <updated>2024-12-29T17:41:40Z</updated>
    <published>2024-12-29T17:41:40Z</published>
    <title>Dynamic Optimization of Storage Systems Using Reinforcement Learning
  Techniques</title>
    <summary>  The exponential growth of data-intensive applications has placed
unprecedented demands on modern storage systems, necessitating dynamic and
efficient optimization strategies. Traditional heuristics employed for storage
performance optimization often fail to adapt to the variability and complexity
of contemporary workloads, leading to significant performance bottlenecks and
resource inefficiencies. To address these challenges, this paper introduces
RL-Storage, a novel reinforcement learning (RL)-based framework designed to
dynamically optimize storage system configurations. RL-Storage leverages deep
Q-learning algorithms to continuously learn from real-time I/O patterns and
predict optimal storage parameters, such as cache size, queue depths, and
readahead settings[1]. The proposed framework operates within the storage
kernel, ensuring minimal latency and low computational overhead. Through an
adaptive feedback mechanism, RL-Storage dynamically adjusts critical
parameters, achieving efficient resource utilization across a wide range of
workloads. Experimental evaluations conducted on a range of benchmarks,
including RocksDB and PostgreSQL, demonstrate significant improvements, with
throughput gains of up to 2.6x and latency reductions of 43% compared to
baseline heuristics. Additionally, RL-Storage achieves these performance
enhancements with a negligible CPU overhead of 0.11% and a memory footprint of
only 5 KB, making it suitable for seamless deployment in production
environments. This work underscores the transformative potential of
reinforcement learning techniques in addressing the dynamic nature of modern
storage systems. By autonomously adapting to workload variations in real time,
RL-Storage provides a robust and scalable solution for optimizing storage
performance, paving the way for next-generation intelligent storage
infrastructures.
</summary>
    <author>
      <name>Chiyu Cheng</name>
    </author>
    <author>
      <name>Chang Zhou</name>
    </author>
    <author>
      <name>Yang Zhao</name>
    </author>
    <author>
      <name>Jin Cao</name>
    </author>
    <link href="http://arxiv.org/abs/2501.00068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.00068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.00977v1</id>
    <updated>2025-01-01T23:08:54Z</updated>
    <published>2025-01-01T23:08:54Z</published>
    <title>Host-guided data placement: whose job is it anyway?</title>
    <summary>  The increasing demand for SSDs coupled with scaling difficulties have left
manufacturers scrambling for newer SSD interfaces which promise better
performance and durability. While these interfaces reduce the rigidity of
traditional abstractions, they require application or system-level changes that
can impact the stability, security, and portability of systems. To make matters
worse, such changes are rendered futile with introduction of next-generation
interfaces. Further, there is little guidance on data placement and hardware
specifics are often abstracted from the application layer. It is no surprise
therefore that such interfaces have seen limited adoption, leaving behind a
graveyard of experimental interfaces ranging from open-channel SSDs to zoned
namespaces.
  In this paper, we show how shim layers can to shield systems from changing
hardware interfaces while benefiting from them. We present Reshim, an
all-userspace shim layer that performs affinity and lifetime based data
placement with no change to the operating system or the application. We
demonstrate Reshim's ease of adoption with host-device coordination for three
widely-used data-intensive systems: RocksDB, MongoDB, and CacheLib. With
Reshim, these systems see 2-6 times highe write throughput, up to 6 times lower
latency, and reduced write amplification compared to filesystems like F2FS.
Reshim performs on par with application-specific backends like ZenFS while
offering more generality, lower latency, and richer data placement. With Reshim
we demonstrate the value of isolating the complexity of the placement logic,
allowing easy deployment of dynamic placement rules across several applications
and storage interfaces.
</summary>
    <author>
      <name>Devashish R. Purandare</name>
    </author>
    <author>
      <name>Peter Alvaro</name>
    </author>
    <author>
      <name>Avani Wildani</name>
    </author>
    <author>
      <name>Darrell D. E. Long</name>
    </author>
    <author>
      <name>Ethan L. Miller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 10 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.00977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.00977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.06716v1</id>
    <updated>2025-01-12T04:50:47Z</updated>
    <published>2025-01-12T04:50:47Z</published>
    <title>Symbol Resolution MatRs: Make it Fast and Observable with Stable Linking</title>
    <summary>  Dynamic linking is the standard mechanism for using external dependencies
since it enables code reuse, streamlines software updates, and reduces
disk/network use. Dynamic linking waits until runtime to calculate an
application's relocation mapping, i.e., the mapping between each externally
referenced symbol in the application to the dependency that provides the
symbol. Unfortunately, it comes with two downsides. First, dynamic linking
limits the performance of current systems since it can take seconds to
calculate a relocation mapping for a large program. Second, dynamic linking
limits the dependency management of applications since it prevents a developer
from accurately observing a relocation mapping except at runtime.
  This paper makes the key insight that the benefits conventionally attributed
to dynamic linking: code reuse, streamlined software updates, and reduced
disk/network use are actually benefits of shared libraries. Thus, we present
stable linking, a new mechanism for using dependencies that uses shared
libraries to retain their benefits but eliminates the downsides of dynamic
linking. Stable linking separates a system's state into management times; when
the system can be modified, and epochs when it cannot. Stable linking
calculates each application's relocation mapping at the beginning of each
epoch, allows developers to inspect the relocation mapping during the epoch,
and reuses the mapping for subsequent executions in the epoch. We design and
build MatR, the first stable linker. We use MatR in three workloads and show
that it improves upon dynamic linking performance by a factor of 2.19 on
average. Additionally, we use the system in three vignettes, or case-studies,
that illustrate the system's improvements to dependency management.
</summary>
    <author>
      <name>Farid Zakaria</name>
    </author>
    <author>
      <name>Andrew Quinn</name>
    </author>
    <author>
      <name>Thomas R. W. Scogland</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.06716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.06716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.11179v2</id>
    <updated>2025-03-19T21:47:24Z</updated>
    <published>2025-01-19T21:37:57Z</published>
    <title>Coach: Exploiting Temporal Patterns for All-Resource Oversubscription in
  Cloud Platforms</title>
    <summary>  Cloud platforms remain underutilized despite multiple proposals to improve
their utilization (e.g., disaggregation, harvesting, and oversubscription). Our
characterization of the resource utilization of virtual machines (VMs) in Azure
reveals that, while CPU is the main underutilized resource, we need to provide
a solution to manage all resources holistically. We also observe that many VMs
exhibit complementary temporal patterns, which can be leveraged to improve the
oversubscription of underutilized resources.
  Based on these insights, we propose Coach: a system that exploits temporal
patterns for all-resource oversubscription in cloud platforms. Coach uses
long-term predictions and an efficient VM scheduling policy to exploit
temporally complementary patterns. We introduce a new general-purpose VM type,
called CoachVM, where we partition each resource allocation into a guaranteed
and an oversubscribed portion. Coach monitors the oversubscribed resources to
detect contention and mitigate any potential performance degradation. We focus
on memory management, which is particularly challenging due to memory's
sensitivity to contention and the overhead required to reassign it between
CoachVMs. Our experiments show that Coach enables platforms to host up to ~26%
more VMs with minimal performance degradation.
</summary>
    <author>
      <name>Benjamin Reidys</name>
    </author>
    <author>
      <name>Pantea Zardoshti</name>
    </author>
    <author>
      <name>Íñigo Goiri</name>
    </author>
    <author>
      <name>Celine Irvene</name>
    </author>
    <author>
      <name>Daniel S. Berger</name>
    </author>
    <author>
      <name>Haoran Ma</name>
    </author>
    <author>
      <name>Kapil Arya</name>
    </author>
    <author>
      <name>Eli Cortez</name>
    </author>
    <author>
      <name>Taylor Stark</name>
    </author>
    <author>
      <name>Eugene Bak</name>
    </author>
    <author>
      <name>Mehmet Iyigun</name>
    </author>
    <author>
      <name>Stanko Novaković</name>
    </author>
    <author>
      <name>Lisa Hsu</name>
    </author>
    <author>
      <name>Karel Trueba</name>
    </author>
    <author>
      <name>Abhisek Pan</name>
    </author>
    <author>
      <name>Chetan Bansal</name>
    </author>
    <author>
      <name>Saravan Rajmohan</name>
    </author>
    <author>
      <name>Jian Huang</name>
    </author>
    <author>
      <name>Ricardo Bianchini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3669940.3707226</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3669940.3707226" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in 30th ACM International Conference on Architectural
  Support for Programming Languages and Operating Systems, Volume 2
  (ASPLOS'25). 15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.11179v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.11179v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.17707v3</id>
    <updated>2025-05-15T08:01:39Z</updated>
    <published>2025-01-29T15:22:19Z</published>
    <title>vNV-Heap: An Ownership-Based Virtually Non-Volatile Heap for Embedded
  Systems</title>
    <summary>  The Internet of Batteryless Things might revolutionize our understanding of
connected devices by harvesting required operational energy from the
environment. These systems come with the system-software challenge that the
intermittently powered IoT devices have to checkpoint their state in
non-volatile memory to later resume with this state when sufficient energy is
available. The scarce energy resources demand that only modified data is
persisted before a power failure, which requires precise modification tracking.
  We present vNV-Heap, the first ownership-based virtually Non-Volatile Heap
for intermittently powered systems with guaranteed power-failure resilience.
The heap exploits ownership systems, a zero-cost (i.e., compile-time)
abstraction for example implemented by Rust, to track modifications and
virtualize object persistence. To achieve power-failure resilience, our heap is
designed and implemented to guarantee bounded operations by static program code
analysis: For example, the heap allows for determining a worst-case energy
consumption for the operation of persisting modified and currently volatile
objects. The evaluation of our open-source implementation on an embedded
hardware platform (i.e., ESP32-C3) shows that using our heap abstraction is
more energy efficient than existing approaches while also providing runtime
guarantees by static worst-case bounds.
</summary>
    <author>
      <name>Markus Elias Gerber</name>
    </author>
    <author>
      <name>Luis Gerhorst</name>
    </author>
    <author>
      <name>Ishwar Mudraje</name>
    </author>
    <author>
      <name>Kai Vogelgesang</name>
    </author>
    <author>
      <name>Thorsten Herfet</name>
    </author>
    <author>
      <name>Peter Wägemann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3735452.3735534</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3735452.3735534" rel="related"/>
    <link href="http://arxiv.org/abs/2501.17707v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.17707v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.02750v1</id>
    <updated>2025-02-04T22:37:17Z</updated>
    <published>2025-02-04T22:37:17Z</published>
    <title>Cache is King: Smart Page Eviction with eBPF</title>
    <summary>  The page cache is a central part of an OS. It reduces repeated accesses to
storage by deciding which pages to retain in memory. As a result, the page
cache has a significant impact on the performance of many applications.
However, its one-size-fits-all eviction policy performs poorly in many
workloads. While the systems community has experimented with a plethora of new
and adaptive eviction policies in non-OS settings (e.g., key-value stores,
CDNs), it is very difficult to implement such policies in the page cache, due
to the complexity of modifying kernel code. To address these shortcomings, we
design a novel eBPF-based framework for the Linux page cache, called
$\texttt{cachebpf}$, that allows developers to customize the page cache without
modifying the kernel. $\texttt{cachebpf}$ enables applications to customize the
page cache policy for their specific needs, while also ensuring that different
applications' policies do not interfere with each other and preserving the page
cache's ability to share memory across different processes. We demonstrate the
flexibility of $\texttt{cachebpf}$'s interface by using it to implement several
eviction policies. Our evaluation shows that it is indeed beneficial for
applications to customize the page cache to match their workloads' unique
properties, and that they can achieve up to 70% higher throughput and 58% lower
tail latency.
</summary>
    <author>
      <name>Tal Zussman</name>
    </author>
    <author>
      <name>Ioannis Zarkadas</name>
    </author>
    <author>
      <name>Jeremy Carin</name>
    </author>
    <author>
      <name>Andrew Cheng</name>
    </author>
    <author>
      <name>Hubertus Franke</name>
    </author>
    <author>
      <name>Jonas Pfefferle</name>
    </author>
    <author>
      <name>Asaf Cidon</name>
    </author>
    <link href="http://arxiv.org/abs/2502.02750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.02750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.06609v1</id>
    <updated>2025-02-10T16:07:55Z</updated>
    <published>2025-02-10T16:07:55Z</published>
    <title>Automatic ISA analysis for Secure Context Switching</title>
    <summary>  Instruction set architectures are complex, with hundreds of registers and
instructions that can modify dozens of them during execution, variably on each
instance. Prose-style ISA specifications struggle to capture these intricacies
of the ISAs, where often the important details about a single register are
spread out across hundreds of pages of documentation. Ensuring that all
ISA-state is swapped in context switch implementations of privileged software
requires meticulous examination of these pages. This manual process is tedious
and error-prone.
  We propose a tool called Sailor that leverages machine-readable ISA
specifications written in Sail to automate this task. Sailor determines the
ISA-state necessary to swap during the context switch using the data collected
from Sail and a novel algorithm to classify ISA-state as security-sensitive.
Using Sailor's output, we identify three different classes of mishandled
ISA-state across four open-source confidential computing systems. We further
reveal five distinct security vulnerabilities that can be exploited using the
mishandled ISA-state. This research exposes an often overlooked attack surface
that stems from mishandled ISA-state, enabling unprivileged adversaries to
exploit system vulnerabilities.
</summary>
    <author>
      <name>Neelu S. Kalani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">EPFL Switzerland</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Research Zurich</arxiv:affiliation>
    </author>
    <author>
      <name>Thomas Bourgeat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">EPFL Switzerland</arxiv:affiliation>
    </author>
    <author>
      <name>Guerney D. H. Hunt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM T. J. Watson Research Center</arxiv:affiliation>
    </author>
    <author>
      <name>Wojciech Ozga</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBM Research Zurich</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 6 figures, 2 tables, 4 listings</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.06609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.06609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.07118v1</id>
    <updated>2025-02-10T23:19:48Z</updated>
    <published>2025-02-10T23:19:48Z</published>
    <title>Analyzing Configuration Dependencies of File Systems</title>
    <summary>  File systems play an essential role in modern society for managing precious
data. To meet diverse needs, they often support many configuration parameters.
Such flexibility comes at the price of additional complexity which can lead to
subtle configuration-related issues. To address this challenge, we study the
configuration-related issues of two major file systems (i.e., Ext4 and XFS) in
depth, and identify a prevalent pattern called multilevel configuration
dependencies. Based on the study, we build an extensible tool called ConfD to
extract the dependencies automatically, and create a set of plugins to address
different configuration-related issues. Our experiments on Ext4, XFS and a
modern copy-on-write file system (i.e., ZFS) show that ConfD was able to
extract 160 configuration dependencies for the file systems with a low false
positive rate. Moreover, the dependency-guided plugins can identify various
configuration issues (e.g., mishandling of configurations, regression test
failures induced by valid configurations). In addition, we also explore the
applicability of ConfD on a popular storage engine (i.e., WiredTiger). We hope
that this comprehensive analysis of configuration dependencies of storage
systems can shed light on addressing configuration-related challenges for the
system community in general.
</summary>
    <author>
      <name>Tabassum Mahmud</name>
    </author>
    <author>
      <name>Om Rameshwar Gatla</name>
    </author>
    <author>
      <name>Duo Zhang</name>
    </author>
    <author>
      <name>Carson Love</name>
    </author>
    <author>
      <name>Ryan Bumann</name>
    </author>
    <author>
      <name>Varun S Girimaji</name>
    </author>
    <author>
      <name>Mai Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.07118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.07118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.10923v2</id>
    <updated>2025-02-18T18:25:56Z</updated>
    <published>2025-02-15T22:47:05Z</published>
    <title>Phoenix -- A Novel Technique for Performance-Aware Orchestration of
  Thread and Page Table Placement in NUMA Systems</title>
    <summary>  The emergence of symmetric multi-processing (SMP) systems with non-uniform
memory access (NUMA) has prompted extensive research on process and data
placement to mitigate the performance impact of NUMA on applications. However,
existing solutions often overlook the coordination between the CPU scheduler
and memory manager, leading to inefficient thread and page table placement.
Moreover, replication techniques employed to improve locality suffer from
redundant replicas, scalability barriers, and performance degradation due to
memory bandwidth and inter-socket interference. In this paper, we present
Phoenix, a novel integrated CPU scheduler and memory manager with on-demand
page table replication mechanism. Phoenix integrates the CPU scheduler and
memory management subsystems, allowing for coordinated thread and page table
placement. By differentiating between data and page table pages, Phoenix
enables direct migration or replication of page tables based on application
behavior. Additionally, Phoenix employs memory bandwidth management mechanism
to maintain Quality of Service (QoS) while mitigating coherency maintenance
overhead. We implemented Phoenix as a loadable kernel module for Linux,
ensuring compatibility with legacy applications and ease of deployment. Our
evaluation on real hardware demonstrates that Phoenix reduces CPU cycles by
2.09x and page-walk cycles by 1.58x compared to state-of-the-art solutions.
</summary>
    <author>
      <name>Mohammad Siavashi</name>
    </author>
    <author>
      <name>Alireza Sanaee</name>
    </author>
    <author>
      <name>Mohsen Sharifi</name>
    </author>
    <author>
      <name>Gianni Antichi</name>
    </author>
    <link href="http://arxiv.org/abs/2502.10923v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.10923v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.18832v2</id>
    <updated>2025-04-28T08:31:36Z</updated>
    <published>2025-02-26T05:16:06Z</published>
    <title>Safe and usable kernel extensions with Rex</title>
    <summary>  Safe kernel extensions have gained significant traction, evolving from simple
packet filters to large, complex programs that customize storage, networking,
and scheduling. Existing kernel extension mechanisms like eBPF rely on
in-kernel verifiers to ensure safety of kernel extensions by static
verification using symbolic execution. We identify significant usability issues
-- safe extensions being rejected by the verifier -- due to the
language-verifier gap, a mismatch between developers' expectation of program
safety provided by a contract with the programming language, and the verifier's
expectation.
  We present Rex, a new kernel extension framework that closes the
language-verifier gap and improves the usability of kernel extensions in terms
of programming experience and maintainability. Rex builds upon language-based
safety to provide safety properties desired by kernel extensions, along with a
lightweight extralingual runtime for properties that are unsuitable for static
analysis, including safe exception handling, stack safety, and termination.
With Rex, kernel extensions are written in safe Rust and interact with the
kernel via a safe interface provided by Rex's kernel crate. No separate static
verification is needed. Rex addresses usability issues of eBPF kernel
extensions without compromising performance.
</summary>
    <author>
      <name>Jinghao Jia</name>
    </author>
    <author>
      <name>Ruowen Qin</name>
    </author>
    <author>
      <name>Milo Craun</name>
    </author>
    <author>
      <name>Egor Lukiyanov</name>
    </author>
    <author>
      <name>Ayush Bansal</name>
    </author>
    <author>
      <name>Michael V. Le</name>
    </author>
    <author>
      <name>Hubertus Franke</name>
    </author>
    <author>
      <name>Hani Jamjoom</name>
    </author>
    <author>
      <name>Tianyin Xu</name>
    </author>
    <author>
      <name>Dan Williams</name>
    </author>
    <link href="http://arxiv.org/abs/2502.18832v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.18832v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.01390v1</id>
    <updated>2025-03-03T10:41:57Z</updated>
    <published>2025-03-03T10:41:57Z</published>
    <title>Scalable and Accurate Application-Level Crash-Consistency Testing via
  Representative Testing</title>
    <summary>  Crash consistency is essential for applications that must persist data.
Crash-consistency testing has been commonly applied to find crash-consistency
bugs in applications. The crash-state space grows exponentially as the number
of operations in the program increases, necessitating techniques for pruning
the search space. However, state-of-the-art crash-state space pruning is far
from ideal. Some techniques look for known buggy patterns or bound the
exploration for efficiency, but they sacrifice coverage and may miss bugs
lodged deep within applications. Other techniques eliminate redundancy in the
search space by skipping identical crash states, but they still fail to scale
to larger applications.
  In this work, we propose representative testing: a new crash-state space
reduction strategy that achieves high scalability and high coverage. Our key
observation is that the consistency of crash states is often correlated, even
if those crash states are not identical. We build Pathfinder, a
crash-consistency testing tool that implements an update behaviors-based
heuristic to approximate a small set of representative crash states.
  We evaluate Pathfinder on POSIX-based and MMIO-based applications, where it
finds 18 (7 new) bugs across 8 production-ready systems. Pathfinder scales more
effectively to large applications than prior works and finds 4x more bugs in
POSIX-based applications and 8x more bugs in MMIO-based applications compared
to state-of-the-art systems.
</summary>
    <author>
      <name>Yile Gu</name>
    </author>
    <author>
      <name>Ian Neal</name>
    </author>
    <author>
      <name>Jiexiao Xu</name>
    </author>
    <author>
      <name>Shaun Christopher Lee</name>
    </author>
    <author>
      <name>Ayman Said</name>
    </author>
    <author>
      <name>Musa Haydar</name>
    </author>
    <author>
      <name>Jacob Van Geffen</name>
    </author>
    <author>
      <name>Rohan Kadekodi</name>
    </author>
    <author>
      <name>Andrew Quinn</name>
    </author>
    <author>
      <name>Baris Kasikci</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3720431</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3720431" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">OOPSLA 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.01390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.01390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.01801v2</id>
    <updated>2025-03-05T20:36:51Z</updated>
    <published>2025-03-03T18:32:31Z</published>
    <title>TUNA: Tuning Unstable and Noisy Cloud Applications</title>
    <summary>  Autotuning plays a pivotal role in optimizing the performance of systems,
particularly in large-scale cloud deployments. One of the main challenges in
performing autotuning in the cloud arises from performance variability. We
first investigate the extent to which noise slows autotuning and find that as
little as $5\%$ noise can lead to a $2.5$x slowdown in converging to the
best-performing configuration. We measure the magnitude of noise in cloud
computing settings and find that while some components (CPU, disk) have almost
no performance variability, there are still sources of significant variability
(caches, memory). Furthermore, variability leads to autotuning finding unstable
configurations. As many as $63.3\%$ of the configurations selected as "best"
during tuning can have their performance degrade by $30\%$ or more when
deployed. Using this as motivation, we propose a novel approach to improve the
efficiency of autotuning systems by (a) detecting and removing outlier
configurations and (b) using ML-based approaches to provide a more stable true
signal of de-noised experiment results to the optimizer. The resulting system,
TUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence
and robust configurations. Tuning postgres running mssales, an enterprise
production workload, we find that TUNA can lead to $1.88$x lower running time
on average with $2.58x$ lower standard deviation compared to traditional
sampling methodologies.
</summary>
    <author>
      <name>Johannes Freischuetz</name>
    </author>
    <author>
      <name>Konstantinos Kanellis</name>
    </author>
    <author>
      <name>Brian Kroth</name>
    </author>
    <author>
      <name>Shivaram Venkataraman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3689031.3717480</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3689031.3717480" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 20 figures, EuroSys'25</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.01801v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.01801v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.03722v3</id>
    <updated>2025-04-03T22:58:50Z</updated>
    <published>2025-03-05T18:21:34Z</published>
    <title>When Radiation Meets Linux: Analyzing Soft Errors in Linux on COTS SoCs
  under Proton Irradiation</title>
    <summary>  The increasing use of Linux on commercial off-the-shelf (COTS) system-on-chip
(SoC) in spaceborne computing inherits COTS susceptibility to radiation-induced
failures like soft errors. Modern SoCs exacerbate this issue as aggressive
transistor scaling reduces critical charge thresholds to induce soft errors and
increases radiation effects within densely packed transistors, degrading
overall reliability. Linux's monolithic architecture amplifies these risks, as
tightly coupled kernel subsystems propagate errors to critical components
(e.g., memory management), while limited error-correcting code (ECC) offers
minimal mitigation. Furthermore, the lack of public soft error data from
irradiation tests on COTS SoCs running Linux hinders reliability improvements.
This study evaluates proton irradiation effects (20-50 MeV) on Linux across
three COTS SoC architectures: Raspberry Pi Zero 2 W (40 nm CMOS, Cortex-A53),
NXP i MX 8M Plus (14 nm FinFET, Cortex-A53), and OrangeCrab (40 nm FPGA,
RISC-V). Irradiation results show the 14 nm FinFET NXP SoC achieved 2-3x longer
Linux uptime without ECC memory versus both 40 nm CMOS counterparts, partially
due to FinFET's reduced charge collection. Additionally, this work presents the
first cross-architecture analysis of soft error-prone Linux kernel components
in modern SoCs to develop targeted mitigations. The findings establish
foundational data on Linux's soft error sensitivity in COTS SoCs, guiding
mission readiness for space applications.
</summary>
    <author>
      <name>Saad Memon</name>
    </author>
    <author>
      <name>Rafal Graczyk</name>
    </author>
    <author>
      <name>Tomasz Rajkowski</name>
    </author>
    <author>
      <name>Jan Swakon</name>
    </author>
    <author>
      <name>Damian Wrobel</name>
    </author>
    <author>
      <name>Sebastian Kusyk</name>
    </author>
    <author>
      <name>Seth Roffe</name>
    </author>
    <author>
      <name>Mike Papadakis</name>
    </author>
    <link href="http://arxiv.org/abs/2503.03722v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.03722v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.22089v1</id>
    <updated>2025-03-28T02:09:27Z</updated>
    <published>2025-03-28T02:09:27Z</published>
    <title>Saving Storage Space Using Files on the Web</title>
    <summary>  As conventional storage density reaches its physical limits, the cost of a
gigabyte of storage is no longer plummeting, but rather has remained mostly
flat for the past decade. Meanwhile, file sizes continue to grow, leading to
ever fuller drives. When a user's storage is full, they must disrupt their
workflow to laboriously find large files that are good candidates for deletion.
Separately, the web acts as a distributed storage network, providing free
access to petabytes of redundant files across 200 million websites. An
automated method of restoring files from the web would enable more efficient
storage management, since files readily recoverable from the web would make
good candidates for removal. Despite this, there are no prescribed methods for
automatically detecting these files and ensuring their easy recoverability from
the web, as little is known about either the biggest files of users or their
origins on the web. This study thus seeks to determine what files consume the
most space in users' storage, and from this, to propose an automated method to
select candidate files for removal. Our investigations show 989 MB of storage
per user can be saved by inspecting preexisting metadata of their 25 largest
files alone, with file recovery from the web 3 months later. This demonstrates
the feasibility of applying such a method in a climate of increasingly scarce
local storage resources.
</summary>
    <author>
      <name>Kevin Saric</name>
    </author>
    <author>
      <name>Gowri Sankar Ramachandran</name>
    </author>
    <author>
      <name>Raja Jurdak</name>
    </author>
    <author>
      <name>Surya Nepal</name>
    </author>
    <link href="http://arxiv.org/abs/2503.22089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.22089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.23611v3</id>
    <updated>2025-04-21T16:57:36Z</updated>
    <published>2025-03-30T22:25:18Z</published>
    <title>My CXL Pool Obviates Your PCIe Switch</title>
    <summary>  Pooling PCIe devices across multiple hosts offers a promising solution to
mitigate stranded I/O resources, enhance device utilization, address device
failures, and reduce total cost of ownership. The only viable option today are
PCIe switches, which decouple PCIe devices from hosts by connecting them
through a hardware switch. However, the high cost and limited flexibility of
PCIe switches hinder their widespread adoption beyond specialized datacenter
use cases.
  This paper argues that PCIe device pooling can be effectively implemented in
software using CXL memory pools. CXL memory pools improve memory utilization
and already have positive return on investment. We find that, once CXL pools
are in place, they can serve as a building block for pooling any kind of PCIe
device. We demonstrate that PCIe devices can directly use CXL memory as I/O
buffers without device modifications, which enables routing PCIe traffic
through CXL pool memory. This software-based approach is deployable on today's
hardware and is more flexible than hardware PCIe switches. In particular, we
explore how disaggregating devices such as NICs can transform datacenter
infrastructure.
</summary>
    <author>
      <name>Yuhong Zhong</name>
    </author>
    <author>
      <name>Daniel S. Berger</name>
    </author>
    <author>
      <name>Pantea Zardoshti</name>
    </author>
    <author>
      <name>Enrique Saurez</name>
    </author>
    <author>
      <name>Jacob Nelson</name>
    </author>
    <author>
      <name>Antonis Psistakis</name>
    </author>
    <author>
      <name>Joshua Fried</name>
    </author>
    <author>
      <name>Asaf Cidon</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3713082.3730393</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3713082.3730393" rel="related"/>
    <link href="http://arxiv.org/abs/2503.23611v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.23611v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.09164v1</id>
    <updated>2025-05-14T05:55:44Z</updated>
    <published>2025-05-14T05:55:44Z</published>
    <title>Adaptive Migration Decision for Multi-Tenant Memory Systems</title>
    <summary>  Tiered memory systems consisting of fast small memory and slow large memory
have emerged to provide high capacity memory in a cost-effective way. The
effectiveness of tiered memory systems relies on how many memory accesses can
be absorbed by the fast first-tier memory by page migration. The recent studies
proposed several different ways of detecting hot pages and migrating them
efficiently. However, our investigation shows that page migration is not always
beneficial as it has the associated cost of detecting and migrating hot pages.
When an application is unfriendly to migration, it is often better not to
migrate pages at all. Based on the observation on migration friendliness, this
paper proposes a migration control framework for multi-tenant tiered memory
systems. First, it proposes a detection mechanism for migration friendliness,
using per-page ping-pong status. Ping-pong pages which are promoted and demoted
repeatedly in a short period of time tells migration effectiveness. Based on
their change behaviors, migration is stopped or continued. After the page
migration is stopped, the second mechanism detects changes of memory access
patterns in a low cost way to determine whether migration needs to be resumed.
Finally, as each application has a different behavior, our framework provides
per-process migration control to selectively stop and start migration depending
on application characteristics. We implement the framework in the Linux kernel.
The evaluation with a commercial CXL-based tiered memory system shows that it
effectively controls migration in single and multi-tenant environments.
</summary>
    <author>
      <name>Hyungjun Cho</name>
    </author>
    <author>
      <name>Igjae Kim</name>
    </author>
    <author>
      <name>Kwanghoon Choi</name>
    </author>
    <author>
      <name>Hongjin Kim</name>
    </author>
    <author>
      <name>Wonjae Lee</name>
    </author>
    <author>
      <name>Junhyeok Im</name>
    </author>
    <author>
      <name>Jinin So</name>
    </author>
    <author>
      <name>Jaehyuk Huh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.09164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.09164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00897v1</id>
    <updated>2017-03-02T18:52:45Z</updated>
    <published>2017-03-02T18:52:45Z</published>
    <title>Adapting the DMTCP Plugin Model for Checkpointing of Hardware Emulation</title>
    <summary>  Checkpoint-restart is now a mature technology. It allows a user to save and
later restore the state of a running process. The new plugin model for the
upcoming version 3.0 of DMTCP (Distributed MultiThreaded Checkpointing) is
described here. This plugin model allows a target application to disconnect
from the hardware emulator at checkpoint time and then re-connect to a possibly
different hardware emulator at the time of restart. The DMTCP plugin model is
important in allowing three distinct parties to seamlessly inter-operate. The
three parties are: the EDA designer, who is concerned with formal verification
of a circuit design; the DMTCP developers, who are concerned with providing
transparent checkpointing during the circuit emulation; and the hardware
emulator vendor, who provides a plugin library that responds to checkpoint,
restart, and other events.
  The new plugin model is an example of process-level virtualization:
virtualization of external abstractions from within a process. This capability
is motivated by scenarios for testing circuit models with the help of a
hardware emulator. The plugin model enables a three-way collaboration: allowing
a circuit designer and emulator vendor to each contribute separate proprietary
plugins while sharing an open source software framework from the DMTCP
developers. This provides a more flexible platform, where different fault
injection models based on plugins can be designed within the DMTCP
checkpointing framework. After initialization, one restarts from a checkpointed
state under the control of the desired plugin. This restart saves the time
spent in simulating the initialization phase, while enabling fault injection
exactly at the region of interest. Upon restart, one can inject faults or
otherwise modify the remainder of the simulation. The work concludes with a
brief survey of checkpointing and process-level virtualization.
</summary>
    <author>
      <name>Rohan Garg</name>
    </author>
    <author>
      <name>Kapil Arya</name>
    </author>
    <author>
      <name>Jiajun Cao</name>
    </author>
    <author>
      <name>Gene Cooperman</name>
    </author>
    <author>
      <name>Jeff Evans</name>
    </author>
    <author>
      <name>Ankit Garg</name>
    </author>
    <author>
      <name>Neil A. Rosenberg</name>
    </author>
    <author>
      <name>K. Suresh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 11 figure, 1 listing; SELSE '17, March 21--22, 2017, Boston,
  MA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.00897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.6.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.09701v1</id>
    <updated>2017-05-26T20:03:18Z</updated>
    <published>2017-05-26T20:03:18Z</published>
    <title>SMORE: A Cold Data Object Store for SMR Drives (Extended Version)</title>
    <summary>  Shingled magnetic recording (SMR) increases the capacity of magnetic hard
drives, but it requires that each zone of a disk be written sequentially and
erased in bulk. This makes SMR a good fit for workloads dominated by large data
objects with limited churn. To explore this possibility, we have developed
SMORE, an object storage system designed to reliably and efficiently store
large, seldom-changing data objects on an array of host-managed or host-aware
SMR disks.
  SMORE uses a log-structured approach to accommodate the constraint that all
writes to an SMR drive must be sequential within large shingled zones. It
stripes data across zones on separate disks, using erasure coding to protect
against drive failure. A separate garbage collection thread reclaims space by
migrating live data out of the emptiest zones so that they can be trimmed and
reused. An index stored on flash and backed up to the SMR drives maps object
identifiers to on-disk locations. SMORE interleaves log records with object
data within SMR zones to enable index recovery after a system crash (or failure
of the flash device) without any additional logging mechanism.
  SMORE achieves full disk bandwidth when ingesting data---with a variety of
object sizes---and when reading large objects. Read performance declines for
smaller object sizes where inter- object seek time dominates. With a worst-case
pattern of random deletions, SMORE has a write amplification (not counting RAID
parity) of less than 2.0 at 80% occupancy. By taking an index snapshot every
two hours, SMORE recovers from crashes in less than a minute. More frequent
snapshots allow faster recovery.
</summary>
    <author>
      <name>Peter Macko</name>
    </author>
    <author>
      <name>Xiongzi Ge</name>
    </author>
    <author>
      <name>John Haskins Jr.</name>
    </author>
    <author>
      <name>James Kelley</name>
    </author>
    <author>
      <name>David Slik</name>
    </author>
    <author>
      <name>Keith A. Smith</name>
    </author>
    <author>
      <name>Maxim G. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 8 figures, full version of 6 page paper published at MSST
  2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.09701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.03701v1</id>
    <updated>2019-03-08T23:58:52Z</updated>
    <published>2019-03-08T23:58:52Z</published>
    <title>Processor in Non-Volatile Memory (PiNVSM): Towards to Data-centric
  Computing in Decentralized Environment</title>
    <summary>  The AI problem has no solution in the environment of existing hardware stack
and OS architecture. CPU-centric model of computation has a huge number of
drawbacks that originate from memory hierarchy and obsolete architecture of the
computing core. The concept of mixing memory and logic has been around since
1960s. However, the concept of Processor-In-Memory (PIM) is unable to resolve
the critical issues of the CPU-centric computing model because of inevitable
replication of von Neumann architecture's drawbacks. The next generation of
NVM/SCM memory is able to give the second birth to the data-centric computing
paradigm. This paper presents a concept of Processor in Non-Volatile Memory
(PiNVSM) architecture. The basis of PiNVSM architecture is the concept of DPU
that contains the NVM memory and dedicated PU. All necessary PU's registers can
be implemented in the space of NVM memory. NVM memory of DPU is the single
space for storing and transformation of data. In the basis of PiNVSM
architecture lies the DPU array is able to overcome the limitations as Turing
machine model as von Neumann architecture. The DPU array hasn't a centralized
computing core. Every data portion has dedicated computing core that excludes
the necessity to transfer data to the place of data processing. Every DPU
contains data portion that is associated with the set of keywords. Any complex
data structure can be split on elementary items that can be stored into
independent DPU with dedicated computing core(s). One DPU is able to apply the
elementary transformation on one item. But the DPU array is able to make the
transformation of complex structure by means of concurrent execution of
elementary transformations in different DPUs. The PiNVSM architecture suggests
a principally new architecture of the computing core that creates a new
opportunity for data self-organization, data and code synthesis.
</summary>
    <author>
      <name>Viacheslav Dubeyko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1805.09612 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.03701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.03701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.11648v3</id>
    <updated>2019-12-26T05:58:49Z</updated>
    <published>2019-08-30T10:53:00Z</published>
    <title>Porting of eChronos RTOS on RISC-V Architecture</title>
    <summary>  eChronos is a formally verified Real Time Operating System(RTOS) designed for
embedded micro-controllers. eChronos was targeted for tightly constrained
devices without memory management units. Currently, eChronos is available on
proprietary designs like ARM, PowerPC and Intel architectures. eChronos is
adopted in safety critical systems like aircraft control system and medical
implant devices. eChronos is one of the very few system software not been
ported to RISC-V. RISC-V is an open-source Instruction Set Architecture (ISA)
that enables new era of processor development. Many standard Operating Systems,
software tool chain have migrated to the RISC-V architecture. According to the
latest trends, RISC-V is replacing many proprietary chips. As a secure RTOS, it
is attractive to port on an open-source ISA. SHAKTI and PicoRV32 are some of
the proven open-source RISC-V designs available. Now having a secure RTOS on an
open-source hardware design, designed based on an open-source ISA makes it more
interesting. In addition to this, the current architectures supported by
eChronos are all proprietary designs, and porting eChronos to the RISC-V
architecture increases the secure system development as a whole. This paper,
presents an idea of porting eChronos on a chip which is open-source and
effective, thus reducing the cost of embedded systems. Designing a open-source
system that is completely open-source reduces the overall cost, increased the
security and can be critically reviewed. This paper explores the design and
architecture aspect involved in porting eChronos to RISC-V. The authors have
successfully ported eChronos to RISC-V architecture and verified it on spike.
The port of RISC-V to eChronos is made available open-source by authors. Along
with that, the safe removal of architectural dependencies and subsequent
changes in eChronos are also analyzed.
</summary>
    <author>
      <name>Shubhendra Pal Singhal</name>
    </author>
    <author>
      <name>M. Sridevi</name>
    </author>
    <author>
      <name>N Sathya Narayanan</name>
    </author>
    <author>
      <name>M J Shankar Raman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, Accepted for Publication for Springer LNCS
  Germany</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.11648v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.11648v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.09277v1</id>
    <updated>2019-10-21T11:59:01Z</updated>
    <published>2019-10-21T11:59:01Z</published>
    <title>PiBooster: A Light-Weight Approach to Performance Improvements in Page
  Table Management for Paravirtual Virtual-Machines</title>
    <summary>  In paravirtualization, the page table management components of the guest
operating systems are properly patched for the security guarantees of the
hypervisor. However, none of them pay enough attentions to the performance
improvements, which results in two noticeable performance issues. First, such
security patches exacerbate the problem that the execution paths of the guest
page table (de)allocations become extremely long, which would consequently
increase the latencies of process creations and exits. Second, the patches
introduce many additional IOTLB flushes, leading to extra IOTLB misses, and the
misses would have negative impacts on I/O performance of all peripheral
devices. In this paper, we propose PiBooster, a novel lightweight approach for
improving the performance in page table management. First, PiBooster shortens
the execution paths of the page table (de)allocations by the PiBooster cache,
which maintains dedicated buffers for serving page table (de)allocations.
Second, PiBooster eliminates the additional IOTLB misses with a fine-grained
validation scheme, which performs page table and DMA validations separately,
instead of doing both together. We implement a prototype on Xen with Linux as
the guest kernel. We do small modifications on Xen (166 SLoC) and Linux kernel
(350 SLoC). We evaluate the I/O performance in both micro and macro ways. The
micro experiment results indicate that PiBooster is able to completely
eliminate the additional IOTLB flushes in the workload-stable environments, and
effectively reduces (de)allocation time of the page table by 47% on average.
The macro benchmarks show that the latencies of the process creations and exits
are expectedly reduced by 16% on average. Moreover, the SPECINT,lmbench and
netperf results indicate that PiBooster has no negative performance impacts on
CPU computation, network I/O, and disk I/O.
</summary>
    <author>
      <name>Zhi Zhang</name>
    </author>
    <author>
      <name>Yueqiang Cheng</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CLOUD.2016.0074</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CLOUD.2016.0074" rel="related"/>
    <link href="http://arxiv.org/abs/1910.09277v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.09277v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.05168v1</id>
    <updated>2020-03-11T08:58:29Z</updated>
    <published>2020-03-11T08:58:29Z</published>
    <title>Multi-Rate Fluid Scheduling of Mixed-Criticality Systems on
  Multiprocessors</title>
    <summary>  In this paper we consider the problem of mixed-criticality (MC) scheduling of
implicit-deadline sporadic task systems on a homogenous multiprocessor
platform. Focusing on dual-criticality systems, algorithms based on the fluid
scheduling model have been proposed in the past. These algorithms use a
dual-rate execution model for each high-criticality task depending on the
system mode. Once the system switches to the high-criticality mode, the
execution rates of such tasks are increased to meet their increased demand.
Although these algorithms are speed-up optimal, they are unable to schedule
several feasible dual-criticality task systems. This is because a single fixed
execution rate for each high-criticality task after the mode switch is not
efficient to handle the high variability in demand during the transition period
immediately following the mode switch. This demand variability exists as long
as the carry-over jobs of high-criticality tasks, that is jobs released before
the mode switch, have not completed. Addressing this shortcoming, we propose a
multi-rate fluid execution model for dual-criticality task systems in this
paper. Under this model, high-criticality tasks are allocated varying execution
rates in the transition period after the mode switch to efficiently handle the
demand variability. We derive a sufficient schedulability test for the proposed
model and show its dominance over the dual-rate fluid execution model. Further,
we also present a speed-up optimal rate assignment strategy for the multi-rate
model, and experimentally show that the proposed model outperforms all the
existing MC scheduling algorithms with known speed-up bounds.
</summary>
    <author>
      <name>Saravanan Ramanathan</name>
    </author>
    <author>
      <name>Arvind Easwaran</name>
    </author>
    <author>
      <name>Hyeonjoong Cho</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11241-017-9296-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11241-017-9296-1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a post-peer-review, pre-copyedit version of an article
  published in Real-Time Systems. The final authenticated version is available
  online at the below DOI</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Springer Real-Time Systems, Issue 54, pages 247-277, April 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.05168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.05168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.05442v1</id>
    <updated>2020-03-11T03:32:55Z</updated>
    <published>2020-03-11T03:32:55Z</published>
    <title>Combining Task-level and System-level Scheduling Modes for Mixed
  Criticality Systems</title>
    <summary>  Different scheduling algorithms for mixed criticality systems have been
recently proposed. The common denominator of these algorithms is to discard low
critical tasks whenever high critical tasks are in lack of computation
resources. This is achieved upon a switch of the scheduling mode from Normal to
Critical. We distinguish two main categories of the algorithms: system-level
mode switch and task-level mode switch. System-level mode algorithms allow low
criticality (LC) tasks to execute only in normal mode. Task-level mode switch
algorithms enable to switch the mode of an individual high criticality task
(HC), from low (LO) to high (HI), to obtain priority over all LC tasks. This
paper investigates an online scheduling algorithm for mixed-criticality systems
that supports dynamic mode switches for both task level and system level. When
a HC task job overruns its LC budget, then only that particular job is switched
to HI mode. If the job cannot be accommodated, then the system switches to
Critical mode. To accommodate for resource availability of the HC jobs, the LC
tasks are degraded by stretching their periods until the Critical mode
exhibiting job complete its execution. The stretching will be carried out until
the resource availability is met. We have mechanized and implemented the
proposed algorithm using Uppaal. To study the efficiency of our scheduling
algorithm, we examine a case study and compare our results to the state of the
art algorithms.
</summary>
    <author>
      <name>Jalil Boudjadar</name>
    </author>
    <author>
      <name>Saravanan Ramanathan</name>
    </author>
    <author>
      <name>Arvind Easwaran</name>
    </author>
    <author>
      <name>Ulrik Nyman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DS-RT47707.2019.8958666</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DS-RT47707.2019.8958666" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">\copyright 2019 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/ACM International Symposium on Distributed Simulation and
  Real Time Applications (DS-RT), Cosenza, Italy, 2019, pages 1-10</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.05442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.05442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.03244v2</id>
    <updated>2020-04-08T17:05:35Z</updated>
    <published>2020-04-07T10:33:37Z</published>
    <title>SoftWear: Software-Only In-Memory Wear-Leveling for Non-Volatile Main
  Memory</title>
    <summary>  Several emerging technologies for byte-addressable non-volatile memory (NVM)
have been considered to replace DRAM as the main memory in computer systems
during the last years. The disadvantage of a lower write endurance, compared to
DRAM, of NVM technologies like Phase-Change Memory (PCM) or Ferroelectric RAM
(FeRAM) has been addressed in the literature. As a solution, in-memory
wear-leveling techniques have been proposed, which aim to balance the
wear-level over all memory cells to achieve an increased memory lifetime.
Generally, to apply such advanced aging-aware wear-leveling techniques proposed
in the literature, additional special hardware is introduced into the memory
system to provide the necessary information about the cell age and thus enable
aging-aware wear-leveling decisions.
  This paper proposes software-only aging-aware wear-leveling based on common
CPU features and does not rely on any additional hardware support from the
memory subsystem. Specifically, we exploit the memory management unit (MMU),
performance counters, and interrupts to approximate the memory write counts as
an aging indicator. Although the software-only approach may lead to slightly
worse wear-leveling, it is applicable on commonly available hardware. We
achieve page-level coarse-grained wear-leveling by approximating the current
cell age through statistical sampling and performing physical memory remapping
through the MMU. This method results in non-uniform memory usage patterns
within a memory page. Hence, we further propose a fine-grained wear-leveling in
the stack region of C / C++ compiled software.
  By applying both wear-leveling techniques, we achieve up to $78.43\%$ of the
ideal memory lifetime, which is a lifetime improvement of more than a factor of
$900$ compared to the lifetime without any wear-leveling.
</summary>
    <author>
      <name>Christian Hakert</name>
    </author>
    <author>
      <name>Kuan-Hsun Chen</name>
    </author>
    <author>
      <name>Paul R. Genssler</name>
    </author>
    <author>
      <name>Georg von der Brüggen</name>
    </author>
    <author>
      <name>Lars Bauer</name>
    </author>
    <author>
      <name>Hussam Amrouch</name>
    </author>
    <author>
      <name>Jian-Jia Chen</name>
    </author>
    <author>
      <name>Jörg Henkel</name>
    </author>
    <link href="http://arxiv.org/abs/2004.03244v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.03244v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.15081v14</id>
    <updated>2022-06-19T04:45:35Z</updated>
    <published>2020-12-30T08:41:51Z</published>
    <title>Fairness-Oriented User Scheduling for Bursty Downlink Transmission Using
  Multi-Agent Reinforcement Learning</title>
    <summary>  In this work, we develop practical user scheduling algorithms for downlink
bursty traffic with emphasis on user fairness. In contrast to the conventional
scheduling algorithms that either equally divides the transmission time slots
among users or maximizing some ratios without physcial meanings, we propose to
use the 5%-tile user data rate (5TUDR) as the metric to evaluate user fairness.
Since it is difficult to directly optimize 5TUDR, we first cast the problem
into the stochastic game framework and subsequently propose a Multi-Agent
Reinforcement Learning (MARL)-based algorithm to perform distributed
optimization on the resource block group (RBG) allocation. Furthermore, each
MARL agent is designed to take information measured by network counters from
multiple network layers (e.g. Channel Quality Indicator, Buffer size) as the
input states while the RBG allocation as action with a proposed reward function
designed to maximize 5TUDR. Extensive simulation is performed to show that the
proposed MARL-based scheduler can achieve fair scheduling while maintaining
good average network throughput as compared to conventional schedulers.
</summary>
    <author>
      <name>Mingqi Yuan</name>
    </author>
    <author>
      <name>Qi Cao</name>
    </author>
    <author>
      <name>Man-on Pun</name>
    </author>
    <author>
      <name>Yi Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 13 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">APSIPA Transactions on Signal and Information Processing (2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.15081v14" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.15081v14" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05306v5</id>
    <updated>2023-11-15T01:49:00Z</updated>
    <published>2021-04-12T09:26:04Z</published>
    <title>WLFC: Write Less in Flash-based Cache</title>
    <summary>  Flash-based disk caches, for example Bcache and Flashcache, has gained
tremendous popularity in industry in the last decade because of its low energy
consumption, non-volatile nature and high I/O speed. But these cache systems
have a worse write performance than the read performance because of the
asymmetric I/O costs and the the internal GC mechanism. In addition to the
performance issues, since the NAND flash is a type of EEPROM device, the
lifespan is also limited by the Program/Erase (P/E) cycles. So how to improve
the performance and the lifespan of flash-based caches in write-intensive
scenarios has always been a hot issue. Benefiting from Open-Channel SSDs
(OCSSDs), we propose a write-friendly flash-based disk cache system, which is
called WLFC (Write Less in the Flash-based Cache). In WLFC, a strictly
sequential writing method is used to minimize the write amplification. A new
replacement algorithm for the write buffer is designed to minimize the erase
count caused by the evicting. And a new data layout strategy is designed to
minimize the metadata size persisted in SSDs. As a result, the Over-Provisioned
(OP) space is completely removed, the erase count of the flash is greatly
reduced, and the metadata size is 1/10 or less than that in BCache. Even with a
small amount of metadata, the data consistency after the crash is still
guaranteed. Compared with the existing mechanism, WLFC brings a 7%-80%
reduction in write latency, a 1.07*-4.5* increment in write throughput, and a
50%-88.9% reduction in erase count, with a moderate overhead in read
performance.
</summary>
    <author>
      <name>Chaos Dong</name>
    </author>
    <author>
      <name>Fang Wang</name>
    </author>
    <author>
      <name>Jianshun Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Need revision</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.05306v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05306v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.08169v1</id>
    <updated>2019-12-16T08:42:59Z</updated>
    <published>2019-12-16T08:42:59Z</published>
    <title>AppStreamer: Reducing Storage Requirements of Mobile Games through
  Predictive Streaming</title>
    <summary>  Storage has become a constrained resource on smartphones. Gaming is a popular
activity on mobile devices and the explosive growth in the number of games
coupled with their growing size contributes to the storage crunch. Even where
storage is plentiful, it takes a long time to download and install a heavy app
before it can be launched. This paper presents AppStreamer, a novel technique
for reducing the storage requirements or startup delay of mobile games, and
heavy mobile apps in general. AppStreamer is based on the intuition that most
apps do not need the entirety of its files (images, audio and video clips,
etc.) at any one time. AppStreamer can, therefore, keep only a small part of
the files on the device, akin to a "cache", and download the remainder from a
cloud storage server or a nearby edge server when it predicts that the app will
need them in the near future. AppStreamer continuously predicts file blocks for
the near future as the user uses the app, and fetches them from the storage
server before the user sees a stall due to missing resources. We implement
AppStreamer at the Android file system layer. This ensures that the apps
require no source code or modification, and the approach generalizes across
apps. We evaluate AppStreamer using two popular games: Dead Effect 2, a 3D
first-person shooter, and Fire Emblem Heroes, a 2D turn-based strategy
role-playing game. Through a user study, 75% and 87% of the users respectively
find that AppStreamer provides the same quality of user experience as the
baseline where all files are stored on the device. AppStreamer cuts down the
storage requirement by 87% for Dead Effect 2 and 86% for Fire Emblem Heroes.
</summary>
    <author>
      <name>Nawanol Theera-Ampornpunt</name>
    </author>
    <author>
      <name>Shikhar Suryavansh</name>
    </author>
    <author>
      <name>Sameer Manchanda</name>
    </author>
    <author>
      <name>Rajesh Panta</name>
    </author>
    <author>
      <name>Kaustubh Joshi</name>
    </author>
    <author>
      <name>Mostafa Ammar</name>
    </author>
    <author>
      <name>Mung Chiang</name>
    </author>
    <author>
      <name>Saurabh Bagchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages; EWSN 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.08169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.08169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.13432v1</id>
    <updated>2020-11-26T18:39:55Z</updated>
    <published>2020-11-26T18:39:55Z</published>
    <title>On the Applicability of PEBS based Online Memory Access Tracking for
  Heterogeneous Memory Management at Scale</title>
    <summary>  Operating systems have historically had to manage only a single type of
memory device. The imminent availability of heterogeneous memory devices based
on emerging memory technologies confronts the classic single memory model and
opens a new spectrum of possibilities for memory management. Transparent data
movement between different memory devices based on access patterns of
applications is a desired feature to make optimal use of such devices and to
hide the complexity of memory management to the end-user. However, capturing
memory access patterns of an application at runtime comes at a cost, which is
particularly challenging for large scale parallel applications that may be
sensitive to system noise.
  In this work, we focus on the access pattern profiling phase prior to the
actual memory relocation. We study the feasibility of using Intel's Processor
Event-Based Sampling (PEBS) feature to record memory accesses by sampling at
runtime and study the overhead at scale. We have implemented a custom PEBS
driver in the IHK/McKernel lightweight multi-kernel operating system, one of
whose advantages is minimal system interference due to the lightweight kernel's
simple design compared to other OS kernels such as Linux. We present the PEBS
overhead of a set of scientific applications and show the access patterns
identified in noise-sensitive HPC applications. Our results show that clear
access patterns can be captured with a 10% overhead in the worst-case and 1% in
the best case when running on up to 128k CPU cores (2,048 Intel Xeon Phi
Knights Landing nodes). We conclude that online memory access profiling using
PEBS at large scale is promising for memory management in heterogeneous memory
environments.
</summary>
    <author>
      <name>Aleix Roca Nonell</name>
    </author>
    <author>
      <name>Balazs Gerofi</name>
    </author>
    <author>
      <name>Leonardo Bautista-Gomez</name>
    </author>
    <author>
      <name>Dominique Martinet</name>
    </author>
    <author>
      <name>Vicenç Beltran Querol</name>
    </author>
    <author>
      <name>Yutaka Ishikawa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3286475.3286477</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3286475.3286477" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 16 figures, conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Workshop on Memory Centric High Performance
  Computing (2018) 50-57</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2011.13432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.13432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.08378v1</id>
    <updated>2022-01-20T18:58:44Z</updated>
    <published>2022-01-20T18:58:44Z</published>
    <title>Adelie: Continuous Address Space Layout Re-randomization for Linux
  Drivers</title>
    <summary>  While address space layout randomization (ASLR) has been extensively studied
for user-space programs, the corresponding OS kernel's KASLR support remains
very limited, making the kernel vulnerable to just-in-time (JIT)
return-oriented programming (ROP) attacks. Furthermore, commodity OSs such as
Linux restrict their KASLR range to 32 bits due to architectural constraints
(e.g., x86-64 only supports 32-bit immediate operands for most instructions),
which makes them vulnerable to even unsophisticated brute-force ROP attacks due
to low entropy. Most in-kernel pointers remain static, exacerbating the problem
when pointers are leaked.
  Adelie, our kernel defense mechanism, overcomes KASLR limitations, increases
KASLR entropy, and makes successful ROP attacks on the Linux kernel much harder
to achieve. First, Adelie enables the position-independent code (PIC) model so
that the kernel and its modules can be placed anywhere in the 64-bit virtual
address space, at any distance apart from each other. Second, Adelie implements
stack re-randomization and address encryption on modules. Finally, Adelie
enables efficient continuous KASLR for modules by using the PIC model to make
it (almost) impossible to inject ROP gadgets through these modules regardless
of gadget's origin.
  Since device drivers (typically compiled as modules) are often developed by
third parties and are typically less tested than core OS parts, they are also
often more vulnerable. By fully re-randomizing device drivers, the last two
contributions together prevent most JIT ROP attacks since vulnerable modules
are very likely to be a starting point of an attack. Furthermore, some OS
instances in virtualized environments are specifically designated to run device
drivers, where drivers are the primary target of JIT ROP attacks. Our
evaluation shows high efficiency of Adelie's approach.
  [full abstract is in the paper]
</summary>
    <author>
      <name>Ruslan Nikolaev</name>
    </author>
    <author>
      <name>Hassan Nadeem</name>
    </author>
    <author>
      <name>Cathlyn Stone</name>
    </author>
    <author>
      <name>Binoy Ravindran</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3503222.3507779</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3503222.3507779" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27th ACM International Conference on Architectural Support for
  Programming Languages and Operating Systems (ASPLOS '22), February 28 - March
  4, 2022, Lausanne, Switzerland</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.08378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.08378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.10618v1</id>
    <updated>2023-07-20T06:36:16Z</updated>
    <published>2023-07-20T06:36:16Z</published>
    <title>FHPM: Fine-grained Huge Page Management For Virtualization</title>
    <summary>  As more data-intensive tasks with large footprints are deployed in virtual
machines (VMs), huge pages are widely used to eliminate the increasing address
translation overhead. However, once the huge page mapping is established, all
the base page regions in the huge page share a single extended page table (EPT)
entry, so that the hypervisor loses awareness of accesses to base page regions.
None of the state-of-the-art solutions can obtain access information at base
page granularity for huge pages. We observe that this can lead to incorrect
decisions by the hypervisor, such as incorrect data placement in a tiered
memory system and unshared base page regions when sharing pages.
  This paper proposes FHPM, a fine-grained huge page management for
virtualization without hardware and guest OS modification. FHPM can identify
access information at base page granularity, and dynamically promote and demote
pages. A key insight of FHPM is to redirect the EPT huge page directory entries
(PDEs) to new companion pages so that the MMU can track access information
within huge pages. Then, FHPM can promote and demote pages according to the
current hot page pressure to balance address translation overhead and memory
usage. At the same time, FHPM proposes a VM-friendly page splitting and
collapsing mechanism to avoid extra VM-exits. In combination, FHPM minimizes
the monitoring and management overhead and ensures that the hypervisor gets
fine-grained VM memory accesses to make the proper decision. We apply FHPM to
improve tiered memory management (FHPM-TMM) and to promote page sharing
(FHPM-Share). FHPM-TMM achieves a performance improvement of up to 33% and 61%
over the pure huge page and base page management. FHPM-Share can save 41% more
memory than Ingens, a state-of-the-art page sharing solution, with comparable
performance.
</summary>
    <author>
      <name>Chuandong Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Peking University</arxiv:affiliation>
    </author>
    <author>
      <name>Sai Sha</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Peking University</arxiv:affiliation>
    </author>
    <author>
      <name>Yangqing Zeng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Peking University</arxiv:affiliation>
    </author>
    <author>
      <name>Xiran Yang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Peking University</arxiv:affiliation>
    </author>
    <author>
      <name>Yingwei Luo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Peking University</arxiv:affiliation>
    </author>
    <author>
      <name>Xiaolin Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Peking University</arxiv:affiliation>
    </author>
    <author>
      <name>Zhenlin Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Michigan Technological University</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2307.10618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.10618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.06513v1</id>
    <updated>2023-09-12T18:42:08Z</updated>
    <published>2023-09-12T18:42:08Z</published>
    <title>RackBlox: A Software-Defined Rack-Scale Storage System with
  Network-Storage Co-Design</title>
    <summary>  Software-defined networking (SDN) and software-defined flash (SDF) have been
serving as the backbone of modern data centers. They are managed separately to
handle I/O requests. At first glance, this is a reasonable design by following
the rack-scale hierarchical design principles. However, it suffers from
suboptimal end-to-end performance, due to the lack of coordination between SDN
and SDF.
  In this paper, we co-design the SDN and SDF stack by redefining the functions
of their control plane and data plane, and splitting up them within a new
architecture named RackBlox. RackBlox decouples the storage management
functions of flash-based solid-state drives (SSDs), and allow the SDN to track
and manage the states of SSDs in a rack. Therefore, we can enable the state
sharing between SDN and SDF, and facilitate global storage resource management.
RackBlox has three major components: (1) coordinated I/O scheduling, in which
it dynamically adjusts the I/O scheduling in the storage stack with the
measured and predicted network latency, such that it can coordinate the effort
of I/O scheduling across the network and storage stack for achieving
predictable end-to-end performance; (2) coordinated garbage collection (GC), in
which it will coordinate the GC activities across the SSDs in a rack to
minimize their impact on incoming I/O requests; (3) rack-scale wear leveling,
in which it enables global wear leveling among SSDs in a rack by periodically
swapping data, for achieving improved device lifetime for the entire rack. We
implement RackBlox using programmable SSDs and switch. Our experiments
demonstrate that RackBlox can reduce the tail latency of I/O requests by up to
5.8x over state-of-the-art rack-scale storage systems.
</summary>
    <author>
      <name>Benjamin Reidys</name>
    </author>
    <author>
      <name>Yuqi Xue</name>
    </author>
    <author>
      <name>Daixuan Li</name>
    </author>
    <author>
      <name>Bharat Sukhwani</name>
    </author>
    <author>
      <name>Wen-mei Hwu</name>
    </author>
    <author>
      <name>Deming Chen</name>
    </author>
    <author>
      <name>Sameh Asaad</name>
    </author>
    <author>
      <name>Jian Huang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3600006.3613170</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3600006.3613170" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages. Published in published in ACM SIGOPS 29th Symposium on
  Operating Systems Principles (SOSP'23)</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.06513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.06513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.03815v2</id>
    <updated>2023-12-09T18:10:39Z</updated>
    <published>2023-12-06T18:50:26Z</published>
    <title>LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent
  Ecosystem</title>
    <summary>  This paper envisions a revolutionary AIOS-Agent ecosystem, where Large
Language Model (LLM) serves as the (Artificial) Intelligent Operating System
(IOS, or AIOS)--an operating system "with soul". Upon this foundation, a
diverse range of LLM-based AI Agent Applications (Agents, or AAPs) are
developed, enriching the AIOS-Agent ecosystem and signaling a paradigm shift
from the traditional OS-APP ecosystem. We envision that LLM's impact will not
be limited to the AI application level, instead, it will in turn revolutionize
the design and implementation of computer system, architecture, software, and
programming language, featured by several main concepts: LLM as OS
(system-level), Agents as Applications (application-level), Natural Language as
Programming Interface (user-level), and Tools as Devices/Libraries
(hardware/middleware-level). We begin by introducing the architecture of
traditional OS. Then we formalize a conceptual framework for AIOS through "LLM
as OS (LLMOS)", drawing analogies between AIOS and traditional OS: LLM is
likened to OS kernel, context window to memory, external storage to file
system, hardware tools to peripheral devices, software tools to programming
libraries, and user prompts to user commands. Subsequently, we introduce the
new AIOS-Agent Ecosystem, where users can easily program Agent Applications
(AAPs) using natural language, democratizing the development of software, which
is different from the traditional OS-APP ecosystem. Following this, we explore
the diverse scope of Agent Applications. We delve into both single-agent and
multi-agent systems, as well as human-agent interaction. Lastly, drawing on the
insights from traditional OS-APP ecosystem, we propose a roadmap for the
evolution of the AIOS-Agent ecosystem. This roadmap is designed to guide the
future research and development, suggesting systematic progresses of AIOS and
its Agent applications.
</summary>
    <author>
      <name>Yingqiang Ge</name>
    </author>
    <author>
      <name>Yujie Ren</name>
    </author>
    <author>
      <name>Wenyue Hua</name>
    </author>
    <author>
      <name>Shuyuan Xu</name>
    </author>
    <author>
      <name>Juntao Tan</name>
    </author>
    <author>
      <name>Yongfeng Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.03815v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.03815v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.08858v1</id>
    <updated>2024-01-16T22:18:51Z</updated>
    <published>2024-01-16T22:18:51Z</published>
    <title>File System Aging</title>
    <summary>  File systems must allocate space for files without knowing what will be added
or removed in the future. Over the life of a file system, this may cause
suboptimal file placement decisions that eventually lead to slower performance,
or aging. Conventional wisdom suggests that file system aging is a solved
problem in the common case; heuristics to avoid aging, such as colocating
related files and data blocks, are effective until a storage device fills up,
at which point space pressure exacerbates fragmentation-based aging. However,
this article describes both realistic and synthetic workloads that can cause
these heuristics to fail, inducing large performance declines due to aging,
even when the storage device is nearly empty.
  We argue that these slowdowns are caused by poor layout. We demonstrate a
correlation between the read performance of a directory scan and the locality
within a file system's access patterns, using a dynamic layout score. We
complement these results with microbenchmarks that show that space pressure can
cause a substantial amount of inter-file and intra-file fragmentation. However,
our results suggest that the effect of free-space fragmentation on read
performance is best described as accelerating the file system aging process.
The effect on write performance is non-existent in some cases, and, in most
cases, an order of magnitude smaller than the read degradation from
fragmentation caused by normal usage.
  In short, many file systems are exquisitely prone to read aging after a
variety of write patterns. We show, however, that aging is not inevitable.
BetrFS, a file system based on write-optimized dictionaries, exhibits almost no
aging in our experiments. We present a framework for understanding and
predicting aging, and identify the key features of BetrFS that avoid aging.
</summary>
    <author>
      <name>Alex Conway</name>
    </author>
    <author>
      <name>Ainesh Bakshi</name>
    </author>
    <author>
      <name>Arghya Bhattacharya</name>
    </author>
    <author>
      <name>Rory Bennett</name>
    </author>
    <author>
      <name>Yizheng Jiao</name>
    </author>
    <author>
      <name>Eric Knorr</name>
    </author>
    <author>
      <name>Yang Zhan</name>
    </author>
    <author>
      <name>Michael A. Bender</name>
    </author>
    <author>
      <name>William Jannen</name>
    </author>
    <author>
      <name>Rob Johnson</name>
    </author>
    <author>
      <name>Bradley C. Kuszmaul</name>
    </author>
    <author>
      <name>Donald E. Porter</name>
    </author>
    <author>
      <name>Jun Yuan</name>
    </author>
    <author>
      <name>Martin Farach-Colton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 12 figures. Article is an extension of Conway et al. FAST
  17. (see
  https://www.usenix.org/conference/fast17/technical-sessions/presentation/conway)
  and Conway et al. HotStorage 19. (see
  https://www.usenix.org/conference/hotstorage19/presentation/conway)</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.08858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.08858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.2; D.4.3; D.4.2; D.4.8; E.1; E.5; H.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.13154v2</id>
    <updated>2024-06-18T01:23:24Z</updated>
    <published>2024-01-24T00:19:19Z</published>
    <title>Nomad: Non-Exclusive Memory Tiering via Transactional Page Migration</title>
    <summary>  With the advent of byte-addressable memory devices, such as CXL memory,
persistent memory, and storage-class memory, tiered memory systems have become
a reality. Page migration is the de facto method within operating systems for
managing tiered memory. It aims to bring hot data whenever possible into fast
memory to optimize the performance of data accesses while using slow memory to
accommodate data spilled from fast memory. While the existing research has
demonstrated the effectiveness of various optimizations on page migration, it
falls short of addressing a fundamental question: Is exclusive memory tiering,
in which a page is either present in fast memory or slow memory, but not both
simultaneously, the optimal strategy for tiered memory management?
  We demonstrate that page migration-based exclusive memory tiering suffers
significant performance degradation when fast memory is under pressure. In this
paper, we propose non-exclusive memory tiering, a page management strategy that
retains a copy of pages recently promoted from slow memory to fast memory to
mitigate memory thrashing. To enable non-exclusive memory tiering, we develop
Nomad, a new page management mechanism for Linux that features transactional
page migration and page shadowing. Nomad helps remove page migration off the
critical path of program execution and makes migration completely asynchronous.
Evaluations with carefully crafted micro-benchmarks and real-world applications
show that Nomad is able to achieve up to 6x performance improvement over the
state-of-the-art transparent page placement (TPP) approach in Linux when under
memory pressure. We also compare Nomad with a recently proposed
hardware-assisted, access sampling-based page migration approach and
demonstrate Nomad's strengths and potential weaknesses in various scenarios.
</summary>
    <author>
      <name>Lingfeng Xiang</name>
    </author>
    <author>
      <name>Zhen Lin</name>
    </author>
    <author>
      <name>Weishu Deng</name>
    </author>
    <author>
      <name>Hui Lu</name>
    </author>
    <author>
      <name>Jia Rao</name>
    </author>
    <author>
      <name>Yifan Yuan</name>
    </author>
    <author>
      <name>Ren Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2401.13154v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.13154v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.15558v1</id>
    <updated>2024-01-28T03:50:14Z</updated>
    <published>2024-01-28T03:50:14Z</published>
    <title>numaPTE: Managing Page-Tables and TLBs on NUMA Systems</title>
    <summary>  Memory management operations that modify page-tables, typically performed
during memory allocation/deallocation, are infamous for their poor performance
in highly threaded applications, largely due to process-wide TLB shootdowns
that the OS must issue due to the lack of hardware support for TLB coherence.
We study these operations in NUMA settings, where we observe up to 40x overhead
for basic operations such as munmap or mprotect. The overhead further increases
if page-table replication is used, where complete coherent copies of the
page-tables are maintained across all NUMA nodes. While eager system-wide
replication is extremely effective at localizing page-table reads during
address translation, we find that it creates additional penalties upon any
page-table changes due to the need to maintain all replicas coherent.
  In this paper, we propose a novel page-table management mechanism, called
numaPTE, to enable transparent, on-demand, and partial page-table replication
across NUMA nodes in order to perform address translation locally, while
avoiding the overheads and scalability issues of system-wide full page-table
replication. We then show that numaPTE's precise knowledge of page-table
sharers can be leveraged to significantly reduce the number of TLB shootdowns
issued upon any memory-management operation. As a result, numaPTE not only
avoids replication-related slowdowns, but also provides significant speedup
over the baseline on memory allocation/deallocation and access control
operations. We implement numaPTEin Linux on x86_64, evaluate it on 4- and
8-socket systems, and show that numaPTE achieves the full benefits of eager
page-table replication on a wide range of applications, while also achieving a
12% and 36% runtime improvement on Webserver and Memcached respectively due to
a significant reduction in TLB shootdowns.
</summary>
    <author>
      <name>Bin Gao</name>
    </author>
    <author>
      <name>Qingxuan Kang</name>
    </author>
    <author>
      <name>Hao-Wei Tee</name>
    </author>
    <author>
      <name>Kyle Timothy Ng Chu</name>
    </author>
    <author>
      <name>Alireza Sanaee</name>
    </author>
    <author>
      <name>Djordje Jevdjic</name>
    </author>
    <link href="http://arxiv.org/abs/2401.15558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.15558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.04993v1</id>
    <updated>2025-01-09T06:18:39Z</updated>
    <published>2025-01-09T06:18:39Z</published>
    <title>ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State
  Drives</title>
    <summary>  Unlike non-volatile memory that resides on the processor memory bus,
memory-semantic solid-state drives (SSDs) support both byte and block access
granularity via PCIe or CXL interconnects. They provide scalable memory
capacity using NAND flash at a much lower cost. In addition, they have
different performance characteristics for their dual byte/block interface
respectively, while offering essential memory semantics for upper-level
software. Such a byte-accessible storage device provides new implications on
the software system design.
  In this paper, we develop a new file system, named ByteFS, by rethinking the
design primitives of file systems and SSD firmware to exploit the advantages of
both byte and block-granular data accesses. ByteFS supports byte-granular data
persistence to retain the persistence nature of SSDs. It extends the core data
structure of file systems by enabling dual byte/block-granular data accesses.
To facilitate the support for byte-granular writes, \pname{} manages the
internal DRAM of SSD firmware in a log-structured manner and enables data
coalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also
enables coordinated data caching between the host page cache and SSD cache for
best utilizing the precious memory resource. We implement ByteFS on both a real
programmable SSD and an emulated memory-semantic SSD for sensitivity study.
Compared to state-of-the-art file systems for non-volatile memory and
conventional SSDs, ByteFS outperforms them by up to 2.7$\times$, while
preserving the essential properties of a file system. ByteFS also reduces the
write traffic to SSDs by up to 5.1$\times$ by alleviating unnecessary writes
caused by both metadata and data updates in file systems.
</summary>
    <author>
      <name>Shaobo Li</name>
    </author>
    <author>
      <name>Yirui Eric Zhou</name>
    </author>
    <author>
      <name>Hao Ren</name>
    </author>
    <author>
      <name>Jian Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted at the 30th Conference on Architectural
  Support for Programming Languages and Operating Systems (ASPLOS 2025)</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.04993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.04993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.08484v1</id>
    <updated>2025-01-14T23:13:14Z</updated>
    <published>2025-01-14T23:13:14Z</published>
    <title>CORD: Co-design of Resource Allocation and Deadline Decomposition with
  Generative Profiling</title>
    <summary>  As multicore hardware is becoming increasingly common in real-time systems,
traditional scheduling techniques that assume a single worst-case execution
time for a task are no longer adequate, since they ignore the impact of shared
resources on execution time. When tasks execute concurrently on different
cores, their execution times often vary substantially with their allocated
budgets of shared resources, such as cache and memory bandwidth. Even under a
specific resource allocation, the resource use pattern of a task also changes
with time during a job execution. It is therefore important to consider the
relationship between multicore resources and execution time in task modeling
and scheduling algorithm design.
  In this paper, we propose a much more precise execution model for DAG-based
real-time tasks that captures the time-varying resource use characteristics of
a task under different budgets of shared resources. We present a generative
resource profiling algorithm that efficiently predicts, from limited
measurement data, the resource profile of a task at any time during its
execution under a given resource budget. The generative profiles can then be
used to construct the execution models for tasks, using which one can make
informed resource allocation decisions. We further introduce a multicore
resource allocation and deadline decomposition co-design technique for
DAG-based tasks that leverages the generated execution models to jointly
allocate resources and deadlines to subtasks, to maximize resource efficiency
and schedulability. Our evaluation results show that our generative profiling
algorithm achieves high accuracy while being efficient, and that our
co-allocation technique substantially improves schedulability compared to a
state-of-the-art deadline decomposition method.
</summary>
    <author>
      <name>Robert Gifford</name>
    </author>
    <author>
      <name>Abby Eisenklam</name>
    </author>
    <author>
      <name>Georgiy A. Bondar</name>
    </author>
    <author>
      <name>Yifan Cai</name>
    </author>
    <author>
      <name>Tushar Sial</name>
    </author>
    <author>
      <name>Linh Thi Xuan Phan</name>
    </author>
    <author>
      <name>Abhishek Halder</name>
    </author>
    <link href="http://arxiv.org/abs/2501.08484v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.08484v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.12826v1</id>
    <updated>2025-02-18T12:46:50Z</updated>
    <published>2025-02-18T12:46:50Z</published>
    <title>Ariadne: A Hotness-Aware and Size-Adaptive Compressed Swap Technique for
  Fast Application Relaunch and Reduced CPU Usage on Mobile Devices</title>
    <summary>  Growing application memory demands and concurrent usage are making mobile
device memory scarce. When memory pressure is high, current mobile systems use
a RAM-based compressed swap scheme (called ZRAM) to compress unused
execution-related data (called anonymous data in Linux) in main memory.
  We observe that the state-of-the-art ZRAM scheme prolongs relaunch latency
and wastes CPU time because it does not differentiate between hot and cold data
or leverage different compression chunk sizes and data locality. We make three
new observations. 1) anonymous data has different levels of hotness. Hot data,
used during application relaunch, is usually similar between consecutive
relaunches. 2) when compressing the same amount of anonymous data, small-size
compression is very fast, while large-size compression achieves a better
compression ratio. 3) there is locality in data access during application
relaunch.
  We propose Ariadne, a compressed swap scheme for mobile devices that reduces
relaunch latency and CPU usage with three key techniques. 1) a low-overhead
hotness-aware data organization scheme aims to quickly identify the hotness of
anonymous data without significant overhead. 2) a size-adaptive compression
scheme uses different compression chunk sizes based on the data's hotness level
to ensure fast decompression of hot and warm data. 3) a proactive decompression
scheme predicts the next set of data to be used and decompresses it in advance,
reducing the impact of data swapping back into main memory during application
relaunch.
  Our experimental evaluation results on Google Pixel 7 show that, on average,
Ariadne reduces application relaunch latency by 50% and decreases the CPU usage
of compression and decompression procedures by 15% compared to the
state-of-the-art ZRAM scheme.
</summary>
    <author>
      <name>Yu Liang</name>
    </author>
    <author>
      <name>Aofeng Shen</name>
    </author>
    <author>
      <name>Chun Jason Xue</name>
    </author>
    <author>
      <name>Riwei Pan</name>
    </author>
    <author>
      <name>Haiyu Mao</name>
    </author>
    <author>
      <name>Nika Mansouri Ghiasi</name>
    </author>
    <author>
      <name>Qingcai Jiang</name>
    </author>
    <author>
      <name>Rakesh Nadig</name>
    </author>
    <author>
      <name>Lei Li</name>
    </author>
    <author>
      <name>Rachata Ausavarungnirun</name>
    </author>
    <author>
      <name>Mohammad Sadrosadati</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an extended version of a paper that will appear in HPCA 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.12826v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.12826v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.01444v1</id>
    <updated>2025-03-03T11:51:22Z</updated>
    <published>2025-03-03T11:51:22Z</published>
    <title>CHRONOS: Compensating Hardware Related Overheads with Native Multi Timer
  Support for Real-Time Operating Systems</title>
    <summary>  The management of timing constraints in a real-time operating system (RTOS)
is usually realized through a global tick counter. This counter acts as the
foundational time unit for all tasks in the systems. In order to establish a
connection between a tick and an amount of elapsed time in the real world,
often this tick counter is periodically incremented by a hardware timer. At a
fixed interval, this timer generates an interrupt that increments the counter.
In an RTOS, jobs can only become ready upon a timer tick. That means, during a
tick interrupt, the tick counter will be incremented, jobs will be released,
and potentially, a scheduling decision will be conducted to select a new job to
be run. As this process naturally uses some processing time, it is beneficial
regarding the system utilization to minimize the time spent in tick interrupts.
In modern microcontrollers, multiple hardware timers are often available. To
utilize multiple timers to reduce the overhead caused by tick interrupts,
multiple methods are introduced in this paper. The number of interrupts that
are triggered by these timers can then be reduced by mapping tasks to timers in
such a manner that the greatest common divisor (GCD) of all task periods in a
subset is maximized, and the GCD is adopted as the interrupt interval of the
timer. To find an optimal mapping of tasks to timers, an MIQCP-model is
presented that minimizes the overall number of tick interrupts that occur in a
system, while ensuring a correct task release behavior. The presented methods
are implemented in FreeRTOS and evaluated on an embedded system. The evaluation
of the methods show, that compared to the baseline implementation in FreeRTOS
that uses a single timer with a fixed period, the presented methods can provide
a significant reduction in overhead of up to $\approx10\times$ in peak and up
to $\approx 6\times$ in average.
</summary>
    <author>
      <name>Kay Heider</name>
    </author>
    <author>
      <name>Christian Hakert</name>
    </author>
    <author>
      <name>Kuan-Hsun Chen</name>
    </author>
    <author>
      <name>Jian-Jia Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2503.01444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.01444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15465v1</id>
    <updated>2025-04-21T22:21:39Z</updated>
    <published>2025-04-21T22:21:39Z</published>
    <title>LithOS: An Operating System for Efficient Machine Learning on GPUs</title>
    <summary>  The surging demand for GPUs in datacenters for machine learning (ML) has made
efficient GPU utilization crucial. However, meeting the diverse needs of ML
models while optimizing resource usage is challenging. To enable transparent,
fine-grained GPU management that maximizes utilization and energy efficiency
while maintaining strong isolation, an operating system (OS) approach is
needed. This paper introduces LithOS, a first step toward a GPU OS. LithOS
includes the following new abstractions and mechanisms for efficient GPU
resource management: (i) a novel TPC Scheduler that supports spatial scheduling
at the granularity of individual TPCs, unlocking efficient TPC stealing between
workloads; (ii) transparent kernel atomization to reduce head-of-line blocking
and enable dynamic resource reallocation mid-execution; (iii) a lightweight
hardware right-sizing mechanism that determines the minimal TPC resources
needed per atom; and (iv) a transparent power management mechanism that reduces
power consumption based on in-flight work behavior. We implement LithOS in Rust
and evaluate its performance across extensive ML environments, comparing it to
state-of-the-art solutions from NVIDIA and prior research. For inference
stacking, LithOS reduces tail latencies by 13x compared to MPS; compared to the
best SotA, it reduces tail latencies by 3x while improving aggregate throughput
by 1.6x. In hybrid inference-training stacking, LithOS reduces tail latencies
by 4.7x compared to MPS; compared to the best SotA, it reduces tail latencies
1.18x while improving aggregate throughput by 1.35x. Finally, for a modest
performance hit under 4%, LithOS's right-sizing provides a quarter of GPU
capacity savings on average, while for a 7% hit, its power management yields a
quarter of a GPU's energy savings. Overall, LithOS increases GPU efficiency,
establishing a foundation for future OS research on GPUs.
</summary>
    <author>
      <name>Patrick H. Coppock</name>
    </author>
    <author>
      <name>Brian Zhang</name>
    </author>
    <author>
      <name>Eliot H. Solomon</name>
    </author>
    <author>
      <name>Vasilis Kypriotis</name>
    </author>
    <author>
      <name>Leon Yang</name>
    </author>
    <author>
      <name>Bikash Sharma</name>
    </author>
    <author>
      <name>Dan Schatzberg</name>
    </author>
    <author>
      <name>Todd C. Mowry</name>
    </author>
    <author>
      <name>Dimitrios Skarlatos</name>
    </author>
    <link href="http://arxiv.org/abs/2504.15465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9903004v4</id>
    <updated>1999-04-01T00:41:47Z</updated>
    <published>1999-03-04T02:13:35Z</published>
    <title>A Flit Level Simulator for Wormhole Routing</title>
    <summary>  Wormhole routing, the latest switching technique to be utilized by massively
parallel computers, enjoys the distinct advantage of a low latency when
compared to other switching techniques. This low latency is due to the nearly
distance insensitive routing traits in the absence of channel contention. The
low latency of wormhole routing brings about a liability of this switching
technique, a chance of deadlock. Deadlock is a concern in wormhole routed
networks due to the fact a message does not release its allocated resources
until all flits of a message have completely traversed the router in which
these resources are associated. The deadlock condition is addressed in the
routing algorithm. Simulation tools are currently needed that will aid in the
size and number of resources necessary to obtain the optimum utilization of
network resources for an algorithm. Some of these resources include the
topology of the network along with the number of nodes for the topology, the
size of the message, and the number and size of buffers at each router.
</summary>
    <author>
      <name>Denvil Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Complete Thesis 359 pages, 32 tables, 70 figures, HTML (add GIF
  files)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9903004v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9903004v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.4.4; C.2.6; D.4.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9904020v1</id>
    <updated>1999-04-26T21:13:00Z</updated>
    <published>1999-04-26T21:13:00Z</published>
    <title>ODP channel objects that provide services transparently for distributing
  processing systems</title>
    <summary>  This paper describes an architecture for a distributing processing system
that would allow remote procedure calls to invoke other services as messages
are passed between clients and servers. It proposes that an additional class of
data processing objects be located in the software communications channel. The
objects in this channel would then be used to enforce protocols on
client-server applications without any additional effort by the application
programmers. For example, services such as key-management, time-stamping,
sequencing and encryption can be implemented at different levels of the
software communications stack to provide a complete authentication service. A
distributing processing environment could be used to control broadband network
data delivery. Architectures and invocation semantics are discussed, Example
classes and interfaces for channel objects are given in the Java programming
language.
</summary>
    <author>
      <name>Walter Eaves</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9904020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9904020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.0; C.2.4; D.4.7; H.5.1; K.6.4; D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9905002v1</id>
    <updated>1999-05-05T01:43:13Z</updated>
    <published>1999-05-05T01:43:13Z</published>
    <title>DRAFT : Task System and Item Architecture (TSIA)</title>
    <summary>  During its execution, a task is independent of all other tasks. For an
application which executes in terms of tasks, the application definition can be
free of the details of the execution. Many projects have demonstrated that a
task system (TS) can provide such an application with a parallel, distributed,
heterogeneous, adaptive, dynamic, real-time, interactive, reliable, secure or
other execution. A task consists of items and thus the application is defined
in terms of items. An item architecture (IA) can support arrays, routines and
other structures of items, thus allowing for a structured application
definition. Taking properties from many projects, the support can extend
through to currying, application defined types, conditional items, streams and
other definition elements. A task system and item architecture (TSIA) thus
promises unprecedented levels of support for application execution and
definition.
</summary>
    <author>
      <name>Burkhard D. Burow</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">vii+244 pages, including 126 figures of diagrams and code examples.
  Submitted to Springer Verlag. For further information see http://www.tsia.org</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9905002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9905002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1;D.1.1;D.1.3;D.1.4;D.2.3;D.2.11;D.3.2;D.3.3;D.3.4;D.4.1;D.4.5;&#10;  D.4.7;E.1;F.1.2;F.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006015v2</id>
    <updated>2000-06-13T22:07:28Z</updated>
    <published>2000-06-08T17:02:51Z</published>
    <title>UNIX Resource Managers: Capacity Planning and Resource Issues</title>
    <summary>  The latest implementations of commercial UNIX to offer mainframe style
capacity management on enterprise servers include: AIX Workload Manager (WLM),
HP-UX Process Resource Manager (PRM), Solaris Resource Manager (SRM), as well
as SGI and Compaq. The ability to manage server capacity is achieved by making
significant modifications to the standard UNIX operating system so that
processes are inherently tied to specific users. Those users, in turn, are
granted only a certain fraction of system resources. Resource usage is
monitored and compared with each users grant to ensure that the assigned
entitlement constraints are met. In this paper, we begin by clearing up some of
the confusion that has surrounded the motivation and the terminology behind the
new technology. The common theme across each of the commercial implementations
is the introduction of the fair-share scheduler. After reviewing some potential
performance pitfalls, we present capacity planning guidelines for migrating to
automated UNIX resource management.
</summary>
    <author>
      <name>Neil J. Gunther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages. Fixed formatting problem. To be presented at the SAGE-AU
  Conference, Bond University, Gold Coast, Australia, July 7, 2000</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0006015v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006015v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4;D.4.1;D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0104012v1</id>
    <updated>2001-04-07T17:17:14Z</updated>
    <published>2001-04-07T17:17:14Z</published>
    <title>System Support for Bandwidth Management and Content Adaptation in
  Internet Applications</title>
    <summary>  This paper describes the implementation and evaluation of an operating system
module, the Congestion Manager (CM), which provides integrated network flow
management and exports a convenient programming interface that allows
applications to be notified of, and adapt to, changing network conditions. We
describe the API by which applications interface with the CM, and the
architectural considerations that factored into the design. To evaluate the
architecture and API, we describe our implementations of TCP; a streaming
layered audio/video application; and an interactive audio application using the
CM, and show that they achieve adaptive behavior without incurring much
end-system overhead. All flows including TCP benefit from the sharing of
congestion information, and applications are able to incorporate new
functionality such as congestion control and adaptive behavior.
</summary>
    <author>
      <name>David G. Andersen</name>
    </author>
    <author>
      <name>Deepak Bansal</name>
    </author>
    <author>
      <name>Dorothy Curtis</name>
    </author>
    <author>
      <name>Srinivasan Seshan</name>
    </author>
    <author>
      <name>Hari Balakrishnan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, appeared in OSDI 2000</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. OSDI 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0104012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0104012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412038v1</id>
    <updated>2004-12-09T01:20:39Z</updated>
    <published>2004-12-09T01:20:39Z</published>
    <title>Tycoon: an Implementation of a Distributed, Market-based Resource
  Allocation System</title>
    <summary>  Distributed clusters like the Grid and PlanetLab enable the same statistical
multiplexing efficiency gains for computing as the Internet provides for
networking. One major challenge is allocating resources in an economically
efficient and low-latency way. A common solution is proportional share, where
users each get resources in proportion to their pre-defined weight. However,
this does not allow users to differentiate the value of their jobs. This leads
to economic inefficiency. In contrast, systems that require reservations impose
a high latency (typically minutes to hours) to acquire resources.
  We present Tycoon, a market based distributed resource allocation system
based on proportional share. The key advantages of Tycoon are that it allows
users to differentiate the value of their jobs, its resource acquisition
latency is limited only by communication delays, and it imposes no manual
bidding overhead on users. We present experimental results using a prototype
implementation of our design.
</summary>
    <author>
      <name>Kevin Lai</name>
    </author>
    <author>
      <name>Lars Rasmusson</name>
    </author>
    <author>
      <name>Eytan Adar</name>
    </author>
    <author>
      <name>Stephen Sorkin</name>
    </author>
    <author>
      <name>Li Zhang</name>
    </author>
    <author>
      <name>Bernardo A. Huberman</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0412038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4; D.4.1; K.6.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412074v1</id>
    <updated>2004-12-17T15:38:19Z</updated>
    <published>2004-12-17T15:38:19Z</published>
    <title>Threats of Human Error in a High-Performance Storage System: Problem
  Statement and Case Study</title>
    <summary>  System administration is a difficult, often tedious, job requiring many
skilled laborers. The data that is protected by system administrators is often
valued at or above the value of the institution maintaining that data. A number
of ethnographic studies have confirmed the skill of these operators, and the
difficulty of providing adequate tools. In an effort to minimize the
maintenance costs, an increasing portion of system administration is subject to
automation - particularly simple, routine tasks such as data backup. While such
tools reduce the risk of errors from carelessness, the same tools may result in
reduced skill and system familiarity in experienced workers. Care should be
taken to ensure that operators maintain system awareness without placing the
operator in a passive, monitoring role.
</summary>
    <author>
      <name>Elizabeth Haubert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0412074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502090v1</id>
    <updated>2005-02-24T14:13:56Z</updated>
    <published>2005-02-24T14:13:56Z</published>
    <title>UNICORE - From Project Results to Production Grids</title>
    <summary>  The UNICORE Grid-technology provides a seamless, secure and intuitive access
to distributed Grid resources. In this paper we present the recent evolution
from project results to production Grids. At the beginning UNICORE was
developed as a prototype software in two projects funded by the German research
ministry (BMBF). Over the following years, in various European-funded projects,
UNICORE evolved to a full-grown and well-tested Grid middleware system, which
today is used in daily production at many supercomputing centers worldwide.
Beyond this production usage, the UNICORE technology serves as a solid basis in
many European and International research projects, which use existing UNICORE
components to implement advanced features, high level services, and support for
applications from a growing range of domains. In order to foster these ongoing
developments, UNICORE is available as open source under BSD licence at
SourceForge, where new releases are published on a regular basis. This paper is
a review of the UNICORE achievements so far and gives a glimpse on the UNICORE
roadmap.
</summary>
    <author>
      <name>A. Streit</name>
    </author>
    <author>
      <name>D. Erwin</name>
    </author>
    <author>
      <name>Th. Lippert</name>
    </author>
    <author>
      <name>D. Mallmann</name>
    </author>
    <author>
      <name>R. Menday</name>
    </author>
    <author>
      <name>M. Rambadt</name>
    </author>
    <author>
      <name>M. Riedel</name>
    </author>
    <author>
      <name>M. Romberg</name>
    </author>
    <author>
      <name>B. Schuller</name>
    </author>
    <author>
      <name>Ph. Wieder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0502090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0503074v1</id>
    <updated>2005-03-28T15:32:43Z</updated>
    <published>2005-03-28T15:32:43Z</published>
    <title>A File System Abstraction for Sense and Respond Systems</title>
    <summary>  The heterogeneity and resource constraints of sense-and-respond systems pose
significant challenges to system and application development. In this paper, we
present a flexible, intuitive file system abstraction for organizing and
managing sense-and-respond systems based on the Plan 9 design principles. A key
feature of this abstraction is the ability to support multiple views of the
system via filesystem namespaces. Constructed logical views present an
application-specific representation of the network, thus enabling high-level
programming of the network. Concurrently, structural views of the network
enable resource-efficient planning and execution of tasks. We present and
motivate the design using several examples, outline research challenges and our
research plan to address them, and describe the current state of
implementation.
</summary>
    <author>
      <name>Sameer Tilak</name>
    </author>
    <author>
      <name>Bhanu Pisupati</name>
    </author>
    <author>
      <name>Kenneth Chiu</name>
    </author>
    <author>
      <name>Geoffrey Brown</name>
    </author>
    <author>
      <name>Nael Abu-Ghazaleh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures Workshop on End-to-End, Sense-and-Respond Systems,
  Applications, and Services In conjunction with MobiSys '05</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0503074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0503074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.3 Distributed file systems; C.2.1 Wireless communication" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0508130v1</id>
    <updated>2005-08-31T01:44:35Z</updated>
    <published>2005-08-31T01:44:35Z</published>
    <title>A Fresh Look at the Reliability of Long-term Digital Storage</title>
    <summary>  Many emerging Web services, such as email, photo sharing, and web site
archives, need to preserve large amounts of quickly-accessible data
indefinitely into the future. In this paper, we make the case that these
applications' demands on large scale storage systems over long time horizons
require us to re-evaluate traditional storage system designs. We examine
threats to long-lived data from an end-to-end perspective, taking into account
not just hardware and software faults but also faults due to humans and
organizations. We present a simple model of long-term storage failures that
helps us reason about the various strategies for addressing these threats in a
cost-effective manner. Using this model we show that the most important
strategies for increasing the reliability of long-term storage are detecting
latent faults quickly, automating fault repair to make it faster and cheaper,
and increasing the independence of data replicas.
</summary>
    <author>
      <name>Mary Baker</name>
    </author>
    <author>
      <name>Mehul Shah</name>
    </author>
    <author>
      <name>David S. H. Rosenthal</name>
    </author>
    <author>
      <name>Mema Roussopoulos</name>
    </author>
    <author>
      <name>Petros Maniatis</name>
    </author>
    <author>
      <name>TJ Giuli</name>
    </author>
    <author>
      <name>Prashanth Bungale</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0508130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0508130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603021v1</id>
    <updated>2006-03-06T00:46:09Z</updated>
    <published>2006-03-06T00:46:09Z</published>
    <title>Language Support for Optional Functionality</title>
    <summary>  We recommend a programming construct - availability check - for programs that
need to automatically adjust to presence or absence of segments of code. The
idea is to check the existence of a valid definition before a function call is
invoked. The syntax is that of a simple 'if' statement. The vision is to enable
customization of application functionality through addition or removal of
optional components, but without requiring complete re-building. Focus is on
C-like compiled procedural languages and UNIX-based systems. Essentially, our
approach attempts to combine the flexibility of dynamic libraries with the
usability of utility (dependency) libraries. We outline the benefits over
prevalent strategies mainly in terms of development complexity, crudely
measured as lesser lines of code. We also allude to performance and flexibility
facets. A Preliminary implementation and figures from early experimental
evaluation are presented.
</summary>
    <author>
      <name>Joy Mukherjee</name>
    </author>
    <author>
      <name>Srinidhi Varadarajan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0603021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603075v1</id>
    <updated>2006-03-18T17:11:04Z</updated>
    <published>2006-03-18T17:11:04Z</published>
    <title>Unmanaged Internet Protocol: Taming the Edge Network Management Crisis</title>
    <summary>  Though appropriate for core Internet infrastructure, the Internet Protocol is
unsuited to routing within and between emerging ad-hoc edge networks due to its
dependence on hierarchical, administratively assigned addresses. Existing
ad-hoc routing protocols address the management problem but do not scale to
Internet-wide networks. The promise of ubiquitous network computing cannot be
fulfilled until we develop an Unmanaged Internet Protocol (UIP), a scalable
routing protocol that manages itself automatically. UIP must route within and
between constantly changing edge networks potentially containing millions or
billions of nodes, and must still function within edge networks disconnected
from the main Internet, all without imposing the administrative burden of
hierarchical address assignment. Such a protocol appears challenging but
feasible. We propose an architecture based on self-certifying, cryptographic
node identities and a routing algorithm adapted from distributed hash tables.
</summary>
    <author>
      <name>Bryan Ford</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Second Workshop on Hot Topics in Networks (HotNets-II), November
  2003, Cambridge, MA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0603075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.1; C.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607049v2</id>
    <updated>2006-08-01T07:04:20Z</updated>
    <published>2006-07-11T18:54:21Z</published>
    <title>Secure Component Deployment in the OSGi(tm) Release 4 Platform</title>
    <summary>  Last years have seen a dramatic increase in the use of component platforms,
not only in classical application servers, but also more and more in the domain
of Embedded Systems. The OSGi(tm) platform is one of these platforms dedicated
to lightweight execution environments, and one of the most prominent. However,
new platforms also imply new security flaws, and a lack of both knowledge and
tools for protecting the exposed systems. This technical report aims at
fostering the understanding of security mechanisms in component deployment. It
focuses on securing the deployment of components. It presents the cryptographic
mechanisms necessary for signing OSGi(tm) bundles, as well as the detailed
process of bundle signature and validation. We also present the SFelix
platform, which is a secure extension to Felix OSGi(tm) framework
implementation. It includes our implementation of the bundle signature process,
as specified by OSGi(tm) Release 4 Security Layer. Moreover, a tool for signing
and publishing bundles, SFelix JarSigner, has been developed to conveniently
integrate bundle signature in the bundle deployment process.
</summary>
    <author>
      <name>Pierre Parrend</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rhône-Alpes</arxiv:affiliation>
    </author>
    <author>
      <name>Stéphane Frénot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rhône-Alpes</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/cs/0607049v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607049v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611117v1</id>
    <updated>2006-11-22T19:28:31Z</updated>
    <published>2006-11-22T19:28:31Z</published>
    <title>2FACE: Bi-Directional Face Traversal for Efficient Geometric Routing</title>
    <summary>  We propose bi-directional face traversal algorithm $2FACE$ to shorten the
path the message takes to reach the destination in geometric routing. Our
algorithm combines the practicality of the best single-direction traversal
algorithms with the worst case message complexity of $O(|E|)$, where $E$ is the
number of network edges. We apply $2FACE$ to a variety of geometric routing
algorithms. Our simulation results indicate that bi-directional face traversal
decreases the latency of message delivery two to three times compared to single
direction face traversal. The thus selected path approaches the shortest
possible route. This gain in speed comes with a similar message overhead
increase. We describe an algorithm which compensates for this message overhead
by recording the preferable face traversal direction. Thus, if a source has
several messages to send to the destination, the subsequent messages follow the
shortest route. Our simulation results show that with most geometric routing
algorithms the message overhead of finding the short route by bi-directional
face traversal is compensated within two to four repeat messages.
</summary>
    <author>
      <name>Mark Miyashita</name>
    </author>
    <author>
      <name>Mikhail Nesterenko</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0611117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.3812v3</id>
    <updated>2007-07-27T13:50:46Z</updated>
    <published>2007-06-26T12:36:37Z</published>
    <title>Java Components Vulnerabilities - An Experimental Classification
  Targeted at the OSGi Platform</title>
    <summary>  The OSGi Platform finds a growing interest in two different applications
domains: embedded systems, and applications servers. However, the security
properties of this platform are hardly studied, which is likely to hinder its
use in production systems. This is all the more important that the dynamic
aspect of OSGi-based applications, that can be extended at runtime, make them
vulnerable to malicious code injection. We therefore perform a systematic audit
of the OSGi platform so as to build a vulnerability catalog that intends to
reference OSGi Vulnerabilities originating in the Core Specification, and in
behaviors related to the use of the Java language. Standard Services are not
considered. To support this audit, a Semi-formal Vulnerability Pattern is
defined, that enables to uniquely characterize fundamental properties for each
vulnerability, to include verbose description in the pattern, to reference
known security protections, and to track the implementation status of the
proof-of-concept OSGi Bundles that exploit the vulnerability. Based on the
analysis of the catalog, a robust OSGi Platform is built, and recommendations
are made to enhance the OSGi Specifications.
</summary>
    <author>
      <name>Pierre Parrend</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rhône-Alpes</arxiv:affiliation>
    </author>
    <author>
      <name>Stéphane Frénot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rhône-Alpes</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0706.3812v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.3812v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.4622v1</id>
    <updated>2009-07-26T02:19:42Z</updated>
    <published>2009-07-26T02:19:42Z</published>
    <title>Aneka: A Software Platform for .NET-based Cloud Computing</title>
    <summary>  Aneka is a platform for deploying Clouds developing applications on top of
it. It provides a runtime environment and a set of APIs that allow developers
to build .NET applications that leverage their computation on either public or
private clouds. One of the key features of Aneka is the ability of supporting
multiple programming models that are ways of expressing the execution logic of
applications by using specific abstractions. This is accomplished by creating a
customizable and extensible service oriented runtime environment represented by
a collection of software containers connected together. By leveraging on these
architecture advanced services including resource reservation, persistence,
storage management, security, and performance monitoring have been implemented.
On top of this infrastructure different programming models can be plugged to
provide support for different scenarios as demonstrated by the engineering,
life science, and industry applications.
</summary>
    <author>
      <name>Christian Vecchiola</name>
    </author>
    <author>
      <name>Xingchen Chu</name>
    </author>
    <author>
      <name>Rajkumar Buyya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.4622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.4622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.4; C.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.3852v1</id>
    <updated>2009-12-19T01:18:05Z</updated>
    <published>2009-12-19T01:18:05Z</published>
    <title>Sharp utilization thresholds for some real-time scheduling problems</title>
    <summary>  Scheduling policies for real-time systems exhibit threshold behavior that is
related to the utilization of the task set they schedule, and in some cases
this threshold is sharp. For the rate monotonic scheduling policy, we show that
periodic workload with utilization less than a threshold $U_{RM}^{*}$ can be
scheduled almost surely and that all workload with utilization greater than
$U_{RM}^{*}$ is almost surely not schedulable. We study such sharp threshold
behavior in the context of processor scheduling using static task priorities,
not only for periodic real-time tasks but for aperiodic real-time tasks as
well. The notion of a utilization threshold provides a simple schedulability
test for most real-time applications. These results improve our understanding
of scheduling policies and provide an interesting characterization of the
typical behavior of policies. The threshold is sharp (small deviations around
the threshold cause schedulability, as a property, to appear or disappear) for
most policies; this is a happy consequence that can be used to address the
limitations of existing utilization-based tests for schedulability. We
demonstrate the use of such an approach for balancing power consumption with
the need to meet deadlines in web servers.
</summary>
    <author>
      <name>Sathish Gopalakrishnan</name>
    </author>
    <link href="http://arxiv.org/abs/0912.3852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.3852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.5045v1</id>
    <updated>2010-05-27T12:28:51Z</updated>
    <published>2010-05-27T12:28:51Z</published>
    <title>File Managing and Program Execution in Web Operating Systems</title>
    <summary>  Web Operating Systems can be seen as an extension of traditional Operating
Systems where the addresses used to manage files and execute programs (via the
basic load/execution mechanism) are extended from local filesystem path-names
to URLs. A first consequence is that, similarly as in traditional web
technologies, executing a program at a given URL, can be done in two
modalities: either the execution is performed client-side at the invoking
machine (and relative URL addressing in the executed program set to refer to
the invoked URL) or it is performed server-side at the machine addressed by the
invoked URL (as, e.g., for a web service). Moreover in this context, user
identification for access to programs and files and workflow-based composition
of service programs is naturally based on token/session-like mechanisms. We
propose a middleware based on client-server protocols and on a set primitives,
for managing files/resources and executing programs (in the form of
client-side/server-side components/services) in Web Operating Systems. We
formally define the semantics of such middleware via a process algebraic
approach.
</summary>
    <author>
      <name>Mario Bravetti</name>
    </author>
    <link href="http://arxiv.org/abs/1005.5045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.5045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.5241v1</id>
    <updated>2010-05-28T08:55:12Z</updated>
    <published>2010-05-28T08:55:12Z</published>
    <title>Simulation de traces réelles d'E/S disque de PC</title>
    <summary>  Under Windows operating system, existing I/O benchmarking tools does not
allow a developer to efficiently define a file access strategy according to the
applications' constraints. This is essentially due to the fact that the
existing tools do allow only a restricted set of I/O workloads that does not
generally correspond to the target applications. To cope with this problem, we
designed and implemented a precise I/O simulator allowing to simulate whatever
real I/O trace on a given defined architecture, and in which most of file and
disk cache strategies, their interactions and the detailed storage system
architecture are implemented. Simulation results on different workloads and
architectures show a very high degree of precision. In fact, the mean error
rate as compared to real measures is of about 6% with a maximum of 10% on
global throughput.
</summary>
    <author>
      <name>Jalil Boukhobza</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Timsit Claude</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">RenPar'17 / SympA'2006 / CFSE'5 / JC'2006, Canet en Roussillon :
  France (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.5241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.5241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.1571v1</id>
    <updated>2010-08-09T19:23:10Z</updated>
    <published>2010-08-09T19:23:10Z</published>
    <title>Scaling Turbo Boost to a 1000 cores</title>
    <summary>  The Intel Core i7 processor code named Nehalem provides a feature named Turbo
Boost which opportunistically varies the frequencies of the processor's cores.
The frequency of a core is determined by core temperature, the number of active
cores, the estimated power consumption, the estimated current consumption, and
operating system frequency scaling requests. For a chip multi-processor(CMP)
that has a small number of physical cores and a small set of performance
states, deciding the Turbo Boost frequency to use on a given core might not be
difficult. However, we do not know the complexity of this decision making
process in the context of a large number of cores, scaling to the 100s, as
predicted by researchers in the field.
</summary>
    <author>
      <name>Ananth Narayan S</name>
    </author>
    <author>
      <name>Somsubhra Sharangi</name>
    </author>
    <author>
      <name>Alexandra Fedorova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, short paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1008.1571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.1571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.3088v2</id>
    <updated>2010-09-26T01:40:36Z</updated>
    <published>2010-09-16T04:43:46Z</published>
    <title>CloneCloud: Boosting Mobile Device Applications Through Cloud Clone
  Execution</title>
    <summary>  Mobile applications are becoming increasingly ubiquitous and provide ever
richer functionality on mobile devices. At the same time, such devices often
enjoy strong connectivity with more powerful machines ranging from laptops and
desktops to commercial clouds. This paper presents the design and
implementation of CloneCloud, a system that automatically transforms mobile
applications to benefit from the cloud. The system is a flexible application
partitioner and execution runtime that enables unmodified mobile applications
running in an application-level virtual machine to seamlessly off-load part of
their execution from mobile devices onto device clones operating in a
computational cloud. CloneCloud uses a combination of static analysis and
dynamic profiling to optimally and automatically partition an application so
that it migrates, executes in the cloud, and re-integrates computation in a
fine-grained manner that makes efficient use of resources. Our evaluation shows
that CloneCloud can achieve up to 21.2x speedup of smartphone applications we
tested and it allows different partitioning for different inputs and networks.
</summary>
    <author>
      <name>Byung-Gon Chun</name>
    </author>
    <author>
      <name>Sunghwan Ihm</name>
    </author>
    <author>
      <name>Petros Maniatis</name>
    </author>
    <author>
      <name>Mayur Naik</name>
    </author>
    <link href="http://arxiv.org/abs/1009.3088v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.3088v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.5571v1</id>
    <updated>2010-10-27T05:04:38Z</updated>
    <published>2010-10-27T05:04:38Z</published>
    <title>An Introduction to Time-Constrained Automata</title>
    <summary>  We present time-constrained automata (TCA), a model for hard real-time
computation in which agents behaviors are modeled by automata and constrained
by time intervals.
  TCA actions can have multiple start time and deadlines, can be aperiodic, and
are selected dynamically following a graph, the time-constrained automaton.
This allows expressing much more precise time constraints than classical
periodic or sporadic model, while preserving the ease of scheduling and
analysis.
  We provide some properties of this model as well as their scheduling
semantics. We show that TCA can be automatically derived from source-code, and
optimally scheduled on single processors using a variant of EDF. We explain how
time constraints can be used to guarantee communication determinism by
construction, and to study when possible agent interactions happen.
</summary>
    <author>
      <name>Matthieu Lemerre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CEA LIST</arxiv:affiliation>
    </author>
    <author>
      <name>Vincent David</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CEA LIST</arxiv:affiliation>
    </author>
    <author>
      <name>Christophe Aussaguès</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CEA LIST</arxiv:affiliation>
    </author>
    <author>
      <name>Guy Vidal-Naquet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SUPELEC</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.38.9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.38.9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings ICE 2010, arXiv:1010.5308</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 38, 2010, pp. 83-98</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1010.5571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.5571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.3087v1</id>
    <updated>2010-11-13T02:17:29Z</updated>
    <published>2010-11-13T02:17:29Z</published>
    <title>Leakage-Aware Reallocation for Periodic Real-Time Tasks on Multicore
  Processors</title>
    <summary>  It is an increasingly important issue to reduce the energy consumption of
computing systems. In this paper, we consider partition based energy-aware
scheduling of periodic real-time tasks on multicore processors. The scheduling
exploits dynamic voltage scaling (DVS) and core sleep scheduling to reduce both
dynamic and leakage energy consumption. If the overhead of core state switching
is non-negligible, however, the performance of this scheduling strategy in
terms of energy efficiency might degrade. To achieve further energy saving, we
extend the static task scheduling with run-time task reallocation. The basic
idea is to aggregate idle time among cores so that as many cores as possible
could be put into sleep in a way that the overall energy consumption is
reduced. Simulation results show that the proposed approach results in up to
20% energy saving over traditional leakage-aware DVS.
</summary>
    <author>
      <name>Hongtao Huang</name>
    </author>
    <author>
      <name>Feng Xia</name>
    </author>
    <author>
      <name>Jijie Wang</name>
    </author>
    <author>
      <name>Siyu Lei</name>
    </author>
    <author>
      <name>Guowei Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/FCST.2010.105</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/FCST.2010.105" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 5th International Conference on Frontier of Computer Science and
  Technology (FCST), IEEE, Changchun, China, August 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.3087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.3087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3; D.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.1930v1</id>
    <updated>2011-11-08T15:03:26Z</updated>
    <published>2011-11-08T15:03:26Z</published>
    <title>The UWB Solution for Multimedia Traffic in Wireless Sensor Networks</title>
    <summary>  Several researches are focused on the QoS (Quality of Service) and Energy
consumption in wireless Multimedia Sensor Networks. Those research projects
invest in theory and practice in order to extend the spectrum of use of norms,
standards and technologies which are emerged in wireless communications. The
performance of these technologies is strongly related to domains of use and
limitations of their characteristics. In this paper, we give a comparison of
ZigBee technology, most widely used in sensor networks, and UWB (Ultra Wide
Band) which presents itself as competitor that present in these work better
results for audiovisual applications with medium-range and high throughput.
</summary>
    <author>
      <name>A. A. Boudhir</name>
    </author>
    <author>
      <name>M. Bouhorma</name>
    </author>
    <author>
      <name>M. Ben Ahmed</name>
    </author>
    <author>
      <name>Elbrak Said</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijwmn</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijwmn" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 11 figures, IJWMN Journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Wireless &amp; Mobile Networks October Issue
  2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1111.1930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.1930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.5880v1</id>
    <updated>2011-11-25T02:10:11Z</updated>
    <published>2011-11-25T02:10:11Z</published>
    <title>Robustness Analysis for Battery Supported Cyber-Physical Systems</title>
    <summary>  This paper establishes a novel analytical approach to quantify robustness of
scheduling and battery management for battery supported cyber-physical systems.
A dynamic schedulability test is introduced to determine whether tasks are
schedulable within a finite time window. The test is used to measure robustness
of a real-time scheduling algorithm by evaluating the strength of computing
time perturbations that break schedulability at runtime. Robustness of battery
management is quantified analytically by an adaptive threshold on the state of
charge. The adaptive threshold significantly reduces the false alarm rate for
battery management algorithms to decide when a battery needs to be replaced.
</summary>
    <author>
      <name>Fumin Zhang</name>
    </author>
    <author>
      <name>Zhenwu Shi</name>
    </author>
    <author>
      <name>Shayok Mukhopadhyay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted by ACM Transactions in Embedded
  Computing Systems (TECS) in October, 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.5880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.5880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3; D.4.1; G.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.5282v2</id>
    <updated>2012-06-02T15:56:26Z</updated>
    <published>2012-02-23T20:12:00Z</published>
    <title>How to Bypass Verified Boot Security in Chromium OS</title>
    <summary>  Verified boot is an interesting feature of Chromium OS that supposedly can
detect any modification in the root file system (rootfs) by a dedicated
adversary. However, by exploiting a design flaw in verified boot, we show that
an adversary can replace the original rootfs by a malicious rootfs containing
exploits such as a spyware or keylogger and still pass the verified boot
process. The exploit is based on the fact that a dedicated adversary can
replace the rootfs and the corresponding verification information in the
bootloader. We experimentally demonstrate an attack using both the base and
developer version of Chromium OS in which the adversary installs a spyware in
the target system to send cached user data to the attacker machine in plain
text which are otherwise encrypted, and thus inaccessible. We also demonstrate
techniques to mitigate this vulnerability.
</summary>
    <author>
      <name>Mohammad Iftekhar Husain</name>
    </author>
    <author>
      <name>Lokesh Mandvekar</name>
    </author>
    <author>
      <name>Chunming Qiao</name>
    </author>
    <author>
      <name>Ramalingam Sridhar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Update information about Chromium OS. Added new and advanced
  exploits. Added mitigation techniques and evaluation</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.5282v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.5282v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.6187v1</id>
    <updated>2012-11-27T02:36:24Z</updated>
    <published>2012-11-27T02:36:24Z</published>
    <title>A Formal Model of a Virtual Filesystem Switch</title>
    <summary>  This work presents a formal model that is part of our effort to construct a
verified file system for Flash memory. To modularize the verification we factor
out generic aspects into a common component that is inspired by the Linux
Virtual Filesystem Switch (VFS) and provides POSIX compatible operations. It
relies on an abstract specification of its internal interface to concrete file
system implementations (AFS). We proved that preconditions of AFS are respected
and that the state is kept consistent. The model can be made executable and
mounted into the Linux directory tree using FUSE.
</summary>
    <author>
      <name>Gidon Ernst</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Augsburg</arxiv:affiliation>
    </author>
    <author>
      <name>Gerhard Schellhorn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Augsburg</arxiv:affiliation>
    </author>
    <author>
      <name>Dominik Haneberg</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Augsburg</arxiv:affiliation>
    </author>
    <author>
      <name>Jörg Pfähler</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Augsburg</arxiv:affiliation>
    </author>
    <author>
      <name>Wolfgang Reif</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Augsburg</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.102.5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.102.5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings SSV 2012, arXiv:1211.5873</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 102, 2012, pp. 33-45</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.6187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.6187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.6190v1</id>
    <updated>2012-11-27T02:36:45Z</updated>
    <published>2012-11-27T02:36:45Z</published>
    <title>On the Use of Underspecified Data-Type Semantics for Type Safety in
  Low-Level Code</title>
    <summary>  In recent projects on operating-system verification, C and C++ data types are
often formalized using a semantics that does not fully specify the precise byte
encoding of objects. It is well-known that such an underspecified data-type
semantics can be used to detect certain kinds of type errors. In general,
however, underspecified data-type semantics are unsound: they assign
well-defined meaning to programs that have undefined behavior according to the
C and C++ language standards.
  A precise characterization of the type-correctness properties that can be
enforced with underspecified data-type semantics is still missing. In this
paper, we identify strengths and weaknesses of underspecified data-type
semantics for ensuring type safety of low-level systems code. We prove
sufficient conditions to detect certain classes of type errors and, finally,
identify a trade-off between the complexity of underspecified data-type
semantics and their type-checking capabilities.
</summary>
    <author>
      <name>Hendrik Tews</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU Dresden</arxiv:affiliation>
    </author>
    <author>
      <name>Marcus Völp</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU Dresden</arxiv:affiliation>
    </author>
    <author>
      <name>Tjark Weber</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Uppsala University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.102.8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.102.8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings SSV 2012, arXiv:1211.5873</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 102, 2012, pp. 73-87</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.6190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.6190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.0621v3</id>
    <updated>2013-06-27T06:04:17Z</updated>
    <published>2013-02-04T09:09:38Z</published>
    <title>RevDedup: A Reverse Deduplication Storage System Optimized for Reads to
  Latest Backups</title>
    <summary>  Scaling up the backup storage for an ever-increasing volume of virtual
machine (VM) images is a critical issue in virtualization environments. While
deduplication is known to effectively eliminate duplicates for VM image
storage, it also introduces fragmentation that will degrade read performance.
We propose RevDedup, a deduplication system that optimizes reads to latest VM
image backups using an idea called reverse deduplication. In contrast with
conventional deduplication that removes duplicates from new data, RevDedup
removes duplicates from old data, thereby shifting fragmentation to old data
while keeping the layout of new data as sequential as possible. We evaluate our
RevDedup prototype using microbenchmark and real-world workloads. For a 12-week
span of real-world VM images from 160 users, RevDedup achieves high
deduplication efficiency with around 97% of saving, and high backup and read
throughput on the order of 1GB/s. RevDedup also incurs small metadata overhead
in backup/read operations.
</summary>
    <author>
      <name>Chun-Ho Ng</name>
    </author>
    <author>
      <name>Patrick P. C. Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A 7-page version appeared in APSys'13</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.0621v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.0621v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5521v1</id>
    <updated>2013-02-22T09:01:24Z</updated>
    <published>2013-02-22T09:01:24Z</published>
    <title>Towards Python-based Domain-specific Languages for Self-reconfigurable
  Modular Robotics Research</title>
    <summary>  This paper explores the role of operating system and high-level languages in
the development of software and domain-specific languages (DSLs) for
self-reconfigurable robotics. We review some of the current trends in
self-reconfigurable robotics and describe the development of a software system
for ATRON II which utilizes Linux and Python to significantly improve software
abstraction and portability while providing some basic features which could
prove useful when using Python, either stand-alone or via a DSL, on a
self-reconfigurable robot system. These features include transparent socket
communication, module identification, easy software transfer and reliable
module-to-module communication. The end result is a software platform for
modular robots that where appropriate builds on existing work in operating
systems, virtual machines, middleware and high-level languages.
</summary>
    <author>
      <name>Mikael Moghadam</name>
    </author>
    <author>
      <name>David Johan Christensen</name>
    </author>
    <author>
      <name>David Brandt</name>
    </author>
    <author>
      <name>Ulrik Pagh Schultz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at DSLRob 2011 (arXiv:1212.3308)</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.5521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.0698v1</id>
    <updated>2013-08-03T14:09:23Z</updated>
    <published>2013-08-03T14:09:23Z</published>
    <title>An Improving Method for Loop Unrolling</title>
    <summary>  In this paper we review main ideas mentioned in several other papers which
talk about optimization techniques used by compilers. Here we focus on loop
unrolling technique and its effect on power consumption, energy usage and also
its impact on program speed up by achieving ILP (Instruction-level
parallelism). Concentrating on superscalar processors, we discuss the idea of
generalized loop unrolling presented by J.C. Hang and T. Leng and then we
present a new method to traverse a linked list to get a better result of loop
unrolling in that case. After that we mention the results of some experiments
carried out on a Pentium 4 processor (as an instance of super scalar
architecture). Furthermore, the results of some other experiments on
supercomputer (the Alliat FX/2800 System) containing superscalar node
processors would be mentioned. These experiments show that loop unrolling has a
slight measurable effect on energy usage as well as power consumption. But it
could be an effective way for program speed up.
</summary>
    <author>
      <name>Meisam Booshehri</name>
    </author>
    <author>
      <name>Abbas Malekpour</name>
    </author>
    <author>
      <name>Peter Luksch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, International Journal of Computer Science and Information
  Security</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, Vol. 11, No. 5, pp. 73-76 , 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1308.0698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.0698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Nxx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5892v1</id>
    <updated>2013-12-20T11:11:54Z</updated>
    <published>2013-12-20T11:11:54Z</published>
    <title>Support for Error Tolerance in the Real-Time Transport Protocol</title>
    <summary>  Streaming applications often tolerate bit errors in their received data well.
This is contrasted by the enforcement of correctness of the packet headers and
payload by network protocols. We investigate a solution for the Real-time
Transport Protocol (RTP) that is tolerant to errors by accepting erroneous
data. It passes potentially corrupted stream data payloads to the codecs. If
errors occur in the header, our solution recovers from these by leveraging the
known state and expected header values for each stream. The solution is fully
receiver-based and incrementally deployable, and as such requires neither
support from the sender nor changes to the RTP specification. Evaluations show
that our header error recovery scheme can recover from almost all errors, with
virtually no erroneous recoveries, up to bit error rates of about 10%.
</summary>
    <author>
      <name>Florian Schmidt</name>
    </author>
    <author>
      <name>David Orlea</name>
    </author>
    <author>
      <name>Klaus Wehrle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 9 figures, published as technical report of the Department
  of Computer Science of RWTH Aachen University</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.6726v1</id>
    <updated>2014-01-27T03:01:33Z</updated>
    <published>2014-01-27T03:01:33Z</published>
    <title>Anception: Application Virtualization For Android</title>
    <summary>  The problem of malware has become significant on Android devices. Library
operating systems and application virtualization are both possible solutions
for confining malware. Unfortunately, such solutions do not exist for Android.
Designing mechanisms for application virtualization is a significant chal-
lenge for several reasons: (1) graphics performance is important due to
popularity of games and (2) applications with the same UID can share state.
This paper presents Anception, the first flexible application virtualization
framework for Android. It is imple- mented as a modification to the Android
kernel and supports application virtualization that addresses the above
requirements. Anception is able to confine many types of malware while
supporting unmodified Android applications. Our Anception- based system
exhibits up to 3.9% overhead on various 2D/3D benchmarks, and 1.8% overhead on
the SunSpider benchmark.
</summary>
    <author>
      <name>Earlence Fernandes</name>
    </author>
    <author>
      <name>Alexander Crowell</name>
    </author>
    <author>
      <name>Ajit Aluri</name>
    </author>
    <author>
      <name>Atul Prakash</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">University of Michigan, Technical Report CSE-TR-583-13</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.6726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.6726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.1165v1</id>
    <updated>2014-03-05T15:43:57Z</updated>
    <published>2014-03-05T15:43:57Z</published>
    <title>A Taxonomy for Attack Patterns on Information Flows in Component-Based
  Operating Systems</title>
    <summary>  We present a taxonomy and an algebra for attack patterns on component-based
operating systems. In a multilevel security scenario, where isolation of
partitions containing data at different security classifications is the primary
security goal and security breaches are mainly defined as undesired disclosure
or modification of classified data, strict control of information flows is the
ultimate goal. In order to prevent undesired information flows, we provide a
classification of information flow types in a component-based operating system
and, by this, possible patterns to attack the system. The systematic
consideration of informations flows reveals a specific type of operating system
covert channel, the covert physical channel, which connects two former isolated
partitions by emitting physical signals into the computer's environment and
receiving them at another interface.
</summary>
    <author>
      <name>Michael Hanspach</name>
    </author>
    <author>
      <name>Jörg Keller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 7th Layered Assurance Workshop, New Orleans,
  LA, USA, December 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1403.1165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.1165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.7754v1</id>
    <updated>2014-10-28T19:23:26Z</updated>
    <published>2014-10-28T19:23:26Z</published>
    <title>A First Look at Firefox OS Security</title>
    <summary>  With Firefox OS, Mozilla is making a serious push for an HTML5-based mobile
platform. In order to assuage security concerns over providing hardware access
to web applications, Mozilla has introduced a number of mechanisms that make
the security landscape of Firefox OS distinct from both the desktop web and
other mobile operating systems. From an application security perspective, the
two most significant of these mechanisms are the the introduction of a default
Content Security Policy and code review in the market. This paper describes how
lightweight static analysis can augment these mechanisms to find
vulnerabilities which have otherwise been missed. We provide examples of
privileged applications in the market that contain vulnerabilities that can be
automatically detected.
  In addition to these findings, we show some of the challenges that occur when
desktop software is repurposed for a mobile operating system. In particular, we
argue that the caching of certificate overrides across applications--a known
problem in Firefox OS--generates a counter-intuitive user experience that
detracts from the security of the system.
</summary>
    <author>
      <name>Daniel Defreez</name>
    </author>
    <author>
      <name>Bhargava Shastry</name>
    </author>
    <author>
      <name>Hao Chen</name>
    </author>
    <author>
      <name>Jean-Pierre Seifert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the Third Workshop on Mobile Security Technologies
  (MoST) 2014 (http://arxiv.org/abs/1410.6674)</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.7754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.7754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.06836v1</id>
    <updated>2015-04-26T14:57:05Z</updated>
    <published>2015-04-26T14:57:05Z</published>
    <title>Monitoring Extreme-scale Lustre Toolkit</title>
    <summary>  We discuss the design and ongoing development of the Monitoring Extreme-scale
Lustre Toolkit (MELT), a unified Lustre performance monitoring and analysis
infrastructure that provides continuous, low-overhead summary information on
the health and performance of Lustre, as well as on-demand, in- depth problem
diagnosis and root-cause analysis. The MELT infrastructure leverages a
distributed overlay network to enable monitoring of center-wide Lustre
filesystems where clients are located across many network domains. We preview
interactive command-line utilities that help administrators and users to
observe Lustre performance at various levels of resolution, from individual
servers or clients to whole filesystems, including job-level reporting.
Finally, we discuss our future plans for automating the root-cause analysis of
common Lustre performance problems.
</summary>
    <author>
      <name>Michael J. Brim</name>
    </author>
    <author>
      <name>Joshua K. Lothian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.06836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.01448v1</id>
    <updated>2015-05-06T18:14:56Z</updated>
    <published>2015-05-06T18:14:56Z</published>
    <title>Taking back control of HPC file systems with Robinhood Policy Engine</title>
    <summary>  Today, the largest Lustre file systems store billions of entries. On such
systems, classic tools based on namespace scanning become unusable. Operations
such as managing file lifetime, scheduling data copies, and generating overall
filesystem statistics become painful as they require collecting, sorting and
aggregating information for billions of records. Robinhood Policy Engine is an
open source software developed to address these challenges. It makes it
possible to schedule automatic actions on huge numbers of filesystem entries.
It also gives a synthetic understanding of file systems contents by providing
overall statistics about data ownership, age and size profiles. Even if it can
be used with any POSIX filesystem, Robinhood supports Lustre specific features
like OSTs, pools, HSM, ChangeLogs, and DNE. It implements specific support for
these features, and takes advantage of them to manage Lustre file systems
efficiently.
</summary>
    <author>
      <name>Thomas Leibovici</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.01448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.01448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.01765v1</id>
    <updated>2015-05-07T16:31:21Z</updated>
    <published>2015-05-07T16:31:21Z</published>
    <title>Development of a Burst Buffer System for Data-Intensive Applications</title>
    <summary>  Modern parallel filesystems such as Lustre are designed to provide high,
scalable I/O bandwidth in response to growing I/O requirements; however, the
bursty I/O characteristics of many data-intensive scientific applications make
it difficult for back-end parallel filesystems to efficiently handle I/O
requests. A burst buffer system, through which data can be temporarily buffered
via high-performance storage mediums, allows for gradual flushing of data to
back-end filesystems. In this paper, we explore issues surrounding the
development of a burst buffer system for data-intensive scientific
applications. Our initial results demonstrate that utilizing a burst buffer
system on top of the Lustre filesystem shows promise for dealing with the
intense I/O traffic generated by application checkpointing.
</summary>
    <author>
      <name>Teng Wang</name>
    </author>
    <author>
      <name>Sarp Oral</name>
    </author>
    <author>
      <name>Michael Pritchard</name>
    </author>
    <author>
      <name>Kevin Vasko</name>
    </author>
    <author>
      <name>Weikuan Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.01765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.01765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02822v2</id>
    <updated>2015-07-26T18:50:32Z</updated>
    <published>2015-06-09T08:30:23Z</published>
    <title>Reproducible and User-Controlled Software Environments in HPC with Guix</title>
    <summary>  Support teams of high-performance computing (HPC) systems often find
themselves between a rock and a hard place: on one hand, they understandably
administrate these large systems in a conservative way, but on the other hand,
they try to satisfy their users by deploying up-to-date tool chains as well as
libraries and scientific software. HPC system users often have no guarantee
that they will be able to reproduce results at a later point in time, even on
the same system-software may have been upgraded, removed, or recompiled under
their feet, and they have little hope of being able to reproduce the same
software environment elsewhere. We present GNU Guix and the functional package
management paradigm and show how it can improve reproducibility and sharing
among researchers with representative use cases.
</summary>
    <author>
      <name>Ludovic Courtès</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Bordeaux - Sud-Ouest</arxiv:affiliation>
    </author>
    <author>
      <name>Ricardo Wurmus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2nd International Workshop on Reproducibility in Parallel Computing
  (RepPar), Aug 2015, Vienne, Austria. http://reppar.org/</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.02822v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02822v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04753v1</id>
    <updated>2015-08-18T19:50:17Z</updated>
    <published>2015-08-18T19:50:17Z</published>
    <title>Cold Object Identification in the Java Virtual Machine</title>
    <summary>  Many Java applications instantiate objects within the Java heap that are
persistent but seldom if ever referenced by the application. Examples include
strings, such as error messages, and collections of value objects that are
preloaded for fast access but they may include objects that are seldom
referenced. This paper describes a stack-based framework for detecting these
"cold" objects at runtime, with a view to marshaling and sequestering them in
designated regions of the heap where they may be preferentially paged out to a
backing store, thereby freeing physical memory pages for occupation by more
active objects. Furthermore, we evaluate the correctness and efficiency of
stack-based approach with an Access Barrier. The experimental results from a
series of SPECjvm2008 benchmarks are presented.
</summary>
    <author>
      <name>Kim T. Briggs</name>
    </author>
    <author>
      <name>Baoguo Zhou</name>
    </author>
    <author>
      <name>Gerhard W. Dueck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">For submission to `Software: Practice and Experience'</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.04753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05228v1</id>
    <updated>2015-08-21T09:59:27Z</updated>
    <published>2015-08-21T09:59:27Z</published>
    <title>A Case Study on Covert Channel Establishment via Software Caches in
  High-Assurance Computing Systems</title>
    <summary>  Covert channels can be utilized to secretly deliver information from high
privileged processes to low privileged processes in the context of a
high-assurance computing system. In this case study, we investigate the
possibility of covert channel establishment via software caches in the context
of a framework for component-based operating systems. While component-based
operating systems offer security through the encapsulation of system service
processes, complete isolation of these processes is not reasonably feasible.
This limitation is practically demonstrated with our concept of a specific
covert timing channel based on file system caching. The stability of the covert
channel is evaluated and a methodology to disrupt the covert channel
transmission is presented. While these kinds of attacks are not limited to
high-assurance computing systems, our study practically demonstrates that even
security-focused computing systems with a minimal trusted computing base are
vulnerable for such kinds of attacks and careful design decisions are necessary
for secure operating system architectures.
</summary>
    <author>
      <name>Wolfgang Schmidt</name>
    </author>
    <author>
      <name>Michael Hanspach</name>
    </author>
    <author>
      <name>Jörg Keller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, based upon the master's thesis of Schmidt</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.05228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.07127v1</id>
    <updated>2015-08-28T08:45:35Z</updated>
    <published>2015-08-28T08:45:35Z</published>
    <title>Virtualization Architecture for NoC-based Reconfigurable Systems</title>
    <summary>  We propose a virtualization architecture for NoC-based reconfigurable
systems. The motivation of this work is to develop a service-oriented
architecture that includes Partial Reconfigurable Region as a Service (PRRaaS)
and Processing Element as a Service (PEaaS) for software applications.
According to the requirements of software applications, new PEs can be created
on-demand by (re)configuring the logic resource of the PRRs in the FPGA, while
the configured PEs can also be virtualized to support multiple application
tasks at the same time. As a result, such a two-level virtualization mechanism,
including the gate-level virtualization and the PE-level virtualization,
enables an SoC to be dynamically adapted to changing application requirements.
Therefore, more software applications can be performed, and system performance
can be further enhanced.
</summary>
    <author>
      <name>Chun-Hsian Huang</name>
    </author>
    <author>
      <name>Kwuan-Wei Tseng</name>
    </author>
    <author>
      <name>Chih-Cheng Lin</name>
    </author>
    <author>
      <name>Fang-Yu Lin</name>
    </author>
    <author>
      <name>Pao-Ann Hsiung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.07127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.07127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.02528v1</id>
    <updated>2015-11-08T21:12:17Z</updated>
    <published>2015-11-08T21:12:17Z</published>
    <title>Proceedings Workshop on Models for Formal Analysis of Real Systems</title>
    <summary>  This volume contains the proceedings of MARS 2015, the first workshop on
Models for Formal Analysis of Real Systems, held on November 23, 2015 in Suva,
Fiji, as an affiliated workshop of LPAR 2015, the 20th International Conference
on Logic for Programming, Artificial Intelligence and Reasoning.
  The workshop emphasises modelling over verification. It aims at discussing
the lessons learned from making formal methods for the verification and
analysis of realistic systems. Examples are:
  (1) Which formalism is chosen, and why?
  (2) Which abstractions have to be made and why?
  (3) How are important characteristics of the system modelled?
  (4) Were there any complications while modelling the system?
  (5) Which measures were taken to guarantee the accuracy of the model?
  We invited papers that present full models of real systems, which may lay the
basis for future comparison and analysis. An aim of the workshop is to present
different modelling approaches and discuss pros and cons for each of them.
Alternative formal descriptions of the systems presented at this workshop are
encouraged, which should foster the development of improved specification
formalisms.
</summary>
    <author>
      <name>Rob van Glabbeek</name>
    </author>
    <author>
      <name>Jan Friso Groote</name>
    </author>
    <author>
      <name>Peter Höfner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.196</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.196" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 196, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.02528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.02528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.03027v1</id>
    <updated>2016-01-12T20:46:07Z</updated>
    <published>2016-01-12T20:46:07Z</published>
    <title>Open Mobile API: Accessing the UICC on Android Devices</title>
    <summary>  This report gives an overview of secure element integration into Android
devices. It focuses on the Open Mobile API as an open interface to access
secure elements from Android applications. The overall architecture of the Open
Mobile API is described and current Android devices are analyzed with regard to
the availability of this API. Moreover, this report summarizes our efforts of
reverse engineering the stock ROM of a Samsung Galaxy S3 in order to analyze
the integration of the Open Mobile API and the interface that is used to
perform APDU-based communication with the UICC (Universal Integrated Circuit
Card). It further provides a detailed explanation on how to integrate this
functionality into CyanogenMod (an after-market firmware for Android devices).
</summary>
    <author>
      <name>Michael Roland</name>
    </author>
    <author>
      <name>Michael Hölzl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">University of Applied Sciences Upper Austria, JR-Center u'smile,
  Technical report, 76 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.03027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.0; C.3; C.5.3; D.2.7; D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07371v1</id>
    <updated>2016-04-25T19:20:18Z</updated>
    <published>2016-04-25T19:20:18Z</published>
    <title>Do the Hard Stuff First: Scheduling Dependent Computations in
  Data-Analytics Clusters</title>
    <summary>  We present a scheduler that improves cluster utilization and job completion
times by packing tasks having multi-resource requirements and
inter-dependencies. While the problem is algorithmically very hard, we achieve
near-optimality on the job DAGs that appear in production clusters at a large
enterprise and in benchmarks such as TPC-DS. A key insight is that carefully
handling the long-running tasks and those with tough-to-pack resource needs
will produce good-enough schedules. However, which subset of tasks to treat
carefully is not clear (and intractable to discover). Hence, we offer a search
procedure that evaluates various possibilities and outputs a preferred schedule
order over tasks. An online component enforces the schedule orders desired by
the various jobs running on the cluster. In addition, it packs tasks, overbooks
the fungible resources and guarantees bounded unfairness for a variety of
desirable fairness schemes. Relative to the state-of-the art schedulers, we
speed up 50% of the jobs by over 30% each.
</summary>
    <author>
      <name>Robert Grandl</name>
    </author>
    <author>
      <name>Srikanth Kandula</name>
    </author>
    <author>
      <name>Sriram Rao</name>
    </author>
    <author>
      <name>Aditya Akella</name>
    </author>
    <author>
      <name>Janardhan Kulkarni</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00571v1</id>
    <updated>2016-08-01T15:33:14Z</updated>
    <published>2016-08-01T15:33:14Z</published>
    <title>TREES: A CPU/GPU Task-Parallel Runtime with Explicit Epoch
  Synchronization</title>
    <summary>  We have developed a task-parallel runtime system, called TREES, that is
designed for high performance on CPU/GPU platforms. On platforms with multiple
CPUs, Cilk's "work-first" principle underlies how task-parallel applications
can achieve performance, but work-first is a poor fit for GPUs. We build upon
work-first to create the "work-together" principle that addresses the specific
strengths and weaknesses of GPUs. The work-together principle extends
work-first by stating that (a) the overhead on the critical path should be paid
by the entire system at once and (b) work overheads should be paid
co-operatively. We have implemented the TREES runtime in OpenCL, and we
experimentally evaluate TREES applications on a CPU/GPU platform.
</summary>
    <author>
      <name>Blake A. Hechtman</name>
    </author>
    <author>
      <name>Andrew D. Hilton</name>
    </author>
    <author>
      <name>Daniel J. Sorin</name>
    </author>
    <link href="http://arxiv.org/abs/1608.00571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04303v1</id>
    <updated>2016-08-15T15:26:22Z</updated>
    <published>2016-08-15T15:26:22Z</published>
    <title>SandBlaster: Reversing the Apple Sandbox</title>
    <summary>  In order to limit the damage of malware on Mac OS X and iOS, Apple uses
sandboxing, a kernel-level security layer that provides tight constraints for
system calls. Particularly used for Apple iOS, sandboxing prevents apps from
executing potentially dangerous actions, by defining rules in a sandbox
profile. Investigating Apple's built-in sandbox profiles is difficult as they
are compiled and stored in binary format. We present SandBlaster, a software
bundle that is able to reverse/decompile Apple binary sandbox profiles to their
original human readable SBPL (SandBox Profile Language) format. We use
SandBlaster to reverse all built-in Apple iOS binary sandbox profiles for iOS
7, 8 and 9. Our tool is, to the best of our knowledge, the first to provide a
full reversing of the Apple sandbox, shedding light into the inner workings of
Apple sandbox profiles and providing essential support for security researchers
and professionals interested in Apple security mechanisms.
</summary>
    <author>
      <name>Răzvan Deaconescu</name>
    </author>
    <author>
      <name>Luke Deshotels</name>
    </author>
    <author>
      <name>Mihai Bucicoiu</name>
    </author>
    <author>
      <name>William Enck</name>
    </author>
    <author>
      <name>Lucas Davi</name>
    </author>
    <author>
      <name>Ahmad-Reza Sadeghi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 9 figures, 14 listings This report is an auxiliary document
  to the paper "SandScout: Automatic Detection of Flaws in iOS Sandbox
  Profiles", to be presented at the ACM Conference on Computer and
  Communications Security (CCS) 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00100v2</id>
    <updated>2016-09-03T22:40:31Z</updated>
    <published>2016-09-01T03:45:39Z</published>
    <title>Suspicious-Taint-Based Access Control for Protecting OS from Network
  Attacks</title>
    <summary>  Today, security threats to operating systems largely come from network.
Traditional discretionary access control mechanism alone can hardly defeat
them. Although traditional mandatory access control models can effectively
protect the security of OS, they have problems of being incompatible with
application software and complex in administration. In this paper, we propose a
new model, Suspicious-Taint-Based Access Control (STBAC) model, for defeating
network attacks while being compatible, simple and maintaining good system
performance. STBAC regards the processes using Non-Trustable-Communications as
the starting points of suspicious taint, traces the activities of the
suspiciously tainted processes by taint rules, and forbids the suspiciously
tainted processes to illegally access vital resources by protection rules. Even
in the cases when some privileged processes are subverted, STBAC can still
protect vital resources from being compromised by the intruder. We implemented
the model in the Linux kernel and evaluated it through experiments. The
evaluation showed that STBAC could protect vital resources effectively without
significant impact on compatibility and performance.
</summary>
    <author>
      <name>Zhiyong Shan</name>
    </author>
    <link href="http://arxiv.org/abs/1609.00100v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00100v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03709v1</id>
    <updated>2017-01-13T16:15:53Z</updated>
    <published>2017-01-13T16:15:53Z</published>
    <title>Power and Execution Time Measurement Methodology for SDF Applications on
  FPGA-based MPSoCs</title>
    <summary>  Timing and power consumption play an important role in the design of embedded
systems. Furthermore, both properties are directly related to the safety
requirements of many embedded systems. With regard to availability
requirements, power considerations are of uttermost importance for battery
operated systems. Validation of timing and power requires observability of
these properties. In many cases this is difficult, because the observability is
either not possible or requires big extra effort in the system validation
process. In this paper, we present a measurement-based approach for the joint
timing and power analysis of Synchronous Dataflow (SDF) applications running on
a shared memory multiprocessor systems-on-chip (MPSoC) architecture. As a
proof-of-concept, we implement an MPSoC system with configurable power and
timing measurement interfaces inside a Field Programmable Gate Array (FPGA).
Our experiments demonstrate the viability of our approach being able of
accurately analyzing different mappings of image processing applications (Sobel
filter and JPEG encoder) on an FPGA-based MPSoC implementation.
</summary>
    <author>
      <name>Christof Schlaak</name>
    </author>
    <author>
      <name>Maher Fakih</name>
    </author>
    <author>
      <name>Ralf Stemmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at HIP3ES, 2017 7 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.03709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.02925v1</id>
    <updated>2017-03-08T17:26:02Z</updated>
    <published>2017-03-08T17:26:02Z</published>
    <title>Assessing Code Authorship: The Case of the Linux Kernel</title>
    <summary>  Code authorship is a key information in large-scale open source systems.
Among others, it allows maintainers to assess division of work and identify key
collaborators. Interestingly, open-source communities lack guidelines on how to
manage authorship. This could be mitigated by setting to build an empirical
body of knowledge on how authorship-related measures evolve in successful
open-source communities. Towards that direction, we perform a case study on the
Linux kernel. Our results show that: (a) only a small portion of developers (26
%) makes significant contributions to the code base; (b) the distribution of
the number of files per author is highly skewed --- a small group of top
authors (3 %) is responsible for hundreds of files, while most authors (75 %)
are responsible for at most 11 files; (c) most authors (62 %) have a specialist
profile; (d) authors with a high number of co-authorship connections tend to
collaborate with others with less connections.
</summary>
    <author>
      <name>Guilherme Avelino</name>
    </author>
    <author>
      <name>Leonardo Passos</name>
    </author>
    <author>
      <name>Andre Hora</name>
    </author>
    <author>
      <name>Marco Tulio Valente</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-57735-7_15</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-57735-7_15" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at 13th International Conference on Open Source Systems
  (OSS). 12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.02925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.02925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08469v1</id>
    <updated>2017-03-24T15:38:17Z</updated>
    <published>2017-03-24T15:38:17Z</published>
    <title>Virtualization technology for distributed time sensitive domains</title>
    <summary>  This paper reports on the state of the art of virtualization technology for
both general purpose domains as well as real-time domains. There exits no
entirely instantaneous data transmission/transfer. There always exist a delay
while transmitting data, either in the processing or in the medium itself.
However most systems are designed to function appropriately with a delay
tolerance. This delay, inevitably, is affected when operating with an extra
layer, the virtualization. For real time systems it is crucial to know the
temporal limits in order not to surpass them. Introducing virtualization in the
real-time domain therefore requires deeper analysis by making use of techniques
that will offer results with deterministic execution times. The study of time
in systems and its behaviour under various possible circumstances is hence a
key for properly assessing this technology applied to both domains, especially
the real-time domain.
</summary>
    <author>
      <name>Carlos Antonio Perea-Gómez</name>
    </author>
    <link href="http://arxiv.org/abs/1703.08469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02561v1</id>
    <updated>2017-05-07T04:07:10Z</updated>
    <published>2017-05-07T04:07:10Z</published>
    <title>A Reconnaissance Attack Mechanism for Fixed-Priority Real-Time Systems</title>
    <summary>  In real-time embedded systems (RTS), failures due to security breaches can
cause serious damage to the system, the environment and/or injury to humans.
Therefore, it is very important to understand the potential threats and attacks
against these systems. In this paper we present a novel reconnaissance attack
that extracts the exact schedule of real-time systems designed using fixed
priority scheduling algorithms. The attack is demonstrated on both a real
hardware platform and a simulator, with a high success rate. Our evaluation
results show that the algorithm is robust even in the presence of execution
time variation.
</summary>
    <author>
      <name>Chien-Ying Chen</name>
    </author>
    <author>
      <name>AmirEmad Ghassami</name>
    </author>
    <author>
      <name>Sibin Mohan</name>
    </author>
    <author>
      <name>Negar Kiyavash</name>
    </author>
    <author>
      <name>Rakesh B. Bobba</name>
    </author>
    <author>
      <name>Rodolfo Pellizzoni</name>
    </author>
    <author>
      <name>Man-Ki Yoon</name>
    </author>
    <link href="http://arxiv.org/abs/1705.02561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07400v1</id>
    <updated>2017-05-21T05:51:21Z</updated>
    <published>2017-05-21T05:51:21Z</published>
    <title>MITHRIL: Mining Sporadic Associations for Cache Prefetching</title>
    <summary>  The growing pressure on cloud application scalability has accentuated storage
performance as a critical bottle- neck. Although cache replacement algorithms
have been extensively studied, cache prefetching - reducing latency by
retrieving items before they are actually requested remains an underexplored
area. Existing approaches to history-based prefetching, in particular, provide
too few benefits for real systems for the resources they cost. We propose
MITHRIL, a prefetching layer that efficiently exploits historical patterns in
cache request associations. MITHRIL is inspired by sporadic association rule
mining and only relies on the timestamps of requests. Through evaluation of 135
block-storage traces, we show that MITHRIL is effective, giving an average of a
55% hit ratio increase over LRU and PROBABILITY GRAPH, a 36% hit ratio gain
over AMP at reasonable cost. We further show that MITHRIL can supplement any
cache replacement algorithm and be readily integrated into existing systems.
Furthermore, we demonstrate the improvement comes from MITHRIL being able to
capture mid-frequency blocks.
</summary>
    <author>
      <name>Juncheng Yang</name>
    </author>
    <author>
      <name>Reza Karimi</name>
    </author>
    <author>
      <name>Trausti Sæmundsson</name>
    </author>
    <author>
      <name>Avani Wildani</name>
    </author>
    <author>
      <name>Ymir Vigfusson</name>
    </author>
    <link href="http://arxiv.org/abs/1705.07400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.05082v2</id>
    <updated>2017-07-18T05:17:14Z</updated>
    <published>2017-07-17T10:38:13Z</published>
    <title>Downgrade Attack on TrustZone</title>
    <summary>  Security-critical tasks require proper isolation from untrusted software.
Chip manufacturers design and include trusted execution environments (TEEs) in
their processors to secure these tasks. The integrity and security of the
software in the trusted environment depend on the verification process of the
system.
  We find a form of attack that can be performed on the current implementations
of the widely deployed ARM TrustZone technology. The attack exploits the fact
that the trustlet (TA) or TrustZone OS loading verification procedure may use
the same verification key and may lack proper rollback prevention across
versions. If an exploit works on an out-of-date version, but the vulnerability
is patched on the latest version, an attacker can still use the same exploit to
compromise the latest system by downgrading the software to an older and
exploitable version.
  We did experiments on popular devices on the market including those from
Google, Samsung and Huawei, and found that all of them have the risk of being
attacked. Also, we show a real-world example to exploit Qualcomm's QSEE.
  In addition, in order to find out which device images share the same
verification key, pattern matching schemes for different vendors are analyzed
and summarized.
</summary>
    <author>
      <name>Yue Chen</name>
    </author>
    <author>
      <name>Yulong Zhang</name>
    </author>
    <author>
      <name>Zhi Wang</name>
    </author>
    <author>
      <name>Tao Wei</name>
    </author>
    <link href="http://arxiv.org/abs/1707.05082v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05082v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06175v1</id>
    <updated>2017-10-17T09:07:13Z</updated>
    <published>2017-10-17T09:07:13Z</published>
    <title>Towards Linux Kernel Memory Safety</title>
    <summary>  The security of billions of devices worldwide depends on the security and
robustness of the mainline Linux kernel. However, the increasing number of
kernel-specific vulnerabilities, especially memory safety vulnerabilities,
shows that the kernel is a popular and practically exploitable target. Two
major causes of memory safety vulnerabilities are reference counter overflows
(temporal memory errors) and lack of pointer bounds checking (spatial memory
errors).
  To succeed in practice, security mechanisms for critical systems like the
Linux kernel must also consider performance and deployability as critical
design objectives. We present and systematically analyze two such mechanisms
for improving memory safety in the Linux kernel: (a) an overflow-resistant
reference counter data structure designed to accommodate typical reference
counter usage in kernel source code, and (b) runtime pointer bounds checking
using Intel MPX in the kernel.
</summary>
    <author>
      <name>Elena Reshetova</name>
    </author>
    <author>
      <name>Hans Liljestrand</name>
    </author>
    <author>
      <name>Andrew Paverd</name>
    </author>
    <author>
      <name>N. Asokan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/spe.2638</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/spe.2638" rel="related"/>
    <link href="http://arxiv.org/abs/1710.06175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09921v1</id>
    <updated>2017-10-26T21:34:34Z</updated>
    <published>2017-10-26T21:34:34Z</published>
    <title>Exploiting Commutativity For Practical Fast Replication</title>
    <summary>  Traditional approaches to replication require client requests to be ordered
before making them durable by copying them to replicas. As a result, clients
must wait for two round-trip times (RTTs) before updates complete. In this
paper, we show that this entanglement of ordering and durability is unnecessary
for strong consistency. Consistent Unordered Replication Protocol (CURP) allows
clients to replicate requests that have not yet been ordered, as long as they
are commutative. This strategy allows most operations to complete in 1 RTT (the
same as an unreplicated system). We implemented CURP in the Redis and RAMCloud
storage systems. In RAMCloud, CURP improved write latency by ~2x (13.8 us -&gt;
7.3 us) and write throughput by 4x. Compared to unreplicated RAMCloud, CURP's
latency overhead for 3-way replication is just 0.4 us (6.9 us vs 7.3 us). CURP
transformed a non-durable Redis cache into a consistent and durable storage
system with only a small performance overhead.
</summary>
    <author>
      <name>Seo Jin Park</name>
    </author>
    <author>
      <name>John Ousterhout</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.09921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08738v3</id>
    <updated>2018-04-27T02:19:42Z</updated>
    <published>2017-12-23T08:44:28Z</published>
    <title>Protecting real-time GPU kernels on integrated CPU-GPU SoC platforms</title>
    <summary>  Integrated CPU-GPU architecture provides excellent acceleration capabilities
for data parallel applications on embedded platforms while meeting the size,
weight and power (SWaP) requirements. However, sharing of main memory between
CPU applications and GPU kernels can severely affect the execution of GPU
kernels and diminish the performance gain provided by GPU. For example, in the
NVIDIA Tegra K1 platform which has the integrated CPU-GPU architecture, we
noticed that in the worst case scenario, the GPU kernels can suffer as much as
4X slowdown in the presence of co-running memory intensive CPU applications
compared to their solo execution. In this paper, we propose a software
mechanism, which we call BWLOCK++, to protect the performance of GPU kernels
from co-scheduled memory intensive CPU applications.
</summary>
    <author>
      <name>Waqar Ali</name>
    </author>
    <author>
      <name>Heechul Yun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will be published at ECRTS-2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.08738v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08738v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04565v1</id>
    <updated>2018-01-14T14:31:46Z</updated>
    <published>2018-01-14T14:31:46Z</published>
    <title>Shai: Enforcing Data-Specific Policies with Near-Zero Runtime Overhead</title>
    <summary>  Data retrieval systems such as online search engines and online social
networks must comply with the privacy policies of personal and selectively
shared data items, regulatory policies regarding data retention and censorship,
and the provider's own policies regarding data use. Enforcing these policies is
difficult and error-prone. Systematic techniques to enforce policies are either
limited to type-based policies that apply uniformly to all data of the same
type, or incur significant runtime overhead.
  This paper presents Shai, the first system that systematically enforces
data-specific policies with near-zero overhead in the common case. Shai's key
idea is to push as many policy checks as possible to an offline, ahead-of-time
analysis phase, often relying on predicted values of runtime parameters such as
the state of access control lists or connected users' attributes. Runtime
interception is used sparingly, only to verify these predictions and to make
any remaining policy checks. Our prototype implementation relies on efficient,
modern OS primitives for sandboxing and isolation. We present the design of
Shai and quantify its overheads on an experimental data indexing and search
pipeline based on the popular search engine Apache Lucene.
</summary>
    <author>
      <name>Eslam Elnikety</name>
    </author>
    <author>
      <name>Deepak Garg</name>
    </author>
    <author>
      <name>Peter Druschel</name>
    </author>
    <link href="http://arxiv.org/abs/1801.04565v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04565v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.08873v1</id>
    <updated>2018-01-26T15:59:51Z</updated>
    <published>2018-01-26T15:59:51Z</published>
    <title>Mirrored and Hybrid Disk Arrays: Organization, Scheduling, Reliability,
  and Performance</title>
    <summary>  Basic mirroring (BM) classified as RAID level 1 replicates data on two disks,
thus doubling disk access bandwidth for read requests. RAID1/0 is an array of
BM pairs with balanced loads due to striping. When a disk fails the read load
on its pair is doubled, which results in halving the maximum attainable
bandwidth. We review RAID1 organizations which attain a balanced load upon disk
failure, but as shown by reliability analysis tend to be less reliable than
RAID1/0. Hybrid disk arrays which store XORed instead of replicated data tend
to have a higher reliability than mirrored disks, but incur a higher overhead
in updating data. Read request response time can be improved by processing them
at a higher priority than writes, since they have a direct effect on
application response time. Shortest seek distance and affinity based routing
both shorten seek time. Anticipatory arm placement places arms optimally to
minimize the seek distance. The analysis of RAID1 in normal, degraded, and
rebuild mode is provided to quantify RAID1/0 performance. We compare the
reliability of mirrored disk organizations against each other and hybrid disks
and erasure coded disk arrays.
</summary>
    <author>
      <name>Alexander Thomasian</name>
    </author>
    <link href="http://arxiv.org/abs/1801.08873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.08873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00673v1</id>
    <updated>2018-02-02T13:21:13Z</updated>
    <published>2018-02-02T13:21:13Z</published>
    <title>Representation Learning for Resource Usage Prediction</title>
    <summary>  Creating a model of a computer system that can be used for tasks such as
predicting future resource usage and detecting anomalies is a challenging
problem. Most current systems rely on heuristics and overly simplistic
assumptions about the workloads and system statistics. These heuristics are
typically a one-size-fits-all solution so as to be applicable in a wide range
of applications and systems environments.
  With this paper, we present our ongoing work of integrating systems telemetry
ranging from standard resource usage statistics to kernel and library calls of
applications into a machine learning model. Intuitively, such a ML model
approximates, at any point in time, the state of a system and allows us to
solve tasks such as resource usage prediction and anomaly detection. To achieve
this goal, we leverage readily-available information that does not require any
changes to the applications run on the system. We train recurrent neural
networks to learn a model of the system under consideration. As a proof of
concept, we train models specifically to predict future resource usage of
running applications.
</summary>
    <author>
      <name>Florian Schmidt</name>
    </author>
    <author>
      <name>Mathias Niepert</name>
    </author>
    <author>
      <name>Felipe Huici</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, SysML 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.00673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00922v1</id>
    <updated>2018-02-03T06:18:56Z</updated>
    <published>2018-02-03T06:18:56Z</published>
    <title>Realizing Uncertainty-Aware Timing Stack in Embedded Operating System</title>
    <summary>  Time awareness is critical to a broad range of emerging applications -- in
Cyber-Physical Systems and Internet of Things -- running on commodity platforms
and operating systems. Traditionally, time is synchronized across devices
through a best-effort background service whose performance is neither
observable nor controllable, thus consuming system resources independently of
application needs while not allowing the applications and OS services to adapt
to changes in uncertainty in system time. We advocate for rethinking how time
is managed in a system stack. In this paper, we propose a new clock model that
characterizes various sources of timing uncertainties in true time. We then
present a Kalman filter based time synchronization protocol that adapts to the
uncertainties exposed by the clock model. Our realization of a
uncertainty-aware clock model and synchronization protocol is based on a
standard embedded Linux platform.
</summary>
    <author>
      <name>Amr Alanwar</name>
    </author>
    <author>
      <name>Fatima M. Anwar</name>
    </author>
    <author>
      <name>Joao P Hespanha</name>
    </author>
    <author>
      <name>Mani Srivastava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proc. of the Embedded Operating Systems Workshop, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.00922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00705v1</id>
    <updated>2018-03-26T03:52:05Z</updated>
    <published>2018-03-26T03:52:05Z</published>
    <title>Reactive NaN Repair for Applying Approximate Memory to Numerical
  Applications</title>
    <summary>  Applications in the AI and HPC fields require much memory capacity, and the
amount of energy consumed by main memory of server machines is ever increasing.
Energy consumption of main memory can be greatly reduced by applying
approximate computing in exchange for increased bit error rates. AI and HPC
applications are to some extent robust to bit errors because small numerical
errors are amortized by their iterative nature. However, a single occurrence of
a NaN due to bit-flips corrupts the whole calculation result. The issue is that
fixing every bit-flip using ECC incurs too much overhead because the bit error
rate is much higher than in normal environments. We propose a low-overhead
method to fix NaNs when approximate computing is applied to main memory. The
main idea is to reactively repair NaNs while leaving other non-fatal numerical
errors as-is to reduce the overhead. We implemented a prototype by leveraging
floating-point exceptions of x86 CPUs, and the preliminary evaluations showed
that our method incurs negligible overhead.
</summary>
    <author>
      <name>Shinsuke Hamada</name>
    </author>
    <author>
      <name>Soramichi Akiyama</name>
    </author>
    <author>
      <name>Mitaro Namiki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in the 8th Workshop on Systems for Multi-core and
  Heterogeneous Architectures (SFMA), co-located with EuroSys'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.00705v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00705v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03546v1</id>
    <updated>2018-07-10T09:24:33Z</updated>
    <published>2018-07-10T09:24:33Z</published>
    <title>Parallel Architecture Hardware and General Purpose Operating System
  Co-design</title>
    <summary>  Because most optimisations to achieve higher computational performance
eventually are limited, parallelism that scales is required. Parallelised
hardware alone is not sufficient, but software that matches the architecture is
required to gain best performance. For decades now, hardware design has been
guided by the basic design of existing software, to avoid the higher cost to
redesign the latter. In doing so, however, quite a variety of superior concepts
is excluded a priori. Consequently, co-design of both hardware and software is
crucial where highest performance is the goal. For special purpose application,
this co-design is common practice. For general purpose application, however, a
precondition for usability of a computer system is an operating system which is
both comprehensive and dynamic. As no such operating system has ever been
designed, a sketch for a comprehensive dynamic operating system is presented,
based on a straightforward hardware architecture to demonstrate how design
decisions regarding software and hardware do coexist and harmonise.
</summary>
    <author>
      <name>Oskar Schirmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">66 pages, 30 figures and tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09696v1</id>
    <updated>2018-07-25T16:15:01Z</updated>
    <published>2018-07-25T16:15:01Z</published>
    <title>Fast &amp; Flexible IO : A Compositional Approach to Storage Construction
  for High-Performance Devices</title>
    <summary>  Building storage systems has remained the domain of systems experts for many
years. They are complex and difficult to implement. Extreme care is needed to
ensure necessary guarantees of performance and operational correctness.
Furthermore, because of restrictions imposed by kernel-based designs, many
legacy implementations have traded software flexibility for performance. Their
implementation is restricted to compiled languages such as C and assembler, and
reuse tends to be difficult or constrained. Nevertheless, storage systems are
implicitly well-suited to software reuse and compositional software
construction. There are many logical functions, such as block allocation,
caching, partitioning, metadata management and so forth, that are common across
most variants of storage. In this paper, we present Comanche, an open-source
project that considers, as first-class concerns, both compositional design and
reuse, and the need for high-performance.
</summary>
    <author>
      <name>Daniel G. Waddington</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06434v1</id>
    <updated>2018-08-20T13:15:56Z</updated>
    <published>2018-08-20T13:15:56Z</published>
    <title>X-Lap: A Systems Approach for Cross-Layer Profiling and Latency Analysis
  for Cyber-Physical Networks</title>
    <summary>  Networked control applications for cyber-physical networks demand predictable
and reliable real-time communication. Applications of this domain have to
cooperate with network protocols, the operating system, and the hardware to
improve safety properties and increase resource efficiency. In consequence, a
cross-layer approach is necessary for the design and holistic optimisation of
cyber-physical systems and networks. This paper presents X-Lap, a cross-layer,
inter-host timing analysis tool tailored to the needs of real-time
communication. We use X-Lap to evaluate the timing behaviour of a reliable
real-time communication protocol. Our analysis identifies parts of the protocol
which are responsible for unwanted jitter. To system designers, X-Lap provides
useful support for the design and evaluation of networked real-time systems.
</summary>
    <author>
      <name>Stefan Reif</name>
    </author>
    <author>
      <name>Andreas Schmidt</name>
    </author>
    <author>
      <name>Timo Hönig</name>
    </author>
    <author>
      <name>Thorsten Herfet</name>
    </author>
    <author>
      <name>Wolfgang Schröder-Preikschat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3267419.3267422</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3267419.3267422" rel="related"/>
    <link href="http://arxiv.org/abs/1808.06434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.01314v1</id>
    <updated>2019-03-04T15:48:00Z</updated>
    <published>2019-03-04T15:48:00Z</published>
    <title>Denial-of-Service Attacks on Shared Cache in Multicore: Analysis and
  Prevention</title>
    <summary>  In this paper we investigate the feasibility of denial-of-service (DoS)
attacks on shared caches in multicore platforms. With carefully engineered
attacker tasks, we are able to cause more than 300X execution time increases on
a victim task running on a dedicated core on a popular embedded multicore
platform, regardless of whether we partition its shared cache or not. Based on
careful experimentation on real and simulated multicore platforms, we identify
an internal hardware structure of a non-blocking cache, namely the cache
writeback buffer, as a potential target of shared cache DoS attacks. We propose
an OS-level solution to prevent such DoS attacks by extending a
state-of-the-art memory bandwidth regulation mechanism. We implement the
proposed mechanism in Linux on a real multicore platform and show its
effectiveness in protecting against cache DoS attacks.
</summary>
    <author>
      <name>Michael G Bechtel</name>
    </author>
    <author>
      <name>Heechul Yun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published as a conference paper at RTAS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.01314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.11560v1</id>
    <updated>2019-04-25T19:57:31Z</updated>
    <published>2019-04-25T19:57:31Z</published>
    <title>A Survey on Tiering and Caching in High-Performance Storage Systems</title>
    <summary>  Although every individual invented storage technology made a big step towards
perfection, none of them is spotless. Different data store essentials such as
performance, availability, and recovery requirements have not met together in a
single economically affordable medium, yet. One of the most influential factors
is price. So, there has always been a trade-off between having a desired set of
storage choices and the costs. To address this issue, a network of various
types of storing media is used to deliver the high performance of expensive
devices such as solid state drives and non-volatile memories, along with the
high capacity of inexpensive ones like hard disk drives. In software, caching
and tiering are long-established concepts for handling file operations and
moving data automatically within such a storage network and manage data backup
in low-cost media. Intelligently moving data around different devices based on
the needs is the key insight for this matter. In this survey, we discuss some
recent pieces of research that have been done to improve high-performance
storage systems with caching and tiering techniques.
</summary>
    <author>
      <name>Morteza Hoseinzadeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ph.D. Research Exam Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.11560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.11560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.08192v1</id>
    <updated>2019-05-20T16:16:17Z</updated>
    <published>2019-05-20T16:16:17Z</published>
    <title>Secure Extensibility for System State Extraction via Plugin Sandboxing</title>
    <summary>  We introduce a new mechanism to securely extend systems data collection
software with potentially untrusted third-party code. Unlike existing tools
which run extension modules or plugins directly inside the monitored endpoint
(the guest), we run plugins inside a specially crafted sandbox, so as to
protect the guest as well as the software core. To get the right mix of
accessibility and constraints required for systems data extraction, we create
our sandbox by combining multiple features exported by an unmodified kernel. We
have tested its applicability by successfully sandboxing plugins of an
opensourced data collection software for containerized guest systems. We have
also verified its security posture in terms of successful containment of
several exploits, which would have otherwise directly impacted a guest, if
shipped inside third-party plugins.
</summary>
    <author>
      <name>Sahil Suneja</name>
    </author>
    <author>
      <name>Canturk Isci</name>
    </author>
    <link href="http://arxiv.org/abs/1905.08192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.09543v1</id>
    <updated>2019-05-23T08:57:45Z</updated>
    <published>2019-05-23T08:57:45Z</published>
    <title>MemoryRanger Prevents Hijacking FILE_OBJECT Structures in Windows Kernel</title>
    <summary>  Windows OS kernel memory is one of the main targets of cyber-attacks. By
launching such attacks, hackers are succeeding in process privilege escalation
and tampering with users data by accessing kernel mode memory. This paper
considers a new example of such an attack, which results in access to the files
opened in an exclusive mode. Windows built-in security features prevent such
legal access, but attackers can circumvent them by patching dynamically
allocated objects. The research shows that the Windows 10, version 1809 x64 is
vulnerable to this attack. The paper provides an example of using MemoryRanger,
a hypervisor-based solution to prevent such attack by running kernel-mode
drivers in isolated kernel memory enclaves.
</summary>
    <author>
      <name>Igor Korkin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures. Korkin, I. (2019, May 15-16). MemoryRanger
  Prevents Hijacking FILE_OBJECT Structures in Windows Kernel. Paper presented
  at the Proceedings of the 14th annual Conference on Digital Forensics,
  Security and Law (CDFSL), Embry-Riddle Aeronautical University, Daytona
  Beach, Florida, USA. Retrieved from
  https://commons.erau.edu/adfsl/2019/paper-presentation/7/</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.09543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.09543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.10141v1</id>
    <updated>2019-05-24T10:49:49Z</updated>
    <published>2019-05-24T10:49:49Z</published>
    <title>Scan-and-Pay on Android is Dangerous</title>
    <summary>  Mobile payments have increased significantly in the recent years and
one-to-one money transfers are offered by a wide variety of smartphone
applications. These applications usually support scan-and-pay -- a technique
that allows a payer to easily scan the destination address of the payment
directly from the payee's smartphone screen. This technique is pervasive
because it does not require any particular hardware, only the camera, which is
present on all modern smartphones. However, in this work we show that a
malicious application can exploit the overlay feature on Android to compromise
the integrity of transactions that make use of the scan-and-pay technique. We
implement Malview, a proof-of-concept malicious application that runs in the
background on the payee's smartphone and show that it succeeds in redirecting
payments to a malicious wallet. We analyze the weaknesses of the current
defense mechanisms and discuss possible countermeasures against the attack.
</summary>
    <author>
      <name>Enis Ulqinaku</name>
    </author>
    <author>
      <name>Julinda Stefa</name>
    </author>
    <author>
      <name>Alessandro Mei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Infocom MobiSec Workshop 2019, Paris, France</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.10141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.10141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.12974v3</id>
    <updated>2020-02-12T10:51:12Z</updated>
    <published>2019-05-30T11:37:37Z</published>
    <title>ExplFrame: Exploiting Page Frame Cache for Fault Analysis of Block
  Ciphers</title>
    <summary>  Page Frame Cache (PFC) is a purely software cache, present in modern Linux
based operating systems (OS), which stores the page frames that are recently
being released by the processes running on a particular CPU. In this paper, we
show that the page frame cache can be maliciously exploited by an adversary to
steer the pages of a victim process to some pre-decided attacker-chosen
locations in the memory. We practically demonstrate an end-to-end attack,
ExplFrame, where an attacker having only user-level privilege is able to force
a victim process's memory pages to vulnerable locations in DRAM and
deterministically conduct Rowhammer to induce faults. We further show that
these faults can be exploited for extracting the secret key of table-based
block cipher implementations. As a case study, we perform a full-key recovery
on OpenSSL AES by Rowhammer-induced single bit faults in the T-tables. We
propose an improvised fault analysis technique which can exploit any
Rowhammer-induced bit-flips in the AES T-tables.
</summary>
    <author>
      <name>Anirban Chakraborty</name>
    </author>
    <author>
      <name>Sarani Bhattacharya</name>
    </author>
    <author>
      <name>Sayandeep Saha</name>
    </author>
    <author>
      <name>Debdeep Mukhopadhyay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figues</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.12974v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.12974v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.05308v1</id>
    <updated>2019-07-11T15:33:34Z</updated>
    <published>2019-07-11T15:33:34Z</published>
    <title>Executable formal semantics for the POSIX shell</title>
    <summary>  The POSIX shell is a widely deployed, powerful tool for managing computer
systems. The shell is the expert's control panel, a necessary tool for
configuring, compiling, installing, maintaining, and deploying systems. Even
though it is powerful, critical infrastructure, the POSIX shell is maligned and
misunderstood. Its power and its subtlety are a dangerous combination.
  We define a formal, mechanized, executable small-step semantics for the POSIX
shell, which we call Smoosh. We compared Smoosh against seven other shells that
aim for some measure of POSIX compliance (bash, dash, zsh, OSH, mksh, ksh93,
and yash). Using three test suites---the POSIX test suite, the Modernish test
suite and shell diagnosis, and a test suite of our own device---we found
Smoosh's semantics to be the most conformant to the POSIX standard. Modernish
judges Smoosh to have the fewest bugs (just one, from using dash's parser) and
no quirks. To show that our semantics is useful beyond yielding a conformant,
executable shell, we also implemented a symbolic stepper to illuminate the
subtle behavior of the shell.
  Smoosh will serve as a foundation for formal study of the POSIX shell,
supporting research on and development of new shells, new tooling for shells,
and new shell designs.
</summary>
    <author>
      <name>Michael Greenberg</name>
    </author>
    <author>
      <name>Austin J. Blatt</name>
    </author>
    <link href="http://arxiv.org/abs/1907.05308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.05308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.10119v2</id>
    <updated>2019-09-07T05:54:07Z</updated>
    <published>2019-07-23T20:24:19Z</published>
    <title>Keystone: An Open Framework for Architecting TEEs</title>
    <summary>  Trusted execution environments (TEEs) are being used in all the devices from
embedded sensors to cloud servers and encompass a range of cost, power
constraints, and security threat model choices. On the other hand, each of the
current vendor-specific TEEs makes a fixed set of trade-offs with little room
for customization. We present Keystone -- the first open-source framework for
building customized TEEs. Keystone uses simple abstractions provided by the
hardware such as memory isolation and a programmable layer underneath untrusted
components (e.g., OS). We build reusable TEE core primitives from these
abstractions while allowing platform-specific modifications and application
features. We showcase how Keystone-based TEEs run on unmodified RISC-V hardware
and demonstrate the strengths of our design in terms of security, TCB size,
execution of a range of benchmarks, applications, kernels, and deployment
models.
</summary>
    <author>
      <name>Dayeol Lee</name>
    </author>
    <author>
      <name>David Kohlbrenner</name>
    </author>
    <author>
      <name>Shweta Shinde</name>
    </author>
    <author>
      <name>Dawn Song</name>
    </author>
    <author>
      <name>Krste Asanović</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.10119v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.10119v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.03638v1</id>
    <updated>2019-08-09T21:33:33Z</updated>
    <published>2019-08-09T21:33:33Z</published>
    <title>Good Motive but Bad Design: Why ARM MPU Has Become an Outcast in
  Embedded Systems</title>
    <summary>  As more and more embedded devices are connected to the Internet, leading to
the emergence of Internet-of-Things (IoT), previously less tested (and
insecure) devices are exposed to miscreants. To prevent them from being
compromised, the memory protection unit (MPU), which is readily available on
many devices, has the potential to become a free lunch for the defenders. To
our surprise, the MPU is seldom used by real-world products. The reasons are
multi-fold. While there are non-technical reasons such as compatibility issues,
more importantly, we found that MPU brings virtually no security enhancement at
the expense of decreased performance and responsiveness. In this work, we
investigate the MPU adoption in major real-time operating systems (RTOSs), in
particular, the FreeRTOS, and try to pinpoint the fundamental reasons to
explain why MPU is not favored. We hope our findings can inspire new remedial
solutions to change the situation. We also review the latest MPU design and
provide technical suggestions to build more secure embedded systems.
</summary>
    <author>
      <name>Wei Zhou</name>
    </author>
    <author>
      <name>Le Guan</name>
    </author>
    <author>
      <name>Peng Liu</name>
    </author>
    <author>
      <name>Yuqing Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1908.03638v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.03638v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.09922v1</id>
    <updated>2019-08-26T21:04:37Z</updated>
    <published>2019-08-26T21:04:37Z</published>
    <title>Tvarak: Software-managed hardware offload for DAX NVM storage redundancy</title>
    <summary>  Tvarak efficiently implements system-level redundancy for direct-access (DAX)
NVM storage. Production storage systems complement device-level ECC (which
covers media errors) with system-checksums and cross-device parity. This
system-level redundancy enables detection of and recovery from data corruption
due to device firmware bugs (e.g., reading data from the wrong physical
location). Direct access to NVM penalizes software-only implementations of
system-level redundancy, forcing a choice between lack of data protection or
significant performance penalties. Offloading the update and verification of
system-level redundancy to Tvarak, a hardware controller co-located with the
last-level cache, enables efficient protection of data from such bugs in memory
controller and NVM DIMM firmware. Simulation-based evaluation with seven
data-intensive applications shows Tvarak's performance and energy efficiency.
For example, Tvarak reduces Redis set-only performance by only 3%, compared to
50% reduction for a state-of-the-art software-only approach.
</summary>
    <author>
      <name>Rajat Kateja</name>
    </author>
    <author>
      <name>Nathan Beckmann</name>
    </author>
    <author>
      <name>Gregory R. Ganger</name>
    </author>
    <link href="http://arxiv.org/abs/1908.09922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.09922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.14546v1</id>
    <updated>2019-10-31T15:47:35Z</updated>
    <published>2019-10-31T15:47:35Z</published>
    <title>Debian Package usage profiler for Debian based Systems</title>
    <summary>  The embedded devices of today due to their CPU, RAM capabilities can run
various Linux distributions but in most cases they are different from general
purpose distributions as they are usually lighter and specific to the needs of
that particular system. In this project, we share the problems associated in
adopting a fully heavy-weight Debian based system like Ubuntu in
embedded/automotive platforms and provide solutions to optimize them to
identify unused/redundant content in the system. This helps developer to reduce
the hefty general purpose distribution to an application specific distribution.
The solution involves collecting usage data in the system in a non-invasive
manner (to avoid any drop in performance) to suggest users the redundant,
unused parts of the system that can be safely removed without impacting the
system functionality.
</summary>
    <author>
      <name>Bharath Honnesara Sreenivasa</name>
    </author>
    <author>
      <name>Ajay Rajan</name>
    </author>
    <link href="http://arxiv.org/abs/1910.14546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.14546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.09829v1</id>
    <updated>2019-11-22T03:29:52Z</updated>
    <published>2019-11-22T03:29:52Z</published>
    <title>Effectively Prefetching Remote Memory with Leap</title>
    <summary>  Memory disaggregation over RDMA can improve the performance of
memory-constrained applications by replacing disk swapping with remote memory
accesses. However, state-of-the-art memory disaggregation solutions still use
data path components designed for slow disks. As a result, applications
experience remote memory access latency significantly higher than that of the
underlying low-latency network, which itself is too high for many applications.
  In this paper, we propose Leap, a prefetching solution for remote memory
accesses due to memory disaggregation. At its core, Leap employs an online,
majority-based prefetching algorithm, which increases the page cache hit rate.
We complement it with a lightweight and efficient data path in the kernel that
isolates each application's data path to the disaggregated memory and mitigates
latency bottlenecks arising from legacy throughput-optimizing operations.
Integration of Leap in the Linux kernel improves the median and tail remote
page access latencies of memory-bound applications by up to 104.04x and 22.62x,
respectively, over the default data path. This leads to up to 10.16x
performance improvements for applications using disaggregated memory in
comparison to the state-of-the-art solutions.
</summary>
    <author>
      <name>Hasan Al Maruf</name>
    </author>
    <author>
      <name>Mosharaf Chowdhury</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2020 USENIX Annual Technical Conference, USENIX ATC 2020, July
  15-17, pages: 843--857</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.09829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.09829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.03490v1</id>
    <updated>2019-12-07T11:44:37Z</updated>
    <published>2019-12-07T11:44:37Z</published>
    <title>Dependability Assessment of the Android OS through Fault Injection</title>
    <summary>  The reliability of mobile devices is a challenge for vendors, since the
mobile software stack has significantly grown in complexity. In this paper, we
study how to assess the impact of faults on the quality of user experience in
the Android mobile OS through fault injection. We first address the problem of
identifying a realistic fault model for the Android OS, by providing to
developers a set of lightweight and systematic guidelines for fault modeling.
Then, we present an extensible fault injection tool (AndroFIT) to apply such
fault model on actual, commercial Android devices. Finally, we present a large
fault injection experimentation on three Android products from major vendors,
and point out several reliability issues and opportunities for improving the
Android OS.
</summary>
    <author>
      <name>Domenico Cotroneo</name>
    </author>
    <author>
      <name>Antonio Ken Iannillo</name>
    </author>
    <author>
      <name>Roberto Natella</name>
    </author>
    <author>
      <name>Stefano Rosiello</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TR.2019.2954384</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TR.2019.2954384" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Reliability, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1912.03490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.03490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.06512v3</id>
    <updated>2020-08-13T18:00:46Z</updated>
    <published>2020-02-16T05:51:41Z</published>
    <title>Privaros: A Framework for Privacy-Compliant Delivery Drones</title>
    <summary>  We present Privaros, a framework to enforce privacy policies on drones.
Privaros is designed for commercial delivery drones, such as the ones that will
likely be used by Amazon Prime Air. Such drones visit a number of host
airspaces, each of which may have different privacy requirements. Privaros
provides an information flow control framework to enforce the policies of these
hosts on the guest delivery drones. The mechanisms in Privaros are built on top
of ROS, a middleware popular in many drone platforms. This paper presents the
design and implementation of these mechanisms, describes how policies are
specified, and shows that Privaros's policy specification can be integrated
with India's Digital Sky portal. Our evaluation shows that a drone running
Privaros can robustly enforce various privacy policies specified by hosts, and
that its core mechanisms only marginally increase communication latency and
power consumption.
</summary>
    <author>
      <name>Rakesh Rajan Beck</name>
    </author>
    <author>
      <name>Abhishek Vijeev</name>
    </author>
    <author>
      <name>Vinod Ganapathy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3372297.3417858</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3372297.3417858" rel="related"/>
    <link href="http://arxiv.org/abs/2002.06512v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06512v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.08454v1</id>
    <updated>2020-02-19T21:35:51Z</updated>
    <published>2020-02-19T21:35:51Z</published>
    <title>A Recurrent Neural Network Based Patch Recommender for Linux Kernel Bugs</title>
    <summary>  Software bugs in a production environment have an undesirable impact on
quality of service, unplanned system downtime, and disruption in good customer
experience, resulting in loss of revenue and reputation. Existing approaches to
automated software bug repair focuses on known bug templates detected using
static code analysis tools and test suites, and in automatic generation of
patch code for these bugs. We describe the typical bug fixing process employed
in the Linux kernel, and motivate the need for a new automated tool flow to fix
bugs. We present an initial design of such an automated tool that uses
Recurrent Neural Network (RNN) based Natural Language Processing to generate
patch recommendations from user generated bug reports. At the 50th percentile
of the test bugs, the correct patch occurs within the top 11.5 patch
recommendations output by the model. Further, we present a Linux kernel
developer's assessment of the quality of patches recommended for new unresolved
kernel bugs.
</summary>
    <author>
      <name>Anusha Bableshwar</name>
    </author>
    <author>
      <name>Arun Ravindran</name>
    </author>
    <author>
      <name>Manoj Iyer</name>
    </author>
    <link href="http://arxiv.org/abs/2002.08454v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.08454v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.11528v1</id>
    <updated>2020-02-25T13:43:53Z</updated>
    <published>2020-02-25T13:43:53Z</published>
    <title>Safe and Efficient Remote Application Code Execution on Disaggregated
  NVM Storage with eBPF</title>
    <summary>  With rapid improvements in NVM storage devices, the performance bottleneck is
gradually shifting to the network, thus giving rise to the notion of "data
movement wall". To reduce the amount of data movement over the network,
researchers have proposed near-data computing by shipping operations and
compute-extensions closer to storage devices. However, running arbitrary,
user-provided extensions in a shared, disaggregated storage environment
presents multiple challenges regarding safety, isolation, and performance.
Instead of approaching this problem from scratch, in this work we make a case
for leveraging the Linux kernel eBPF framework to program disaggregated NVM
storage devices. eBPF offers a safe, verifiable, and high-performance way of
executing untrusted, user-defined code in a shared runtime. In this paper, we
describe our experiences building a first prototype that supports remote
operations on storage using eBPF, discuss the limitations of our approach, and
directions for addressing them.
</summary>
    <author>
      <name>Kornilios Kourtis</name>
    </author>
    <author>
      <name>Animesh Trivedi</name>
    </author>
    <author>
      <name>Nikolas Ioannou</name>
    </author>
    <link href="http://arxiv.org/abs/2002.11528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.07945v2</id>
    <updated>2020-03-19T02:43:49Z</updated>
    <published>2020-03-17T21:09:24Z</published>
    <title>Co-Optimizing Performance and Memory FootprintVia Integrated CPU/GPU
  Memory Management, anImplementation on Autonomous Driving Platform</title>
    <summary>  Cutting-edge embedded system applications, such as self-driving cars and
unmanned drone software, are reliant on integrated CPU/GPU platforms for their
DNNs-driven workload, such as perception and other highly parallel components.
In this work, we set out to explore the hidden performance implication of GPU
memory management methods of integrated CPU/GPU architecture. Through a series
of experiments on micro-benchmarks and real-world workloads, we find that the
performance under different memory management methods may vary according to
application characteristics. Based on this observation, we develop a
performance model that can predict system overhead for each memory management
method based on application characteristics. Guided by the performance model,
we further propose a runtime scheduler. By conducting per-task memory
management policy switching and kernel overlapping, the scheduler can
significantly relieve the system memory pressure and reduce the multitasking
co-run response time. We have implemented and extensively evaluated our system
prototype on the NVIDIA Jetson TX2, Drive PX2, and Xavier AGX platforms, using
both Rodinia benchmark suite and two real-world case studies of drone software
and autonomous driving software.
</summary>
    <author>
      <name>Soroush Bateni</name>
    </author>
    <author>
      <name>Zhendong Wang</name>
    </author>
    <author>
      <name>Yuankun Zhu</name>
    </author>
    <author>
      <name>Yang Hu</name>
    </author>
    <author>
      <name>Cong Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2003.07945v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.07945v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.09252v1</id>
    <updated>2020-04-20T13:03:14Z</updated>
    <published>2020-04-20T13:03:14Z</published>
    <title>MemShield: GPU-assisted software memory encryption</title>
    <summary>  Cryptographic algorithm implementations are vulnerable to Cold Boot attacks,
which consist in exploiting the persistence of RAM cells across reboots or
power down cycles to read the memory contents and recover precious sensitive
data. The principal defensive weapon against Cold Boot attacks is memory
encryption. In this work we propose MemShield, a memory encryption framework
for user space applications that exploits a GPU to safely store the master key
and perform the encryption/decryption operations. We developed a prototype that
is completely transparent to existing applications and does not require changes
to the OS kernel. We discuss the design, the related works, the implementation,
the security analysis, and the performances of MemShield.
</summary>
    <author>
      <name>Pierpaolo Santucci</name>
    </author>
    <author>
      <name>Emiliano Ingrassia</name>
    </author>
    <author>
      <name>Giulio Picierro</name>
    </author>
    <author>
      <name>Marco Cesati</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 figures. In proceedings of the 18th International
  Conference on Applied Cryptography and Network Security, ACNS 2020, October
  19-22 2020, Rome, Italy</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.09252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.09252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6; K.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.00323v2</id>
    <updated>2021-03-11T19:49:06Z</updated>
    <published>2020-05-01T11:57:22Z</published>
    <title>Designing Robust API Monitoring Solutions</title>
    <summary>  Tracing the sequence of library and system calls that a program makes is very
helpful in the characterization of its interactions with the surrounding
environment and ultimately of its semantics. Due to entanglements of real-world
software stacks, accomplishing this task can be surprisingly challenging as we
take accuracy, reliability, and transparency into the equation. To manage these
dimensions effectively, we identify six challenges that API monitoring
solutions should overcome and outline actionable design points for them,
reporting insights from our experience in building API tracers for software
security research. We detail two implementation variants, based on
hardware-assisted virtualization (realizing the first general-purpose
user-space tracer of this kind) and on dynamic binary translation, that achieve
API monitoring robustly. We share our SNIPER system as open source.
</summary>
    <author>
      <name>Daniele Cono D'Elia</name>
    </author>
    <author>
      <name>Simone Nicchi</name>
    </author>
    <author>
      <name>Matteo Mariani</name>
    </author>
    <author>
      <name>Matteo Marini</name>
    </author>
    <author>
      <name>Federico Palmaro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE for possible publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.00323v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.00323v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.01354v1</id>
    <updated>2020-06-02T02:41:39Z</updated>
    <published>2020-06-02T02:41:39Z</published>
    <title>Flex: Closing the Gaps between Usage and Allocation</title>
    <summary>  Data centers are giant factories of Internet data and services. Worldwide
data centers consume energy and emit emissions more than airline industry.
Unfortunately, most of data centers are significantly underutilized. One of the
major reasons is the big gaps between the real usage and the provisioned
resources because users tend to over-estimate their demand and data center
operators often rely on users' requests for resource allocation. In this paper,
we first conduct an in-depth analysis of a Google cluster trace to unveil the
root causes for low utilization and highlight the great potential to improve
it. We then developed an online resource manager Flex to maximize the cluster
utilization while satisfying the Quality of Service (QoS). Large-scale
evaluations based on real-world traces show that Flex admits up to 1.74x more
requests and 1.6x higher utilization compared to tradition schedulers while
maintaining the QoS.
</summary>
    <author>
      <name>Tan N. Le</name>
    </author>
    <author>
      <name>Zhenhua Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2006.01354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.01354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.01159v1</id>
    <updated>2021-01-04T18:42:54Z</updated>
    <published>2021-01-04T18:42:54Z</published>
    <title>New Directions in Cloud Programming</title>
    <summary>  Nearly twenty years after the launch of AWS, it remains difficult for most
developers to harness the enormous potential of the cloud. In this paper we lay
out an agenda for a new generation of cloud programming research aimed at
bringing research ideas to programmers in an evolutionary fashion. Key to our
approach is a separation of distributed programs into a PACT of four facets:
Program semantics, Availablity, Consistency and Targets of optimization. We
propose to migrate developers gradually to PACT programming by lifting familiar
code into our more declarative level of abstraction. We then propose a
multi-stage compiler that emits human-readable code at each stage that can be
hand-tuned by developers seeking more control. Our agenda raises numerous
research challenges across multiple areas including language design, query
optimization, transactions, distributed consistency, compilers and program
synthesis.
</summary>
    <author>
      <name>Alvin Cheung</name>
    </author>
    <author>
      <name>Natacha Crooks</name>
    </author>
    <author>
      <name>Joseph M. Hellerstein</name>
    </author>
    <author>
      <name>Mae Milano</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.01159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.01159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.03109v1</id>
    <updated>2020-11-27T12:27:31Z</updated>
    <published>2020-11-27T12:27:31Z</published>
    <title>Routing Approach for P2P Systems Over MANET Network</title>
    <summary>  Thanks to the great progress in mobile and wireless technologies,
Internet-distributed applications like P2P file sharing are nowadays deployed
over MANET (i.e., P2P mobile systems). These applications allow users to search
and share diverse multimedia resources over MANET. Due the nature of MANET, P2P
mobile systems brought up many new thriving challenges regarding the query
routing issue. To tackle this problem, we introduce a novel context-aware query
routing protocol for unstructured P2P mobile file sharing systems. Our protocol
(i) locates relevant peers sharing pertinent resources for user's query and
(ii) ensures that those peers would be reached by considering different MANET
constraints (e.g., query content, peer mobility, battery energy, peer load). In
order to consider all these constraints for choosing the relevant peers, we are
based on the technique for order preferences by similarity to ideal solution
(TOPSIS). We implemented the proposed protocol and compared its routing
efficiency and retrieval effectiveness with another protocol taken from the
literature. Experimental results show that our scheme carries out better than
the baseline protocol with respect to accuracy
</summary>
    <author>
      <name>Sofian Hamad</name>
    </author>
    <author>
      <name>Taoufik Yeferny</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSNS International Journal of Computer Science and Network
  Security, VOL.20 No.3, March 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.03109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.03109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.08744v3</id>
    <updated>2021-09-01T14:53:54Z</updated>
    <published>2021-01-14T21:38:57Z</published>
    <title>Enabling Large Neural Networks on Tiny Microcontrollers with Swapping</title>
    <summary>  Running neural networks (NNs) on microcontroller units (MCUs) is becoming
increasingly important, but is very difficult due to the tiny SRAM size of MCU.
Prior work proposes many algorithm-level techniques to reduce NN memory
footprints, but all at the cost of sacrificing accuracy and generality, which
disqualifies MCUs for many important use cases. We investigate a system
solution for MCUs to execute NNs out of core: dynamically swapping NN data
chunks between an MCU's tiny SRAM and its large, low-cost external flash.
Out-of-core NNs on MCUs raise multiple concerns: execution slowdown, storage
wear out, energy consumption, and data security. We present a study showing
that none is a showstopper; the key benefit -- MCUs being able to run large NNs
with full accuracy and generality -- triumphs the overheads. Our findings
suggest that MCUs can play a much greater role in edge intelligence.
</summary>
    <author>
      <name>Hongyu Miao</name>
    </author>
    <author>
      <name>Felix Xiaozhu Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2101.08744v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.08744v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.09282v1</id>
    <updated>2021-01-23T22:43:28Z</updated>
    <published>2021-01-23T22:43:28Z</published>
    <title>Resilient Virtualized Systems Using ReHype</title>
    <summary>  System-level virtualization introduces critical vulnerabilities to failures
of the software components that implement virtualization -- the virtualization
infrastructure (VI). To mitigate the impact of such failures, we introduce a
resilient VI (RVI) that can recover individual VI components from failure,
caused by hardware or software faults, transparently to the hosted virtual
machines (VMs). Much of the focus is on the ReHype mechanism for recovery from
hypervisor failures, that can lead to state corruption and to inconsistencies
among the states of system components. ReHype's implementation for the Xen
hypervisor was done incrementally, using fault injection results to identify
sources of critical corruption and inconsistencies. This implementation
involved 900 LOC, with memory space overhead of 2.1MB. Fault injection
campaigns, with a variety of fault types, show that ReHype can successfully
recover, in less than 750ms, from over 88% of detected hypervisor failures. In
addition to ReHype, recovery mechanisms for the other VI components are
described. The overall effectiveness of our RVI is evaluated hosting a Web
service application, on a cluster of VMs. With faults in any VI component, for
over 87% of detected failures, our recovery mechanisms allow services provided
by the application to be continuously maintained despite the resulting failures
of VI components.
</summary>
    <author>
      <name>Michael Le</name>
    </author>
    <author>
      <name>Yuval Tamir</name>
    </author>
    <link href="http://arxiv.org/abs/2101.09282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.09282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.09284v1</id>
    <updated>2021-01-21T23:28:55Z</updated>
    <published>2021-01-21T23:28:55Z</published>
    <title>User-Level Memory Scheduler for Optimizing Application Performance in
  NUMA-Based Multicore Systems</title>
    <summary>  Multicore CPU architectures have been established as a structure for
general-purpose systems for high-performance processing of applications. Recent
multicore CPU has evolved as a system architecture based on non-uniform memory
architecture. For the technique of using the kernel space that shifts the tasks
to the ideal memory node, the characteristics of the applications of the
user-space cannot be considered. Therefore, kernel level approaches cannot
execute memory scheduling to recognize the importance of user applications.
Moreover, users need to run applications after sufficiently understanding the
multicore CPU based on non-uniform memory architecture to ensure the high
performance of the user's applications. This paper presents a user-space memory
scheduler that allocates the ideal memory node for tasks by monitoring the
characteristics of non-uniform memory architecture. From our experiment, the
proposed system improved the performance of the application by up to 25%
compared to the existing system.
</summary>
    <author>
      <name>Geunsik Lim</name>
    </author>
    <author>
      <name>Sang-Bum Suh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICSESS.2014.6933553</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICSESS.2014.6933553" rel="related"/>
    <link href="http://arxiv.org/abs/2101.09284v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.09284v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.14951v2</id>
    <updated>2021-08-16T16:24:21Z</updated>
    <published>2021-03-27T17:44:29Z</published>
    <title>A First Look at RISC-V Virtualization from an Embedded Systems
  Perspective</title>
    <summary>  This article describes the first public implementation and evaluation of the
latest version of the RISC-V hypervisor extension (H-extension v0.6.1)
specification in a Rocket chip core. To perform a meaningful evaluation for
modern multi-core embedded and mixedcriticality systems, we have ported Bao, an
open-source static partitioning hypervisor, to RISC-V. We have also extended
the RISC-V platformlevel interrupt controller (PLIC) to enable direct guest
interrupt injection with low and deterministic latency and we have enhanced the
timer infrastructure to avoid trap and emulation overheads. Experiments were
carried out in FireSim, a cycle-accurate, FPGA-accelerated simulator, and the
system was also successfully deployed and tested in a Zynq UltraScale+ MPSoC
ZCU104. Our hardware implementation was opensourced and is currently in use by
the RISC-V community towards the ratification of the H-extension specification.
</summary>
    <author>
      <name>Bruno Sá</name>
    </author>
    <author>
      <name>José Martins</name>
    </author>
    <author>
      <name>Sandro Pinto</name>
    </author>
    <link href="http://arxiv.org/abs/2103.14951v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.14951v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.02393v1</id>
    <updated>2021-04-06T09:49:56Z</updated>
    <published>2021-04-06T09:49:56Z</published>
    <title>Detecting and Mitigating Network Packet Overloads on Real-Time Devices
  in IoT Systems</title>
    <summary>  Manufacturing, automotive, and aerospace environments use embedded systems
for control and automation and need to fulfill strict real-time guarantees. To
facilitate more efficient business processes and remote control, such devices
are being connected to IP networks. Due to the difficulty in predicting network
packets and the interrelated workloads of interrupt handlers and drivers,
devices controlling time critical processes stand under the risk of missing
process deadlines when under high network loads. Additionally, devices at the
edge of large networks and the internet are subject to a high risk of load
spikes and network packet overloads.
  In this paper, we investigate strategies to detect network packet overloads
in real-time and present four approaches to adaptively mitigate local deadline
misses. In addition to two strategies mitigating network bursts with and
without hysteresis, we present and discuss two novel mitigation algorithms,
called Budget and Queue Mitigation. In an experimental evaluation, all
algorithms showed mitigating effects, with the Queue Mitigation strategy
enabling most packet processing while preventing lateness of critical tasks.
</summary>
    <author>
      <name>Robert Danicki</name>
    </author>
    <author>
      <name>Martin Haug</name>
    </author>
    <author>
      <name>Ilja Behnke</name>
    </author>
    <author>
      <name>Laurenz Mädje</name>
    </author>
    <author>
      <name>Lauritz Thamsen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3434770.3459733</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3434770.3459733" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EdgeSys '21</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.02393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.02393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04528v1</id>
    <updated>2021-04-09T13:16:06Z</updated>
    <published>2021-04-09T13:16:06Z</published>
    <title>SchedGuard: Protecting against Schedule Leaks Using Linux Containers</title>
    <summary>  Real-time systems have recently been shown to be vulnerable to timing
inference attacks, mainly due to their predictable behavioral patterns.
Existing solutions such as schedule randomization lack the ability to protect
against such attacks, often limited by the system's real-time nature. This
paper presents SchedGuard: a temporal protection framework for Linux-based hard
real-time systems that protects against posterior scheduler side-channel
attacks by preventing untrusted tasks from executing during specific time
segments. SchedGuard is integrated into the Linux kernel using cgroups, making
it amenable to use with container frameworks. We demonstrate the effectiveness
of our system using a realistic radio-controlled rover platform and
synthetically generated workloads. Not only is SchedGuard able to protect
against the attacks mentioned above, but it also ensures that the real-time
tasks/containers meet their temporal requirements.
</summary>
    <author>
      <name>Jiyang Chen</name>
    </author>
    <author>
      <name>Tomasz Kloda</name>
    </author>
    <author>
      <name>Ayoosh Bansal</name>
    </author>
    <author>
      <name>Rohan Tabish</name>
    </author>
    <author>
      <name>Chien-Ying Chen</name>
    </author>
    <author>
      <name>Bo Liu</name>
    </author>
    <author>
      <name>Sibin Mohan</name>
    </author>
    <author>
      <name>Marco Caccamo</name>
    </author>
    <author>
      <name>Lui Sha</name>
    </author>
    <link href="http://arxiv.org/abs/2104.04528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.14862v1</id>
    <updated>2021-04-30T09:37:14Z</updated>
    <published>2021-04-30T09:37:14Z</published>
    <title>WELES: Policy-driven Runtime Integrity Enforcement of Virtual Machines</title>
    <summary>  Trust is of paramount concern for tenants to deploy their security-sensitive
services in the cloud. The integrity of VMs in which these services are
deployed needs to be ensured even in the presence of powerful adversaries with
administrative access to the cloud. Traditional approaches for solving this
challenge leverage trusted computing techniques, e.g., vTPM, or hardware CPU
extensions, e.g., AMD SEV. But, they are vulnerable to powerful adversaries, or
they provide only load time (not runtime) integrity measurements of VMs.
  We propose WELES, a protocol allowing tenants to establish and maintain trust
in VM runtime integrity of software and its configuration. WELES is transparent
to the VM configuration and setup. It performs an implicit attestation of VMs
during a secure login and binds the VM integrity state with the secure
connection. Our prototype's evaluation shows that WELES is practical and incurs
low performance overhead.
</summary>
    <author>
      <name>Wojciech Ozga</name>
    </author>
    <author>
      <name>Do Le Quoc</name>
    </author>
    <author>
      <name>Christof Fetzer</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 2021 IEEE International Conference on Cloud
  Computing (IEEE CLOUD'21)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.14862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.14862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.10397v2</id>
    <updated>2021-09-03T12:47:12Z</updated>
    <published>2021-05-14T14:08:10Z</published>
    <title>NVCache: A Plug-and-Play NVMM-based I/O Booster for Legacy Systems</title>
    <summary>  This paper introduces NVCache, an approach that uses a non-volatile main
memory (NVMM) as a write cache to improve the write performance of legacy
applications. We compare NVCache against file systems tailored for NVMM
(Ext4-DAX and NOVA) and with I/O-heavy applications (SQLite, RocksDB). Our
evaluation shows that NVCache reaches the performance level of the existing
state-of-the-art systems for NVMM, but without their limitations: NVCache does
not limit the size of the stored data to the size of the NVMM, and works
transparently with unmodified legacy applications, providing additional
persistence guarantees even when their source code is not available.
</summary>
    <author>
      <name>Rémi Dulong</name>
    </author>
    <author>
      <name>Rafael Pires</name>
    </author>
    <author>
      <name>Andreia Correia</name>
    </author>
    <author>
      <name>Valerio Schiavoni</name>
    </author>
    <author>
      <name>Pedro Ramalhete</name>
    </author>
    <author>
      <name>Pascal Felber</name>
    </author>
    <author>
      <name>Gaël Thomas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DSN48987.2021.00033</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DSN48987.2021.00033" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures, to be published in the 51th IEEE/IFIP
  International Conference on Dependable Systems and Networks (DSN 21)</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.10397v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.10397v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.2; D.4.3; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.13894v1</id>
    <updated>2021-05-28T14:57:49Z</updated>
    <published>2021-05-28T14:57:49Z</published>
    <title>Performance Evaluation of Snapshot Methods to Warm the Serverless Cold
  Start</title>
    <summary>  The serverless computing model strengthens the cloud computing tendency to
abstract resource management. Serverless platforms are responsible for
deploying and scaling the developer's applications. Serverless also
incorporated the pay-as-you-go billing model, which only considers the time
spent processing client requests. Such a decision created a natural incentive
for improving the platform's efficient resource usage. This search for
efficiency can lead to the cold start problem, which represents a delay to
execute serverless applications. Among the solutions proposed to deal with the
cold start, those based on the snapshot method stand out. Despite the rich
exploration of the technique, there is a lack of research that evaluates the
solution's trade-offs. In this direction, this work compares two solutions to
mitigate the cold start: Prebaking and SEUSS. We analyzed the solution's
performance with functions of different levels of complexity: NoOp, a function
that renders Markdown to HTML, and a function that loads 41 MB of dependencies.
Preliminary results indicated that Prebaking showed a 33% and 25% superior
performance to startup the NoOp and Markdown functions, respectively. Further
analysis also revealed that Prebaking's warmup mechanism reduced the Markdown
first request processing time by 69%.
</summary>
    <author>
      <name>Paulo Silva</name>
    </author>
    <author>
      <name>Thiago Emmanuel Pereira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Portuguese</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.13894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.13894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.03617v3</id>
    <updated>2021-08-12T22:35:55Z</updated>
    <published>2021-06-07T13:43:05Z</published>
    <title>PAIO: A Software-Defined Storage Data Plane Framework</title>
    <summary>  We propose PAIO, the first general-purpose framework that enables system
designers to build custom-made Software-Defined Storage (SDS) data plane
stages. It provides the means to implement storage optimizations adaptable to
different workflows and user-defined policies, and allows straightforward
integration with existing applications and I/O layers. PAIO allows stages to be
integrated with modern SDS control planes to ensure holistic control and
system-wide optimal performance. We demonstrate the performance and
applicability of PAIO with two use cases. The first improves 99th percentile
latency by 4x in industry-standard LSM-based key-value stores. The second
ensures dynamic per-application bandwidth guarantees under shared storage
environments.
</summary>
    <author>
      <name>Ricardo Macedo</name>
    </author>
    <author>
      <name>Yusuke Tanimura</name>
    </author>
    <author>
      <name>Jason Haga</name>
    </author>
    <author>
      <name>Vijay Chidambaram</name>
    </author>
    <author>
      <name>José Pereira</name>
    </author>
    <author>
      <name>João Paulo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.03617v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.03617v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.04514v1</id>
    <updated>2021-06-05T14:06:20Z</updated>
    <published>2021-06-05T14:06:20Z</published>
    <title>GearV: A Two-Gear Hypervisor for Mixed-Criticality IoT Systems</title>
    <summary>  This paper presents GearV, a two-gear lightweight hypervisor architecture to
address the some known challenges. By dividing hypervisor into some partitions,
and dividing scheduling policies into Gear1 and Gear2 respectively, GearV
creates a consolidated platform to run best-effort system and safety-critical
system simultaneously with managed engineering effort. The two-gears
architecture also simplifies retrofitting the virtualization systems. We
believe that GearV can serves as a reasonable hypervisor architecture for the
mix-critical IoT systems.
</summary>
    <author>
      <name>Kaiwen Long</name>
    </author>
    <author>
      <name>Chong Xing</name>
    </author>
    <author>
      <name>Yuebin Qi</name>
    </author>
    <author>
      <name>Pei Zhang</name>
    </author>
    <author>
      <name>Changsong Wu</name>
    </author>
    <author>
      <name>Wenxiao Fang</name>
    </author>
    <author>
      <name>Jing Tan</name>
    </author>
    <author>
      <name>Jie Chen</name>
    </author>
    <author>
      <name>Shiming Zhang</name>
    </author>
    <author>
      <name>Zuosheng Wang</name>
    </author>
    <author>
      <name>Zuanmin Liu</name>
    </author>
    <author>
      <name>Cao Liang</name>
    </author>
    <author>
      <name>Jiaxiang Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures, 11 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.04514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.04514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.06065v1</id>
    <updated>2021-06-10T21:56:49Z</updated>
    <published>2021-06-10T21:56:49Z</published>
    <title>Windows Kernel Hijacking Is Not an Option: MemoryRanger Comes to the
  Rescue Again</title>
    <summary>  The security of a computer system depends on OS kernel protection. It is
crucial to reveal and inspect new attacks on kernel data, as these are used by
hackers. The purpose of this paper is to continue research into attacks on
dynamically allocated data in the Windows OS kernel and demonstrate the
capacity of MemoryRanger to prevent these attacks. This paper discusses three
new hijacking attacks on kernel data, which are based on bypassing OS security
mechanisms. The first two hijacking attacks result in illegal access to files
open in exclusive access. The third attack escalates process privileges,
without applying token swapping. Although Windows security experts have issued
new protection features, access attempts to the dynamically allocated data in
the kernel are not fully controlled. MemoryRanger hypervisor is designed to
fill this security gap. The updated MemoryRanger prevents these new attacks as
well as supporting the Windows 10 1903 x64.
</summary>
    <author>
      <name>Igor Korkin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 7 figures. Korkin, I. (2021, June 10). Windows Kernel
  Hijacking Is Not an Option: MemoryRanger Comes to the Rescue Again. Journal
  of Digital Forensics, Security and Law, Vol 16, No.1, Article 4. Available
  at: https://commons.erau.edu/jdfsl/vol16/iss1/4</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.06065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.06065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.04605v1</id>
    <updated>2021-09-10T01:05:52Z</updated>
    <published>2021-09-10T01:05:52Z</published>
    <title>Analytical Process Scheduling Optimization for Heterogeneous Multi-core
  Systems</title>
    <summary>  In this paper, we propose the first optimum process scheduling algorithm for
an increasingly prevalent type of heterogeneous multicore (HEMC) system that
combines high-performance big cores and energy-efficient small cores with the
same instruction-set architecture (ISA). Existing algorithms are all
heuristics-based, and the well-known IPC-driven approach essentially tries to
schedule high scaling factor processes on big cores. Our analysis shows that,
for optimum solutions, it is also critical to consider placing long running
processes on big cores. Tests of SPEC 2006 cases on various big-small core
combinations show that our proposed optimum approach is up to 34% faster than
the IPC-driven heuristic approach in terms of total workload completion time.
The complexity of our algorithm is O(NlogN) where N is the number of processes.
Therefore, the proposed optimum algorithm is practical for use.
</summary>
    <author>
      <name>Chien-Hao Chen</name>
    </author>
    <author>
      <name>Ren-Song Tsay</name>
    </author>
    <link href="http://arxiv.org/abs/2109.04605v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.04605v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.08685v1</id>
    <updated>2021-10-17T00:25:21Z</updated>
    <published>2021-10-17T00:25:21Z</published>
    <title>A Learning-based Approach Towards Automated Tuning of SSD Configurations</title>
    <summary>  Thanks to the mature manufacturing techniques, solid-state drives (SSDs) are
highly customizable for applications today, which brings opportunities to
further improve their storage performance and resource utilization. However,
the SSD efficiency is usually determined by many hardware parameters, making it
hard for developers to manually tune them and determine the optimal SSD
configurations.
  In this paper, we present an automated learning-based framework, named
LearnedSSD, that utilizes both supervised and unsupervised machine learning
(ML) techniques to drive the tuning of hardware configurations for SSDs.
LearnedSSD automatically extracts the unique access patterns of a new workload
using its block I/O traces, maps the workload to previously workloads for
utilizing the learned experiences, and recommends an optimal SSD configuration
based on the validated storage performance. LearnedSSD accelerates the
development of new SSD devices by automating the hard-ware parameter
configurations and reducing the manual efforts. We develop LearnedSSD with
simple yet effective learning algorithms that can run efficiently on multi-core
CPUs. Given a target storage workload, our evaluation shows that LearnedSSD can
always deliver an optimal SSD configuration for the target workload, and this
configuration will not hurt the performance of non-target workloads.
</summary>
    <author>
      <name>Daixuan Li</name>
    </author>
    <author>
      <name>Jian Huang</name>
    </author>
    <link href="http://arxiv.org/abs/2110.08685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.08685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.11602v1</id>
    <updated>2021-10-22T05:36:52Z</updated>
    <published>2021-10-22T05:36:52Z</published>
    <title>An O(1) algorithm for implementing the LFU cache eviction scheme</title>
    <summary>  Cache eviction algorithms are used widely in operating systems, databases and
other systems that use caches to speed up execution by caching data that is
used by the application. There are many policies such as MRU (Most Recently
Used), MFU (Most Frequently Used), LRU (Least Recently Used) and LFU (Least
Frequently Used) which each have their advantages and drawbacks and are hence
used in specific scenarios. By far, the most widely used algorithm is LRU, both
for its $O(1)$ speed of operation as well as its close resemblance to the kind
of behaviour that is expected by most applications. The LFU algorithm also has
behaviour desirable by many real world workloads. However, in many places, the
LRU algorithm is is preferred over the LFU algorithm because of its lower run
time complexity of $O(1)$ versus $O(\log n)$. We present here an LFU cache
eviction algorithm that has a runtime complexity of $O(1)$ for all of its
operations, which include insertion, access and deletion(eviction).
</summary>
    <author>
      <name>Dhruv Matani</name>
    </author>
    <author>
      <name>Ketan Shah</name>
    </author>
    <author>
      <name>Anirban Mitra</name>
    </author>
    <link href="http://arxiv.org/abs/2110.11602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.11602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.02821v1</id>
    <updated>2021-11-03T16:34:07Z</updated>
    <published>2021-11-03T16:34:07Z</published>
    <title>VOSySmonitoRV: a mixed-criticality solution on Linux-capable RISC-V
  platforms</title>
    <summary>  Embedded systems are pervasively used in many fields nowadays. In
mixed-criticality environments (automotive, industry 4.0, drones, etc.) they
need to run real-time applications with certain time and safety constraints
alongside a rich operating system (OS). This is usually possible thanks to
virtualization techniques, that leverage on hardware virtualization extensions
on the machine. However, these hardware extensions might not cope with the
security and safety requirements of the specific use case, and additionally,
they might not always be available. A notable example is the emerging RISC-V
architecture, that is today gaining a lot of traction in the mixed criticality
field, but that do not offer today hardware virtualization extensions. In this
paper VOSySmonitoRV is proposed as a mixed-criticality solution for RISC-V
systems. VOSySmonitoRVallows the co-execution of two or more operating systems
in a secure and isolated manner by running in the highest privileged machine
level. A specific benchmark, measuring the interrupt latency and context switch
time is done to assess the system performance in mixed criticality systems.
</summary>
    <author>
      <name>Flavia Caforio</name>
    </author>
    <author>
      <name>Pierpaolo Iannicelli</name>
    </author>
    <author>
      <name>Michele Paolino</name>
    </author>
    <author>
      <name>Daniel Raho</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MECO52532.2021.9460246</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MECO52532.2021.9460246" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Also available at
  http://www.virtualopensystems.com/en/research/scientific-contributions/vosysmonitorv-risc-v-meco2021/</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.02821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.02821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.03065v1</id>
    <updated>2021-11-04T19:39:11Z</updated>
    <published>2021-11-04T19:39:11Z</published>
    <title>Safe and Practical GPU Acceleration in TrustZone</title>
    <summary>  We present a holistic design for GPU-accelerated computation in TrustZone
TEE. Without pulling the complex GPU software stack into the TEE, we follow a
simple approach: record the CPU/GPU interactions ahead of time, and replay the
interactions in the TEE at run time. This paper addresses the approach's key
missing piece -- the recording environment, which needs both strong security
and access to diverse mobile GPUs. To this end, we present a novel architecture
called CODY, in which a mobile device (which possesses the GPU hardware) and a
trustworthy cloud service (which runs the GPU software) exercise the GPU
hardware/software in a collaborative, distributed fashion. To overcome numerous
network round trips and long delays, CODY contributes optimizations specific to
mobile GPUs: register access deferral, speculation, and metastate-only
synchronization. With these optimizations, recording a compute workload takes
only tens of seconds, which is up to 95% less than a naive approach; replay
incurs 25% lower delays compared to insecure, native execution.
</summary>
    <author>
      <name>Heejin Park</name>
    </author>
    <author>
      <name>Felix Xiaozhu Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2111.03065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.03065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.06875v1</id>
    <updated>2021-12-13T18:32:53Z</updated>
    <published>2021-12-13T18:32:53Z</published>
    <title>Virtualizing Mixed-Criticality Systems: A Survey on Industrial Trends
  and Issues</title>
    <summary>  Virtualization is gaining attraction in the industry as it promises a
flexible way to integrate, manage, and re-use heterogeneous software components
with mixed-criticality levels, on a shared hardware platform, while obtaining
isolation guarantees. This work surveys the state-of-the-practice of real-time
virtualization technologies by discussing common issues in the industry. In
particular, we analyze how different virtualization approaches and solutions
can impact isolation guarantees and testing/certification activities, and how
they deal with dependability challenges. The aim is to highlight current
industry trends and support industrial practitioners to choose the most
suitable solution according to their application domains.
</summary>
    <author>
      <name>Marcello Cinque</name>
    </author>
    <author>
      <name>Domenico Cotroneo</name>
    </author>
    <author>
      <name>Luigi De Simone</name>
    </author>
    <author>
      <name>Stefano Rosiello</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.future.2021.12.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.future.2021.12.002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Elsevier Future Generation Computer
  Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.06875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.06875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.07213v1</id>
    <updated>2021-12-14T07:49:32Z</updated>
    <published>2021-12-14T07:49:32Z</published>
    <title>In-Kernel Control-Flow Integrity on Commodity OSes using ARM Pointer
  Authentication</title>
    <summary>  This paper presents an in-kernel, hardware-based control-flow integrity (CFI)
protection, called PAL, that utilizes ARM's Pointer Authentication (PA). It
provides three important benefits over commercial, state-of-the-art PA-based
CFIs like iOS's: 1) enhancing CFI precision via automated refinement
techniques, 2) addressing hindsight problems of PA for in kernel uses such as
preemptive hijacking and brute-forcing attacks, and 3) assuring the algorithmic
or implementation correctness via post validation. PAL achieves these goals in
an OS-agnostic manner, so could be applied to commodity OSes like Linux and
FreeBSD. The precision of the CFI protection can be adjusted for better
performance or improved for better security with minimal engineering efforts if
a user opts in to. Our evaluation shows that PAL incurs negligible performance
overhead: e.g., &lt;1% overhead for Apache benchmark and 3~5% overhead for Linux
perf benchmark on the latest Mac mini (M1). Our post-validation approach helps
us ensure the security invariant required for the safe uses of PA inside the
kernel, which also reveals new attack vectors on the iOS kernel. PAL as well as
the CFI-protected kernels will be open sourced.
</summary>
    <author>
      <name>Sungbae Yoo</name>
    </author>
    <author>
      <name>Jinbum Park</name>
    </author>
    <author>
      <name>Seolheui Kim</name>
    </author>
    <author>
      <name>Yeji Kim</name>
    </author>
    <author>
      <name>Taesoo Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2112.07213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.07213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.04803v1</id>
    <updated>2022-03-09T15:32:40Z</updated>
    <published>2022-03-09T15:32:40Z</published>
    <title>Limited Associativity Caching in the Data Plane</title>
    <summary>  In-network caching promises to improve the performance of networked and edge
applications as it shortens the paths data need to travel. This is by storing
so-called hot items in the network switches on-route between clients who access
the data and the storage servers who maintain it. Since the data flows through
those switches in any case, it is natural to cache hot items there.
  Most software-managed caches treat the cache as a fully associative region.
Alas, a fully associative design seems to be at odds with programmable
switches' goal of handling packets in a short bounded amount of time, as well
as their restricted programming model. In this work, we present PKache, a
generic limited associativity cache implementation in the programmable
switches' domain-specific P4 language, and demonstrate its utility by realizing
multiple popular cache management schemes.
</summary>
    <author>
      <name>Roy Friedman</name>
    </author>
    <author>
      <name>Or Goaz</name>
    </author>
    <author>
      <name>Dor Hovav</name>
    </author>
    <link href="http://arxiv.org/abs/2203.04803v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.04803v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.09857v1</id>
    <updated>2022-03-18T10:54:33Z</updated>
    <published>2022-03-18T10:54:33Z</published>
    <title>A Framework for Formal Specification and Verification of Security
  Properties of the Android Permissions System</title>
    <summary>  Android is a widely deployed operating system that employs a permission-based
access control model. The Android Permissions System (APS) is responsible for
mediating resource requests from applications. APS is a critical component of
the Android security mechanism. A failure in the design of APS can potentially
lead to vulnerabilities that grant unauthorized access to resources by
malicious applications. Researchers have employed formal methods for analyzing
the security properties of APS. Since Android is constantly evolving, we intend
to design and implement a framework for formal specification and verification
of the security properties of APS. In particular, we intend to present a
behavioral model of APS that represents the non-binary, context dependent
permissions introduced in Android 10 and temporal permissions introduced in
Android 11.
</summary>
    <author>
      <name>Amirhosein Sayyadabdi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at the EuroSys Doctoral Workshop 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.09857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.09857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.10359v3</id>
    <updated>2022-08-21T21:00:41Z</updated>
    <published>2022-03-19T17:21:01Z</published>
    <title>FPGA-extended General Purpose Computer Architecture</title>
    <summary>  This paper introduces a computer architecture, where part of the instruction
set architecture (ISA) is implemented on small highly-integrated
field-programmable gate arrays (FPGAs). Small FPGAs inside a general-purpose
processor (CPU) can be used effectively to implement custom or standardised
instructions. Our proposed architecture directly address related challenges for
high-end CPUs, where such highly-integrated FPGAs would have the highest
impact, such as on main memory bandwidth. This also enables
software-transparent context-switching. The simulation-based evaluation of a
dynamically reconfigurable core shows promising results approaching the
performance of an equivalent core with all enabled instructions. Finally, the
feasibility of adopting the proposed architecture in today's CPUs is studied
through the prototyping of fast-reconfigurable FPGAs and studying the miss
behaviour of opcodes.
</summary>
    <author>
      <name>Philippos Papaphilippou</name>
    </author>
    <author>
      <name>Myrtle Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the 18th International Symposium on Applied
  Reconfigurable Computing (ARC) 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.10359v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.10359v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.15324v1</id>
    <updated>2022-03-29T08:10:06Z</updated>
    <published>2022-03-29T08:10:06Z</published>
    <title>syslrn: Learning What to Monitor for Efficient Anomaly Detection</title>
    <summary>  While monitoring system behavior to detect anomalies and failures is
important, existing methods based on log-analysis can only be as good as the
information contained in the logs, and other approaches that look at the
OS-level software state introduce high overheads. We tackle the problem with
syslrn, a system that first builds an understanding of a target system offline,
and then tailors the online monitoring instrumentation based on the learned
identifiers of normal behavior. While our syslrn prototype is still preliminary
and lacks many features, we show in a case study for the monitoring of
OpenStack failures that it can outperform state-of-the-art log-analysis systems
with little overhead.
</summary>
    <author>
      <name>Davide Sanvito</name>
    </author>
    <author>
      <name>Giuseppe Siracusano</name>
    </author>
    <author>
      <name>Sharan Santhanam</name>
    </author>
    <author>
      <name>Roberto Gonzalez</name>
    </author>
    <author>
      <name>Roberto Bifulco</name>
    </author>
    <link href="http://arxiv.org/abs/2203.15324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.15324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.08846v1</id>
    <updated>2022-04-19T12:23:21Z</updated>
    <published>2022-04-19T12:23:21Z</published>
    <title>Differentiating Network Flows for Priority-Aware Scheduling of Incoming
  Packets in Real-Time IoT Systems</title>
    <summary>  When IP-packet processing is unconditionally carried out on behalf of an
operating system kernel thread, processing systems can experience overload in
high incoming traffic scenarios. This is especially worrying for embedded
real-time devices controlling their physical environment in industrial IoT
scenarios and automotive systems. We propose an embedded real-time aware IP
stack adaption with an early demultiplexing scheme for incoming packets and
subsequent per-flow aperiodic scheduling. By instrumenting existing embedded IP
stacks, rigid prioritization with minimal latency is deployed without the need
of further task resources. Simple mitigation techniques can be applied to
individual flows, causing hardly measurable overhead while at the same time
protecting the system from overload conditions. Our IP stack adaption is able
to reduce the low-priority packet processing time by over 86% compared to an
unmodified stack. The network subsystem can thereby remain active at a 7x
higher general traffic load before disabling the receive IRQ as a last resort
to assure deadlines.
</summary>
    <author>
      <name>Christoph Blumschein</name>
    </author>
    <author>
      <name>Ilja Behnke</name>
    </author>
    <author>
      <name>Lauritz Thamsen</name>
    </author>
    <author>
      <name>Odej Kao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25th International Symposium on Real-Time Distributed Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.08846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.08846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.3338v1</id>
    <updated>2008-03-23T19:10:00Z</updated>
    <published>2008-03-23T19:10:00Z</published>
    <title>Performance Evaluation of Multiple TCP connections in iSCSI</title>
    <summary>  Scaling data storage is a significant concern in enterprise systems and
Storage Area Networks (SANs) are deployed as a means to scale enterprise
storage. SANs based on Fibre Channel have been used extensively in the last
decade while iSCSI is fast becoming a serious contender due to its reduced
costs and unified infrastructure. This work examines the performance of iSCSI
with multiple TCP connections. Multiple TCP connections are often used to
realize higher bandwidth but there may be no fairness in how bandwidth is
distributed. We propose a mechanism to share congestion information across
multiple flows in ``Fair-TCP'' for improved performance. Our results show that
Fair-TCP significantly improves the performance for I/O intensive workloads.
</summary>
    <author>
      <name>Bhargava Kumar K</name>
    </author>
    <author>
      <name>Ganesh M. Narayan</name>
    </author>
    <author>
      <name>K. Gopinath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10pt, 11 pages, two column, 15 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 24th IEEE Conference on Mass Storage Systems
  and Technologies, 2007 - MSST '07</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.3338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.3338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.3.2; D.4.2; H.3.4; C.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.2882v1</id>
    <updated>2012-10-10T12:13:30Z</updated>
    <published>2012-10-10T12:13:30Z</published>
    <title>Online Adaptive Fault Tolerant based Feedback Control Scheduling
  Algorithm for Multiprocessor Embedded Systems</title>
    <summary>  Since some years ago, use of Feedback Control Scheduling Algorithm (FCSA) in
the control scheduling co-design of multiprocessor embedded system has
increased. FCSA provides Quality of Service (QoS) in terms of overall system
performance and resource allocation in open and unpredictable environment. FCSA
uses quality control feedback loop to keep CPU utilization under desired
unitization bound by avoiding overloading and deadline miss ratio. Integrated
Fault tolerance (FT) based FCSA design methodology guarantees that the Safety
Critical (SC) tasks will meet their deadlines in the presence of faults.
However, current FCSA design model does not provide the optimal solution with
dynamic load fluctuation. This paper presented a novel methodology of designing
an online adaptive fault tolerant based feedback control algorithm for
multiprocessor embedded systems. This procedure is important for control
scheduling co-design for multiprocessor embedded systems.
</summary>
    <author>
      <name>Oumair Naseer</name>
    </author>
    <author>
      <name>Rana Atif Ali Khan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.2882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.2882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.02294v2</id>
    <updated>2017-11-08T19:08:26Z</updated>
    <published>2017-11-07T05:41:15Z</published>
    <title>AppSwitch: Resolving the Application Identity Crisis</title>
    <summary>  Networked applications traditionally derive their identity from the identity
of the host on which they run. The default application identity acquired from
the host results in subtle and substantial problems related to application
deployment, discovery and access, especially for modern distributed
applications. A number of mechanisms and workarounds, often quite elaborate,
are used to address those problems but they only address them indirectly and
incompletely.
  This paper presents AppSwitch, a novel transport layer network element that
decouples applications from underlying network at the system call layer and
enables them to be identified independently of the network. Without requiring
changes to existing applications or infrastructure, it removes the cost and
complexity associated with operating distributed applications while offering a
number of benefits including an efficient implementation of common network
functions such as application firewall and load balancer. Experiments with our
implementation show that AppSwitch model also effectively removes the
performance penalty associated with unnecessary data path processing that is
typical in those application environments.
</summary>
    <author>
      <name>Dinesh Subhraveti</name>
    </author>
    <author>
      <name>Sri Goli</name>
    </author>
    <author>
      <name>Serge Hallyn</name>
    </author>
    <author>
      <name>Ravi Chamarthy</name>
    </author>
    <author>
      <name>Christos Kozyrakis</name>
    </author>
    <link href="http://arxiv.org/abs/1711.02294v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02294v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04808v1</id>
    <updated>2017-11-13T19:30:22Z</updated>
    <published>2017-11-13T19:30:22Z</published>
    <title>A Design-Space Exploration for Allocating Security Tasks in Multicore
  Real-Time Systems</title>
    <summary>  The increased capabilities of modern real-time systems (RTS) expose them to
various security threats. Recently, frameworks that integrate security tasks
without perturbing the real-time tasks have been proposed, but they only target
single core systems. However, modern RTS are migrating towards multicore
platforms. This makes the problem of integrating security mechanisms more
complex, as designers now have multiple choices for where to allocate the
security tasks. In this paper we propose HYDRA, a design space exploration
algorithm that finds an allocation of security tasks for multicore RTS using
the concept of opportunistic execution. HYDRA allows security tasks to operate
with existing real-time tasks without perturbing system parameters or normal
execution patterns, while still meeting the desired monitoring frequency for
intrusion detection. Our evaluation uses a representative real-time control
system (along with synthetic task sets for a broader exploration) to illustrate
the efficacy of HYDRA.
</summary>
    <author>
      <name>Monowar Hasan</name>
    </author>
    <author>
      <name>Sibin Mohan</name>
    </author>
    <author>
      <name>Rodolfo Pellizzoni</name>
    </author>
    <author>
      <name>Rakesh B. Bobba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication, 21st DATE (Design, Automation &amp; Test in
  Europe) conference, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.04808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07278v1</id>
    <updated>2017-11-20T12:17:23Z</updated>
    <published>2017-11-20T12:17:23Z</published>
    <title>Software Distribution Transparency and Auditability</title>
    <summary>  A large user base relies on software updates provided through package
managers. This provides a unique lever for improving the security of the
software update process. We propose a transparency system for software updates
and implement it for a widely deployed Linux package manager, namely APT. Our
system is capable of detecting targeted backdoors without producing overhead
for maintainers. In addition, in our system, the availability of source code is
ensured, the binding between source and binary code is verified using
reproducible builds, and the maintainer responsible for distributing a specific
package can be identified. We describe a novel "hidden version" attack against
current software transparency systems and propose as well as integrate a
suitable defense. To address equivocation attacks by the transparency log
server, we introduce tree root cross logging, where the log's Merkle tree root
is submitted into a separately operated log server. This significantly relaxes
the inter-operator cooperation requirements compared to other systems. Our
implementation is evaluated by replaying over 3000 updates of the Debian
operating system over the course of two years, demonstrating its viability and
identifying numerous irregularities.
</summary>
    <author>
      <name>Benjamin Hof</name>
    </author>
    <author>
      <name>Georg Carle</name>
    </author>
    <link href="http://arxiv.org/abs/1711.07278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00121v1</id>
    <updated>2018-09-01T06:07:39Z</updated>
    <published>2018-09-01T06:07:39Z</published>
    <title>A Review of Software-Defined WLANs: Architectures and Central Control
  Mechanisms</title>
    <summary>  The significant growth in the number of WiFi-enabled devices as well as the
increase in the traffic conveyed through wireless local area networks (WLANs)
necessitate the adoption of new network control mechanisms. Specifically, dense
deployment of access points, client mobility, and emerging QoS demands bring
about challenges that cannot be effectively addressed by distributed
mechanisms. Recent studies show that software-defined WLANs (SDWLANs) simplify
network control, improve QoS provisioning, and lower the deployment cost of new
network control mechanisms. In this paper, we present an overview of SDWLAN
architectures and provide a qualitative comparison in terms of features such as
programmability and virtualization. In addition, we classify and investigate
the two important classes of centralized network control mechanisms: (i)
association control (AsC) and (ii) channel assignment (ChA). We study the basic
ideas employed by these mechanisms, and in particular, we focus on the metrics
utilized and the problem formulation techniques proposed. We present a
comparison of these mechanisms and identify open research problems.
</summary>
    <author>
      <name>Behnam Dezfouli</name>
    </author>
    <author>
      <name>Vahid Esmaeelzadeh</name>
    </author>
    <author>
      <name>Jaykumar Sheth</name>
    </author>
    <author>
      <name>Marjan Radi</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.09822v5</id>
    <updated>2019-08-29T04:53:09Z</updated>
    <published>2018-12-24T03:55:32Z</published>
    <title>MI6: Secure Enclaves in a Speculative Out-of-Order Processor</title>
    <summary>  Recent attacks have broken process isolation by exploiting microarchitectural
side channels that allow indirect access to shared microarchitectural state.
Enclaves strengthen the process abstraction to restore isolation guarantees.
  We propose MI6, an aggressive, speculative out-of-order processor capable of
providing secure enclaves under a threat model that includes an untrusted OS
and an attacker capable of mounting any software attack currently considered
practical, including control flow speculation attacks. MI6 is inspired by
Sanctum [16] and extends its isolation guarantee to more realistic memory
hierarchies. It also introduces a purge instruction, which is used only when a
secure process is scheduled, and implements it for a complex processor
microarchitecture. We model the performance impact of enclaves in MI6 through
FPGA emulation on AWS F1 FPGAs by running SPEC CINT2006 benchmarks on top of an
untrusted Linux OS. Security comes at the cost of approximately 16.4% average
slowdown for protected programs.
</summary>
    <author>
      <name>Thomas Bourgeat</name>
    </author>
    <author>
      <name>Ilia Lebedev</name>
    </author>
    <author>
      <name>Andrew Wright</name>
    </author>
    <author>
      <name>Sizhuo Zhang</name>
    </author>
    <author>
      <name> Arvind</name>
    </author>
    <author>
      <name>Srinivas Devadas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.09822v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.09822v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.09920v1</id>
    <updated>2018-12-24T13:30:40Z</updated>
    <published>2018-12-24T13:30:40Z</published>
    <title>Divide et Impera: MemoryRanger Runs Drivers in Isolated Kernel Spaces</title>
    <summary>  One of the main issues in the OS security is to provide trusted code
execution in an untrusted environment. During executing, kernel-mode drivers
allocate and process memory data: OS internal structures, users private
information, and sensitive data of third-party drivers. All this data and the
drivers code can be tampered with by kernel-mode malware. Microsoft security
experts integrated new features to fill this gap, but they are not enough:
allocated data can be stolen and patched and the drivers code can be dumped
without any security reaction. The proposed hypervisor-based system
(MemoryRanger) tackles this issue by executing drivers in separate kernel
enclaves with specific memory attributes. MemoryRanger protects code and data
using Intel VT-x and EPT features with low performance degradation on Windows
10 x64.
</summary>
    <author>
      <name>Igor Korkin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Korkin, I. (2018, December 5-6). Divide et Impera: MemoryRanger Runs
  Drivers in Isolated Kernel Spaces. In Proceedings of the BlackHat Europe
  Conference, London, UK. 23 pages, 4 figures, 2 tables, 49 references.
  Retrieved from
  https://www.blackhat.com/eu-18/briefings/schedule/#divide-et-impera-memoryranger-runs-drivers-in-isolated-kernel-spaces-12668</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.09920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.09920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.10239v1</id>
    <updated>2019-06-24T21:23:18Z</updated>
    <published>2019-06-24T21:23:18Z</published>
    <title>Container Density Improvements with Dynamic Memory Extension using NAND
  Flash</title>
    <summary>  While containers efficiently implement the idea of operating-system-level
application virtualization, they are often insufficient to increase the server
utilization to a desirable level. The reason is that in practice many
containerized applications experience a limited amount of load while there are
few containers with a high load. In such a scenario, the virtual memory
management system can become the limiting factor to container density even
though the working set of active containers would fit into main memory. In this
paper, we describe and evaluate a system for transparently moving memory pages
in and out of DRAM and to a NAND Flash medium which is attached through the
memory bus. This technique, called Diablo Memory Expansion (DMX), operates on a
prediction model and is able to relieve the pressure on the memory system. We
present a benchmark for container density and show that even under an overall
constant workload, adding additional containers adversely affects
performance-critical applications in Docker. When using the DMX technology of
the Memory1 system, however, the performance of the critical workload remains
stable.
</summary>
    <author>
      <name>Jan S. Rellermeyer</name>
    </author>
    <author>
      <name>Maher Amer</name>
    </author>
    <author>
      <name>Richard Smutzer</name>
    </author>
    <author>
      <name>Karthick Rajamani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3265723.3265740</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3265723.3265740" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">APSys 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.10239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.10860v2</id>
    <updated>2019-07-15T09:50:24Z</updated>
    <published>2019-06-26T06:16:13Z</published>
    <title>Lawn: an Unbound Low Latency Timer Data Structure for Large Scale, High
  Throughput Systems</title>
    <summary>  As demand for Real-Time applications rises among the general public, the
importance of enabling large-scale, unbound algorithms to solve conventional
problems with low to no latency is critical for product viability. Timer
algorithms are prevalent in the core mechanisms behind operating systems,
network protocol implementation, stream processing, and several database
capabilities. This paper presents a field-tested algorithm for low latency,
unbound range timer structure, based upon the well excepted Timing Wheel
algorithm. Using a set of queues hashed by TTL, the algorithm allows for a
simpler implementation, minimal overhead no overflow and no performance
degradation in comparison to the current state of the algorithms under typical
use cases.
</summary>
    <author>
      <name>Adam Lev-Libfeld</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.10860v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10860v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.01442v1</id>
    <updated>2020-01-06T09:04:59Z</updated>
    <published>2020-01-06T09:04:59Z</published>
    <title>Runtime Verification of Linux Kernel Security Module</title>
    <summary>  The Linux kernel is one of the most important Free/Libre Open Source Software
(FLOSS) projects. It is installed on billions of devices all over the world,
which process various sensitive, confidential or simply private data. It is
crucial to establish and prove its security properties. This work-in-progress
paper presents a method to verify the Linux kernel for conformance with an
abstract security policy model written in the Event-B specification language.
The method is based on system call tracing and aims at checking that the
results of system call execution do not lead to accesses that violate security
policy requirements. As a basis for it, we use an additional Event-B
specification of the Linux system call interface that is formally proved to
satisfy all the requirements of the security policy model. In order to perform
the conformance checks we use it to reproduce intercepted system calls and
verify accesses.
</summary>
    <author>
      <name>Denis Efremov</name>
    </author>
    <author>
      <name>Ilya Shchepetkov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures, 3 listings, OpenCERT 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.01442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.06159v1</id>
    <updated>2020-01-17T05:11:55Z</updated>
    <published>2020-01-17T05:11:55Z</published>
    <title>A New Fairness Model based on User's Objective for Multi-user
  Multi-processor Online Scheduling</title>
    <summary>  Resources of a multi-user system in multi-processor online scheduling are
shared by competing users in which fairness is a major performance criterion
for resource allocation. Fairness ensures equality in resource sharing among
the users. According to our knowledge, fairness based on the user's objective
has neither been comprehensively studied nor a formal fairness model has been
well defined in the literature. This motivates us to explore and define a new
model to ensure algorithmic fairness with quantitative performance measures
based on optimization of the user's objective. In this paper, we propose a new
model for fairness in Multi-user Multi-processor Online Scheduling
Problem(MUMPOSP). We introduce and formally define quantitative fairness
measures based on user's objective by optimizing makespan for individual user
in our proposed fairness model. We also define the unfairness of deprived users
and absolute fairness of an algorithm. We obtain lower bound results for the
absolute fairness for m identical machines with equal length jobs. We show that
our proposed fairness model can serve as a framework for measuring algorithmic
fairness by considering various optimality criteria such as flow time and sum
of completion times.
</summary>
    <author>
      <name>Debasis Dwibedy</name>
    </author>
    <author>
      <name>Rakesh Mohanty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.06159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.06159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.06152v1</id>
    <updated>2020-08-14T01:18:04Z</updated>
    <published>2020-08-14T01:18:04Z</published>
    <title>Consideration for effectively handling parallel workloads on public
  cloud system</title>
    <summary>  We retrieved and analyzed parallel storage workloads of the FUJITSU K5 cloud
service to clarify how to build cost-effective hybrid storage systems. A hybrid
storage system consists of fast but low-capacity tier (first tier) and slow but
high-capacity tier (second tier). And, it typically consists of either SSDs and
HDDs or NVMs and SSDs. As a result, we found that 1) regions for first tier
should be assigned only if a workload includes large number of IO accesses for
a whole day, 2) the regions that include a large number of IO accesses should
be dynamically chosen and moved from second tier to first tier for a short
interval, and 3) if a cache hit ratio is regularly low, use of the cache for
the workload should be cancelled, and the whole workload region should be
assigned to the region for first tier. These workloads already have been
released from the SNIA web site.
</summary>
    <author>
      <name>Kazuichi Oe</name>
    </author>
    <link href="http://arxiv.org/abs/2008.06152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.06152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.11868v3</id>
    <updated>2021-05-09T19:21:04Z</updated>
    <published>2020-08-27T00:20:00Z</published>
    <title>BumbleBee: Application-aware adaptation for container orchestration</title>
    <summary>  Modern applications have embraced separation of concerns as a first-order
organizing principle through the use of containers, container orchestration,
and service meshes. However, adaptation to unexpected network variation has not
followed suit. We present BumbleBee, a lightweight extension to the container
ecosystem that supports application-aware adaptation. BumbleBee provides a
simple abstraction for making decisions about network data using application
semantics. Because this abstraction is placed within the communications
framework of a modern service mesh, it is closer to the point at which changes
are detected, providing more responsive and effective adaptation than possible
at endpoints.
</summary>
    <author>
      <name>HyunJong Lee</name>
    </author>
    <author>
      <name>Shadi Noghabi</name>
    </author>
    <author>
      <name>Brian Noble</name>
    </author>
    <author>
      <name>Matthew Furlong</name>
    </author>
    <author>
      <name>Landon P. Cox</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This version fixes problems (e.g., with the video-streaming
  experiments) from the previous versions</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.11868v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11868v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.01869v3</id>
    <updated>2020-11-23T14:19:30Z</updated>
    <published>2020-09-03T18:30:02Z</published>
    <title>Enclave-Aware Compartmentalization and Secure Sharing with Sirius</title>
    <summary>  Hardware-assisted trusted execution environments (TEEs) are critical building
blocks of many modern applications. However, they have a one-way isolation
model that introduces a semantic gap between a TEE and its outside world. This
lack of information causes an ever-increasing set of attacks on TEE-enabled
applications that exploit various insecure interactions with the host OSs,
applications, or other enclaves. We introduce Sirius, the first
compartmentalization framework that achieves strong isolation and secure
sharing in TEE-assisted applications by controlling the dataflows within
primary kernel objects (e.g. threads, processes, address spaces, files,
sockets, pipes) in both the secure and normal worlds. Sirius replaces ad-hoc
interactions in current TEE systems with a principled approach that adds strong
inter- and intra-address space isolation and effectively eliminates a wide
range of attacks. We evaluate Sirius on ARM platforms and find that it is
lightweight ($\approx 15K$ LoC) and only adds $\approx 10.8\%$ overhead to
enable TEE support on applications such as httpd, and improves the performance
of existing TEE-enabled applications such as the Darknet ML framework and ARM's
LibDDSSec by $0.05\%-5.6\%$.
</summary>
    <author>
      <name>Zahra Tarkhani</name>
    </author>
    <author>
      <name>Anil Madhavapeddy</name>
    </author>
    <link href="http://arxiv.org/abs/2009.01869v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.01869v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.09845v1</id>
    <updated>2020-09-16T08:16:27Z</updated>
    <published>2020-09-16T08:16:27Z</published>
    <title>A FaaS File System for Serverless Computing</title>
    <summary>  Serverless computing with cloud functions is quickly gaining adoption, but
constrains programmers with its limited support for state management. We
introduce a shared file system for cloud functions. It offers familiar POSIX
semantics while taking advantage of distinctive aspects of cloud functions to
achieve scalability and performance beyond what traditional shared file systems
can offer. We take advantage of the function-grained fault tolerance model of
cloud functions to proceed optimistically using local state, safe in the
knowledge that we can restart if cache reads or lock activity cannot be
reconciled upon commit. The boundaries of cloud functions provide implicit
commit and rollback points, giving us the flexibility to use transaction
processing techniques without changing the programming model or API. This
allows a variety of stateful sever-based applications to benefit from the
simplicity and scalability of serverless computing, often with little or no
modification.
</summary>
    <author>
      <name>Johann Schleier-Smith</name>
    </author>
    <author>
      <name>Leonhard Holz</name>
    </author>
    <author>
      <name>Nathan Pemberton</name>
    </author>
    <author>
      <name>Joseph M. Hellerstein</name>
    </author>
    <link href="http://arxiv.org/abs/2009.09845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.15572v2</id>
    <updated>2020-12-10T09:37:11Z</updated>
    <published>2020-10-29T13:44:58Z</published>
    <title>Experimental Analysis of Communication Relaying Delay in Low-Energy
  Ad-hoc Networks</title>
    <summary>  In recent years, more and more applications use ad-hoc networks for local M2M
communications, but in some cases such as when using WSNs, the software
processing delay induced by packets relaying may not be negligible. In this
paper, we planned and carried out a delay measurement experiment using
Raspberry Pi Zero W. The results demonstrated that, in low-energy ad-hoc
networks, processing delay of the application is always too large to ignore; it
is at least ten times greater than the kernel routing and corresponds to 30% of
the transmission delay. Furthermore, if the task is CPU-intensive, such as
packet encryption, the processing delay can be greater than the transmission
delay and its behavior is represented by a simple linear model. Our findings
indicate that the key factor for achieving QoS in ad-hoc networks is an
appropriate node-to-node load balancing that takes into account the CPU
performance and the amount of traffic passing through each node.
</summary>
    <author>
      <name>Taichi Miya</name>
    </author>
    <author>
      <name>Kohta Ohshima</name>
    </author>
    <author>
      <name>Yoshiaki Kitaguchi</name>
    </author>
    <author>
      <name>Katsunori Yamaoka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 19 figures, IEEE Consumer Communications &amp; Networking
  Conference 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.15572v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.15572v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.07160v1</id>
    <updated>2020-11-13T22:55:15Z</updated>
    <published>2020-11-13T22:55:15Z</published>
    <title>Phoebe: Reuse-Aware Online Caching with Reinforcement Learning for
  Emerging Storage Models</title>
    <summary>  With data durability, high access speed, low power efficiency and byte
addressability, NVMe and SSD, which are acknowledged representatives of
emerging storage technologies, have been applied broadly in many areas.
However, one key issue with high-performance adoption of these technologies is
how to properly define intelligent cache layers such that the performance gap
between emerging technologies and main memory can be well bridged. To this end,
we propose Phoebe, a reuse-aware reinforcement learning framework for the
optimal online caching that is applicable for a wide range of emerging storage
models. By continuous interacting with the cache environment and the data
stream, Phoebe is capable to extract critical temporal data dependency and
relative positional information from a single trace, becoming ever smarter over
time. To reduce training overhead during online learning, we utilize periodical
training to amortize costs. Phoebe is evaluated on a set of Microsoft cloud
storage workloads. Experiment results show that Phoebe is able to close the gap
of cache miss rate from LRU and a state-of-the-art online learning based cache
policy to the Belady's optimal policy by 70.3% and 52.6%, respectively.
</summary>
    <author>
      <name>Nan Wu</name>
    </author>
    <author>
      <name>Pengcheng Li</name>
    </author>
    <link href="http://arxiv.org/abs/2011.07160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.07160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.15065v2</id>
    <updated>2021-05-21T21:47:51Z</updated>
    <published>2020-11-30T18:03:28Z</published>
    <title>No Crash, No Exploit: Automated Verification of Embedded Kernels</title>
    <summary>  The kernel is the most safety- and security-critical component of many
computer systems, as the most severe bugs lead to complete system crash or
exploit. It is thus desirable to guarantee that a kernel is free from these
bugs using formal methods, but the high cost and expertise required to do so
are deterrent to wide applicability. We propose a method that can verify both
absence of runtime errors (i.e. crashes) and absence of privilege escalation
(i.e. exploits) in embedded kernels from their binary executables. The method
can verify the kernel runtime independently from the application, at the
expense of only a few lines of simple annotations. When given a specific
application, the method can verify simple kernels without any human
intervention. We demonstrate our method on two different use cases: we use our
tool to help the development of a new embedded real-time kernel, and we verify
an existing industrial real-time kernel executable with no modification.
Results show that the method is fast, simple to use, and can prevent real
errors and security vulnerabilities.
</summary>
    <author>
      <name>Olivier Nicole</name>
    </author>
    <author>
      <name>Matthieu Lemerre</name>
    </author>
    <author>
      <name>Sébastien Bardin</name>
    </author>
    <author>
      <name>Xavier Rival</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/RTAS52030.2021.00011</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/RTAS52030.2021.00011" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in IEEE Real-Time and Embedded Technology and Applications
  Symposium (RTAS'21)</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.15065v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.15065v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.09980v3</id>
    <updated>2022-03-04T16:51:28Z</updated>
    <published>2021-02-19T15:20:51Z</published>
    <title>A flow-based IDS using Machine Learning in eBPF</title>
    <summary>  eBPF is a new technology which allows dynamically loading pieces of code into
the Linux kernel. It can greatly speed up networking since it enables the
kernel to process certain packets without the involvement of a userspace
program. So far eBPF has been used for simple packet filtering applications
such as firewalls or Denial of Service protection. We show that it is possible
to develop a flow based network intrusion detection system based on machine
learning entirely in eBPF. Our solution uses a decision tree and decides for
each packet whether it is malicious or not, considering the entire previous
context of the network flow. We achieve a performance increase of over 20%
compared to the same solution implemented as a userspace program.
</summary>
    <author>
      <name>Maximilian Bachl</name>
    </author>
    <author>
      <name>Joachim Fabini</name>
    </author>
    <author>
      <name>Tanja Zseby</name>
    </author>
    <link href="http://arxiv.org/abs/2102.09980v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.09980v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.12892v1</id>
    <updated>2021-02-04T21:56:28Z</updated>
    <published>2021-02-04T21:56:28Z</published>
    <title>Restoring Uniqueness in MicroVM Snapshots</title>
    <summary>  Code initialization -- the step of loading code, executing static code,
filling caches, and forming re-used connections -- tends to dominate cold-start
time in serverless compute systems such as AWS Lambda. Post-initialization
memory snapshots, cloned and restored on start, have emerged as a viable
solution to this problem, with incremental snapshot and fast restore support in
VMMs like Firecracker.
  Saving memory introduces the challenge of managing high-value memory
contents, such as cryptographic secrets. Cloning introduces the challenge of
restoring the uniqueness of the VMs, to allow them to do unique things like
generate UUIDs, secrets, and nonces. This paper examines solutions to these
problems in the every microsecond counts context of serverless cold-start, and
discusses the state-of-the-art of available solutions. We present two new
interfaces aimed at solving this problem -- MADV\_WIPEONSUSPEND and SysGenId --
and compare them to alternative solutions.
</summary>
    <author>
      <name>Marc Brooker</name>
    </author>
    <author>
      <name>Adrian Costin Catangiu</name>
    </author>
    <author>
      <name>Mike Danilov</name>
    </author>
    <author>
      <name>Alexander Graf</name>
    </author>
    <author>
      <name>Colm MacCarthaigh</name>
    </author>
    <author>
      <name>Andrei Sandu</name>
    </author>
    <link href="http://arxiv.org/abs/2102.12892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.12892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.00594v1</id>
    <updated>2022-01-03T11:39:46Z</updated>
    <published>2022-01-03T11:39:46Z</published>
    <title>A Priority-Aware Multiqueue NIC Design</title>
    <summary>  Low-level embedded systems are used to control cyber-phyiscal systems in
industrial and autonomous applications. They need to meet hard real-time
requirements as unanticipated controller delays on moving machines can have
devastating effects. Modern developments such as the industrial Internet of
Things and autonomous machines require these devices to connect to large IP
networks. Since Network Interface Controllers (NICs) trigger interrupts for
incoming packets, real-time embedded systems are subject to unpredictable
preemptions when connected to such networks.
  In this work, we propose a priority-aware NIC design to moderate
network-generated interrupts by mapping IP flows to processes and based on
that, consolidates their packets into different queues. These queues apply
priority-dependent interrupt moderation. First experimental evaluations show
that 93% of interrupts can be saved leading to an 80% decrease of processing
delay of critical tasks in the configurations investigated.
</summary>
    <author>
      <name>Ilja Behnke</name>
    </author>
    <author>
      <name>Philipp Wiesner</name>
    </author>
    <author>
      <name>Robert Danicki</name>
    </author>
    <author>
      <name>Lauritz Thamsen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3477314.3507165</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3477314.3507165" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 37th ACM/SIGAPP Symposium on Applied Computing (SAC '22)</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.00594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.00594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4; B.4.1; D.4.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.03256v1</id>
    <updated>2022-01-10T10:19:49Z</updated>
    <published>2022-01-10T10:19:49Z</published>
    <title>DiOS -- An Extended Reality Operating System for the Metaverse</title>
    <summary>  Driven by the recent improvements in device and networks capabilities,
Extended Reality (XR) is becoming more pervasive; industry and academia alike
envision ambitious projects such as the metaverse. However, XR is still limited
by the current architecture of mobile systems. This paper makes the case for an
XR-specific operating system (XROS). Such an XROS integrates hardware-support,
computer vision algorithms, and XR-specific networking as the primitives
supporting XR technology. These primitives represent the physical-digital world
as a single shared resource among applications. Such an XROS allows for the
development of coherent and system-wide interaction and display methods,
systematic privacy preservation on sensor data, and performance improvement
while simplifying application development.
</summary>
    <author>
      <name>Tristan Braud</name>
    </author>
    <author>
      <name>Lik-Hang Lee</name>
    </author>
    <author>
      <name>Ahmad Alhilal</name>
    </author>
    <author>
      <name>Carlos Bermejo Fernández</name>
    </author>
    <author>
      <name>Pan Hui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.03256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.03256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.02223v1</id>
    <updated>2022-02-03T07:05:13Z</updated>
    <published>2022-02-03T07:05:13Z</published>
    <title>Systems for Memory Disaggregation: Challenges &amp; Opportunities</title>
    <summary>  Memory disaggregation addresses memory imbalance in a cluster by decoupling
CPU and memory allocations of applications while also increasing the effective
memory capacity for (memory-intensive) applications beyond the local memory
limit imposed by traditional fixed-capacity servers. As the network speeds in
the tightly-knit environments like modern datacenters inch closer to the DRAM
speeds, there has been a recent proliferation of work in this space ranging
from software solutions that pool memory of traditional servers for the shared
use of the cluster to systems targeting the memory disaggregation in the
hardware. In this report, we look at some of these recent memory disaggregation
systems and study the important factors that guide their design, such as the
interface through which the memory is exposed to the application, their runtime
design and relevant optimizations to retain the near-native application
performance, various approaches they employ in managing cluster memory to
maximize utilization, etc. and we analyze the associated trade-offs. We
conclude with a discussion on some open questions and potential future
directions that can render disaggregation more amenable for adoption.
</summary>
    <author>
      <name>Anil Yelam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.02223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.02223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.08882v2</id>
    <updated>2022-08-31T15:42:00Z</updated>
    <published>2022-05-18T12:12:05Z</published>
    <title>Hyperion: A Case for Unified, Self-Hosting, Zero-CPU Data-Processing
  Units (DPUs)</title>
    <summary>  Since the inception of computing, we have been reliant on CPU-powered
architectures. However, today this reliance is challenged by manufacturing
limitations (CMOS scaling), performance expectations (stalled clocks, Turing
tax), and security concerns (microarchitectural attacks). To re-imagine our
computing architecture, in this work we take a more radical but pragmatic
approach and propose to eliminate the CPU with its design baggage, and
integrate three primary pillars of computing, i.e., networking, storage, and
computing, into a single, self-hosting, unified CPU-free Data Processing Unit
(DPU) called Hyperion. In this paper, we present the case for Hyperion, its
design choices, initial work-in-progress details, and seek feedback from the
systems community.
</summary>
    <author>
      <name>Marco Spaziani Brunella</name>
    </author>
    <author>
      <name>Marco Bonola</name>
    </author>
    <author>
      <name>Animesh Trivedi</name>
    </author>
    <link href="http://arxiv.org/abs/2205.08882v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.08882v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.5; D.3; D.4; B.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.04507v2</id>
    <updated>2023-11-06T22:34:46Z</updated>
    <published>2022-06-09T13:47:18Z</published>
    <title>Software Mitigation of RISC-V Spectre Attacks</title>
    <summary>  Speculative attacks are still an active threat today that, even if initially
focused on the x86 platform, reach across all modern hardware architectures.
RISC-V is a newly proposed open instruction set architecture that has seen
traction from both the industry and academia in recent years. In this paper we
focus on the RISC-V cores where speculation is enabled and, as we show, where
Spectre attacks are as effective as on x86. Even though RISC-V hardware
mitigations were proposed in the past, they have not yet passed the prototype
phase. Instead, we propose low-overhead software mitigations for Spectre-BTI,
inspired from those used on the x86 architecture, and for Spectre-RSB, to our
knowledge the first such mitigation to be proposed. We show that these
mitigations work in practice and that they can be integrated in the LLVM
toolchain. For transparency and reproducibility, all our programs and data are
made publicly available online.
</summary>
    <author>
      <name>Ruxandra Bălucea</name>
    </author>
    <author>
      <name>Paul Irofti</name>
    </author>
    <link href="http://arxiv.org/abs/2206.04507v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.04507v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.01635v2</id>
    <updated>2022-12-06T15:27:44Z</updated>
    <published>2022-09-04T14:55:41Z</published>
    <title>Towards Adaptive Storage Views in Virtual Memory</title>
    <summary>  Traditionally, DBMSs separate their storage layer from their indexing layer.
While the storage layer physically materializes the database and provides
low-level access methods to it, the indexing layer on top enables a faster
locating of searched-for entries. While this clearly separates concerns, it
also adds a level of indirection to the already complex execution path. In this
work, we propose an alternative design: Instead of conservatively separating
both layers, we naturally fuse them by integrating an adaptive coarse-granular
indexing scheme directly into the storage layer. We do so by utilizing tools of
the virtual memory management subsystem provided by the OS: On the lowest
level, we materialize the database content in form of physical main memory. On
top of that, we allow the creation of arbitrarily many virtual memory storage
views that map to subsets of the database having certain properties of
interest. This creation happens fully adaptively as a side-product of query
processing. To speed up query answering, we route each query automatically to
the most fitting virtual view(s). By this, we naturally index the storage layer
in its core and gradually improve the provided scan performance.
</summary>
    <author>
      <name>Felix Schuhknecht</name>
    </author>
    <author>
      <name>Justus Henneberg</name>
    </author>
    <link href="http://arxiv.org/abs/2209.01635v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.01635v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.09645v1</id>
    <updated>2022-09-18T13:22:53Z</updated>
    <published>2022-09-18T13:22:53Z</published>
    <title>Multi-level Explanation of Deep Reinforcement Learning-based Scheduling</title>
    <summary>  Dependency-aware job scheduling in the cluster is NP-hard. Recent work shows
that Deep Reinforcement Learning (DRL) is capable of solving it. It is
difficult for the administrator to understand the DRL-based policy even though
it achieves remarkable performance gain. Therefore the complex model-based
scheduler is not easy to gain trust in the system where simplicity is favored.
In this paper, we give the multi-level explanation framework to interpret the
policy of DRL-based scheduling. We dissect its decision-making process to job
level and task level and approximate each level with interpretable models and
rules, which align with operational practices. We show that the framework gives
the system administrator insights into the state-of-the-art scheduler and
reveals the robustness issue in regards to its behavior pattern.
</summary>
    <author>
      <name>Shaojun Zhang</name>
    </author>
    <author>
      <name>Chen Wang</name>
    </author>
    <author>
      <name>Albert Zomaya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in the MLSys'22 Workshop on Cloud Intelligence / AIOps</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.09645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.09645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.02266v1</id>
    <updated>2022-11-04T05:31:48Z</updated>
    <published>2022-11-04T05:31:48Z</published>
    <title>Rescuing the End-user systems from Vulnerable Applications using
  Virtualization Techniques</title>
    <summary>  In systems owned by normal end-users, many times security attacks are mounted
by sneaking in malicious applications or exploiting existing software
vulnerabilities through security non-conforming actions of users.
Virtualization approaches can address this problem by providing a quarantine
environment for applications, malicious devices, and device drivers, which are
mostly used as entry points for security attacks. However, the existing methods
to provide quarantine environments using virtualization are not transparent to
the user, both in terms of application interface transparency and file system
transparency. Further, software configuration level solutions like remote
desktops and remote application access mechanisms combined with shared file
systems do not meet the user transparency and security requirements. We propose
qOS, a VM-based solution combined with certain OS extensions to meet the
security requirements of end-point systems owned by normal users, in a
transparent and efficient manner. We demonstrate the efficacy of qOS by
empirically evaluating the prototype implementation in the Linux+KVM system in
terms of efficiency, security, and user transparency.
</summary>
    <author>
      <name>Vinayak Trivedi</name>
    </author>
    <author>
      <name>Tushar Gurjar</name>
    </author>
    <author>
      <name>Sumaiya Shaikh</name>
    </author>
    <author>
      <name>Saketh Maddamsetty</name>
    </author>
    <author>
      <name>Debadatta Mishra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.02266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.02266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.03054v1</id>
    <updated>2022-12-06T15:31:20Z</updated>
    <published>2022-12-06T15:31:20Z</published>
    <title>DisTRaC: Accelerating High Performance Compute Processing for Temporary
  Data Storage</title>
    <summary>  High Performance Compute (HPC) clusters often produce intermediate files as
part of code execution and message passing is not always possible to supply
data to these cluster jobs. In these cases, I/O goes back to central
distributed storage to allow cross node data sharing. These systems are often
high performance and characterised by their high cost per TB and sensitivity to
workload type such as being tuned to small or large file I/O. However, compute
nodes often have large amounts of RAM, so when dealing with intermediate files
where longevity or reliability of the system is not as important, local RAM
disks can be used to obtain performance benefits. In this paper we show how
this problem was tackled by creating a RAM block that could interact with the
object storage system Ceph, as well as creating a deployment tool to deploy
Ceph on HPC infrastructure effectively. This work resulted in a system that was
more performant than the central high performance distributed storage system
used at Diamond reducing I/O overhead and processing time for Savu, a
tomography data processing application, by 81.04% and 8.32% respectively.
</summary>
    <author>
      <name>Gabryel Mason-Williams</name>
    </author>
    <author>
      <name>Dave Bond</name>
    </author>
    <author>
      <name>Mark Basham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.03054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.03054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.04197v1</id>
    <updated>2022-12-08T11:23:48Z</updated>
    <published>2022-12-08T11:23:48Z</published>
    <title>HyperEnclave: An Open and Cross-platform Trusted Execution Environment</title>
    <summary>  A number of trusted execution environments (TEEs) have been proposed by both
academia and industry. However, most of them require specific hardware or
firmware changes and are bound to specific hardware vendors (such as Intel,
AMD, ARM, and IBM). In this paper, we propose HyperEnclave, an open and
cross-platform process-based TEE that relies on the widely-available
virtualization extension to create the isolated execution environment. In
particular, HyperEnclave is designed to support the flexible enclave operation
modes to fulfill the security and performance demands under various enclave
workloads. We provide the enclave SDK to run existing SGX programs on
HyperEnclave with little or no source code changes. We have implemented
HyperEnclave on commodity AMD servers and deployed the system in a
world-leading FinTech company to support real-world privacy-preserving
computations. The evaluation on both micro-benchmarks and application
benchmarks shows the design of HyperEnclave introduces only a small overhead.
</summary>
    <author>
      <name>Yuekai Jia</name>
    </author>
    <author>
      <name>Shuang Liu</name>
    </author>
    <author>
      <name>Wenhao Wang</name>
    </author>
    <author>
      <name>Yu Chen</name>
    </author>
    <author>
      <name>Zhengde Zhai</name>
    </author>
    <author>
      <name>Shoumeng Yan</name>
    </author>
    <author>
      <name>Zhengyu He</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In 2022 USENIX Annual Technical Conference (USENIX ATC 22), pages
  437-454, Carlsbad, CA, July 2022. USENIX Association</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2212.04197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.04197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.00562v1</id>
    <updated>2023-01-02T08:53:44Z</updated>
    <published>2023-01-02T08:53:44Z</published>
    <title>Age-Optimal Multi-Channel-Scheduling under Energy and Tolerance
  Constraints</title>
    <summary>  We study the optimal scheduling problem where n source nodes attempt to
transmit updates over L shared wireless on/off fading channels to optimize
their age performance under energy and age-violation tolerance constraints.
Specifically, we provide a generic formulation of age-optimization in the form
of a constrained Markov Decision Processes (CMDP), and obtain the optimal
scheduler as the solution of an associated Linear Programming problem. We
investigate the characteristics of the optimal single-user multi-channel
scheduler for the important special cases of average-age and violation-rate
minimization. This leads to several key insights on the nature of the optimal
allocation of the limited energy, where a usual threshold-based policy does not
apply and will be useful in guiding scheduler designers. We then investigate
the stability region of the optimal scheduler for the multi-user case. We also
develop an online scheduler using Lyapunov-drift-minimization methods that do
not require the knowledge of channel statistics. Our numerical studies compare
the stability region of our online scheduler to the optimal scheduler to reveal
that it performs closely with unknown channel statistics.
</summary>
    <author>
      <name>Xujin Zhou</name>
    </author>
    <author>
      <name>Irem Koprulu</name>
    </author>
    <author>
      <name>Atilla Eryilmaz</name>
    </author>
    <link href="http://arxiv.org/abs/2301.00562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.00562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.13421v3</id>
    <updated>2024-06-07T03:23:39Z</updated>
    <published>2023-01-31T05:31:45Z</published>
    <title>MOAT: Towards Safe BPF Kernel Extension</title>
    <summary>  The Linux kernel extensively uses the Berkeley Packet Filter (BPF) to allow
user-written BPF applications to execute in the kernel space. The BPF employs a
verifier to check the security of user-supplied BPF code statically. Recent
attacks show that BPF programs can evade security checks and gain unauthorized
access to kernel memory, indicating that the verification process is not
flawless. In this paper, we present MOAT, a system that isolates potentially
malicious BPF programs using Intel Memory Protection Keys (MPK). Enforcing BPF
program isolation with MPK is not straightforward; MOAT is designed to
alleviate technical obstacles, such as limited hardware keys and the need to
protect a wide variety of BPF helper functions. We implement MOAT on Linux
(ver. 6.1.38), and our evaluation shows that MOAT delivers low-cost isolation
of BPF programs under mainstream use cases, such as isolating a BPF packet
filter with only 3% throughput loss.
</summary>
    <author>
      <name>Hongyi Lu</name>
    </author>
    <author>
      <name>Shuai Wang</name>
    </author>
    <author>
      <name>Yechang Wu</name>
    </author>
    <author>
      <name>Wanning He</name>
    </author>
    <author>
      <name>Fengwei Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2301.13421v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.13421v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.09468v2</id>
    <updated>2023-05-01T18:43:34Z</updated>
    <published>2023-02-19T03:49:02Z</published>
    <title>Rethinking Memory Profiling and Migration for Multi-Tiered Large Memory
  Systems</title>
    <summary>  Multi-tiered large memory systems call for rethinking of memory profiling and
migration because of the unique problems unseen in the traditional memory
systems with smaller capacity and fewer tiers. We develop MTM, an
application-transparent page management system based on three principles: (1)
connecting the control of profiling overhead with the profiling mechanism for
high-quality profiling; (2) building a universal page migration policy on the
complex multi-tiered memory for high performance; and (3) introducing huge page
awareness. We evaluate MTM using common big-data applications with realistic
working sets (hundreds of GB to 1 TB). MTM outperforms seven state-of-the-art
solutions by up to 42% (17% on average)
</summary>
    <author>
      <name>Jie Ren</name>
    </author>
    <author>
      <name>Dong Xu</name>
    </author>
    <author>
      <name>Ivy Peng</name>
    </author>
    <author>
      <name>Junhee Ryu</name>
    </author>
    <author>
      <name>Kwangsik Shin</name>
    </author>
    <author>
      <name>Daewoo Kim</name>
    </author>
    <author>
      <name>Dong Li</name>
    </author>
    <link href="http://arxiv.org/abs/2302.09468v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.09468v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.11358v1</id>
    <updated>2023-02-22T13:10:07Z</updated>
    <published>2023-02-22T13:10:07Z</published>
    <title>Faabric: Fine-Grained Distribution of Scientific Workloads in the Cloud</title>
    <summary>  With their high parallelism and resource needs, many scientific applications
benefit from cloud deployments. Today, scientific applications are executed on
dedicated pools of VMs, resulting in resource fragmentation: users pay for
underutilised resources, and providers cannot reallocate unused resources
between applications. While serverless cloud computing could address these
issues, its programming model is incompatible with the use of shared memory and
message passing in scientific applications: serverless functions do not share
memory directly on the same VM or support message passing semantics when
scheduling functions dynamically.
  We describe Faabric, a new serverless cloud runtime that transparently
distributes applications with shared memory and message passing across VMs.
Faabric achieves this by scheduling computation in a fine-grained
(thread/process) fashion through a new execution abstraction called Granules.
To support shared memory, Granules are isolated using WebAssembly but share
memory directly; to support message passing, Granules offer asynchronous
point-to-point communication. Faabric schedules Granules to meet an
application's parallelism needs. It also synchronises changes to Granule's
shared memory, and migrates Granules to improve locality.
</summary>
    <author>
      <name>Simon Shillaker</name>
    </author>
    <author>
      <name>Carlos Segarra</name>
    </author>
    <author>
      <name>Eleftheria Mappoura</name>
    </author>
    <author>
      <name>Mayeul Fournial</name>
    </author>
    <author>
      <name>Lluis Vilanova</name>
    </author>
    <author>
      <name>Peter Pietzuch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.11358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.11358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.15540v1</id>
    <updated>2023-03-27T18:38:28Z</updated>
    <published>2023-03-27T18:38:28Z</published>
    <title>Intel TDX Demystified: A Top-Down Approach</title>
    <summary>  Intel Trust Domain Extensions (TDX) is a new architectural extension in the
4th Generation Intel Xeon Scalable Processor that supports confidential
computing. TDX allows the deployment of virtual machines in the
Secure-Arbitration Mode (SEAM) with encrypted CPU state and memory, integrity
protection, and remote attestation. TDX aims to enforce hardware-assisted
isolation for virtual machines and minimize the attack surface exposed to host
platforms, which are considered to be untrustworthy or adversarial in the
confidential computing's new threat model. TDX can be leveraged by regulated
industries or sensitive data holders to outsource their computations and data
with end-to-end protection in public cloud infrastructure.
  This paper aims to provide a comprehensive understanding of TDX to potential
adopters, domain experts, and security researchers looking to leverage the
technology for their own purposes. We adopt a top-down approach, starting with
high-level security principles and moving to low-level technical details of
TDX. Our analysis is based on publicly available documentation and source code,
offering insights from security researchers outside of Intel.
</summary>
    <author>
      <name>Pau-Chen Cheng</name>
    </author>
    <author>
      <name>Wojciech Ozga</name>
    </author>
    <author>
      <name>Enriquillo Valdez</name>
    </author>
    <author>
      <name>Salman Ahmed</name>
    </author>
    <author>
      <name>Zhongshu Gu</name>
    </author>
    <author>
      <name>Hani Jamjoom</name>
    </author>
    <author>
      <name>Hubertus Franke</name>
    </author>
    <author>
      <name>James Bottomley</name>
    </author>
    <link href="http://arxiv.org/abs/2303.15540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.15540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.08569v1</id>
    <updated>2023-04-17T19:14:37Z</updated>
    <published>2023-04-17T19:14:37Z</published>
    <title>Diagnosing applications' I/O behavior through system call observability</title>
    <summary>  We present DIO, a generic tool for observing inefficient and erroneous I/O
interactions between applications and in-kernel storage systems that lead to
performance, dependability, and correctness issues. DIO facilitates the
analysis and enables near real-time visualization of complex I/O patterns for
data-intensive applications generating millions of storage requests. This is
achieved by non-intrusively intercepting system calls, enriching collected data
with relevant context, and providing timely analysis and visualization for
traced events. We demonstrate its usefulness by analyzing two production-level
applications. Results show that DIO enables diagnosing resource contention in
multi-threaded I/O that leads to high tail latency and erroneous file accesses
that cause data loss.
</summary>
    <author>
      <name>Tânia Esteves</name>
    </author>
    <author>
      <name>Ricardo Macedo</name>
    </author>
    <author>
      <name>Rui Oliveira</name>
    </author>
    <author>
      <name>João Paulo</name>
    </author>
    <link href="http://arxiv.org/abs/2304.08569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.08569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.01519v1</id>
    <updated>2023-05-01T02:56:43Z</updated>
    <published>2023-05-01T02:56:43Z</published>
    <title>BCEdge: SLO-Aware DNN Inference Services with Adaptive Batching on Edge
  Platforms</title>
    <summary>  As deep neural networks (DNNs) are being applied to a wide range of edge
intelligent applications, it is critical for edge inference platforms to have
both high-throughput and low-latency at the same time. Such edge platforms with
multiple DNN models pose new challenges for scheduler designs. First, each
request may have different service level objectives (SLOs) to improve quality
of service (QoS). Second, the edge platforms should be able to efficiently
schedule multiple heterogeneous DNN models so that system utilization can be
improved. To meet these two goals, this paper proposes BCEdge, a novel
learning-based scheduling framework that takes adaptive batching and concurrent
execution of DNN inference services on edge platforms. We define a utility
function to evaluate the trade-off between throughput and latency. The
scheduler in BCEdge leverages maximum entropy-based deep reinforcement learning
(DRL) to maximize utility by 1) co-optimizing batch size and 2) the number of
concurrent models automatically. Our prototype implemented on different edge
platforms shows that the proposed BCEdge enhances utility by up to 37.6% on
average, compared to state-of-the-art solutions, while satisfying SLOs.
</summary>
    <author>
      <name>Ziyang Zhang</name>
    </author>
    <author>
      <name>Huan Li</name>
    </author>
    <author>
      <name>Yang Zhao</name>
    </author>
    <author>
      <name>Changyao Lin</name>
    </author>
    <author>
      <name>Jie Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2305.01519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.01519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.05701v1</id>
    <updated>2023-06-09T06:41:57Z</updated>
    <published>2023-06-09T06:41:57Z</published>
    <title>CAWL: A Cache-aware Write Performance Model of Linux Systems</title>
    <summary>  The performance of data intensive applications is often dominated by their
input/output (I/O) operations but the I/O stack of systems is complex and
severely depends on system specific settings and hardware components. This
situation makes generic performance optimisation challenging and costly for
developers as they would have to run their application on a large variety of
systems to evaluate their improvements. Here, simulation frameworks can help
reducing the experimental overhead but they typically handle the topic of I/O
rather coarse-grained, which leads to significant inaccuracies in performance
predictions. Here, we propose a more accurate model of the write performance of
Linux-based systems that takes different I/O methods and levels (via system
calls, library calls, direct or indirect, etc.), the page cache, background
writing, and the I/O throttling capabilities of the Linux kernel into account.
With our model, we reduce, for example, the relative prediction error compared
to a standard I/O model included in SimGrid for a random I/O scenario from 67 %
down to 10 % relative error against real measurements of the simulated
workload. In other scenarios the differences are even more pronounced.
</summary>
    <author>
      <name>Masoud Gholami</name>
    </author>
    <author>
      <name>Florian Schintke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 9 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.05701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.05701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.06003v1</id>
    <updated>2023-06-09T16:18:19Z</updated>
    <published>2023-06-09T16:18:19Z</published>
    <title>Semi-online Scheduling with Lookahead</title>
    <summary>  The knowledge of future partial information in the form of a lookahead to
design efficient online algorithms is a theoretically-efficient and realistic
approach to solving computational problems. Design and analysis of semi-online
algorithms with extra-piece-of-information (EPI) as a new input parameter has
gained the attention of the theoretical computer science community in the last
couple of decades. Though competitive analysis is a pessimistic worst-case
performance measure to analyze online algorithms, it has immense theoretical
value in developing the foundation and advancing the state-of-the-art
contributions in online and semi-online scheduling. In this paper, we study and
explore the impact of lookahead as an EPI in the context of online scheduling
in identical machine frameworks. We introduce a $k$-lookahead model and design
improved competitive semi-online algorithms. For a $2$-identical machine
setting, we prove a lower bound of $\frac{4}{3}$ and design an optimal
algorithm with a matching upper bound of $\frac{4}{3}$ on the competitive
ratio. For a $3$-identical machine setting, we show a lower bound of
$\frac{15}{11}$ and design a $\frac{16}{11}$-competitive improved semi-online
algorithm.
</summary>
    <author>
      <name>Debasis Dwibedy</name>
    </author>
    <author>
      <name>Rakesh Mohanty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.06003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.06003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.11227v3</id>
    <updated>2024-05-07T22:56:15Z</updated>
    <published>2023-06-20T01:30:08Z</published>
    <title>An Introduction to the Compute Express Link (CXL) Interconnect</title>
    <summary>  The Compute Express Link (CXL) is an open industry-standard interconnect
between processors and devices such as accelerators, memory buffers, smart
network interfaces, persistent memory, and solid-state drives. CXL offers
coherency and memory semantics with bandwidth that scales with PCIe bandwidth
while achieving significantly lower latency than PCIe. All major CPU vendors,
device vendors, and datacenter operators have adopted CXL as a common standard.
This enables an inter-operable ecosystem that supports key computing use cases
including highly efficient accelerators, server memory bandwidth and capacity
expansion, multi-server resource pooling and sharing, and efficient
peer-to-peer communication. This survey provides an introduction to CXL
covering the standards CXL 1.0, CXL 2.0, and CXL 3.0. We further survey CXL
implementations, discuss CXL's impact on the datacenter landscape, and future
directions.
</summary>
    <author>
      <name>Debendra Das Sharma</name>
    </author>
    <author>
      <name>Robert Blankenship</name>
    </author>
    <author>
      <name>Daniel S. Berger</name>
    </author>
    <link href="http://arxiv.org/abs/2306.11227v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.11227v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.14202v1</id>
    <updated>2023-06-25T10:50:15Z</updated>
    <published>2023-06-25T10:50:15Z</published>
    <title>Enabling Lightweight Privilege Separation in Applications with
  MicroGuards</title>
    <summary>  Application compartmentalization and privilege separation are our primary
weapons against ever-increasing security threats and privacy concerns on
connected devices. Despite significant progress, it is still challenging to
privilege separate inside an application address space and in multithreaded
environments, particularly on resource-constrained and mobile devices. We
propose MicroGuards, a lightweight kernel modification and set of security
primitives and APIs aimed at flexible and fine-grained in-process memory
protection and privilege separation in multithreaded applications. MicroGuards
take advantage of hardware support in modern CPUs and are high-level enough to
be adaptable to various architectures. This paper focuses on enabling
MicroGuards on embedded and mobile devices running Linux kernel and utilizes
tagged memory support to achieve good performance. Our evaluation show that
MicroGuards add small runtime overhead (less than 3.5\%), minimal memory
footprint, and are practical to get integrated with existing applications to
enable fine-grained privilege separation.
</summary>
    <author>
      <name>Zahra Tarkhani</name>
    </author>
    <author>
      <name>Anil Madhavapeddy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2004.04846</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.14202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.14202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.11993v3</id>
    <updated>2024-01-12T05:30:32Z</updated>
    <published>2023-07-22T06:39:20Z</published>
    <title>Verifiable Sustainability in Data Centers</title>
    <summary>  Data centers have significant energy needs, both embodied and operational,
affecting sustainability adversely. The current techniques and tools for
collecting, aggregating, and reporting verifiable sustainability data are
vulnerable to cyberattacks and misuse, requiring new security and
privacy-preserving solutions. This paper outlines security challenges and
research directions for addressing these pressing requirements.
</summary>
    <author>
      <name>Syed Rafiul Hussain</name>
    </author>
    <author>
      <name>Patrick McDaniel</name>
    </author>
    <author>
      <name>Anshul Gandhi</name>
    </author>
    <author>
      <name>Kanad Ghose</name>
    </author>
    <author>
      <name>Kartik Gopalan</name>
    </author>
    <author>
      <name>Dongyoon Lee</name>
    </author>
    <author>
      <name>Yu David Liu</name>
    </author>
    <author>
      <name>Zhenhua Liu</name>
    </author>
    <author>
      <name>Shuai Mu</name>
    </author>
    <author>
      <name>Erez Zadok</name>
    </author>
    <link href="http://arxiv.org/abs/2307.11993v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.11993v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.16288v1</id>
    <updated>2023-07-30T17:53:08Z</updated>
    <published>2023-07-30T17:53:08Z</published>
    <title>Towards Learned Predictability of Storage Systems</title>
    <summary>  With the rapid development of cloud computing and big data technologies,
storage systems have become a fundamental building block of datacenters,
incorporating hardware innovations such as flash solid state drives and
non-volatile memories, as well as software infrastructures such as RAID and
distributed file systems. Despite the growing popularity and interests in
storage, designing and implementing reliable storage systems remains
challenging, due to their performance instability and prevailing hardware
failures.
  Proactive prediction greatly strengthens the reliability of storage systems.
There are two dimensions of prediction: performance and failure. Ideally,
through detecting in advance the slow IO requests, and predicting device
failures before they really happen, we can build storage systems with
especially low tail latency and high availability. While its importance is well
recognized, such proactive prediction in storage systems, on the other hand, is
particularly difficult. To move towards predictability of storage systems,
various mechanisms and field studies have been proposed in the past few years.
In this report, we present a survey of these mechanisms and field studies,
focusing on machine learning based black-box approaches. Based on three
representative research works, we discuss where and how machine learning should
be applied in this field. The strengths and limitations of each research work
are also evaluated in detail.
</summary>
    <author>
      <name>Chenyuan Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2307.16288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.16288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.03628v3</id>
    <updated>2024-03-13T08:57:29Z</updated>
    <published>2023-09-07T10:50:32Z</published>
    <title>OSMOSIS: Enabling Multi-Tenancy in Datacenter SmartNICs</title>
    <summary>  Multi-tenancy is essential for unleashing SmartNIC's potential in
datacenters. Our systematic analysis in this work shows that existing on-path
SmartNICs have resource multiplexing limitations. For example, existing
solutions lack multi-tenancy capabilities such as performance isolation and QoS
provisioning for compute and IO resources. Compared to standard NIC data paths
with a well-defined set of offloaded functions, unpredictable execution times
of SmartNIC kernels make conventional approaches for multi-tenancy and QoS
insufficient. We fill this gap with OSMOSIS, a SmartNICs resource manager
co-design. OSMOSIS extends existing OS mechanisms to enable dynamic hardware
resource multiplexing of the on-path packet processing data plane. We integrate
OSMOSIS within an open-source RISC-V-based 400Gbit/s SmartNIC. Our performance
results demonstrate that OSMOSIS fully supports multi-tenancy and enables
broader adoption of SmartNICs in datacenters with low overhead.
</summary>
    <author>
      <name>Mikhail Khalilov</name>
    </author>
    <author>
      <name>Marcin Chrapek</name>
    </author>
    <author>
      <name>Siyuan Shen</name>
    </author>
    <author>
      <name>Alessandro Vezzu</name>
    </author>
    <author>
      <name>Thomas Benz</name>
    </author>
    <author>
      <name>Salvatore Di Girolamo</name>
    </author>
    <author>
      <name>Timo Schneider</name>
    </author>
    <author>
      <name>Daniele De Sensi</name>
    </author>
    <author>
      <name>Luca Benini</name>
    </author>
    <author>
      <name>Torsten Hoefler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 14 figures, 103 references</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.03628v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.03628v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.09291v1</id>
    <updated>2023-09-17T14:58:33Z</updated>
    <published>2023-09-17T14:58:33Z</published>
    <title>OSmosis: No more Déjà vu in OS isolation</title>
    <summary>  Operating systems provide an abstraction layer between the hardware and
higher-level software. Many abstractions, such as threads, processes,
containers, and virtual machines, are mechanisms to provide isolation. New
application scenarios frequently introduce new isolation mechanisms.
Implementing each isolation mechanism as an independent abstraction makes it
difficult to reason about the state and resources shared among different tasks,
leading to security vulnerabilities and performance interference. We present
OSmosis, an isolation model that expresses the precise level of resource
sharing, a framework in which to implement isolation mechanisms based on the
model, and an implementation of the framework on seL4. The OSmosis model lets
the user determine the degree of isolation guarantee that they need from the
system. This determination empowers developers to make informed decisions about
isolation and performance trade-offs, and the framework enables them to create
mechanisms with the desired degree of isolation.
</summary>
    <author>
      <name>Sidhartha Agrawal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of British Columbia</arxiv:affiliation>
    </author>
    <author>
      <name>Reto Achermann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of British Columbia</arxiv:affiliation>
    </author>
    <author>
      <name>Margo Seltzer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of British Columbia</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.09291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.09291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6; D.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.09618v1</id>
    <updated>2023-09-18T09:42:36Z</updated>
    <published>2023-09-18T09:42:36Z</published>
    <title>A Discussion on Generalization in Next-Activity Prediction</title>
    <summary>  Next activity prediction aims to forecast the future behavior of running
process instances. Recent publications in this field predominantly employ deep
learning techniques and evaluate their prediction performance using publicly
available event logs. This paper presents empirical evidence that calls into
question the effectiveness of these current evaluation approaches. We show that
there is an enormous amount of example leakage in all of the commonly used
event logs, so that rather trivial prediction approaches perform almost as well
as ones that leverage deep learning. We further argue that designing robust
evaluations requires a more profound conceptual engagement with the topic of
next-activity prediction, and specifically with the notion of generalization to
new data. To this end, we present various prediction scenarios that necessitate
different types of generalization to guide future research.
</summary>
    <author>
      <name>Luka Abb</name>
    </author>
    <author>
      <name>Peter Pfeiffer</name>
    </author>
    <author>
      <name>Peter Fettke</name>
    </author>
    <author>
      <name>Jana-Rebecca Rehse</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pre-print, published at the AI4BPM workshop at BPM 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.09618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.09618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.09124v1</id>
    <updated>2023-10-13T14:13:46Z</updated>
    <published>2023-10-13T14:13:46Z</published>
    <title>Taking the Shortcut: Actively Incorporating the Virtual Memory Index of
  the OS to Hardware-Accelerate Database Indexing</title>
    <summary>  Index structures often materialize one or multiple levels of explicit
indirections (aka pointers) to allow for a quick traversal to the data of
interest. Unfortunately, dereferencing a pointer to go from one level to the
other is costly since additionally to following the address, it involves two
address translations from virtual memory to physical memory under the hood. In
the worst case, such an address translation is resolved by an index access
itself, namely by a lookup into the page table, a central hardware-accelerated
index structure of the OS. However, if the page table is anyways constantly
queried, it raises the question whether we can actively incorporate it into our
database indexes and make it work for us. Precisely, instead of materializing
indirections in form of pointers, we propose to express these indirections
directly in the page table wherever possible. By introducing such shortcuts, we
(a) effectively reduce the height of traversal during lookups and (b) exploit
the hardware-acceleration of lookups in the page table. In this work, we
analyze the strengths and considerations of this approach and showcase its
effectiveness at the case of the real-world indexing scheme extendible hashing.
</summary>
    <author>
      <name>Felix Schuhknecht</name>
    </author>
    <link href="http://arxiv.org/abs/2310.09124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.09124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.18481v1</id>
    <updated>2023-10-27T20:50:56Z</updated>
    <published>2023-10-27T20:50:56Z</published>
    <title>MOSEL: Inference Serving Using Dynamic Modality Selection</title>
    <summary>  Rapid advancements over the years have helped machine learning models reach
previously hard-to-achieve goals, sometimes even exceeding human capabilities.
However, to attain the desired accuracy, the model sizes and in turn their
computational requirements have increased drastically. Thus, serving
predictions from these models to meet any target latency and cost requirements
of applications remains a key challenge, despite recent work in building
inference-serving systems as well as algorithmic approaches that dynamically
adapt models based on inputs. In this paper, we introduce a form of dynamism,
modality selection, where we adaptively choose modalities from inference inputs
while maintaining the model quality. We introduce MOSEL, an automated inference
serving system for multi-modal ML models that carefully picks input modalities
per request based on user-defined performance and accuracy requirements. MOSEL
exploits modality configurations extensively, improving system throughput by
3.6$\times$ with an accuracy guarantee and shortening job completion times by
11$\times$.
</summary>
    <author>
      <name>Bodun Hu</name>
    </author>
    <author>
      <name>Le Xu</name>
    </author>
    <author>
      <name>Jeongyoon Moon</name>
    </author>
    <author>
      <name>Neeraja J. Yadwadkar</name>
    </author>
    <author>
      <name>Aditya Akella</name>
    </author>
    <link href="http://arxiv.org/abs/2310.18481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.18481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.03585v1</id>
    <updated>2023-11-06T22:35:53Z</updated>
    <published>2023-11-06T22:35:53Z</published>
    <title>OpenBSD formal driver verification with SeL4</title>
    <summary>  The seL4 microkernel is currently the only kernel that has been fully
formally verified. In general, the increased interest in ensuring the security
of a kernel's code results from its important role in the entire operating
system. One of the basic features of an operating system is that it abstracts
the handling of devices. This abstraction is represented by device drivers -
the software that manages the hardware. A proper verification of the software
component could ensure that the device would work properly unless there is a
hardware failure.In this paper, we choose to model the behavior of a device
driver and build the proof that the code implementation matches the expected
behavior. The proof was written in Isabelle/HOL, the code translation from C to
Isabelle was done automatically by the use of the C-to-Isabelle Parser and
AutoCorres tools. We choose Isabelle theorem prover because its efficiency was
already shown through the verification of seL4 microkernel.
</summary>
    <author>
      <name>Adriana Nicolae</name>
    </author>
    <author>
      <name>Paul Irofti</name>
    </author>
    <author>
      <name>Ioana Leustean</name>
    </author>
    <link href="http://arxiv.org/abs/2311.03585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.03585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.08274v3</id>
    <updated>2024-04-29T07:48:24Z</updated>
    <published>2023-11-14T16:11:35Z</published>
    <title>Laccolith: Hypervisor-Based Adversary Emulation with Anti-Detection</title>
    <summary>  Advanced Persistent Threats (APTs) represent the most threatening form of
attack nowadays since they can stay undetected for a long time. Adversary
emulation is a proactive approach for preparing against these attacks. However,
adversary emulation tools lack the anti-detection abilities of APTs. We
introduce Laccolith, a hypervisor-based solution for adversary emulation with
anti-detection to fill this gap. We also present an experimental study to
compare Laccolith with MITRE CALDERA, a state-of-the-art solution for adversary
emulation, against five popular anti-virus products. We found that CALDERA
cannot evade detection, limiting the realism of emulated attacks, even when
combined with a state-of-the-art anti-detection framework. Our experiments show
that Laccolith can hide its activities from all the tested anti-virus products,
thus making it suitable for realistic emulations.
</summary>
    <author>
      <name>Vittorio Orbinato</name>
    </author>
    <author>
      <name>Marco Carlo Feliciano</name>
    </author>
    <author>
      <name>Domenico Cotroneo</name>
    </author>
    <author>
      <name>Roberto Natella</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TDSC.2024.3376129</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TDSC.2024.3376129" rel="related"/>
    <link href="http://arxiv.org/abs/2311.08274v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.08274v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.09449v2</id>
    <updated>2025-03-21T10:25:32Z</updated>
    <published>2023-11-15T23:36:14Z</published>
    <title>HAL 9000: a Risk Manager for ITSs</title>
    <summary>  HAL 9000 is an Intrusion Tolerant Systems (ITSs) Risk Manager, which assesses
configuration risks against potential intrusions. It utilizes gathered threat
knowledge and remains operational, even in the absence of updated information.
Based on its advice, the ITSs can dynamically and proactively adapt to recent
threats to minimize and mitigate future intrusions from malicious adversaries.
Our goal is to reduce the risk linked to the exploitation of recently uncovered
vulnerabilities that have not been classified and/or do not have a script to
reproduce the exploit, considering the potential that they may have already
been exploited as zero-day exploits. Our experiments demonstrate that the
proposed solution can effectively learn and replicate National Vulnerability
Database's evaluation process with 99% accuracy.
</summary>
    <author>
      <name>Tadeu Freitas</name>
    </author>
    <author>
      <name>Carlos Novo</name>
    </author>
    <author>
      <name>Joao Soares</name>
    </author>
    <author>
      <name>Ines Dutra</name>
    </author>
    <author>
      <name>Manuel E. Correia</name>
    </author>
    <author>
      <name>Behnam Shariati</name>
    </author>
    <author>
      <name>Rolando Martins</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPS-ISA62245.2024.00044</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPS-ISA62245.2024.00044" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.09449v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.09449v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.10458v1</id>
    <updated>2023-11-17T11:26:56Z</updated>
    <published>2023-11-17T11:26:56Z</published>
    <title>Memory Management Strategies for an Internet of Things System</title>
    <summary>  The rise of the Internet has brought about significant changes in our lives,
and the rapid expansion of the Internet of Things (IoT) is poised to have an
even more substantial impact by connecting a wide range of devices across
various application domains. IoT devices, especially low-end ones, are
constrained by limited memory and processing capabilities, necessitating
efficient memory management within IoT operating systems. This paper delves
into the importance of memory management in IoT systems, with a primary focus
on the design and configuration of such systems, as well as the scalability and
performance of scene management. Effective memory management is critical for
optimizing resource usage, responsiveness, and adaptability as the IoT
ecosystem continues to grow. The study offers insights into memory allocation,
scene execution, memory reduction, and system scalability within the context of
an IoT system, ultimately highlighting the vital role that memory management
plays in facilitating a seamless and efficient IoT experience.
</summary>
    <author>
      <name>Ana-Maria Comeagă</name>
    </author>
    <author>
      <name>Iuliana Marin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Symposium on Fundamentals of Electrical Engineering
  2023</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Symposium on Fundamentals of Electrical Engineering
  2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2311.10458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.10458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.04871v1</id>
    <updated>2023-12-08T07:13:03Z</updated>
    <published>2023-12-08T07:13:03Z</published>
    <title>SYSFLOW: Efficient Execution Platform for IoT Devices</title>
    <summary>  Traditional executable delivery models pose challenges for IoT devices with
limited storage, necessitating the download of complete executables and
dependencies. Network solutions like NFS, designed for data files, encounter
high IO overhead for irregular access patterns. This paper introduces SYSFLOW,
a lightweight network-based executable delivery system for IoT. SYSFLOW
delivers on-demand, redirecting local disk IO to the server through optimized
network IO. To optimize cache hit rates, SYSFLOW employs server-side
action-based prefetching, reducing latency by 45.1% to 75.8% compared to native
Linux filesystems on SD cards. In wired environments, SYSFLOW's latency is up
to 67.7% lower than NFS. In wireless scenarios, SYSFLOW performs 22.9% worse
than Linux, comparable with Linux and outperforming NFS by up to 60.7%. While
SYSFLOW's power consumption may be 6.7% higher than NFS, it offers energy
savings due to lower processing time.
</summary>
    <author>
      <name>Jun Lu</name>
    </author>
    <author>
      <name>Zhenya Ma</name>
    </author>
    <author>
      <name>Yinggang Gao</name>
    </author>
    <author>
      <name>Ju Ren</name>
    </author>
    <author>
      <name>Yaoxue Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2312.04871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.04871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.00316v1</id>
    <updated>2023-12-30T20:37:37Z</updated>
    <published>2023-12-30T20:37:37Z</published>
    <title>RASP for LSASS: Preventing Mimikatz-Related Attacks</title>
    <summary>  The Windows authentication infrastructure relies on the Local Security
Authority (LSA) system, with its integral component being lsass.exe.
Regrettably, this framework is not impervious, presenting vulnerabilities that
attract threat actors with malicious intent. By exploiting documented
vulnerabilities sourced from the CVE database or leveraging sophisticated tools
such as mimikatz, adversaries can successfully compromise user password-address
information.
  In this comprehensive analysis, we delve into proactive measures aimed at
fortifying the local authentication subsystem against potential threats.
Moreover, we present empirical evidence derived from practical assessments of
various defensive methodologies, including those articulated previously. This
examination not only underscores the importance of proactive security measures
but also assesses the practical efficacy of these strategies in real-world
contexts.
</summary>
    <author>
      <name>Anna Revazova</name>
    </author>
    <author>
      <name>Igor Korkin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.00316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.00316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68N25 (Primary) 00-02 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.01357v1</id>
    <updated>2023-12-11T21:26:55Z</updated>
    <published>2023-12-11T21:26:55Z</published>
    <title>Security, extensibility, and redundancy in the Metabolic Operating
  System</title>
    <summary>  People living with Type 1 Diabetes (T1D) lose the ability to produce insulin
naturally. To compensate, they inject synthetic insulin. One common way to
inject insulin is through automated insulin delivery systems, which use sensors
to monitor their metabolic state and an insulin pump device to adjust insulin
to adapt.
  In this paper, we present the Metabolic Operating System, a new automated
insulin delivery system that we designed from the ground up using security
first principles. From an architecture perspective, we apply separation
principles to simplify the core system and isolate non-critical functionality
from the core closed-loop algorithm. From an algorithmic perspective, we
evaluate trends in insulin technology and formulate a simple, but effective,
algorithm given the state-of-the-art. From a safety perspective, we build in
multiple layers of redundancy to ensure that the person using our system
remains safe.
  Fundamentally, this paper is a paper on real-world experiences building and
running an automated insulin delivery system. We report on the design
iterations we make based on experiences working with one individual using our
system. Our evaluation shows that an automated insulin delivery system built
from the ground up using security first principles can still help manage T1D
effectively.
  Our source code is open source and available on GitHub (link omitted).
</summary>
    <author>
      <name>Samuel T. King</name>
    </author>
    <link href="http://arxiv.org/abs/2401.01357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.01357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.01376v1</id>
    <updated>2023-12-30T16:58:28Z</updated>
    <published>2023-12-30T16:58:28Z</published>
    <title>ALPC Is In Danger: ALPChecker Detects Spoofing and Blinding</title>
    <summary>  The purpose of this study is to evaluate the possibility of implementing an
attack on ALPC connection in the Windows operating system through the kernel
without closing the connection covertly from programs and the operating system
and to propose a method of protection against this type of attacks.
Asynchronous Local Procedure Call technology (ALPC) is used in various Windows
information protection systems, including antivirus systems (AV) and Endpoint
Detection and Response systems (EDR). To ensure the concealment of malicious
software, attackers need to disrupt the operation of AV, EDR tools, which in
turn can be achieved by destructive impact on the components of the ALPC
technology. Examples of such attacks already exist and are covered in this
paper. To counteract such new threats, it is necessary to advance the
improvement of information security systems and the ALPC security research was
conducted. The most difficult case, Windows kernel driver attack, was
considered. Three attacks on the ALPC connection were carried out, based on
changing the ALPC structures in the kernel memory, which led to creation of
illegitimate connections in the system and the disruption of correct
connections. ALPChecker protection tool has been developed. The tool was
successfully tested on three demonstrated attacks.
</summary>
    <author>
      <name>Anastasiia Kropova</name>
    </author>
    <author>
      <name>Igor Korkin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.01376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.01376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68N25 (Primary) 00-02 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.08710v1</id>
    <updated>2024-01-15T12:04:04Z</updated>
    <published>2024-01-15T12:04:04Z</published>
    <title>Dynamic Voltage and Frequency Scaling for Intermittent Computing</title>
    <summary>  We present hardware/software techniques to intelligently regulate supply
voltage and clock frequency of intermittently-computing devices. These devices
rely on ambient energy harvesting to power their operation and small capacitors
as energy buffers. Statically setting their clock frequency fails to capture
the unique relations these devices expose between capacitor voltage, energy
efficiency at a given operating frequency, and the corresponding operating
range. Existing dynamic voltage and frequency scaling techniques are also
largely inapplicable due to extreme energy scarcity and peculiar hardware
features. We introduce two hardware/software co-designs that accommodate the
distinct hardware features and function within a constrained energy envelope,
offering varied trade-offs and functionalities. Our experimental evaluation
combines tests on custom-manufactured hardware and detailed emulation
experiments. The data gathered indicate that our approaches result in up to
3.75x reduced energy consumption and 12x swifter execution times compared to
the considered baselines, all while utilizing smaller capacitors to accomplish
identical workloads.
</summary>
    <author>
      <name>Andrea Maioli</name>
    </author>
    <author>
      <name>Kevin A. Quinones</name>
    </author>
    <author>
      <name>Saad Ahmed</name>
    </author>
    <author>
      <name>Muhammad H. Alizai</name>
    </author>
    <author>
      <name>Luca Mottola</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3714470</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3714470" rel="related"/>
    <link href="http://arxiv.org/abs/2401.08710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.08710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.11642v1</id>
    <updated>2024-01-22T01:06:55Z</updated>
    <published>2024-01-22T01:06:55Z</published>
    <title>SyzRetrospector: A Large-Scale Retrospective Study of Syzbot</title>
    <summary>  Over the past 6 years, Syzbot has fuzzed the Linux kernel day and night to
report over 5570 bugs, of which 4604 have been patched [11]. While this is
impressive, we have found the average time to find a bug is over 405 days.
Moreover, we have found that current metrics commonly used, such as
time-to-find and number of bugs found, are inaccurate in evaluating Syzbot
since bugs often spend the majority of their lives hidden from the fuzzer. In
this paper, we set out to better understand and quantify Syzbot's performance
and improvement in finding bugs. Our tool, SyzRetrospector, takes a different
approach to evaluating Syzbot by finding the earliest that Syzbot was capable
of finding a bug, and why that bug was revealed. We use SyzRetrospector on a
large scale to analyze 559 bugs and find that bugs are hidden for an average of
331.17 days before Syzbot is even able to find them. We further present
findings on the behaviors of revealing factors, how some bugs are harder to
reveal than others, the trends in delays over the past 6 years, and how bug
location relates to delays. We also provide key takeaways for improving
Syzbot's delays.
</summary>
    <author>
      <name>Joseph Bursey</name>
    </author>
    <author>
      <name>Ardalan Amiri Sani</name>
    </author>
    <author>
      <name>Zhiyun Qian</name>
    </author>
    <link href="http://arxiv.org/abs/2401.11642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.11642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.00365v1</id>
    <updated>2024-02-01T06:05:53Z</updated>
    <published>2024-02-01T06:05:53Z</published>
    <title>bypass4netns: Accelerating TCP/IP Communications in Rootless Containers</title>
    <summary>  "Rootless containers" is a concept to run the entire container runtimes and
containers without the root privileges. It protects the host environment from
attackers exploiting container runtime vulnerabilities. However, when rootless
containers communicate with external endpoints, the network performance is low
compared to rootful containers because of the overhead of rootless networking
components. In this paper, we propose bypass4netns that accelerates TCP/IP
communications in rootless containers by bypassing slow networking components.
bypass4netns uses sockets allocated on the host. It switches sockets in
containers to the host's sockets by intercepting syscalls and injecting the
file descriptors using Seccomp. Our method with Seccomp can handle statically
linked applications that previous works could not handle. Also, we propose
high-performance rootless multi-node communication. We confirmed that rootless
containers with bypass4netns achieve more than 30x faster throughput than
rootless containers without it. In addition, we evaluated performance with
applications and it showed large improvements on some applications.
</summary>
    <author>
      <name>Naoki Matsumoto</name>
    </author>
    <author>
      <name>Akihiro Suda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.00365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.00365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.06860v1</id>
    <updated>2024-02-10T01:50:15Z</updated>
    <published>2024-02-10T01:50:15Z</published>
    <title>Age-Memory Trade-off in Read-Copy-Update</title>
    <summary>  In the realm of shared memory systems, the challenge of reader-writer
synchronization is closely coupled with the potential for readers to access
outdated updates. Read-Copy-Update (RCU) is a synchronization primitive that
allows for concurrent and non-blocking read access to fresh data. This is
achieved through the creation of updated data copies, with each prior version
retained until all associated read-locks are released. Given the principle that
frequent updating keeps information fresh, the concern is whether we accumulate
an infinite number of update copies, leading to excessively large memory usage.
This paper analyzes trade-offs between memory usage and update age within
real-time status updating systems, focusing specifically on RCU. The analysis
demonstrates that with finite read time and read request rate, the average
number of updates within the system remains bounded.
</summary>
    <author>
      <name>Vishakha Ramani</name>
    </author>
    <author>
      <name>Jiachen Chen</name>
    </author>
    <author>
      <name>Roy D. Yates</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE INFOCOM Age and Semantics of Information (ASoI)
  Workshop 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.06860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.06860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.07033v3</id>
    <updated>2025-05-01T09:58:34Z</updated>
    <published>2024-02-10T19:54:08Z</published>
    <title>Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts
  Models</title>
    <summary>  Large Language Models (LLMs) with the Mixture-of-Experts (MoE) architectures
have shown promising performance on various tasks. However, due to the huge
model sizes, running them in resource-constrained environments where the GPU
memory is not abundant is challenging. Some existing systems propose to use CPU
resources to solve that, but they either suffer from the significant overhead
of frequently moving data between CPU and GPU, or fail to consider distinct
characteristics of CPUs and GPUs. This paper proposes Fiddler, a
resource-efficient inference system for MoE models with limited GPU resources.
Fiddler strategically utilizes CPU and GPU resources by determining the optimal
execution strategy. Our evaluation shows that, unlike state-of-the-art systems
that optimize for specific scenarios such as single batch inference or long
prefill, Fiddler performs better in all scenarios. Compared against different
baselines, Fiddler achieves 1.26 times speed up in single batch inference, 1.30
times in long prefill processing, and 11.57 times in beam search inference. The
code of Fiddler is publicly available at https://github.com/efeslab/fiddler.
</summary>
    <author>
      <name>Keisuke Kamahori</name>
    </author>
    <author>
      <name>Tian Tang</name>
    </author>
    <author>
      <name>Yile Gu</name>
    </author>
    <author>
      <name>Kan Zhu</name>
    </author>
    <author>
      <name>Baris Kasikci</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.07033v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.07033v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.13281v1</id>
    <updated>2024-02-18T15:45:38Z</updated>
    <published>2024-02-18T15:45:38Z</published>
    <title>Fight Hardware with Hardware: System-wide Detection and Mitigation of
  Side-Channel Attacks using Performance Counters</title>
    <summary>  We present a kernel-level infrastructure that allows system-wide detection of
malicious applications attempting to exploit cache-based side-channel attacks
to break the process confinement enforced by standard operating systems. This
infrastructure relies on hardware performance counters to collect information
at runtime from all applications running on the machine. High-level detection
metrics are derived from these measurements to maximize the likelihood of
promptly detecting a malicious application. Our experimental assessment shows
that we can catch a large family of side-channel attacks with a significantly
reduced overhead. We also discuss countermeasures that can be enacted once a
process is suspected of carrying out a side-channel attack to increase the
overall tradeoff between the system's security level and the delivered
performance under non-suspected process executions.
</summary>
    <author>
      <name>Stefano Carnà</name>
    </author>
    <author>
      <name>Serena Ferracci</name>
    </author>
    <author>
      <name>Francesco Quaglia</name>
    </author>
    <author>
      <name>Alessandro Pellegrini</name>
    </author>
    <link href="http://arxiv.org/abs/2402.13281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.13281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.08656v1</id>
    <updated>2024-03-13T16:10:04Z</updated>
    <published>2024-03-13T16:10:04Z</published>
    <title>Physical Memory Attacks and a Memory Safe Management System for Memory
  Defense</title>
    <summary>  Programming errors, defective hardware components (such as hard disk spindle
defects), and environmental hazards can lead to invalid memory operations. In
addition, less predictable forms of environmental stress, such as radiation,
thermal influence, and energy fluctuations, can induce hardware faults.
Sometimes, a soft error can occur instead of a complete failure, such as a
bit-flip. The 'natural' factors that can cause bit-flips are replicable through
targeted attacks that result in significant compromises, including full
privileged system access. Existing physical defense solutions have consistently
been circumvented shortly after deployment. We will explore the concept of a
novel software-based low-level layer that can protect vulnerable memory
targeted by physical attack vectors related to bit-flip vulnerabilities.
</summary>
    <author>
      <name>Alon Hillel-Tuch</name>
    </author>
    <author>
      <name>Aspen Olmstead</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Science, Computer Engineering, and Applied Computing (CSCE)
  Conference 2022</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Springer Nature - Book Series: Transactions on Computational
  Science &amp; Computational Intelligence 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2403.08656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.08656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.15884v1</id>
    <updated>2024-03-23T16:35:37Z</updated>
    <published>2024-03-23T16:35:37Z</published>
    <title>UPSS: a User-centric Private Storage System with its applications</title>
    <summary>  Strong confidentiality, integrity, user control, reliability and performance
are critical requirements in privacy-sensitive applications. Such applications
would benefit from a data storage and sharing infrastructure that provides
these properties even in decentralized topologies with untrusted storage
backends, but users today are forced to choose between systemic security
properties and system reliability or performance. As an alternative to this
status quo we present UPSS: the user-centric private sharing system, a
cryptographic storage system that can be used as a conventional filesystem or
as the foundation for security-sensitive applications such as redaction with
integrity and private revision control. We demonstrate that both the security
and performance properties of UPSS exceed that of existing cryptographic
filesystems and that its performance is comparable to mature conventional
filesystems - in some cases, even superior. Whether used directly via its Rust
API or as a conventional filesystem, UPSS provides strong security and
practical performance on untrusted storage.
</summary>
    <author>
      <name>Arastoo Bozorgi</name>
    </author>
    <author>
      <name>Mahya Soleimani Jadidi</name>
    </author>
    <author>
      <name>Jonathan Anderson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5220/0012306600003648</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5220/0012306600003648" rel="related"/>
    <link href="http://arxiv.org/abs/2403.15884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.15884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.08716v1</id>
    <updated>2024-04-12T04:55:13Z</updated>
    <published>2024-04-12T04:55:13Z</published>
    <title>Securing Monolithic Kernels using Compartmentalization</title>
    <summary>  Monolithic operating systems, where all kernel functionality resides in a
single, shared address space, are the foundation of most mainstream computer
systems. However, a single flaw, even in a non-essential part of the kernel
(e.g., device drivers), can cause the entire operating system to fall under an
attacker's control. Kernel hardening techniques might prevent certain types of
vulnerabilities, but they fail to address a fundamental weakness: the lack of
intra-kernel security that safely isolates different parts of the kernel. We
survey kernel compartmentalization techniques that define and enforce
intra-kernel boundaries and propose a taxonomy that allows the community to
compare and discuss future work. We also identify factors that complicate
comparisons among compartmentalized systems, suggest new ways to compare future
approaches with existing work meaningfully, and discuss emerging research
directions.
</summary>
    <author>
      <name>Soo Yee Lim</name>
    </author>
    <author>
      <name>Sidhartha Agrawal</name>
    </author>
    <author>
      <name>Xueyuan Han</name>
    </author>
    <author>
      <name>David Eyers</name>
    </author>
    <author>
      <name>Dan O'Keeffe</name>
    </author>
    <author>
      <name>Thomas Pasquier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.08716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.08716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.03883v1</id>
    <updated>2024-05-06T22:01:50Z</updated>
    <published>2024-05-06T22:01:50Z</published>
    <title>sqlelf: a SQL-centric Approach to ELF Analysis</title>
    <summary>  The exploration and understanding of Executable and Linkable Format (ELF)
objects underpin various critical activities in computer systems, from
debugging to reverse engineering. Traditional UNIX tooling like readelf, nm,
and objdump have served the community reliably over the years. However, as the
complexity and scale of software projects has grown, there arises a need for
more intuitive, flexible, and powerful methods to investigate ELF objects. In
this paper, we introduce sqlelf, an innovative tool that empowers users to
probe ELF objects through the expressive power of SQL. By modeling ELF objects
as relational databases, sqlelf offers the following advantages over
conventional methods.
  Our evaluations demonstrate that sqlelf not only provides more nuanced and
comprehensive insights into ELF objects but also significantly reduces the
effort and time traditionally required for ELF exploration tasks
</summary>
    <author>
      <name>Farid Zakaria</name>
    </author>
    <author>
      <name>Zheyuan Chen</name>
    </author>
    <author>
      <name>Andrew Quinn</name>
    </author>
    <author>
      <name>Thomas R. W. Scogland</name>
    </author>
    <link href="http://arxiv.org/abs/2405.03883v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.03883v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.04437v3</id>
    <updated>2025-01-29T04:10:41Z</updated>
    <published>2024-05-07T16:00:32Z</published>
    <title>vAttention: Dynamic Memory Management for Serving LLMs without
  PagedAttention</title>
    <summary>  PagedAttention is a popular approach for dynamic memory allocation in LLM
serving systems. It enables on-demand allocation of GPU memory to mitigate KV
cache fragmentation -- a phenomenon that crippled the batch size (and
consequently throughput) in prior systems. However, in trying to allocate
physical memory at runtime, PagedAttention ends up changing the virtual memory
layout of the KV cache from contiguous to non-contiguous. Such a design leads
to non-trivial programming and performance overheads.
  We present vAttention -- an approach that mitigates fragmentation in physical
memory while retaining the contiguity of KV cache in virtual memory. We achieve
this by decoupling the allocation of virtual and physical memory using CUDA
virtual memory management APIs. We also introduce various LLM-specific
optimizations to address the limitations of CUDA virtual memory support.
Overall, vAttention is a simpler, portable, and performant alternative to
PagedAttention: it supports various attention kernels out-of-the-box and
improves LLM serving throughput by up to 1.23x compared to the use of
PagedAttention-based kernels of FlashAttention and FlashInfer.
</summary>
    <author>
      <name>Ramya Prabhu</name>
    </author>
    <author>
      <name>Ajay Nayak</name>
    </author>
    <author>
      <name>Jayashree Mohan</name>
    </author>
    <author>
      <name>Ramachandran Ramjee</name>
    </author>
    <author>
      <name>Ashish Panwar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ASPLOS 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.04437v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.04437v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.08187v1</id>
    <updated>2024-05-13T21:09:32Z</updated>
    <published>2024-05-13T21:09:32Z</published>
    <title>Optimizing Task Scheduling in Heterogeneous Computing Environments: A
  Comparative Analysis of CPU, GPU, and ASIC Platforms Using E2C Simulator</title>
    <summary>  Efficient task scheduling in heterogeneous computing environments is
imperative for optimizing resource utilization and minimizing task completion
times. In this study, we conducted a comprehensive benchmarking analysis to
evaluate the performance of four scheduling algorithms First Come, First-Served
(FCFS), FCFS with No Queuing (FCFS-NQ), Minimum Expected Completion Time
(MECT), and Minimum Expected Execution Time (MEET) across varying workload
scenarios. We defined three workload scenarios: low, medium, and high, each
representing different levels of computational demands. Through rigorous
experimentation and analysis, we assessed the effectiveness of each algorithm
in terms of total completion percentage, energy consumption, wasted energy, and
energy per completion. Our findings highlight the strengths and limitations of
each algorithm, with MECT and MEET emerging as robust contenders, dynamically
prioritizing tasks based on comprehensive estimates of completion and execution
times. Furthermore, MECT and MEET exhibit superior energy efficiency compared
to FCFS and FCFS-NQ, underscoring their suitability for resource-constrained
environments. This study provides valuable insights into the efficacy of task
scheduling algorithms in heterogeneous computing environments, enabling
informed decision-making to enhance resource allocation, minimize task
completion times, and improve energy efficiency
</summary>
    <author>
      <name>Ali Mohammadjafari</name>
    </author>
    <author>
      <name>Poorya Khajouie</name>
    </author>
    <link href="http://arxiv.org/abs/2405.08187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.08187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.01339v1</id>
    <updated>2024-06-03T14:03:04Z</updated>
    <published>2024-06-03T14:03:04Z</published>
    <title>Recover as It is Designed to Be: Recovering from Compatibility Mobile
  App Crashes by Reusing User Flows</title>
    <summary>  Android OS is severely fragmented by API updates and device vendors' OS
customization, creating a market condition where vastly different OS versions
coexist. This gives rise to compatibility crash problems where Android apps
crash on certain Android versions but not on others. Although well-known, this
problem is extremely challenging for app developers to overcome due to the
sheer number of Android versions in the market that must be tested. We present
RecoFlow, a framework for enabling app developers to automatically recover an
app from a crash by programming user flows with our API and visual tools.
RecoFlow tracks app feature usage with the user flows on user devices and
recovers an app from a crash by replaying UI actions of the app feature
disrupted by the crash. To prevent recurring compatibility crashes, RecoFlow
executes a previously crashed app in compatibility mode that is enabled by our
novel Android OS virtualization technique. Our evaluation with professional
Android developers shows that our API and tools are easy to use and effective
in recovering from compatibility crashes.
</summary>
    <author>
      <name>Donghwi Kim</name>
    </author>
    <author>
      <name>Hyungjun Yoon</name>
    </author>
    <author>
      <name>Chang Min Park</name>
    </author>
    <author>
      <name>Sujin Han</name>
    </author>
    <author>
      <name>Youngjin Kwon</name>
    </author>
    <author>
      <name>Steven Y. Ko</name>
    </author>
    <author>
      <name>Sung-Ju Lee</name>
    </author>
    <link href="http://arxiv.org/abs/2406.01339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.01339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.10635v1</id>
    <updated>2024-06-15T13:39:20Z</updated>
    <published>2024-06-15T13:39:20Z</published>
    <title>ROSfs: A User-Level File System for ROS</title>
    <summary>  We present ROSfs, a novel user-level file system for the Robot Operating
System (ROS). ROSfs interprets a robot file as a group of sub-files, with each
having a distinct label. ROSfs applies a time index structure to enhance the
flexible data query while the data file is under modification. It provides
multi-robot systems (MRS) with prompt cross-robot data acquisition and
collaboration. We implemented a ROSfs prototype and integrated it into a
mainstream ROS platform. We then applied and evaluated ROSfs on real-world UAVs
and data servers. Evaluation results show that compared with traditional ROS
storage methods, ROSfs improves the offline query performance by up to 129x and
reduces inter-robot online data query latency under a wireless network by up to
7x.
</summary>
    <author>
      <name>Zijun Xu</name>
    </author>
    <author>
      <name>Xuanjun Wen</name>
    </author>
    <author>
      <name>Yanjie Song</name>
    </author>
    <author>
      <name>Shu Yin</name>
    </author>
    <link href="http://arxiv.org/abs/2406.10635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.10635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.16250v1</id>
    <updated>2024-06-24T01:10:20Z</updated>
    <published>2024-06-24T01:10:20Z</published>
    <title>Evaluating Serverless Machine Learning Performance on Google Cloud Run</title>
    <summary>  End-users can get functions-as-a-service from serverless platforms, which
promise lower hosting costs, high availability, fault tolerance, and dynamic
flexibility for hosting individual functions known as microservices. Machine
learning tools are seen to be reliably useful, and the services created using
these tools are in increasing demand on a large scale. The serverless platforms
are uniquely suited for hosting these machine learning services to be used for
large-scale applications. These platforms are well known for their cost
efficiency, fault tolerance, resource scaling, robust APIs for communication,
and global reach. However, machine learning services are different from the
web-services in that these serverless platforms were originally designed to
host web services. We aimed to understand how these serverless platforms handle
machine learning workloads with our study. We examine machine learning
performance on one of the serverless platforms - Google Cloud Run, which is a
GPU-less infrastructure that is not designed for machine learning application
deployment.
</summary>
    <author>
      <name>Prerana Khatiwada</name>
    </author>
    <author>
      <name>Pranjal Dhakal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.16250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.16250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.00832v1</id>
    <updated>2024-06-30T21:03:43Z</updated>
    <published>2024-06-30T21:03:43Z</published>
    <title>Boxer: FaaSt Ephemeral Elasticity for Off-the-Shelf Cloud Applications</title>
    <summary>  Elasticity is a key property of cloud computing. However, elasticity is
offered today at the granularity of virtual machines, which take tens of
seconds to start. This is insufficient to react to load spikes and sudden
failures in latency sensitive applications, leading users to resort to
expensive overprovisioning. Function-as-a-Service (FaaS) provides significantly
higher elasticity than VMs, but comes coupled with an event-triggered
programming model and a constrained execution environment that makes them
unsuitable for off-the-shelf applications. Previous work tries to overcome
these obstacles but often requires re-architecting the applications. In this
paper, we show how off-the-shelf applications can transparently benefit from
ephemeral elasticity with FaaS. We built Boxer, an interposition layer spanning
VMs and AWS Lambda, that intercepts application execution and emulates the
network-of-hosts environment that applications expect when deployed in a
conventional VM/container environment. The ephemeral elasticity of Boxer
enables significant performance and cost savings for off-the-shelf applications
with, e.g., recovery times over 5x faster than EC2 instances and absorbing load
spikes comparable to overprovisioned EC2 VM instances.
</summary>
    <author>
      <name>Michael Wawrzoniak</name>
    </author>
    <author>
      <name>Rodrigo Bruno</name>
    </author>
    <author>
      <name>Ana Klimovic</name>
    </author>
    <author>
      <name>Gustavo Alonso</name>
    </author>
    <link href="http://arxiv.org/abs/2407.00832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.00832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.05251v1</id>
    <updated>2024-08-08T20:20:36Z</updated>
    <published>2024-08-08T20:20:36Z</published>
    <title>Columbo: Low Level End-to-End System Traces through Modular Full-System
  Simulation</title>
    <summary>  Fully understanding performance is a growing challenge when building
next-generation cloud systems. Often these systems build on next-generation
hardware, and evaluation in realistic physical testbeds is out of reach. Even
when physical testbeds are available, visibility into essential system aspects
is a challenge in modern systems where system performance depends on often
sub-$\mu s$ interactions between HW and SW components. Existing tools such as
performance counters, logging, and distributed tracing provide aggregate or
sampled information, but remain insufficient for understanding individual
requests in-depth. In this paper, we explore a fundamentally different approach
to enable in-depth understanding of cloud system behavior at the software and
hardware level, with (almost) arbitrarily fine-grained visibility. Our proposal
is to run cloud systems in detailed full-system simulations, configure the
simulators to collect detailed events without affecting the system, and finally
assemble these events into end-to-end system traces that can be analyzed by
existing distributed tracing tools.
</summary>
    <author>
      <name>Jakob Görgen</name>
    </author>
    <author>
      <name>Vaastav Anand</name>
    </author>
    <author>
      <name>Hejing Li</name>
    </author>
    <author>
      <name>Jialin Li</name>
    </author>
    <author>
      <name>Antoine Kaufmann</name>
    </author>
    <link href="http://arxiv.org/abs/2408.05251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.05251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.06822v2</id>
    <updated>2024-08-15T08:35:59Z</updated>
    <published>2024-08-13T11:29:30Z</published>
    <title>CRISP: Confidentiality, Rollback, and Integrity Storage Protection for
  Confidential Cloud-Native Computing</title>
    <summary>  Trusted execution environments (TEEs) protect the integrity and
confidentiality of running code and its associated data. Nevertheless, TEEs'
integrity protection does not extend to the state saved on disk. Furthermore,
modern cloud-native applications heavily rely on orchestration (e.g., through
systems such as Kubernetes) and, thus, have their services frequently
restarted. During restarts, attackers can revert the state of confidential
services to a previous version that may aid their malicious intent. This paper
presents CRISP, a rollback protection mechanism that uses an existing runtime
for Intel SGX and transparently prevents rollback. Our approach can constrain
the attack window to a fixed and short period or give developers the tools to
avoid the vulnerability window altogether. Finally, experiments show that
applying CRISP in a critical stateful cloud-native application may incur a
resource increase but only a minor performance penalty.
</summary>
    <author>
      <name>Ardhi Putra Pratama Hartono</name>
    </author>
    <author>
      <name>Andrey Brito</name>
    </author>
    <author>
      <name>Christof Fetzer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CLOUD62652.2024.00026</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CLOUD62652.2024.00026" rel="related"/>
    <link href="http://arxiv.org/abs/2408.06822v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.06822v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.07378v2</id>
    <updated>2024-09-24T09:26:45Z</updated>
    <published>2024-08-14T08:54:29Z</published>
    <title>Inspection of I/O Operations from System Call Traces using
  Directly-Follows-Graph</title>
    <summary>  We aim to identify the differences in Input/Output(I/O) behavior between
multiple user programs through the inspection of system calls (i.e., requests
made to the operating system). A typical program issues a large number of I/O
requests to the operating system, thereby making the process of inspection
challenging. In this paper, we address this challenge by presenting a
methodology to synthesize I/O system call traces into a specific type of
directed graph, known as the Directly-Follows-Graph (DFG). Based on the DFG, we
present a technique to compare the traces from multiple programs or different
configurations of the same program, such that it is possible to identify the
differences in the I/O behavior. We apply our methodology to the IOR benchmark,
and compare the contentions for file accesses when the benchmark is run with
different options for file output and software interface.
</summary>
    <author>
      <name>Aravind Sankaran</name>
    </author>
    <author>
      <name>Ilya Zhukov</name>
    </author>
    <author>
      <name>Wolfgang Frings</name>
    </author>
    <author>
      <name>Paolo Bientinesi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SCW63240.2024.00196</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SCW63240.2024.00196" rel="related"/>
    <link href="http://arxiv.org/abs/2408.07378v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.07378v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.11173v1</id>
    <updated>2024-08-20T20:11:33Z</updated>
    <published>2024-08-20T20:11:33Z</published>
    <title>Delegation with Trust&lt;T&gt;: A Scalable, Type- and Memory-Safe Alternative
  to Locks</title>
    <summary>  We present Trust&lt;T&gt;, a general, type- and memory-safe alternative to locking
in concurrent programs. Instead of synchronizing multi-threaded access to an
object of type T with a lock, the programmer may place the object in a
Trust&lt;T&gt;. The object is then no longer directly accessible. Instead a
designated thread, the object's trustee, is responsible for applying any
requested operations to the object, as requested via the Trust&lt;T&gt; API. Locking
is often said to offer a limited throughput per lock. Trust&lt;T&gt; is based on
delegation, a message-passing technique which does not suffer this per-lock
limitation. Instead, per-object throughput is limited by the capacity of the
object's trustee, which is typically considerably higher. Our evaluation shows
Trust&lt;T&gt; consistently and considerably outperforming locking where lock
contention exists, with up to 22x higher throughput in microbenchmarks, and
5-9x for a home grown key-value store, as well as memcached, in situations with
high lock contention. Moreover, Trust&lt;T&gt; is competitive with locks even in the
absence of lock contention.
</summary>
    <author>
      <name>Noaman Ahmad</name>
    </author>
    <author>
      <name>Ben Baenen</name>
    </author>
    <author>
      <name>Chen Chen</name>
    </author>
    <author>
      <name>Jakob Eriksson</name>
    </author>
    <link href="http://arxiv.org/abs/2408.11173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.11173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.04484v1</id>
    <updated>2024-09-06T07:11:44Z</updated>
    <published>2024-09-06T07:11:44Z</published>
    <title>The HitchHiker's Guide to High-Assurance System Observability Protection
  with Efficient Permission Switches</title>
    <summary>  Protecting system observability records (logs) from compromised OSs has
gained significant traction in recent times, with several note-worthy
approaches proposed. Unfortunately, none of the proposed approaches achieve
high performance with tiny log protection delays. They also leverage risky
environments for protection (\eg many use general-purpose hypervisors or
TrustZone, which have large TCB and attack surfaces). HitchHiker is an attempt
to rectify this problem. The system is designed to ensure (a) in-memory
protection of batched logs within a short and configurable real-time deadline
by efficient hardware permission switching, and (b) an end-to-end
high-assurance environment built upon hardware protection primitives with
debloating strategies for secure log protection, persistence, and management.
Security evaluations and validations show that HitchHiker reduces log
protection delay by 93.3--99.3% compared to the state-of-the-art, while
reducing TCB by 9.4--26.9X. Performance evaluations show HitchHiker incurs a
geometric mean of less than 6% overhead on diverse real-world programs,
improving on the state-of-the-art approach by 61.9--77.5%.
</summary>
    <author>
      <name>Chuqi Zhang</name>
    </author>
    <author>
      <name>Jun Zeng</name>
    </author>
    <author>
      <name>Yiming Zhang</name>
    </author>
    <author>
      <name>Adil Ahmad</name>
    </author>
    <author>
      <name>Fengwei Zhang</name>
    </author>
    <author>
      <name>Hai Jin</name>
    </author>
    <author>
      <name>Zhenkai Liang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3658644.3690188</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3658644.3690188" rel="related"/>
    <link href="http://arxiv.org/abs/2409.04484v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.04484v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.07508v1</id>
    <updated>2024-09-11T13:58:51Z</updated>
    <published>2024-09-11T13:58:51Z</published>
    <title>SafeBPF: Hardware-assisted Defense-in-depth for eBPF Kernel Extensions</title>
    <summary>  The eBPF framework enables execution of user-provided code in the Linux
kernel. In the last few years, a large ecosystem of cloud services has
leveraged eBPF to enhance container security, system observability, and network
management. Meanwhile, incessant discoveries of memory safety vulnerabilities
have left the systems community with no choice but to disallow unprivileged
eBPF programs, which unfortunately limits eBPF use to only privileged users. To
improve run-time safety of the framework, we introduce SafeBPF, a general
design that isolates eBPF programs from the rest of the kernel to prevent
memory safety vulnerabilities from being exploited. We present a pure software
implementation using a Software-based Fault Isolation (SFI) approach and a
hardware-assisted implementation that leverages ARM's Memory Tagging Extension
(MTE). We show that SafeBPF incurs up to 4% overhead on macrobenchmarks while
achieving desired security properties.
</summary>
    <author>
      <name>Soo Yee Lim</name>
    </author>
    <author>
      <name>Tanya Prasad</name>
    </author>
    <author>
      <name>Xueyuan Han</name>
    </author>
    <author>
      <name>Thomas Pasquier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.07508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.07508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.08618v1</id>
    <updated>2024-10-11T08:33:58Z</updated>
    <published>2024-10-11T08:33:58Z</published>
    <title>AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems
  with In-Network Coordination</title>
    <summary>  Distributed filesystems typically employ synchronous metadata updates, facing
inherent challenges for access efficiency, load balancing, and directory
contention, especially under dynamic and skewed workloads. This paper argues
that synchronous updates are overly conservative for distributed filesystems.
We propose AsyncFS with asynchronous metadata updates, allowing operations to
return early and defer directory updates until respective read to enable
latency hiding and conflict resolution. The key challenge is efficiently
maintaining the synchronous semantics of metadata updates. To address this,
AsyncFS is co-designed with a programmable switch, leveraging the constrained
on-switch resources to holistically track directory states in the network with
negligible cost. This allows AsyncFS to timely aggregate and efficiently apply
delayed updates using batching and consolidation before directory reads.
Evaluation shows that AsyncFS achieves up to 13.34$\times$ and 3.85$\times$
higher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art
distributed filesystems, InfiniFS and CFS-KV, respectively, on skewed
workloads. For real-world workloads, AsyncFS improves end-to-end throughput by
21.1$\times$, 1.1$\times$ and 30.1% over Ceph, IndexFS and CFS-KV,
respectively.
</summary>
    <author>
      <name>Jingwei Xu</name>
    </author>
    <author>
      <name>Mingkai Dong</name>
    </author>
    <author>
      <name>Qiulin Tian</name>
    </author>
    <author>
      <name>Ziyi Tian</name>
    </author>
    <author>
      <name>Tong Xin</name>
    </author>
    <author>
      <name>Haibo Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2410.08618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.08618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.15492v1</id>
    <updated>2024-10-20T20:13:46Z</updated>
    <published>2024-10-20T20:13:46Z</published>
    <title>Reinforcement Learning for Dynamic Memory Allocation</title>
    <summary>  In recent years, reinforcement learning (RL) has gained popularity and has
been applied to a wide range of tasks. One such popular domain where RL has
been effective is resource management problems in systems. We look to extend
work on RL for resource management problems by considering the novel domain of
dynamic memory allocation management. We consider dynamic memory allocation to
be a suitable domain for RL since current algorithms like first-fit, best-fit,
and worst-fit can fail to adapt to changing conditions and can lead to
fragmentation and suboptimal efficiency. In this paper, we present a framework
in which an RL agent continuously learns from interactions with the system to
improve memory management tactics. We evaluate our approach through various
experiments using high-level and low-level action spaces and examine different
memory allocation patterns. Our results show that RL can successfully train
agents that can match and surpass traditional allocation strategies,
particularly in environments characterized by adversarial request patterns. We
also explore the potential of history-aware policies that leverage previous
allocation requests to enhance the allocator's ability to handle complex
request patterns. Overall, we find that RL offers a promising avenue for
developing more adaptive and efficient memory allocation strategies,
potentially overcoming limitations of hardcoded allocation algorithms.
</summary>
    <author>
      <name>Arisrei Lim</name>
    </author>
    <author>
      <name>Abhiram Maddukuri</name>
    </author>
    <link href="http://arxiv.org/abs/2410.15492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.15492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.17465v1</id>
    <updated>2024-10-22T22:49:01Z</updated>
    <published>2024-10-22T22:49:01Z</published>
    <title>Bauplan: zero-copy, scale-up FaaS for data pipelines</title>
    <summary>  Chaining functions for longer workloads is a key use case for FaaS platforms
in data applications. However, modern data pipelines differ significantly from
typical serverless use cases (e.g., webhooks and microservices); this makes it
difficult to retrofit existing pipeline frameworks due to structural
constraints. In this paper, we describe these limitations in detail and
introduce bauplan, a novel FaaS programming model and serverless runtime
designed for data practitioners. bauplan enables users to declaratively define
functional Directed Acyclic Graphs (DAGs) along with their runtime
environments, which are then efficiently executed on cloud-based workers. We
show that bauplan achieves both better performance and a superior developer
experience for data workloads by making the trade-off of reducing generality in
favor of data-awareness
</summary>
    <author>
      <name>Jacopo Tagliabue</name>
    </author>
    <author>
      <name>Tyler Caraza-Harter</name>
    </author>
    <author>
      <name>Ciro Greco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for the 10th International Workshop on Serverless Computing
  (pre-print)</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.17465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.17465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.17577v1</id>
    <updated>2024-10-23T06:04:04Z</updated>
    <published>2024-10-23T06:04:04Z</published>
    <title>Arcus: SLO Management for Accelerators in the Cloud with Traffic Shaping</title>
    <summary>  Cloud servers use accelerators for common tasks (e.g., encryption,
compression, hashing) to improve CPU/GPU efficiency and overall performance.
However, users' Service-level Objectives (SLOs) can be violated due to
accelerator-related contention. The root cause is that existing solutions for
accelerators only focus on isolation or fair allocation of compute and memory
resources; they overlook the contention for communication-related resources.
Specifically, three communication-induced challenges drive us to re-think the
problem: (1) Accelerator traffic patterns are diverse, hard to predict, and
mixed across users, (2) communication-related components lack effective
low-level isolation mechanism to configure, and (3) computational heterogeneity
of accelerators lead to unique relationships between the traffic mixture and
the corresponding accelerator performance. The focus of this work is meeting
SLOs in accelerator-rich systems. We present \design{}, treating accelerator
SLO management as traffic management with proactive traffic shaping. We develop
an SLO-aware protocol coupled with an offloaded interface on an architecture
that supports precise and scalable traffic shaping. We guarantee accelerator
SLO for various circumstances, with up to 45% tail latency reduction and less
than 1% throughput variance.
</summary>
    <author>
      <name>Jiechen Zhao</name>
    </author>
    <author>
      <name>Ran Shu</name>
    </author>
    <author>
      <name>Katie Lim</name>
    </author>
    <author>
      <name>Zewen Fan</name>
    </author>
    <author>
      <name>Thomas Anderson</name>
    </author>
    <author>
      <name>Mingyu Gao</name>
    </author>
    <author>
      <name>Natalie Enright Jerger</name>
    </author>
    <link href="http://arxiv.org/abs/2410.17577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.17577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.02814v1</id>
    <updated>2024-11-05T05:22:14Z</updated>
    <published>2024-11-05T05:22:14Z</published>
    <title>The Hitchhiker's Guide to Programming and Optimizing CXL-Based
  Heterogeneous Systems</title>
    <summary>  We present a thorough analysis of the use of CXL-based heterogeneous systems.
We built a cluster of server systems that combines different vendor's CPUs and
various types of CXL devices. We further developed a heterogeneous memory
benchmark suite, Heimdall, to profile the performance of such heterogeneous
systems. By leveraging Heimdall, we unveiled the detailed architecture design
in these systems, drew observations on optimizing performance for workloads,
and pointed out directions for future development of CXL-based heterogeneous
systems.
</summary>
    <author>
      <name>Zixuan Wang</name>
    </author>
    <author>
      <name>Suyash Mahar</name>
    </author>
    <author>
      <name>Luyi Li</name>
    </author>
    <author>
      <name>Jangseon Park</name>
    </author>
    <author>
      <name>Jinpyo Kim</name>
    </author>
    <author>
      <name>Theodore Michailidis</name>
    </author>
    <author>
      <name>Yue Pan</name>
    </author>
    <author>
      <name>Tajana Rosing</name>
    </author>
    <author>
      <name>Dean Tullsen</name>
    </author>
    <author>
      <name>Steven Swanson</name>
    </author>
    <author>
      <name>Kyung Chang Ryoo</name>
    </author>
    <author>
      <name>Sungjoo Park</name>
    </author>
    <author>
      <name>Jishen Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/2411.02814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.02814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.17710v1</id>
    <updated>2024-11-15T08:47:08Z</updated>
    <published>2024-11-15T08:47:08Z</published>
    <title>Agent Centric Operating System -- a Comprehensive Review and Outlook for
  Operating System</title>
    <summary>  The operating system (OS) is the backbone of modern computing, providing
essential services and managing resources for computer hardware and software.
This review paper offers an in-depth analysis of operating systems' evolution,
current state, and prospects. We begin with an overview of the concept and
significance of operating systems in the digital era. In the second section, we
delve into the existing released operating systems, examining their
architectures, functionalities, and the ecosystems they support. We then
explore recent advances in OS evolution, highlighting innovations in real-time
processing, distributed computing, and security. The third section focuses on
the new era of operating systems, discussing emerging trends like the Internet
of Things (IoT), cloud computing, and artificial intelligence (AI) integration.
We also consider the challenges and opportunities presented by these
developments. This review concludes with a synthesis of the current landscape
and a forward-looking discussion on the future trajectories of operating
systems, including open issues and areas ripe for further research and
innovation. Finally, we put forward a new OS architecture.
</summary>
    <author>
      <name>Shian Jia</name>
    </author>
    <author>
      <name>Xinbo Wang</name>
    </author>
    <author>
      <name>Mingli Song</name>
    </author>
    <author>
      <name>Gang Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2411.17710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.17710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.02110v2</id>
    <updated>2024-12-04T02:47:40Z</updated>
    <published>2024-12-03T03:08:27Z</published>
    <title>Retrofitting XoM for Stripped Binaries without Embedded Data Relocation</title>
    <summary>  In this paper, we present PXoM, a practical technique to seamlessly retrofit
XoM into stripped binaries on the x86-64 platform. As handling the mixture of
code and data is a well-known challenge for XoM, most existing methods require
the strict separation of code and data areas via either compile-time
transformation or binary patching, so that the unreadable permission can be
safely enforced at the granularity of memory pages. In contrast to previous
approaches, we provide a fine-grained memory permission control mechanism to
restrict the read permission of code while allowing legitimate data reads
within code pages. This novelty enables PXoM to harden stripped binaries but
without resorting to error-prone embedded data relocation. We leverage Intel's
hardware feature, Memory Protection Keys, to offer an efficient fine-grained
permission control. We measure PXoM's performance with both micro- and
macro-benchmarks, and it only introduces negligible runtime overhead. Our
security evaluation shows that PXoM leaves adversaries with little wiggle room
to harvest all of the required gadgets, suggesting PXoM is practical for
real-world deployment.
</summary>
    <author>
      <name>Chenke Luo</name>
    </author>
    <author>
      <name>Jiang Ming</name>
    </author>
    <author>
      <name>Mengfei Xie</name>
    </author>
    <author>
      <name>Guojun Peng</name>
    </author>
    <author>
      <name>Jianming Fu</name>
    </author>
    <link href="http://arxiv.org/abs/2412.02110v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.02110v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.12579v1</id>
    <updated>2024-12-17T06:18:56Z</updated>
    <published>2024-12-17T06:18:56Z</published>
    <title>Scaling Inter-procedural Dataflow Analysis on the Cloud</title>
    <summary>  Apart from forming the backbone of compiler optimization, static dataflow
analysis has been widely applied in a vast variety of applications, such as bug
detection, privacy analysis, program comprehension, etc. Despite its
importance, performing interprocedural dataflow analysis on large-scale
programs is well known to be challenging. In this paper, we propose a novel
distributed analysis framework supporting the general interprocedural dataflow
analysis. Inspired by large-scale graph processing, we devise dedicated
distributed worklist algorithms for both whole-program analysis and incremental
analysis. We implement these algorithms and develop a distributed framework
called BigDataflow running on a large-scale cluster. The experimental results
validate the promising performance of BigDataflow -- BigDataflow can finish
analyzing the program of millions lines of code in minutes. Compared with the
state-of-the-art, BigDataflow achieves much more analysis efficiency.
</summary>
    <author>
      <name>Zewen Sun</name>
    </author>
    <author>
      <name>Yujin Zhang</name>
    </author>
    <author>
      <name>Duanchen Xu</name>
    </author>
    <author>
      <name>Yiyu Zhang</name>
    </author>
    <author>
      <name>Yun Qi</name>
    </author>
    <author>
      <name>Yueyang Wang</name>
    </author>
    <author>
      <name>Yi Li</name>
    </author>
    <author>
      <name>Zhaokang Wang</name>
    </author>
    <author>
      <name>Yue Li</name>
    </author>
    <author>
      <name>Xuandong Li</name>
    </author>
    <author>
      <name>Zhiqiang Zuo</name>
    </author>
    <author>
      <name>Qingda Lu</name>
    </author>
    <author>
      <name>Wenwen Peng</name>
    </author>
    <author>
      <name>Shengjian Guo</name>
    </author>
    <link href="http://arxiv.org/abs/2412.12579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.12579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.03427v1</id>
    <updated>2025-01-06T23:15:26Z</updated>
    <published>2025-01-06T23:15:26Z</published>
    <title>Boosting Cross-Architectural Emulation Performance by Foregoing the
  Intermediate Representation Model</title>
    <summary>  As more applications utilize virtualization and emulation to run
mission-critical tasks, the performance requirements of emulated and
virtualized platforms continue to rise. Hardware virtualization is not
universally available for all systems, and is incapable of emulating CPU
architectures, requiring software emulation to be used. QEMU, the premier
cross-architecture emulator for Linux and some BSD systems, currently uses
dynamic binary translation (DBT) through intermediate representations using its
Tiny Code Generator (TCG) model. While using intermediate representations of
translated code allows QEMU to quickly add new host and guest architectures, it
creates additional steps in the emulation pipeline which decrease performance.
We construct a proof of concept emulator to demonstrate the slowdown caused by
the usage of intermediate representations in TCG; this emulator performed up to
35x faster than QEMU with TCG, indicating substantial room for improvement in
QEMU's design. We propose an expansion of QEMU's two-tier engine system (Linux
KVM versus TCG) to include a middle tier using direct binary translation for
commonly paired architectures such as RISC-V, x86, and ARM. This approach
provides a slidable trade-off between development effort and performance
depending on the needs of end users.
</summary>
    <author>
      <name>Amy Iris Parker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures. Submitted to the 5th International Conference on
  Electrical, Computer and Energy Technologies</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.03427v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.03427v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.8; D.4.7; C.1.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.08249v1</id>
    <updated>2025-01-14T16:40:05Z</updated>
    <published>2025-01-14T16:40:05Z</published>
    <title>Verifying Device Drivers with Pancake</title>
    <summary>  Device driver bugs are the leading cause of OS compromises, and their formal
verification is therefore highly desirable. To the best of our knowledge, no
realistic and performant driver has been verified for a non-trivial device. We
propose Pancake, an imperative language for systems programming that features a
well-defined and verification-friendly semantics. Leveraging the verified
compiler backend of the CakeML functional language, we develop a compiler for
Pancake that guarantees that the binary retains the semantics of the source
code. Usng automatic translation of Pancake to the Viper SMT front-end, we
verify a performant driver for an Ethernet NIC.
</summary>
    <author>
      <name>Junming Zhao</name>
    </author>
    <author>
      <name>Alessandro Legnani</name>
    </author>
    <author>
      <name>Tiana Tsang Ung</name>
    </author>
    <author>
      <name>H. Truong</name>
    </author>
    <author>
      <name>Tsun Wang Sau</name>
    </author>
    <author>
      <name>Miki Tanaka</name>
    </author>
    <author>
      <name>Johannes Åman Pohjola</name>
    </author>
    <author>
      <name>Thomas Sewell</name>
    </author>
    <author>
      <name>Rob Sison</name>
    </author>
    <author>
      <name>Hira Syeda</name>
    </author>
    <author>
      <name>Magnus Myreen</name>
    </author>
    <author>
      <name>Michael Norrish</name>
    </author>
    <author>
      <name>Gernot Heiser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.08249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.08249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.4; D.3.4; D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.10002v1</id>
    <updated>2025-01-17T07:37:15Z</updated>
    <published>2025-01-17T07:37:15Z</published>
    <title>SyzParam: Introducing Runtime Parameters into Kernel Driver Fuzzing</title>
    <summary>  This paper introduces a novel fuzzing framework, SyzParam which incorporates
runtime parameters into the fuzzing process. Achieving this objective requires
addressing several key challenges, including valid value extraction,
inter-device relation construction, and fuzz engine integration. By inspecting
the data structures and functions associated with the LKDM, our tool can
extract runtime parameters across various drivers through static analysis.
Additionally, SyzParam collects inter-device relations and identifies
associations between runtime parameters and drivers. Furthermore, SyzParam
proposes a novel mutation strategy, which leverages these relations and
prioritizes parameter modification during related driver execution. Our
evaluation demonstrates that SyzParam outperforms existing fuzzing works in
driver code coverage and bug-detection capabilities. To date, we have
identified 30 unique bugs in the latest kernel upstreams, with 20 confirmed and
14 patched into the mainline kernel, including 9 CVEs.
</summary>
    <author>
      <name>Yue Sun</name>
    </author>
    <author>
      <name>Yan Kang</name>
    </author>
    <author>
      <name>Chenggang Wu</name>
    </author>
    <author>
      <name>Kangjie Lu</name>
    </author>
    <author>
      <name>Jiming Wang</name>
    </author>
    <author>
      <name>Xingwei Li</name>
    </author>
    <author>
      <name>Yuhao Hu</name>
    </author>
    <author>
      <name>Jikai Ren</name>
    </author>
    <author>
      <name>Yuanming Lai</name>
    </author>
    <author>
      <name>Mengyao Xie</name>
    </author>
    <author>
      <name>Zhe Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.10002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.10002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.11366v1</id>
    <updated>2025-01-20T09:55:15Z</updated>
    <published>2025-01-20T09:55:15Z</published>
    <title>Towards Online Code Specialization of Systems</title>
    <summary>  Specializing low-level systems to specifics of the workload they serve and
platform they are running on often significantly improves performance. However,
specializing systems is difficult because of three compounding challenges: i)
specialization for optimal performance requires in-depth compile-time changes;
ii) the right combination of specialization choices for optimal performance is
hard to predict a priori; and iii) workloads and platform details often change
online. In practice, benefits of specialization are thus not attainable for
many low-level systems. To address this, we advocate for a radically different
approach for performance-critical low-level systems: designing and implementing
systems with and for runtime code specialization. We leverage just-in-time
compilation to change systems code based on developer-specified specialization
points as the system runs. The JIT runtime automatically tries out
specialization choices and measures their impact on system performance, e.g.
request latency or throughput, to guide the search. With Iridescent, our early
prototype, we demonstrate that online specialization (i) is feasible even for
low-level systems code, such as network stacks, (ii) improves system
performance without the need for complex cost models, (iii) incurs low
developer effort, especially compared to manual exploration. We conclude with
future opportunities online system code specialization enables.
</summary>
    <author>
      <name>Vaastav Anand</name>
    </author>
    <author>
      <name>Deepak Garg</name>
    </author>
    <author>
      <name>Antoine Kaufmann</name>
    </author>
    <link href="http://arxiv.org/abs/2501.11366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.11366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.14771v2</id>
    <updated>2025-01-28T20:33:43Z</updated>
    <published>2024-12-29T17:39:37Z</published>
    <title>Dynamic Adaptation in Data Storage: Real-Time Machine Learning for
  Enhanced Prefetching</title>
    <summary>  The exponential growth of data storage demands has necessitated the evolution
of hierarchical storage management strategies [1]. This study explores the
application of streaming machine learning [3] to revolutionize data prefetching
within multi-tiered storage systems. Unlike traditional batch-trained models,
streaming machine learning [5] offers adaptability, real-time insights, and
computational efficiency, responding dynamically to workload variations. This
work designs and validates an innovative framework that integrates streaming
classification models for predicting file access patterns, specifically the
next file offset. Leveraging comprehensive feature engineering and real-time
evaluation over extensive production traces, the proposed methodology achieves
substantial improvements in prediction accuracy, memory efficiency, and system
adaptability. The results underscore the potential of streaming models in
real-time storage management, setting a precedent for advanced caching and
tiering strategies.
</summary>
    <author>
      <name>Chiyu Cheng</name>
    </author>
    <author>
      <name>Chang Zhou</name>
    </author>
    <author>
      <name>Yang Zhao</name>
    </author>
    <author>
      <name>Jin Cao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">I uploaded the paper without obtaining consent from all the authors.
  One of the authors now refuses to publish this paper, as it has been
  demonstrated to be unreliable, contains significant flaws in prior research,
  and is missing proper citations in Sections 2 and 3</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.14771v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.14771v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.19167v1</id>
    <updated>2025-01-31T14:33:29Z</updated>
    <published>2025-01-31T14:33:29Z</published>
    <title>On Measuring Available Capacity in High-speed Cloud Networks</title>
    <summary>  Measurement of available path capacity with high accuracy over high-speed
links deployed in cloud and transport networks is vital for performance
assessment and traffic engineering. Methods for measuring the available path
capacity rely on sending and receiving time stamped probe packets. A
requirement for accurate estimates of the available path capacity is the
ability to generate probe packets at a desired rate and also time stamping with
high precision and accuracy. This is challenging especially for measurement
systems deployed using general purpose hardware. To touch upon the challenge
this paper describes and evaluates four approaches for sending and receiving
probe packets in high-speed networks (10+ Gbps). The evaluation shows that the
baseline approach, based on the native UDP socket, is suitable for available
path capacity measurements over links with capacities up to 2.5 Gbps. For
higher capacities we show that an implementation based on Data Plane
Development Kit (DPDK) gives good results up to 10 Gbps.
</summary>
    <author>
      <name>Ganapathy Raman Madanagopal</name>
    </author>
    <author>
      <name>Christofer Flinta</name>
    </author>
    <author>
      <name>Andreas Johnsson</name>
    </author>
    <author>
      <name>Farnaz Moradi</name>
    </author>
    <author>
      <name>Daniel Turull</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages including reference. 3 figures, 2 tables with detailed
  experiements</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.19167v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.19167v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.02349v1</id>
    <updated>2025-02-04T14:33:44Z</updated>
    <published>2025-02-04T14:33:44Z</published>
    <title>Random Adaptive Cache Placement Policy</title>
    <summary>  This paper presents a new hybrid cache replacement algorithm that combines
random allocation with a modified V-Way cache implementation. Our RAC adapts to
complex cache access patterns and optimizes cache usage by improving the
utilization of cache sets, unlike traditional cache policies. The algorithm
utilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic
allocation and flexible tag management. RAC extends the V-Way cache design and
its variants by optimizing tag and data storage for enhanced efficiency.
  We evaluated the algorithm using the ChampSim simulator with four diverse
benchmark traces and observed significant improvements in cache hit rates up to
80.82% hit rate. Although the improvements in the instructions per cycle (IPC)
were moderate, our findings emphasize the algorithm's potential to enhance
cache utilization and reduce memory access times.
</summary>
    <author>
      <name>Vrushank Ahire</name>
    </author>
    <author>
      <name>Pranav Menon</name>
    </author>
    <author>
      <name>Aniruddh Muley</name>
    </author>
    <author>
      <name>Abhinandan S. Prasad</name>
    </author>
    <link href="http://arxiv.org/abs/2502.02349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.02349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.09281v1</id>
    <updated>2025-02-13T12:51:15Z</updated>
    <published>2025-02-13T12:51:15Z</published>
    <title>Fast Userspace Networking for the Rest of Us</title>
    <summary>  After a decade of research in userspace network stacks, why do new solutions
remain inaccessible to most developers? We argue that this is because they
ignored (1) the hardware constraints of public cloud NICs (vNICs) and (2) the
flexibility required by applications. Concerning the former, state-of-the-art
proposals rely on specific NIC features (e.g., flow steering, deep buffers)
that are not broadly available in vNICs. As for the latter, most of these
stacks enforce a restrictive execution model that does not align well with
cloud application requirements.
  We propose a new userspace network stack, Machnet, built for public cloud
VMs. Central to Machnet is a new ''Least Common Denominator'' model, a
conceptual NIC with a minimal feature set supported by all kernel-bypass vNICs.
The challenge is to build a new solution with performance comparable to
existing stacks while relying only on basic features (e.g., no flow steering,
no RSS reconfiguration). Machnet uses a microkernel design to provide higher
flexibility in application execution compared to a library OS design; we show
that microkernels' inter-process communication overhead is negligible on large
cloud networks.
</summary>
    <author>
      <name>Alireza Sanaee</name>
    </author>
    <author>
      <name>Vahab Jabrayilov</name>
    </author>
    <author>
      <name>Ilias Marinos</name>
    </author>
    <author>
      <name>Anuj Kalia</name>
    </author>
    <author>
      <name>Divyanshu Saxena</name>
    </author>
    <author>
      <name>Prateesh Goyal</name>
    </author>
    <author>
      <name>Kostis Kaffes</name>
    </author>
    <author>
      <name>Gianni Antichi</name>
    </author>
    <link href="http://arxiv.org/abs/2502.09281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.09281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.15712v1</id>
    <updated>2025-01-22T18:32:12Z</updated>
    <published>2025-01-22T18:32:12Z</published>
    <title>GPUs, CPUs, and... NICs: Rethinking the Network's Role in Serving
  Complex AI Pipelines</title>
    <summary>  The increasing prominence of AI necessitates the deployment of inference
platforms for efficient and effective management of AI pipelines and compute
resources. As these pipelines grow in complexity, the demand for distributed
serving rises and introduces much-dreaded network delays. In this paper, we
investigate how the network can instead be a boon to the excessively high
resource overheads of AI pipelines. To alleviate these overheads, we discuss
how resource-intensive data processing tasks -- a key facet of growing AI
pipeline complexity -- are well-matched for the computational characteristics
of packet processing pipelines and how they can be offloaded onto SmartNICs. We
explore the challenges and opportunities of offloading, and propose a research
agenda for integrating network hardware into AI pipelines, unlocking new
opportunities for optimization.
</summary>
    <author>
      <name>Mike Wong</name>
    </author>
    <author>
      <name>Ulysses Butler</name>
    </author>
    <author>
      <name>Emma Farkash</name>
    </author>
    <author>
      <name>Praveen Tammana</name>
    </author>
    <author>
      <name>Anirudh Sivaraman</name>
    </author>
    <author>
      <name>Ravi Netravali</name>
    </author>
    <link href="http://arxiv.org/abs/2502.15712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.15712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.17296v1</id>
    <updated>2025-02-24T16:25:20Z</updated>
    <published>2025-02-24T16:25:20Z</published>
    <title>Qoala: an Application Execution Environment for Quantum Internet Nodes</title>
    <summary>  Recently, a first-of-its-kind operating system for programmable quantum
network nodes was developed, called QNodeOS. Here, we present an extension of
QNodeOS called Qoala, which introduces (1) a unified program format for hybrid
interactive classical-quantum programs, providing a well-defined target for
compilers, and (2) a runtime representation of a program that allows joint
scheduling of the hybrid classical-quantum program, multitasking, and
asynchronous program execution. Based on concrete design considerations, we put
forward the architecture of Qoala, including the program structure and
execution mechanism. We implement Qoala in the form of a modular and extendible
simulator that is validated against real-world quantum network hardware
(available online). However, Qoala is not meant to be purely a simulator, and
implementation is planned on real hardware. We evaluate Qoala's effectiveness
and performance sensitivity to latencies and network schedules using an
extensive simulation study. Qoala provides a framework that opens the door for
future computer science research into quantum network applications, including
scheduling algorithms and compilation strategies that can now readily be
explored using the framework and tools provided.
</summary>
    <author>
      <name>Bart van der Vecht</name>
    </author>
    <author>
      <name>Atak Talay Yücel</name>
    </author>
    <author>
      <name>Hana Jirovská</name>
    </author>
    <author>
      <name>Stephanie Wehner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 10 figures, appendix (28 pages, 17 figures)</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.17296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.17296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.17439v2</id>
    <updated>2025-02-26T03:02:29Z</updated>
    <published>2024-12-16T12:48:04Z</published>
    <title>Large Language Models as Realistic Microservice Trace Generators</title>
    <summary>  Workload traces are essential to understand complex computer systems'
behavior and manage processing and memory resources. Since real-world traces
are hard to obtain, synthetic trace generation is a promising alternative. This
paper proposes a first-of-a-kind approach that relies on training a large
language model (LLM) to generate synthetic workload traces, specifically
microservice call graphs. To capture complex and arbitrary hierarchical
structures and implicit constraints in such traces, we show how to fine-tune
LLMs to generate recursively, making call graph generation a sequence of easier
steps. To further enforce learning constraints in traces and generate uncommon
situations, we argue for applying additional instruction tuning steps to align
our model with the desired trace features. Our evaluation results show that we
can generate diverse realistic traces under various conditions and outperform
existing methods in accuracy and validity. We demonstrate that our
synthetically generated traces can effectively replace real data to optimize
important microservice management tasks. Additionally, our model adapts to
downstream trace-related tasks, such as predicting key trace features and
infilling missing data.
</summary>
    <author>
      <name>Donghyun Kim</name>
    </author>
    <author>
      <name>Sriram Ravula</name>
    </author>
    <author>
      <name>Taemin Ha</name>
    </author>
    <author>
      <name>Alexandros G. Dimakis</name>
    </author>
    <author>
      <name>Daehyeok Kim</name>
    </author>
    <author>
      <name>Aditya Akella</name>
    </author>
    <link href="http://arxiv.org/abs/2502.17439v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.17439v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.18323v1</id>
    <updated>2025-02-25T16:18:15Z</updated>
    <published>2025-02-25T16:18:15Z</published>
    <title>Accelerated Training on Low-Power Edge Devices</title>
    <summary>  Training on edge devices poses several challenges as these devices are
generally resource-constrained, especially in terms of power. State-of-the-art
techniques at the device level reduce the GPU frequency to enforce power
constraints, leading to a significant increase in training time. To accelerate
training, we propose to jointly adjust the system and application parameters
(in our case, the GPU frequency and the batch size of the training task) while
adhering to the power constraints on devices. We introduce a novel cross-layer
methodology that combines predictions of batch size efficiency and device
profiling to achieve the desired optimization. Our evaluation on real hardware
shows that our method outperforms the current baselines that depend on state of
the art techniques, reducing the training time by $2.4\times$ with results very
close to optimal. Our measurements also indicate a substantial reduction in the
overall energy used for the training process. These gains are achieved without
reduction in the performance of the trained model.
</summary>
    <author>
      <name>Mohamed Aboelenien Ahmed</name>
    </author>
    <author>
      <name>Kilian Pfeiffer</name>
    </author>
    <author>
      <name>Heba Khdr</name>
    </author>
    <author>
      <name>Osama Abboud</name>
    </author>
    <author>
      <name>Ramin Khalili</name>
    </author>
    <author>
      <name>Jörg Henkel</name>
    </author>
    <link href="http://arxiv.org/abs/2502.18323v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.18323v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.02097v1</id>
    <updated>2025-03-03T22:32:59Z</updated>
    <published>2025-03-03T22:32:59Z</published>
    <title>Bomfather: An eBPF-based Kernel-level Monitoring Framework for Accurate
  Identification of Unknown, Unused, and Dynamically Loaded Dependencies in
  Modern Software Supply Chains</title>
    <summary>  Inaccuracies in conventional dependency-tracking methods frequently undermine
the security and integrity of modern software supply chains. This paper
introduces a kernel-level framework leveraging extended Berkeley Packet Filter
(eBPF) to capture software build dependencies transparently in real time. Our
approach provides tamper-evident, intrinsic identifiers of build-time
dependencies by computing cryptographic hashes of files accessed during
compilation and constructing Merkle trees based on the observed file content.
  In contrast to traditional static analysis, this kernel-level methodology
accounts for conditional compilation, dead-code, selective library usage, and
dynamic dependencies, yielding more precise Software Bills of Materials (SBOMs)
and Artifact Dependency Graphs (ADGs). We illustrate how existing SBOMs may
omit dynamically loaded or ephemeral dependencies and discuss how kernel-level
tracing can mitigate these omissions. The proposed system enhances
trustworthiness in software artifacts by offering independently verifiable,
kernel-level evidence of build provenance, thereby reducing supply chain risks
and facilitating more accurate vulnerability management.
</summary>
    <author>
      <name>Naveen Srinivasan</name>
    </author>
    <author>
      <name>Nathan Naveen</name>
    </author>
    <author>
      <name>Neil Naveen</name>
    </author>
    <link href="http://arxiv.org/abs/2503.02097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.02097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.05117v1</id>
    <updated>2025-03-07T03:29:52Z</updated>
    <published>2025-03-07T03:29:52Z</published>
    <title>HyperGraph ROS: An Open-Source Robot Operating System for Hybrid
  Parallel Computing based on Computational HyperGraph</title>
    <summary>  This paper presents HyperGraph ROS, an open-source robot operating system
that unifies intra-process, inter-process, and cross-device computation into a
computational hypergraph for efficient message passing and parallel execution.
In order to optimize communication, HyperGraph ROS dynamically selects the
optimal communication mechanism while maintaining a consistent API. For
intra-process messages, Intel-TBB Flow Graph is used with C++ pointer passing,
which ensures zero memory copying and instant delivery. Meanwhile,
inter-process and cross-device communication seamlessly switch to ZeroMQ. When
a node receives a message from any source, it is immediately activated and
scheduled for parallel execution by Intel-TBB. The computational hypergraph
consists of nodes represented by TBB flow graph nodes and edges formed by TBB
pointer-based connections for intra-process communication, as well as ZeroMQ
links for inter-process and cross-device communication. This structure enables
seamless distributed parallelism. Additionally, HyperGraph ROS provides
ROS-like utilities such as a parameter server, a coordinate transformation
tree, and visualization tools. Evaluation in diverse robotic scenarios
demonstrates significantly higher transmission and throughput efficiency
compared to ROS 2. Our work is available at
https://github.com/wujiazheng2020a/hyper_graph_ros.
</summary>
    <author>
      <name>Shufang Zhang</name>
    </author>
    <author>
      <name>Jiazheng Wu</name>
    </author>
    <author>
      <name>Jiacheng He</name>
    </author>
    <author>
      <name>Kaiyi Wang</name>
    </author>
    <author>
      <name>Shan An</name>
    </author>
    <link href="http://arxiv.org/abs/2503.05117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.05117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.11444v1</id>
    <updated>2025-03-14T14:29:17Z</updated>
    <published>2025-03-14T14:29:17Z</published>
    <title>Cerebrum (AIOS SDK): A Platform for Agent Development, Deployment,
  Distribution, and Discovery</title>
    <summary>  Autonomous LLM-based agents have emerged as a powerful paradigm for complex
task execution, yet the field lacks standardized tools for development,
deployment, distribution and discovery of agents. We present Cerebrum, an Agent
SDK for AIOS that addresses this gap through three key components: (1) a
comprehensive SDK featuring a modular four-layer architecture for agent
development, encompassing LLM, memory, storage, and tool management; (2) a
community-driven Agent Hub for sharing and discovering agents, complete with
version control and dependency management; (3) an interactive web interface for
testing and evaluating agents. The platform's effectiveness is demonstrated
through implementations of various agent architectures, including Chain of
Thought (CoT), ReAct, and tool-use agents. Cerebrum advances the field by
providing a unified framework that standardizes agent development while
maintaining flexibility for researchers and developers to innovate and
distribute their agents. The live website is at https://app.aios.foundation,
the code is at https://github.com/agiresearch/Cerebrum, and video is at
https://app.aios.foundation/video-demo.
</summary>
    <author>
      <name>Balaji Rama</name>
    </author>
    <author>
      <name>Kai Mei</name>
    </author>
    <author>
      <name>Yongfeng Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 2025 Annual Conference of the North American Chapter
  of the Association for Computational Linguistics (NAACL) - System
  Demonstration Track</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.11444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.11444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.03109v1</id>
    <updated>2025-04-04T01:33:34Z</updated>
    <published>2025-04-04T01:33:34Z</published>
    <title>Extending Data Spatial Semantics for Scale Agnostic Programming</title>
    <summary>  We introduce extensions to Data Spatial Programming (DSP) that enable
scale-agnostic programming for application development. Building on DSP's
paradigm shift from data-to-compute to compute-to-data, we formalize additional
intrinsic language constructs that abstract persistent state, multi-user
contexts, multiple entry points, and cross-machine distribution for
applications. By introducing a globally accessible root node and treating
walkers as potential entry points, we demonstrate how programs can be written
once and executed across scales, from single-user to multi-user, from local to
distributed, without modification. These extensions allow developers to focus
on domain logic while delegating runtime concerns of persistence, multi-user
support, distribution, and API interfacing to the execution environment. Our
approach makes scale-agnostic programming a natural extension of the
topological semantics of DSP, allowing applications to seamlessly transition
from single-user to multi-user scenarios, from ephemeral to persistent
execution contexts, and from local to distributed execution environments.
</summary>
    <author>
      <name>Jason Mars</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.03109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.03109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.09474v1</id>
    <updated>2025-04-13T08:08:37Z</updated>
    <published>2025-04-13T08:08:37Z</published>
    <title>MigGPT: Harnessing Large Language Models for Automated Migration of
  Out-of-Tree Linux Kernel Patches Across Versions</title>
    <summary>  Out-of-tree kernel patches are essential for adapting the Linux kernel to new
hardware or enabling specific functionalities. Maintaining and updating these
patches across different kernel versions demands significant effort from
experienced engineers. Large language models (LLMs) have shown remarkable
progress across various domains, suggesting their potential for automating
out-of-tree kernel patch migration. However, our findings reveal that LLMs,
while promising, struggle with incomplete code context understanding and
inaccurate migration point identification. In this work, we propose MigGPT, a
framework that employs a novel code fingerprint structure to retain code
snippet information and incorporates three meticulously designed modules to
improve the migration accuracy and efficiency of out-of-tree kernel patches.
Furthermore, we establish a robust benchmark using real-world out-of-tree
kernel patch projects to evaluate LLM capabilities. Evaluations show that
MigGPT significantly outperforms the direct application of vanilla LLMs,
achieving an average completion rate of 72.59% (50.74% improvement) for
migration tasks.
</summary>
    <author>
      <name>Pucheng Dang</name>
    </author>
    <author>
      <name>Di Huang</name>
    </author>
    <author>
      <name>Dong Li</name>
    </author>
    <author>
      <name>Kang Chen</name>
    </author>
    <author>
      <name>Yuanbo Wen</name>
    </author>
    <author>
      <name>Qi Guo</name>
    </author>
    <author>
      <name>Xing Hu</name>
    </author>
    <author>
      <name>Ninghui Sun</name>
    </author>
    <link href="http://arxiv.org/abs/2504.09474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.09474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15302v1</id>
    <updated>2025-04-17T15:04:47Z</updated>
    <published>2025-04-17T15:04:47Z</published>
    <title>RAGDoll: Efficient Offloading-based Online RAG System on a Single GPU</title>
    <summary>  Retrieval-Augmented Generation (RAG) enhances large language model (LLM)
generation quality by incorporating relevant external knowledge. However,
deploying RAG on consumer-grade platforms is challenging due to limited memory
and the increasing scale of both models and knowledge bases. In this work, we
introduce RAGDoll, a resource-efficient, self-adaptive RAG serving system
integrated with LLMs, specifically designed for resource-constrained platforms.
RAGDoll exploits the insight that RAG retrieval and LLM generation impose
different computational and memory demands, which in a traditional serial
workflow result in substantial idle times and poor resource utilization. Based
on this insight, RAGDoll decouples retrieval and generation into parallel
pipelines, incorporating joint memory placement and dynamic batch scheduling
strategies to optimize resource usage across diverse hardware devices and
workloads. Extensive experiments demonstrate that RAGDoll adapts effectively to
various hardware configurations and LLM scales, achieving up to 3.6 times
speedup in average latency compared to serial RAG systems based on vLLM.
</summary>
    <author>
      <name>Weiping Yu</name>
    </author>
    <author>
      <name>Ningyi Liao</name>
    </author>
    <author>
      <name>Siqiang Luo</name>
    </author>
    <author>
      <name>Junfeng Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2504.15302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.17904v1</id>
    <updated>2025-04-24T19:44:12Z</updated>
    <published>2025-04-24T19:44:12Z</published>
    <title>Biting the CHERI bullet: Blockers, Enablers and Security Implications of
  CHERI in Defence</title>
    <summary>  There is growing interest in securing the hardware foundations software
stacks build upon. However, before making any investment decision, software and
hardware supply chain stakeholders require evidence from realistic, multiple
long-term studies of adoption. We present results from a 12 month evaluation of
one such secure hardware solution, CHERI, where 15 teams from industry and
academia ported software relevant to Defence to Arm's experimental Morello
board. We identified six types of blocker inhibiting adoption: dependencies, a
knowledge premium, missing utilities, performance, platform instability, and
technical debt. We also identified three types of enabler: tool assistance,
improved quality, and trivial code porting. Finally, we identified five types
of potential vulnerability that CHERI could, if not appropriately configured,
expand a system's attack surface: state leaks, memory leaks, use after free
vulnerabilities, unsafe defaults, and tool chain instability. Future work
should remove potentially insecure defaults from CHERI tooling, and develop a
CHERI body of knowledge to further adoption.
</summary>
    <author>
      <name>Shamal Faily</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at ICMCIS 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.17904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.17904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.20964v1</id>
    <updated>2025-04-29T17:34:49Z</updated>
    <published>2025-04-29T17:34:49Z</published>
    <title>OSVBench: Benchmarking LLMs on Specification Generation Tasks for
  Operating System Verification</title>
    <summary>  We introduce OSVBench, a new benchmark for evaluating Large Language Models
(LLMs) in generating complete specification code pertaining to operating system
kernel verification tasks. The benchmark first defines the specification
generation problem into a program synthesis problem within a confined scope of
syntax and semantics by providing LLMs with the programming model. The LLMs are
required to understand the provided verification assumption and the potential
syntax and semantics space to search for, then generate the complete
specification for the potentially buggy operating system code implementation
under the guidance of the high-level functional description of the operating
system. This benchmark is built upon a real-world operating system kernel,
Hyperkernel, and consists of 245 complex specification generation tasks in
total, each is a long context task of about 20k-30k tokens. Our comprehensive
evaluation of 12 LLMs exhibits the limited performance of the current LLMs on
the specification generation tasks for operating system verification.
Significant disparities in their performance on the benchmark highlight
differences in their ability to handle long-context code generation tasks. The
evaluation toolkit and benchmark are available at
https://github.com/lishangyu-hkust/OSVBench.
</summary>
    <author>
      <name>Shangyu Li</name>
    </author>
    <author>
      <name>Juyong Jiang</name>
    </author>
    <author>
      <name>Tiancheng Zhao</name>
    </author>
    <author>
      <name>Jiasi Shen</name>
    </author>
    <link href="http://arxiv.org/abs/2504.20964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.20964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.21518v2</id>
    <updated>2025-05-01T07:09:22Z</updated>
    <published>2025-04-30T11:13:52Z</published>
    <title>Confidential Serverless Computing</title>
    <summary>  Although serverless computing offers compelling cost and deployment
simplicity advantages, a significant challenge remains in securely managing
sensitive data as it flows through the network of ephemeral function executions
in serverless computing environments within untrusted clouds. While
Confidential Virtual Machines (CVMs) offer a promising secure execution
environment, their integration with serverless architectures currently faces
fundamental limitations in key areas: security, performance, and resource
efficiency. We present Hacher, a confidential computing system for secure
serverless deployments to overcome these limitations. By employing nested
confidential execution and a decoupled guest OS within CVMs, Hacher runs each
function in a minimal "trustlet", significantly improving security through a
reduced Trusted Computing Base (TCB). Furthermore, by leveraging a data-centric
I/O architecture built upon a lightweight LibOS, Hacher optimizes network
communication to address performance and resource efficiency challenges. Our
evaluation shows that compared to CVM-based deployments, Hacher has 4.3x
smaller TCB, improves end-to-end latency (15-93%), achieves higher function
density (up to 907x), and reduces inter-function communication (up to 27x) and
function chaining latency (16.7-30.2x); thus, Hacher offers a practical system
for confidential serverless computing.
</summary>
    <author>
      <name>Patrick Sabanic</name>
    </author>
    <author>
      <name>Masanori Misono</name>
    </author>
    <author>
      <name>Teofil Bodea</name>
    </author>
    <author>
      <name>Julian Pritzi</name>
    </author>
    <author>
      <name>Michael Hackl</name>
    </author>
    <author>
      <name>Dimitrios Stavrakakis</name>
    </author>
    <author>
      <name>Pramod Bhatotia</name>
    </author>
    <link href="http://arxiv.org/abs/2504.21518v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.21518v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.06625v1</id>
    <updated>2025-05-10T12:16:50Z</updated>
    <published>2025-05-10T12:16:50Z</published>
    <title>CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated
  NPUs</title>
    <summary>  With the rapid development of DNN applications, multi-tenant execution, where
multiple DNNs are co-located on a single SoC, is becoming a prevailing trend.
Although many methods are proposed in prior works to improve multi-tenant
performance, the impact of shared cache is not well studied. This paper
proposes CaMDN, an architecture-scheduling co-design to enhance cache
efficiency for multi-tenant DNNs on integrated NPUs. Specifically, a
lightweight architecture is proposed to support model-exclusive, NPU-controlled
regions inside shared cache to eliminate unexpected cache contention. Moreover,
a cache scheduling method is proposed to improve shared cache utilization. In
particular, it includes a cache-aware mapping method for adaptability to the
varying available cache capacity and a dynamic allocation algorithm to adjust
the usage among co-located DNNs at runtime. Compared to prior works, CaMDN
reduces the memory access by 33.4% on average and achieves a model speedup of
up to 2.56$\times$ (1.88$\times$ on average).
</summary>
    <author>
      <name>Tianhao Cai</name>
    </author>
    <author>
      <name>Liang Wang</name>
    </author>
    <author>
      <name>Limin Xiao</name>
    </author>
    <author>
      <name>Meng Han</name>
    </author>
    <author>
      <name>Zeyu Wang</name>
    </author>
    <author>
      <name>Lin Sun</name>
    </author>
    <author>
      <name>Xiaojian Liao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 9 figures. This paper has been accepted to the 2025 Design
  Automation Conference (DAC)</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.06625v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06625v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.06741v1</id>
    <updated>2025-05-10T19:24:37Z</updated>
    <published>2025-05-10T19:24:37Z</published>
    <title>Online Job Scheduler for Fault-tolerant Quantum Multiprogramming</title>
    <summary>  Fault-tolerant quantum computers are expected to be offered as cloud services
due to their significant resource and infrastructure requirements. Quantum
multiprogramming, which runs multiple quantum jobs in parallel, is a promising
approach to maximize the utilization of such systems. A key challenge in this
setting is the need for an online scheduler capable of handling jobs submitted
dynamically while other programs are already running. In this study, we
formulate the online job scheduling problem for fault-tolerant quantum
computing systems based on lattice surgery and propose an efficient scheduler
to address it. To meet the responsiveness required in an online environment,
our scheduler approximates lattice surgery programs, originally represented as
polycubes, by using simpler cuboid representations. This approximation enables
efficient scheduling while improving overall throughput. In addition, we
incorporate a defragmentation mechanism into the scheduling process,
demonstrating that it can further enhance QPU utilization.
</summary>
    <author>
      <name>Shin Nishio</name>
    </author>
    <author>
      <name>Ryo Wakizaka</name>
    </author>
    <author>
      <name>Daisuke Sakuma</name>
    </author>
    <author>
      <name>Yosuke Ueno</name>
    </author>
    <author>
      <name>Yasunari Suzuki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures. Comments are welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.06741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; D.4; C.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.07833v1</id>
    <updated>2025-05-01T18:58:26Z</updated>
    <published>2025-05-01T18:58:26Z</published>
    <title>Patchwork: A Unified Framework for RAG Serving</title>
    <summary>  Retrieval Augmented Generation (RAG) has emerged as a new paradigm for
enhancing Large Language Model reliability through integration with external
knowledge sources. However, efficient deployment of these systems presents
significant technical challenges due to their inherently heterogeneous
computational pipelines comprising LLMs, databases, and specialized processing
components. We introduce Patchwork, a comprehensive end-to-end RAG serving
framework designed to address these efficiency bottlenecks. Patchwork's
architecture offers three key innovations: First, it provides a flexible
specification interface enabling users to implement custom RAG pipelines.
Secondly, it deploys these pipelines as distributed inference systems while
optimizing for the unique scalability characteristics of individual RAG
components. Third, Patchwork incorporates an online scheduling mechanism that
continuously monitors request load and execution progress, dynamically
minimizing SLO violations through strategic request prioritization and resource
auto-scaling. Our experimental evaluation across four distinct RAG
implementations demonstrates that Patchwork delivers substantial performance
improvements over commercial alternatives, achieving throughput gains exceeding
48% while simultaneously reducing SLO violations by ~24%.
</summary>
    <author>
      <name>Bodun Hu</name>
    </author>
    <author>
      <name>Luis Pabon</name>
    </author>
    <author>
      <name>Saurabh Agarwal</name>
    </author>
    <author>
      <name>Aditya Akella</name>
    </author>
    <link href="http://arxiv.org/abs/2505.07833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.07833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.15213v1</id>
    <updated>2025-05-21T07:43:52Z</updated>
    <published>2025-05-21T07:43:52Z</published>
    <title>KernelOracle: Predicting the Linux Scheduler's Next Move with Deep
  Learning</title>
    <summary>  Efficient task scheduling is paramount in the Linux kernel, where the
Completely Fair Scheduler (CFS) meticulously manages CPU resources to balance
high utilization with interactive responsiveness. This research pioneers the
use of deep learning techniques to predict the sequence of tasks selected by
CFS, aiming to evaluate the feasibility of a more generalized and potentially
more adaptive task scheduler for diverse workloads. Our core contributions are
twofold: first, the systematic generation and curation of a novel scheduling
dataset from a running Linux kernel, capturing real-world CFS behavior; and
second, the development, training, and evaluation of a Long Short-Term Memory
(LSTM) network designed to accurately forecast the next task to be scheduled.
This paper further discusses the practical pathways and implications of
integrating such a predictive model into the kernel's scheduling framework. The
findings and methodologies presented herein open avenues for data-driven
advancements in kernel scheduling, with the full source code provided for
reproducibility and further exploration.
</summary>
    <author>
      <name>Sampanna Yashwant Kahu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 11 figures, pre-print. The source code and data used in this
  work is available at: https://github.com/SampannaKahu/KernelOracle</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.15213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.15213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.1; I.2.0; I.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9908002v1</id>
    <updated>1999-08-03T14:50:09Z</updated>
    <published>1999-08-03T14:50:09Z</published>
    <title>After Compilers and Operating Systems : The Third Advance in Application
  Support</title>
    <summary>  After compilers and operating systems, TSIAs are the third advance in
application support. A compiler supports a high level application definition in
a programming language. An operating system supports a high level interface to
the resources used by an application execution. A Task System and Item
Architecture (TSIA) provides an application with a transparent reliable,
distributed, heterogeneous, adaptive, dynamic, real-time, interactive,
parallel, secure or other execution. In addition to supporting the application
execution, a TSIA also supports the application definition. This run-time
support for the definition is complementary to the compile-time support of a
compiler. For example, this allows a language similar to Fortran or C to
deliver features promised by functional computing. While many TSIAs exist, they
previously have not been recognized as such and have served only a particular
type of application. Existing TSIAs and other projects demonstrate that TSIAs
are feasible for most applications. As the next paradigm for application
support, the TSIA simplifies and unifies existing computing practice and
research. By solving many outstanding problems, the TSIA opens many, many new
opportunities for computing.
</summary>
    <author>
      <name>Burkhard D. Burow</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages including 13 figures of diagrams and code examples. Based on
  invited seminars held in May-July 1999 at IBM, Caltech and elsewhere. For
  further information see http://www.tsia.org</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9908002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9908002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1;D.1.1;D.1.3;D.1.4;D.2.11;D.3.2;D.3.3;D.3.4;D.4.5;D.4.7;E.1;F.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006014v1</id>
    <updated>2000-06-08T16:50:24Z</updated>
    <published>2000-06-08T16:50:24Z</published>
    <title>Solaris System Resource Manager: All I Ever Wanted Was My Unfair
  Advantage (And Why You Can't Have It!)</title>
    <summary>  Traditional UNIX time-share schedulers attempt to be fair to all users by
employing a round-robin style algorithm for allocating CPU time. Unfortunately,
a loophole exists whereby the scheduler can be biased in favor of a greedy user
running many short CPU-time processes. This loophole is not a defect but an
intrinsic property of the round-robin scheduler that ensures responsiveness to
the short CPU demands associated with multiple interactive users. A new
generation of UNIX system resource management software constrains the scheduler
to be equitable to all users regardless of the number of processes each may be
running. This "fair-share" scheduling draws on the concept of pro rating
resource "shares" across users and groups and then dynamically adjusting CPU
usage to meet those share proportions. The simple notion of statically
allocating these shares, however, belies the potential consequences for
performance as measured by user response time and service level targets. We
demonstrate this point by modeling several simple share allocation scenarios
and analyzing the corresponding performance effects. A brief comparison of
commercial system resource management implementations from HP, IBM, and SUN is
also given.
</summary>
    <author>
      <name>Neil J. Gunther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, Updated since the 25th Computer Measurement Group
  Conference, Reno NV, Dec.5-10, 1999</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. CMG'99 Conf. p.194-205</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0006014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4;D.4.1;D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0210031v1</id>
    <updated>2002-10-31T01:51:00Z</updated>
    <published>2002-10-31T01:51:00Z</published>
    <title>The Weaves Reconfigurable Programming Framework</title>
    <summary>  This research proposes a language independent intra-process framework for
object based composition of unmodified code modules. Intuitively, the two major
programming models, threads and processes, can be considered as extremes along
a sharing axis. Multiple threads through a process share all global state,
whereas instances of a process (or independent processes) share no global
state. Weaves provide the generalized framework that allows arbitrary
(selective) sharing of state between multiple control flows through a process.
The Weaves framework supports multiple independent components in a single
process, with flexible state sharing and scheduling, all of which is achieved
without requiring any modification to existing code bases. Furthermore, the
framework allows dynamic instantiation of code modules and control flows
through them. In effect, weaves create intra-process modules (similar to
objects in OOP) from code written in any language. The Weaves paradigm allows
objects to be arbitrarily shared, it is a true superset of both processes as
well as threads, with code sharing and fast context switching time similar to
threads. Weaves does not require any special support from either the language
or application code, practically any code can be weaved. Weaves also include
support for fast automatic checkpointing and recovery with no application
support. This paper presents the elements of the Weaves framework and results
from our implementation that works by reverse-analyzing source-code independent
ELF object files. The current implementation has been validated over Sweep3D, a
benchmark for 3D discrete ordinates neutron transport [Koch et al., 1992], and
a user-level port of the Linux 2.4 family kernel TCP/IP protocol stack.
</summary>
    <author>
      <name>Srinidhi Varadarajan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be submitted to ACM TOCS</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0210031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0210031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.11 D.2.12 D.1.3 D.3.2 D.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603076v1</id>
    <updated>2006-03-18T17:27:22Z</updated>
    <published>2006-03-18T17:27:22Z</published>
    <title>User-Relative Names for Globally Connected Personal Devices</title>
    <summary>  Nontechnical users who own increasingly ubiquitous network-enabled personal
devices such as laptops, digital cameras, and smart phones need a simple,
intuitive, and secure way to share information and services between their
devices. User Information Architecture, or UIA, is a novel naming and
peer-to-peer connectivity architecture addressing this need. Users assign UIA
names by "introducing" devices to each other on a common local-area network,
but these names remain securely bound to their target as devices migrate.
Multiple devices owned by the same user, once introduced, automatically merge
their namespaces to form a distributed "personal cluster" that the owner can
access or modify from any of his devices. Instead of requiring users to
allocate globally unique names from a central authority, UIA enables users to
assign their own "user-relative" names both to their own devices and to other
users. With UIA, for example, Alice can always access her iPod from any of her
own personal devices at any location via the name "ipod", and her friend Bob
can access her iPod via a relative name like "ipod.Alice".
</summary>
    <author>
      <name>Bryan Ford</name>
    </author>
    <author>
      <name>Jacob Strauss</name>
    </author>
    <author>
      <name>Chris Lesniewski-Laas</name>
    </author>
    <author>
      <name>Sean Rhea</name>
    </author>
    <author>
      <name>Frans Kaashoek</name>
    </author>
    <author>
      <name>Robert Morris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">5th International Workshop on Peer-to-Peer Systems, February 2006
  (IPTPS 2006), Santa Barbara, CA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0603076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.1; C.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611116v1</id>
    <updated>2006-11-22T18:25:43Z</updated>
    <published>2006-11-22T18:25:43Z</published>
    <title>Discovering Network Topology in the Presence of Byzantine Faults</title>
    <summary>  We study the problem of Byzantine-robust topology discovery in an arbitrary
asynchronous network. We formally state the weak and strong versions of the
problem. The weak version requires that either each node discovers the topology
of the network or at least one node detects the presence of a faulty node. The
strong version requires that each node discovers the topology regardless of
faults. We focus on non-cryptographic solutions to these problems. We explore
their bounds. We prove that the weak topology discovery problem is solvable
only if the connectivity of the network exceeds the number of faults in the
system. Similarly, we show that the strong version of the problem is solvable
only if the network connectivity is more than twice the number of faults. We
present solutions to both versions of the problem. The presented algorithms
match the established graph connectivity bounds. The algorithms do not require
the individual nodes to know either the diameter or the size of the network.
The message complexity of both programs is low polynomial with respect to the
network size. We describe how our solutions can be extended to add the property
of termination, handle topology changes and perform neighborhood discovery.
</summary>
    <author>
      <name>Mikhail Nesterenko</name>
    </author>
    <author>
      <name>Sébastien Tixeuil</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/11780823_17</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/11780823_17" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">13th Colloquium on Structural Information and Communication
  Complexity (SIROCCO), LNCS Volume 4056 pp. 212-226, Chester, UK, July 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0611116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.2525v1</id>
    <updated>2009-03-14T04:28:55Z</updated>
    <published>2009-03-14T04:28:55Z</published>
    <title>CloudSim: A Novel Framework for Modeling and Simulation of Cloud
  Computing Infrastructures and Services</title>
    <summary>  Cloud computing focuses on delivery of reliable, secure, fault-tolerant,
sustainable, and scalable infrastructures for hosting Internet-based
application services. These applications have different composition,
configuration, and deployment requirements. Quantifying the performance of
scheduling and allocation policy on a Cloud infrastructure (hardware, software,
services) for different application and service models under varying load,
energy performance (power consumption, heat dissipation), and system size is an
extremely challenging problem to tackle. To simplify this process, in this
paper we propose CloudSim: a new generalized and extensible simulation
framework that enables seamless modelling, simulation, and experimentation of
emerging Cloud computing infrastructures and management services. The
simulation framework has the following novel features: (i) support for
modelling and instantiation of large scale Cloud computing infrastructure,
including data centers on a single physical computing node and java virtual
machine; (ii) a self-contained platform for modelling data centers, service
brokers, scheduling, and allocations policies; (iii) availability of
virtualization engine, which aids in creation and management of multiple,
independent, and co-hosted virtualized services on a data center node; and (iv)
flexibility to switch between space-shared and time-shared allocation of
processing cores to virtualized services.
</summary>
    <author>
      <name>Rodrigo N. Calheiros</name>
    </author>
    <author>
      <name>Rajiv Ranjan</name>
    </author>
    <author>
      <name>Cesar A. F. De Rose</name>
    </author>
    <author>
      <name>Rajkumar Buyya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.2525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.2525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.4052v1</id>
    <updated>2009-10-21T11:31:14Z</updated>
    <published>2009-10-21T11:31:14Z</published>
    <title>Virtual-Threading: Advanced General Purpose Processors Architecture</title>
    <summary>  The paper describes the new computers architecture, the main features of
which has been claimed in the Russian Federation patent 2312388 and in the US
patent application 11/991331. This architecture is intended to effective
support of the General Purpose Parallel Computing (GPPC), the essence of which
is extremely frequent switching of threads between states of activity and
states of viewed in the paper the algorithmic latency. To emphasize the same
impact of the architectural latency and the algorithmic latency upon GPPC, is
introduced the new notion of the generalized latency and is defined its
quantitative measure - the Generalized Latency Tolerance (GLT). It is shown
that a well suited for GPPC implementation architecture should have high level
of GLT and is described such architecture, which is called the Virtual-Threaded
Machine. This architecture originates a processor virtualization in the
direction of activities virtualization, which is orthogonal to the well-known
direction of memory virtualization. The key elements of the architecture are 1)
the distributed fine grain representation of the architectural register file,
which elements are hardware swapped through levels of a microarchitectural
memory, 2) the prioritized fine grain direct hardware multiprogramming, 3) the
access controlled virtual addressing and 4) the hardware driven semaphores. The
composition of these features lets to introduce new styles of operating system
(OS) programming, which is free of interruptions, and of applied programming
with a very rare using the OS services.
</summary>
    <author>
      <name>Andrei I. Yafimau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">56 pages, 5 PNG figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0910.4052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.4052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.2; D.4.1; D.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.0298v1</id>
    <updated>2010-02-01T18:31:06Z</updated>
    <published>2010-02-01T18:31:06Z</published>
    <title>A Data Capsule Framework For Web Services: Providing Flexible Data
  Access Control To Users</title>
    <summary>  This paper introduces the notion of a secure data capsule, which refers to an
encapsulation of sensitive user information (such as a credit card number)
along with code that implements an interface suitable for the use of such
information (such as charging for purchases) by a service (such as an online
merchant). In our capsule framework, users provide their data in the form of
such capsules to web services rather than raw data. Capsules can be deployed in
a variety of ways, either on a trusted third party or the user's own computer
or at the service itself, through the use of a variety of hardware or software
modules, such as a virtual machine monitor or trusted platform module: the only
requirement is that the deployment mechanism must ensure that the user's data
is only accessed via the interface sanctioned by the user. The framework
further allows an user to specify policies regarding which services or machines
may host her capsule, what parties are allowed to access the interface, and
with what parameters. The combination of interface restrictions and policy
control lets us bound the impact of an attacker who compromises the service to
gain access to the user's capsule or a malicious insider at the service itself.
</summary>
    <author>
      <name>Jayanthkumar Kannan</name>
    </author>
    <author>
      <name>Petros Maniatis</name>
    </author>
    <author>
      <name>Byung-Gon Chun</name>
    </author>
    <link href="http://arxiv.org/abs/1002.0298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.0298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.3071v1</id>
    <updated>2010-12-14T16:10:08Z</updated>
    <published>2010-12-14T16:10:08Z</published>
    <title>Seamless Flow Migration on Smartphones without Network Support</title>
    <summary>  This paper addresses the following question: Is it possible to migrate TCP/IP
flows between different networks on modern mobile devices, without
infrastructure support or protocol changes? To answer this question, we make
three research contributions. (i) We report a comprehensive characterization of
IP traffic on smartphones using traces collected from 27 iPhone 3GS users for
three months. (ii) Driven by the findings from the characterization, we devise
two novel system mechanisms for mobile devices to sup-port seamless flow
migration without network support, and extensively evaluate their effectiveness
using our field collected traces of real-life usage. Wait-n-Migrate leverages
the fact that most flows are short lived. It establishes new flows on newly
available networks but allows pre-existing flows on the old network to
terminate naturally, effectively decreasing, or even eliminating, connectivity
gaps during network switches. Resumption Agent takes advantage of the
functionality integrated into many modern protocols to securely resume flows
without application intervention. When combined, Wait-n-Migrate and Resumption
Agent provide an unprecedented opportunity to immediately deploy performance
and efficiency-enhancing policies that leverage multiple networks to improve
the performance, efficiency, and connectivity of mobile devices. (iii) Finally,
we report an iPhone 3GS based implementation of these two system mechanisms and
show that their overhead is negligible. Furthermore, we employ an example
network switching policy, called AutoSwitch, to demonstrate their performance.
AutoSwitch improves the Wi-Fi user experience by intelligently migrating TCP
flows between Wi-Fi and cellular networks. Through traces and field
measurements, we show that AutoSwitch reduces the number of user disruptions by
an order of magnitude.
</summary>
    <author>
      <name>Ahmad Rahmati</name>
    </author>
    <author>
      <name>Clay Shepard</name>
    </author>
    <author>
      <name>Chad Tossell</name>
    </author>
    <author>
      <name>Angela Nicoara</name>
    </author>
    <author>
      <name>Lin Zhong</name>
    </author>
    <author>
      <name>Phil Kortum</name>
    </author>
    <author>
      <name>Jatinder Singh</name>
    </author>
    <link href="http://arxiv.org/abs/1012.3071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.3071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.2003v2</id>
    <updated>2011-07-12T08:46:58Z</updated>
    <published>2011-07-11T11:51:06Z</published>
    <title>Efficient Deterministic Replay Using Complete Race Detection</title>
    <summary>  Data races can significantly affect the executions of multi-threaded
programs. Hence, one has to recur the results of data races to
deterministically replay a multi-threaded program. However, data races are
concealed in enormous number of memory operations in a program. Due to the
difficulty of accurately identifying data races, previous multi-threaded
deterministic record/replay schemes for commodity multi-processor system give
up to record data races directly. Consequently, they either record all shared
memory operations, which brings remarkable slowdown to the production run, or
record the synchronization only, which introduces significant efforts to
replay.
  Inspired by the advances in data race detection, we propose an efficient
software-only deterministic replay scheme for commodity multi-processor
systems, which is named RacX. The key insight of RacX is as follows: although
it is NP-hard to accurately identify the existence of data races between a pair
of memory operations, we can find out all potential data races in a
multi-threaded program, in which the false positives can be reduced to a small
amount with our automatic false positive reduction techniques. As a result,
RacX can efficiently monitor all potential data races to deterministically
replay a multi-threaded program.
  To evaluate RacX, we have carried out experiments over a number of well-known
multi-threaded programs from SPLASH-2 benchmark suite and large-scale
commercial programs. RacX can precisely recur production runs of these programs
with value determinism. Averagely, RacX causes only about 1.21%, 1.89%, 2.20%,
and 8.41% slowdown to the original run during recording (for 2-, 4-, 8- and
16-thread programs, respectively). The soundness, efficiency, scalability, and
portability of RacX well demonstrate its superiority.
</summary>
    <author>
      <name>Qi Guo</name>
    </author>
    <author>
      <name>Yunji Chen</name>
    </author>
    <author>
      <name>Tianshi chen</name>
    </author>
    <author>
      <name>Ling Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.2003v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.2003v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6213v1</id>
    <updated>2012-06-27T09:33:06Z</updated>
    <published>2012-06-27T09:33:06Z</published>
    <title>The Necessity for Hardware QoS Support for Server Consolidation and
  Cloud Computing</title>
    <summary>  Chip multiprocessors (CMPs) are ubiquitous in most of today's computing
fields. Although they provide noticeable benefits in terms of performance, cost
and power efficiency, they also introduce some new issues. In this paper we
analyze how the interference from Virtual Private Servers running in other
cores is a significant component of performance unpredictability and can
threaten the attainment of cloud computing. Even if virtualization is used, the
sharing of the on-chip section of the memory hierarchy by different cores makes
performance isolation strongly dependent on what is running elsewhere in the
system. We will show in three actual computing systems, based on Sun UltraSparc
T1, Sun UltraSparc T2 and Intel Xeon processors, how state-of-the-art
virtualization techniques are unable to guarantee performance isolation in a
representative workload such as SPECweb2005. In an especially conceived near
worst-case scenario, it is possible to reduce the performance achieved by a
Solaris Zones consolidated server for this suite of benchmarks in a Sun Fire
T1000 and a Sun Enterprise T5120 by up to 80%. The performance drop observed by
a Xen consolidated server running in a HP Proliant DL160 G5 is almost 45%. For
all systems under study, off-chip bandwidth is shown to be the most critical
resource.
</summary>
    <author>
      <name>Javier Merino</name>
    </author>
    <author>
      <name>Valentin Puente</name>
    </author>
    <author>
      <name>José Ángel Gregorio</name>
    </author>
    <link href="http://arxiv.org/abs/1206.6213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.0687v1</id>
    <updated>2012-09-04T16:16:20Z</updated>
    <published>2012-09-04T16:16:20Z</published>
    <title>Security Issues in the Android Cross-Layer Architecture</title>
    <summary>  The security of Android has been recently challenged by the discovery of a
number of vulnerabilities involving different layers of the Android stack. We
argue that such vulnerabilities are largely related to the interplay among
layers composing the Android stack. Thus, we also argue that such interplay has
been underestimated from a security point-of-view and a systematic analysis of
the Android interplay has not been carried out yet. To this aim, in this paper
we provide a simple model of the Android cross-layer interactions based on the
concept of flow, as a basis for analyzing the Android interplay. In particular,
our model allows us to reason about the security implications associated with
the cross-layer interactions in Android, including a recently discovered
vulnerability that allows a malicious application to make Android devices
totally unresponsive. We used the proposed model to carry out an empirical
assessment of some flows within the Android cross-layered architecture. Our
experiments indicate that little control is exercised by the Android Security
Framework (ASF) over cross-layer interactions in Android. In particular, we
observed that the ASF lacks in discriminating the originator of a flow and
sensitive security issues arise between the Android stack and the Linux kernel,
thereby indicating that the attack surface of the Android platform is wider
than expected.
</summary>
    <author>
      <name>Alessandro Armando</name>
    </author>
    <author>
      <name>Alessio Merlo</name>
    </author>
    <author>
      <name>Luca Verderame</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, double column, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.0687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.0687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.2; C.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.6196v1</id>
    <updated>2012-11-27T02:37:28Z</updated>
    <published>2012-11-27T02:37:28Z</published>
    <title>Chiefly Symmetric: Results on the Scalability of Probabilistic Model
  Checking for Operating-System Code</title>
    <summary>  Reliability in terms of functional properties from the safety-liveness
spectrum is an indispensable requirement of low-level operating-system (OS)
code. However, with evermore complex and thus less predictable hardware,
quantitative and probabilistic guarantees become more and more important.
Probabilistic model checking is one technique to automatically obtain these
guarantees. First experiences with the automated quantitative analysis of
low-level operating-system code confirm the expectation that the naive
probabilistic model checking approach rapidly reaches its limits when
increasing the numbers of processes. This paper reports on our work-in-progress
to tackle the state explosion problem for low-level OS-code caused by the
exponential blow-up of the model size when the number of processes grows. We
studied the symmetry reduction approach and carried out our experiments with a
simple test-and-test-and-set lock case study as a representative example for a
wide range of protocols with natural inter-process dependencies and long-run
properties. We quickly see a state-space explosion for scenarios where
inter-process dependencies are insignificant. However, once inter-process
dependencies dominate the picture models with hundred and more processes can be
constructed and analysed.
</summary>
    <author>
      <name>Christel Baier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU Dresden</arxiv:affiliation>
    </author>
    <author>
      <name>Marcus Daum</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU Dresden</arxiv:affiliation>
    </author>
    <author>
      <name>Benjamin Engel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU Dresden</arxiv:affiliation>
    </author>
    <author>
      <name>Hermann Härtig</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU Dresden</arxiv:affiliation>
    </author>
    <author>
      <name>Joachim Klein</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU Dresden</arxiv:affiliation>
    </author>
    <author>
      <name>Sascha Klüppelholz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU Dresden</arxiv:affiliation>
    </author>
    <author>
      <name>Steffen Märcker</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU Dresden</arxiv:affiliation>
    </author>
    <author>
      <name>Hendrik Tews</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU Dresden</arxiv:affiliation>
    </author>
    <author>
      <name>Marcus Völp</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU Dresden</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.102.14</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.102.14" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings SSV 2012, arXiv:1211.5873</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 102, 2012, pp. 156-166</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.6196v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.6196v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.2778v1</id>
    <updated>2012-12-12T11:37:48Z</updated>
    <published>2012-12-12T11:37:48Z</published>
    <title>Feasibility Tests for Recurrent Real-Time Tasks in the Sporadic DAG
  Model</title>
    <summary>  A model has been proposed in [Baruah et al., in Proceedings of the IEEE
Real-Time Systems Symposium 2012] for representing recurrent
precedence-constrained tasks to be executed on multiprocessor platforms, where
each recurrent task is modeled by a directed acyclic graph (DAG), a period, and
a relative deadline. Each vertex of the DAG represents a sequential job, while
the edges of the DAG represent precedence constraints between these jobs. All
the jobs of the DAG are released simultaneously and have to be completed within
some specified relative deadline. The task may release jobs in this manner an
unbounded number of times, with successive releases occurring at least the
specified period apart. The feasibility problem is to determine whether such a
recurrent task can be scheduled to always meet all deadlines on a specified
number of dedicated processors.
  The case of a single task has been considered in [Baruah et al., 2012]. The
main contribution of this paper is to consider the case of multiple tasks. We
show that EDF has a speedup bound of 2-1/m, where m is the number of
processors. Moreover, we present polynomial and pseudopolynomial schedulability
tests, of differing effectiveness, for determining whether a set of sporadic
DAG tasks can be scheduled by EDF to meet all deadlines on a specified number
of processors.
</summary>
    <author>
      <name>Vincenzo Bonifaci</name>
    </author>
    <author>
      <name>Alberto Marchetti-Spaccamela</name>
    </author>
    <author>
      <name>Sebastian Stiller</name>
    </author>
    <author>
      <name>Andreas Wiese</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ECRTS.2013.32</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ECRTS.2013.32" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2013 25th Euromicro Conference on Real-Time
  Systems</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1212.2778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.2778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.1; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.1306v1</id>
    <updated>2013-02-06T10:06:39Z</updated>
    <published>2013-02-06T10:06:39Z</published>
    <title>Parametric Schedulability Analysis of Fixed Priority Real-Time
  Distributed Systems</title>
    <summary>  Parametric analysis is a powerful tool for designing modern embedded systems,
because it permits to explore the space of design parameters, and to check the
robustness of the system with respect to variations of some uncontrollable
variable. In this paper, we address the problem of parametric schedulability
analysis of distributed real-time systems scheduled by fixed priority. In
particular, we propose two different approaches to parametric analysis: the
first one is a novel technique based on classical schedulability analysis,
whereas the second approach is based on model checking of Parametric Timed
Automata (PTA).
  The proposed analytic method extends existing sensitivity analysis for single
processors to the case of a distributed system, supporting preemptive and
non-preemptive scheduling, jitters and unconstrained deadlines. Parametric
Timed Automata are used to model all possible behaviours of a distributed
system, and therefore it is a necessary and sufficient analysis. Both
techniques have been implemented in two software tools, and they have been
compared with classical holistic analysis on two meaningful test cases. The
results show that the analytic method provides results similar to classical
holistic analysis in a very efficient way, whereas the PTA approach is slower
but covers the entire space of solutions.
</summary>
    <author>
      <name>Youcheng Sun</name>
    </author>
    <author>
      <name>Romain Soulat</name>
    </author>
    <author>
      <name>Giuseppe Lipari</name>
    </author>
    <author>
      <name>Étienne André</name>
    </author>
    <author>
      <name>Laurent Fribourg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ECRTS 2013 (http://ecrts.eit.uni-kl.de/ecrts13)</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.1306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.1306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.6007v1</id>
    <updated>2013-04-22T16:23:24Z</updated>
    <published>2013-04-22T16:23:24Z</published>
    <title>Paging with dynamic memory capacity</title>
    <summary>  We study a generalization of the classic paging problem that allows the
amount of available memory to vary over time - capturing a fundamental property
of many modern computing realities, from cloud computing to multi-core and
energy-optimized processors. It turns out that good performance in the
"classic" case provides no performance guarantees when memory capacity
fluctuates: roughly speaking, moving from static to dynamic capacity can mean
the difference between optimality within a factor 2 in space and time, and
suboptimality by an arbitrarily large factor. More precisely, adopting the
competitive analysis framework, we show that some online paging algorithms,
despite having an optimal (h,k)-competitive ratio when capacity remains
constant, are not (3,k)-competitive for any arbitrarily large k in the presence
of minimal capacity fluctuations. In this light it is surprising that several
classic paging algorithms perform remarkably well even if memory capacity
changes adversarially - even without taking those changes into explicit
account! In particular, we prove that LFD still achieves the minimum number of
faults, and that several classic online algorithms such as LRU have a "dynamic"
(h,k)-competitive ratio that is the best one can achieve without knowledge of
future page requests, even if one had perfect knowledge of future capacity
fluctuations (an exact characterization of this ratio shows it is almost,
albeit not quite, equal to the "classic" ratio k/(k-h+1)). In other words, with
careful management, knowing/predicting future memory resources appears far less
crucial to performance than knowing/predicting future data accesses.
</summary>
    <author>
      <name>Enoch Peserico</name>
    </author>
    <link href="http://arxiv.org/abs/1304.6007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.6007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.0901v1</id>
    <updated>2013-10-03T05:51:48Z</updated>
    <published>2013-10-03T05:51:48Z</published>
    <title>Cudagrind: A Valgrind Extension for CUDA</title>
    <summary>  Valgrind, and specifically the included tool Memcheck, offers an easy and
reliable way for checking the correctness of memory operations in programs.
This works in an unintrusive way where Valgrind translates the program into
intermediate code and executes it on an emulated CPU. The heavy weight tool
Memcheck uses this to keep a full shadow copy of the memory used by a program
and tracking accesses to it. This allows the detection of memory leaks and
checking the validity of accesses.
  Though suited for a wide variety of programs, this approach still fails when
accelerator based programming models are involved. The code running on these
devices is separate from the code running on the host. Access to memory on the
device and starting of kernels is being handled by an API provided by the
driver being used. Hence Valgrind is unable to understand and instrument
operations being run on the device.
  To circumvent this limitation a new set of wrapper functions have been
introduced. These wrap a subset of the CUDA Driver API function that is
responsible for (de-)allocation memory regions on the device and the respective
memory copy operations. This allows to check whether memory is fully allocated
during a transfer and, through the functionality provided by Valgrind, whether
the memory transfered to the device from the host is defined and addressable.
Through this technique it is possible to detect a number of common programming
mistakes, which are very difficult to debug by other means. The combination of
these wrappers together with the Valgrind tool Memcheck is being called
Cudagrind.
</summary>
    <author>
      <name>Thomas M. Baumann</name>
    </author>
    <author>
      <name>Jose Gracia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures, accepted for publication in ParCo 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.0901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.0901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.2148v1</id>
    <updated>2013-10-03T22:49:06Z</updated>
    <published>2013-10-03T22:49:06Z</published>
    <title>C2MS: Dynamic Monitoring and Management of Cloud Infrastructures</title>
    <summary>  Server clustering is a common design principle employed by many organisations
who require high availability, scalability and easier management of their
infrastructure. Servers are typically clustered according to the service they
provide whether it be the application(s) installed, the role of the server or
server accessibility for example. In order to optimize performance, manage load
and maintain availability, servers may migrate from one cluster group to
another making it difficult for server monitoring tools to continuously monitor
these dynamically changing groups. Server monitoring tools are usually
statically configured and with any change of group membership requires manual
reconfiguration; an unreasonable task to undertake on large-scale cloud
infrastructures.
  In this paper we present the Cloudlet Control and Management System (C2MS); a
system for monitoring and controlling dynamic groups of physical or virtual
servers within cloud infrastructures. The C2MS extends Ganglia - an open source
scalable system performance monitoring tool - by allowing system administrators
to define, monitor and modify server groups without the need for server
reconfiguration. In turn administrators can easily monitor group and individual
server metrics on large-scale dynamic cloud infrastructures where roles of
servers may change frequently. Furthermore, we complement group monitoring with
a control element allowing administrator-specified actions to be performed over
servers within service groups as well as introduce further customized
monitoring metrics. This paper outlines the design, implementation and
evaluation of the C2MS.
</summary>
    <author>
      <name>Gary A. McGilvary</name>
    </author>
    <author>
      <name>Josep Rius</name>
    </author>
    <author>
      <name>Íñigo Goiri</name>
    </author>
    <author>
      <name>Francesc Solsona</name>
    </author>
    <author>
      <name>Adam Barker</name>
    </author>
    <author>
      <name>Malcolm Atkinson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CloudCom.2013.45</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CloudCom.2013.45" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the The 5th IEEE International Conference on Cloud
  Computing Technology and Science (CloudCom 2013), 8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.2148v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.2148v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.2362v1</id>
    <updated>2013-11-11T06:21:29Z</updated>
    <published>2013-11-11T06:21:29Z</published>
    <title>Efficient Runtime Monitoring with Metric Temporal Logic: A Case Study in
  the Android Operating System</title>
    <summary>  We present a design and an implementation of a security policy specification
language based on metric linear-time temporal logic (MTL). MTL features
temporal operators that are indexed by time intervals, allowing one to specify
timing-dependent security policies. The design of the language is driven by the
problem of runtime monitoring of applications in mobile devices. A main case
the study is the privilege escalation attack in the Android operating system,
where an app gains access to certain resource or functionalities that are not
explicitly granted to it by the user, through indirect control flow. To capture
these attacks, we extend MTL with recursive definitions, that are used to
express call chains betwen apps. We then show how the metric operators of MTL,
in combination with recursive definitions, can be used to specify policies to
detect privilege escalation, under various fine grained constraints. We present
a new algorithm, extending that of linear time temporal logic, for monitoring
safety policies written in our specification language. The monitor does not
need to store the entire history of events generated by the apps, something
that is crucial for practical implementations. We modified the Android OS
kernel to allow us to insert our generated monitors modularly. We have tested
the modified OS on an actual device, and show that it is effective in detecting
policy violations.
</summary>
    <author>
      <name>Hendra Gunadi</name>
    </author>
    <author>
      <name>Alwen Tiu</name>
    </author>
    <link href="http://arxiv.org/abs/1311.2362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.2362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.3.1; D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.6100v1</id>
    <updated>2014-01-09T00:04:41Z</updated>
    <published>2014-01-09T00:04:41Z</published>
    <title>Performance Impact of Lock-Free Algorithms on Multicore Communication
  APIs</title>
    <summary>  Data race conditions in multi-tasking software applications are prevented by
serializing access to shared memory resources, ensuring data consistency and
deterministic behavior. Traditionally tasks acquire and release locks to
synchronize operations on shared memory. Unfortunately, lock management can add
significant processing overhead especially for multicore deployments where
tasks on different cores convoy in queues waiting to acquire a lock.
Implementing more than one lock introduces the risk of deadlock and using
spinlocks constrains which cores a task can run on. The better alternative is
to eliminate locks and validate that real-time properties are met, which is not
directly considered in many embedded applications. Removing the locks is
non-trivial and packaging lock-free algorithms for developers reduces the
possibility of concurrency defects. This paper details how a multicore
communication API implementation is enhanced to support lock-free messaging and
the impact this has on data exchange latency between tasks. Throughput and
latency are compared on Windows and Linux between lock-based and lock-free
implementations for data exchange of messages, packets, and scalars. A model of
the lock-free exchange predicts performance at the system architecture level
and provides a stop criterion for the refactoring. The results show that
migration from single to multicore hardware architectures degrades lock-based
performance, and increases lock-free performance.
</summary>
    <author>
      <name>K. Eric Harper</name>
    </author>
    <author>
      <name>Thijmen de Gooijer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 8 figures, 36 references, Embedded World Conference 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.6100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.6100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.5569v1</id>
    <updated>2014-06-21T02:04:56Z</updated>
    <published>2014-06-21T02:04:56Z</published>
    <title>On the Reverse Engineering of the Citadel Botnet</title>
    <summary>  Citadel is an advanced information-stealing malware which targets financial
information. This malware poses a real threat against the confidentiality and
integrity of personal and business data. A joint operation was recently
conducted by the FBI and the Microsoft Digital Crimes Unit in order to take
down Citadel command-and-control servers. The operation caused some disruption
in the botnet but has not stopped it completely. Due to the complex structure
and advanced anti-reverse engineering techniques, the Citadel malware analysis
process is both challenging and time-consuming. This allows cyber criminals to
carry on with their attacks while the analysis is still in progress. In this
paper, we present the results of the Citadel reverse engineering and provide
additional insight into the functionality, inner workings, and open source
components of the malware. In order to accelerate the reverse engineering
process, we propose a clone-based analysis methodology. Citadel is an offspring
of a previously analyzed malware called Zeus; thus, using the former as a
reference, we can measure and quantify the similarities and differences of the
new variant. Two types of code analysis techniques are provided in the
methodology, namely assembly to source code matching and binary clone
detection. The methodology can help reduce the number of functions requiring
manual analysis. The analysis results prove that the approach is promising in
Citadel malware analysis. Furthermore, the same approach is applicable to
similar malware analysis scenarios.
</summary>
    <author>
      <name>Ashkan Rahimian</name>
    </author>
    <author>
      <name>Raha Ziarati</name>
    </author>
    <author>
      <name>Stere Preda</name>
    </author>
    <author>
      <name>Mourad Debbabi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-05302-8_25</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-05302-8_25" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 17 figures. This is an updated / edited version of a paper
  appeared in FPS 2013</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNCS 8352, 2014, pp 408-425</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.5569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.5569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6; K.6.5; E.3; D.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.4346v1</id>
    <updated>2014-07-16T15:35:47Z</updated>
    <published>2014-07-16T15:35:47Z</published>
    <title>Faults in Linux 2.6</title>
    <summary>  In August 2011, Linux entered its third decade. Ten years before, Chou et al.
published a study of faults found by applying a static analyzer to Linux
versions 1.0 through 2.4.1. A major result of their work was that the drivers
directory contained up to 7 times more of certain kinds of faults than other
directories. This result inspired numerous efforts on improving the reliability
of driver code. Today, Linux is used in a wider range of environments, provides
a wider range of services, and has adopted a new development and release model.
What has been the impact of these changes on code quality? To answer this
question, we have transported Chou et al.'s experiments to all versions of
Linux 2.6; released between 2003 and 2011. We find that Linux has more than
doubled in size during this period, but the number of faults per line of code
has been decreasing. Moreover, the fault rate of drivers is now below that of
other directories, such as arch. These results can guide further development
and research efforts for the decade to come. To allow updating these results as
Linux evolves, we define our experimental protocol and make our checkers
available.
</summary>
    <author>
      <name>Nicolas Palix</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Grenoble 1 UJF, LIG</arxiv:affiliation>
    </author>
    <author>
      <name>Gaël Thomas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIP6, INRIA Paris-Rocquencourt</arxiv:affiliation>
    </author>
    <author>
      <name>Suman Saha</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIP6, INRIA Paris-Rocquencourt</arxiv:affiliation>
    </author>
    <author>
      <name>Christophe Calvès</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIP6, INRIA Paris-Rocquencourt</arxiv:affiliation>
    </author>
    <author>
      <name>Gilles Muller</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIP6, INRIA Paris-Rocquencourt</arxiv:affiliation>
    </author>
    <author>
      <name>Julia L. Lawall</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIP6, INRIA Paris-Rocquencourt</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2619090</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2619090" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Computer Systems 32, 2 (2014) 1--40</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.4346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.4346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.5567v1</id>
    <updated>2014-09-19T09:30:49Z</updated>
    <published>2014-09-19T09:30:49Z</published>
    <title>Rank-Aware Dynamic Migrations and Adaptive Demotions for DRAM Power
  Management</title>
    <summary>  Modern DRAM architectures allow a number of low-power states on individual
memory ranks for advanced power management. Many previous studies have taken
advantage of demotions on low-power states for energy saving. However, most of
the demotion schemes are statically performed on a limited number of
pre-selected low-power states, and are suboptimal for different workloads and
memory architectures. Even worse, the idle periods are often too short for
effective power state transitions, especially for memory intensive
applications. Wrong decisions on power state transition incur significant
energy and delay penalties. In this paper, we propose a novel memory system
design named RAMZzz with rank-aware energy saving optimizations including
dynamic page migrations and adaptive demotions. Specifically, we group the
pages with similar access locality into the same rank with dynamic page
migrations. Ranks have their hotness: hot ranks are kept busy for high
utilization and cold ranks can have more lengthy idle periods for power state
transitions. We further develop adaptive state demotions by considering all
low-power states for each rank and a prediction model to estimate the
power-down timeout among states. We experimentally compare our algorithm with
other energy saving policies with cycle-accurate simulation. Experiments with
benchmark workloads show that RAMZzz achieves significant improvement on
energy-delay2 and energy consumption over other energy saving techniques.
</summary>
    <author>
      <name>Yanchao Lu</name>
    </author>
    <author>
      <name>Donghong Wu</name>
    </author>
    <author>
      <name>Bingsheng He</name>
    </author>
    <author>
      <name>Xueyan Tang</name>
    </author>
    <author>
      <name>Jianliang Xu</name>
    </author>
    <author>
      <name>Minyi Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.5567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.5567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.7747v1</id>
    <updated>2014-10-28T19:20:34Z</updated>
    <published>2014-10-28T19:20:34Z</published>
    <title>Sprobes: Enforcing Kernel Code Integrity on the TrustZone Architecture</title>
    <summary>  Many smartphones now deploy conventional operating systems, so the rootkit
attacks so prevalent on desktop and server systems are now a threat to
smartphones. While researchers have advocated using virtualization to detect
and prevent attacks on operating systems (e.g., VM introspection and trusted
virtual domains), virtualization is not practical on smartphone systems due to
the lack of virtualization support and/or the expense of virtualization.
Current smartphone processors do have hardware support for running a protected
environment, such as the ARM TrustZone extensions, but such hardware does not
control the operating system operations sufficiently to enable VM
introspection. In particular, a conventional operating system running with
TrustZone still retains full control of memory management, which a rootkit can
use to prevent traps on sensitive instructions or memory accesses necessary for
effective introspection. In this paper, we present SPROBES, a novel primitive
that enables introspection of operating systems running on ARM TrustZone
hardware. Using SPROBES, an introspection mechanism protected by TrustZone can
instrument individual operating system instructions of its choice, receiving an
unforgeable trap whenever any SPROBE is executed. The key challenge in
designing SPROBES is preventing the rootkit from removing them, but we identify
a set of five invariants whose enforcement is sufficient to restrict rootkits
to execute only approved, SPROBE-injected kernel code. We implemented a
proof-of-concept version of SPROBES for the ARM Fast Models emulator,
demonstrating that in Linux kernel 2.6.38, only 12 SPROBES are sufficient to
enforce all five of these invariants. With SPROBES we show that it is possible
to leverage the limited TrustZone extensions to limit conventional kernel
execution to approved code comprehensively.
</summary>
    <author>
      <name>Xinyang Ge</name>
    </author>
    <author>
      <name>Hayawardh Vijayakumar</name>
    </author>
    <author>
      <name>Trent Jaeger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the Third Workshop on Mobile Security Technologies
  (MoST) 2014 (http://arxiv.org/abs/1410.6674)</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.7747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.7747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.01666v1</id>
    <updated>2015-04-07T16:58:31Z</updated>
    <published>2015-04-07T16:58:31Z</published>
    <title>Garbage Collection Techniques for Flash-Resident Page-Mapping FTLs</title>
    <summary>  Storage devices based on flash memory have replaced hard disk drives (HDDs)
due to their superior performance, increasing density, and lower power
consumption. Unfortunately, flash memory is subject to challenging
idiosyncrasies like erase-before-write and limited block lifetime. These
constraints are handled by a flash translation layer (FTL), which performs
out-of-place updates, wear-leveling and garbage-collection behind the scene,
while offering the application a virtualization of the physical address space.
  A class of relevant FTLs employ a flash-resident page-associative mapping
table from logical to physical addresses, with a smaller RAM-resident cache for
frequently mapped entries. In this paper, we address the problem of performing
garbage-collection under such FTLs. We observe two problems. Firstly,
maintaining the metadata needed to perform garbage-collection under these
schemes is problematic, because at write-time we do not necessarily know the
physical address of the before-image. Secondly, the size of this metadata must
remain small, because it makes RAM unavailable for caching frequently accessed
entries. We propose two complementary techniques, called Lazy Gecko and
Logarithmic Gecko, which address these issues. Lazy Gecko works well when RAM
is plentiful enough to store the GC metadata. Logarithmic Gecko works well when
RAM isn't plentiful and efficiently stores the GC metadata in flash. Thus,
these techniques are applicable to a wide range of flash devices with varying
amounts of embedded RAM.
</summary>
    <author>
      <name>Niv Dayan</name>
    </author>
    <author>
      <name>Philippe Bonnet</name>
    </author>
    <link href="http://arxiv.org/abs/1504.01666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.01666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03875v1</id>
    <updated>2015-04-15T12:05:59Z</updated>
    <published>2015-04-15T12:05:59Z</published>
    <title>RIOT OS Paves the Way for Implementation of High-Performance MAC
  Protocols</title>
    <summary>  Implementing new, high-performance MAC protocols requires real-time features,
to be able to synchronize correctly between different unrelated devices. Such
features are highly desirable for operating wireless sensor networks (WSN) that
are designed to be part of the Internet of Things (IoT). Unfortunately, the
operating systems commonly used in this domain cannot provide such features. On
the other hand, "bare-metal" development sacrifices portability, as well as the
mul-titasking abilities needed to develop the rich applications that are useful
in the domain of the Internet of Things. We describe in this paper how we
helped solving these issues by contributing to the development of a port of
RIOT OS on the MSP430 microcontroller, an architecture widely used in
IoT-enabled motes. RIOT OS offers rich and advanced real-time features,
especially the simultaneous use of as many hardware timers as the underlying
platform (microcontroller) can offer. We then demonstrate the effectiveness of
these features by presenting a new implementation, on RIOT OS, of S-CoSenS, an
efficient MAC protocol that uses very low processing power and energy.
</summary>
    <author>
      <name>Kévin Roussel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Nancy - Grand Est / LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Ye-Qiong Song</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Nancy - Grand Est / LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Zendra</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Nancy - Grand Est / LORIA</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5220/0005237600050014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5220/0005237600050014" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SCITEPRESS. SENSORNETS 2015, Feb 2015, Angers, France.
  http://www.scitepress.org</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.03875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.02155v3</id>
    <updated>2016-09-23T07:13:59Z</updated>
    <published>2015-05-08T19:52:46Z</published>
    <title>Evaluate and Compare Two Utilization-Based Schedulability-Test
  Frameworks for Real-Time Systems</title>
    <summary>  This report summarizes two general frameworks, namely k2Q and k2U, that have
been recently developed by us. The purpose of this report is to provide
detailed evaluations and comparisons of these two frameworks. These two
frameworks share some similar characteristics, but they are useful for
different application cases. These two frameworks together provide
comprehensive means for the users to automatically convert the pseudo
polynomial-time tests (or even exponential-time tests) into polynomial-time
tests with closed mathematical forms. With the quadratic and hyperbolic forms,
k2Q and k2U frameworks can be used to provide many quantitive features to be
measured and evaluated, like the total utilization bounds, speed-up factors,
etc., not only for uniprocessor scheduling but also for multiprocessor
scheduling. These frameworks can be viewed as "blackbox" interfaces for
providing polynomial-time schedulability tests and response time analysis for
real-time applications. We have already presented their advantages for being
applied in some models in the previous papers. However, it was not possible to
present a more comprehensive comparison between these two frameworks. We hope
this report can help the readers and users clearly understand the difference of
these two frameworks, their unique characteristics, and their advantages. We
demonstrate their differences and properties by using the traditional sporadic
realtime task models in uniprocessor scheduling and multiprocessor global
scheduling.
</summary>
    <author>
      <name>Jian-Jia Chen</name>
    </author>
    <author>
      <name>Wen-Hung Huang</name>
    </author>
    <author>
      <name>Cong Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1501.07084</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.02155v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.02155v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04170v1</id>
    <updated>2015-11-13T06:27:48Z</updated>
    <published>2015-11-13T06:27:48Z</published>
    <title>Controlled Owicki-Gries Concurrency: Reasoning about the Preemptible
  eChronos Embedded Operating System</title>
    <summary>  We introduce a controlled concurrency framework, derived from the
Owicki-Gries method, for describing a hardware interface in detail sufficient
to support the modelling and verification of small, embedded operating systems
(OS's) whose run-time responsiveness is paramount. Such real-time systems run
with interrupts mostly enabled, including during scheduling. That differs from
many other successfully modelled and verified OS's that typically reduce the
complexity of concurrency by running on uniprocessor platforms and by switching
interrupts off as much as possible. Our framework builds on the traditional
Owicki-Gries method, for its fine-grained concurrency is needed for
high-performance system code. We adapt it to support explicit concurrency
control, by providing a simple, faithful representation of the hardware
interface that allows software to control the degree of interleaving between
user code, OS code, interrupt handlers and a scheduler that controls context
switching. We then apply this framework to model the interleaving behavior of
the eChronos OS, a preemptible real-time OS for embedded micro-controllers. We
discuss the accuracy and usability of our approach when instantiated to model
the eChronos OS. Both our framework and the eChronos model are formalised in
the Isabelle/HOL theorem prover, taking advantage of the high level of
automation in modern reasoning tools.
</summary>
    <author>
      <name>June Andronick</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NICTA and UNSW</arxiv:affiliation>
    </author>
    <author>
      <name>Corey Lewis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NICTA</arxiv:affiliation>
    </author>
    <author>
      <name>Carroll Morgan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NICTA and UNSW</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.196.2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.196.2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings MARS 2015, arXiv:1511.02528</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 196, 2015, pp. 10-24</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.04170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00320v1</id>
    <updated>2016-04-01T16:32:47Z</updated>
    <published>2016-04-01T16:32:47Z</published>
    <title>AuDroid: Preventing Attacks on Audio Channels in Mobile Devices</title>
    <summary>  Voice control is a popular way to operate mobile devices, enabling users to
communicate requests to their devices. However, adversaries can leverage voice
control to trick mobile devices into executing commands to leak secrets or to
modify critical information. Contemporary mobile operating systems fail to
prevent such attacks because they do not control access to the speaker at all
and fail to control when untrusted apps may use the microphone, enabling
authorized apps to create exploitable communication channels. In this paper, we
propose a security mechanism that tracks the creation of audio communication
channels explicitly and controls the information flows over these channels to
prevent several types of attacks.We design and implement AuDroid, an extension
to the SELinux reference monitor integrated into the Android operating system
for enforcing lattice security policies over the dynamically changing use of
system audio resources. To enhance flexibility, when information flow errors
are detected, the device owner, system apps and services are given the
opportunity to resolve information flow errors using known methods, enabling
AuDroid to run many configurations safely. We evaluate our approach on 17
widely-used apps that make extensive use of the microphone and speaker, finding
that AuDroid prevents six types of attack scenarios on audio channels while
permitting all 17 apps to run effectively. AuDroid shows that it is possible to
prevent attacks using audio channels without compromising functionality or
introducing significant performance overhead.
</summary>
    <author>
      <name>Giuseppe Petracca</name>
    </author>
    <author>
      <name>Yuqiong Sun</name>
    </author>
    <author>
      <name>Ahmad Atamli</name>
    </author>
    <author>
      <name>Trent Jaeger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2015 Annual Computer Security Applications Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04386v1</id>
    <updated>2016-06-14T14:22:25Z</updated>
    <published>2016-06-14T14:22:25Z</published>
    <title>A Note on the Period Enforcer Algorithm for Self-Suspending Tasks</title>
    <summary>  The period enforcer algorithm for self-suspending real-time tasks is a
technique for suppressing the "back-to-back" scheduling penalty associated with
deferred execution. Originally proposed in 1991, the algorithm has attracted
renewed interest in recent years. This note revisits the algorithm in the light
of recent developments in the analysis of self-suspending tasks, carefully
re-examines and explains its underlying assumptions and limitations, and points
out three observations that have not been made in the literature to date: (i)
period enforcement is not strictly superior (compared to the base case without
enforcement) as it can cause deadline misses in self-suspending task sets that
are schedulable without enforcement; (ii) to match the assumptions underlying
the analysis of the period enforcer, a schedulability analysis of
self-suspending tasks subject to period enforcement requires a task set
transformation for which no solution is known in the general case, and which is
subject to exponential time complexity (with current techniques) in the limited
case of a single self-suspending task; and (iii) the period enforcer algorithm
is incompatible with all existing analyses of suspension-based locking
protocols, and can in fact cause ever-increasing suspension times until a
deadline is missed.
</summary>
    <author>
      <name>Jian-Jia Chen</name>
    </author>
    <author>
      <name>Björn B. Brandenburg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/LITES-v004-i001-a001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/LITES-v004-i001-a001" rel="related"/>
    <link href="http://arxiv.org/abs/1606.04386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3; D.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05794v1</id>
    <updated>2016-06-18T18:55:42Z</updated>
    <published>2016-06-18T18:55:42Z</published>
    <title>Scalability of VM Provisioning Systems</title>
    <summary>  Virtual machines and virtualized hardware have been around for over half a
century. The commoditization of the x86 platform and its rapidly growing
hardware capabilities have led to recent exponential growth in the use of
virtualization both in the enterprise and high performance computing (HPC). The
startup time of a virtualized environment is a key performance metric for high
performance computing in which the runtime of any individual task is typically
much shorter than the lifetime of a virtualized service in an enterprise
context. In this paper, a methodology for accurately measuring the startup
performance on an HPC system is described. The startup performance overhead of
three of the most mature, widely deployed cloud management frameworks
(OpenStack, OpenNebula, and Eucalyptus) is measured to determine their
suitability for workloads typically seen in an HPC environment. A 10x
performance difference is observed between the fastest (Eucalyptus) and the
slowest (OpenNebula) framework. This time difference is primarily due to delays
in waiting on networking in the cloud-init portion of the startup. The
methodology and measurements presented should facilitate the optimization of
startup across a variety of virtualization environments.
</summary>
    <author>
      <name>Mike Jones</name>
    </author>
    <author>
      <name>Bill Arcand</name>
    </author>
    <author>
      <name>Bill Bergeron</name>
    </author>
    <author>
      <name>David Bestor</name>
    </author>
    <author>
      <name>Chansup Byun</name>
    </author>
    <author>
      <name>Lauren Milechin</name>
    </author>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Matt Hubbell</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Pete Michaleas</name>
    </author>
    <author>
      <name>Julie Mullen</name>
    </author>
    <author>
      <name>Andy Prout</name>
    </author>
    <author>
      <name>Tony Rosa</name>
    </author>
    <author>
      <name>Siddharth Samsi</name>
    </author>
    <author>
      <name>Charles Yee</name>
    </author>
    <author>
      <name>Albert Reuther</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HPEC.2016.7761629</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HPEC.2016.7761629" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages; 6 figures; accepted to the IEEE High Performance Extreme
  Computing (HPEC) conference 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07763v1</id>
    <updated>2016-07-15T14:52:57Z</updated>
    <published>2016-07-15T14:52:57Z</published>
    <title>Energy-Efficient Real-Time Scheduling for Two-Type Heterogeneous
  Multiprocessors</title>
    <summary>  We propose three novel mathematical optimization formulations that solve the
same two-type heterogeneous multiprocessor scheduling problem for a real-time
taskset with hard constraints. Our formulations are based on a global
scheduling scheme and a fluid model. The first formulation is a mixed-integer
nonlinear program, since the scheduling problem is intuitively considered as an
assignment problem. However, by changing the scheduling problem to first
determine a task workload partition and then to find the execution order of all
tasks, the computation time can be significantly reduced. Specifically, the
workload partitioning problem can be formulated as a continuous nonlinear
program for a system with continuous operating frequency, and as a continuous
linear program for a practical system with a discrete speed level set. The task
ordering problem can be solved by an algorithm with a complexity that is linear
in the total number of tasks. The work is evaluated against existing global
energy/feasibility optimal workload allocation formulations. The results
illustrate that our algorithms are both feasibility optimal and energy optimal
for both implicit and constrained deadline tasksets. Specifically, our
algorithm can achieve up to 40% energy saving for some simulated tasksets with
constrained deadlines. The benefit of our formulation compared with existing
work is that our algorithms can solve a more general class of scheduling
problems due to incorporating a scheduling dynamic model in the formulations
and allowing for a time-varying speed profile. Moreover, our algorithms can be
applied to both online and offline scheduling schemes.
</summary>
    <author>
      <name>Mason Thammawichai</name>
    </author>
    <author>
      <name>Eric C. Kerrigan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11241-017-9291-6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11241-017-9291-6" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Real-Time Systems, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.07763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07995v2</id>
    <updated>2016-09-24T01:49:03Z</updated>
    <published>2016-07-27T07:46:13Z</published>
    <title>System-level Scalable Checkpoint-Restart for Petascale Computing</title>
    <summary>  Fault tolerance for the upcoming exascale generation has long been an area of
active research. One of the components of a fault tolerance strategy is
checkpointing. Petascale-level checkpointing is demonstrated through a new
mechanism for virtualization of the InfiniBand UD (unreliable datagram) mode,
and for updating the remote address on each UD-based send, due to lack of a
fixed peer. Note that InfiniBand UD is required to support modern MPI
implementations. An extrapolation from the current results to future SSD-based
storage systems provides evidence that the current approach will remain
practical in the exascale generation. This transparent checkpointing approach
is evaluated using a framework of the DMTCP checkpointing package. Results are
shown for HPCG (linear algebra), NAMD (molecular dynamics), and the NAS NPB
benchmarks. In tests up to 32,752 MPI processes on 32,752 CPU cores,
checkpointing of a computation with a 38 TB memory footprint in 11 minutes is
demonstrated. Runtime overhead is reduced to less than 1%. The approach is also
evaluated across three widely used MPI implementations.
</summary>
    <author>
      <name>Jiajun Cao</name>
    </author>
    <author>
      <name>Kapil Arya</name>
    </author>
    <author>
      <name>Rohan Garg</name>
    </author>
    <author>
      <name>Shawn Matott</name>
    </author>
    <author>
      <name>Dhabaleswar K. Panda</name>
    </author>
    <author>
      <name>Hari Subramoni</name>
    </author>
    <author>
      <name>Jérôme Vienne</name>
    </author>
    <author>
      <name>Gene Cooperman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 5 figures, to be published in ICPADS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.07995v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07995v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; D.1.3; D.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02067v1</id>
    <updated>2016-09-07T16:53:40Z</updated>
    <published>2016-09-07T16:53:40Z</published>
    <title>Practical Data Compression for Modern Memory Hierarchies</title>
    <summary>  In this thesis, we describe a new, practical approach to integrating
hardware-based data compression within the memory hierarchy, including on-chip
caches, main memory, and both on-chip and off-chip interconnects. This new
approach is fast, simple, and effective in saving storage space. A key insight
in our approach is that access time (including decompression latency) is
critical in modern memory hierarchies. By combining inexpensive hardware
support with modest OS support, our holistic approach to compression achieves
substantial improvements in performance and energy efficiency across the memory
hierarchy. Using this new approach, we make several major contributions in this
thesis. First, we propose a new compression algorithm, Base-Delta-Immediate
Compression (BDI), that achieves high compression ratio with very low
compression/decompression latency. BDI exploits the existing low dynamic range
of values present in many cache lines to compress them to smaller sizes using
Base+Delta encoding. Second, we observe that the compressed size of a cache
block can be indicative of its reuse. We use this observation to develop a new
cache insertion policy for compressed caches, the Size-based Insertion Policy
(SIP), which uses the size of a compressed block as one of the metrics to
predict its potential future reuse. Third, we propose a new main memory
compression framework, Linearly Compressed Pages (LCP), that significantly
reduces the complexity and power cost of supporting main memory compression. We
demonstrate that any compression algorithm can be adapted to fit the
requirements of LCP, and that LCP can be efficiently integrated with the
existing cache compression designs, avoiding extra compression/decompression.
</summary>
    <author>
      <name>Gennady Pekhimenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD Thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.02067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03052v3</id>
    <updated>2018-11-22T17:40:49Z</updated>
    <published>2016-10-10T19:59:32Z</published>
    <title>Verification of the Tree-Based Hierarchical Read-Copy Update in the
  Linux Kernel</title>
    <summary>  Read-Copy Update (RCU) is a scalable, high-performance Linux-kernel
synchronization mechanism that runs low-overhead readers concurrently with
updaters. Production-quality RCU implementations for multi-core systems are
decidedly non-trivial. Giving the ubiquity of Linux, a rare "million-year" bug
can occur several times per day across the installed base. Stringent validation
of RCU's complex behaviors is thus critically important. Exhaustive testing is
infeasible due to the exponential number of possible executions, which suggests
use of formal verification.
  Previous verification efforts on RCU either focus on simple implementations
or use modeling languages, the latter requiring error-prone manual translation
that must be repeated frequently due to regular changes in the Linux kernel's
RCU implementation. In this paper, we first describe the implementation of Tree
RCU in the Linux kernel. We then discuss how to construct a model directly from
Tree RCU's source code in C, and use the CBMC model checker to verify its
safety and liveness properties. To our best knowledge, this is the first
verification of a significant part of RCU's source code, and is an important
step towards integration of formal verification into the Linux kernel's
regression test suite.
</summary>
    <author>
      <name>Lihao Liang</name>
    </author>
    <author>
      <name>Paul E. McKenney</name>
    </author>
    <author>
      <name>Daniel Kroening</name>
    </author>
    <author>
      <name>Tom Melham</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23919/DATE.2018.8341980</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23919/DATE.2018.8341980" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a long version of a conference paper published in the 2018
  Design, Automation and Test in Europe Conference (DATE)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Design, Automation and Test in Europe Conference (2018): 61-66</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.03052v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03052v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.4; D.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.00445v2</id>
    <updated>2017-08-21T20:09:02Z</updated>
    <published>2016-12-01T17:45:18Z</published>
    <title>Near-Memory Address Translation</title>
    <summary>  Memory and logic integration on the same chip is becoming increasingly cost
effective, creating the opportunity to offload data-intensive functionality to
processing units placed inside memory chips. The introduction of memory-side
processing units (MPUs) into conventional systems faces virtual memory as the
first big showstopper: without efficient hardware support for address
translation MPUs have highly limited applicability. Unfortunately, conventional
translation mechanisms fall short of providing fast translations as
contemporary memories exceed the reach of TLBs, making expensive page walks
common.
  In this paper, we are the first to show that the historically important
flexibility to map any virtual page to any page frame is unnecessary in today's
servers. We find that while limiting the associativity of the
virtual-to-physical mapping incurs no penalty, it can break the
translate-then-fetch serialization if combined with careful data placement in
the MPU's memory, allowing for translation and data fetch to proceed
independently and in parallel. We propose the Distributed Inverted Page Table
(DIPTA), a near-memory structure in which the smallest memory partition keeps
the translation information for its data share, ensuring that the translation
completes together with the data fetch. DIPTA completely eliminates the
performance overhead of translation, achieving speedups of up to 3.81x and
2.13x over conventional translation using 4KB and 1GB pages respectively.
</summary>
    <author>
      <name>Javier Picorel</name>
    </author>
    <author>
      <name>Djordje Jevdjic</name>
    </author>
    <author>
      <name>Babak Falsafi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.00445v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.00445v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00999v1</id>
    <updated>2017-04-04T13:26:31Z</updated>
    <published>2017-04-04T13:26:31Z</published>
    <title>A Backward Algorithm for the Multiprocessor Online Feasibility of
  Sporadic Tasks</title>
    <summary>  The online feasibility problem (for a set of sporadic tasks) asks whether
there is a scheduler that always prevents deadline misses (if any), whatever
the sequence of job releases, which is a priori} unknown to the scheduler. In
the multiprocessor setting, this problem is notoriously difficult. The only
exact test for this problem has been proposed by Bonifaci and
Marchetti-Spaccamela: it consists in modelling all the possible behaviours of
the scheduler and of the tasks as a graph; and to interpret this graph as a
game between the tasks and the scheduler, which are seen as antagonistic
players. Then, computing a correct scheduler is equivalent to finding a winning
strategy for the `scheduler player', whose objective in the game is to avoid
deadline misses. In practice, however this approach is limited by the
intractable size of the graph. In this work, we consider the classical
attractor algorithm to solve such games, and introduce antichain techniques to
optimise its performance in practice and overcome the huge size of the game
graph. These techniques are inspired from results from the formal methods
community, and exploit the specific structure of the feasibility problem. We
demonstrate empirically that our approach allows to dramatically improve the
performance of the game solving algorithm.
</summary>
    <author>
      <name>Gilles Geeraerts</name>
    </author>
    <author>
      <name>Joël Goossens</name>
    </author>
    <author>
      <name>Thi-Van-Anh Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Long version of a conference paper accepted to ACSD 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05600v2</id>
    <updated>2017-06-23T02:43:41Z</updated>
    <published>2017-04-19T03:30:09Z</published>
    <title>TrustShadow: Secure Execution of Unmodified Applications with ARM
  TrustZone</title>
    <summary>  The rapid evolution of Internet-of-Things (IoT) technologies has led to an
emerging need to make it smarter. A variety of applications now run
simultaneously on an ARM-based processor. For example, devices on the edge of
the Internet are provided with higher horsepower to be entrusted with storing,
processing and analyzing data collected from IoT devices. This significantly
improves efficiency and reduces the amount of data that needs to be transported
to the cloud for data processing, analysis and storage. However, commodity OSes
are prone to compromise. Once they are exploited, attackers can access the data
on these devices. Since the data stored and processed on the devices can be
sensitive, left untackled, this is particularly disconcerting.
  In this paper, we propose a new system, TrustShadow that shields legacy
applications from untrusted OSes. TrustShadow takes advantage of ARM TrustZone
technology and partitions resources into the secure and normal worlds. In the
secure world, TrustShadow constructs a trusted execution environment for
security-critical applications. This trusted environment is maintained by a
lightweight runtime system that coordinates the communication between
applications and the ordinary OS running in the normal world. The runtime
system does not provide system services itself. Rather, it forwards requests
for system services to the ordinary OS, and verifies the correctness of the
responses. To demonstrate the efficiency of this design, we prototyped
TrustShadow on a real chip board with ARM TrustZone support, and evaluated its
performance using both microbenchmarks and real-world applications. We showed
TrustShadow introduces only negligible overhead to real-world applications.
</summary>
    <author>
      <name>Le Guan</name>
    </author>
    <author>
      <name>Peng Liu</name>
    </author>
    <author>
      <name>Xinyu Xing</name>
    </author>
    <author>
      <name>Xinyang Ge</name>
    </author>
    <author>
      <name>Shengzhi Zhang</name>
    </author>
    <author>
      <name>Meng Yu</name>
    </author>
    <author>
      <name>Trent Jaeger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3081333.3081349</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3081333.3081349" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MobiSys 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.05600v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05600v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00138v3</id>
    <updated>2017-05-23T19:00:17Z</updated>
    <published>2017-04-29T06:22:32Z</published>
    <title>Contego: An Adaptive Framework for Integrating Security Tasks in
  Real-Time Systems</title>
    <summary>  Embedded real-time systems (RTS) are pervasive. Many modern RTS are exposed
to unknown security flaws, and threats to RTS are growing in both number and
sophistication. However, until recently, cyber-security considerations were an
afterthought in the design of such systems. Any security mechanisms integrated
into RTS must (a) co-exist with the real- time tasks in the system and (b)
operate without impacting the timing and safety constraints of the control
logic. We introduce Contego, an approach to integrating security tasks into RTS
without affecting temporal requirements. Contego is specifically designed for
legacy systems, viz., the real-time control systems in which major alterations
of the system parameters for constituent tasks is not always feasible. Contego
combines the concept of opportunistic execution with hierarchical scheduling to
maintain compatibility with legacy systems while still providing flexibility by
allowing security tasks to operate in different modes. We also define a metric
to measure the effectiveness of such integration. We evaluate Contego using
synthetic workloads as well as with an implementation on a realistic embedded
platform (an open- source ARM CPU running real-time Linux).
</summary>
    <author>
      <name>Monowar Hasan</name>
    </author>
    <author>
      <name>Sibin Mohan</name>
    </author>
    <author>
      <name>Rodolfo Pellizzoni</name>
    </author>
    <author>
      <name>Rakesh B. Bobba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication, 29th Euromicro Conference on Real-Time
  Systems (ECRTS17)</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.00138v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00138v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03623v1</id>
    <updated>2017-05-10T06:47:40Z</updated>
    <published>2017-05-10T06:47:40Z</published>
    <title>Improving the Performance and Endurance of Persistent Memory with
  Loose-Ordering Consistency</title>
    <summary>  Persistent memory provides high-performance data persistence at main memory.
Memory writes need to be performed in strict order to satisfy storage
consistency requirements and enable correct recovery from system crashes.
Unfortunately, adhering to such a strict order significantly degrades system
performance and persistent memory endurance. This paper introduces a new
mechanism, Loose-Ordering Consistency (LOC), that satisfies the ordering
requirements at significantly lower performance and endurance loss. LOC
consists of two key techniques. First, Eager Commit eliminates the need to
perform a persistent commit record write within a transaction. We do so by
ensuring that we can determine the status of all committed transactions during
recovery by storing necessary metadata information statically with blocks of
data written to memory. Second, Speculative Persistence relaxes the write
ordering between transactions by allowing writes to be speculatively written to
persistent memory. A speculative write is made visible to software only after
its associated transaction commits. To enable this, our mechanism supports the
tracking of committed transaction ID and multi-versioning in the CPU cache. Our
evaluations show that LOC reduces the average performance overhead of memory
persistence from 66.9% to 34.9% and the memory write traffic overhead from
17.1% to 3.4% on a variety of workloads.
</summary>
    <author>
      <name>Youyou Lu</name>
    </author>
    <author>
      <name>Jiwu Shu</name>
    </author>
    <author>
      <name>Long Sun</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPDS.2017.2701364</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPDS.2017.2701364" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted by IEEE Transactions on Parallel and
  Distributed Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.03623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.05260v4</id>
    <updated>2018-04-19T00:06:48Z</updated>
    <published>2017-07-17T16:12:15Z</published>
    <title>Deterministic Memory Abstraction and Supporting Multicore System
  Architecture</title>
    <summary>  Poor time predictability of multicore processors has been a long-standing
challenge in the real-time systems community. In this paper, we make a case
that a fundamental problem that prevents efficient and predictable real-time
computing on multicore is the lack of a proper memory abstraction to express
memory criticality, which cuts across various layers of the system: the
application, OS, and hardware. We, therefore, propose a new holistic resource
management approach driven by a new memory abstraction, which we call
Deterministic Memory. The key characteristic of deterministic memory is that
the platform - the OS and hardware - guarantees small and tightly bounded
worst-case memory access timing. In contrast, we call the conventional memory
abstraction as best-effort memory in which only highly pessimistic worst-case
bounds can be achieved. We propose to utilize both abstractions to achieve high
time predictability but without significantly sacrificing performance. We
present deterministic memory-aware OS and architecture designs, including
OS-level page allocator, hardware-level cache, and DRAM controller designs. We
implement the proposed OS and architecture extensions on Linux and gem5
simulator. Our evaluation results, using a set of synthetic and real-world
benchmarks, demonstrate the feasibility and effectiveness of our approach.
</summary>
    <author>
      <name>Farzad Farshchi</name>
    </author>
    <author>
      <name>Prathap Kumar Valsan</name>
    </author>
    <author>
      <name>Renato Mancuso</name>
    </author>
    <author>
      <name>Heechul Yun</name>
    </author>
    <link href="http://arxiv.org/abs/1707.05260v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05260v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06931v1</id>
    <updated>2017-08-23T09:31:28Z</updated>
    <published>2017-08-23T09:31:28Z</published>
    <title>Bringing Fault-Tolerant GigaHertz-Computing to Space: A Multi-Stage
  Software-Side Fault-Tolerance Approach for Miniaturized Spacecraft</title>
    <summary>  Modern embedded technology is a driving factor in satellite miniaturization,
contributing to a massive boom in satellite launches and a rapidly evolving new
space industry. Miniaturized satellites, however, suffer from low reliability,
as traditional hardware-based fault-tolerance (FT) concepts are ineffective for
on-board computers (OBCs) utilizing modern systems-on-a-chip (SoC). Therefore,
larger satellites continue to rely on proven processors with large feature
sizes. Software-based concepts have largely been ignored by the space industry
as they were researched only in theory, and have not yet reached the level of
maturity necessary for implementation. We present the first integral,
real-world solution to enable fault-tolerant general-purpose computing with
modern multiprocessor-SoCs (MPSoCs) for spaceflight, thereby enabling their use
in future high-priority space missions. The presented multi-stage approach
consists of three FT stages, combining coarse-grained thread-level distributed
self-validation, FPGA reconfiguration, and mixed criticality to assure
long-term FT and excellent scalability for both resource constrained and
critical high-priority space missions. Early benchmark results indicate a
drastic performance increase over state-of-the-art radiation-hard OBC designs
and considerably lower software- and hardware development costs. This approach
was developed for a 4-year European Space Agency (ESA) project, and we are
implementing a tiled MPSoC prototype jointly with two industrial partners.
</summary>
    <author>
      <name>Christian M. Fuchs</name>
    </author>
    <author>
      <name>Todor Stefanov</name>
    </author>
    <author>
      <name>Nadia Murillo</name>
    </author>
    <author>
      <name>Aske Plaat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26th IEEE Asian Test Symposium 2017, 27-30 Nov 2017, Taipei City,
  Taiwan</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.06931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03943v3</id>
    <updated>2017-12-19T03:20:19Z</updated>
    <published>2017-12-11T18:49:08Z</published>
    <title>EmLog: Tamper-Resistant System Logging for Constrained Devices with TEEs</title>
    <summary>  Remote mobile and embedded devices are used to deliver increasingly impactful
services, such as medical rehabilitation and assistive technologies. Secure
system logging is beneficial in these scenarios to aid audit and forensic
investigations particularly if devices bring harm to end-users. Logs should be
tamper-resistant in storage, during execution, and when retrieved by a trusted
remote verifier. In recent years, Trusted Execution Environments (TEEs) have
emerged as the go-to root of trust on constrained devices for isolated
execution of sensitive applications. Existing TEE-based logging systems,
however, focus largely on protecting server-side logs and offer little
protection to constrained source devices. In this paper, we introduce EmLog --
a tamper-resistant logging system for constrained devices using the
GlobalPlatform TEE. EmLog provides protection against complex software
adversaries and offers several additional security properties over past
schemes. The system is evaluated across three log datasets using an
off-the-shelf ARM development board running an open-source,
GlobalPlatform-compliant TEE. On average, EmLog runs with low run-time memory
overhead (1MB heap and stack), 430--625 logs/second throughput, and five-times
persistent storage overhead versus unprotected logs.
</summary>
    <author>
      <name>Carlton Shepherd</name>
    </author>
    <author>
      <name>Raja Naeem Akram</name>
    </author>
    <author>
      <name>Konstantinos Markantonakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the 11th IFIP International Conference on Information
  Security Theory and Practice (WISTP '17)</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.03943v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03943v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.02833v1</id>
    <updated>2018-01-09T08:28:11Z</updated>
    <published>2018-01-09T08:28:11Z</published>
    <title>Connecting the World of Embedded Mobiles: The RIOT Approach to
  Ubiquitous Networking for the Internet of Things</title>
    <summary>  The Internet of Things (IoT) is rapidly evolving based on low-power compliant
protocol standards that extend the Internet into the embedded world. Pioneering
implementations have proven it is feasible to inter-network very constrained
devices, but had to rely on peculiar cross-layered designs and offer a
minimalistic set of features. In the long run, however, professional use and
massive deployment of IoT devices require full-featured, cleanly composed, and
flexible network stacks.
  This paper introduces the networking architecture that turns RIOT into a
powerful IoT system, to enable low-power wireless scenarios. RIOT networking
offers (i) a modular architecture with generic interfaces for plugging in
drivers, protocols, or entire stacks, (ii) support for multiple heterogeneous
interfaces and stacks that can concurrently operate, and (iii) GNRC, its
cleanly layered, recursively composed default network stack. We contribute an
in-depth analysis of the communication performance and resource efficiency of
RIOT, both on a micro-benchmarking level as well as by comparing IoT
communication across different platforms. Our findings show that, though it is
based on significantly different design trade-offs, the networking subsystem of
RIOT achieves a performance equivalent to that of Contiki and TinyOS, the two
operating systems which pioneered IoT software platforms.
</summary>
    <author>
      <name>Martine Lenders</name>
    </author>
    <author>
      <name>Peter Kietzmann</name>
    </author>
    <author>
      <name>Oliver Hahm</name>
    </author>
    <author>
      <name>Hauke Petersen</name>
    </author>
    <author>
      <name>Cenk Gündoğan</name>
    </author>
    <author>
      <name>Emmanuel Baccelli</name>
    </author>
    <author>
      <name>Kaspar Schleiser</name>
    </author>
    <author>
      <name>Thomas C. Schmidt</name>
    </author>
    <author>
      <name>Matthias Wählisch</name>
    </author>
    <link href="http://arxiv.org/abs/1801.02833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.02833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00696v1</id>
    <updated>2018-02-02T14:23:00Z</updated>
    <published>2018-02-02T14:23:00Z</published>
    <title>Size-aware Sharding For Improving Tail Latencies in In-memory Key-value
  Stores</title>
    <summary>  This paper introduces the concept of size-aware sharding to improve tail
latencies for in-memory key-value stores, and describes its implementation in
the Minos key-value store. Tail latencies are crucial in distributed
applications with high fan-out ratios, because overall response time is
determined by the slowest response. Size-aware sharding distributes requests
for keys to cores according to the size of the item associated with the key. In
particular, requests for small and large items are sent to disjoint subsets of
cores. Size-aware sharding improves tail latencies by avoiding head-of-line
blocking, in which a request for a small item gets queued behind a request for
a large item. Alternative size-unaware approaches to sharding, such as
keyhash-based sharding, request dispatching and stealing do not avoid
head-of-line blocking, and therefore exhibit worse tail latencies. The
challenge in implementing size-aware sharding is to maintain high throughput by
avoiding the cost of software dispatching and by achieving load balancing
between different cores. Minos uses hardware dispatch for all requests for
small items, which form the very large majority of all requests. It achieves
load balancing by adapting the number of cores handling requests for small and
large items to their relative presence in the workload. We compare Minos to
three state-of-the-art designs of in-memory KV stores. Compared to its closest
competitor, Minos achieves a 99th percentile latency that is up to two orders
of magnitude lower. Put differently, for a given value for the 99th percentile
latency equal to 10 times the mean service time, Minos achieves a throughput
that is up to 7.4 times higher.
</summary>
    <author>
      <name>Diego Didona</name>
    </author>
    <author>
      <name>Willy Zwaenepoel</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05802v1</id>
    <updated>2018-02-15T23:38:27Z</updated>
    <published>2018-02-15T23:38:27Z</published>
    <title>End-to-end Analysis and Design of a Drone Flight Controller</title>
    <summary>  Timing guarantees are crucial to cyber-physical applications that must bound
the end-to-end delay between sensing, processing and actuation. For example, in
a flight controller for a multirotor drone, the data from a gyro or inertial
sensor must be gathered and processed to determine the attitude of the
aircraft. Sensor data fusion is followed by control decisions that adjust the
flight of a drone by altering motor speeds. If the processing pipeline between
sensor input and actuation is not bounded, the drone will lose control and
possibly fail to maintain flight.
  Motivated by the implementation of a multithreaded drone flight controller on
the Quest RTOS, we develop a composable pipe model based on the system's task,
scheduling and communication abstractions. This pipe model is used to analyze
two semantics of end-to-end time: reaction time and freshness time. We also
argue that end-to-end timing properties should be factored in at the early
stage of application design. Thus, we provide a mathematical framework to
derive feasible task periods that satisfy both a given set of end-to-end timing
constraints and the schedulability requirement. We demonstrate the
applicability of our design approach by using it to port the Cleanflight flight
controller firmware to Quest on the Intel Aero board. Experiments show that
Cleanflight ported to Quest is able to achieve end-to-end latencies within the
predicted time bounds derived by analysis.
</summary>
    <author>
      <name>Zhuoqun Cheng</name>
    </author>
    <author>
      <name>Richard West</name>
    </author>
    <author>
      <name>Craig Einstein</name>
    </author>
    <link href="http://arxiv.org/abs/1802.05802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07062v2</id>
    <updated>2018-11-29T22:55:38Z</updated>
    <published>2018-02-20T11:08:31Z</published>
    <title>KASR: A Reliable and Practical Approach to Attack Surface Reduction of
  Commodity OS Kernels</title>
    <summary>  Commodity OS kernels have broad attack surfaces due to the large code base
and the numerous features such as device drivers. For a real-world use case
(e.g., an Apache Server), many kernel services are unused and only a small
amount of kernel code is used. Within the used code, a certain part is invoked
only at runtime while the rest are executed at startup and/or shutdown phases
in the kernel's lifetime run. In this paper, we propose a reliable and
practical system, named KASR, which transparently reduces attack surfaces of
commodity OS kernels at runtime without requiring their source code. The KASR
system, residing in a trusted hypervisor, achieves the attack surface reduction
through a two-step approach: (1) reliably depriving unused code of executable
permissions, and (2) transparently segmenting used code and selectively
activating them. We implement a prototype of KASR on Xen-4.8.2 hypervisor and
evaluate its security effectiveness on Linux kernel-4.4.0-87-generic. Our
evaluation shows that KASR reduces the kernel attack surface by 64% and trims
off 40% of CVE vulnerabilities. Besides, KASR successfully detects and blocks
all 6 real-world kernel rootkits. We measure its performance overhead with
three benchmark tools (i.e., SPECINT, httperf and bonnie++). The experimental
results indicate that KASR imposes less than 1% performance overhead (compared
to an unmodified Xen hypervisor) on all the benchmarks.
</summary>
    <author>
      <name>Zhi Zhang</name>
    </author>
    <author>
      <name>Yueqiang Cheng</name>
    </author>
    <author>
      <name>Surya Nepal</name>
    </author>
    <author>
      <name>Dongxi Liu</name>
    </author>
    <author>
      <name>Qingni Shen</name>
    </author>
    <author>
      <name>Fethi Rabhi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-00470-5_32</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-00470-5_32" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The work has been accepted at the 21st International Symposium on
  Research in Attacks, Intrusions, and Defenses 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.07062v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07062v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05308v1</id>
    <updated>2018-07-14T00:02:55Z</updated>
    <published>2018-07-14T00:02:55Z</published>
    <title>TabulaROSA: Tabular Operating System Architecture for Massively Parallel
  Heterogeneous Compute Engines</title>
    <summary>  The rise in computing hardware choices is driving a reevaluation of operating
systems. The traditional role of an operating system controlling the execution
of its own hardware is evolving toward a model whereby the controlling
processor is distinct from the compute engines that are performing most of the
computations. In this context, an operating system can be viewed as software
that brokers and tracks the resources of the compute engines and is akin to a
database management system. To explore the idea of using a database in an
operating system role, this work defines key operating system functions in
terms of rigorous mathematical semantics (associative array algebra) that are
directly translatable into database operations. These operations possess a
number of mathematical properties that are ideal for parallel operating systems
by guaranteeing correctness over a wide range of parallel operations. The
resulting operating system equations provide a mathematical specification for a
Tabular Operating System Architecture (TabulaROSA) that can be implemented on
any platform. Simulations of forking in TabularROSA are performed using an
associative array implementation and compared to Linux on a 32,000+ core
supercomputer. Using over 262,000 forkers managing over 68,000,000,000
processes, the simulations show that TabulaROSA has the potential to perform
operating system functions on a massively parallel scale. The TabulaROSA
simulations show 20x higher performance as compared to Linux while managing
2000x more processes in fully searchable tables.
</summary>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Ron Brightwell</name>
    </author>
    <author>
      <name>Alan Edelman</name>
    </author>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Hayden Jananthan</name>
    </author>
    <author>
      <name>Michael Jones</name>
    </author>
    <author>
      <name>Sam Madden</name>
    </author>
    <author>
      <name>Peter Michaleas</name>
    </author>
    <author>
      <name>Hamed Okhravi</name>
    </author>
    <author>
      <name>Kevin Pedretti</name>
    </author>
    <author>
      <name>Albert Reuther</name>
    </author>
    <author>
      <name>Thomas Sterling</name>
    </author>
    <author>
      <name>Mike Stonebraker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HPEC.2018.8547577</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HPEC.2018.8547577" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures, accepted at IEEE HPEC 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05078v2</id>
    <updated>2019-06-05T20:52:44Z</updated>
    <published>2018-08-02T21:21:50Z</published>
    <title>StreamBox-TZ: Secure Stream Analytics at the Edge with TrustZone</title>
    <summary>  While it is compelling to process large streams of IoT data on the cloud
edge, doing so exposes the data to a sophisticated, vulnerable software stack
on the edge and hence security threats. To this end, we advocate isolating the
data and its computations in a trusted execution environment (TEE) on the edge,
shielding them from the remaining edge software stack which we deem untrusted.
This approach faces two major challenges: (1) executing high-throughput,
low-delay stream analytics in a single TEE, which is constrained by a low
trusted computing base (TCB) and limited physical memory; (2) verifying
execution of stream analytics as the execution involves untrusted software
components on the edge. In response, we present StreamBox-TZ (SBT), a stream
analytics engine for an edge platform that offers strong data security,
verifiable results, and good performance. SBT contributes a data plane designed
and optimized for a TEE based on ARM TrustZone. It supports continuous remote
attestation for analytics correctness and result freshness while incurring low
overhead. SBT only adds 42.5 KB executable to the TCB (16% of the entire TCB).
On an octa core ARMv8 platform, it delivers the state-of-the-art performance by
processing input events up to 140 MB/sec (12M events/sec) with sub-second
delay. The overhead incurred by SBT's security mechanism is less than 25%.
</summary>
    <author>
      <name>Heejin Park</name>
    </author>
    <author>
      <name>Shuang Zhai</name>
    </author>
    <author>
      <name>Long Lu</name>
    </author>
    <author>
      <name>Felix Xiaozhu Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05078v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05078v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05579v1</id>
    <updated>2018-08-02T21:37:29Z</updated>
    <published>2018-08-02T21:37:29Z</published>
    <title>Regulating Access to System Sensors in Cooperating Programs</title>
    <summary>  Modern operating systems such as Android, iOS, Windows Phone, and Chrome OS
support a cooperating program abstraction. Instead of placing all functionality
into a single program, programs cooperate to complete tasks requested by users.
However, untrusted programs may exploit interactions with other programs to
obtain unauthorized access to system sensors either directly or through
privileged services. Researchers have proposed that programs should only be
authorized to access system sensors on a user-approved input event, but these
methods do not account for possible delegation done by the program receiving
the user input event. Furthermore, proposed delegation methods do not enable
users to control the use of their input events accurately. In this paper, we
propose ENTRUST, a system that enables users to authorize sensor operations
that follow their input events, even if the sensor operation is performed by a
program different from the program receiving the input event. ENTRUST tracks
user input as well as delegation events and restricts the execution of such
events to compute unambiguous delegation paths to enable accurate and reusable
authorization of sensor operations. To demonstrate this approach, we implement
the ENTRUST authorization system for Android. We find, via a laboratory user
study, that attacks can be prevented at a much higher rate (54-64%
improvement); and via a field user study, that ENTRUST requires no more than
three additional authorizations per program with respect to the first-use
approach, while incurring modest performance (&lt;1%) and memory overheads (5.5 KB
per program).
</summary>
    <author>
      <name>Giuseppe Petracca</name>
    </author>
    <author>
      <name>Jens Grossklags</name>
    </author>
    <author>
      <name>Patrick McDaniel</name>
    </author>
    <author>
      <name>Trent Jaeger</name>
    </author>
    <link href="http://arxiv.org/abs/1808.05579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06049v2</id>
    <updated>2018-08-25T05:50:43Z</updated>
    <published>2018-08-18T06:39:03Z</published>
    <title>Runtime Analysis of Whole-System Provenance</title>
    <summary>  Identifying the root cause and impact of a system intrusion remains a
foundational challenge in computer security. Digital provenance provides a
detailed history of the flow of information within a computing system,
connecting suspicious events to their root causes. Although existing
provenance-based auditing techniques provide value in forensic analysis, they
assume that such analysis takes place only retrospectively. Such post-hoc
analysis is insufficient for realtime security applications, moreover, even for
forensic tasks, prior provenance collection systems exhibited poor performance
and scalability, jeopardizing the timeliness of query responses.
  We present CamQuery, which provides inline, realtime provenance analysis,
making it suitable for implementing security applications. CamQuery is a Linux
Security Module that offers support for both userspace and in-kernel execution
of analysis applications. We demonstrate the applicability of CamQuery to a
variety of runtime security applications including data loss prevention,
intrusion detection, and regulatory compliance. In evaluation, we demonstrate
that CamQuery reduces the latency of realtime query mechanisms, while imposing
minimal overheads on system execution. CamQuery thus enables the further
deployment of provenance-based technologies to address central challenges in
computer security.
</summary>
    <author>
      <name>Thomas Pasquier</name>
    </author>
    <author>
      <name>Xueyuan Han</name>
    </author>
    <author>
      <name>Thomas Moyer</name>
    </author>
    <author>
      <name>Adam Bates</name>
    </author>
    <author>
      <name>Olivier Hermant</name>
    </author>
    <author>
      <name>David Eyers</name>
    </author>
    <author>
      <name>Jean Bacon</name>
    </author>
    <author>
      <name>Margo Seltzer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 12 figures, 25th ACM Conference on Computer and
  Communications Security 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06049v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06049v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.10664v2</id>
    <updated>2019-09-08T15:33:45Z</updated>
    <published>2019-01-30T04:10:01Z</published>
    <title>User Space Network Drivers</title>
    <summary>  The rise of user space packet processing frameworks like DPDK and netmap
makes low-level code more accessible to developers and researchers. Previously,
driver code was hidden in the kernel and rarely modified, or even looked at, by
developers working at higher layers. These barriers are gone nowadays, yet
developers still treat user space drivers as black-boxes magically accelerating
applications. We want to change this: every researcher building high-speed
network applications should understand the intricacies of the underlying
drivers, especially if they impact performance. We present ixy, a user space
network driver designed for simplicity and educational purposes to show that
fast packet IO is not black magic but careful engineering. ixy focuses on the
bare essentials of user space packet processing: a packet forwarder including
the whole NIC driver uses less than 1,000 lines of C code.
  This paper is partially written in tutorial style on the case study of our
implementations of drivers for both the Intel 82599 family and for virtual
VirtIO NICs. The former allows us to reason about driver and framework
performance on a stripped-down implementation to assess individual
optimizations in isolation. VirtIO support ensures that everyone can run it in
a virtual machine.
  Our code is available as free and open source under the BSD license at
https://github.com/emmericp/ixy
</summary>
    <author>
      <name>Paul Emmerich</name>
    </author>
    <author>
      <name>Maximilian Pudelko</name>
    </author>
    <author>
      <name>Simon Bauer</name>
    </author>
    <author>
      <name>Stefan Huber</name>
    </author>
    <author>
      <name>Thomas Zwickl</name>
    </author>
    <author>
      <name>Georg Carle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in ACM/IEEE Symposium on Architectures for Networking and
  Communications Systems (ANCS 2019), 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.10664v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10664v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.09493v1</id>
    <updated>2019-02-22T00:39:43Z</updated>
    <published>2019-02-22T00:39:43Z</published>
    <title>Dynamic Fault Tolerance Through Resource Pooling</title>
    <summary>  Miniaturized satellites are currently not considered suitable for critical,
high-priority, and complex multi-phased missions, due to their low reliability.
As hardware-side fault tolerance (FT) solutions designed for larger spacecraft
can not be adopted aboard very small satellites due to budget, energy, and size
constraints, we developed a hybrid FT-approach based upon only COTS components,
commodity processor cores, library IP, and standard software. This approach
facilitates fault detection, isolation, and recovery in software, and utilizes
fault-coverage techniques across the embedded stack within an multiprocessor
system-on-chip (MPSoC). This allows our FPGA-based proof-of-concept
implementation to deliver strong fault-coverage even for missions with a long
duration, but also to adapt to varying performance requirements during the
mission. The operator of a spacecraft utilizing this approach can define
performance profiles, which allow an on-board computer (OBC) to trade between
processing capacity, fault coverage, and energy consumption using simple
heuristics. The software-side FT approach developed also offers advantages if
deployed aboard larger spacecraft through spare resource pooling, enabling an
OBC to more efficiently handle permanent faults. This FT approach in part
mimics a critical biological systems's way of tolerating and adjusting to
failures, enabling graceful ageing of an MPSoC.
</summary>
    <author>
      <name>Christian M. Fuchs</name>
    </author>
    <author>
      <name>Nadia M. Murillo</name>
    </author>
    <author>
      <name>Aske Plaat</name>
    </author>
    <author>
      <name>Erik van der Kouwe</name>
    </author>
    <author>
      <name>Todor Stefanov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/AHS.2018.8541457</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/AHS.2018.8541457" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2018 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1902.09493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.09493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.05012v1</id>
    <updated>2019-04-10T06:15:28Z</updated>
    <published>2019-04-10T06:15:28Z</published>
    <title>KEY-SSD: Access-Control Drive to Protect Files from Ransomware Attacks</title>
    <summary>  Traditional techniques to prevent damage from ransomware attacks are to
detect and block attacks by monitoring the known behaviors such as frequent
name changes, recurring access to cryptographic libraries and exchange keys
with remote servers. Unfortunately, intelligent ransomware can easily bypass
these techniques. Another prevention technique is to recover from the backup
copy when a file is infected with ransomware. However, the data backup
technique requires extra storage space and can be removed with ransomware. In
this paper, we propose to implement an access control mechanism on a disk
drive, called a KEY-SSD disk drive. KEY-SSD is the data store and the last
barrier to data protection. Unauthorized applications will not be able to read
file data even if they bypass the file system defense, thus denying the block
request without knowing the disk's registered block key and completely
eliminating the possibility of the file becoming hostage to ransomware. We have
prototyped KEY-SSD and validated the usefulness of KEY-SSD by demonstrating 1)
selective block access control, 2) unauthorized data access blocking and 3)
negligible performance overhead. Our comprehensive evaluation of KEY-SSD for
various workloads show the KEY-SSD performance is hardly degraded due to OS
lightweight key transmission and access control drive optimization. We also
confirmed that KEY-SSD successfully protects the files in the actual ransomware
sample.
</summary>
    <author>
      <name>Jinwoo Ahn</name>
    </author>
    <author>
      <name>Donggyu Park</name>
    </author>
    <author>
      <name>Chang-Gyu Lee</name>
    </author>
    <author>
      <name>Donghyun Min</name>
    </author>
    <author>
      <name>Junghee Lee</name>
    </author>
    <author>
      <name>Sungyong Park</name>
    </author>
    <author>
      <name>Qian Chen</name>
    </author>
    <author>
      <name>Youngjae Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 20 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.05012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.05572v3</id>
    <updated>2024-01-09T01:36:16Z</updated>
    <published>2019-04-11T08:20:07Z</published>
    <title>The Android Platform Security Model (2023)</title>
    <summary>  Android is the most widely deployed end-user focused operating system. With
its growing set of use cases encompassing communication, navigation, media
consumption, entertainment, finance, health, and access to sensors, actuators,
cameras, or microphones, its underlying security model needs to address a host
of practical threats in a wide variety of scenarios while being useful to
non-security experts. To support this flexibility, Android's security model
must strike a difficult balance between security, privacy, and usability for
end users; provide assurances for app developers; and maintain system
performance under tight hardware constraints. This paper aims to both document
the assumed threat model and discuss its implications, with a focus on the
ecosystem context in which Android exists. We analyze how different security
measures in past and current Android implementations work together to mitigate
these threats, and, where there are special cases in applying the security
model in practice; we discuss these deliberate deviations and examine their
impact.
</summary>
    <author>
      <name>René Mayrhofer</name>
    </author>
    <author>
      <name>Jeffrey Vander Stoep</name>
    </author>
    <author>
      <name>Chad Brubaker</name>
    </author>
    <author>
      <name>Dianne Hackborn</name>
    </author>
    <author>
      <name>Bram Bonné</name>
    </author>
    <author>
      <name>Güliz Seray Tuncay</name>
    </author>
    <author>
      <name>Roger Piqueras Jover</name>
    </author>
    <author>
      <name>Michael A. Specter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3448609</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3448609" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Privacy and Security, Volume 24, Issue 3,
  Article No. 19, 2021, pp 1-35</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.05572v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05572v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.09984v1</id>
    <updated>2019-04-23T14:14:49Z</updated>
    <published>2019-04-23T14:14:49Z</published>
    <title>IOArbiter: Dynamic Provisioning of Backend Block Storage in the Cloud</title>
    <summary>  With the advent of virtualization technology, cloud computing realizes
on-demand computing. The capability of dynamic resource provisioning is a
fundamental driving factor for users to adopt the cloud technology. The aspect
is important for cloud service providers to optimize the expense for running
the infrastructure as well. Despite many technological advances in related
areas, however, it is still the case that the infrastructure providers must
decide hardware configuration before deploying a cloud infrastructure,
especially from the storage's perspective. This static nature of the storage
provisioning practice can cause many problems in meeting tenant requirements,
which often come later into the picture. In this paper, we propose a system
called IOArbiter that enables the dynamic creation of underlying storage
implementation in the cloud. IOArbiter defers storage provisioning to the time
at which a tenant actually requests a storage space. As a result, an underlying
storage implementation, e.g., RAID-5, 6 or Ceph storage pool with 6+3 erasure
coding, will be materialized at the volume creation time. Using our prototype
implementation with Openstack Cinder, we show that IOArbiter can simultaneously
satisfy a number of different tenant demands, which may not be possible with a
static configuration strategy. Additionally QoS mechanisms such as admission
control and dynamic throttling help the system mitigate a noisy neighbor
problem significantly.
</summary>
    <author>
      <name>Moo-Ryong Ra</name>
    </author>
    <author>
      <name>Hee Won Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.09984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.09984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.12595v1</id>
    <updated>2019-04-20T16:11:54Z</updated>
    <published>2019-04-20T16:11:54Z</published>
    <title>MANA for MPI: MPI-Agnostic Network-Agnostic Transparent Checkpointing</title>
    <summary>  Transparently checkpointing MPI for fault tolerance and load balancing is a
long-standing problem in HPC. The problem has been complicated by the need to
provide checkpoint-restart services for all combinations of an MPI
implementation over all network interconnects. This work presents MANA
(MPI-Agnostic Network-Agnostic transparent checkpointing), a single code base
which supports all MPI implementation and interconnect combinations. The
agnostic properties imply that one can checkpoint an MPI application under one
MPI implementation and perhaps over TCP, and then restart under a second MPI
implementation over InfiniBand on a cluster with a different number of CPU
cores per node. This technique is based on a novel "split-process" approach,
which enables two separate programs to co-exist within a single process with a
single address space. This work overcomes the limitations of the two most
widely adopted transparent checkpointing solutions, BLCR and DMTCP/InfiniBand,
which require separate modifications to each MPI implementation and/or
underlying network API. The runtime overhead is found to be insignificant both
for checkpoint-restart within a single host, and when comparing a local MPI
computation that was migrated to a remote cluster against an ordinary MPI
computation running natively on that same remote cluster.
</summary>
    <author>
      <name>Rohan Garg</name>
    </author>
    <author>
      <name>Gregory Price</name>
    </author>
    <author>
      <name>Gene Cooperman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages; 9 figures; accepted at HPDC-2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.12595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.12595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.02529v1</id>
    <updated>2019-05-07T13:09:32Z</updated>
    <published>2019-05-07T13:09:32Z</published>
    <title>Programming Unikernels in the Large via Functor Driven Development</title>
    <summary>  Compiling applications as unikernels allows them to be tailored to diverse
execution environments. Dependency on a monolithic operating system is replaced
with linkage against libraries that provide specific services. Doing so in
practice has revealed a major barrier: managing the configuration matrix across
heterogenous execution targets. A realistic unikernel application depends on
hundreds of libraries, each of which may place different demands on the
different target execution platforms (e.g.,~cryptographic acceleration).
  We propose a modular approach to structuring large scale codebases that
cleanly separates configuration, application and operating system logic. Our
implementation is built on the \mirage unikernel framework, using the \ocaml
language's powerful abstraction and metaprogramming facilities. Leveraging
modules allows us to build many components independently, with only loose
coupling through a set of standardised signatures. Components can be
parameterized by other components and composed. Our approach accounts for
state, dependency ordering, and error management, and our usage over the years
has demonstrated significant efficiency benefits by leveraging compiler
features such as global link-time optimisation during the configuration
process. We describe our application architecture and experiences via some
practical applications of our approach, and discuss how library development in
\mirage can facilitate adoption in other unikernel frameworks and programming
languages.
</summary>
    <author>
      <name>Gabriel Radanne</name>
    </author>
    <author>
      <name>Thomas Gazagnaire</name>
    </author>
    <author>
      <name>Anil Madhavapeddy</name>
    </author>
    <author>
      <name>Jeremy Yallop</name>
    </author>
    <author>
      <name>Richard Mortier</name>
    </author>
    <author>
      <name>Hannes Mehnert</name>
    </author>
    <author>
      <name>Mindy Preston</name>
    </author>
    <author>
      <name>David Scott</name>
    </author>
    <link href="http://arxiv.org/abs/1905.02529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.02529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.05975v1</id>
    <updated>2019-05-15T06:54:55Z</updated>
    <published>2019-05-15T06:54:55Z</published>
    <title>Neverland: Lightweight Hardware Extensions for Enforcing Operating
  System Integrity</title>
    <summary>  The security of applications hinges on the trustworthiness of the operating
system, as applications rely on the OS to protect code and data. As a result,
multiple protections for safeguarding the integrity of kernel code and data are
being continuously proposed and deployed. These existing protections, however,
are far from ideal as they either provide partial protection, or require
complex and high overhead hardware and software stacks.
  In this work, we present Neverland: a low-overhead, hardware-assisted, memory
protection scheme that safeguards the operating system from rootkits and
kernel-mode malware. Once the system is done booting, Neverland's hardware
takes away the operating system's ability to overwrite certain configuration
registers, as well as portions of its own physical address space that contain
kernel code and security-critical data. Furthermore, it prohibits the CPU from
fetching privileged code from any memory region lying outside the physical
addresses assigned to the OS kernel and drivers (regardless of virtual page
permissions). This combination of protections makes it extremely hard for an
attacker to tamper with the kernel or introduce new privileged code into the
system -- even in the presence of kernel vulnerabilities. Our evaluations show
that the extra hardware required to support these protections incurs minimal
silicon and energy overheads. Neverland enables operating systems to reduce
their attack surface without having to rely on complex integrity monitoring
software or hardware.
</summary>
    <author>
      <name>Salessawi Ferede Yitbarek</name>
    </author>
    <author>
      <name>Todd Austin</name>
    </author>
    <link href="http://arxiv.org/abs/1905.05975v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.05975v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.11788v1</id>
    <updated>2019-05-28T13:09:06Z</updated>
    <published>2019-05-28T13:09:06Z</published>
    <title>$Δ$elta: Differential Energy-Efficiency, Latency, and Timing
  Analysis for Real-Time Networks</title>
    <summary>  The continuously increasing degree of automation in many areas (e.g.
manufacturing engineering, public infrastructure) lead to the construction of
cyber-physical systems and cyber-physical networks. To both, time and energy
are the most critical operating resources. Considering for instance the Tactile
Internet specification, end-to-end latencies in these systems must be below
1ms, which means that both communication and system latencies are in the same
order of magnitude and must be predictably low. As control loops are commonly
handled over different variants of network infrastructure (e.g. mobile and
fibre links) particular attention must be payed to the design of reliable, yet
fast and energy-efficient data-transmission channels that are robust towards
unexpected transmission failures. As design goals are often conflicting (e.g.
high performance vs. low energy), it is necessary to analyze and investigate
trade-offs with regards to design decisions during the construction of
cyber-physical networks. In this paper, we present $\Delta$elta, an approach
towards a tool-supported construction process for cyber-physical networks.
$\Delta$elta extends the previously presented X-Lap tool by new analysis
features, but keeps the original measurements facilities unchanged.
$\Delta$elta jointly analyzes and correlates the runtime behavior (i.e.
performance, latency) and energy demand of individual system components. It
provides an automated analysis with precise thread-local time interpolation,
control-flow extraction, and examination of latency criticality. We further
demonstrate the applicability of $\Delta$elta with an evaluation of a
prototypical implementation.
</summary>
    <author>
      <name>Stefan Reif</name>
    </author>
    <author>
      <name>Andreas Schmidt</name>
    </author>
    <author>
      <name>Timo Hönig</name>
    </author>
    <author>
      <name>Thorsten Herfet</name>
    </author>
    <author>
      <name>Wolfgang Schröder-Preikschat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3314206.3314211</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3314206.3314211" rel="related"/>
    <link href="http://arxiv.org/abs/1905.11788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.11788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.07159v1</id>
    <updated>2019-08-20T04:22:58Z</updated>
    <published>2019-08-20T04:22:58Z</published>
    <title>MicroTEE: Designing TEE OS Based on the Microkernel Architecture</title>
    <summary>  ARM TrustZone technology is widely used to provide Trusted Execution
Environments (TEE) for mobile devices. However, most TEE OSes are implemented
as monolithic kernels. In such designs, device drivers, kernel services and
kernel modules all run in the kernel, which results in large size of the
kernel. It is difficult to guarantee that all components of the kernel have no
security vulnerabilities in the monolithic kernel architecture, such as the
integer overflow vulnerability in Qualcomm QSEE TrustZone and the TZDriver
vulnerability in HUAWEI Hisilicon TEE architecture. This paper presents
MicroTEE, a TEE OS based on the microkernel architecture. In MicroTEE, the
microkernel provides strong isolation for TEE OS's basic services, such as
crypto service and platform key management service. The kernel is only
responsible for providing core services such as address space management,
thread management, and inter-process communication. Other fundamental services,
such as crypto service and platform key management service are implemented as
applications at the user layer. Crypto Services and Key Management are used to
provide Trusted Applications (TAs) with sensitive information encryption, data
signing, and platform attestation functions. Our design avoids the compromise
of the whole TEE OS if only one kernel service is vulnerable. A monitor has
also been added to perform the switch between the secure world and the normal
world. Finally, we implemented a MicroTEE prototype on the Freescale i.MX6Q
Sabre Lite development board and tested its performance. Evaluation results
show that the performance of cryptographic operations in MicroTEE is better
than it in Linux when the size of data is small.
</summary>
    <author>
      <name>Dongxu Ji</name>
    </author>
    <author>
      <name>Qianying Zhang</name>
    </author>
    <author>
      <name>Shijun Zhao</name>
    </author>
    <author>
      <name>Zhiping Shi</name>
    </author>
    <author>
      <name>Yong Guan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TrustCom/BigDataSE.2019.00014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TrustCom/BigDataSE.2019.00014" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.07159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.07159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.05106v2</id>
    <updated>2020-06-02T02:54:13Z</updated>
    <published>2019-10-07T02:13:19Z</published>
    <title>Assise: Performance and Availability via NVM Colocation in a Distributed
  File System</title>
    <summary>  The adoption of very low latency persistent memory modules (PMMs) upends the
long-established model of disaggregated file system access. Instead, by
colocating computation and PMM storage, we can provide applications much higher
I/O performance, sub-second application failover, and strong consistency. To
demonstrate this, we built the Assise distributed file system, based on a
persistent, replicated coherence protocol for managing a set of
server-colocated PMMs as a fast, crash-recoverable cache between applications
and slower disaggregated storage, such as SSDs. Unlike disaggregated file
systems, Assise maximizes locality for all file IO by carrying out IO on
colocated PMM whenever possible and minimizes coherence overhead by maintaining
consistency at IO operation granularity, rather than at fixed block sizes.
  We compare Assise to Ceph/Bluestore, NFS, and Octopus on a cluster with Intel
Optane DC PMMs and SSDs for common cloud applications and benchmarks, such as
LevelDB, Postfix, and FileBench. We find that Assise improves write latency up
to 22x, throughput up to 56x, fail-over time up to 103x, and scales up to 6x
better than its counterparts, while providing stronger consistency semantics.
Assise promises to beat the MinuteSort world record by 1.5x.
</summary>
    <author>
      <name>Thomas E. Anderson</name>
    </author>
    <author>
      <name>Marco Canini</name>
    </author>
    <author>
      <name>Jongyul Kim</name>
    </author>
    <author>
      <name>Dejan Kostić</name>
    </author>
    <author>
      <name>Youngjin Kwon</name>
    </author>
    <author>
      <name>Simon Peter</name>
    </author>
    <author>
      <name>Waleed Reda</name>
    </author>
    <author>
      <name>Henry N. Schuh</name>
    </author>
    <author>
      <name>Emmett Witchel</name>
    </author>
    <link href="http://arxiv.org/abs/1910.05106v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.05106v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.13208v3</id>
    <updated>2022-09-06T11:10:37Z</updated>
    <published>2019-11-26T21:05:00Z</published>
    <title>Intelligent Resource Scheduling for Co-located Latency-critical
  Services: A Multi-Model Collaborative Learning Approach</title>
    <summary>  Latency-critical services have been widely deployed in cloud environments.
For cost-efficiency, multiple services are usually co-located on a server.
Thus, run-time resource scheduling becomes the pivot for QoS control in these
complicated co-location cases. However, the scheduling exploration space
enlarges rapidly with the increasing server resources, making the schedulers
hardly provide ideal solutions quickly. More importantly, we observe that there
are "resource cliffs" in the scheduling exploration space. They affect the
exploration efficiency and always lead to severe QoS fluctuations. Resource
cliffs cannot be easily avoided in previous schedulers. To address these
problems, we propose a novel ML-based intelligent scheduler - OSML. It learns
the correlation between architectural hints (e.g., IPC, cache misses, memory
footprint, etc.), scheduling solutions and the QoS demands based on a data set
we collected from 11 widely deployed services running on off-the-shelf servers.
OSML employs multiple ML models to work collaboratively to predict QoS
variations, shepherd the scheduling, and recover from QoS violations in
complicated co-location cases. OSML can intelligently avoid resource cliffs
during scheduling and reach an optimal solution much faster than previous
approaches for co-located LC services. Experimental results show that OSML
supports higher loads and meets QoS targets with lower scheduling overheads and
shorter convergence time than previous studies.
</summary>
    <author>
      <name>Lei Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1911.13208v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.13208v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.03923v1</id>
    <updated>2019-12-09T09:39:31Z</updated>
    <published>2019-12-09T09:39:31Z</published>
    <title>Nova -- A rainbow cloud over the Alps</title>
    <summary>  A pooled and shared on-demand Infrastructure as a Service (IaaS), based on
the Openstack software suite, was rolled out on the Grenoble university campus
in 2018 and updated in 2019.We present the methods used to deploy and manage
the infrastructure: racadm and preseed for basic system installation, then
Kolla for Openstack deployment. This latter solution, based on containers for
each service, enables a centralised and logged configuration (GitLab) of
controllers and calculation nodes. The solution is the benchmark solution for a
reproducible deployment of Openstack. We have been able to expand our cloud
easily with new nodes. The change in version of the basic OS was also
successfully tested despite a few small hitches... As security is a key element
in the proper operation of this type of shared service, each project has been
made watertight and its data perfectly isolated from other projects, thanks to
the encryption of all network flows in VXLANs.This OpenStack infotainment
platform is operational. What is it all for? For example, our first users use
the Jupyter Notebook through the provision of Jupyterhub servers (web portal);
the Distributed Health Assessment IT System (SIDES project); the continuous
integration in connection with the GitLab platform; the test for the Kubernetes
container scheduler or the calculation and visualisation software, etc. Highly
varied uses that other platforms had difficulty offering.Nova, a new platform,
was born.
</summary>
    <author>
      <name>Nicolas Gibelin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GRICAD</arxiv:affiliation>
    </author>
    <author>
      <name>Rémi Cailletaud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">OSUG</arxiv:affiliation>
    </author>
    <author>
      <name>Gabriel Moreau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LEGI</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-François Scariot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GRICAD</arxiv:affiliation>
    </author>
    <author>
      <name>Gabrielle Feltin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GRICAD</arxiv:affiliation>
    </author>
    <author>
      <name>Anthony Defize</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GRICAD</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Vid{\'e}o
  https://replay.jres.org/videos/watch/c0ce8c10-fc41-4cf7-9069-9d2c225f9e0a ,
  in French, Congr\`es JRES : Les Journ\'ees R\'eseaux de l'Enseignement et de
  la Recherche, RENATER, Dec 2019, Dijon, France</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.03923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.03923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.06863v1</id>
    <updated>2019-12-14T15:17:00Z</updated>
    <published>2019-12-14T15:17:00Z</published>
    <title>Survivor: A Fine-Grained Intrusion Response and Recovery Approach for
  Commodity Operating Systems</title>
    <summary>  Despite the deployment of preventive security mechanisms to protect the
assets and computing platforms of users, intrusions eventually occur. We
propose a novel intrusion survivability approach to withstand ongoing
intrusions. Our approach relies on an orchestration of fine-grained recovery
and per-service responses (e.g., privileges removal). Such an approach may put
the system into a degraded mode. This degraded mode prevents attackers to
reinfect the system or to achieve their goals if they managed to reinfect it.
It maintains the availability of core functions while waiting for patches to be
deployed. We devised a cost-sensitive response selection process to ensure that
while the service is in a degraded mode, its core functions are still
operating. We built a Linux-based prototype and evaluated the effectiveness of
our approach against different types of intrusions. The results show that our
solution removes the effects of the intrusions, that it can select appropriate
responses, and that it allows services to survive when reinfected. In terms of
performance overhead, in most cases, we observed a small overhead, except in
the rare case of services that write many small files asynchronously in a
burst, where we observed a higher but acceptable overhead.
</summary>
    <author>
      <name>Ronny Chevalier</name>
    </author>
    <author>
      <name>David Plaquin</name>
    </author>
    <author>
      <name>Chris Dalton</name>
    </author>
    <author>
      <name>Guillaume Hiet</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3359789.3359792</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3359789.3359792" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The final version of this paper has been published in the Proceedings
  of the 35th Annual Computer Security Applications Conference (ACSAC), 2019.
  14 pages, 5 figures, 6 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 35th Annual Computer Security Applications
  Conference. ACM, 2019. p. 762-775</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1912.06863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.06863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.10367v2</id>
    <updated>2020-06-12T09:57:12Z</updated>
    <published>2019-12-22T00:58:54Z</published>
    <title>Dispel: Byzantine SMR with Distributed Pipelining</title>
    <summary>  Byzantine State Machine Replication (SMR) is a long studied topic that
received increasing attention recently with the advent of blockchains as
companies are trying to scale them to hundreds of nodes. Byzantine SMRs try to
increase throughput by either reducing the latency of consensus instances that
they run sequentially or by reducing the number of replicas that send messages
to others in order to reduce the network usage. Unfortunately, the former
approach makes use of resources in burst whereas the latter requires
CPU-intensive authentication mechanisms.
  In this paper, we propose a new Byzantine SMR called Dispel (Distributed
Pipeline) that allows any node to distributively start new consensus instances
whenever they detect sufficient resources locally. We evaluate the performance
of Dispel within a single datacenter and across up to 380 machines over 3
continents by comparing it against four other SMRs. On 128 nodes, Dispel speeds
up HotStuff, the Byzantine fault tolerant SMR being integrated within
Facebook's blockchain, by more than 12 times. In addition, we also test Dispel
under isolated and correlated failures and show that the Dispel distributed
design is more robust than HotStuff. Finally, we evaluate Dispel in a
cryptocurrency application with Bitcoin transactions and show that this SMR is
not the bottleneck.
</summary>
    <author>
      <name>Gauthier Voron</name>
    </author>
    <author>
      <name>Vincent Gramoli</name>
    </author>
    <link href="http://arxiv.org/abs/1912.10367v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.10367v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.12516v1</id>
    <updated>2020-02-28T02:37:26Z</updated>
    <published>2020-02-28T02:37:26Z</published>
    <title>Bringing Inter-Thread Cache Benefits to Federated Scheduling -- Extended
  Results &amp; Technical Report</title>
    <summary>  Multiprocessor scheduling of hard real-time tasks modeled by directed acyclic
graphs (DAGs) exploits the inherent parallelism presented by the model. For DAG
tasks, a node represents a request to execute an object on one of the available
processors. In one DAG task, there may be multiple execution requests for one
object, each represented by a distinct node. These distinct execution requests
offer an opportunity to reduce their combined cache overhead through
coordinated scheduling of objects as threads within a parallel task. The goal
of this work is to realize this opportunity by incorporating the cache-aware
BUNDLE-scheduling algorithm into federated scheduling of sporadic DAG task
sets.
  This is the first work to incorporate instruction cache sharing into
federated scheduling. The result is a modification of the DAG model named the
DAG with objects and threads (DAG-OT). Under the DAG-OT model, descriptions of
nodes explicitly include their underlying executable object and number of
threads. When possible, nodes assigned the same executable object are collapsed
into a single node; joining their threads when BUNDLE-scheduled. Compared to
the DAG model, the DAG-OT model with cache-aware scheduling reduces the number
of cores allocated to individual tasks by approximately 20 percent in the
synthetic evaluation and up to 50 percent on a novel parallel computing
platform implementation. By reducing the number of allocated cores, the DAG-OT
model is able to schedule a subset of previously infeasible task sets.
</summary>
    <author>
      <name>Corey Tessler</name>
    </author>
    <author>
      <name>Venkata P. Modekurthy</name>
    </author>
    <author>
      <name>Nathan Fisher</name>
    </author>
    <author>
      <name>Abusayeed Saifullah</name>
    </author>
    <link href="http://arxiv.org/abs/2002.12516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.12516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.01107v1</id>
    <updated>2020-03-03T05:33:23Z</updated>
    <published>2020-03-03T05:33:23Z</published>
    <title>Reconfigurable Parallel Architecture of High Speed Round Robin Arbiter</title>
    <summary>  With a view to managing the increasing traffic in computer networks, round
robin arbiter has been proposed to work with packet switching system to have
increased speed in providing access and scheduling. Round robin arbiter is a
doorway to a particular bus based on request along with equal priority and
gives turns to devices connected to it in a cyclic order. Considering the rapid
growth in computer networking and the emergence of computer automation which
will need much more access to the existing limited resources, this paper
emphasizes on designing a reconfigurable round robin arbiter over FPGA which
takes parallel requests and processes them with high efficiency and less delay
than existing designs. Proposed round robin arbiter encounters with 4 to 12
devices. Results show that with 200% increment in the number of connected
devices, only 2.69% increment has been found in the delay. With less delay,
proposed round robin arbiter exhibits high speed performance with higher
traffic, which is a new feature in comparison with the existing designs.
</summary>
    <author>
      <name>Arnab Paul</name>
    </author>
    <author>
      <name>Mamdudul Haque Khan</name>
    </author>
    <author>
      <name>M. Muktadir Rahman</name>
    </author>
    <author>
      <name>Tanvir Zaman Khan</name>
    </author>
    <author>
      <name>Prajoy Podder</name>
    </author>
    <author>
      <name>Md. Yeasir Akram Khan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/EESCO.2015.7253744</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/EESCO.2015.7253744" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in 2015 International Conference on Electrical,
  Electronics, Signals, Communication and Optimization (EESCO)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2015 International Conference on Electrical, Electronics, Signals,
  Communication and Optimization (EESCO)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.01107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.01107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.12452v1</id>
    <updated>2020-03-25T00:37:56Z</updated>
    <published>2020-03-25T00:37:56Z</published>
    <title>FLIC: A Distributed Fog Cache for City-Scale Applications</title>
    <summary>  We present FLIC, a distributed software data caching framework for fogs that
reduces network traffic and latency. FLICis targeted toward city-scale
deployments of cooperative IoT devices in which each node gathers and shares
data with surrounding devices. As machine learning and other data processing
techniques that require large volumes of training data are ported to low-cost
and low-power IoT systems, we expect that data analysis will be moved away from
the cloud. Separation from the cloud will reduce reliance on power-hungry
centralized cloud-based infrastructure. However, city-scale deployments of
cooperative IoT devices often connect to the Internet with cellular service, in
which service charges are proportional to network usage. IoT system architects
must be clever in order to keep costs down in these scenarios. To reduce the
network bandwidth required to operate city-scale deployments of cooperative IoT
systems, FLIC implements a distributed cache on the IoT nodes in the fog. FLIC
allows the IoT network to share its data without repetitively interacting with
a simple cloud storage service reducing calls out to a backing store. Our
results displayed a less than 2% miss rate on reads. Thus, allowing for only 5%
of requests needing the backing store. We were also able to achieve more than
50% reduction in bytes transmitted per second.
</summary>
    <author>
      <name>Jack West</name>
    </author>
    <author>
      <name>Neil Kingensmith</name>
    </author>
    <author>
      <name>George K. Thiruvathukal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at 2020 IEEE International Conference on Fog Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.12452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.12452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.05518v1</id>
    <updated>2020-04-12T01:25:04Z</updated>
    <published>2020-04-12T01:25:04Z</published>
    <title>Hardware Memory Management for Future Mobile Hybrid Memory Systems</title>
    <summary>  The current mobile applications have rapidly growing memory footprints,
posing a great challenge for memory system design. Insufficient DRAM main
memory will incur frequent data swaps between memory and storage, a process
that hurts performance, consumes energy and deteriorates the write endurance of
typical flash storage devices. Alternately, a larger DRAM has higher leakage
power and drains the battery faster. Further, DRAM scaling trends make further
growth of DRAMin the mobile space prohibitive due to cost. Emerging
non-volatile memory (NVM) has the potential to alleviate these issues due to
its higher capacity per cost than DRAM and mini-mal static power. Recently, a
wide spectrum of NVM technologies, including phase-change memories (PCM),
memristor, and 3D XPoint have emerged. Despite the mentioned advantages, NVM
has longer access latency compared to DRAMand NVM writes can incur higher
latencies and wear costs. Therefore integration of these new memory
technologies in the memory hierarchy requires a fundamental rearchitect-ing of
traditional system designs. In this work, we propose a hardware-accelerated
memory manager (HMMU) that addresses both types of memory in a flat space
address space. We design a set of data placement and data migration policies
within this memory manager, such that we may exploit the advantages of each
memory technology. By augmenting the system with this HMMU, we reduce the
overall memory latency while also reducing writes to the NVM. Experimental
results show that our design achieves a 39% reduction in energy consumption
with only a 12% performance degradation versus an all-DRAM baseline that is
likely untenable in the future.
</summary>
    <author>
      <name>Fei Wen</name>
    </author>
    <author>
      <name>Mian Qin</name>
    </author>
    <author>
      <name>Paul Gratz</name>
    </author>
    <author>
      <name>Narasimha Reddy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCAD.2020.3012213</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCAD.2020.3012213" rel="related"/>
    <link href="http://arxiv.org/abs/2004.05518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.05518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.11050v1</id>
    <updated>2020-05-22T08:14:04Z</updated>
    <published>2020-05-22T08:14:04Z</published>
    <title>Autonomous Task Dropping Mechanism to Achieve Robustness in
  Heterogeneous Computing Systems</title>
    <summary>  Robustness of a distributed computing system is defined as the ability to
maintain its performance in the presence of uncertain parameters. Uncertainty
is a key problem in heterogeneous (and even homogeneous) distributed computing
systems that perturbs system robustness. Notably, the performance of these
systems is perturbed by uncertainty in both task execution time and arrival.
Accordingly, our goal is to make the system robust against these uncertainties.
Considering task execution time as a random variable, we use probabilistic
analysis to develop an autonomous proactive task dropping mechanism to attain
our robustness goal. Specifically, we provide a mathematical model that
identifies the optimality of a task dropping decision, so that the system
robustness is maximized. Then, we leverage the mathematical model to develop a
task dropping heuristic that achieves the system robustness within a feasible
time complexity. Although the proposed model is generic and can be applied to
any distributed system, we concentrate on heterogeneous computing (HC) systems
that have a higher degree of exposure to uncertainty than homogeneous systems.
Experimental results demonstrate that the autonomous proactive dropping
mechanism can improve the system robustness by up to 20%.
</summary>
    <author>
      <name>Ali Mokhtari</name>
    </author>
    <author>
      <name>Chavit Denninnart</name>
    </author>
    <author>
      <name>Mohsen Amini Salehi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">in 29th Heterogeneity in Computing Workshop (HCW 2019), in the
  Proceedings of the IPDPS 2019 Workshops &amp; PhD Forum (IPDPSW)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.11050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.11050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.02055v1</id>
    <updated>2020-06-03T05:47:14Z</updated>
    <published>2020-06-03T05:47:14Z</published>
    <title>The Art of CPU-Pinning: Evaluating and Improving the Performance of
  Virtualization and Containerization Platforms</title>
    <summary>  Cloud providers offer a variety of execution platforms in form of bare-metal,
VM, and containers. However, due to the pros and cons of each execution
platform, choosing the appropriate platform for a specific cloud-based
application has become a challenge for solution architects. The possibility to
combine these platforms (e.g. deploying containers within VMs) offers new
capacities that makes the challenge even further complicated. However, there is
a little study in the literature on the pros and cons of deploying different
application types on various execution platforms. In particular, evaluation of
diverse hardware configurations and different CPU provisioning methods, such as
CPU pinning, have not been sufficiently studied in the literature. In this
work, the performance overhead of container, VM, and bare-metal execution
platforms are measured and analyzed for four categories of real-world
applications, namely video processing, parallel processing (MPI), web
processing, and No-SQL, respectively representing CPU intensive, parallel
processing, and two IO intensive processes. Our analyses reveal a set of
interesting and sometimes counterintuitive findings that can be used as best
practices by the solution architects to efficiently deploy cloud-based
applications. Here are some notable mentions: (A) Under specific circumstances,
containers can impose a higher overhead than VMs; (B) Containers on top of VMs
can mitigate the overhead of VMs for certain applications; (C) Containers with
a large number of cores impose a lower overhead than those with a few cores.
</summary>
    <author>
      <name>Davood Ghatreh Samani</name>
    </author>
    <author>
      <name>Chavit Denninnart</name>
    </author>
    <author>
      <name>Josef Bacik</name>
    </author>
    <author>
      <name>Mohsen Amini Salehi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 49th International Conference on Parallel Processing (ICPP
  2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2006.02055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.02055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.07086v2</id>
    <updated>2020-08-03T06:04:27Z</updated>
    <published>2020-06-12T11:18:04Z</published>
    <title>An Adaptive Approach to Recoverable Mutual Exlcusion</title>
    <summary>  Mutual exclusion (ME) is one of the most commonly used techniques to handle
conflicts in concurrent systems. Traditionally, mutual exclusion algorithms
have been designed under the assumption that a process does not fail while
acquiring/releasing a lock or while executing its critical section. However,
failures do occur in real life, potentially leaving the lock in an inconsistent
state. This gives rise to the problem of \emph{recoverable mutual exclusion
(RME)} that involves designing a mutual exclusion algorithm that can tolerate
failures, while maintaining safety and liveness properties.
  One of the important measures of performance of any ME algorithm, including
an RME algorithm, is the number of \emph{remote memory references (RMRs)} made
by a process (for acquiring and releasing a lock as well as recovering the lock
structure after a failure). The best known RME algorithm solves the problem for
$n$ processes in sub-logarithmic number of RMRs, given by
$\mathcal{O}(\frac{\log n}{\log \log n})$, irrespective of the number of
failures in the system.
  In this work, we present a new algorithm for solving the RME problem whose
RMR complexity gradually \emph{adapts} to the number of failures that have
occurred in the system "recently". In the absence of failures, our algorithm
generates only $\mathcal{O}(1)$ RMRs. Furthermore, its RMR complexity is given
by $\mathcal{O}(\min\{ \sqrt{F}, \frac{\log n}{\log \log n} \})$ where $F$ is
the total number of failures in the "recent" past. In addition to read and
write instructions, our algorithm uses compare-and-swap (\CAS{}) and
fetch-and-store (\FAS{}) hardware instructions, both of which are commonly
available in most modern processors.
</summary>
    <author>
      <name>Sahil Dhoked</name>
    </author>
    <author>
      <name>Neeraj Mittal</name>
    </author>
    <link href="http://arxiv.org/abs/2006.07086v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.07086v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.07163v2</id>
    <updated>2020-06-16T08:32:14Z</updated>
    <published>2020-06-12T13:21:59Z</published>
    <title>Nefele: Process Orchestration for the Cloud</title>
    <summary>  Virtualization, either at OS- or hardware level, plays an important role in
cloud computing. It enables easier automation and faster deployment in
distributed environments. While virtualized infrastructures provide a level of
management flexibility, they lack practical abstraction of the distributed
resources. A developer in such an environment still needs to deal with all the
complications of building a distributed software system. Different
orchestration systems are built to provide that abstraction; however, they do
not solve the inherent challenges of distributed systems, such as
synchronization issues or resilience to failures. This paper introduces Nefele,
a decentralized process orchestration system that automatically deploys and
manages individual processes, rather than containers/VMs, within a cluster.
Nefele is inspired by the Single System Image (SSI) vision of mitigating the
intricacies of remote execution, yet it maintains the flexibility and
performance of virtualized infrastructures. Nefele offers a set of APIs for
building cloud-native applications that lets the developer easily build,
deploy, and scale applications in a cloud environment. We have implemented and
deployed Nefele on a cluster in our datacenter and evaluated its performance.
Our evaluations show that Nefele can effectively deploy, scale, and monitor
processes across a distributed environment, while it incorporates essential
primitives to build a distributed software system.
</summary>
    <author>
      <name>Mina Sedaghat</name>
    </author>
    <author>
      <name>Pontus Sköldström</name>
    </author>
    <author>
      <name>Daniel Turull</name>
    </author>
    <author>
      <name>Vinay Yadhav</name>
    </author>
    <author>
      <name>Joacim Halén</name>
    </author>
    <author>
      <name>Madhubala Ganesan</name>
    </author>
    <author>
      <name>Amardeep Mehta</name>
    </author>
    <author>
      <name>Wolfgang John</name>
    </author>
    <link href="http://arxiv.org/abs/2006.07163v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.07163v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.7; D.4.8; D.4.1; D.4.3; D.4.4; F.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.06775v3</id>
    <updated>2021-01-19T18:35:27Z</updated>
    <published>2020-07-14T02:16:56Z</published>
    <title>Analyzing and Mitigating Data Stalls in DNN Training</title>
    <summary>  Training Deep Neural Networks (DNNs) is resource-intensive and
time-consuming. While prior research has explored many different ways of
reducing DNN training time, the impact of input data pipeline, i.e., fetching
raw data items from storage and performing data pre-processing in memory, has
been relatively unexplored. This paper makes the following contributions: (1)
We present the first comprehensive analysis of how the input data pipeline
affects the training time of widely-used computer vision and audio Deep Neural
Networks (DNNs), that typically involve complex data preprocessing. We analyze
nine different models across three tasks and four datasets while varying
factors such as the amount of memory, number of CPU threads, storage device,
GPU generation etc on servers that are a part of a large production cluster at
Microsoft. We find that in many cases, DNN training time is dominated by data
stall time: time spent waiting for data to be fetched and preprocessed. (2) We
build a tool, DS-Analyzer to precisely measure data stalls using a differential
technique, and perform predictive what-if analysis on data stalls. (3) Finally,
based on the insights from our analysis, we design and implement three simple
but effective techniques in a data-loading library, CoorDL, to mitigate data
stalls. Our experiments on a range of DNN tasks, models, datasets, and hardware
configs show that when PyTorch uses CoorDL instead of the state-of-the-art DALI
data loading library, DNN training time is reduced significantly (by as much as
5x on a single server).
</summary>
    <author>
      <name>Jayashree Mohan</name>
    </author>
    <author>
      <name>Amar Phanishayee</name>
    </author>
    <author>
      <name>Ashish Raniwala</name>
    </author>
    <author>
      <name>Vijay Chidambaram</name>
    </author>
    <link href="http://arxiv.org/abs/2007.06775v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.06775v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.07917v1</id>
    <updated>2020-12-14T20:02:26Z</updated>
    <published>2020-12-14T20:02:26Z</published>
    <title>The Design and Implementation of a Verified File System with End-to-End
  Data Integrity</title>
    <summary>  Despite significant research and engineering efforts, many of today's
important computer systems suffer from bugs. To increase the reliability of
software systems, recent work has applied formal verification to certify the
correctness of such systems, with recent successes including certified file
systems and certified cryptographic protocols, albeit using quite different
proof tactics and toolchains. Unifying these concepts, we present the first
certified file system that uses cryptographic primitives to protect itself
against tampering. Our certified file system defends against adversaries that
might wish to tamper with the raw disk. Such an "untrusted storage" threat
model captures the behavior of storage devices that might silently return
erroneous bits as well as adversaries who might have limited access to a disk,
perhaps while in transit. In this paper, we present IFSCQ, a certified
cryptographic file system with strong integrity guarantees. IFSCQ combines and
extends work on cryptographic file systems and formally certified file systems
to prove that our design is correct. It is the first certified file system that
is secure against strong adversaries that can maliciously corrupt on-disk data
and metadata, including attempting to roll back the disk to earlier versions of
valid data. IFSCQ achieves this by constructing a Merkle hash tree of the whole
disk, and by proving that tampered disk blocks will always be detected if they
ever occur. We demonstrate that IFSCQ runs with reasonable overhead while
detecting several kinds of attacks.
</summary>
    <author>
      <name>Daniel W. Song</name>
    </author>
    <author>
      <name>Konstantinos Mamouras</name>
    </author>
    <author>
      <name>Ang Chen</name>
    </author>
    <author>
      <name>Nathan Dautenhahn</name>
    </author>
    <author>
      <name>Dan S. Wallach</name>
    </author>
    <link href="http://arxiv.org/abs/2012.07917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.07917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.4; D.4.3; D.4.5; D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.12868v4</id>
    <updated>2021-12-08T19:57:34Z</updated>
    <published>2020-12-23T18:34:45Z</published>
    <title>Flat-Combining-Based Persistent Data Structures for Non-Volatile Memory</title>
    <summary>  Flat combining (FC) is a synchronization paradigm in which a single thread,
holding a global lock, collects requests by multiple threads for accessing a
concurrent data structure and applies their combined requests to it. Although
FC is sequential, it significantly reduces synchronization overheads and cache
invalidations and thus often provides better performance than that of lock-free
implementations. The recent emergence of non-volatile memory (NVM) technologies
increases the interest in the development of persistent concurrent objects.
These are objects that are able to recover from system failures and ensure
consistency by retaining their state in NVM and fixing it, if required, upon
recovery. Of particular interest are detectable objects that, in addition to
ensuring consistency, allow recovery code to infer if a failed operation took
effect before the crash and, if it did, obtain its response. In this work, we
present the first FC-based persistent object implementations. Specifically, we
introduce a detectable FC-based implementation of a concurrent LIFO stack, a
concurrent FIFO queue, and a double-ended queue. Our empirical evaluation
establishes that due to flat combining, the novel implementations require a
much smaller number of costly persistence instructions than competing
algorithms and are therefore able to significantly outperform them.
</summary>
    <author>
      <name>Matan Rusanovsky</name>
    </author>
    <author>
      <name>Hagit Attiya</name>
    </author>
    <author>
      <name>Ohad Ben-Baruch</name>
    </author>
    <author>
      <name>Tom Gerby</name>
    </author>
    <author>
      <name>Danny Hendler</name>
    </author>
    <author>
      <name>Pedro Ramalhete</name>
    </author>
    <link href="http://arxiv.org/abs/2012.12868v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.12868v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.14219v3</id>
    <updated>2022-07-06T10:10:41Z</updated>
    <published>2020-12-28T13:03:04Z</published>
    <title>SimBricks: End-to-End Network System Evaluation with Modular Simulation</title>
    <summary>  Full system "end-to-end" measurements in physical testbeds are the gold
standard for network systems evaluation but are often not feasible. When
physical testbeds are not available we frequently turn to simulation for
evaluation. Unfortunately, existing simulators are insufficient for end-to-end
evaluation, as they either cannot simulate all components, or simulate them
with inadequate detail. We address this through modular simulation, flexibly
combining and connecting multiple existing simulators for different components,
including processor and memory, devices, and network, into virtual end-to-end
testbeds tuned for each use-case. Our architecture, SimBricks, combines
well-defined component interfaces for extensibility and modularity, efficient
communication channels for local and distributed simulation, and a co-designed
efficient synchronization mechanism for accurate timing across simulators. We
demonstrate SimBricks scales to 1000 simulated hosts, each running a full
software stack including Linux, and that it can simulate testbeds with existing
NIC and switch RTL implementations. We also reproduce key findings from prior
work in congestion control, NIC architecture, and in-network computing in
SimBricks.
</summary>
    <author>
      <name>Hejing Li</name>
    </author>
    <author>
      <name>Jialin Li</name>
    </author>
    <author>
      <name>Antoine Kaufmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3544216.3544253</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3544216.3544253" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 13 figures, appeared in In Proceedings of ACM SIGCOMM 2022
  Conference (SIGCOMM '22), August 22-26, 2022, Amsterdam, Netherlands</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.14219v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.14219v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.14635v2</id>
    <updated>2021-06-20T00:16:47Z</updated>
    <published>2020-12-29T07:17:38Z</published>
    <title>Sensifi: A Wireless Sensing System for Ultra-High-Rate Applications</title>
    <summary>  Wireless Sensor Networks (WSNs) are being used in various applications such
as structural health monitoring and industrial control. Since energy efficiency
is one of the major design factors, the existing WSNs primarily rely on
low-power, low-rate wireless technologies such as 802.15.4 and Bluetooth. In
this paper, we strive to tackle the challenges of developing ultra-high-rate
WSNs based on 802.11 (WiFi) standard by proposing Sensifi. As an illustrative
application of this system, we consider vibration test monitoring of spacecraft
and identify system design requirements and challenges. Our main contributions
are as follows. First, we propose packet encoding methods to reduce the
overhead of assigning accurate timestamps to samples. Second, we propose energy
efficiency methods to enhance the system's lifetime. Third, we reduce the
overhead of processing outgoing packets through network stack to enhance
sampling rate and mitigate sampling rate instability. Fourth, we study and
reduce the delay of processing incoming packets through network stack to
enhance the accuracy of time synchronization among nodes. Fifth, we propose a
low-power node design for ultra-high-rate applications. Sixth, we use our node
design to empirically evaluate the system.
</summary>
    <author>
      <name>Chia-Chi Li</name>
    </author>
    <author>
      <name>Vikram K. Ramanna</name>
    </author>
    <author>
      <name>Daniel Webber</name>
    </author>
    <author>
      <name>Cole Hunter</name>
    </author>
    <author>
      <name>Tyler Hack</name>
    </author>
    <author>
      <name>Behnam Dezfouli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JIOT.2021.3089159</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JIOT.2021.3089159" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Internet of Things Journal 9, no. 3 (2021): 2025-2043</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.14635v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.14635v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.07200v1</id>
    <updated>2021-01-15T01:30:48Z</updated>
    <published>2021-01-15T01:30:48Z</published>
    <title>Tuning the Frequency of Periodic Data Movements over Hybrid Memory
  Systems</title>
    <summary>  Emerging hybrid memory systems that comprise technologies such as Intel's
Optane DC Persistent Memory, exhibit disparities in the access speeds and
capacity ratios of their heterogeneous memory components. This breaks many
assumptions and heuristics designed for traditional DRAM-only platforms. High
application performance is feasible via dynamic data movement across memory
units, which maximizes the capacity use of DRAM while ensuring efficient use of
the aggregate system resources. Newly proposed solutions use performance models
and machine intelligence to optimize which and how much data to move
dynamically; however, the decision of when to move this data is based on
empirical selection of time intervals, or left to the applications. Our
experimental evaluation shows that failure to properly configure the data
movement frequency can lead to 10%-100% slowdown for a given data movement
policy; yet, there is no established methodology on how to properly configure
this value for a given workload, platform and policy. We propose Cori, a
system-level tuning solution that identifies and extracts the necessary
application-level data reuse information, and guides the selection of data
movement frequency to deliver gains in application performance and system
resource efficiency. Experimental evaluation shows that Cori configures data
movement frequencies that provide application performance within 3% of the
optimal one, and that it can achieve this up to 5x more quickly than random or
brute-force approaches. System-level validation of Cori on a platform with DRAM
and Intel's Optane DC PMEM confirms its practicality and tuning efficiency.
</summary>
    <author>
      <name>Thaleia Dimitra Doudali</name>
    </author>
    <author>
      <name>Daniel Zahka</name>
    </author>
    <author>
      <name>Ada Gavrilovska</name>
    </author>
    <link href="http://arxiv.org/abs/2101.07200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.07200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.07327v1</id>
    <updated>2021-01-18T21:02:16Z</updated>
    <published>2021-01-18T21:02:16Z</published>
    <title>OpenUVR: an Open-Source System Framework for Untethered Virtual Reality
  Applications</title>
    <summary>  Advancements in heterogeneous computing technologies enable the significant
potential of virtual reality (VR) applications. To offer the best user
experience (UX), a system should adopt an untethered, wireless-network-based
architecture to transfer VR content between the user and the content generator.
However, modern wireless network technologies make implementing such an
architecture challenging, as VR applications require superior video quality --
with high resolution, high frame rates, and very low latency.
  This paper presents OpenUVR, an open-source framework that uses commodity
hardware components to satisfy the demands of interactive, real-time VR
applications. OpenUVR significantly improves UX through a redesign of the
system stack and addresses the most time-sensitive issues associated with
redundant memory copying in modern computing systems. OpenUVR presents a
cross-layered VR datapath to avoid redundant data operations and computation
among system components, OpenUVR customizes the network stack to eliminate
unnecessary memory operations incurred by mismatching data formats in each
layer, and OpenUVR uses feedback from mobile devices to remove memory buffers.
  Together, these modifications allow OpenUVR to reduce VR application delays
to 14.32 ms, meeting the 20 ms minimum latency in avoiding motion sickness. As
an open-source system that is fully compatible with commodity hardware, OpenUVR
offers the research community an opportunity to develop, investigate, and
optimize applications for untethered, high-performance VR architectures.
</summary>
    <author>
      <name>Alec Rohloff</name>
    </author>
    <author>
      <name>Zackary Allen</name>
    </author>
    <author>
      <name>Kung-Min Lin</name>
    </author>
    <author>
      <name>Joshua Okrend</name>
    </author>
    <author>
      <name>Chengyi Nie</name>
    </author>
    <author>
      <name>Yu-Chia Liu</name>
    </author>
    <author>
      <name>Hung-Wei Tseng</name>
    </author>
    <link href="http://arxiv.org/abs/2101.07327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.07327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.08877v1</id>
    <updated>2021-01-21T22:54:21Z</updated>
    <published>2021-01-21T22:54:21Z</published>
    <title>Virtual Memory Partitioning for Enhancing Application Performance in
  Mobile Platforms</title>
    <summary>  Recently, the amount of running software on smart mobile devices is gradually
increasing due to the introduction of application stores. The application store
is a type of digital distribution platform for application software, which is
provided as a component of an operating system on a smartphone or tablet.
Mobile devices have limited memory capacity and, unlike server and desktop
systems, due to their mobility they do not have a memory slot that can expand
the memory capacity. Low memory killer (LMK) and out-of-memory killer (OOMK)
are widely used memory management solutions in mobile systems. They forcibly
terminate applications when the available physical memory becomes insufficient.
In addition, before the forced termination, the memory shortage incurs
thrashing and fragmentation, thus slowing down application performance.
Although the existing page reclamation mechanism is designed to secure
available memory, it could seriously degrade user responsiveness due to the
thrashing. Memory management is therefore still important especially in mobile
devices with small memory capacity. This paper presents a new memory
partitioning technique that resolves the deterioration of the existing
application life cycle induced by LMK and OOMK. It provides a completely
isolated virtual memory node at the operating system level. Evaluation results
demonstrate that the proposed method improves application execution time under
memory shortage, compared with methods in previous studies.
</summary>
    <author>
      <name>Geunsik Lim</name>
    </author>
    <author>
      <name>Changwoo Min</name>
    </author>
    <author>
      <name>Young Ik Eom</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCE.2013.6689690</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCE.2013.6689690" rel="related"/>
    <link href="http://arxiv.org/abs/2101.08877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.08877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.09360v1</id>
    <updated>2021-01-21T08:32:13Z</updated>
    <published>2021-01-21T08:32:13Z</published>
    <title>BB: Booting Booster for Consumer Electronics with Modern OS</title>
    <summary>  Unconventional computing platforms have spread widely and rapidly following
smart phones and tablets: consumer electronics such as smart TVs and digital
cameras. For such devices, fast booting is a critical requirement; waiting tens
of seconds for a TV or a camera to boot up is not acceptable, unlike a PC or
smart phone. Moreover, the software platforms of these devices have become as
rich as conventional computing devices to provide comparable services. As a
result, the booting procedure to start every required OS service, hardware
component, and application, the quantity of which is ever increasing, may take
unbearable time for most consumers. To accelerate booting, this paper
introduces \textit{Booting Booster} (BB), which is used in all 2015 Samsung
Smart TV models, and which runs the Linux-based Tizen OS. BB addresses the init
scheme of Linux, which launches initial user-space OS services and applications
and manages the life cycles of all user processes, by identifying and isolating
booting-critical tasks, deferring non-critical tasks, and enabling execution of
more tasks in parallel. BB has been successfully deployed in Samsung Smart TV
2015 models achieving a cold boot in 3.5 s (compared to 8.1 s with full
commercial-grade optimizations without BB) without the need for suspend-to-RAM
or hibernation. After this successful deployment, we have released the source
code via http://opensource.samsung.com, and BB will be included in the
open-source OS, Tizen (http://tizen.org).
</summary>
    <author>
      <name>Geunsik Lim</name>
    </author>
    <author>
      <name>MyungJoo Ham</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2901318.2901320</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2901318.2901320" rel="related"/>
    <link href="http://arxiv.org/abs/2101.09360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.09360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.09584v1</id>
    <updated>2021-01-23T21:08:25Z</updated>
    <published>2021-01-23T21:08:25Z</published>
    <title>HyCoR: Fault-Tolerant Replicated Containers Based on Checkpoint and
  Replay</title>
    <summary>  HyCoR is a fully-operational fault tolerance mechanism for multiprocessor
workloads, based on container replication, using a hybrid of checkpointing and
replay. HyCoR derives from two insights regarding replication mechanisms: 1)
deterministic replay can overcome a key disadvantage of checkpointing alone --
unacceptably long delays of outputs to clients, and 2) checkpointing can
overcome a key disadvantage of active replication with deterministic replay
alone -- vulnerability to even rare replay failures due to an untracked
nondeterministic events. With HyCoR, the primary sends periodic checkpoints to
the backup and logs the outcomes of sources of nondeterminism. Outputs to
clients are delayed only by the short time it takes to send the corresponding
log to the backup. Upon primary failure, the backup replays only the short
interval since the last checkpoint, thus minimizing the window of
vulnerability. HyCoR includes a "best effort" mechanism that results in a high
recovery rate even in the presence of data races, as long as their rate is low.
The evaluation includes measurement of the recovery rate and recovery latency
based on fault injection. On average, HyCoR delays responses to clients by less
than 1ms and recovers in less than 1s. For a set of eight real-world
benchmarks, if data races are eliminated, the performance overhead of HyCoR is
under 59%.
</summary>
    <author>
      <name>Diyu Zhou</name>
    </author>
    <author>
      <name>Yuval Tamir</name>
    </author>
    <link href="http://arxiv.org/abs/2101.09584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.09584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.10779v1</id>
    <updated>2021-03-16T08:46:59Z</updated>
    <published>2021-03-16T08:46:59Z</published>
    <title>Page Table Management for Heterogeneous Memory Systems</title>
    <summary>  Modern enterprise servers are increasingly embracing tiered memory systems
with a combination of low latency DRAMs and large capacity but high latency
non-volatile main memories (NVMMs) such as Intel's Optane DC PMM. Prior works
have focused on efficient placement and migration of data on a tiered memory
system, but have not studied the optimal placement of page tables.
  Explicit and efficient placement of page tables is crucial for large memory
footprint applications with high TLB miss rates because they incur dramatically
higher page walk latency when page table pages are placed in NVMM. We show that
(i) page table pages can end up on NVMM even when enough DRAM memory is
available and (ii) page table pages that spill over to NVMM due to DRAM memory
pressure are not migrated back later when memory is available in DRAM.
  We study the performance impact of page table placement in a tiered memory
system and propose an efficient and transparent page table management technique
that (i) applies different placement policies for data and page table pages,
(ii) introduces a differentiating policy for page table pages by placing a
small but critical part of the page table in DRAM, and (iii) dynamically and
judiciously manages the rest of the page table by transparently migrating the
page table pages between DRAM and NVMM. Our implementation on a real system
equipped with Intel's Optane NVMM running Linux reduces the page table walk
cycles by 12% and total cycles by 20% on an average. This improves the runtime
by 20% on an average for a set of synthetic and real-world large memory
footprint applications when compared with various default Linux kernel
techniques.
</summary>
    <author>
      <name>Sandeep Kumar</name>
    </author>
    <author>
      <name>Aravinda Prasad</name>
    </author>
    <author>
      <name>Smruti R. Sarangi</name>
    </author>
    <author>
      <name>Sreenivas Subramoney</name>
    </author>
    <link href="http://arxiv.org/abs/2103.10779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.10779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05456v1</id>
    <updated>2021-04-12T13:23:58Z</updated>
    <published>2021-04-12T13:23:58Z</published>
    <title>TermAdventure: Interactively Teaching UNIX Command Line, Text Adventure
  Style</title>
    <summary>  Introductory UNIX courses are typically organized as lectures, accompanied by
a set of exercises, whose solutions are submitted to and reviewed by the
lecturers. While this arrangement has become standard practice, it often
requires the use of an external tool or interface for submission and does not
automatically check its correctness. That in turn leads to increased workload
and makes it difficult to deal with potential plagiarism.
  In this work we present TermAdventure (TA), a suite of tools for creating
interactive UNIX exercises. These resemble text adventure games, which immerse
the user in a text environment and let them interact with it using textual
commands. In our case the ''adventure'' takes place inside a UNIX system and
the user interaction happens via the standard UNIX command line. The adventure
is a set of exercises, which are presented and automatically evaluated by the
system, all from within the command line environment. The suite is released
under an open source license, has minimal dependencies and can be used either
on a UNIX-style server or a desktop computer running any major OS platform
through Docker.
  We also reflect on our experience of using the presented suite as the primary
teaching tool for an introductory UNIX course for Data Scientists and discuss
the implications of its deployment in similar courses. The suite is released
under the terms of an open-source license at
\url{https://github.com/NaiveNeuron/TermAdventure}.
</summary>
    <author>
      <name>Marek Šuppa</name>
    </author>
    <author>
      <name>Ondrej Jariabka</name>
    </author>
    <author>
      <name>Adrián Matejov</name>
    </author>
    <author>
      <name>Marek Nagy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3430665.3456387</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3430665.3456387" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ITiCSE 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.05456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.13671v1</id>
    <updated>2021-04-28T09:50:35Z</updated>
    <published>2021-04-28T09:50:35Z</published>
    <title>Continual Learning Approach for Improving the Data and Computation
  Mapping in Near-Memory Processing System</title>
    <summary>  The resurgence of near-memory processing (NMP) with the advent of big data
has shifted the computation paradigm from processor-centric to memory-centric
computing. To meet the bandwidth and capacity demands of memory-centric
computing, 3D memory has been adopted to form a scalable memory-cube network.
Along with NMP and memory system development, the mapping for placing data and
guiding computation in the memory-cube network has become crucial in driving
the performance improvement in NMP. However, it is very challenging to design a
universal optimal mapping for all applications due to unique application
behavior and intractable decision space. In this paper, we propose an
artificially intelligent memory mapping scheme, AIMM, that optimizes data
placement and resource utilization through page and computation remapping. Our
proposed technique involves continuously evaluating and learning the impact of
mapping decisions on system performance for any application. AIMM uses a neural
network to achieve a near-optimal mapping during execution, trained using a
reinforcement learning algorithm that is known to be effective for exploring a
vast design space. We also provide a detailed AIMM hardware design that can be
adopted as a plugin module for various NMP systems. Our experimental evaluation
shows that AIMM improves the baseline NMP performance in single and multiple
program scenario by up to 70% and 50%, respectively.
</summary>
    <author>
      <name>Pritam Majumder</name>
    </author>
    <author>
      <name>Jiayi Huang</name>
    </author>
    <author>
      <name>Sungkeun Kim</name>
    </author>
    <author>
      <name>Abdullah Muzahid</name>
    </author>
    <author>
      <name>Dylan Siegers</name>
    </author>
    <author>
      <name>Chia-Che Tsai</name>
    </author>
    <author>
      <name>Eun Jung Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2104.13671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.13671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.05590v2</id>
    <updated>2021-05-18T10:40:07Z</updated>
    <published>2021-05-12T11:09:47Z</published>
    <title>Budget-based real-time Executor for Micro-ROS</title>
    <summary>  The Robot Operating System (ROS) is a popular robotics middleware framework.
In the last years, it underwent a redesign and reimplementation under the name
ROS~2. It now features QoS-configurable communication and a flexible layered
architecture. Micro-ROS is a variant developed specifically for
resource-constrained microcontrollers (MCU). Such MCUs are commonly used in
robotics for sensors and actuators, for time-critical control functions, and
for safety. While the execution management of ROS 2 has been addressed by an
Executor concept, its lack of real-time capabilities make it unsuitable for
industrial use. Neither defining an execution order nor the assignment of
scheduling parameters to tasks is possible, despite the fact that advanced
real-time scheduling algorithms are well-known and available in modern RTOS's.
For example, the NuttX RTOS supports a variant of the reservation-based
scheduling which is very attractive for industrial applications: It allows to
assign execution time budgets to software components so that a system
integrator can thereby guarantee the real-time requirements of the entire
system. This paper presents for the first time a ROS~2 Executor design which
enables the real-time scheduling capabilities of the operating system. In
particular, we successfully demonstrate the budget-based scheduling of the
NuttX RTOS with a micro-ROS application on an STM32 microcontroller.
</summary>
    <author>
      <name>Jan Staschulat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Robert Bosch GmbH, Stuttgart, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Ralph Lange</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Robert Bosch GmbH, Stuttgart, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Dakshina Narahari Dasari</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Robert Bosch GmbH, Stuttgart, Germany</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures, submitted to RTAS conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.05590v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.05590v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.12553v2</id>
    <updated>2021-11-03T16:56:49Z</updated>
    <published>2021-06-10T10:54:29Z</published>
    <title>Femto-Containers: DevOps on Microcontrollers with Lightweight
  Virtualization &amp; Isolation for IoT Software Modules</title>
    <summary>  Development, deployment and maintenance of networked software has been
revolutionized by DevOps, which have become essential to boost system software
quality and to enable agile evolution. Meanwhile the Internet of Things (IoT)
connects more and more devices which are not covered by DevOps tools:
low-power, microcontroller-based devices. In this paper, we contribute to
bridge this gap by designing Femto-Containers, a new architecture which enables
containerization, virtualization and secure deployment of software modules
embedded on microcontrollers over low-power networks. As proof-of-concept, we
implemented and evaluated Femto-Containers on popular microcontroller
architectures (Arm Cortex-M, ESP32 and RISC-V), using eBPF virtualization, and
RIOT, a common operating system in this space. We show that Femto-Containers
can virtualize and isolate multiple software modules, executed concurrently,
with very small memory footprint overhead (below 10%) and very small startup
time (tens of microseconds) compared to native code execution. We show that
Femto-Containers can satisfy the constraints of both low-level debug logic
inserted in a hot code path, and high-level business logic coded in a variety
of common programming languages. Compared to prior work, Femto-Containers thus
offer an attractive trade-off in terms of memory footprint, energy consumption,
agility and security.
</summary>
    <author>
      <name>Koen Zandberg</name>
    </author>
    <author>
      <name>Emmanuel Baccelli</name>
    </author>
    <link href="http://arxiv.org/abs/2106.12553v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.12553v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.00444v1</id>
    <updated>2021-08-01T12:36:13Z</updated>
    <published>2021-08-01T12:36:13Z</published>
    <title>An efficient reverse-lookup table based strategy for solving the synonym
  and cache coherence problem in virtually indexed, virtually tagged caches</title>
    <summary>  Virtually indexed and virtually tagged (VIVT) caches are an attractive option
for micro-processor level-1 caches, because of their fast response time and
because they are cheaper to implement than more complex caches such as
virtually-indexed physical-tagged (VIPT) caches. The level-1 VIVT cache becomes
even simpler to construct if it is implemented as a direct-mapped cache
(VIVT-DM cache). However, VIVT and VIVT-DM caches have some drawbacks. When the
number of sets in the cache is larger than the smallest page size, there is a
possibility of synonyms (two or more virtual addresses mapped to the same
physical address) existing in the cache. Further, maintenance of cache
coherence across multiple processors requires a physical to virtual translation
mechanism in the hardware. We describe a simple, efficient reverse lookup table
based approach to address the synonym and the coherence problems in VIVT (both
set associative and direct-mapped) caches. In particular, the proposed scheme
does not disturb the critical memory access paths in a typical micro-processor,
and requires a low overhead for its implementation. We have implemented and
validated the scheme in the AJIT 32-bit microprocessor core (an implementation
of the SPARC-V8 ISA) and the implementation uses approximately 2% of the gates
and 5.3% of the memory bits in the processor core.
</summary>
    <author>
      <name>Madhav P. Desai</name>
    </author>
    <author>
      <name>Aniket Deshmukh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.00444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.00444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.06253v2</id>
    <updated>2022-10-04T12:18:24Z</updated>
    <published>2021-10-12T18:08:38Z</published>
    <title>StateAFL: Greybox Fuzzing for Stateful Network Servers</title>
    <summary>  Fuzzing network servers is a technical challenge, since the behavior of the
target server depends on its state over a sequence of multiple messages.
Existing solutions are costly and difficult to use, as they rely on
manually-customized artifacts such as protocol models, protocol parsers, and
learning frameworks. The aim of this work is to develop a greybox fuzzer
(StateaAFL) for network servers that only relies on lightweight analysis of the
target program, with no manual customization, in a similar way to what the AFL
fuzzer achieved for stateless programs. The proposed fuzzer instruments the
target server at compile-time, to insert probes on memory allocations and
network I/O operations. At run-time, it infers the current protocol state of
the target server by taking snapshots of long-lived memory areas, and by
applying a fuzzy hashing algorithm (Locality-Sensitive Hashing) to map memory
contents to a unique state identifier. The fuzzer incrementally builds a
protocol state machine for guiding fuzzing.
  We implemented and released StateaAFL as open-source software. As a basis for
reproducible experimentation, we integrated StateaAFL with a large set of
network servers for popular protocols, with no manual customization to
accomodate for the protocol. The experimental results show that the fuzzer can
be applied with no manual customization on a large set of network servers for
popular protocols, and that it can achieve comparable, or even better code
coverage and bug detection than customized fuzzing. Moreover, our qualitative
analysis shows that states inferred from memory better reflect the server
behavior than only using response codes from messages.
</summary>
    <author>
      <name>Roberto Natella</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10664-022-10233-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10664-022-10233-3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The tool is available at https://github.com/stateafl/stateafl</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Empir Software Eng 27, 191 (2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.06253v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.06253v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.10919v2</id>
    <updated>2022-03-13T19:04:02Z</updated>
    <published>2021-10-21T06:19:31Z</published>
    <title>FlexTOE: Flexible TCP Offload with Fine-Grained Parallelism</title>
    <summary>  FlexTOE is a flexible, yet high-performance TCP offload engine (TOE) to
SmartNICs. FlexTOE eliminates almost all host data-path TCP processing and is
fully customizable. FlexTOE interoperates well with other TCP stacks, is robust
under adverse network conditions, and supports POSIX sockets.
  FlexTOE focuses on data-path offload of established connections, avoiding
complex control logic and packet buffering in the NIC. FlexTOE leverages
fine-grained parallelization of the TCP data-path and segment reordering for
high performance on wimpy SmartNIC architectures, while remaining flexible via
a modular design. We compare FlexTOE on an Agilio-CX40 to host TCP stacks Linux
and TAS, and to the Chelsio Terminator TOE. We find that Memcached scales up to
38% better on FlexTOE versus TAS, while saving up to 81% host CPU cycles versus
Chelsio. FlexTOE provides competitive performance for RPCs, even with wimpy
SmartNICs. FlexTOE cuts 99.99th-percentile RPC RTT by 3.2$\times$ and 50%
versus Chelsio and TAS, respectively. FlexTOE's data-path parallelism
generalizes across hardware architectures, improving single connection RPC
throughput up to 2.4$\times$ on x86 and 4$\times$ on BlueField. FlexTOE
supports C and XDP programs written in eBPF. It allows us to implement popular
data center transport features, such as TCP tracing, packet filtering and
capture, VLAN stripping, flow classification, firewalling, and connection
splicing.
</summary>
    <author>
      <name>Rajath Shashidhara</name>
    </author>
    <author>
      <name>Timothy Stamler</name>
    </author>
    <author>
      <name>Antoine Kaufmann</name>
    </author>
    <author>
      <name>Simon Peter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in 19th USENIX Symposium on Networked Systems Design and
  Implementation (NSDI 22). See
  https://www.usenix.org/conference/nsdi22/presentation/shashidhara</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.10919v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.10919v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.00642v1</id>
    <updated>2022-03-01T17:34:36Z</updated>
    <published>2022-03-01T17:34:36Z</published>
    <title>Relaxed virtual memory in Armv8-A (extended version)</title>
    <summary>  Virtual memory is an essential mechanism for enforcing security boundaries,
but its relaxed-memory concurrency semantics has not previously been
investigated in detail. The concurrent systems code managing virtual memory has
been left on an entirely informal basis, and OS and hypervisor verification has
had to make major simplifying assumptions.
  We explore the design space for relaxed virtual memory semantics in the
Armv8-A architecture, to support future system-software verification. We
identify many design questions, in discussion with Arm; develop a test suite,
including use cases from the pKVM production hypervisor under development by
Google; delimit the design space with axiomatic-style concurrency models; prove
that under simple stable configurations our architectural model collapses to
previous "user" models; develop tooling to compute allowed behaviours in the
model integrated with the full Armv8-A ISA semantics; and develop a hardware
test harness.
  This lays out some of the main issues in relaxed virtual memory bringing
these security-critical systems phenomena into the domain of
programming-language semantics and verification with foundational architecture
semantics.
  This document is an extended version of a paper in ESOP 2022, with additional
explanation and examples in the main body, and appendices detailing our litmus
tests, models, proofs, and test results.
</summary>
    <author>
      <name>Ben Simner</name>
    </author>
    <author>
      <name>Alasdair Armstrong</name>
    </author>
    <author>
      <name>Jean Pichon-Pharabod</name>
    </author>
    <author>
      <name>Christopher Pulte</name>
    </author>
    <author>
      <name>Richard Grisenthwaite</name>
    </author>
    <author>
      <name>Peter Sewell</name>
    </author>
    <link href="http://arxiv.org/abs/2203.00642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.00642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.2; D.3.1; F.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.08284v2</id>
    <updated>2022-10-21T03:50:58Z</updated>
    <published>2022-03-15T22:03:23Z</published>
    <title>Minimizing Trust with Exclusively-Used Physically-Isolated Hardware</title>
    <summary>  Smartphone owners often need to run security-critical programs on the same
device as other untrusted and potentially malicious programs. This requires
users to trust hardware and system software to correctly sandbox malicious
programs, trust that is often misplaced.
  Our goal is to minimize the number and complexity of hardware and software
components that a smartphone owner needs to trust to withstand adversarial
inputs. We present a multi-domain hardware design composed of
statically-partitioned, physically-isolated trust domains. We introduce a few
simple, formally-verified hardware components to enable a program to gain
provably exclusive and simultaneous access to both computation and I/O on a
temporary basis. To manage this hardware, we present OctopOS, an OS composed of
mutually distrustful subsystems.
  We present a prototype of this machine (hardware and OS) on a CPU-FPGA board
and show that it incurs a small hardware cost compared to modern SoCs. For
security-critical programs, we show that this machine significantly reduces the
required trust compared to mainstream TEEs while achieving decent performance.
For normal programs, performance is similar to a legacy machine.
</summary>
    <author>
      <name>Zhihao Yao</name>
    </author>
    <author>
      <name>Seyed Mohammadjavad Seyed Talebi</name>
    </author>
    <author>
      <name>Mingyi Chen</name>
    </author>
    <author>
      <name>Ardalan Amiri Sani</name>
    </author>
    <author>
      <name>Thomas Anderson</name>
    </author>
    <link href="http://arxiv.org/abs/2203.08284v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.08284v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.07167v2</id>
    <updated>2022-09-22T06:33:47Z</updated>
    <published>2022-04-15T14:42:24Z</published>
    <title>Towards Porting Operating Systems with Program Synthesis</title>
    <summary>  The end of Moore's Law has ushered in a diversity of hardware not seen in
decades. Operating system (and system software) portability is accordingly
becoming increasingly critical. Simultaneously, there has been tremendous
progress in program synthesis. We set out to explore the feasibility of using
modern program synthesis to generate the machine-dependent parts of an
operating system. Our ultimate goal is to generate new ports automatically from
descriptions of new machines. One of the issues involved is writing
specifications, both for machine-dependent operating system functionality and
for instruction set architectures. We designed two domain-specific languages:
Alewife for machine-independent specifications of machine-dependent operating
system functionality and Cassiopea for describing instruction set architecture
semantics. Automated porting also requires an implementation. We developed a
toolchain that, given an Alewife specification and a Cassiopea machine
description, specializes the machine-independent specification to the target
instruction set architecture and synthesizes an implementation in assembly
language with a customized symbolic execution engine. Using this approach, we
demonstrate successful synthesis of a total of 140 OS components from two
pre-existing OSes for four real hardware platforms. We also developed several
optimization methods for OS-related assembly synthesis to improve scalability.
The effectiveness of our languages and ability to synthesize code for all 140
specifications is evidence of the feasibility of program synthesis for
machine-dependent OS code. However, many research challenges remain; we also
discuss the benefits and limitations of our synthesis-based approach to
automated OS porting.
</summary>
    <author>
      <name>Jingmei Hu</name>
    </author>
    <author>
      <name>Eric Lu</name>
    </author>
    <author>
      <name>David A. Holland</name>
    </author>
    <author>
      <name>Ming Kawaguchi</name>
    </author>
    <author>
      <name>Stephen Chong</name>
    </author>
    <author>
      <name>Margo I. Seltzer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3563943</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3563943" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Programming Languages and Systems. Accepted on
  August 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.07167v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.07167v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.3232v1</id>
    <updated>2011-05-16T21:45:54Z</updated>
    <published>2011-05-16T21:45:54Z</published>
    <title>Unleashing the Power of Mobile Cloud Computing using ThinkAir</title>
    <summary>  Smartphones have exploded in popularity in recent years, becoming ever more
sophisticated and capable. As a result, developers worldwide are building
increasingly complex applications that require ever increasing amounts of
computational power and energy. In this paper we propose ThinkAir, a framework
that makes it simple for developers to migrate their smartphone applications to
the cloud. ThinkAir exploits the concept of smartphone virtualization in the
cloud and provides method level computation offloading. Advancing on previous
works, it focuses on the elasticity and scalability of the server side and
enhances the power of mobile cloud computing by parallelizing method execution
using multiple Virtual Machine (VM) images. We evaluate the system using a
range of benchmarks starting from simple micro-benchmarks to more complex
applications. First, we show that the execution time and energy consumption
decrease two orders of magnitude for the N-queens puzzle and one order of
magnitude for a face detection and a virus scan application, using cloud
offloading. We then show that if a task is parallelizable, the user can request
more than one VM to execute it, and these VMs will be provided dynamically. In
fact, by exploiting parallelization, we achieve a greater reduction on the
execution time and energy consumption for the previous applications. Finally,
we use a memory-hungry image combiner tool to demonstrate that applications can
dynamically request VMs with more computational power in order to meet their
computational requirements.
</summary>
    <author>
      <name>Sokol Kosta</name>
    </author>
    <author>
      <name>Andrius Aucinas</name>
    </author>
    <author>
      <name>Pan Hui</name>
    </author>
    <author>
      <name>Richard Mortier</name>
    </author>
    <author>
      <name>Xinwen Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.3232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.3232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.0846v1</id>
    <updated>2013-06-04T16:27:30Z</updated>
    <published>2013-06-04T16:27:30Z</published>
    <title>V-BOINC: The Virtualization of BOINC</title>
    <summary>  The Berkeley Open Infrastructure for Network Computing (BOINC) is an open
source client-server middleware system created to allow projects with large
computational requirements, usually set in the scientific domain, to utilize a
technically unlimited number of volunteer machines distributed over large
physical distances. However various problems exist deploying applications over
these heterogeneous machines using BOINC: applications must be ported to each
machine architecture type, the project server must be trusted to supply
authentic applications, applications that do not regularly checkpoint may lose
execution progress upon volunteer machine termination and applications that
have dependencies may find it difficult to run under BOINC.
  To solve such problems we introduce virtual BOINC, or V-BOINC, where virtual
machines are used to run computations on volunteer machines. Application
developers can then compile their applications on a single architecture,
checkpointing issues are solved through virtualization API's and many security
concerns are addressed via the virtual machine's sandbox environment. In this
paper we focus on outlining a unique approach on how virtualization can be
introduced into BOINC and demonstrate that V-BOINC offers acceptable
computational performance when compared to regular BOINC. Finally we show that
applications with dependencies can easily run under V-BOINC in turn increasing
the computational potential volunteer computing offers to the general public
and project developers.
</summary>
    <author>
      <name>Gary A. McGilvary</name>
    </author>
    <author>
      <name>Adam Barker</name>
    </author>
    <author>
      <name>Ashley Lloyd</name>
    </author>
    <author>
      <name>Malcolm Atkinson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CCGrid.2013.14</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CCGrid.2013.14" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, Proceedings of the 13th IEEE/ACM International Symposium on
  Cluster, Cloud and Grid Computing (CCGrid 2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.0846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.0846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04030v1</id>
    <updated>2017-11-02T15:45:05Z</updated>
    <published>2017-11-02T15:45:05Z</published>
    <title>Ocasta: Clustering Configuration Settings For Error Recovery</title>
    <summary>  Effective machine-aided diagnosis and repair of configuration errors
continues to elude computer systems designers. Most of the literature targets
errors that can be attributed to a single erroneous configuration setting.
However, a recent study found that a significant amount of configuration errors
require fixing more than one setting together. To address this limitation,
Ocasta statistically clusters dependent configuration settings based on the
application's accesses to its configuration settings and utilizes the extracted
clustering of configuration settings to fix configuration errors involving more
than one configuration settings. Ocasta treats applications as black-boxes and
only relies on the ability to observe application accesses to their
configuration settings.
  We collected traces of real application usage from 24 Linux and 5 Windows
desktops computers and found that Ocasta is able to correctly identify clusters
with 88.6% accuracy. To demonstrate the effectiveness of Ocasta, we evaluated
it on 16 real-world configuration errors of 11 Linux and Windows applications.
Ocasta is able to successfully repair all evaluated configuration errors in 11
minutes on average and only requires the user to examine an average of 3
screenshots of the output of the application to confirm that the error is
repaired. A user study we conducted shows that Ocasta is easy to use by both
expert and non-expert users and is more efficient than manual configuration
error troubleshooting.
</summary>
    <author>
      <name>Zhen Huang</name>
    </author>
    <author>
      <name>David Lie</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DSN.2014.51</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DSN.2014.51" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Proceedings of the 44th Annual IEEE/IFIP International
  Conference on Dependable Systems and Networks (DSN 2014)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">44th Annual IEEE/IFIP International Conference on Dependable
  Systems and Networks, 2014, pages={479-490}</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.04030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.1; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.07701v1</id>
    <updated>2018-09-17T12:17:01Z</updated>
    <published>2018-09-17T12:17:01Z</published>
    <title>On the Fly Orchestration of Unikernels: Tuning and Performance
  Evaluation of Virtual Infrastructure Managers</title>
    <summary>  Network operators are facing significant challenges meeting the demand for
more bandwidth, agile infrastructures, innovative services, while keeping costs
low. Network Functions Virtualization (NFV) and Cloud Computing are emerging as
key trends of 5G network architectures, providing flexibility, fast
instantiation times, support of Commercial Off The Shelf hardware and
significant cost savings. NFV leverages Cloud Computing principles to move the
data-plane network functions from expensive, closed and proprietary hardware to
the so-called Virtual Network Functions (VNFs). In this paper we deal with the
management of virtual computing resources (Unikernels) for the execution of
VNFs. This functionality is performed by the Virtual Infrastructure Manager
(VIM) in the NFV MANagement and Orchestration (MANO) reference architecture. We
discuss the instantiation process of virtual resources and propose a generic
reference model, starting from the analysis of three open source VIMs, namely
OpenStack, Nomad and OpenVIM. We improve the aforementioned VIMs introducing
the support for special-purpose Unikernels and aiming at reducing the duration
of the instantiation process. We evaluate some performance aspects of the VIMs,
considering both stock and tuned versions. The VIM extensions and performance
evaluation tools are available under a liberal open source licence.
</summary>
    <author>
      <name>Pier Luigi Ventre</name>
    </author>
    <author>
      <name>Paolo Lungaroni</name>
    </author>
    <author>
      <name>Giuseppe Siracusano</name>
    </author>
    <author>
      <name>Claudio Pisa</name>
    </author>
    <author>
      <name>Florian Schmidt</name>
    </author>
    <author>
      <name>Francesco Lombardo</name>
    </author>
    <author>
      <name>Stefano Salsano</name>
    </author>
    <link href="http://arxiv.org/abs/1809.07701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.07701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.00805v1</id>
    <updated>2019-09-02T17:28:05Z</updated>
    <published>2019-09-02T17:28:05Z</published>
    <title>CrowdOS: A Ubiquitous Operating System for Crowdsourcing and Mobile
  Crowd Sensing</title>
    <summary>  With the rise of crowdsourcing and mobile crowdsensing techniques, a large
number of crowdsourcing applications or platforms (CAP) have appeared. In the
mean time, CAP-related models and frameworks based on different research
hypotheses are rapidly emerging, and they usually address specific issues from
a certain perspective. Due to different settings and conditions, different
models are not compatible with each other. However, CAP urgently needs to
combine these techniques to form a unified framework. In addition, these models
needs to be learned and updated online with the extension of crowdsourced data
and task types, thus requiring a unified architecture that integrates lifelong
learning concepts and breaks down the barriers between different modules. This
paper draws on the idea of ubiquitous operating systems and proposes a novel OS
(CrowdOS), which is an abstract software layer running between native OS and
application layer. In particular, based on an in-depth analysis of the complex
crowd environment and diverse characteristics of heterogeneous tasks, we
construct the OS kernel and three core frameworks including Task Resolution and
Assignment Framework (TRAF), Integrated Resource Management (IRM), and Task
Result quality Optimization (TRO). In addition, we validate the usability of
CrowdOS, module correctness and development efficiency. Our evaluation further
reveals TRO brings enormous improvement in efficiency and a reduction in energy
consumption.
</summary>
    <author>
      <name>Yimeng Liu</name>
    </author>
    <author>
      <name>Zhiwen Yu</name>
    </author>
    <author>
      <name>Bin Guo</name>
    </author>
    <author>
      <name>Qi Han</name>
    </author>
    <author>
      <name>Jiangbin Su</name>
    </author>
    <author>
      <name>Jiahao Liao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMC.2020.3015750</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMC.2020.3015750" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Mobile Computing, vol. 21, no. 3, pp.
  878-894, 1 March 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1909.00805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.01843v1</id>
    <updated>2019-09-04T14:39:47Z</updated>
    <published>2019-09-04T14:39:47Z</published>
    <title>Mapping Spiking Neural Networks to Neuromorphic Hardware</title>
    <summary>  Neuromorphic hardware platforms implement biological neurons and synapses to
execute spiking neural networks (SNNs) in an energy-efficient manner. We
present SpiNeMap, a design methodology to map SNNs to crossbar-based
neuromorphic hardware, minimizing spike latency and energy consumption.
SpiNeMap operates in two steps: SpiNeCluster and SpiNePlacer. SpiNeCluster is a
heuristic-based clustering technique to partition SNNs into clusters of
synapses, where intracluster local synapses are mapped within crossbars of the
hardware and inter-cluster global synapses are mapped to the shared
interconnect. SpiNeCluster minimizes the number of spikes on global synapses,
which reduces spike congestion on the shared interconnect, improving
application performance. SpiNePlacer then finds the best placement of local and
global synapses on the hardware using a meta-heuristic-based approach to
minimize energy consumption and spike latency. We evaluate SpiNeMap using
synthetic and realistic SNNs on the DynapSE neuromorphic hardware. We show that
SpiNeMap reduces average energy consumption by 45% and average spike latency by
21%, compared to state-of-the-art techniques.
</summary>
    <author>
      <name>Adarsha Balaji</name>
    </author>
    <author>
      <name>Anup Das</name>
    </author>
    <author>
      <name>Yuefeng Wu</name>
    </author>
    <author>
      <name>Khanh Huynh</name>
    </author>
    <author>
      <name>Francesco Dell'Anna</name>
    </author>
    <author>
      <name>Giacomo Indiveri</name>
    </author>
    <author>
      <name>Jeffrey L. Krichmar</name>
    </author>
    <author>
      <name>Nikil Dutt</name>
    </author>
    <author>
      <name>Siebren Schaafsma</name>
    </author>
    <author>
      <name>Francky Catthoor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 14 images, 69 references, Accepted in IEEE Transactions on
  Very Large Scale Integration (VLSI) Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.01843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.01843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.05349v2</id>
    <updated>2021-06-27T23:34:25Z</updated>
    <published>2019-09-11T20:47:58Z</published>
    <title>Cache Where you Want! Reconciling Predictability and Coherent Caching</title>
    <summary>  Real-time and cyber-physical systems need to interact with and respond to
their physical environment in a predictable time. While multicore platforms
provide incredible computational power and throughput, they also introduce new
sources of unpredictability. Large fluctuations in latency to access data
shared between multiple cores is an important contributor to the overall
execution-time variability. In addition to the temporal unpredictability
introduced by caching, parallel applications with data shared across multiple
cores also pay additional latency overheads due to data coherence. Analyzing
the impact of data coherence on the worst-case execution-time of real-time
applications is challenging because only scarce implementation details are
revealed by manufacturers. This paper presents application level control for
caching data at different levels of the cache hierarchy. The rationale is that
by caching data only in shared cache it is possible to bypass private caches.
The access latency to data present in caches becomes independent of its
coherence state. We discuss the existing architectural support as well as the
required hardware and OS modifications to support the proposed cacheability
control. We evaluate the system on an architectural simulator. We show that the
worst case execution time for a single memory write request is reduced by 52%.
Benchmark evaluations show that proposed technique has a minimal impact on
average performance.
</summary>
    <author>
      <name>Ayoosh Bansal</name>
    </author>
    <author>
      <name>Jayati Singh</name>
    </author>
    <author>
      <name>Yifan Hao</name>
    </author>
    <author>
      <name>Jen-Yang Wen</name>
    </author>
    <author>
      <name>Renato Mancuso</name>
    </author>
    <author>
      <name>Marco Caccamo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MECO49872.2020.9134262</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MECO49872.2020.9134262" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 10 figures, v2 update includes overview section with formal
  solution definition. This is a long version of a prior publication</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2020 9th Mediterranean Conference on Embedded Computing (MECO),
  2020, pp. 1-6</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1909.05349v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.05349v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.0; C.3; C.4; D.4.7; J.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.09294v1</id>
    <updated>2019-09-20T02:08:24Z</updated>
    <published>2019-09-20T02:08:24Z</published>
    <title>Making Code Re-randomization Practical with MARDU</title>
    <summary>  Defense techniques such as Data Execution Prevention (DEP) and Address Space
Layout Randomization (ASLR) were the early role models preventing primitive
code injection and return-oriented programming (ROP) attacks. Notably, these
techniques did so in an elegant and utilitarian manner, keeping performance and
scalability in the forefront, making them one of the few widely-adopted defense
techniques. As code re-use has evolved in complexity from JIT-ROP, to BROP and
data-only attacks, defense techniques seem to have tunneled on defending at all
costs, losing-their-way in pragmatic defense design. Some fail to provide
comprehensive coverage, being too narrow in scope, while others provide
unrealistic overheads leaving users willing to take their chances to maintain
performance expectations.
  We present Mardu, an on-demand system-wide re-randomization technique that
improves re-randomization and refocuses efforts to simultaneously embrace key
characteristics of defense techniques: security, performance, and scalability.
Our code sharing with diversification is achieved by implementing reactive and
scalable, rather than continuous or one-time diversification while the use of
hardware supported eXecute-only Memory (XoM) and shadow stack prevent memory
disclosure; entwining and enabling code sharing further minimizes needed
tracking, patching costs, and memory overhead. Mardu's evaluation shows
performance and scalability to have low average overhead in both
compute-intensive (5.5% on SPEC) and real-world applications (4.4% on NGINX).
With this design, Mardu demonstrates that strong and scalable security
guarantees are possible to achieve at a practical cost to encourage deployment.
</summary>
    <author>
      <name>Christopher Jelesnianski</name>
    </author>
    <author>
      <name>Jinwoo Yom</name>
    </author>
    <author>
      <name>Changwoo Min</name>
    </author>
    <author>
      <name>Yeongjin Jang</name>
    </author>
    <link href="http://arxiv.org/abs/1909.09294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.09294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.07045v1</id>
    <updated>2020-01-20T10:23:12Z</updated>
    <published>2020-01-20T10:23:12Z</published>
    <title>SPARTA: A Divide and Conquer Approach to Address Translation for
  Accelerators</title>
    <summary>  Virtual memory (VM) is critical to the usability and programmability of
hardware accelerators. Unfortunately, implementing accelerator VM efficiently
is challenging because the area and power constraints make it difficult to
employ the large multi-level TLBs used in general-purpose CPUs. Recent research
proposals advocate a number of restrictions on virtual-to-physical address
mappings in order to reduce the TLB size or increase its reach. However, such
restrictions are unattractive because they forgo many of the original benefits
of traditional VM, such as demand paging and copy-on-write.
  We propose SPARTA, a divide and conquer approach to address translation.
SPARTA splits the address translation into accelerator-side and memory-side
parts. The accelerator-side translation hardware consists of a tiny TLB
covering only the accelerator's cache hierarchy (if any), while the translation
for main memory accesses is performed by shared memory-side TLBs. Performing
the translation for memory accesses on the memory side allows SPARTA to overlap
data fetch with translation, and avoids the replication of TLB entries for data
shared among accelerators. To further improve the performance and efficiency of
the memory-side translation, SPARTA logically partitions the memory space,
delegating translation to small and efficient per-partition translation
hardware. Our evaluation on index-traversal accelerators shows that SPARTA
virtually eliminates translation overhead, reducing it by over 30x on average
(up to 47x) and improving performance by 57%. At the same time, SPARTA requires
minimal accelerator-side translation hardware, reduces the total number of TLB
entries in the system, gracefully scales with memory size, and preserves all
key VM functionalities.
</summary>
    <author>
      <name>Javier Picorel</name>
    </author>
    <author>
      <name>Seyed Alireza Sanaee Kohroudi</name>
    </author>
    <author>
      <name>Zi Yan</name>
    </author>
    <author>
      <name>Abhishek Bhattacharjee</name>
    </author>
    <author>
      <name>Babak Falsafi</name>
    </author>
    <author>
      <name>Djordje Jevdjic</name>
    </author>
    <link href="http://arxiv.org/abs/2001.07045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.07045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09991v1</id>
    <updated>2020-01-26T16:24:08Z</updated>
    <published>2020-01-26T16:24:08Z</published>
    <title>Intel Page Modification Logging, a hardware virtualization feature:
  study and improvement for virtual machine working set estimation</title>
    <summary>  Intel Page Modification Logging (PML) is a novel hardware feature for
tracking virtual machine (VM) accessed memory pages. This task is essential in
today's data centers since it allows, among others, checkpointing, live
migration and working set size (WSS) estimation. Relying on the Xen hypervisor,
this paper studies PML from three angles: power consumption, efficiency, and
performance impact on user applications. Our findings are as follows. First,
PML does not incur any power consumption overhead. Second, PML reduces by up to
10.18% both VM live migration and checkpointing time. Third, PML slightly
reduces by up to 0.95% the performance degradation on applications incurred by
live migration and checkpointing. Fourth, PML however does not allow accurate
WSS estimation because read accesses are not tracked and hot pages cannot be
identified. A naive extension of PML for addressing these limitations could
lead to severe performance degradation (up to 34.8%) for the VM whose WSS is
computed.
  This paper presents Page Reference Logging (PRL), a smart extension of PML
for allowing both read and write accesses to be tracked. It does this without
impacting user VMs. The paper also presents a WSS estimation system which
leverages PRL and shows how this algorithm can be integrated into a data center
which implements memory overcommitment. We implement PRL and the WSS estimation
system under Gem5, a very popular hardware simulator. The evaluation results
validate the accuracy of PRL in the estimation of WSS. They also show that PRL
incurs no performance degradation for user VMs.
</summary>
    <author>
      <name>Stella Bitchebe</name>
    </author>
    <author>
      <name>Djob Mvondo</name>
    </author>
    <author>
      <name>Alain Tchana</name>
    </author>
    <author>
      <name>Laurent Réveillère</name>
    </author>
    <author>
      <name>Noël De Palma</name>
    </author>
    <link href="http://arxiv.org/abs/2001.09991v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09991v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.06536v1</id>
    <updated>2020-08-14T18:35:38Z</updated>
    <published>2020-08-14T18:35:38Z</published>
    <title>Making Distributed Mobile Applications SAFE: Enforcing User Privacy
  Policies on Untrusted Applications with Secure Application Flow Enforcement</title>
    <summary>  Today's mobile devices sense, collect, and store huge amounts of personal
information, which users share with family and friends through a wide range of
applications. Once users give applications access to their data, they must
implicitly trust that the apps correctly maintain data privacy. As we know from
both experience and all-too-frequent press articles, that trust is often
misplaced. While users do not trust applications, they do trust their mobile
devices and operating systems. Unfortunately, sharing applications are not
limited to mobile clients but must also run on cloud services to share data
between users. In this paper, we leverage the trust that users have in their
mobile OSes to vet cloud services. To do so, we define a new Secure Application
Flow Enforcement (SAFE) framework, which requires cloud services to attest to a
system stack that will enforce policies provided by the mobile OS for user
data. We implement a mobile OS that enforces SAFE policies on unmodified mobile
apps and two systems for enforcing policies on untrusted cloud services. Using
these prototypes, we demonstrate that it is possible to enforce existing user
privacy policies on unmodified applications.
</summary>
    <author>
      <name>Adriana Szekeres</name>
    </author>
    <author>
      <name>Irene Zhang</name>
    </author>
    <author>
      <name>Katelin Bailey</name>
    </author>
    <author>
      <name>Isaac Ackerman</name>
    </author>
    <author>
      <name>Haichen Shen</name>
    </author>
    <author>
      <name>Franziska Roesner</name>
    </author>
    <author>
      <name>Dan R. K. Ports</name>
    </author>
    <author>
      <name>Arvind Krishnamurthy</name>
    </author>
    <author>
      <name>Henry M. Levy</name>
    </author>
    <link href="http://arxiv.org/abs/2008.06536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.06536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.12501v1</id>
    <updated>2020-08-28T06:52:41Z</updated>
    <published>2020-08-28T06:52:41Z</published>
    <title>Analysis of Interference between RDMA and Local Access on Hybrid Memory
  System</title>
    <summary>  We can use a hybrid memory system consisting of DRAM and Intel Optane DC
Persistent Memory (We call it DCPM in this paper) as DCPM is now commercially
available since April 2019. Even if the latency for DCPM is several times
higher than that for DRAM, the capacity for DCPM is several times higher than
that for DRAM and the cost of DCPM is also several times lower than that for
DRAM. In addition, DCPM is non-volatile. A Server with this hybrid memory
system could improve the performance for in-memory database systems and virtual
machine (VM) systems because these systems often consume a large amount of
memory. Moreover, a high-speed shared storage system can be implemented by
accessing DCPM via remote direct memory access (RDMA). I assume that some of
the DCPM is often assigned as a shared area among other remote servers because
applications executed on a server with a hybrid memory system often cannot use
the entire capacity of DCPM. This paper evaluates the interference between
local memory access and RDMA from a remote server. As a result, I indicate that
the interference on this hybrid memory system is significantly different from
that on a conventional DRAM-only memory system. I also believe that some kind
of throttling implementation is needed when this interference occures.
</summary>
    <author>
      <name>Kazuichi Oe</name>
    </author>
    <link href="http://arxiv.org/abs/2008.12501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.12501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.12400v1</id>
    <updated>2020-10-23T13:43:45Z</updated>
    <published>2020-10-23T13:43:45Z</published>
    <title>Towards Efficiently Establishing Mutual Distrust Between Host
  Application and Enclave for SGX</title>
    <summary>  Since its debut, SGX has been used in many applications, e.g., secure data
processing. However, previous systems usually assume a trusted enclave and
ignore the security issues caused by an untrusted enclave. For instance, a
vulnerable (or even malicious) third-party enclave can be exploited to attack
the host application and the rest of the system. In this paper, we propose an
efficient mechanism to confine an untrusted enclave's behaviors. The threats of
an untrusted enclave come from the enclave-host asymmetries. They can be abused
to access arbitrary memory regions of its host application, jump to any code
location after leaving the enclave and forge the stack register to manipulate
the saved context. Our solution breaks such asymmetries and establishes mutual
distrust between the host application and the enclave. It leverages Intel MPK
for efficient memory isolation and the x86 single-step debugging mechanism to
capture the event when an enclave is existing. It then performs the integrity
check for the jump target and the stack pointer. We have solved two practical
challenges and implemented a prototype system. The evaluation with multiple
micro-benchmarks and representative real-world applications demonstrated the
efficiency of our system, with less than 4% performance overhead.
</summary>
    <author>
      <name>Yuan Chen</name>
    </author>
    <author>
      <name>Jiaqi Li</name>
    </author>
    <author>
      <name>Guorui Xu</name>
    </author>
    <author>
      <name>Yajin Zhou</name>
    </author>
    <author>
      <name>Zhi Wang</name>
    </author>
    <author>
      <name>Cong Wang</name>
    </author>
    <author>
      <name>Kui Ren</name>
    </author>
    <link href="http://arxiv.org/abs/2010.12400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.12400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.10249v2</id>
    <updated>2022-04-13T07:10:20Z</updated>
    <published>2020-11-20T07:48:27Z</published>
    <title>SIMF: Single-Instruction Multiple-Flush Mechanism for Processor Temporal
  Isolation</title>
    <summary>  Microarchitectural timing attacks are a type of information leakage attack,
which exploit the time-shared microarchitectural components, such as caches,
translation look-aside buffers (TLBs), branch prediction unit (BPU), and
speculative execution, in modern processors to leak critical information from a
victim process or thread. To mitigate such attacks, the mechanism for flushing
the on-core state is extensively used by operating-system-level solutions,
since on-core state is too expensive to partition. In these systems, the
flushing operations are implemented in software (using cache maintenance
instructions), which severely limit the efficiency of timing attack protection.
  To bridge this gap, we propose specialized hardware support, a
single-instruction multiple-flush (SIMF) mechanism to flush the core-level
state, which consists of L1 caches, BPU, TLBs, and register file. We
demonstrate SIMF by implementing it as an ISA extension, i.e., flushx
instruction, in scalar in-order RISC-V processor. The resultant processor is
prototyped on Xilinx ZCU102 FPGA and validated with state-of-art seL4
microkernel, Linux kernel in multi-core scenarios, and a cache timing attack.
Our evaluation shows that SIMF significantly alleviates the overhead of
flushing by more than a factor of two in execution time and reduces dynamic
instruction count by orders-of-magnitude.
</summary>
    <author>
      <name>Tuo Li</name>
    </author>
    <author>
      <name>Bradley Hopkins</name>
    </author>
    <author>
      <name>Sri Parameswaran</name>
    </author>
    <link href="http://arxiv.org/abs/2011.10249v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.10249v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.12047v2</id>
    <updated>2020-12-09T08:15:30Z</updated>
    <published>2020-11-24T11:46:00Z</published>
    <title>Minimal Virtual Machines on IoT Microcontrollers: The Case of Berkeley
  Packet Filters with rBPF</title>
    <summary>  Virtual machines (VM) are widely used to host and isolate software modules.
However, extremely small memory and low-energy budgets have so far prevented
wide use of VMs on typical microcontroller-based IoT devices. In this paper, we
explore the potential of two minimal VM approaches on such low-power hardware.
We design rBPF, a register-based VM based on extended Berkeley Packet Filters
(eBPF). We compare it with a stack-based VM based on WebAssembly (Wasm) adapted
for embedded systems. We implement prototypes of each VM, hosted in the IoT
operating system RIOT. We perform measurements on commercial off-the-shelf IoT
hardware. Unsurprisingly, we observe that both Wasm and rBPF virtual machines
yield execution time and memory overhead, compared to not using a VM. We show
however that this execution time overhead is tolerable for low-throughput,
low-energy IoT devices. We further show that, while using a VM based on Wasm
entails doubling the memory budget for a simple networked IoT application using
a 6LoWPAN/CoAP stack, using a VM based on rBPF requires only negligible memory
overhead (less than 10% more memory). rBPF is thus a promising approach to host
small software modules, isolated from OS software, and updatable on-demand,
over low-power networks.
</summary>
    <author>
      <name>Koen Zandberg</name>
    </author>
    <author>
      <name>Emmanuel Baccelli</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In proceedings of IFIP/IEEE PEMWN 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2011.12047v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.12047v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.10269v2</id>
    <updated>2021-12-12T11:14:22Z</updated>
    <published>2021-02-20T06:20:33Z</published>
    <title>SoftTRR: Protect Page Tables Against RowHammer Attacks using
  Software-only Target Row Refresh</title>
    <summary>  Rowhammer attacks that corrupt level-1 page tables to gain kernel privilege
are the most detrimental to system security and hard to mitigate. However,
recently proposed software-only mitigations are not effective against such
kernel privilege escalation attacks. In this paper, we propose an effective and
practical software-only defense, called SoftTRR, to protect page tables from
all existing rowhammer attacks on x86. The key idea of SoftTRR is to refresh
the rows occupied by page tables when a suspicious rowhammer activity is
detected. SoftTRR is motivated by DRAM-chip-based target row refresh (ChipTRR)
but eliminates its main security limitation (i.e., ChipTRR tracks a limited
number of rows and thus can be bypassed by many-sided hammer). Specifically,
SoftTRR protects an unlimited number of page tables by tracking memory accesses
to the rows that are in close proximity to page-table rows and refreshing the
page-table rows once the tracked access count exceeds a pre-defined threshold.
We implement a prototype of SoftTRR as a loadable kernel module, and evaluate
its security effectiveness, performance overhead, and memory consumption. The
experimental results show that SoftTRR protects page tables from real-world
rowhammer attacks and incurs small performance overhead as well as memory cost.
</summary>
    <author>
      <name>Zhi Zhang</name>
    </author>
    <author>
      <name>Yueqiang Cheng</name>
    </author>
    <author>
      <name>Minghua Wang</name>
    </author>
    <author>
      <name>Wei He</name>
    </author>
    <author>
      <name>Wenhao Wang</name>
    </author>
    <author>
      <name>Nepal Surya</name>
    </author>
    <author>
      <name>Yansong Gao</name>
    </author>
    <author>
      <name>Kang Li</name>
    </author>
    <author>
      <name>Zhe Wang</name>
    </author>
    <author>
      <name>Chenggang Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2102.10269v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.10269v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.07255v1</id>
    <updated>2021-07-15T11:26:31Z</updated>
    <published>2021-07-15T11:26:31Z</published>
    <title>PHiLIP on the HiL: Automated Multi-platform OS Testing with External
  Reference Devices</title>
    <summary>  Developing an operating system (OS) for low-end embedded devices requires
continuous adaptation to new hardware architectures and components, while
serviceability of features needs to be assured for each individual platform
under tight resource constraints. It is challenging to design a versatile and
accurate heterogeneous test environment that is agile enough to cover a
continuous evolution of the code base and platforms. This mission is even
morehallenging when organized in an agile open-source community process with
many contributors such as for the RIOT OS. Hardware in the Loop (HiL) testing
and Continuous Integration (CI) are automatable approaches to verify
functionality, prevent regressions, and improve the overall quality at
development speed in large community projects. In this paper, we present PHiLIP
(Primitive Hardware in the Loop Integration Product), an open-source external
reference device together with tools that validate the system software while it
controls hardware and interprets physical signals. Instead of focusing on a
specific test setting, PHiLIP takes the approach of a tool-assisted agile HiL
test process, designed for continuous evolution and deployment cycles. We
explain its design, describe how it supports HiL tests, evaluate performance
metrics, and report on practical experiences of employing PHiLIP in an
automated CI test infrastructure. Our initial deployment comprises 22 unique
platforms, each of which executes 98 peripheral tests every night. PHiLIP
allows for easy extension of low-cost, adaptive testing infrastructures but
serves testing techniques and tools to a much wider range of applications.
</summary>
    <author>
      <name>Kevin Weiss</name>
    </author>
    <author>
      <name>Michel Rottleuthner</name>
    </author>
    <author>
      <name>Thomas C. Schmidt</name>
    </author>
    <author>
      <name>Matthias Wählisch</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3477040</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3477040" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Embedded Computing Systems, Volume 20, Issue
  5s, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2107.07255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.07255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.1; D.2.5; C.3; D.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.07711v1</id>
    <updated>2022-01-19T16:51:18Z</updated>
    <published>2022-01-19T16:51:18Z</published>
    <title>Enhancing the Security &amp; Privacy of Wearable Brain-Computer Interfaces</title>
    <summary>  Brain computing interfaces (BCI) are used in a plethora of
safety/privacy-critical applications, ranging from healthcare to smart
communication and control. Wearable BCI setups typically involve a head-mounted
sensor connected to a mobile device, combined with ML-based data processing.
Consequently, they are susceptible to a multiplicity of attacks across the
hardware, software, and networking stacks used that can leak users' brainwave
data or at worst relinquish control of BCI-assisted devices to remote
attackers. In this paper, we: (i) analyse the whole-system security and privacy
threats to existing wearable BCI products from an operating system and
adversarial machine learning perspective; and (ii) introduce Argus, the first
information flow control system for wearable BCI applications that mitigates
these attacks. Argus' domain-specific design leads to a lightweight
implementation on Linux ARM platforms suitable for existing BCI use-cases. Our
proof of concept attacks on real-world BCI devices (Muse, NeuroSky, and
OpenBCI) led us to discover more than 300 vulnerabilities across the stacks of
six major attack vectors. Our evaluation shows Argus is highly effective in
tracking sensitive dataflows and restricting these attacks with an acceptable
memory and performance overhead (&lt;15%).
</summary>
    <author>
      <name>Zahra Tarkhani</name>
    </author>
    <author>
      <name>Lorena Qendro</name>
    </author>
    <author>
      <name>Malachy O'Connor Brown</name>
    </author>
    <author>
      <name>Oscar Hill</name>
    </author>
    <author>
      <name>Cecilia Mascolo</name>
    </author>
    <author>
      <name>Anil Madhavapeddy</name>
    </author>
    <link href="http://arxiv.org/abs/2201.07711v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.07711v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.13160v1</id>
    <updated>2022-01-31T12:13:17Z</updated>
    <published>2022-01-31T12:13:17Z</published>
    <title>AnyCall: Fast and Flexible System-Call Aggregation</title>
    <summary>  Operating systems rely on system calls to allow the controlled communication
of isolated processes with the kernel and other processes. Every system call
includes a processor mode switch from the unprivileged user mode to the
privileged kernel mode. Although processor mode switches are the essential
isolation mechanism to guarantee the system's integrity, they induce direct and
indirect performance costs as they invalidate parts of the processor state. In
recent years, high-performance networks and storage hardware has made the
user/kernel transition overhead the bottleneck for IO-heavy applications. To
make matters worse, security vulnerabilities in modern processors (e.g.,
Meltdown) have prompted kernel mitigations that further increase the transition
overhead. To decouple system calls from user/kernel transitions we propose
AnyCall, which uses an in-kernel compiler to execute safety-checked user
bytecode in kernel mode. This allows for very fast system calls interleaved
with error checking and processing logic using only a single user/kernel
transition. We have implemented AnyCall based on the Linux kernel's eBPF
subsystem. Our evaluation demonstrates that system call bursts are up to 55
times faster using AnyCall and that real-world applications can be sped up by
24% even if only a minimal part of their code is run by AnyCall.
</summary>
    <author>
      <name>Luis Gerhorst</name>
    </author>
    <author>
      <name>Benedict Herzog</name>
    </author>
    <author>
      <name>Stefan Reif</name>
    </author>
    <author>
      <name>Wolfgang Schröder-Preikschat</name>
    </author>
    <author>
      <name>Timo Hönig</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3477113.3487267</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3477113.3487267" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PLOS'21: Proceedings of the 11th Workshop on Programming Languages
  and Operating Systems. 2021. Association for Computing Machinery (ACM), New
  York, NY, USA, Pages 1-8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2201.13160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.13160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.10963v2</id>
    <updated>2022-05-24T18:57:20Z</updated>
    <published>2022-05-22T23:55:23Z</published>
    <title>Protecting File Activities via Deception for ARM TrustZone</title>
    <summary>  A TrustZone TEE often invokes an external filesystem. While filedata can be
encrypted, the revealed file activities can leak secrets. To hide the file
activities from the filesystem and its OS, we propose Enigma, a deception-based
defense injecting sybil file activities as the cover of the actual file
activities.
  Enigma contributes three new designs. (1) To make the deception credible, the
TEE generates sybil calls by replaying file calls from the TEE code under
protection. (2) To make sybil activities cheap, the TEE requests the OS to run
K filesystem images simultaneously. Concealing the disk, the TEE backs only one
image with the actual disk while backing other images by only storing their
metadata. (3) To protect filesystem image identities, the TEE shuffles the
images frequently, preventing the OS from observing any image for long.
  Enigma works with unmodified filesystems shipped withLinux. On a low-cost Arm
SoC with EXT4 and F2FS, our system can concurrently run as many as 50
filesystem images with 1% of disk overhead per additional image. Compared to
common obfuscation for hiding addresses in a flat space, Enigma hides file
activities with richer semantics. Its cost is lower by one order of magnitude
while achieving the same level of probabilistic security guarantees.
</summary>
    <author>
      <name>Liwei Guo</name>
    </author>
    <author>
      <name>Kaiyang Zhao</name>
    </author>
    <author>
      <name>Yiying Zhang</name>
    </author>
    <author>
      <name>Felix Xiaozhu Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.10963v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.10963v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.05616v1</id>
    <updated>2022-06-11T21:18:16Z</updated>
    <published>2022-06-11T21:18:16Z</published>
    <title>Is Kernel Code Different From Non-Kernel Code? A Case Study of BSD
  Family Operating Systems</title>
    <summary>  Code churn and code velocity describe the evolution of a code base. Current
research quantifies and studies code churn and velocity at a high level of
abstraction, often at the overall project level or even at the level of an
entire company. We argue that such an approach ignores noticeable differences
among the subsystems of large projects. We conducted an exploratory study on
four BSD family operating systems: DragonFlyBSD, FreeBSD, NetBSD, and OpenBSD.
We mine 797,879 commits to characterize code churn in terms of the annual
growth rate, commit types, change type ratio, and size taxonomy of commits for
different subsystems (kernel, non-kernel, and mixed). We also investigate
differences among various code review periods, i.e., time-to-first-response,
time-to-accept, and time-to-merge, as indicators of code velocity. Our study
provides empirical evidence that quantifiable evolutionary code characteristics
at a global system scope fail to take into account significant individual
differences that exist at a subsystem level. We found that while there exist
similarities in the code base growth rate and distribution of commit types
(neutral, additive, and subtractive) across BSD subsystems, (a) most commits
contain kernel or non-kernel code exclusively, (b) kernel commits are larger
than non-kernel commits, and (c) code reviews for kernel code take longer than
non-kernel code.
</summary>
    <author>
      <name>Gunnar Kudrjavets</name>
    </author>
    <author>
      <name>Jeff Thomas</name>
    </author>
    <author>
      <name>Nachiappan Nagappan</name>
    </author>
    <author>
      <name>Ayushi Rastogi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICSME55016.2022.00027</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICSME55016.2022.00027" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages. To be published in 38th IEEE International Conference on
  Software Maintenance and Evolution (ICSME 2022), Oct 3-7, 2022, Limassol,
  Cyprus</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.05616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.05616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.02699v1</id>
    <updated>2022-08-04T14:54:09Z</updated>
    <published>2022-08-04T14:54:09Z</published>
    <title>Ellipsis: Towards Efficient System Auditing for Real-Time Systems</title>
    <summary>  System auditing is a powerful tool that provides insight into the nature of
suspicious events in computing systems, allowing machine operators to detect
and subsequently investigate security incidents. While auditing has proven
invaluable to the security of traditional computers, existing audit frameworks
are rarely designed with consideration for Real-Time Systems (RTS). The
transparency provided by system auditing would be of tremendous benefit in a
variety of security-critical RTS domains, (e.g., autonomous vehicles); however,
if audit mechanisms are not carefully integrated into RTS, auditing can be
rendered ineffectual and violate the real-world temporal requirements of the
RTS.
  In this paper, we demonstrate how to adapt commodity audit frameworks to RTS.
Using Linux Audit as a case study, we first demonstrate that the volume of
audit events generated by commodity frameworks is unsustainable within the
temporal and resource constraints of real-time (RT) applications. To address
this, we present Ellipsis, a set of kernel-based reduction techniques that
leverage the periodic repetitive nature of RT applications to aggressively
reduce the costs of system-level auditing. Ellipsis generates succinct
descriptions of RT applications' expected activity while retaining a detailed
record of unexpected activities, enabling analysis of suspicious activity while
meeting temporal constraints. Our evaluation of Ellipsis, using ArduPilot (an
open-source autopilot application suite) demonstrates up to 93% reduction in
audit log generation.
</summary>
    <author>
      <name>Ayoosh Bansal</name>
    </author>
    <author>
      <name>Anant Kandikuppa</name>
    </author>
    <author>
      <name>Chien-Ying Chen</name>
    </author>
    <author>
      <name>Monowar Hasan</name>
    </author>
    <author>
      <name>Adam Bates</name>
    </author>
    <author>
      <name>Sibin Mohan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper accepted at ESORICS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.02699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.02699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6; C.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.02775v1</id>
    <updated>2022-10-06T09:26:34Z</updated>
    <published>2022-10-06T09:26:34Z</published>
    <title>Paging with Succinct Predictions</title>
    <summary>  Paging is a prototypical problem in the area of online algorithms. It has
also played a central role in the development of learning-augmented algorithms
-- a recent line of research that aims to ameliorate the shortcomings of
classical worst-case analysis by giving algorithms access to predictions. Such
predictions can typically be generated using a machine learning approach, but
they are inherently imperfect. Previous work on learning-augmented paging has
investigated predictions on (i) when the current page will be requested again
(reoccurrence predictions), (ii) the current state of the cache in an optimal
algorithm (state predictions), (iii) all requests until the current page gets
requested again, and (iv) the relative order in which pages are requested.
  We study learning-augmented paging from the new perspective of requiring the
least possible amount of predicted information. More specifically, the
predictions obtained alongside each page request are limited to one bit only.
We consider two natural such setups: (i) discard predictions, in which the
predicted bit denotes whether or not it is ``safe'' to evict this page, and
(ii) phase predictions, where the bit denotes whether the current page will be
requested in the next phase (for an appropriate partitioning of the input into
phases). We develop algorithms for each of the two setups that satisfy all
three desirable properties of learning-augmented algorithms -- that is, they
are consistent, robust and smooth -- despite being limited to a one-bit
prediction per request. We also present lower bounds establishing that our
algorithms are essentially best possible.
</summary>
    <author>
      <name>Antonios Antoniadis</name>
    </author>
    <author>
      <name>Joan Boyar</name>
    </author>
    <author>
      <name>Marek Eliáš</name>
    </author>
    <author>
      <name>Lene M. Favrholdt</name>
    </author>
    <author>
      <name>Ruben Hoeksma</name>
    </author>
    <author>
      <name>Kim S. Larsen</name>
    </author>
    <author>
      <name>Adam Polak</name>
    </author>
    <author>
      <name>Bertrand Simon</name>
    </author>
    <link href="http://arxiv.org/abs/2210.02775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.02775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.02821v1</id>
    <updated>2022-10-06T11:25:05Z</updated>
    <published>2022-10-06T11:25:05Z</published>
    <title>Microsoft Defender Will Be Defended: MemoryRanger Prevents Blinding
  Windows AV</title>
    <summary>  Windows OS is facing a huge rise in kernel attacks. An overview of popular
techniques that result in loading kernel drivers will be presented. One of the
key targets of modern threats is disabling and blinding Microsoft Defender, a
default Windows AV. The analysis of recent driver-based attacks will be given,
the challenge is to block them. The survey of user- and kernel-level attacks on
Microsoft Defender will be given. One of the recently published attackers
techniques abuses Mandatory Integrity Control (MIC) and Security Reference
Monitor (SRM) by modifying Integrity Level and Debug Privileges for the
Microsoft Defender via syscalls. However, this user-mode attack can be blocked
via the Windows 'trust labels' mechanism. The presented paper discovered the
internals of MIC and SRM, including the analysis of Microsoft Defender during
malware detection. We show how attackers can attack Microsoft Defender using a
kernel-mode driver. This driver modifies the fields of the Token structure
allocated for the Microsoft Defender application. The presented attack resulted
in disabling Microsoft Defender, without terminating any of its processes and
without triggering any Windows security features, such as PatchGuard. The
customized hypervisor-based solution named MemoryRanger was used to protect the
Windows Defender kernel structures. The experiments show that MemoryRanger
successfully restricts access to the sensitive kernel data from illegal access
attempts with affordable performance degradation.
</summary>
    <author>
      <name>Denis Pogonin</name>
    </author>
    <author>
      <name>Igor Korkin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 17 figures, 1 table, In Proceedings of the ADFSL 2022, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.02821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.02821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.04328v2</id>
    <updated>2022-12-28T06:53:56Z</updated>
    <published>2022-10-09T19:26:27Z</published>
    <title>seL4 Microkernel for virtualization use-cases: Potential directions
  towards a standard VMM</title>
    <summary>  Virtualization plays an essential role in providing security to computational
systems by isolating execution environments. Many software solutions, called
hypervisors, have been proposed to provide virtualization capabilities.
However, only a few were designed for being deployed at the edge of the
network, in devices with fewer computation resources when compared with servers
in the Cloud. Among the few lightweight software that can play the hypervisor
role, seL4 stands out by providing a small Trusted Computing Base and formally
verified components, enhancing its security. Despite today being more than a
decade with seL4 microkernel technology, its existing userland and tools are
still scarce and not very mature. Over the last few years, the main effort has
been put into increasing the maturity of the kernel itself and not the tools
and applications that can be hosted on top. Therefore, it currently lacks
proper support for a full-featured userland Virtual Machine Monitor, and the
existing one is quite fragmented. This article discusses the potential
directions to a standard VMM by presenting our view of design principles and
feature set needed. This article does not intend to define a standard VMM, we
intend to instigate this discussion through the seL4 community.
</summary>
    <author>
      <name>Everton de Matos</name>
    </author>
    <author>
      <name>Markku Ahvenjärvi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/electronics11244201</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/electronics11244201" rel="related"/>
    <link href="http://arxiv.org/abs/2210.04328v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.04328v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.16735v1</id>
    <updated>2022-11-30T04:48:07Z</updated>
    <published>2022-11-30T04:48:07Z</published>
    <title>Unsafe at Any Copy: Name Collisions from Mixing Case Sensitivities</title>
    <summary>  File name confusion attacks, such as malicious symbolic links and file
squatting, have long been studied as sources of security vulnerabilities.
However, a recently emerged type, i.e., case-sensitivity-induced name
collisions, has not been scrutinized. These collisions are introduced by
differences in name resolution under case-sensitive and case-insensitive file
systems or directories. A prominent example is the recent Git vulnerability
(CVE-2021-21300) which can lead to code execution on a victim client when it
clones a maliciously crafted repository onto a case-insensitive file system.
With trends including ext4 adding support for per-directory case-insensitivity
and the broad deployment of the Windows Subsystem for Linux, the prerequisites
for such vulnerabilities are increasingly likely to exist even in a single
system.
  In this paper, we make a first effort to investigate how and where the lack
of any uniform approach to handling name collisions leads to a diffusion of
responsibility and resultant vulnerabilities. Interestingly, we demonstrate the
existence of a range of novel security challenges arising from name collisions
and their inconsistent handling by low-level utilities and applications.
Specifically, our experiments show that utilities handle many name collision
scenarios unsafely, leaving the responsibility to applications whose developers
are unfortunately not yet aware of the threats. We examine three case studies
as a first step towards systematically understanding the emerging type of name
collision vulnerability.
</summary>
    <author>
      <name>Aditya Basu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The Pennsylvania State University</arxiv:affiliation>
    </author>
    <author>
      <name>John Sampson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The Pennsylvania State University</arxiv:affiliation>
    </author>
    <author>
      <name>Zhiyun Qian</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of California, Riverside</arxiv:affiliation>
    </author>
    <author>
      <name>Trent Jaeger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The Pennsylvania State University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 1 appendix, 2 tables, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.16735v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.16735v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.04344v1</id>
    <updated>2022-11-09T12:54:45Z</updated>
    <published>2022-11-09T12:54:45Z</published>
    <title>Performance Characterization of AutoNUMA Memory Tiering on Graph
  Analytics</title>
    <summary>  Non-Volatile Memory (NVM) can deliver higher density and lower cost per bit
when compared with DRAM. Its main drawback is that it is slower than DRAM. On
the other hand, DRAM has scalability problems due to its cost and energy
consumption. NVM will likely coexist with DRAM in computer systems and the
biggest challenge is to know which data to allocate on each type of memory. A
state-of-the-art approach is AutoNUMA, in the Linux kernel. Prior work is
limited to measuring AutoNUMA solely in terms of the application execution
time, without understanding AutoNUMA's behavior. In this work we provide a more
in-depth characterization of AutoNUMA, for instance, identifying where exactly
a set of pages are allocated, while keeping track of promotion and demotion
decisions performed by AutoNUMA. Our analysis shows that AutoNUMA's benefits
can be modest when running graph processing applications, or graph analytics,
because most pages have only one access over the entire execution time and
other pages accesses have no temporal locality. We make a case for exploring
application characteristics using object-level mappings between DRAM and NVM.
Our preliminary experiments show that an object-level memory tiering can better
capture the application behavior and reduce the execution time of graph
analytics by 21% (avg) and 51% (max), when compared to AutoNUMA, while
significantly reducing the number of memory accesses in NVM.
</summary>
    <author>
      <name>Diego Moura</name>
    </author>
    <author>
      <name>Vinicius Petrucci</name>
    </author>
    <author>
      <name>Daniel Mosse</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IISWC55918.2022.00024</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IISWC55918.2022.00024" rel="related"/>
    <link href="http://arxiv.org/abs/2212.04344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.04344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.11333v1</id>
    <updated>2022-12-21T20:15:17Z</updated>
    <published>2022-12-21T20:15:17Z</published>
    <title>E2C: A Visual Simulator for Heterogeneous Computing Systems</title>
    <summary>  Heterogeneity has been an indispensable aspect of distributed computing
throughout the history of these systems. In particular, with the increasing
prevalence of accelerator technologies (e.g., GPUs and TPUs) and the emergence
of domain-specific computing via ASICs and FPGA, the matter of heterogeneity
and harnessing it has become a more critical challenge than ever before.
Harnessing system heterogeneity has been a longstanding challenge in
distributed systems and has been investigated extensively in the past. Making
use of real infrastructure (such as those offered by the public cloud
providers) for benchmarking the performance of heterogeneous machines, for
different applications, with respect to different objectives, and under various
workload intensities is cost- and time-prohibitive. To mitigate this burden, we
develop an open-source simulation tool, called E2C, that can help researchers
and practitioners study any type of heterogeneous computing system and measure
its performance under various system configurations. E2C has an intuitive
graphical user interface (GUI) that enables its users to easily examine
system-level solutions (scheduling, load balancing, scalability, etc.) in a
controlled environment within a short time and at no cost. In particular, E2C
offers the following features: (i) simulating a heterogeneous computing system;
(ii) implementing a newly developed scheduling method and plugging it into the
system, (iii) measuring energy consumption and other output-related metrics;
and (iv) powerful visual aspects to ease the learning curve for students.
Potential users of E2C can be undergraduate and graduate students in computer
science/engineering, researchers, and practitioners.
</summary>
    <author>
      <name>Ali Mokhtari</name>
    </author>
    <author>
      <name>Mohsen Amini Salehi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">https://hpcclab.github.io/E2C-Sim-docs/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Tutorial at 15th ACM/IEEE Utility Cloud Computing (UCC '22)
  conference, Vancouver, Washington, USA, Dec. 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2212.11333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.11333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.07479v1</id>
    <updated>2023-01-18T12:47:14Z</updated>
    <published>2023-01-18T12:47:14Z</published>
    <title>Extensions for Shared Resource Orchestration in Kubernetes to Support
  RT-Cloud Containers</title>
    <summary>  Industries are considering the adoption of cloud computing for real-time
applications due to current improvements in network latencies and the advent of
Fog and Edge computing. To create an RT-cloud capable of hosting real-time
applications, it is increasingly significant to improve the entire stack,
including the containerization of applications, and their deployment and
orchestration across nodes. However, state-of-the-art orchestrators (e.g.,
Kubernetes) and underlying container engines are designed for general-purpose
applications. They ignore orchestration and management of shared resources
(e.g. memory bandwidth, cache, shared interconnect) making them unsuitable for
use with an RT-cloud. Taking inspiration from existing resource management
architectures for multicore nodes, such as ACTORS, and for distributed
mixed-criticality systems, such as the DREAMS, we propose a series of
extensions in the way shared resources are orchestrated by Kubernetes and
managed by the underlying Linux layers. Our approach allows fine-grained
monitoring and allocation of low-level shared resources on nodes to provide
better isolation to real-time containers and supports dynamic orchestration and
balancing of containers across the nodes based on the availability and demand
of shared resources.
</summary>
    <author>
      <name>Gabriele Monaco</name>
    </author>
    <author>
      <name>Gautam Gala</name>
    </author>
    <author>
      <name>Gerhard Fohler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, Accepted at the Real-time Cloud Workshop of the
  34th Euromicro Conference on Real-time Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.07479v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.07479v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3; D.4.7; C.2.4; D.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.06418v2</id>
    <updated>2023-03-24T00:25:31Z</updated>
    <published>2023-02-13T14:55:06Z</published>
    <title>PADLL: Taming Metadata-intensive HPC Jobs Through Dynamic,
  Application-agnostic QoS Control</title>
    <summary>  Modern I/O applications that run on HPC infrastructures are increasingly
becoming read and metadata intensive. However, having multiple concurrent
applications submitting large amounts of metadata operations can easily
saturate the shared parallel file system's metadata resources, leading to
overall performance degradation and I/O unfairness. We present PADLL, an
application and file system agnostic storage middleware that enables QoS
control of data and metadata workflows in HPC storage systems. It adopts ideas
from Software-Defined Storage, building data plane stages that mediate and rate
limit POSIX requests submitted to the shared file system, and a control plane
that holistically coordinates how all I/O workflows are handled. We demonstrate
its performance and feasibility under multiple QoS policies using synthetic
benchmarks, real-world applications, and traces collected from a production
file system. Results show that PADLL can enforce complex storage QoS policies
over concurrent metadata-aggressive jobs, ensuring fairness and prioritization.
</summary>
    <author>
      <name>Ricardo Macedo</name>
    </author>
    <author>
      <name>Mariana Miranda</name>
    </author>
    <author>
      <name>Yusuke Tanimura</name>
    </author>
    <author>
      <name>Jason Haga</name>
    </author>
    <author>
      <name>Amit Ruhela</name>
    </author>
    <author>
      <name>Stephen Lien Harrell</name>
    </author>
    <author>
      <name>Richard Todd Evans</name>
    </author>
    <author>
      <name>José Pereira</name>
    </author>
    <author>
      <name>João Paulo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at 23rd IEEE/ACM International Symposium on Cluster, Cloud
  and Internet Computing (CCGrid'23)</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.06418v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.06418v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.13863v2</id>
    <updated>2023-03-09T07:23:25Z</updated>
    <published>2023-02-27T15:03:15Z</published>
    <title>Capstone: A Capability-based Foundation for Trustless Secure Memory
  Access (Extended Version)</title>
    <summary>  Capability-based memory isolation is a promising new architectural primitive.
Software can access low-level memory only via capability handles rather than
raw pointers, which provides a natural interface to enforce security
restrictions. Existing architectural capability designs such as CHERI provide
spatial safety, but fail to extend to other memory models that
security-sensitive software designs may desire. In this paper, we propose
Capstone, a more expressive architectural capability design that supports
multiple existing memory isolation models in a trustless setup, i.e., without
relying on trusted software components. We show how Capstone is well-suited for
environments where privilege boundaries are fluid (dynamically extensible),
memory sharing/delegation are desired both temporally and spatially, and where
such needs are to be balanced with availability concerns. Capstone can also be
implemented efficiently. We present an implementation sketch and through
evaluation show that its overhead is below 50% in common use cases. We also
prototype a functional emulator for Capstone and use it to demonstrate the
runnable implementations of six real-world memory models without trusted
software components: three types of enclave-based TEEs, a thread scheduler, a
memory allocator, and Rust-style memory safety -- all within the interface of
Capstone.
</summary>
    <author>
      <name>Jason Zhijingcheng Yu</name>
    </author>
    <author>
      <name>Conrad Watt</name>
    </author>
    <author>
      <name>Aditya Badole</name>
    </author>
    <author>
      <name>Trevor E. Carlson</name>
    </author>
    <author>
      <name>Prateek Saxena</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 10 figures. This is an extended version of a paper to
  appear at 32nd USENIX Security Symposium, August 2023; acknowledgments
  updated</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.13863v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.13863v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.03700v2</id>
    <updated>2023-07-14T03:11:45Z</updated>
    <published>2023-03-07T07:31:46Z</published>
    <title>EavesDroid: Eavesdropping User Behaviors via OS Side-Channels on
  Smartphones</title>
    <summary>  As the Internet of Things (IoT) continues to evolve, smartphones have become
essential components of IoT systems. However, with the increasing amount of
personal information stored on smartphones, user privacy is at risk of being
compromised by malicious attackers. Although malware detection engines are
commonly installed on smartphones against these attacks, attacks that can evade
these defenses may still emerge. In this paper, we analyze the return values of
system calls on Android smartphones and find two never-disclosed vulnerable
return values that can leak fine-grained user behaviors. Based on this
observation, we present EavesDroid, an application-embedded side-channel attack
on Android smartphones that allows unprivileged attackers to accurately
identify fine-grained user behaviors (e.g., viewing messages and playing
videos) via on-screen operations. Our attack relies on the correlation between
user behaviors and the return values associated with hardware and system
resources. While this attack is challenging since these return values are
susceptible to fluctuation and misalignment caused by many factors, we show
that attackers can eavesdrop on fine-grained user behaviors using a CNN-GRU
classification model that adopts min-max normalization and multiple return
value fusion. Our experiments on different models and versions of Android
smartphones demonstrate that EavesDroid can achieve 98% and 86% inference
accuracy for 17 classes of user behaviors in the test set and real-world
settings, highlighting the risk of our attack on user privacy. Finally, we
recommend effective malware detection, carefully designed obfuscation methods,
or restrictions on reading vulnerable return values to mitigate this attack.
</summary>
    <author>
      <name>Quancheng Wang</name>
    </author>
    <author>
      <name>Ming Tang</name>
    </author>
    <author>
      <name>Jianming Fu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JIOT.2023.3298992</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JIOT.2023.3298992" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 25 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.03700v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.03700v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.06153v1</id>
    <updated>2023-03-10T04:37:07Z</updated>
    <published>2023-03-10T04:37:07Z</published>
    <title>CXLMemSim: A pure software simulated CXL.mem for performance
  characterization</title>
    <summary>  The emerging CXL.mem standard provides a new type of byte-addressable remote
memory with a variety of memory types and hierarchies. With CXL.mem, multiple
layers of memory -- e.g., local DRAM and CXL-attached remote memory at
different locations -- are exposed to operating systems and user applications,
bringing new challenges and research opportunities. Unfortunately, since
CXL.mem devices are not commercially available, it is difficult for researchers
to conduct systems research that uses CXL.mem. In this paper, we present our
ongoing work, CXLMemSim, a fast and lightweight CXL.mem simulator for
performance characterization. CXLMemSim uses a performance model driven using
performance monitoring events, which are supported by most commodity
processors. Specifically, CXLMemSim attaches to an existing, unmodified
program, and divides the execution of the program into multiple epochs; once an
epoch finishes, CXLMemSim collects performance monitoring events and calculates
the simulated execution time of the epoch based on these events. Through this
method, CXLMemSim avoids the performance overhead of a full-system simulator
(e.g., Gem5) and allows the memory hierarchy and latency to be easily adjusted,
enabling research such as memory scheduling for complex applications. Our
preliminary evaluation shows that CXLMemSim slows down the execution of the
attached program by 4.41x on average for real-world applications.
</summary>
    <author>
      <name>Yiwei Yang</name>
    </author>
    <author>
      <name>Pooneh Safayenikoo</name>
    </author>
    <author>
      <name>Jiacheng Ma</name>
    </author>
    <author>
      <name>Tanvir Ahmed Khan</name>
    </author>
    <author>
      <name>Andrew Quinn</name>
    </author>
    <link href="http://arxiv.org/abs/2303.06153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.06153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.12817v1</id>
    <updated>2023-03-22T12:16:00Z</updated>
    <published>2023-03-22T12:16:00Z</published>
    <title>IRIS: a Record and Replay Framework to Enable Hardware-assisted
  Virtualization Fuzzing</title>
    <summary>  Nowadays, industries are looking into virtualization as an effective means to
build safe applications, thanks to the isolation it can provide among virtual
machines (VMs) running on the same hardware. In this context, a fundamental
issue is understanding to what extent the isolation is guaranteed, despite
possible (or induced) problems in the virtualization mechanisms. Uncovering
such isolation issues is still an open challenge, especially for
hardware-assisted virtualization, since the search space should include all the
possible VM states (and the linked hypervisor state), which is prohibitive. In
this paper, we propose IRIS, a framework to record (learn) sequences of inputs
(i.e., VM seeds) from the real guest execution (e.g., OS boot), replay them
as-is to reach valid and complex VM states, and finally use them as valid seed
to be mutated for enabling fuzzing solutions for hardware-assisted hypervisors.
We demonstrate the accuracy and efficiency of IRIS in automatically reproducing
valid VM behaviors, with no need to execute guest workloads. We also provide a
proof-of-concept fuzzer, based on the proposed architecture, showing its
potential on the Xen hypervisor.
</summary>
    <author>
      <name>Carmine Cesarano</name>
    </author>
    <author>
      <name>Marcello Cinque</name>
    </author>
    <author>
      <name>Domenico Cotroneo</name>
    </author>
    <author>
      <name>Luigi De Simone</name>
    </author>
    <author>
      <name>Giorgio Farina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, Accepted for publication at The 53rd Annual IEEE/IFIP
  International Conference on Dependable Systems and Networks (DSN)</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.12817v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.12817v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.13226v2</id>
    <updated>2024-04-25T16:40:31Z</updated>
    <published>2023-03-23T12:43:57Z</published>
    <title>LearnedFTL: A Learning-Based Page-Level FTL for Reducing Double Reads in
  Flash-Based SSDs</title>
    <summary>  We present LearnedFTL, a new on-demand page-level flash translation layer
(FTL) design, which employs learned indexes to improve the address translation
efficiency of flash-based SSDs. The first of its kind, it reduces the number of
double reads induced by address translation in random read accesses. LearnedFTL
proposes three key techniques: an in-place-update linear model to build learned
indexes efficiently, a virtual PPN representation to obtain contiguous PPNs for
sorted LPNs, and a group-based allocation and model training via GC/rewrite
strategy to reduce the training overhead. By tightly integrating the
aforementioned key techniques, LearnedFTL considerably speeds up address
translation while reducing the number of flash read accesses caused by the
address translation. Our extensive experiments on a FEMU-based prototype show
that LearnedFTL can reduce up to 55.5\% address translation-induced double
reads. As a result, LearnedFTL reduces the P99 tail latency by 2.9$\times$
$\sim$ 12.2$\times$ with an average of 5.5$\times$ and 8.2$\times$ compared to
the state-of-the-art TPFTL and LeaFTL schemes, respectively.
</summary>
    <author>
      <name>Shengzhe Wang</name>
    </author>
    <author>
      <name>Zihang Lin</name>
    </author>
    <author>
      <name>Suzhen Wu</name>
    </author>
    <author>
      <name>Hong Jiang</name>
    </author>
    <author>
      <name>Jie Zhang</name>
    </author>
    <author>
      <name>Bo Mao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HPCA57654.2024.00054</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HPCA57654.2024.00054" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in 2024 IEEE International Symposium on High-Performance
  Computer Architecture (HPCA'24)</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.13226v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.13226v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.05148v1</id>
    <updated>2023-04-11T11:29:51Z</updated>
    <published>2023-04-11T11:29:51Z</published>
    <title>High-performance and Scalable Software-based NVMe Virtualization
  Mechanism with I/O Queues Passthrough</title>
    <summary>  NVMe(Non-Volatile Memory Express) is an industry standard for solid-state
drives (SSDs) that has been widely adopted in data centers. NVMe virtualization
is crucial in cloud computing as it allows for virtualized NVMe devices to be
used by virtual machines (VMs), thereby improving the utilization of storage
resources. However, traditional software-based solutions have flexibility
benefits but often come at the cost of performance degradation or high CPU
overhead. On the other hand, hardware-assisted solutions offer high performance
and low CPU usage, but their adoption is often limited by the need for special
hardware support or the requirement for new hardware development.
  In this paper, we propose LightIOV, a novel software-based NVMe
virtualization mechanism that achieves high performance and scalability without
consuming valuable CPU resources and without requiring special hardware
support. LightIOV can support thousands of VMs on each server. The key idea
behind LightIOV is NVMe hardware I/O queues passthrough, which enables VMs to
directly access I/O queues of NVMe devices, thus eliminating virtualization
overhead and providing near-native performance. Results from our experiments
show that LightIOV can provide comparable performance to VFIO, with an IOPS of
97.6%-100.2% of VFIO. Furthermore, in high-density VMs environments, LightIOV
achieves 31.4% lower latency than SPDK-Vhost when running 200 VMs, and an
improvement of 27.1% in OPS performance in real-world applications.
</summary>
    <author>
      <name>Yiquan Chen</name>
    </author>
    <author>
      <name>Zhen Jin</name>
    </author>
    <author>
      <name>Yijing Wang</name>
    </author>
    <author>
      <name>Yi Chen</name>
    </author>
    <author>
      <name>Hao Yu</name>
    </author>
    <author>
      <name>Jiexiong Xu</name>
    </author>
    <author>
      <name>Jinlong Chen</name>
    </author>
    <author>
      <name>Wenhai Lin</name>
    </author>
    <author>
      <name>Kanghua Fang</name>
    </author>
    <author>
      <name>Chengkun Wei</name>
    </author>
    <author>
      <name>Qiang Liu</name>
    </author>
    <author>
      <name>Yuan Xie</name>
    </author>
    <author>
      <name>Wenzhi Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2304.05148v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.05148v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.07349v1</id>
    <updated>2023-04-14T18:47:55Z</updated>
    <published>2023-04-14T18:47:55Z</published>
    <title>Remote Procedure Call as a Managed System Service</title>
    <summary>  Remote Procedure Call (RPC) is a widely used abstraction for cloud computing.
The programmer specifies type information for each remote procedure, and a
compiler generates stub code linked into each application to marshal and
unmarshal arguments into message buffers. Increasingly, however, application
and service operations teams need a high degree of visibility and control over
the flow of RPCs between services, leading many installations to use sidecars
or service mesh proxies for manageability and policy flexibility. These
sidecars typically involve inspection and modification of RPC data that the
stub compiler had just carefully assembled, adding needless overhead. Further,
upgrading diverse application RPC stubs to use advanced hardware capabilities
such as RDMA or DPDK is a long and involved process, and often incompatible
with sidecar policy control.
  In this paper, we propose, implement, and evaluate a novel approach, where
RPC marshalling and policy enforcement are done as a system service rather than
as a library linked into each application. Applications specify type
information to the RPC system as before, while the RPC service executes policy
engines and arbitrates resource use, and then marshals data customized to the
underlying network hardware capabilities. Our system, mRPC, also supports live
upgrades so that both policy and marshalling code can be updated transparently
to application code. Compared with using a sidecar, mRPC speeds up a standard
microservice benchmark, DeathStarBench, by up to 2.5$\times$ while having a
higher level of policy flexibility and availability.
</summary>
    <author>
      <name>Jingrong Chen</name>
    </author>
    <author>
      <name>Yongji Wu</name>
    </author>
    <author>
      <name>Shihan Lin</name>
    </author>
    <author>
      <name>Yechen Xu</name>
    </author>
    <author>
      <name>Xinhao Kong</name>
    </author>
    <author>
      <name>Thomas Anderson</name>
    </author>
    <author>
      <name>Matthew Lentz</name>
    </author>
    <author>
      <name>Xiaowei Yang</name>
    </author>
    <author>
      <name>Danyang Zhuo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NSDI 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.07349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.07349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.08717v2</id>
    <updated>2023-07-19T17:30:33Z</updated>
    <published>2023-04-18T03:49:17Z</published>
    <title>InversOS: Efficient Control-Flow Protection for AArch64 Applications
  with Privilege Inversion</title>
    <summary>  With the increasing popularity of AArch64 processors in general-purpose
computing, securing software running on AArch64 systems against control-flow
hijacking attacks has become a critical part toward secure computation. Shadow
stacks keep shadow copies of function return addresses and, when protected from
illegal modifications and coupled with forward-edge control-flow integrity,
form an effective and proven defense against such attacks. However, AArch64
lacks native support for write-protected shadow stacks, while software
alternatives either incur prohibitive performance overhead or provide weak
security guarantees.
  We present InversOS, the first hardware-assisted write-protected shadow
stacks for AArch64 user-space applications, utilizing commonly available
features of AArch64 to achieve efficient intra-address space isolation (called
Privilege Inversion) required to protect shadow stacks. Privilege Inversion
adopts unconventional design choices that run protected applications in the
kernel mode and mark operating system (OS) kernel memory as user-accessible;
InversOS therefore uses a novel combination of OS kernel modifications,
compiler transformations, and another AArch64 feature to ensure the safety of
doing so and to support legacy applications. We show that InversOS is secure by
design, effective against various control-flow hijacking attacks, and
performant on selected benchmarks and applications (incurring overhead of 7.0%
on LMBench, 7.1% on SPEC CPU 2017, and 3.0% on Nginx web server).
</summary>
    <author>
      <name>Zhuojia Shen</name>
    </author>
    <author>
      <name>John Criswell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 9 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.08717v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.08717v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.05455v3</id>
    <updated>2024-06-04T09:32:28Z</updated>
    <published>2023-05-04T10:15:06Z</published>
    <title>ONCache: A Cache-Based Low-Overhead Container Overlay Network</title>
    <summary>  Recent years have witnessed a widespread adoption of containers. While
containers simplify and accelerate application development, existing container
network technologies either incur significant overhead, which hurts performance
for distributed applications, or lose flexibility or compatibility, which
hinders the widespread deployment in production.
  We carefully analyze the kernel data path of an overlay network, quantifying
the time consumed by each segment of the data path and identifying the
\emph{extra overhead} in an overlay network compared to bare metal. We observe
that this extra overhead generates repetitive results among packets, which
inspires us to introduce caches within an overlay network.
  We design and implement ONCache (\textbf{O}verlay \textbf{N}etwork
\textbf{Cache}), a cache-based container overlay network, to eliminate the
extra overhead while maintaining flexibility and compatibility. We implement
ONCache using the extended Berkeley Packet Filter (eBPF) with only 524 lines of
code, and integrate it as a plugin of Antrea. With ONCache, containers attain
networking performance akin to that of bare metal. Compared to the standard
overlay networks, ONCache improves throughput and request-response transaction
rate by 12\% and 36\% for TCP (20\% and 34\% for UDP), respectively, while
significantly reducing per-packet CPU overhead. Popular distributed
applications also benefit from ONCache.
</summary>
    <author>
      <name>Shengkai Lin</name>
    </author>
    <author>
      <name>Shizhen Zhao</name>
    </author>
    <author>
      <name>Peirui Cao</name>
    </author>
    <author>
      <name>Xinchi Han</name>
    </author>
    <author>
      <name>Quan Tian</name>
    </author>
    <author>
      <name>Wenfeng Liu</name>
    </author>
    <author>
      <name>Qi Wu</name>
    </author>
    <author>
      <name>Donghai Han</name>
    </author>
    <author>
      <name>Xinbing Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2305.05455v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.05455v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.07147v1</id>
    <updated>2023-05-11T21:26:22Z</updated>
    <published>2023-05-11T21:26:22Z</published>
    <title>COLA: Characterizing and Optimizing the Tail Latency for Safe Level-4
  Autonomous Vehicle Systems</title>
    <summary>  Autonomous vehicles (AVs) are envisioned to revolutionize our life by
providing safe, relaxing, and convenient ground transportation. The computing
systems in such vehicles are required to interpret various sensor data and
generate responses to the environment in a timely manner to ensure driving
safety. However, such timing-related safety requirements are largely unexplored
in prior works.
  In this paper, we conduct a systematic study to understand the timing
requirements of AV systems. We focus on investigating and mitigating the
sources of tail latency in Level-4 AV computing systems. We observe that the
performance of AV algorithms is not uniformly distributed -- instead, the
latency is susceptible to vehicle environment fluctuations, such as traffic
density. This contributes to burst computation and memory access in response to
the traffic, and further leads to tail latency in the system. Furthermore, we
observe that tail latency also comes from a mismatch between the pre-configured
AV computation pipeline and the dynamic latency requirements in real-world
driving scenarios.
  Based on these observations, we propose a set of system designs to mitigate
AV tail latency. We demonstrate our design on widely-used industrial Level-4 AV
systems, Baidu Apollo and Autoware. The evaluation shows that our design
achieves 1.65 X improvement over the worst-case latency and 1.3 X over the
average latency, and avoids 93% of accidents on Apollo.
</summary>
    <author>
      <name>Haolan Liu</name>
    </author>
    <author>
      <name>Zixuan Wang</name>
    </author>
    <author>
      <name>Jishen Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/2305.07147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.07147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.10863v1</id>
    <updated>2023-05-18T10:34:23Z</updated>
    <published>2023-05-18T10:34:23Z</published>
    <title>Quiver: Supporting GPUs for Low-Latency, High-Throughput GNN Serving
  with Workload Awareness</title>
    <summary>  Systems for serving inference requests on graph neural networks (GNN) must
combine low latency with high throughout, but they face irregular computation
due to skew in the number of sampled graph nodes and aggregated GNN features.
This makes it challenging to exploit GPUs effectively: using GPUs to sample
only a few graph nodes yields lower performance than CPU-based sampling; and
aggregating many features exhibits high data movement costs between GPUs and
CPUs. Therefore, current GNN serving systems use CPUs for graph sampling and
feature aggregation, limiting throughput.
  We describe Quiver, a distributed GPU-based GNN serving system with
low-latency and high-throughput. Quiver's key idea is to exploit workload
metrics for predicting the irregular computation of GNN requests, and governing
the use of GPUs for graph sampling and feature aggregation: (1) for graph
sampling, Quiver calculates the probabilistic sampled graph size, a metric that
predicts the degree of parallelism in graph sampling. Quiver uses this metric
to assign sampling tasks to GPUs only when the performance gains surpass
CPU-based sampling; and (2) for feature aggregation, Quiver relies on the
feature access probability to decide which features to partition and replicate
across a distributed GPU NUMA topology. We show that Quiver achieves up to 35
times lower latency with an 8 times higher throughput compared to
state-of-the-art GNN approaches (DGL and PyG).
</summary>
    <author>
      <name>Zeyuan Tan</name>
    </author>
    <author>
      <name>Xiulong Yuan</name>
    </author>
    <author>
      <name>Congjie He</name>
    </author>
    <author>
      <name>Man-Kit Sit</name>
    </author>
    <author>
      <name>Guo Li</name>
    </author>
    <author>
      <name>Xiaoze Liu</name>
    </author>
    <author>
      <name>Baole Ai</name>
    </author>
    <author>
      <name>Kai Zeng</name>
    </author>
    <author>
      <name>Peter Pietzuch</name>
    </author>
    <author>
      <name>Luo Mai</name>
    </author>
    <link href="http://arxiv.org/abs/2305.10863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.10863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.18639v3</id>
    <updated>2024-10-03T01:58:25Z</updated>
    <published>2023-05-29T22:27:37Z</published>
    <title>Securing Cloud File Systems with Trusted Execution</title>
    <summary>  Cloud file systems offer organizations a scalable and reliable file storage
solution. However, cloud file systems have become prime targets for
adversaries, and traditional designs are not equipped to protect organizations
against the myriad of attacks that may be initiated by a malicious cloud
provider, co-tenant, or end-client. Recently proposed designs leveraging
cryptographic techniques and trusted execution environments (TEEs) still force
organizations to make undesirable trade-offs, consequently leading to either
security, functional, or performance limitations. In this paper, we introduce
BFS, a cloud file system that leverages the security capabilities provided by
TEEs to bootstrap new security protocols that deliver strong security
guarantees, high-performance, and a transparent POSIX-like interface to
clients. BFS delivers stronger security guarantees and up to a 2.5X speedup
over a state-of-the-art secure file system. Moreover, compared to the industry
standard NFS, BFS achieves up to 2.2X speedups across micro-benchmarks and
incurs &lt;1X overhead for most macro-benchmark workloads. BFS demonstrates a
holistic cloud file system design that does not sacrifice an organizations'
security yet can embrace all of the functional and performance advantages of
outsourcing.
</summary>
    <author>
      <name>Quinn Burke</name>
    </author>
    <author>
      <name>Yohan Beugin</name>
    </author>
    <author>
      <name>Blaine Hoak</name>
    </author>
    <author>
      <name>Rachel King</name>
    </author>
    <author>
      <name>Eric Pauley</name>
    </author>
    <author>
      <name>Ryan Sheatsley</name>
    </author>
    <author>
      <name>Mingli Yu</name>
    </author>
    <author>
      <name>Ting He</name>
    </author>
    <author>
      <name>Thomas La Porta</name>
    </author>
    <author>
      <name>Patrick McDaniel</name>
    </author>
    <link href="http://arxiv.org/abs/2305.18639v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.18639v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.01811v3</id>
    <updated>2023-06-23T07:34:40Z</updated>
    <published>2023-06-02T07:00:42Z</published>
    <title>DVFO: Learning-Based DVFS for Energy-Efficient Edge-Cloud Collaborative
  Inference</title>
    <summary>  Due to limited resources on edge and different characteristics of deep neural
network (DNN) models, it is a big challenge to optimize DNN inference
performance in terms of energy consumption and end-to-end latency on edge
devices. In addition to the dynamic voltage frequency scaling (DVFS) technique,
the edge-cloud architecture provides a collaborative approach for efficient DNN
inference. However, current edge-cloud collaborative inference methods have not
optimized various compute resources on edge devices. Thus, we propose DVFO, a
novel DVFS-enabled edge-cloud collaborative inference framework, which
co-optimizes DVFS and offloading parameters via deep reinforcement learning
(DRL). Specifically, DVFO automatically co-optimizes 1) the CPU, GPU and memory
frequencies of edge devices, and 2) the feature maps to be offloaded to cloud
servers. In addition, it leverages a thinking-while-moving concurrent mechanism
to accelerate the DRL learning process, and a spatial-channel attention
mechanism to extract DNN feature maps of secondary importance for workload
offloading. This approach improves inference performance for different DNN
models under various edge-cloud network conditions. Extensive evaluations using
two datasets and six widely-deployed DNN models on three heterogeneous edge
devices show that DVFO significantly reduces the energy consumption by 33% on
average, compared to state-of-the-art schemes. Moreover, DVFO achieves up to
28.6%-59.1% end-to-end latency reduction, while maintaining accuracy within 1%
loss on average.
</summary>
    <author>
      <name>Ziyang Zhang</name>
    </author>
    <author>
      <name>Yang Zhao</name>
    </author>
    <author>
      <name>Huan Li</name>
    </author>
    <author>
      <name>Changyao Lin</name>
    </author>
    <author>
      <name>Jie Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2306.01811v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.01811v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.11043v2</id>
    <updated>2023-07-05T02:31:46Z</updated>
    <published>2023-06-19T16:09:07Z</published>
    <title>DFlow: Efficient Dataflow-based Invocation Workflow Execution for
  Function-as-a-Service</title>
    <summary>  The Serverless Computing is becoming increasingly popular due to its ease of
use and fine-grained billing. These features make it appealing for stateful
application or serverless workflow. However, current serverless workflow
systems utilize a controlflow-based invocation pattern to invoke functions. In
this execution pattern, the function invocation depends on the state of the
function. A function can only begin executing once all its precursor functions
have completed. As a result, this pattern may potentially lead to longer
end-to-end execution time. We design and implement the DFlow, a novel
dataflow-based serverless workflow system that achieves high performance for
serverless workflow. DFlow introduces a distributed scheduler (DScheduler) by
using the dataflow-based invocation pattern to invoke functions. In this
pattern, the function invocation depends on the data dependency between
functions. The function can start to execute even its precursor functions are
still running. DFlow further features a distributed store (DStore) that
utilizes effective fine-grained optimization techniques to eliminate function
interaction, thereby enabling efficient data exchange. With the support of
DScheduler and DStore, DFlow can achieving an average improvement of 60% over
CFlow, 40% over FaaSFlow, 25% over FaasFlowRedis, and 40% over KNIX on 99%-ile
latency respectively. Further, it can improve network bandwidth utilization by
2x-4x over CFlow and 1.5x-3x over FaaSFlow, FaaSFlowRedis and KNIX,
respectively. DFlow effectively reduces the cold startup latency, achieving an
average improvement of 5.6x over CFlow and 1.1x over FaaSFlow
</summary>
    <author>
      <name>Xiaoxiang Shi</name>
    </author>
    <author>
      <name>Chao Li</name>
    </author>
    <author>
      <name>Zijun Li</name>
    </author>
    <author>
      <name>Zihan Liu</name>
    </author>
    <author>
      <name>Dianmo Sheng</name>
    </author>
    <author>
      <name>Quan Chen</name>
    </author>
    <author>
      <name>Jingwen Leng</name>
    </author>
    <author>
      <name>Minyi Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.11043v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.11043v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.02896v2</id>
    <updated>2023-11-11T16:22:45Z</updated>
    <published>2023-08-05T15:09:12Z</published>
    <title>Towards Fast, Adaptive, and Hardware-Assisted User-Space Scheduling</title>
    <summary>  Modern datacenter applications are prone to high tail latencies since their
requests typically follow highly-dispersive distributions. Delivering fast
interrupts is essential to reducing tail latency. Prior work has proposed both
OS- and system-level solutions to reduce tail latencies for microsecond-scale
workloads through better scheduling. Unfortunately, existing approaches like
customized dataplane OSes, require significant OS changes, experience
scalability limitations, or do not reach the full performance capabilities
hardware offers.
  The emergence of new hardware features like UINTR exposed new opportunities
to rethink the design paradigms and abstractions of traditional scheduling
systems. We propose LibPreemptible, a preemptive user-level threading library
that is flexible, lightweight, and adaptive. LibPreemptible was built with a
set of optimizations like LibUtimer for scalability, and deadline-oriented API
for flexible policies, time-quantum controller for adaptiveness. Compared to
the prior state-of-the-art scheduling system Shinjuku, our system achieves
significant tail latency and throughput improvements for various workloads
without modifying the kernel. We also demonstrate the flexibility of
LibPreemptible across scheduling policies for real applications experiencing
varying load levels and characteristics.
</summary>
    <author>
      <name> Lisa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yueying</arxiv:affiliation>
    </author>
    <author>
      <name> Li</name>
    </author>
    <author>
      <name>Nikita Lazarev</name>
    </author>
    <author>
      <name>David Koufaty</name>
    </author>
    <author>
      <name>Yijun Yin</name>
    </author>
    <author>
      <name>Andy Anderson</name>
    </author>
    <author>
      <name>Zhiru Zhang</name>
    </author>
    <author>
      <name>Edward Suh</name>
    </author>
    <author>
      <name>Kostis Kaffes</name>
    </author>
    <author>
      <name>Christina Delimitrou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by HPCA2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.02896v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.02896v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.02206v1</id>
    <updated>2023-09-05T13:11:40Z</updated>
    <published>2023-09-05T13:11:40Z</published>
    <title>Language Models for Novelty Detection in System Call Traces</title>
    <summary>  Due to the complexity of modern computer systems, novel and unexpected
behaviors frequently occur. Such deviations are either normal occurrences, such
as software updates and new user activities, or abnormalities, such as
misconfigurations, latency issues, intrusions, and software bugs. Regardless,
novel behaviors are of great interest to developers, and there is a genuine
need for efficient and effective methods to detect them. Nowadays, researchers
consider system calls to be the most fine-grained and accurate source of
information to investigate the behavior of computer systems. Accordingly, this
paper introduces a novelty detection methodology that relies on a probability
distribution over sequences of system calls, which can be seen as a language
model. Language models estimate the likelihood of sequences, and since
novelties deviate from previously observed behaviors by definition, they would
be unlikely under the model. Following the success of neural networks for
language models, three architectures are evaluated in this work: the widespread
LSTM, the state-of-the-art Transformer, and the lower-complexity Longformer.
However, large neural networks typically require an enormous amount of data to
be trained effectively, and to the best of our knowledge, no massive modern
datasets of kernel traces are publicly available. This paper addresses this
limitation by introducing a new open-source dataset of kernel traces comprising
over 2 million web requests with seven distinct behaviors. The proposed
methodology requires minimal expert hand-crafting and achieves an F-score and
AuROC greater than 95% on most novelties while being data- and task-agnostic.
The source code and trained models are publicly available on GitHub while the
datasets are available on Zenodo.
</summary>
    <author>
      <name>Quentin Fournier</name>
    </author>
    <author>
      <name>Daniel Aloise</name>
    </author>
    <author>
      <name>Leandro R. Costa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.02206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.02206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.03779v1</id>
    <updated>2023-09-07T15:28:03Z</updated>
    <published>2023-09-07T15:28:03Z</published>
    <title>CPU frequency scheduling of real-time applications on embedded devices
  with temporal encoding-based deep reinforcement learning</title>
    <summary>  Small devices are frequently used in IoT and smart-city applications to
perform periodic dedicated tasks with soft deadlines. This work focuses on
developing methods to derive efficient power-management methods for periodic
tasks on small devices. We first study the limitations of the existing Linux
built-in methods used in small devices. We illustrate three typical
workload/system patterns that are challenging to manage with Linux's built-in
solutions. We develop a reinforcement-learning-based technique with temporal
encoding to derive an effective DVFS governor even with the presence of the
three system patterns. The derived governor uses only one performance counter,
the same as the built-in Linux mechanism, and does not require an explicit task
model for the workload. We implemented a prototype system on the Nvidia Jetson
Nano Board and experimented with it with six applications, including two
self-designed and four benchmark applications. Under different deadline
constraints, our approach can quickly derive a DVFS governor that can adapt to
performance requirements and outperform the built-in Linux approach in energy
saving. On Mibench workloads, with performance slack ranging from 0.04 s to 0.4
s, the proposed method can save 3% - 11% more energy compared to Ondemand.
AudioReg and FaceReg applications tested have 5%- 14% energy-saving
improvement. We have open-sourced the implementation of our in-kernel quantized
neural network engine. The codebase can be found at:
https://github.com/coladog/tinyagent.
</summary>
    <author>
      <name>Ti Zhou</name>
    </author>
    <author>
      <name>Man Lin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.sysarc.2023.102955</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.sysarc.2023.102955" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Journal of Systems Architecture</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Systems Architecture, 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2309.03779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.03779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.05537v1</id>
    <updated>2023-09-11T15:19:57Z</updated>
    <published>2023-09-11T15:19:57Z</published>
    <title>D2WFP: A Novel Protocol for Forensically Identifying, Extracting, and
  Analysing Deep and Dark Web Browsing Activities</title>
    <summary>  The use of the un-indexed web, commonly known as the deep web and dark web,
to commit or facilitate criminal activity has drastically increased over the
past decade. The dark web is an in-famously dangerous place where all kinds of
criminal activities take place [1-2], despite advances in web forensics
techniques, tools, and methodologies, few studies have formally tackled the
dark and deep web forensics and the technical differences in terms of
investigative techniques and artefacts identification and extraction. This
research proposes a novel and comprehensive protocol to guide and assist
digital forensics professionals in investigating crimes committed on or via the
deep and dark web, The protocol named D2WFP establishes a new sequential
approach for performing investigative activities by observing the order of
volatility and implementing a systemic approach covering all browsing related
hives and artefacts which ultimately resulted into improv-ing the accuracy and
effectiveness. Rigorous quantitative and qualitative research has been
conducted by assessing D2WFP following a scientifically-sound and comprehensive
process in different scenarios and the obtained results show an apparent
increase in the number of artefacts re-covered when adopting D2WFP which
outperform any current industry or opensource browsing forensics tools. The
second contribution of D2WFP is the robust formulation of artefact correlation
and cross-validation within D2WFP which enables digital forensics professionals
to better document and structure their analysis of host-based deep and dark web
browsing artefacts.
</summary>
    <author>
      <name>Mohamed Chahine Ghanem</name>
    </author>
    <author>
      <name>Patrick Mulvihill</name>
    </author>
    <author>
      <name>Karim Ouazzane</name>
    </author>
    <author>
      <name>Ramzi Djemai</name>
    </author>
    <author>
      <name>Dipo Dunsin</name>
    </author>
    <link href="http://arxiv.org/abs/2309.05537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.05537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.07657v2</id>
    <updated>2024-06-19T15:37:58Z</updated>
    <published>2023-09-14T12:22:29Z</published>
    <title>Sync+Sync: A Covert Channel Built on fsync with Storage</title>
    <summary>  Scientists have built a variety of covert channels for secretive information
transmission with CPU cache and main memory. In this paper, we turn to a lower
level in the memory hierarchy, i.e., persistent storage. Most programs store
intermediate or eventual results in the form of files and some of them call
fsync to synchronously persist a file with storage device for orderly
persistence. Our quantitative study shows that one program would undergo
significantly longer response time for fsync call if the other program is
concurrently calling fsync, although they do not share any data. We further
find that, concurrent fsync calls contend at multiple levels of storage stack
due to sharing software structures (e.g., Ext4's journal) and hardware
resources (e.g., disk's I/O dispatch queue).
  We accordingly build a covert channel named Sync+Sync. Sync+Sync delivers a
transmission bandwidth of 20,000 bits per second at an error rate of about
0.40% with an ordinary solid-state drive. Sync+Sync can be conducted in
cross-disk partition, cross-file system, cross-container, cross-virtual
machine, and even cross-disk drive fashions, without sharing data between
programs. Next, we launch side-channel attacks with Sync+Sync and manage to
precisely detect operations of a victim database (e.g., insert/update and
B-Tree node split). We also leverage Sync+Sync to distinguish applications and
websites with high accuracy by detecting and analyzing their fsync frequencies
and flushed data volumes. These attacks are useful to support further
fine-grained information leakage.
</summary>
    <author>
      <name>Qisheng Jiang</name>
    </author>
    <author>
      <name>Chundong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A full version for the paper with the same title accepted by the 33rd
  USENIX Security Symposium (USENIX Security 2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.07657v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.07657v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.11332v2</id>
    <updated>2023-09-21T08:14:29Z</updated>
    <published>2023-09-20T14:07:20Z</published>
    <title>Software Compartmentalization Trade-Offs with Hardware Capabilities</title>
    <summary>  Compartmentalization is a form of defensive software design in which an
application is broken down into isolated but communicating components.
Retrofitting compartmentalization into existing applications is often thought
to be expensive from the engineering effort and performance overhead points of
view. Still, recent years have seen proposals of compartmentalization methods
with promises of low engineering efforts and reduced performance impact. ARM
Morello combines a modern ARM processor with an implementation of Capability
Hardware Enhanced RISC Instructions (CHERI) aiming to provide efficient and
secure compartmentalization. Past works exploring CHERI-based
compartmentalization were restricted to emulated/FPGA prototypes.
  In this paper, we explore possible compartmentalization schemes with CHERI on
the Morello chip. We propose two approaches representing different trade-offs
in terms of engineering effort, security, scalability, and performance impact.
We describe and implement these approaches on a prototype OS running bare metal
on the Morello chip, compartmentalize two popular applications, and investigate
the performance overheads. Furthermore, we show that compartmentalization can
be achieved with an engineering cost that can be quite low if one is willing to
trade off on scalability and security, and that performance overheads are
similar to other intra-address space isolation mechanisms.
</summary>
    <author>
      <name>John Alistair Kressel</name>
    </author>
    <author>
      <name>Hugo Lefeuvre</name>
    </author>
    <author>
      <name>Pierre Olivier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12th Workshop on Programming Languages and Operating Systems (PLOS
  2023)</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.11332v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.11332v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.14016v4</id>
    <updated>2025-04-24T18:25:43Z</updated>
    <published>2023-09-25T10:29:06Z</published>
    <title>Virtuoso: High Resource Utilization and μs-scale Performance
  Isolation in a Shared Virtual Machine TCP Network Stack</title>
    <summary>  Virtualization improves resource efficiency and ensures security and
performance isolation for cloud applications. Today, operators use a layered
architecture with separate network stack instances in each VM and container
connected to a virtual switch. Decoupling through layering reduces complexity,
but induces performance and resource overheads at odds with increasing demands
for network bandwidth, connection scalability, and low latency.
  We present Virtuoso, a new software network stack for VMs and containers.
Virtuoso re-organizes the network stack to maximize CPU utilization, enforce
isolation, and minimize processing overheads. We maximize utilization by
running one elastically shared network stack instance on dedicated cores; we
enforce isolation by performing central and fine-grained per-packet resource
accounting and scheduling; we reduce overheads by building a single-layer data
path with a one-shot fast-path incorporating all processing from the TCP
transport layer through network virtualization and virtual switching. Virtuoso
improves resource efficiency by up to 82%, latencies by up to 58% compared to
other virtualized network stacks without sacrificing isolation, and keeps
processing overhead within 6.7% of unvirtualized stacks.
</summary>
    <author>
      <name>Matheus Stolet</name>
    </author>
    <author>
      <name>Liam Arzola</name>
    </author>
    <author>
      <name>Simon Peter</name>
    </author>
    <author>
      <name>Antoine Kaufmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under submission for conference peer review</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.14016v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.14016v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.14477v1</id>
    <updated>2023-09-25T19:22:25Z</updated>
    <published>2023-09-25T19:22:25Z</published>
    <title>Carbon Containers: A System-level Facility for Managing
  Application-level Carbon Emissions</title>
    <summary>  To reduce their environmental impact, cloud datacenters' are increasingly
focused on optimizing applications' carbon-efficiency, or work done per mass of
carbon emitted. To facilitate such optimizations, we present Carbon Containers,
a simple system-level facility, which extends prior work on power containers,
that automatically regulates applications' carbon emissions in response to
variations in both their workload's intensity and their energy's
carbon-intensity. Specifically, \carbonContainerS enable applications to
specify a maximum carbon emissions rate (in g$\cdot$CO$_2$e/hr), and then
transparently enforce this rate via a combination of vertical scaling,
container migration, and suspend/resume while maximizing either
energy-efficiency or performance.
  Carbon Containers are especially useful for applications that i) must
continue running even during high-carbon periods, and ii) execute in regions
with few variations in carbon-intensity. These low-variability regions also
tend to have high average carbon-intensity, which increases the importance of
regulating carbon emissions. We implement a Carbon Containers prototype by
extending Linux Containers to incorporate the mechanisms above and evaluate it
using real workload traces and carbon-intensity data from multiple regions. We
compare Carbon Containers with prior work that regulates carbon emissions by
suspending/resuming applications during high/low carbon periods. We show that
Carbon Containers are more carbon-efficient and improve performance while
maintaining similar carbon emissions.
</summary>
    <author>
      <name>John Thiede</name>
    </author>
    <author>
      <name>Noman Bashir</name>
    </author>
    <author>
      <name>David Irwin</name>
    </author>
    <author>
      <name>Prashant Shenoy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3620678.3624644</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3620678.3624644" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Symposium on Cloud Computing (SoCC)</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.14477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.14477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.14821v2</id>
    <updated>2025-01-08T10:39:19Z</updated>
    <published>2023-09-26T10:39:59Z</published>
    <title>Shattering the Ephemeral Storage Cost Barrier for Data-Intensive
  Serverless Workflows</title>
    <summary>  Serverless computing is a popular cloud deployment paradigm where developers
implement applications as workflows of functions that invoke each other. Cloud
providers automatically scale function instances on demand and forward workflow
requests to appropriate instances. However, current serverless clouds lack
efficient cross-function data transfer, limiting the execution of
data-intensive applications. Functions often rely on third-party services like
AWS S3, AWS ElastiCache, or multi-tier solutions for intermediate data
transfers, which introduces inefficiencies.
  We demonstrate that such through-storage transfers make data-intensive
deployments economically impractical, with storage costs comprising more than
24-99% of the total serverless bill. To address this, we introduce Zipline, a
fast, API-preserving data communication method for serverless platforms.
Zipline enables direct function-to-function transfers, where the sender
function buffers payloads in memory and sends a reference to the receiver. The
receiver retrieves the data directly from the sender's memory, guided by the
load balancer and autoscaler. Zipline integrates seamlessly with existing
autoscaling, maintains invocation semantics, and eliminates the costs and
overheads of intermediate services. We prototype Zipline in vHive/Knative on
AWS EC2 nodes, demonstrating significant improvements. Zipline reduces costs
and enhances latency and bandwidth compared to AWS S3 (the lowest-cost
solution) and ElastiCache (the highest-performance solution). On real-world
applications, Zipline lowers costs by 2-5x and reduces execution times by
1.3-3.4x versus S3. Compared to ElastiCache, Zipline achieves 17-772x cost
reductions while improving performance by 2-5%.
</summary>
    <author>
      <name>Dmitrii Ustiugov</name>
    </author>
    <author>
      <name>Shyam Jesalpura</name>
    </author>
    <author>
      <name>Mert Bora Alper</name>
    </author>
    <author>
      <name>Michal Baczun</name>
    </author>
    <author>
      <name>Rustem Feyzkhanov</name>
    </author>
    <author>
      <name>Edouard Bugnion</name>
    </author>
    <author>
      <name>Boris Grot</name>
    </author>
    <author>
      <name>Marios Kogias</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">added cost reduction details</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.14821v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.14821v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.09690v2</id>
    <updated>2024-04-02T06:02:07Z</updated>
    <published>2023-10-15T00:50:27Z</published>
    <title>Configuration Validation with Large Language Models</title>
    <summary>  Misconfigurations are major causes of software failures. Existing practices
rely on developer-written rules or test cases to validate configurations, which
are expensive. Machine learning (ML) for configuration validation is considered
a promising direction, but has been facing challenges such as the need of
large-scale field data and system-specific models. Recent advances in Large
Language Models (LLMs) show promise in addressing some of the long-lasting
limitations of ML-based configuration validation. We present a first analysis
on the feasibility and effectiveness of using LLMs for configuration
validation. We empirically evaluate LLMs as configuration validators by
developing a generic LLM-based configuration validation framework, named Ciri.
Ciri employs effective prompt engineering with few-shot learning based on both
valid configuration and misconfiguration data. Ciri checks outputs from LLMs
when producing results, addressing hallucination and nondeterminism of LLMs. We
evaluate Ciri's validation effectiveness on eight popular LLMs using
configuration data of ten widely deployed open-source systems. Our analysis (1)
confirms the potential of using LLMs for configuration validation, (2) explores
design space of LLMbased validators like Ciri, and (3) reveals open challenges
such as ineffectiveness in detecting certain types of misconfigurations and
biases towards popular configuration parameters.
</summary>
    <author>
      <name>Xinyu Lian</name>
    </author>
    <author>
      <name>Yinfang Chen</name>
    </author>
    <author>
      <name>Runxiang Cheng</name>
    </author>
    <author>
      <name>Jie Huang</name>
    </author>
    <author>
      <name>Parth Thakkar</name>
    </author>
    <author>
      <name>Minjia Zhang</name>
    </author>
    <author>
      <name>Tianyin Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2310.09690v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.09690v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.16300v1</id>
    <updated>2023-10-25T02:14:05Z</updated>
    <published>2023-10-25T02:14:05Z</published>
    <title>Snapshot: Fast, Userspace Crash Consistency for CXL and PM Using msync</title>
    <summary>  Crash consistency using persistent memory programming libraries requires
programmers to use complex transactions and manual annotations. In contrast,
the failure-atomic msync() (FAMS) interface is much simpler as it transparently
tracks updates and guarantees that modified data is atomically durable on a
call to the failure-atomic variant of msync(). However, FAMS suffers from
several drawbacks, like the overhead of msync() and the write amplification
from page-level dirty data tracking.
  To address these drawbacks while preserving the advantages of FAMS, we
propose Snapshot, an efficient userspace implementation of FAMS.
  Snapshot uses compiler-based annotation to transparently track updates in
userspace and syncs them with the backing byte-addressable storage copy on a
call to msync(). By keeping a copy of application data in DRAM, Snapshot
improves access latency. Moreover, with automatic tracking and syncing changes
only on a call to msync(), Snapshot provides crash-consistency guarantees,
unlike the POSIX msync() system call.
  For a KV-Store backed by Intel Optane running the YCSB benchmark, Snapshot
achieves at least 1.2$\times$ speedup over PMDK while significantly
outperforming conventional (non-crash-consistent) msync(). On an emulated CXL
memory semantic SSD, Snapshot outperforms PMDK by up to 10.9$\times$ on all but
one YCSB workload, where PMDK is 1.2$\times$ faster than Snapshot. Further,
Kyoto Cabinet commits perform up to 8.0$\times$ faster with Snapshot than its
built-in, msync()-based crash-consistency mechanism.
</summary>
    <author>
      <name>Suyash Mahar</name>
    </author>
    <author>
      <name>Mingyao Shen</name>
    </author>
    <author>
      <name>Terence Kelly</name>
    </author>
    <author>
      <name>Steven Swanson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A shorter version of this paper appeared in the Proceedings of ICCD
  2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.16300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.16300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.04789v1</id>
    <updated>2023-12-08T02:03:55Z</updated>
    <published>2023-12-08T02:03:55Z</published>
    <title>Lightweight Frequency-Based Tiering for CXL Memory Systems</title>
    <summary>  Modern workloads are demanding increasingly larger memory capacity. Compute
Express Link (CXL)-based memory tiering has emerged as a promising solution for
addressing this trend by utilizing traditional DRAM alongside slow-tier
CXL-memory devices in the same system. Unfortunately, most prior tiering
systems are recency-based, which cannot accurately identify hot and cold pages,
since a recently accessed page is not necessarily a hot page. On the other
hand, more accurate frequency-based systems suffer from high memory and runtime
overhead as a result of tracking large memories.
  In this paper, we propose FreqTier, a fast and accurate frequency-based
tiering system for CXL memory. We observe that memory tiering systems can
tolerate a small amount of tracking inaccuracy without compromising the overall
application performance. Based on this observation, FreqTier probabilistically
tracks the access frequency of each page, enabling accurate identification of
hot and cold pages while maintaining minimal memory overhead. Finally, FreqTier
intelligently adjusts the intensity of tiering operations based on the
application's memory access behavior, thereby significantly reducing the amount
of migration traffic and application interference.
  We evaluate FreqTier on two emulated CXL memory devices with different
bandwidths. On the high bandwidth CXL device, FreqTier can outperform
state-of-the-art tiering systems while using 4$\times$ less local DRAM memory
for in-memory caching workloads. On GAP graph analytics and XGBoost workloads
with 1:32 local DRAM to CXL-memory ratio, FreqTier outperforms prior works by
1.04$-$2.04$\times$ (1.39$\times$ on average). Even on the low bandwidth CXL
device, FreqTier outperforms AutoNUMA by 1.14$\times$ on average.
</summary>
    <author>
      <name>Kevin Song</name>
    </author>
    <author>
      <name>Jiacheng Yang</name>
    </author>
    <author>
      <name>Sihang Liu</name>
    </author>
    <author>
      <name>Gennady Pekhimenko</name>
    </author>
    <link href="http://arxiv.org/abs/2312.04789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.04789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.12456v2</id>
    <updated>2024-12-12T12:38:12Z</updated>
    <published>2023-12-16T02:27:00Z</published>
    <title>PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU</title>
    <summary>  This paper introduces PowerInfer, a high-speed Large Language Model (LLM)
inference engine on a personal computer (PC) equipped with a single
consumer-grade GPU. The key principle underlying the design of PowerInfer is
exploiting the high locality inherent in LLM inference, characterized by a
power-law distribution in neuron activation. This distribution indicates that a
small subset of neurons, termed hot neurons, are consistently activated across
inputs, while the majority, cold neurons, vary based on specific inputs.
PowerInfer exploits such an insight to design a GPU-CPU hybrid inference
engine: hot-activated neurons are preloaded onto the GPU for fast access, while
cold-activated neurons are computed on the CPU, thus significantly reducing GPU
memory demands and CPU-GPU data transfers. PowerInfer further integrates
adaptive predictors and neuron-aware sparse operators, optimizing the
efficiency of neuron activation and computational sparsity. The evaluation
shows that PowerInfer significantly outperforms llama.cpp by up to 11.69x while
retaining model accuracy across various LLMs (including OPT-175B) on a single
NVIDIA RTX 4090 GPU. For the OPT-30B model, PowerInfer achieves performance
comparable to that of a high-end server-grade A100 GPU, reaching 82% of its
token generation rate on a single consumer-grade RTX 4090 GPU.
</summary>
    <author>
      <name>Yixin Song</name>
    </author>
    <author>
      <name>Zeyu Mi</name>
    </author>
    <author>
      <name>Haotong Xie</name>
    </author>
    <author>
      <name>Haibo Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SOSP 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.12456v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.12456v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.01826v1</id>
    <updated>2024-01-03T16:43:09Z</updated>
    <published>2024-01-03T16:43:09Z</published>
    <title>Data-Driven Power Modeling and Monitoring via Hardware Performance
  Counters Tracking</title>
    <summary>  In the current high-performance and embedded computing era, full-stack
energy-centric design is paramount. Use cases require increasingly high
performance at an affordable power budget, often under real-time constraints.
Extreme heterogeneity and parallelism address these issues but greatly
complicate online power consumption assessment, which is essential for dynamic
hardware and software stack adaptations. We introduce a novel
architecture-agnostic power modeling methodology with state-of-the-art
accuracy, low overhead, and high responsiveness. Our methodology identifies the
best Performance Monitoring Counters (PMCs) to model the power consumption of
each hardware sub-system at each Dynamic Voltage and Frequency Scaling (DVFS)
state. The individual linear models are combined into a complete model that
effectively describes the power consumption of the whole system, achieving high
accuracy and low overhead. Our evaluation reports an average estimation error
of 7.5 % for power consumption and 1.3 % for energy. Furthermore, we propose
Runmeter, an open-source, PMC-based monitoring framework integrated into the
Linux kernel. Runmeter manages PMC samples collection and manipulation,
efficiently evaluating our power models at runtime. With a time overhead of
only 0.7 % in the worst case, Runmeter provides responsive and accurate power
measurements directly in the kernel, which can be employed for actuation
policies such as Dynamic Power Management (DPM) and power-aware task
scheduling.
</summary>
    <author>
      <name>Sergio Mazzola</name>
    </author>
    <author>
      <name>Gabriele Ara</name>
    </author>
    <author>
      <name>Thomas Benz</name>
    </author>
    <author>
      <name>Björn Forsberg</name>
    </author>
    <author>
      <name>Tommaso Cucinotta</name>
    </author>
    <author>
      <name>Luca Benini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures, submitted to the IEEE for possible publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.01826v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.01826v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.06362v3</id>
    <updated>2024-02-22T04:15:45Z</updated>
    <published>2023-12-23T05:46:05Z</published>
    <title>Attention, Distillation, and Tabularization: Towards Practical Neural
  Network-Based Prefetching</title>
    <summary>  Attention-based Neural Networks (NN) have demonstrated their effectiveness in
accurate memory access prediction, an essential step in data prefetching.
However, the substantial computational overheads associated with these models
result in high inference latency, limiting their feasibility as practical
prefetchers. To close the gap, we propose a new approach based on
tabularization that significantly reduces model complexity and inference
latency without sacrificing prediction accuracy. Our novel tabularization
methodology takes as input a distilled, yet highly accurate attention-based
model for memory access prediction and efficiently converts its expensive
matrix multiplications into a hierarchy of fast table lookups. As an exemplar
of the above approach, we develop DART, a prefetcher comprised of a simple
hierarchy of tables. With a modest 0.09 drop in F1-score, DART reduces 99.99%
of arithmetic operations from the large attention-based model and 91.83% from
the distilled model. DART accelerates the large model inference by 170x and the
distilled model by 9.4x. DART has comparable latency and storage costs as
state-of-the-art rule-based prefetcher BO but surpasses it by 6.1% in IPC
improvement. DART outperforms state-of-the-art NN-based prefetchers TransFetch
by 33.1% and Voyager by 37.2% in terms of IPC improvement, primarily due to its
low prefetching latency.
</summary>
    <author>
      <name>Pengmiao Zhang</name>
    </author>
    <author>
      <name>Neelesh Gupta</name>
    </author>
    <author>
      <name>Rajgopal Kannan</name>
    </author>
    <author>
      <name>Viktor K. Prasanna</name>
    </author>
    <link href="http://arxiv.org/abs/2401.06362v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.06362v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.17618v2</id>
    <updated>2024-09-07T22:49:29Z</updated>
    <published>2024-01-31T06:16:00Z</published>
    <title>Beyond Control: Exploring Novel File System Objects for Data-Only
  Attacks on Linux Systems</title>
    <summary>  The widespread deployment of control-flow integrity has propelled non-control
data attacks into the mainstream. In the domain of OS kernel exploits, by
corrupting critical non-control data, local attackers can directly gain root
access or privilege escalation without hijacking the control flow. As a result,
OS kernels have been restricting the availability of such non-control data.
This forces attackers to continue to search for more exploitable non-control
data in OS kernels. However, discovering unknown non-control data can be
daunting because they are often tied heavily to semantics and lack universal
patterns.
  We make two contributions in this paper: (1) discover critical non-control
objects in the file subsystem and (2) analyze their exploitability. This work
represents the first study, with minimal domain knowledge, to
semi-automatically discover and evaluate exploitable non-control data within
the file subsystem of the Linux kernel. Our solution utilizes a custom analysis
and testing framework that statically and dynamically identifies promising
candidate objects. Furthermore, we categorize these discovered objects into
types that are suitable for various exploit strategies, including a novel
strategy necessary to overcome the defense that isolates many of these objects.
These objects have the advantage of being exploitable without requiring KASLR,
thus making the exploits simpler and more reliable. We use 18 real-world CVEs
to evaluate the exploitability of the file system objects using various exploit
strategies. We develop 10 end-to-end exploits using a subset of CVEs against
the kernel with all state-of-the-art mitigations enabled.
</summary>
    <author>
      <name>Jinmeng Zhou</name>
    </author>
    <author>
      <name>Jiayi Hu</name>
    </author>
    <author>
      <name>Ziyue Pan</name>
    </author>
    <author>
      <name>Jiaxun Zhu</name>
    </author>
    <author>
      <name>Wenbo Shen</name>
    </author>
    <author>
      <name>Guoren Li</name>
    </author>
    <author>
      <name>Zhiyun Qian</name>
    </author>
    <link href="http://arxiv.org/abs/2401.17618v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.17618v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.04079v1</id>
    <updated>2024-02-06T15:34:00Z</updated>
    <published>2024-02-06T15:34:00Z</published>
    <title>Design and implementation of a real-time onboard system for a
  stratospheric balloon mission using commercial off-the-self components and a
  model-based approach</title>
    <summary>  Stratospheric balloons have emerged as an affordable and flexible alternative
to traditional spacecrafts as they are implemented using commercial
off-the-shelf (COTS) equipment without following strict methodologies.
HERCCULES is a stratospheric balloon mission that aims to characterize the
convective heat and radiative environment in the stratosphere. The purpose of
this article is to present the HERCCULES onboard software (OBSW) whose design
and complexity is comparable to that of satellite systems, since it must
control about sixty COTS equipment using a single Raspberry Pi 4B as onboard
computer and ensure the real-time requirements. Compared to similar systems,
novel contributions are presented as the OBSW is developed following modelbased
and component-based approaches using the TASTE toolchain from the European
Space Agency (ESA) for automatic code generation. Besides, the OBSW is verified
and validated following the ESA standards and the results obtained demonstrate
the suitability and efficiency of the solution and the selected methodologies.
</summary>
    <author>
      <name>Angel-Grover Perez-Munoz</name>
    </author>
    <author>
      <name>Jose-Carlos Gamazo-Real</name>
    </author>
    <author>
      <name>David Gonzalez-Barcena</name>
    </author>
    <author>
      <name>Juan Zamorano</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.compeleceng.2023.108953</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.compeleceng.2023.108953" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers and Electrical Engineering, 2023, vol. 111, Part B, no.
  108953, pp. 1-22, ISSN 0045-7906</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2402.04079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.04079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.04466v1</id>
    <updated>2024-02-06T23:20:34Z</updated>
    <published>2024-02-06T23:20:34Z</published>
    <title>Towards Deterministic End-to-end Latency for Medical AI Systems in
  NVIDIA Holoscan</title>
    <summary>  The introduction of AI and ML technologies into medical devices has
revolutionized healthcare diagnostics and treatments. Medical device
manufacturers are keen to maximize the advantages afforded by AI and ML by
consolidating multiple applications onto a single platform. However, concurrent
execution of several AI applications, each with its own visualization
components, leads to unpredictable end-to-end latency, primarily due to GPU
resource contentions. To mitigate this, manufacturers typically deploy separate
workstations for distinct AI applications, thereby increasing financial,
energy, and maintenance costs. This paper addresses these challenges within the
context of NVIDIA's Holoscan platform, a real-time AI system for streaming
sensor data and images. We propose a system design optimized for heterogeneous
GPU workloads, encompassing both compute and graphics tasks. Our design
leverages CUDA MPS for spatial partitioning of compute workloads and isolates
compute and graphics processing onto separate GPUs. We demonstrate significant
performance improvements across various end-to-end latency determinism metrics
through empirical evaluation with real-world Holoscan medical device
applications. For instance, the proposed design reduces maximum latency by
21-30% and improves latency distribution flatness by 17-25% for up to five
concurrent endoscopy tool tracking AI applications, compared to a single-GPU
baseline. Against a default multi-GPU setup, our optimizations decrease maximum
latency by 35% for up to six concurrent applications by improving GPU
utilization by 42%. This paper provides clear design insights for AI
applications in the edge-computing domain including medical systems, where
performance predictability of concurrent and heterogeneous GPU workloads is a
critical requirement.
</summary>
    <author>
      <name>Soham Sinha</name>
    </author>
    <author>
      <name>Shekhar Dwivedi</name>
    </author>
    <author>
      <name>Mahdi Azizian</name>
    </author>
    <link href="http://arxiv.org/abs/2402.04466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.04466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3; J.7; D.2.11; D.2.10; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.07664v1</id>
    <updated>2024-02-12T14:16:37Z</updated>
    <published>2024-02-12T14:16:37Z</published>
    <title>Enabling performance portability of data-parallel OpenMP applications on
  asymmetric multicore processors</title>
    <summary>  Asymmetric multicore processors (AMPs) couple high-performance big cores and
low-power small cores with the same instruction-set architecture but different
features, such as clock frequency or microarchitecture. Previous work has shown
that asymmetric designs may deliver higher energy efficiency than symmetric
multicores for diverse workloads. Despite their benefits, AMPs pose significant
challenges to runtime systems of parallel programming models. While previous
work has mainly explored how to efficiently execute task-based parallel
applications on AMPs, via enhancements in the runtime system, improving the
performance of unmodified data-parallel applications on these architectures is
still a big challenge. In this work we analyze the particular case of
loop-based OpenMP applications, which are widely used today in scientific and
engineering domains, and constitute the dominant application type in many
parallel benchmark suites used for performance evaluation on multicore systems.
We observed that conventional loop-scheduling OpenMP approaches are unable to
efficiently cope with the load imbalance that naturally stems from the
different performance delivered by big and small cores.
  To address this shortcoming, we propose \textit{Asymmetric Iteration
Distribution} (AID), a set of novel loop-scheduling methods for AMPs that
distribute iterations unevenly across worker threads to efficiently deal with
performance asymmetry. We implemented AID in \textit{libgomp} --the GNU OpenMP
runtime system--, and evaluated it on two different asymmetric multicore
platforms. Our analysis reveals that the AID methods constitute effective
replacements of the \texttt{static} and \texttt{dynamic} methods on AMPs, and
are capable of improving performance over these conventional strategies by up
to 56\% and 16.8\%, respectively.
</summary>
    <author>
      <name>Juan Carlos Saez</name>
    </author>
    <author>
      <name>Fernando Castro</name>
    </author>
    <author>
      <name>Manuel Prieto-Matias</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3404397.3404441</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3404397.3404441" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 49th International Conference on Parallel
  Processing (ICPP 2020). ACM, Article 51, 1-11</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2402.07664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.07664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.14105v2</id>
    <updated>2024-02-26T18:42:34Z</updated>
    <published>2024-02-21T19:57:08Z</published>
    <title>Formal Definitions and Performance Comparison of Consistency Models for
  Parallel File Systems</title>
    <summary>  The semantics of HPC storage systems are defined by the consistency models to
which they abide. Storage consistency models have been less studied than their
counterparts in memory systems, with the exception of the POSIX standard and
its strict consistency model. The use of POSIX consistency imposes a
performance penalty that becomes more significant as the scale of parallel file
systems increases and the access time to storage devices, such as node-local
solid storage devices, decreases. While some efforts have been made to adopt
relaxed storage consistency models, these models are often defined informally
and ambiguously as by-products of a particular implementation. In this work, we
establish a connection between memory consistency models and storage
consistency models and revisit the key design choices of storage consistency
models from a high-level perspective. Further, we propose a formal and unified
framework for defining storage consistency models and a layered implementation
that can be used to easily evaluate their relative performance for different
I/O workloads. Finally, we conduct a comprehensive performance comparison of
two relaxed consistency models on a range of commonly-seen parallel I/O
workloads, such as checkpoint/restart of scientific applications and random
reads of deep learning applications. We demonstrate that for certain I/O
scenarios, a weaker consistency model can significantly improve the I/O
performance. For instance, in small random reads that typically found in deep
learning applications, session consistency achieved an 5x improvement in I/O
bandwidth compared to commit consistency, even at small scales.
</summary>
    <author>
      <name>Chen Wang</name>
    </author>
    <author>
      <name>Kathryn Mohror</name>
    </author>
    <author>
      <name>Marc Snir</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPDS.2024.3391058</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPDS.2024.3391058" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages. Submitted to IEEE TPDS</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Parallel and Distributed Systems, 2024,
  Volume 35, Issue 6, Pages 937-951</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2402.14105v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.14105v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.06120v1</id>
    <updated>2024-03-10T07:35:31Z</updated>
    <published>2024-03-10T07:35:31Z</published>
    <title>I/O Transit Caching for PMem-based Block Device</title>
    <summary>  Byte-addressable non-volatile memory (NVM) sitting on the memory bus is
employed to make persistent memory (PMem) in general-purpose computing systems
and embedded systems for data storage. Researchers develop software drivers
such as the block translation table (BTT) to build block devices on PMem, so
programmers can keep using mature and reliable conventional storage stack while
expecting high performance by exploiting fast PMem. However, our quantitative
study shows that BTT underutilizes PMem and yields inferior performance, due to
the absence of the imperative in-device cache. We add a conventional I/O
staging cache made of DRAM space to BTT. As DRAM and PMem have comparable
access latency, I/O staging cache is likely to be fully filled over time.
Continual cache evictions and fsyncs thus cause on-demand flushes with severe
stalls, such that the I/O staging cache is concretely unappealing for
PMem-based block devices. We accordingly propose an algorithm named Caiti with
novel I/O transit caching. Caiti eagerly evicts buffered data to PMem through
CPU's multi-cores. It also conditionally bypasses a full cache and directly
writes data into PMem to further alleviate I/O stalls. Experiments confirm that
Caiti significantly boosts the performance with BTT by up to 3.6x, without loss
of block-level write atomicity.
</summary>
    <author>
      <name>Qing Xu</name>
    </author>
    <author>
      <name>Qisheng Jiang</name>
    </author>
    <author>
      <name>Chundong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by the Journal of Systems Architecture: Embedded Software
  Design (JSA)</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.06120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.06120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.00057v1</id>
    <updated>2024-03-26T20:10:31Z</updated>
    <published>2024-03-26T20:10:31Z</published>
    <title>PerOS: Personalized Self-Adapting Operating Systems in the Cloud</title>
    <summary>  Operating systems (OSes) are foundational to computer systems, managing
hardware resources and ensuring secure environments for diverse applications.
However, despite their enduring importance, the fundamental design objectives
of OSes have seen minimal evolution over decades. Traditionally prioritizing
aspects like speed, memory efficiency, security, and scalability, these
objectives often overlook the crucial aspect of intelligence as well as
personalized user experience. The lack of intelligence becomes increasingly
critical amid technological revolutions, such as the remarkable advancements in
machine learning (ML).
  Today's personal devices, evolving into intimate companions for users, pose
unique challenges for traditional OSes like Linux and iOS, especially with the
emergence of specialized hardware featuring heterogeneous components.
Furthermore, the rise of large language models (LLMs) in ML has introduced
transformative capabilities, reshaping user interactions and software
development paradigms.
  While existing literature predominantly focuses on leveraging ML methods for
system optimization or accelerating ML workloads, there is a significant gap in
addressing personalized user experiences at the OS level. To tackle this
challenge, this work proposes PerOS, a personalized OS ingrained with LLM
capabilities. PerOS aims to provide tailored user experiences while
safeguarding privacy and personal data through declarative interfaces,
self-adaptive kernels, and secure data management in a scalable cloud-centric
architecture; therein lies the main research question of this work: How can we
develop intelligent, secure, and scalable OSes that deliver personalized
experiences to thousands of users?
</summary>
    <author>
      <name>Hongyu Hè</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.00057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.00057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.16393v3</id>
    <updated>2024-10-28T20:43:00Z</updated>
    <published>2024-04-25T08:01:11Z</published>
    <title>Dirigent: Lightweight Serverless Orchestration</title>
    <summary>  While Function as a Service (FaaS) platforms can initialize function
sandboxes on worker nodes in 10-100s of milliseconds, the latency to schedule
functions in real FaaS clusters can be orders of magnitude higher. The current
approach of building FaaS cluster managers on top of legacy orchestration
systems (e.g., Kubernetes) leads to high scheduling delays when clusters
experience high sandbox churn, which is common for FaaS. Generic cluster
managers use many hierarchical abstractions and internal components to manage
and reconcile cluster state with frequent persistent updates. This becomes a
bottleneck for FaaS since the cluster state frequently changes as sandboxes are
created on the critical path of requests. Based on our root cause analysis of
performance issues in existing FaaS cluster managers, we propose Dirigent, a
clean-slate system architecture for FaaS orchestration with three key
principles. First, Dirigent optimizes internal cluster manager abstractions to
simplify state management. Second, it eliminates persistent state updates on
the critical path of function invocations, leveraging the fact that FaaS
abstracts sandbox locations from users to relax exact state reconstruction
guarantees. Finally, Dirigent runs monolithic control and data planes to
minimize internal communication overheads and maximize throughput. We compare
Dirigent to state-of-the-art FaaS platforms and show that Dirigent reduces 99th
percentile per-function scheduling latency for a production workload by 2.79x
compared to AWS Lambda. Dirigent can spin up 2500 sandboxes per second at low
latency, which is 1250x more than Knative.
</summary>
    <author>
      <name>Lazar Cvetković</name>
    </author>
    <author>
      <name>François Costa</name>
    </author>
    <author>
      <name>Mihajlo Djokic</name>
    </author>
    <author>
      <name>Michal Friedman</name>
    </author>
    <author>
      <name>Ana Klimovic</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3694715.3695966</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3694715.3695966" rel="related"/>
    <link href="http://arxiv.org/abs/2404.16393v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.16393v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.16856v3</id>
    <updated>2024-08-17T02:12:39Z</updated>
    <published>2024-04-04T11:44:08Z</published>
    <title>HookChain: A new perspective for Bypassing EDR Solutions</title>
    <summary>  In the current digital security ecosystem, where threats evolve rapidly and
with complexity, companies developing Endpoint Detection and Response (EDR)
solutions are in constant search for innovations that not only keep up but also
anticipate emerging attack vectors. In this context, this article introduces
the HookChain, a look from another perspective at widely known techniques,
which when combined, provide an additional layer of sophisticated evasion
against traditional EDR systems. Through a precise combination of IAT Hooking
techniques, dynamic SSN resolution, and indirect system calls, HookChain
redirects the execution flow of Windows subsystems in a way that remains
invisible to the vigilant eyes of EDRs that only act on Ntdll.dll, without
requiring changes to the source code of the applications and malwares involved.
This work not only challenges current conventions in cybersecurity but also
sheds light on a promising path for future protection strategies, leveraging
the understanding that continuous evolution is key to the effectiveness of
digital security. By developing and exploring the HookChain technique, this
study significantly contributes to the body of knowledge in endpoint security,
stimulating the development of more robust and adaptive solutions that can
effectively address the ever-changing dynamics of digital threats. This work
aspires to inspire deep reflection and advancement in the research and
development of security technologies that are always several steps ahead of
adversaries.
</summary>
    <author>
      <name>Helvio Carvalho Junior</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">50 pages, 23 figures, HookChain, Bypass EDR, Evading EDR, IAT Hook,
  Halo's Gate</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.16856v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.16856v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.00038v1</id>
    <updated>2024-03-26T19:03:50Z</updated>
    <published>2024-03-26T19:03:50Z</published>
    <title>Getting a Handle on Unmanaged Memory</title>
    <summary>  The inability to relocate objects in unmanaged languages brings with it a
menagerie of problems. Perhaps the most impactful is memory fragmentation,
which has long plagued applications such as databases and web servers. These
issues either fester or require Herculean programmer effort to address on a
per-application basis because, in general, heap objects cannot be moved in
unmanaged languages. In contrast, managed languages like C# cleanly address
fragmentation through the use of compacting garbage collection techniques built
upon heap object movement. In this work, we bridge this gap between unmanaged
and managed languages through the use of handles, a level of indirection
allowing heap object movement. Handles open the door to seamlessly employ
runtime features from managed languages in existing, unmodified code written in
unmanaged languages. We describe a new compiler and runtime system, ALASKA,
that acts as a drop-in replacement for malloc. Without any programmer effort,
the ALASKA compiler transforms pointer-based code to utilize handles, with
optimizations to reduce performance impact. A codesigned runtime system manages
this level of indirection and exploits heap object movement via an extensible
service interface. We investigate the overheads of ALASKA on large benchmarks
and applications spanning multiple domains. To show the power and extensibility
of handles, we use ALASKA to eliminate fragmentation on the heap through
compaction, reducing memory usage by up to 40% in Redis.
</summary>
    <author>
      <name>Nick Wanninger</name>
    </author>
    <author>
      <name>Tommy McMichen</name>
    </author>
    <author>
      <name>Simone Campanoni</name>
    </author>
    <author>
      <name>Peter Dinda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">0.1145/3620666.3651326</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/0.1145/3620666.3651326" rel="related"/>
    <link href="http://arxiv.org/abs/2405.00038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.00038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.00078v3</id>
    <updated>2025-01-08T17:43:32Z</updated>
    <published>2024-04-30T12:34:23Z</published>
    <title>VeriFence: Lightweight and Precise Spectre Defenses for Untrusted Linux
  Kernel Extensions</title>
    <summary>  High-performance IO demands low-overhead communication between user- and
kernel space. This demand can no longer be fulfilled by traditional system
calls. Linux's extended Berkeley Packet Filter (BPF) avoids user-/kernel
transitions by just-in-time compiling user-provided bytecode and executing it
in kernel mode with near-native speed. To still isolate BPF programs from the
kernel, they are statically analyzed for memory- and type-safety, which imposes
some restrictions but allows for good expressiveness and high performance.
However, to mitigate the Spectre vulnerabilities disclosed in 2018, defenses
which reject potentially-dangerous programs had to be deployed. We find that
this affects 31% to 54% of programs in a dataset with 844 real-world BPF
programs from popular open-source projects. To solve this, users are forced to
disable the defenses to continue using the programs, which puts the entire
system at risk.
  To enable secure and expressive untrusted Linux kernel extensions, we propose
VeriFence, an enhancement to the kernel's Spectre defenses that reduces the
number of BPF application programs rejected from 54% to zero. We measure
VeriFence's overhead for all mainstream performance-sensitive applications of
BPF (i.e., event tracing, profiling, and packet processing) and find that it
improves significantly upon the status-quo where affected BPF programs are
either unusable or enable transient execution attacks on the kernel.
</summary>
    <author>
      <name>Luis Gerhorst</name>
    </author>
    <author>
      <name>Henriette Herzog</name>
    </author>
    <author>
      <name>Peter Wägemann</name>
    </author>
    <author>
      <name>Maximilian Ott</name>
    </author>
    <author>
      <name>Rüdiger Kapitza</name>
    </author>
    <author>
      <name>Timo Hönig</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3678890.3678907</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3678890.3678907" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RAID'24</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.00078v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.00078v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M25" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.01263v2</id>
    <updated>2024-06-17T12:01:01Z</updated>
    <published>2024-05-02T13:11:53Z</published>
    <title>An Online Gradient-Based Caching Policy with Logarithmic Complexity and
  Regret Guarantees</title>
    <summary>  Commonly used caching policies, such as LRU (Least Recently Used) or LFU
(Least Frequently Used), exhibit optimal performance only under specific
traffic patterns. Even advanced machine learning-based methods, which detect
patterns in historical request data, struggle when future requests deviate from
past trends. Recently, a new class of policies has emerged that are robust to
varying traffic patterns. These algorithms address an online optimization
problem, enabling continuous adaptation to the context. They offer theoretical
guarantees on the regret metric, which measures the performance gap between the
online policy and the optimal static cache allocation in hindsight. However,
the high computational complexity of these solutions hinders their practical
adoption.
  In this study, we introduce a new variant of the gradient-based online
caching policy that achieves groundbreaking logarithmic computational
complexity relative to catalog size, while also providing regret guarantees.
This advancement allows us to test the policy on large-scale, real-world traces
featuring millions of requests and items - a significant achievement, as such
scales have been beyond the reach of existing policies with regret guarantees.
To the best of our knowledge, our experimental results demonstrate for the
first time that the regret guarantees of gradient-based caching policies offer
substantial benefits in practical scenarios.
</summary>
    <author>
      <name>Damiano Carra</name>
    </author>
    <author>
      <name>Giovanni Neglia</name>
    </author>
    <link href="http://arxiv.org/abs/2405.01263v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.01263v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.12079v1</id>
    <updated>2024-05-20T14:49:45Z</updated>
    <published>2024-05-20T14:49:45Z</published>
    <title>PARALLELGPUOS: A Concurrent OS-level GPU Checkpoint and Restore System
  using Validated Speculation</title>
    <summary>  Checkpointing (C) and restoring (R) are key components for GPU tasks. POS is
an OS-level GPU C/R system: It can transparently checkpoint or restore
processes that use the GPU, without requiring any cooperation from the
application, a key feature required by modern systems like the cloud. Moreover,
POS is the first OS-level C/R system that can concurrently execute C/R with the
application execution: a critical feature that can be trivially achieved when
the processes only running on the CPU, but becomes challenging when the
processes use GPU. The problem is how to ensure consistency during concurrent
execution with the lack of application semantics due to transparency. CPU
processes can leverage OS and hardware paging to fix inconsistency without
application semantics. Unfortunately, GPU bypasses OS and paging for high
performance. POS fills the semantic gap by speculatively extracting buffer
access information of GPU kernels during runtime. Thanks to the simple and
well-structured nature of GPU kernels, our speculative extraction (with runtime
validation) achieves 100% accuracy on applications from training to inference
whose domains span from vision, large language models, and reinforcement
learning. Based on the extracted semantics, we systematically overlap C/R with
application execution, and achieves orders of magnitude higher performance
under various tasks compared with the state-of-the-art OS-level GPU C/R,
including training fault tolerance, live GPU process migration, and cold starts
acceleration in GPU-based serverless computing.
</summary>
    <author>
      <name>Zhuobin Huang</name>
    </author>
    <author>
      <name>Xingda Wei</name>
    </author>
    <author>
      <name>Yingyi Hao</name>
    </author>
    <author>
      <name>Rong Chen</name>
    </author>
    <author>
      <name>Mingcong Han</name>
    </author>
    <author>
      <name>Jinyu Gu</name>
    </author>
    <author>
      <name>Haibo Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2405.12079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.12079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.17442v1</id>
    <updated>2024-05-15T22:34:52Z</updated>
    <published>2024-05-15T22:34:52Z</published>
    <title>Leveraging Machine Learning for Accurate IoT Device Identification in
  Dynamic Wireless Contexts</title>
    <summary>  Identifying IoT devices is crucial for network monitoring, security
enforcement, and inventory tracking. However, most existing identification
methods rely on deep packet inspection, which raises privacy concerns and adds
computational complexity. More importantly, existing works overlook the impact
of wireless channel dynamics on the accuracy of layer-2 features, thereby
limiting their effectiveness in real-world scenarios. In this work, we define
and use the latency of specific probe-response packet exchanges, referred to as
"device latency," as the main feature for device identification. Additionally,
we reveal the critical impact of wireless channel dynamics on the accuracy of
device identification based on device latency. Specifically, this work
introduces "accumulation score" as a novel approach to capturing fine-grained
channel dynamics and their impact on device latency when training machine
learning models. We implement the proposed methods and measure the accuracy and
overhead of device identification in real-world scenarios. The results confirm
that by incorporating the accumulation score for balanced data collection and
training machine learning algorithms, we achieve an F1 score of over 97% for
device identification, even amidst wireless channel dynamics, a significant
improvement over the 75% F1 score achieved by disregarding the impact of
channel dynamics on data collection and device latency.
</summary>
    <author>
      <name>Bhagyashri Tushir</name>
    </author>
    <author>
      <name>Vikram K Ramanna</name>
    </author>
    <author>
      <name>Yuhong Liu</name>
    </author>
    <author>
      <name>Behnam Dezfouli</name>
    </author>
    <link href="http://arxiv.org/abs/2405.17442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.17442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.01872v1</id>
    <updated>2024-06-04T00:51:12Z</updated>
    <published>2024-06-04T00:51:12Z</published>
    <title>A Survey of Unikernel Security: Insights and Trends from a Quantitative
  Analysis</title>
    <summary>  Unikernels, an evolution of LibOSs, are emerging as a virtualization
technology to rival those currently used by cloud providers. Unikernels combine
the user and kernel space into one "uni"fied memory space and omit
functionality that is not necessary for its application to run, thus
drastically reducing the required resources. The removed functionality however
is far-reaching and includes components that have become common security
technologies such as Address Space Layout Randomization (ASLR), Data Execution
Prevention (DEP), and Non-executable bits (NX bits). This raises questions
about the real-world security of unikernels. This research presents a
quantitative methodology using TF-IDF to analyze the focus of security
discussions within unikernel research literature. Based on a corpus of 33
unikernel-related papers spanning 2013-2023, our analysis found that Memory
Protection Extensions and Data Execution Prevention were the least frequently
occurring topics, while SGX was the most frequent topic. The findings quantify
priorities and assumptions in unikernel security research, bringing to light
potential risks from underexplored attack surfaces. The quantitative approach
is broadly applicable for revealing trends and gaps in niche security domains.
</summary>
    <author>
      <name>Alex Wollman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dakota State University</arxiv:affiliation>
    </author>
    <author>
      <name>John Hastings</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dakota State University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CARS61786.2024.10778787</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CARS61786.2024.10778787" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, 7 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2024 IEEE Cyber Awareness and Research Symposium (CARS), Grand
  Forks, ND, USA, 2024, pp. 1-9</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2406.01872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.01872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4; K.6.5; D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.19120v2</id>
    <updated>2025-04-15T15:28:55Z</updated>
    <published>2024-06-27T12:05:27Z</published>
    <title>QOS: A Quantum Operating System</title>
    <summary>  Quantum computers face challenges due to hardware constraints, noise errors,
and heterogeneity, and face fundamental design tradeoffs between key
performance metrics such as \textit{quantum fidelity} and system utilization.
This substantially complicates managing quantum resources to scale the size and
number of quantum algorithms that can be executed reliably in a given time.
  We introduce QOS, a cloud operating system for managing quantum resources
while mitigating their inherent limitations and balancing the design tradeoffs
of quantum computing. QOS exposes a hardware-agnostic API for transparent
quantum job execution, mitigates hardware errors, and systematically
multi-programs and schedules the jobs across space and time to achieve high
quantum fidelity in a resource-efficient manner. To achieve this, it leverages
two key insights: First, to maximize utilization and minimize fidelity loss,
some jobs are more compatible than others for multi-programming on the same
quantum computer. Second, sacrificing minimal fidelity can significantly reduce
job waiting times.
  We evaluate QOS on real quantum devices hosted by IBM, using 7000 real
quantum runs of more than 70.000 benchmark instances. We show that the QOS
achieves 2.6--456.5$\times$ higher fidelity, increases resource utilization by
up to 9.6$\times$, and reduces waiting times by up to 5$\times$ while
sacrificing only 1--3\% fidelity, on average, compared to the baselines.
</summary>
    <author>
      <name>Emmanouil Giortamis</name>
    </author>
    <author>
      <name>Francisco Romão</name>
    </author>
    <author>
      <name>Nathaniel Tornow</name>
    </author>
    <author>
      <name>Pramod Bhatotia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at OSDI '25</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.19120v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.19120v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.00313v1</id>
    <updated>2024-06-29T04:31:52Z</updated>
    <published>2024-06-29T04:31:52Z</published>
    <title>FastMig: Leveraging FastFreeze to Establish Robust Service Liquidity in
  Cloud 2.0</title>
    <summary>  Service liquidity across edge-to-cloud or multi-cloud will serve as the
cornerstone of the next generation of cloud computing systems (Cloud 2.0).
Provided that cloud-based services are predominantly containerized, an
efficient and robust live container migration solution is required to
accomplish service liquidity. In a nod to this growing requirement, in this
research, we leverage FastFreeze, a popular platform for process
checkpoint/restore within a container, and promote it to be a robust solution
for end-to-end live migration of containerized services. In particular, we
develop a new platform, called FastMig that proactively controls the
checkpoint/restore operations of FastFreeze, thereby, allowing for robust live
migration of containerized services via standard HTTP interfaces. The proposed
platform introduces post-checkpointing and pre-restoration operations to
enhance migration robustness. Notably, the pre-restoration operation includes
containerized service startup options, enabling warm restoration and reducing
the migration downtime. In addition, we develop a method to make FastFreeze
robust against failures that commonly happen during the migration and even
during the normal operation of a containerized service. Experimental results
under real-world settings show that the migration downtime of a containerized
service can be reduced by 30X compared to the situation where the original
FastFreeze was deployed for the migration. Moreover, we demonstrate that
FastMig and warm restoration method together can significantly mitigate the
container startup overhead. Importantly, these improvements are achieved
without any significant performance reduction and only incurs a small resource
usage overhead, compared to the bare (\ie non-FastFreeze) containerized
services.
</summary>
    <author>
      <name>Sorawit Manatura</name>
    </author>
    <author>
      <name>Thanawat Chanikaphon</name>
    </author>
    <author>
      <name>Chantana Chantrapornchai</name>
    </author>
    <author>
      <name>Mohsen Amini Salehi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in IEEE Cloud '24 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.00313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.00313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.00839v1</id>
    <updated>2024-06-30T21:47:09Z</updated>
    <published>2024-06-30T21:47:09Z</published>
    <title>Imaginary Machines: A Serverless Model for Cloud Applications</title>
    <summary>  Serverless Function-as-a-Service (FaaS) platforms provide applications with
resources that are highly elastic, quick to instantiate, accounted at fine
granularity, and without the need for explicit runtime resource orchestration.
This combination of the core properties underpins the success and popularity of
the serverless FaaS paradigm. However, these benefits are not available to most
cloud applications because they are designed for networked virtual
machines/containers environments. Since such cloud applications cannot take
advantage of the highly elastic resources of serverless and require run-time
orchestration systems to operate, they suffer from lower resource utilization,
additional management complexity, and costs relative to their FaaS serverless
counterparts.
  We propose Imaginary Machines, a new serverless model for cloud applications.
This model (1.) exposes the highly elastic resources of serverless platforms as
the traditional network-of-hosts model that cloud applications expect, and (2.)
it eliminates the need for explicit run-time orchestration by transparently
managing application resources based on signals generated during cloud
application executions. With the Imaginary Machines model, unmodified cloud
applications become serverless applications. While still based on the
network-of-host model, they benefit from the highly elastic resources and do
not require runtime orchestration, just like their specialized serverless FaaS
counterparts, promising increased resource utilization while reducing
management costs.
</summary>
    <author>
      <name>Michael Wawrzoniak</name>
    </author>
    <author>
      <name>Rodrigo Bruno</name>
    </author>
    <author>
      <name>Ana Klimovic</name>
    </author>
    <author>
      <name>Gustavo Alonso</name>
    </author>
    <link href="http://arxiv.org/abs/2407.00839v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.00839v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.18431v2</id>
    <updated>2024-09-06T18:42:43Z</updated>
    <published>2024-07-25T23:46:27Z</published>
    <title>Rusty Linux: Advances in Rust for Linux Kernel Development</title>
    <summary>  Context: The integration of Rust into kernel development is a transformative
endeavor aimed at enhancing system security and reliability by leveraging
Rust's strong memory safety guarantees. Objective: We aim to find the current
advances in using Rust in Kernel development to reduce the number of memory
safety vulnerabilities in one of the most critical pieces of software that
underpins all modern applications. Method: By analyzing a broad spectrum of
studies, we identify the advantages Rust offers, highlight the challenges
faced, and emphasize the need for community consensus on Rust's adoption.
Results: Our findings suggest that while the initial implementations of Rust in
the kernel show promising results in terms of safety and stability, significant
challenges remain. These challenges include achieving seamless interoperability
with existing kernel components, maintaining performance, and ensuring adequate
support and tooling for developers. Conclusions: This study underscores the
need for continued research and practical implementation efforts to fully
realize the benefits of Rust. By addressing these challenges, the integration
of Rust could mark a significant step forward in the evolution of operating
system development towards safer and more reliable systems
</summary>
    <author>
      <name>Shane K. Panter</name>
    </author>
    <author>
      <name>Nasir U. Eisty</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3674805.3690756</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3674805.3690756" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted for publication and presentation at ESEM
  2024 Emerging Results, Vision and Reflection Papers Track to be held in
  Barcelona, Spain on October 24-25, 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.18431v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.18431v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.00341v2</id>
    <updated>2024-11-14T11:30:49Z</updated>
    <published>2024-08-01T07:25:15Z</published>
    <title>Enhancing Attack Resilience in Real-Time Systems through Variable
  Control Task Sampling Rates</title>
    <summary>  Cyber-physical systems (CPSs) in modern real-time applications integrate
numerous control units linked through communication networks, each responsible
for executing a mix of real-time safety-critical and non-critical tasks. To
ensure predictable timing behaviour, most safety-critical tasks are scheduled
with fixed sampling periods, which supports rigorous safety and performance
analyses. However, this deterministic execution can be exploited by attackers
to launch inference-based attacks on safety-critical tasks. This paper
addresses the challenge of preventing such timing inference or schedule-based
attacks by dynamically adjusting the execution rates of safety-critical tasks
while maintaining their performance. We propose a novel schedule vulnerability
analysis methodology, enabling runtime switching between valid schedules for
various control task sampling rates. Leveraging this approach, we present the
Multi-Rate Attack-Aware Randomized Scheduling (MAARS) framework for preemptive
fixed-priority schedulers, designed to reduce the success rate of timing
inference attacks on real-time systems. To our knowledge, this is the first
method that combines attack-aware schedule randomization with preserved control
and scheduling integrity. The framework's efficacy in attack prevention is
evaluated on automotive benchmarks using a Hardware-in-the-Loop (HiL) setup.
</summary>
    <author>
      <name>Arkaprava Sain</name>
    </author>
    <author>
      <name>Sunandan Adhikary</name>
    </author>
    <author>
      <name>Ipsita Koley</name>
    </author>
    <author>
      <name>Soumyajit Dey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages including references, Total 10 figures (with 3 having
  subfigures)</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.00341v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.00341v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.01013v1</id>
    <updated>2024-08-02T05:15:38Z</updated>
    <published>2024-08-02T05:15:38Z</published>
    <title>Understanding and Enhancing Linux Kernel-based Packet Switching on WiFi
  Access Points</title>
    <summary>  As the number of WiFi devices and their traffic demands continue to rise, the
need for a scalable and high-performance wireless infrastructure becomes
increasingly essential. Central to this infrastructure are WiFi Access Points
(APs), which facilitate packet switching between Ethernet and WiFi interfaces.
Despite APs' reliance on the Linux kernel's data plane for packet switching,
the detailed operations and complexities of switching packets between Ethernet
and WiFi interfaces have not been investigated in existing works. This paper
makes the following contributions towards filling this research gap. Through
macro and micro-analysis of empirical experiments, our study reveals insights
in two distinct categories. Firstly, while the kernel's statistics offer
valuable insights into system operations, we identify and discuss potential
pitfalls that can severely affect system analysis. For instance, we reveal the
implications of device drivers on the meaning and accuracy of the statistics
related to packet-switching tasks and processor utilization. Secondly, we
analyze the impact of the packet switching path and core configuration on
performance and power consumption. Specifically, we identify the differences in
Ethernet-to-WiFi and WiFi-to-Ethernet data paths regarding processing
components, multi-core utilization, and energy efficiency. We show that the
WiFi-to-Ethernet data path leverages better multi-core processing and exhibits
lower power consumption.
</summary>
    <author>
      <name>Shiqi Zhang</name>
    </author>
    <author>
      <name>Mridul Gupta</name>
    </author>
    <author>
      <name>Behnam Dezfouli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE for possible publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.01013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.01013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.04104v3</id>
    <updated>2024-09-13T02:48:33Z</updated>
    <published>2024-08-07T21:45:01Z</published>
    <title>Hardware-Assisted Virtualization of Neural Processing Units for Cloud
  Platforms</title>
    <summary>  Cloud platforms today have been deploying hardware accelerators like neural
processing units (NPUs) for powering machine learning (ML) inference services.
To maximize the resource utilization while ensuring reasonable quality of
service, a natural approach is to virtualize NPUs for efficient resource
sharing for multi-tenant ML services. However, virtualizing NPUs for modern
cloud platforms is not easy. This is not only due to the lack of system
abstraction support for NPU hardware, but also due to the lack of architectural
and ISA support for enabling fine-grained dynamic operator scheduling for
virtualized NPUs.
  We present Neu10, a holistic NPU virtualization framework. We investigate
virtualization techniques for NPUs across the entire software and hardware
stack. Neu10 consists of (1) a flexible NPU abstraction called vNPU, which
enables fine-grained virtualization of the heterogeneous compute units in a
physical NPU (pNPU); (2) a vNPU resource allocator that enables pay-as-you-go
computing model and flexible vNPU-to-pNPU mappings for improved resource
utilization and cost-effectiveness; (3) an ISA extension of modern NPU
architecture for facilitating fine-grained tensor operator scheduling for
multiple vNPUs. We implement Neu10 based on a production-level NPU simulator.
Our experiments show that Neu10 improves the throughput of ML inference
services by up to 1.4$\times$ and reduces the tail latency by up to
4.6$\times$, while improving the NPU utilization by 1.2$\times$ on average,
compared to state-of-the-art NPU sharing approaches.
</summary>
    <author>
      <name>Yuqi Xue</name>
    </author>
    <author>
      <name>Yiqi Liu</name>
    </author>
    <author>
      <name>Lifeng Nai</name>
    </author>
    <author>
      <name>Jian Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to MICRO'24</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.04104v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.04104v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.04898v1</id>
    <updated>2024-08-09T06:55:00Z</updated>
    <published>2024-08-09T06:55:00Z</published>
    <title>Object as a Service: Simplifying Cloud-Native Development through
  Serverless Object Abstraction</title>
    <summary>  The function-as-a-service (FaaS) paradigm is envisioned as the next
generation of cloud computing systems that mitigate the burden for cloud-native
application developers by abstracting them from cloud resource management.
However, it does not deal with the application data aspects. As such,
developers have to intervene and undergo the burden of managing the application
data, often via separate cloud storage services. To further streamline
cloud-native application development, in this work, we propose a new paradigm,
known as Object as a Service (OaaS) that encapsulates application data and
functions into the cloud object abstraction. OaaS relieves developers from
resource and data management burden while offering built-in optimization
features. Inspired by OOP, OaaS incorporates access modifiers and inheritance
into the serverless paradigm that: (a) prevents developers from compromising
the system via accidentally accessing underlying data; and (b) enables software
reuse in cloud-native application development. Furthermore, OaaS natively
supports dataflow semantics. It enables developers to define function workflows
while transparently handling data navigation, synchronization, and parallelism
issues. To establish the OaaS paradigm, we develop a platform named Oparaca
that offers state abstraction for structured and unstructured data with
consistency and fault-tolerant guarantees. We evaluated Oparaca under
real-world settings against state-of-the-art platforms with respect to the
imposed overhead, scalability, and ease of use. The results demonstrate that
the object abstraction provided by OaaS can streamline flexible and scalable
cloud-native application development with an insignificant overhead on the
underlying serverless system.
</summary>
    <author>
      <name>Pawissanutt Lertpongrujikorn</name>
    </author>
    <author>
      <name>Mohsen Amini Salehi</name>
    </author>
    <link href="http://arxiv.org/abs/2408.04898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.04898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.08440v2</id>
    <updated>2024-12-30T06:00:31Z</updated>
    <published>2024-08-15T22:13:11Z</published>
    <title>Timing Analysis and Priority-driven Enhancements of ROS 2 Multi-threaded
  Executors</title>
    <summary>  The second generation of Robotic Operating System, ROS 2, has gained much
attention for its potential to be used for safety-critical robotic
applications. The need to provide a solid foundation for timing correctness and
scheduling mechanisms is therefore growing rapidly. Although there are some
pioneering studies conducted on formally analyzing the response time of
processing chains in ROS 2, the focus has been limited to single-threaded
executors, and multi-threaded executors, despite their advantages, have not
been studied well. To fill this knowledge gap, in this paper, we propose a
comprehensive response-time analysis framework for chains running on ROS 2
multi-threaded executors. We first analyze the timing behavior of the default
scheduling scheme in ROS 2 multi-threaded executors, and then present
priority-driven scheduling enhancements to address the limitations of the
default scheme. Our framework can analyze chains with both arbitrary and
constrained deadlines and also the effect of mutually-exclusive callback
groups. Evaluation is conducted by a case study on NVIDIA Jetson AGX Xavier and
schedulability experiments using randomly-generated chains. The results
demonstrate that our analysis framework can safely upper-bound response times
under various conditions and the priority-driven scheduling enhancements not
only reduce the response time of critical chains but also improve analytical
bounds.
</summary>
    <author>
      <name>Hoora Sobhani</name>
    </author>
    <author>
      <name>Hyunjong Choi</name>
    </author>
    <author>
      <name>Hyoseung Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2408.08440v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.08440v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.11325v1</id>
    <updated>2024-08-21T04:16:49Z</updated>
    <published>2024-08-21T04:16:49Z</published>
    <title>Telepathic Datacenters: Fast RPCs using Shared CXL Memory</title>
    <summary>  Datacenter applications often rely on remote procedure calls (RPCs) for fast,
efficient, and secure communication. However, RPCs are slow, inefficient, and
hard to use as they require expensive serialization and compression to
communicate over a packetized serial network link. Compute Express Link 3.0
(CXL) offers an alternative solution, allowing applications to share data using
a cache-coherent, shared-memory interface across clusters of machines.
  RPCool is a new framework that exploits CXL's shared memory capabilities.
RPCool avoids serialization by passing pointers to data structures in shared
memory. While avoiding serialization is useful, directly sharing pointer-rich
data eliminates the isolation that copying data over traditional networks
provides, leaving the receiver vulnerable to invalid pointers and concurrent
updates to shared data by the sender. RPCool restores this safety with careful
and efficient management of memory permissions. Another significant challenge
with CXL shared memory capabilities is that they are unlikely to scale to an
entire datacenter. RPCool addresses this by falling back to RDMA-based
communication.
  Overall, RPCool reduces the round-trip latency by 1.93$\times$ and
7.2$\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,
respectively. Moreover, RPCool performs either comparably or better than other
RPC mechanisms across a range of workloads.
</summary>
    <author>
      <name>Suyash Mahar</name>
    </author>
    <author>
      <name>Ehsan Hajyjasini</name>
    </author>
    <author>
      <name>Seungjin Lee</name>
    </author>
    <author>
      <name>Zifeng Zhang</name>
    </author>
    <author>
      <name>Mingyao Shen</name>
    </author>
    <author>
      <name>Steven Swanson</name>
    </author>
    <link href="http://arxiv.org/abs/2408.11325v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.11325v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.02381v1</id>
    <updated>2024-09-04T02:01:31Z</updated>
    <published>2024-09-04T02:01:31Z</published>
    <title>FlexBSO: Flexible Block Storage Offload for Datacenters</title>
    <summary>  Efficient virtualization of CPU and memory is standardized and mature.
Capabilities such as Intel VT-x [3] have been added by manufacturers for
efficient hypervisor support. In contrast, virtualization of a block device and
its presentation to the virtual machines on the host can be done in multiple
ways. Indeed, hyperscalers develop in-house solutions to improve performance
and cost-efficiency of their storage solutions for datacenters. Unfortunately,
these storage solutions are based on specialized hardware and software which
are not publicly available. The traditional solution is to expose virtual block
device to the VM through a paravirtualized driver like virtio [2]. virtio
provides significantly better performance than real block device driver
emulation because of host OS and guest OS cooperation. The IO requests are then
fulfilled by the host OS either with a local block device such as an SSD drive
or with some form of disaggregated storage over the network like NVMe-oF or
iSCSI. There are three main problems to the traditional solution. 1) Cost. IO
operations consume host CPU cycles due to host OS involvement. These CPU cycles
are doing useless work from the application point of view. 2) Inflexibility.
Any change of the virtualized storage stack requires host OS and/or guest OS
cooperation and cannot be done silently in production. 3) Performance. IO
operations are causing recurring VM EXITs to do the transition from non-root
mode to root mode on the host CPU. This results into excessive IO performance
impact. We propose FlexBSO, a hardware-assisted solution, which solves all the
mentioned issues. Our prototype is based on the publicly available Bluefield-2
SmartNIC with NVIDIA SNAP support, hence can be deployed without any obstacles.
</summary>
    <author>
      <name>Vojtech Aschenbrenner</name>
    </author>
    <author>
      <name>John Shawger</name>
    </author>
    <author>
      <name>Sadman Sakib</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.02381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.02381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.08141v3</id>
    <updated>2025-04-24T08:39:13Z</updated>
    <published>2024-09-12T15:34:23Z</published>
    <title>Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent
  Interconnects</title>
    <summary>  Conventional wisdom holds that an efficient interface between an OS running
on a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)
to offload data transfer, descriptor rings for buffering and queuing, and
interrupts for asynchrony between cores and device.
  In this paper we question this wisdom in the light of two trends: modern and
emerging cache-coherent interconnects like CXL3.0, and workloads, particularly
microservices and serverless computing. Like some others before us, we argue
that the assumptions of the DMA-based model are obsolete, and in many use-cases
programmed I/O, where the CPU explicitly transfers data and control information
to and from a device via loads and stores, delivers a more efficient system.
  However, we push this idea much further. We show, in a real hardware
implementation, the gains in latency for fine-grained communication achievable
using an open cache-coherence protocol which exposes cache transitions to a
smart device, and that throughput is competitive with DMA over modern
interconnects. We also demonstrate three use-cases: fine-grained RPC-style
invocation of functions on an accelerator, offloading of operators in a
streaming dataflow engine, and a network interface targeting serverless
functions, comparing our use of coherence with both traditional DMA-style
interaction and a highly-optimized implementation using memory-mapped
programmed I/O over PCIe.
</summary>
    <author>
      <name>Anastasiia Ruzhanskaia</name>
    </author>
    <author>
      <name>Pengcheng Xu</name>
    </author>
    <author>
      <name>David Cock</name>
    </author>
    <author>
      <name>Timothy Roscoe</name>
    </author>
    <link href="http://arxiv.org/abs/2409.08141v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.08141v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.09606v1</id>
    <updated>2024-09-15T04:11:26Z</updated>
    <published>2024-09-15T04:11:26Z</published>
    <title>BULKHEAD: Secure, Scalable, and Efficient Kernel Compartmentalization
  with PKS</title>
    <summary>  The endless stream of vulnerabilities urgently calls for principled
mitigation to confine the effect of exploitation. However, the monolithic
architecture of commodity OS kernels, like the Linux kernel, allows an attacker
to compromise the entire system by exploiting a vulnerability in any kernel
component. Kernel compartmentalization is a promising approach that follows the
least-privilege principle. However, existing mechanisms struggle with the
trade-off on security, scalability, and performance, given the challenges
stemming from mutual untrustworthiness among numerous and complex components.
  In this paper, we present BULKHEAD, a secure, scalable, and efficient kernel
compartmentalization technique that offers bi-directional isolation for
unlimited compartments. It leverages Intel's new hardware feature PKS to
isolate data and code into mutually untrusted compartments and benefits from
its fast compartment switching. With untrust in mind, BULKHEAD introduces a
lightweight in-kernel monitor that enforces multiple important security
invariants, including data integrity, execute-only memory, and compartment
interface integrity. In addition, it provides a locality-aware two-level scheme
that scales to unlimited compartments. We implement a prototype system on Linux
v6.1 to compartmentalize loadable kernel modules (LKMs). Extensive evaluation
confirms the effectiveness of our approach. As the system-wide impacts,
BULKHEAD incurs an average performance overhead of 2.44% for real-world
applications with 160 compartmentalized LKMs. While focusing on a specific
compartment, ApacheBench tests on ipv6 show an overhead of less than 2%.
Moreover, the performance is almost unaffected by the number of compartments,
which makes it highly scalable.
</summary>
    <author>
      <name>Yinggang Guo</name>
    </author>
    <author>
      <name>Zicheng Wang</name>
    </author>
    <author>
      <name>Weiheng Bai</name>
    </author>
    <author>
      <name>Qingkai Zeng</name>
    </author>
    <author>
      <name>Kangjie Lu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14722/ndss.2025.23328</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14722/ndss.2025.23328" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to appear in NDSS'25</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.09606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.09606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.13327v1</id>
    <updated>2024-09-20T08:36:40Z</updated>
    <published>2024-09-20T08:36:40Z</published>
    <title>Flexible Swapping for the Cloud</title>
    <summary>  Memory has become the primary cost driver in cloud data centers. Yet, a
significant portion of memory allocated to VMs in public clouds remains unused.
To optimize this resource, "cold" memory can be reclaimed from VMs and stored
on slower storage or compressed, enabling memory overcommit. Current overcommit
systems rely on general-purpose OS swap mechanisms, which are not optimized for
virtualized workloads, leading to missed memory-saving opportunities and
ineffective use of optimizations like prefetchers.
  This paper introduces a userspace memory management framework designed for
VMs. It enables custom policies that have full control over the virtual
machines' memory using a simple userspace API, supports huge page-based
swapping to satisfy VM performance requirements, is easy to deploy by
leveraging Linux/KVM, and supports zero-copy I/O virtualization with shared VM
memory.
  Our evaluation demonstrates that an overcommit system based on our framework
outperforms the state-of-the-art solutions on both micro-benchmarks and
commonly used cloud workloads. Specifically our implementation outperforms the
Linux Kernel baseline implementation by up to 25% while saving a similar amount
of memory. We also demonstrate the benefits of custom policies by implementing
workload-specific reclaimers and prefetchers that save $10\%$ additional
memory, improve performance in a limited memory scenario by 30% over the Linux
baseline, and recover faster from hard limit releases.
</summary>
    <author>
      <name>Milan Pandurov</name>
    </author>
    <author>
      <name>Lukas Humbel</name>
    </author>
    <author>
      <name>Dmitry Sepp</name>
    </author>
    <author>
      <name>Adamos Ttofari</name>
    </author>
    <author>
      <name>Leon Thomm</name>
    </author>
    <author>
      <name>Do Le Quoc</name>
    </author>
    <author>
      <name>Siddharth Chandrasekaran</name>
    </author>
    <author>
      <name>Sharan Santhanam</name>
    </author>
    <author>
      <name>Chuan Ye</name>
    </author>
    <author>
      <name>Shai Bergman</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Sven Lundgren</name>
    </author>
    <author>
      <name>Konstantinos Sagonas</name>
    </author>
    <author>
      <name>Alberto Ros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.13327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.13327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.16576v1</id>
    <updated>2024-09-25T03:14:01Z</updated>
    <published>2024-09-25T03:14:01Z</published>
    <title>FusionANNS: An Efficient CPU/GPU Cooperative Processing Architecture for
  Billion-scale Approximate Nearest Neighbor Search</title>
    <summary>  Approximate nearest neighbor search (ANNS) has emerged as a crucial component
of database and AI infrastructure. Ever-increasing vector datasets pose
significant challenges in terms of performance, cost, and accuracy for ANNS
services. None of modern ANNS systems can address these issues simultaneously.
We present FusionANNS, a high-throughput, low-latency, cost-efficient, and
high-accuracy ANNS system for billion-scale datasets using SSDs and only one
entry-level GPU. The key idea of FusionANNS lies in CPU/GPU collaborative
filtering and re-ranking mechanisms, which significantly reduce I/O operations
across CPUs, GPU, and SSDs to break through the I/O performance bottleneck.
Specifically, we propose three novel designs: (1) multi-tiered indexing to
avoid data swapping between CPUs and GPU, (2) heuristic re-ranking to eliminate
unnecessary I/Os and computations while guaranteeing high accuracy, and (3)
redundant-aware I/O deduplication to further improve I/O efficiency. We
implement FusionANNS and compare it with the state-of-the-art SSD-based ANNS
system -- SPANN and GPU-accelerated in-memory ANNS system -- RUMMY.
Experimental results show that FusionANNS achieves 1) 9.4-13.1X higher query
per second (QPS) and 5.7-8.8X higher cost efficiency compared with SPANN; 2)
and 2-4.9X higher QPS and 2.3-6.8X higher cost efficiency compared with RUMMY,
while guaranteeing low latency and high accuracy.
</summary>
    <author>
      <name>Bing Tian</name>
    </author>
    <author>
      <name>Haikun Liu</name>
    </author>
    <author>
      <name>Yuhang Tang</name>
    </author>
    <author>
      <name>Shihai Xiao</name>
    </author>
    <author>
      <name>Zhuohui Duan</name>
    </author>
    <author>
      <name>Xiaofei Liao</name>
    </author>
    <author>
      <name>Xuecang Zhang</name>
    </author>
    <author>
      <name>Junhua Zhu</name>
    </author>
    <author>
      <name>Yu Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 26 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.16576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.16576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.06145v1</id>
    <updated>2024-10-08T15:45:29Z</updated>
    <published>2024-10-08T15:45:29Z</published>
    <title>Serverless Cold Starts and Where to Find Them</title>
    <summary>  This paper releases and analyzes a month-long trace of 85 billion user
requests and 11.9 million cold starts from Huawei's serverless cloud platform.
Our analysis spans workloads from five data centers. We focus on cold starts
and provide a comprehensive examination of the underlying factors influencing
the number and duration of cold starts. These factors include trigger types,
request synchronicity, runtime languages, and function resource allocations. We
investigate components of cold starts, including pod allocation time, code and
dependency deployment time, and scheduling delays, and examine their
relationships with runtime languages, trigger types, and resource allocation.
We introduce pod utility ratio to measure the pod's useful lifetime relative to
its cold start time, giving a more complete picture of cold starts, and see
that some pods with long cold start times have longer useful lifetimes. Our
findings reveal the complexity and multifaceted origins of the number,
duration, and characteristics of cold starts, driven by differences in trigger
types, runtime languages, and function resource allocations. For example, cold
starts in Region 1 take up to 7 seconds, dominated by dependency deployment
time and scheduling. In Region 2, cold starts take up to 3 seconds and are
dominated by pod allocation time. Based on this, we identify opportunities to
reduce the number and duration of cold starts using strategies for multi-region
scheduling. Finally, we suggest directions for future research to address these
challenges and enhance the performance of serverless cloud platforms. Our
datasets and code are available here https://github.com/sir-lab/data-release
</summary>
    <author>
      <name>Artjom Joosen</name>
    </author>
    <author>
      <name>Ahmed Hassan</name>
    </author>
    <author>
      <name>Martin Asenov</name>
    </author>
    <author>
      <name>Rajkarn Singh</name>
    </author>
    <author>
      <name>Luke Darlow</name>
    </author>
    <author>
      <name>Jianfeng Wang</name>
    </author>
    <author>
      <name>Qiwen Deng</name>
    </author>
    <author>
      <name>Adam Barker</name>
    </author>
    <link href="http://arxiv.org/abs/2410.06145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.06145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; D.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.06492v1</id>
    <updated>2024-10-09T02:35:50Z</updated>
    <published>2024-10-09T02:35:50Z</published>
    <title>Overcoming Autoware-Ubuntu Incompatibility in Autonomous Driving
  Systems-Equipped Vehicles: Lessons Learned</title>
    <summary>  Autonomous vehicles have been rapidly developed as demand that provides
safety and efficiency in transportation systems. As autonomous vehicles are
designed based on open-source operating and computing systems, there are
numerous resources aimed at building an operating platform composed of Ubuntu,
Autoware, and Robot Operating System (ROS). However, no explicit guidelines
exist to help scholars perform trouble-shooting due to incompatibility between
the Autoware platform and Ubuntu operating systems installed in autonomous
driving systems-equipped vehicles (i.e., Chrysler Pacifica). The paper presents
an overview of integrating the Autoware platform into the autonomous vehicle's
interface based on lessons learned from trouble-shooting processes for
resolving incompatible issues. The trouble-shooting processes are presented
based on resolving the incompatibility and integration issues of Ubuntu 20.04,
Autoware.AI, and ROS Noetic software installed in an autonomous driving
systems-equipped vehicle. Specifically, the paper focused on common
incompatibility issues and code-solving protocols involving Python
compatibility, Compute Unified Device Architecture (CUDA) installation,
Autoware installation, and simulation in Autoware.AI. The objective of the
paper is to provide an explicit and detail-oriented presentation to showcase
how to address incompatibility issues among an autonomous vehicle's operating
interference. The lessons and experience presented in the paper will be useful
for researchers who encountered similar issues and could follow up by
performing trouble-shooting activities and implementing ADS-related projects in
the Ubuntu, Autoware, and ROS operating systems.
</summary>
    <author>
      <name>Dada Zhang</name>
    </author>
    <author>
      <name>Md Ruman Islam</name>
    </author>
    <author>
      <name>Pei-Chi Huang</name>
    </author>
    <author>
      <name>Chun-Hsing Ho</name>
    </author>
    <link href="http://arxiv.org/abs/2410.06492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.06492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.08434v1</id>
    <updated>2024-10-11T00:38:45Z</updated>
    <published>2024-10-11T00:38:45Z</published>
    <title>SoK: Software Compartmentalization</title>
    <summary>  Decomposing large systems into smaller components with limited privileges has
long been recognized as an effective means to minimize the impact of exploits.
Despite historical roots, demonstrated benefits, and a plethora of research
efforts in academia and industry, the compartmentalization of software is still
not a mainstream practice. This paper investigates why, and how this status quo
can be improved. Noting that existing approaches are fraught with
inconsistencies in terminology and analytical methods, we propose a unified
model for the systematic analysis, comparison, and directing of
compartmentalization approaches. We use this model to review 211 research
efforts and analyze 61 mainstream compartmentalized systems, confronting them
to understand the limitations of both research and production works. Among
others, our findings reveal that mainstream efforts largely rely on manual
methods, custom abstractions, and legacy mechanisms, poles apart from recent
research. We conclude with recommendations: compartmentalization should be
solved holistically; progress is needed towards simplifying the definition of
compartmentalization policies; towards better challenging our threat models in
the light of confused deputies and hardware limitations; as well as towards
bridging the gaps we pinpoint between research and mainstream needs. This paper
not only maps the historical and current landscape of compartmentalization, but
also sets forth a framework to foster their evolution and adoption.
</summary>
    <author>
      <name>Hugo Lefeuvre</name>
    </author>
    <author>
      <name>Nathan Dautenhahn</name>
    </author>
    <author>
      <name>David Chisnall</name>
    </author>
    <author>
      <name>Pierre Olivier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to appear in the 46th IEEE Symposium on Security and Privacy
  (S&amp;P'25)</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.08434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.08434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.12588v1</id>
    <updated>2024-10-16T14:07:35Z</updated>
    <published>2024-10-16T14:07:35Z</published>
    <title>FALCON: Pinpointing and Mitigating Stragglers for Large-Scale
  Hybrid-Parallel Training</title>
    <summary>  Fail-slows, or stragglers, are common but largely unheeded problems in
large-scale hybrid-parallel training that spans thousands of GPU servers and
runs for weeks to months. Yet, these problems are not well studied, nor can
they be quickly detected and effectively mitigated. In this paper, we first
present a characterization study on a shared production cluster with over
10,000 GPUs1. We find that fail-slows are caused by various CPU/GPU computation
and cross-node networking issues, lasting from tens of seconds to nearly ten
hours, and collectively delaying the average job completion time by 1.34%. The
current practice is to manually detect these fail-slows and simply treat them
as fail-stops using a checkpoint-and-restart failover approach, which are
labor-intensive and time-consuming. In this paper, we propose FALCON, a
framework that rapidly identifies fail-slowed GPUs and/or communication links,
and effectively tackles them with a novel multi-level mitigation mechanism, all
without human intervention. We have applied FALCON to detect human-labeled
fail-slows in a production cluster with over 99% accuracy. Cluster deployment
further demonstrates that FALCON effectively handles manually injected
fail-slows, mitigating the training slowdown by 60.1%.
</summary>
    <author>
      <name>Tianyuan Wu</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Yinghao Yu</name>
    </author>
    <author>
      <name>Siran Yang</name>
    </author>
    <author>
      <name>Wenchao Wu</name>
    </author>
    <author>
      <name>Qinkai Duan</name>
    </author>
    <author>
      <name>Guodong Yang</name>
    </author>
    <author>
      <name>Jiamang Wang</name>
    </author>
    <author>
      <name>Lin Qu</name>
    </author>
    <author>
      <name>Liping Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 20 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.12588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.12588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18053v1</id>
    <updated>2024-10-23T17:26:52Z</updated>
    <published>2024-10-23T17:26:52Z</published>
    <title>B-Side: Binary-Level Static System Call Identification</title>
    <summary>  System call filtering is widely used to secure programs in multi-tenant
environments, and to sandbox applications in modern desktop software deployment
and package management systems. Filtering rules are hard to write and maintain
manually, hence generating them automatically is essential. To that aim,
analysis tools able to identify every system call that can legitimately be
invoked by a program are needed. Existing static analysis works lack precision
because of a high number of false positives, and/or assume the availability of
program/libraries source code -- something unrealistic in many scenarios such
as cloud production environments.
  We present B-Side, a static binary analysis tool able to identify a superset
of the system calls that an x86-64 static/dynamic executable may invoke at
runtime. B-Side assumes no access to program/libraries sources, and shows a
good degree of precision by leveraging symbolic execution, combined with a
heuristic to detect system call wrappers, which represent an important source
of precision loss in existing works. B-Side also allows to statically detect
phases of execution in a program in which different filtering rules can be
applied. We validate B-Side and demonstrate its higher precision compared to
state-of-the-art works: over a set of popular applications, B-Side's average
$F_1$ score is 0.81, vs. 0.31 and 0.53 for competitors. Over 557 static and
dynamically-compiled binaries taken from the Debian repositories, B-Side
identifies an average of 43 system calls, vs. 271 and 95 for two state-of-the
art competitors. We further evaluate the strictness of the phase-based
filtering policies that can be obtained with B-Side.
</summary>
    <author>
      <name>Gaspard Thévenon</name>
    </author>
    <author>
      <name>Kevin Nguetchouang</name>
    </author>
    <author>
      <name>Kahina Lazri</name>
    </author>
    <author>
      <name>Alain Tchana</name>
    </author>
    <author>
      <name>Pierre Olivier</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3652892.3700761</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3652892.3700761" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to appear in the 25th ACM/IFIP International Middleware
  Conference (Middleware'24)</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.18053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.19274v2</id>
    <updated>2024-10-29T17:33:19Z</updated>
    <published>2024-10-25T03:01:19Z</published>
    <title>Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware
  Neuron Management</title>
    <summary>  Large Language Models (LLMs) have achieved remarkable success across various
domains, yet deploying them on mobile devices remains an arduous challenge due
to their extensive computational and memory demands. While lightweight LLMs
have been developed to fit mobile environments, they suffer from degraded model
accuracy. In contrast, sparsity-based techniques minimize DRAM usage by
selectively transferring only relevant neurons to DRAM while retaining the full
model in external storage, such as flash. However, such approaches are
critically limited by numerous I/O operations, particularly on smartphones with
severe IOPS constraints.
  In this paper, we propose Ripple, a novel approach that accelerates LLM
inference on smartphones by optimizing neuron placement in flash memory. Ripple
leverages the concept of Neuron Co-Activation, where neurons frequently
activated together are linked to facilitate continuous read access and optimize
data transfer efficiency. Our approach incorporates a two-stage solution: an
offline stage that reorganizes neuron placement based on co-activation
patterns, and an online stage that employs tailored data access and caching
strategies to align well with hardware characteristics. Evaluations conducted
on a variety of smartphones and LLMs demonstrate that Ripple achieves up to
5.93x improvements in I/O latency compared to the state-of-the-art. As the
first solution to optimize storage placement under sparsity, Ripple explores a
new optimization space at the intersection of sparsity-driven algorithm and
storage-level system co-design in LLM inference.
</summary>
    <author>
      <name>Tuowei Wang</name>
    </author>
    <author>
      <name>Ruwen Fan</name>
    </author>
    <author>
      <name>Minxing Huang</name>
    </author>
    <author>
      <name>Zixu Hao</name>
    </author>
    <author>
      <name>Kun Li</name>
    </author>
    <author>
      <name>Ting Cao</name>
    </author>
    <author>
      <name>Youyou Lu</name>
    </author>
    <author>
      <name>Yaoxue Zhang</name>
    </author>
    <author>
      <name>Ju Ren</name>
    </author>
    <link href="http://arxiv.org/abs/2410.19274v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.19274v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.20005v1</id>
    <updated>2024-10-25T23:18:43Z</updated>
    <published>2024-10-25T23:18:43Z</published>
    <title>Enhancing Battery Storage Energy Arbitrage with Deep Reinforcement
  Learning and Time-Series Forecasting</title>
    <summary>  Energy arbitrage is one of the most profitable sources of income for battery
operators, generating revenues by buying and selling electricity at different
prices. Forecasting these revenues is challenging due to the inherent
uncertainty of electricity prices. Deep reinforcement learning (DRL) emerged in
recent years as a promising tool, able to cope with uncertainty by training on
large quantities of historical data. However, without access to future
electricity prices, DRL agents can only react to the currently observed price
and not learn to plan battery dispatch. Therefore, in this study, we combine
DRL with time-series forecasting methods from deep learning to enhance the
performance on energy arbitrage. We conduct a case study using price data from
Alberta, Canada that is characterized by irregular price spikes and highly
non-stationary. This data is challenging to forecast even when state-of-the-art
deep learning models consisting of convolutional layers, recurrent layers, and
attention modules are deployed. Our results show that energy arbitrage with
DRL-enabled battery control still significantly benefits from these imperfect
predictions, but only if predictors for several horizons are combined. Grouping
multiple predictions for the next 24-hour window, accumulated rewards increased
by 60% for deep Q-networks (DQN) compared to the experiments without forecasts.
We hypothesize that multiple predictors, despite their imperfections, convey
useful information regarding the future development of electricity prices
through a "majority vote" principle, enabling the DRL agent to learn more
profitable control policies.
</summary>
    <author>
      <name>Manuel Sage</name>
    </author>
    <author>
      <name>Joshua Campbell</name>
    </author>
    <author>
      <name>Yaoyao Fiona Zhao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1115/ES2024-130538</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1115/ES2024-130538" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at the 18th ASME International Conference on
  Energy Sustainability</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.20005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.20005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.05309v1</id>
    <updated>2024-11-08T03:49:54Z</updated>
    <published>2024-11-08T03:49:54Z</published>
    <title>GPUVM: GPU-driven Unified Virtual Memory</title>
    <summary>  Graphics Processing Units (GPUs) leverage massive parallelism and large
memory bandwidth to support high-performance computing applications, such as
multimedia rendering, crypto-mining, deep learning, and natural language
processing. These applications require models and datasets that are getting
bigger in size and currently challenge the memory capacity of a single GPU,
causing substantial performance overheads. To address this problem, a
programmer has to partition the data and manually transfer data in and out of
the GPU. This approach requires programmers to carefully tune their
applications and can be impractical for workloads with irregular access
patterns, such as deep learning, recommender systems, and graph applications.
To ease programmability, programming abstractions such as unified virtual
memory (UVM) can be used, creating a virtually unified memory space across the
whole system and transparently moving the data on demand as it is accessed.
However, UVM brings in the overhead of the OS involvement and inefficiencies
due to generating many transfer requests especially when the GPU memory is
oversubscribed. This paper proposes GPUVM, a GPU memory management system that
uses an RDMA-capable network device to construct a virtual memory system
without involving the CPU/OS. GPUVM enables on-demand paging for GPU
applications and relies on GPU threads for memory management and page
migration. Since CPU chipsets do not support GPU-driven memory management, we
use a network interface card to facilitate transparent page migration from/to
the GPU. GPUVM achieves performance up to 4x higher than UVM for latency-bound
applications while providing accessible programming abstractions that do not
require the users to manage memory transfers directly.
</summary>
    <author>
      <name>Nurlan Nazaraliyev</name>
    </author>
    <author>
      <name>Elaheh Sadredini</name>
    </author>
    <author>
      <name>Nael Abu-Ghazaleh</name>
    </author>
    <link href="http://arxiv.org/abs/2411.05309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.05309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.10612v2</id>
    <updated>2024-12-22T20:51:08Z</updated>
    <published>2024-11-15T22:30:07Z</published>
    <title>Contextualizing Security and Privacy of Software-Defined Vehicles: State
  of the Art and Industry Perspectives</title>
    <summary>  The growing reliance on software in vehicles has given rise to the concept of
Software-Defined Vehicles (SDVs), fundamentally reshaping the vehicles and the
automotive industry. This survey explores the cybersecurity and privacy
challenges posed by SDVs, which increasingly integrate features like
Over-the-Air (OTA) updates and Vehicle-to-Everything (V2X) communication. While
these advancements enhance vehicle capabilities and flexibility, they also come
with a flip side: increased exposure to security risks including API
vulnerabilities, third-party software risks, and supply-chain threats. The
transition to SDVs also raises significant privacy concerns, with vehicles
collecting vast amounts of sensitive data, such as location and driver
behavior, that could be exploited using inference attacks. This work aims to
provide a detailed overview of security threats, mitigation strategies, and
privacy risks in SDVs, primarily through a literature review, enriched with
insights from a targeted questionnaire with industry experts. Key topics
include defining SDVs, comparing them to Connected Vehicles (CVs) and
Autonomous Vehicles (AVs), discussing the security challenges associated with
OTA updates and the impact of SDV features on data privacy. Our findings
highlight the need for robust security frameworks, standardized communication
protocols, and privacy-preserving techniques to address the issues of SDVs.
This work ultimately emphasizes the importance of a multi-layered defense
strategy,integrating both in-vehicle and cloud-based security solutions, to
safeguard future SDVs and increase user trust.
</summary>
    <author>
      <name>Marco De Vincenzi</name>
    </author>
    <author>
      <name>Mert D. Pesé</name>
    </author>
    <author>
      <name>Chiara Bodei</name>
    </author>
    <author>
      <name>Ilaria Matteucci</name>
    </author>
    <author>
      <name>Richard R. Brooks</name>
    </author>
    <author>
      <name>Monowar Hasan</name>
    </author>
    <author>
      <name>Andrea Saracino</name>
    </author>
    <author>
      <name>Mohammad Hamad</name>
    </author>
    <author>
      <name>Sebastian Steinhorst</name>
    </author>
    <link href="http://arxiv.org/abs/2411.10612v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.10612v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.15150v1</id>
    <updated>2024-11-07T13:01:55Z</updated>
    <published>2024-11-07T13:01:55Z</published>
    <title>Real-Time Aware IP-Networking for Resource-Constrained Embedded Devices</title>
    <summary>  This dissertation explores the area of real-time IP networking for embedded
devices, especially those with limited computational resources. With the
increasing convergence of information and operational technologies in various
industries, and the growing complexity of communication requirements in
(semi-)autonomous machines, there is a need for more advanced and reliable
networking solutions. This research focuses on the challenge of integrating
real-time embedded devices into packet-switched networks. Through a
comprehensive review of current real-time communication technologies,
standards, and practices in the context of Industry 4.0, a notable gap is
identified: the lack of a robust real-time communication standard tailored for
wireless mobile machines, and insufficient research on real-time embedded
devices in highly networked environments. The study includes detailed
experimentation with commercially available off-the-shelf networked
microcontrollers, revealing a priority inversion problem where network packet
processing interrupts real-time tasks, potentially causing real-time
violations. To address this challenge, this thesis proposes mitigation methods
and system designs that include software and hardware implementations. These
include a new embedded network subsystem that prioritizes packet processing
based on task priority, and a real-time-aware network interface controller that
moderates interrupt requests. In addition, a hybrid hardware-software co-design
approach is developed to ensure predictable and reliable real-time task
execution despite network congestion. Furthermore, the research extends to task
offloading in wireless Industrial Internet of Things environments, presenting a
system architecture and scheduler capable of maintaining real-time constraints
even under heavy loads and network uncertainties.
</summary>
    <author>
      <name>Ilja Behnke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14279/depositonce-20667</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14279/depositonce-20667" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">doctoral dissertation</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.15150v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.15150v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.4; C.3; B.4; C.0; C.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.17741v1</id>
    <updated>2024-11-24T16:20:57Z</updated>
    <published>2024-11-24T16:20:57Z</published>
    <title>Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM
  Inference Environments</title>
    <summary>  The widespread adoption of LLMs has driven an exponential rise in their
deployment, imposing substantial demands on inference clusters. These clusters
must handle numerous concurrent queries for different LLM downstream tasks. To
handle multi-task settings with vast LLM parameter counts, methods like
Low-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most
of the base LLM model across tasks. Hence, they allow concurrent task serving
with minimal memory requirements. However, existing LLM serving systems face
inefficiencies: they overlook workload heterogeneity, impose high link
bandwidth from frequent adapter loading, and suffer from head-of-line blocking
in their schedulers. To address these challenges, we present Chameleon, a novel
LLM serving system optimized for many adapter environments, that relies on two
core ideas: adapter caching and adapter-aware scheduling. First, Chameleon
caches popular adapters in GPU memory, minimizing the adapter loading times.
Importantly, it uses the otherwise idle GPU memory, avoiding extra memory
costs. Second, Chameleon uses a non-preemptive multi-queue scheduling to
efficiently account for workload heterogeneity. In this way, Chameleon
simultaneously prevents head of line blocking and starvation. We implement
Chameleon on top of a state-of-the-art LLM serving platform and evaluate it
with real-world production traces and open-source LLMs. Under high loads,
Chameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,
while improving throughput by 1.5x compared to state-of-the-art baselines.
</summary>
    <author>
      <name>Nikoleta Iliakopoulou</name>
    </author>
    <author>
      <name>Jovan Stojkovic</name>
    </author>
    <author>
      <name>Chloe Alverti</name>
    </author>
    <author>
      <name>Tianyin Xu</name>
    </author>
    <author>
      <name>Hubertus Franke</name>
    </author>
    <author>
      <name>Josep Torrellas</name>
    </author>
    <link href="http://arxiv.org/abs/2411.17741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.17741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.0; D.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.18845v2</id>
    <updated>2025-01-05T05:28:09Z</updated>
    <published>2024-11-28T01:24:16Z</published>
    <title>An Integrated Artificial Intelligence Operating System for Advanced
  Low-Altitude Aviation Applications</title>
    <summary>  This paper introduces a high-performance artificial intelligence operating
system tailored for low-altitude aviation, designed to address key challenges
such as real-time task execution, computational efficiency, and seamless
modular collaboration. Built on a powerful hardware platform and leveraging the
UNIX architecture, the system implements a distributed data processing strategy
that ensures rapid and efficient synchronization across critical modules,
including vision, navigation, and perception. By adopting dynamic resource
management, it optimally allocates computational resources, such as CPU and
GPU, based on task priority and workload, ensuring high performance for
demanding tasks like real-time video processing and AI model inference.
Furthermore, the system features an advanced interrupt handling mechanism that
allows for quick responses to sudden environmental changes, such as obstacle
detection, by prioritizing critical tasks, thus improving safety and mission
success rates. Robust security measures, including data encryption, access
control, and fault tolerance, ensure the system's resilience against external
threats and its ability to recover from potential hardware or software
failures. Complementing these core features are modular components for image
analysis, multi-sensor fusion, dynamic path planning, multi-drone coordination,
and ground station monitoring. Additionally, a low-code development platform
simplifies user customization, making the system adaptable to various
mission-specific needs. This comprehensive approach ensures the system meets
the evolving demands of intelligent aviation, providing a stable, efficient,
and secure environment for complex drone operations.
</summary>
    <author>
      <name>Minzhe Tan</name>
    </author>
    <author>
      <name>Xinlin Fan</name>
    </author>
    <author>
      <name>Jian He</name>
    </author>
    <author>
      <name>Yi Hou</name>
    </author>
    <author>
      <name>Zhan Liu</name>
    </author>
    <author>
      <name>Yaopeng Jiang</name>
    </author>
    <author>
      <name>Y. M. Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/2411.18845v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.18845v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.01059v3</id>
    <updated>2024-12-05T02:38:03Z</updated>
    <published>2024-12-02T02:40:05Z</published>
    <title>Blindfold: Confidential Memory Management by Untrusted Operating System</title>
    <summary>  Confidential Computing (CC) has received increasing attention in recent years
as a mechanism to protect user data from untrusted operating systems (OSes).
Existing CC solutions hide confidential memory from the OS and/or encrypt it to
achieve confidentiality. In doing so, they render OS memory optimization
unusable or complicate the trusted computing base (TCB) required for
optimization. This paper presents our results toward overcoming these
limitations, synthesized in a CC design named Blindfold. Like many other CC
solutions, Blindfold relies on a small trusted software component running at a
higher privilege level than the kernel, called Guardian. It features three
techniques that can enhance existing CC solutions. First, instead of nesting
page tables, Guardian mediates how the OS accesses memory and handles
exceptions by switching page and interrupt tables. Second, Blindfold employs a
lightweight capability system to regulate the kernel semantic access to user
memory, unifying case-by-case approaches in previous work. Finally, Blindfold
provides carefully designed secure ABI for confidential memory management
without encryption. We report an implementation of Blindfold that works on
ARMv8-A/Linux. Using Blindfold prototype, we are able to evaluate the cost of
enabling confidential memory management by the untrusted Linux kernel. We show
Blindfold has a smaller runtime TCB than related systems and enjoys competitive
performance. More importantly, we show that the Linux kernel, including all of
its memory optimizations except memory compression, can function properly for
confidential memory. This requires only about 400 lines of kernel
modifications.
</summary>
    <author>
      <name>Caihua Li</name>
    </author>
    <author>
      <name>Seung-seob Lee</name>
    </author>
    <author>
      <name>Lin Zhong</name>
    </author>
    <link href="http://arxiv.org/abs/2412.01059v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.01059v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.02192v1</id>
    <updated>2024-12-03T06:14:11Z</updated>
    <published>2024-12-03T06:14:11Z</published>
    <title>Thallus: An RDMA-based Columnar Data Transport Protocol</title>
    <summary>  The volume of data generated and stored in contemporary global data centers
is experiencing exponential growth. This rapid data growth necessitates
efficient processing and analysis to extract valuable business insights. In
distributed data processing systems, data undergoes exchanges between the
compute servers that contribute significantly to the total data processing
duration in adequately large clusters, necessitating efficient data transport
protocols. Traditionally, data transport frameworks such as JDBC and ODBC have
used TCP/IP-over-Ethernet as their underlying network protocol. Such frameworks
require serializing the data into a single contiguous buffer before handing it
off to the network card, primarily due to the requirement of contiguous data in
TCP/IP. In OLAP use cases, this serialization process is costly for columnar
data batches as it involves numerous memory copies that hurt data transport
duration and overall data processing performance. We study the serialization
overhead in the context of a widely-used columnar data format, Apache Arrow,
and propose leveraging RDMA to transport Arrow data over Infiniband in a
zero-copy manner. We design and implement Thallus, an RDMA-based columnar data
transport protocol for Apache Arrow based on the Thallium framework from the
Mochi ecosystem, compare it with a purely Thallium RPC-based implementation,
and show substantial performance improvements can be achieved by using RDMA for
columnar data transport.
</summary>
    <author>
      <name>Jayjeet Chakraborty</name>
    </author>
    <author>
      <name>Matthieu Dorier</name>
    </author>
    <author>
      <name>Philip Carns</name>
    </author>
    <author>
      <name>Robert Ross</name>
    </author>
    <author>
      <name>Carlos Maltzahn</name>
    </author>
    <author>
      <name>Heiner Litz</name>
    </author>
    <link href="http://arxiv.org/abs/2412.02192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.02192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.05784v3</id>
    <updated>2024-12-15T14:21:17Z</updated>
    <published>2024-12-08T02:30:23Z</published>
    <title>ASC-Hook: fast and transparent system call hook for Arm</title>
    <summary>  Intercepting system calls is crucial for tools that aim to modify or monitor
application behavior. However, existing system call interception tools on the
ARM platform still suffer from limitations in terms of performance and
completeness. This paper presents an efficient and comprehensive binary
rewriting framework, ASC-Hook, specifically designed for intercepting system
calls on the ARM platform. ASC-Hook addresses two key challenges on the ARM
architecture: the misalignment of the target address caused by directly
replacing the SVC instruction with br x8, and the return to the original
control flow after system call interception. This is achieved through a hybrid
replacement strategy and our specially designed trampoline mechanism. By
implementing multiple completeness strategies specifically for system calls, we
ensured comprehensive and thorough interception. Experimental results show that
ASC-Hook reduces overhead to at least 1/29 of that of existing system call
interception tools. We conducted extensive performance evaluations of ASC-Hook,
and the average performance loss for system call-intensive applications is
3.7\% .
</summary>
    <author>
      <name>Yang Shen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National University of Defense Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Min Xie</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National University of Defense Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Wenzhe Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National University of Defense Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Tao Wu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Changsha University of Science and Technology</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages (including appendix), 6 figures, not yet published</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.05784v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.05784v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.10487v2</id>
    <updated>2024-12-17T10:35:33Z</updated>
    <published>2024-12-13T15:18:39Z</published>
    <title>HyperGraphOS: A Modern Meta-Operating System for the Scientific and
  Engineering Domains</title>
    <summary>  This paper presents HyperGraphOS, a significant innovation in the domain of
operating systems, specifically designed to address the needs of scientific and
engineering domains. This platform aims to combine model-based engineering,
graph modeling, data containers, and documents, along with tools for handling
computational elements. HyperGraphOS functions as an Operating System offering
to users an infinite workspace for creating and managing complex models
represented as graphs with customizable semantics. By leveraging a web-based
architecture, it requires only a modern web browser for access, allowing
organization of knowledge, documents, and content into models represented in a
network of workspaces. Elements of the workspace are defined in terms of
domain-specific languages (DSLs). These DSLs are pivotal for navigating
workspaces, generating code, triggering AI components, and organizing
information and processes. The models' dual nature as both visual drawings and
data structures allows dynamic modifications and inspections both interactively
as well as programaticaly. We evaluated HyperGraphOS's efficiency and
applicability across a large set of diverse domains, including the design and
development of a virtual Avatar dialog system, a robotic task planner based on
large language models (LLMs), a new meta-model for feature-based code
development and many others. Our findings show that HyperGraphOS offers
substantial benefits in the interaction with a computer as information system,
as platoform for experiments and data analysis, as streamlined engineering
processes, demonstrating enhanced flexibility in managing data, computation and
documents, showing an innovative approaches to persistent desktop environments.
</summary>
    <author>
      <name>Antonello Ceravola</name>
    </author>
    <author>
      <name>Frank Joublin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 Pages, 10 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.10487v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.10487v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.17246v1</id>
    <updated>2024-12-23T03:38:46Z</updated>
    <published>2024-12-23T03:38:46Z</published>
    <title>Fast and Live Model Auto Scaling with O(1) Host Caching</title>
    <summary>  Model autoscaling is the key mechanism to achieve serverless
model-as-a-service, but it faces a fundamental trade-off between scaling speed
and storage/memory usage to cache parameters, and cannot meet frequent scaling
requirements across multiple hosts. The key problem is that data plane
performance is slow, and scaled instances remain stopped while parameters are
loading. We first show that data plane can be made fast with no/O(1) caching by
loading parameters through the compute network between GPUs because: (1) its
speed is comparable host cache and is underutilized; (2) scaling multiple
instances requires no or O(1) caching with network-optimized multicast. Second,
autoscaling can be made live by breaking the scaling abstraction from a
coarse-grained instance-level to a fine-grained layer-level. This allows us to
offload the layer computation from the overloaded serving instances to the
scaled instance with cooperative execution, thus handles cases even when the
compute network is not sufficiently fast. Our system BLITZSCALE reduces the
serving tail latencies by up to 86% without caching, and we achieve comparable
performance (or even better) to an optimal setup where all the parameters are
cached at all the host for autoscaling.
</summary>
    <author>
      <name>Dingyan Zhang</name>
    </author>
    <author>
      <name>Haotian Wang</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Xingda Wei</name>
    </author>
    <author>
      <name>Yizhou Shan</name>
    </author>
    <author>
      <name>Rong Chen</name>
    </author>
    <author>
      <name>Haibo Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2412.17246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.17246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.01334v1</id>
    <updated>2025-01-02T16:36:41Z</updated>
    <published>2025-01-02T16:36:41Z</published>
    <title>Analysis of Security in OS-Level Virtualization</title>
    <summary>  Virtualization is a technique that allows multiple instances typically
running different guest operating systems on top of single physical hardware. A
hypervisor, a layer of software running on top of the host operating system,
typically runs and manages these different guest operating systems. Rather than
to run different services on different servers for reliability and security
reasons, companies started to employ virtualization over their servers to run
these services within a single server. This approach proves beneficial to the
companies as it provides much better reliability, stronger isolation, improved
security and resource utilization compared to running services on multiple
servers. Although hypervisor based virtualization offers better resource
utilization and stronger isolation, it also suffers from high overhead as the
host operating system has to maintain different guest operating systems.
  To tackle this issue, another form of virtualization known as Operating
System-level virtualization has emerged. This virtualization provides
light-weight, minimal and efficient virtualization, as the different instances
are run on top of the same host operating system, sharing the resources of the
host operating system. But due to instances sharing the same host operating
system affects the isolation of the instances. In this paper, we will first
establish the basic concepts of virtualization and point out the differences
between the hyper-visor based virtualization and operating system-level
virtualization. Next, we will discuss the container creation life-cycle which
helps in forming a container threat model for the container systems, which
allows to map different potential attack vectors within these systems. Finally,
we will discuss a case study, which further looks at isolation provided by the
containers.
</summary>
    <author>
      <name>Krishna Sai Ketha</name>
    </author>
    <author>
      <name>Guanqun Song</name>
    </author>
    <author>
      <name>Ting Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2501.01334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.01334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.03898v1</id>
    <updated>2025-01-07T16:05:27Z</updated>
    <published>2025-01-07T16:05:27Z</published>
    <title>SPECTRE: A Hybrid System for an Adaptative and Optimised Cyber Threats
  Detection, Response and Investigation in Volatile Memory</title>
    <summary>  The increasing sophistication of modern cyber threats, particularly file-less
malware relying on living-off-the-land techniques, poses significant challenges
to traditional detection mechanisms. Memory forensics has emerged as a crucial
method for uncovering such threats by analysing dynamic changes in memory. This
research introduces SPECTRE (Snapshot Processing, Emulation, Comparison, and
Threat Reporting Engine), a modular Cyber Incident Response System designed to
enhance threat detection, investigation, and visualization. By adopting
Volatility JSON format as an intermediate output, SPECTRE ensures compatibility
with widely used DFIR tools, minimizing manual data transformations and
enabling seamless integration into established workflows. Its emulation
capabilities safely replicate realistic attack scenarios, such as credential
dumping and malicious process injections, for controlled experimentation and
validation. The anomaly detection module addresses critical attack vectors,
including RunDLL32 abuse and malicious IP detection, while the IP forensics
module enhances threat intelligence by integrating tools like Virus Total and
geolocation APIs. SPECTRE advanced visualization techniques transform raw
memory data into actionable insights, aiding Red, Blue and Purple teams in
refining strategies and responding effectively to threats. Bridging gaps
between memory and network forensics, SPECTRE offers a scalable, robust
platform for advancing threat detection, team training, and forensic research
in combating sophisticated cyber threats.
</summary>
    <author>
      <name>Arslan Tariq Syed</name>
    </author>
    <author>
      <name>Mohamed Chahine Ghanem</name>
    </author>
    <author>
      <name>Elhadj Benkhelifa</name>
    </author>
    <author>
      <name>Fauzia Idrees Abro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 Version 2.1</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.03898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.03898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.04580v1</id>
    <updated>2025-01-08T15:51:02Z</updated>
    <published>2025-01-08T15:51:02Z</published>
    <title>Goldilocks Isolation: High Performance VMs with Edera</title>
    <summary>  Organizations run applications on cloud infrastructure shared between
multiple users and organizations. Popular tooling for this shared
infrastructure, including Docker and Kubernetes, supports such multi-tenancy
through the use of operating system virtualization. With operating system
virtualization (known as containerization), multiple applications share the
same kernel, reducing the runtime overhead. However, this shared kernel
presents a large attack surface and has led to a proliferation of container
escape attacks in which a kernel exploit lets an attacker escape the isolation
of operating system virtualization to access other applications or the
operating system itself. To address this, some systems have proposed a return
to hypervisor virtualization for stronger isolation between applications.
However, no existing system has achieved both the isolation of hypervisor
virtualization and the performance and usability of operating system
virtualization.
  We present Edera, an optimized type 1 hypervisor that uses paravirtualization
to improve the runtime of hypervisor virtualization. We illustrate Edera's
usability and performance through two use cases. First, we create a container
runtime compatible with Kubernetes that runs on the Edera hypervisor. This
implementation can be used as a drop-in replacement for the Kubernetes runtime
and is compatible with all the tooling in the Kubernetes ecosystem. Second, we
use Edera to provide driver isolation for hardware drivers, including those for
networking, storage, and GPUs. This use of isolation protects the hypervisor
and other applications from driver vulnerabilities. We find that Edera has
runtime comparable to Docker with .9% slower cpu speeds, an average of 3%
faster system call performance, and memory performance 0-7% faster. It achieves
this with a 648 millisecond increase in startup time from Docker's 177.4
milliseconds.
</summary>
    <author>
      <name>Marina Moore</name>
    </author>
    <author>
      <name>Alex Zenla</name>
    </author>
    <link href="http://arxiv.org/abs/2501.04580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.04580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.14770v2</id>
    <updated>2025-01-28T20:35:23Z</updated>
    <published>2024-12-29T17:37:18Z</published>
    <title>Optimizing SSD Caches for Cloud Block Storage Systems Using Machine
  Learning Approaches</title>
    <summary>  The growing demand for efficient cloud storage solutions has led to the
widespread adoption of Solid-State Drives (SSDs) for caching in cloud block
storage systems. The management of data writes to SSD caches plays a crucial
role in improving overall system performance, reducing latency, and extending
the lifespan of storage devices. A critical challenge arises from the large
volume of write-only data, which significantly impacts the performance of SSD
caches when handled inefficiently. Specifically, writes that have not been read
for a certain period may introduce unnecessary write traffic to the SSD cache
without offering substantial benefits for cache performance. This paper
proposes a novel approach to mitigate this issue by leveraging machine learning
techniques to dynamically optimize the write policy in cloud-based storage
systems. The proposed method identifies write-only data and selectively filters
it out in real-time, thereby minimizing the number of unnecessary write
operations and improving the overall performance of the cache system.
Experimental results demonstrate that the proposed machine learning-based
policy significantly outperforms traditional approaches by reducing the number
of harmful writes and optimizing cache utilization. This solution is
particularly suitable for cloud environments with varying and unpredictable
workloads, where traditional cache management strategies often fall short.
</summary>
    <author>
      <name>Chiyu Cheng</name>
    </author>
    <author>
      <name>Chang Zhou</name>
    </author>
    <author>
      <name>Yang Zhao</name>
    </author>
    <author>
      <name>Jin Cao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">I uploaded the paper without obtaining consent from all the authors.
  One of the authors now refuses to publish this paper, as it has been
  demonstrated to be unreliable, contains significant flaws in prior research,
  and is missing citations in Sections 2</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.14770v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.14770v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.16165v1</id>
    <updated>2025-01-27T16:03:14Z</updated>
    <published>2025-01-27T16:03:14Z</published>
    <title>Demystifying OS Kernel Fuzzing with a Novel Taxonomy</title>
    <summary>  The Operating System (OS) kernel is foundational in modern computing,
especially with the proliferation of diverse computing devices. However, its
development also comes with vulnerabilities that can lead to severe security
breaches. Kernel fuzzing, a technique used to uncover these vulnerabilities,
poses distinct challenges when compared to userspace fuzzing. These include the
complexity of configuring the testing environment and addressing the
statefulness inherent to both the kernel and the fuzzing process. Despite the
significant interest from the security community, a comprehensive understanding
of kernel fuzzing remains lacking, hindering further progress in the field. In
this paper, we present the first systematic study dedicated to OS kernel
fuzzing. It begins by summarizing the progress of 99 academic studies from
top-tier venues between 2017 and 2024. Following this, we introduce a
stage-based fuzzing model and a novel fuzzing taxonomy that highlights nine
core functionalities unique to kernel fuzzing. These functionalities are
examined alongside their corresponding methodological approaches based on
qualitative evaluation criteria. Our systematization identifies challenges in
meeting functionality requirements and proposes potential technical solutions.
Finally, we outline promising and practical future directions to guide
forthcoming research in kernel security, supported in part by insights derived
from our case study.
</summary>
    <author>
      <name>Jiacheng Xu</name>
    </author>
    <author>
      <name>He Sun</name>
    </author>
    <author>
      <name>Shihao Jiang</name>
    </author>
    <author>
      <name>Qinying Wang</name>
    </author>
    <author>
      <name>Mingming Zhang</name>
    </author>
    <author>
      <name>Xiang Li</name>
    </author>
    <author>
      <name>Kaiwen Shen</name>
    </author>
    <author>
      <name>Peng Cheng</name>
    </author>
    <author>
      <name>Jiming Chen</name>
    </author>
    <author>
      <name>Charles Zhang</name>
    </author>
    <author>
      <name>Shouling Ji</name>
    </author>
    <link href="http://arxiv.org/abs/2501.16165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.16165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.08830v1</id>
    <updated>2025-02-12T22:38:50Z</updated>
    <published>2025-02-12T22:38:50Z</published>
    <title>Investigation of Advanced Persistent Threats Network-based Tactics,
  Techniques and Procedures</title>
    <summary>  The scarcity of data and the high complexity of Advanced Persistent Threats
(APTs) attacks have created challenges in comprehending their behavior and
hindered the exploration of effective detection techniques. To create an
effective APT detection strategy, it is important to examine the Tactics,
Techniques, and Procedures (TTPs) that have been reported by the industry.
These TTPs can be difficult to classify as either malicious or legitimate. When
developing an approach for the next generation of network intrusion detection
systems (NIDS), it is necessary to take into account the specific context of
the attack explained in this paper.
  In this study, we select 33 APT campaigns based on the fair distribution over
the past 22 years to observe the evolution of APTs over time. We focus on their
evasion techniques and how they stay undetected for months or years. We found
that APTs cannot continue their operations without C&amp;C servers, which are
mostly addressed by Domain Name System (DNS). We identify several TTPs used for
DNS, such as Dynamic DNS, typosquatting, and TLD squatting. The next step for
APT operators is to start communicating with a victim. We found that the most
popular protocol to deploy evasion techniques is using HTTP(S) with 81% of APT
campaigns. HTTP(S) can evade firewall filtering and pose as legitimate
web-based traffic. DNS protocol is also widely used by 45% of APTs for DNS
resolution and tunneling. We identify and analyze the TTPs associated with
using HTTP(S) based on real artifacts.
</summary>
    <author>
      <name>Almuthanna Alageel</name>
    </author>
    <author>
      <name>Sergio Maffeis</name>
    </author>
    <author>
      <name>Imperial College London</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 14 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.08830v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.08830v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.09002v2</id>
    <updated>2025-04-18T21:40:39Z</updated>
    <published>2025-03-12T02:30:19Z</published>
    <title>KNighter: Transforming Static Analysis with LLM-Synthesized Checkers</title>
    <summary>  Static analysis is a powerful technique for bug detection in critical systems
like operating system kernels. However, designing and implementing static
analyzers is challenging, time-consuming, and typically limited to predefined
bug patterns. While large language models (LLMs) have shown promise for static
analysis, directly applying them to scan large systems remains impractical due
to computational constraints and contextual limitations.
  We present KNighter, the first approach that unlocks scalable LLM-based
static analysis by automatically synthesizing static analyzers from historical
bug patterns. Rather than using LLMs to directly analyze massive systems, our
key insight is leveraging LLMs to generate specialized static analyzers guided
by historical patch knowledge. KNighter implements this vision through a
multi-stage synthesis pipeline that validates checker correctness against
original patches and employs an automated refinement process to iteratively
reduce false positives. Our evaluation on the Linux kernel demonstrates that
KNighter generates high-precision checkers capable of detecting diverse bug
patterns overlooked by existing human-written analyzers. To date,
KNighter-synthesized checkers have discovered 92 new, critical, long-latent
bugs (average 4.3 years) in the Linux kernel; 77 are confirmed, 57 fixed, and
16 have been assigned CVE numbers. This work establishes an entirely new
paradigm for scalable, reliable, and traceable LLM-based static analysis for
real-world systems via checker synthesis.
</summary>
    <author>
      <name>Chenyuan Yang</name>
    </author>
    <author>
      <name>Zijie Zhao</name>
    </author>
    <author>
      <name>Zichen Xie</name>
    </author>
    <author>
      <name>Haoyu Li</name>
    </author>
    <author>
      <name>Lingming Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2503.09002v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.09002v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.10725v1</id>
    <updated>2025-03-13T10:34:15Z</updated>
    <published>2025-03-13T10:34:15Z</published>
    <title>Samoyeds: Accelerating MoE Models with Structured Sparsity Leveraging
  Sparse Tensor Cores</title>
    <summary>  The escalating size of Mixture-of-Experts (MoE) based Large Language Models
(LLMs) presents significant computational and memory challenges, necessitating
innovative solutions to enhance efficiency without compromising model accuracy.
Structured sparsity emerges as a compelling strategy to address these
challenges by leveraging the emerging sparse computing hardware. Prior works
mainly focus on the sparsity in model parameters, neglecting the inherent
sparse patterns in activations. This oversight can lead to additional
computational costs associated with activations, potentially resulting in
suboptimal performance.
  This paper presents Samoyeds, an innovative acceleration system for MoE LLMs
utilizing Sparse Tensor Cores (SpTCs). Samoyeds is the first to apply sparsity
simultaneously to both activations and model parameters. It introduces a
bespoke sparse data format tailored for MoE computation and develops a
specialized sparse-sparse matrix multiplication kernel. Furthermore, Samoyeds
incorporates systematic optimizations specifically designed for the execution
of dual-side structured sparse MoE LLMs on SpTCs, further enhancing system
performance. Evaluations show that Samoyeds outperforms SOTA works by up to
1.99$\times$ at the kernel level and 1.58$\times$ at the model level. Moreover,
it enhances memory efficiency, increasing maximum supported batch sizes by
4.41$\times$ on average. Additionally, Samoyeds surpasses existing SOTA
structured sparse solutions in both model accuracy and hardware portability.
</summary>
    <author>
      <name>Chenpeng Wu</name>
    </author>
    <author>
      <name>Qiqi Gu</name>
    </author>
    <author>
      <name>Heng Shi</name>
    </author>
    <author>
      <name>Jianguo Yao</name>
    </author>
    <author>
      <name>Haibing Guan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3689031.3717455</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3689031.3717455" rel="related"/>
    <link href="http://arxiv.org/abs/2503.10725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.10725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.15065v1</id>
    <updated>2025-03-19T10:02:54Z</updated>
    <published>2025-03-19T10:02:54Z</published>
    <title>A Comprehensive Quantification of Inconsistencies in Memory Dumps</title>
    <summary>  Memory forensics is a powerful technique commonly adopted to investigate
compromised machines and to detect stealthy computer attacks that do not store
data on non-volatile storage. To employ this technique effectively, the analyst
has to first acquire a faithful copy of the system's volatile memory after the
incident. However, almost all memory acquisition tools capture the content of
physical memory without stopping the system's activity and by following the
ascending order of the physical pages, which can lead to inconsistencies and
errors in the dump. In this paper we developed a system to track all write
operations performed by the OS kernel during a memory acquisition process. This
allows us to quantify, for the first time, the exact number and type of
inconsistencies observed in memory dumps. We examine the runtime activity of
three different operating systems and the way the manage physical memory. Then,
focusing on Linux, we quantify how different acquisition modes, file systems,
and hardware targets influence the frequency of kernel writes during the dump.
We also analyze the impact of inconsistencies on the reconstruction of page
tables and major kernel data structures used by Volatility to extract forensic
artifacts. Our results show that inconsistencies are very common and that their
presence can undermine the reliability and validity of memory forensics
analysis.
</summary>
    <author>
      <name>Andrea Oliveri</name>
    </author>
    <author>
      <name>Davide Balzarotti</name>
    </author>
    <link href="http://arxiv.org/abs/2503.15065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.15065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.17588v1</id>
    <updated>2025-03-22T00:14:47Z</updated>
    <published>2025-03-22T00:14:47Z</published>
    <title>LEMIX: Enabling Testing of Embedded Applications as Linux Applications</title>
    <summary>  Dynamic analysis, through rehosting, is an important capability for security
assessment in embedded systems software. Existing rehosting techniques aim to
provide high-fidelity execution by accurately emulating hardware and peripheral
interactions. However, these techniques face challenges in adoption due to the
increasing number of available peripherals and the complexities involved in
designing emulation models for diverse hardware. Additionally, contrary to the
prevailing belief that guides existing works, our analysis of reported bugs
shows that high-fidelity execution is not required to expose most bugs in
embedded software. Our key hypothesis is that security vulnerabilities are more
likely to arise at higher abstraction levels. To substantiate our hypothesis,
we introduce LEMIX, a framework enabling dynamic analysis of embedded
applications by rehosting them as x86 Linux applications decoupled from
hardware dependencies. Enabling embedded applications to run natively on Linux
facilitates security analysis using available techniques and takes advantage of
the powerful hardware available on the Linux platform for higher testing
throughput. We develop various techniques to address the challenges involved in
converting embedded applications to Linux applications. We evaluated LEMIX on
18 real-world embedded applications across four RTOSes and found 21 new bugs in
12 of the applications and all 4 of the RTOS kernels. We report that LEMIX is
superior to existing state-of-the-art techniques both in terms of code coverage
(~2x more coverage) and bug detection (18 more bugs).
</summary>
    <author>
      <name>Sai Ritvik Tanksalkar</name>
    </author>
    <author>
      <name>Siddharth Muralee</name>
    </author>
    <author>
      <name>Srihari Danduri</name>
    </author>
    <author>
      <name>Paschal Amusuo</name>
    </author>
    <author>
      <name>Antonio Bianchi</name>
    </author>
    <author>
      <name>James C Davis</name>
    </author>
    <author>
      <name>Aravind Kumar Machiry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Latex Version 3.14 (TexLive 2023), 25 pages, 7 Figures, 9 Tables and
  14 Listings</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.17588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.17588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6; D.4.9; K.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.23774v1</id>
    <updated>2025-03-31T06:48:58Z</updated>
    <published>2025-03-31T06:48:58Z</published>
    <title>Who is in Charge here? Understanding How Runtime Configuration Affects
  Software along with Variables&amp;Constants</title>
    <summary>  Runtime misconfiguration can lead to software performance degradation and
even cause failure. Developers typically perform sanity checks during the
configuration parsing stage to prevent invalid parameter values. However, we
discovered that even valid values that pass these checks can also lead to
unexpected severe consequences. Our study reveals the underlying reason: the
value of runtime configuration parameters may interact with other constants and
variables when propagated and used, altering its original effect on software
behavior. Consequently, parameter values may no longer be valid when
encountering complex runtime environments and workloads. Therefore, it is
extremely challenging for users to properly configure the software before it
starts running. This paper presents the first comprehensive and in-depth study
(to the best of our knowledge) on how configuration affects software at runtime
through the interaction with constants, and variables (PCV Interaction).
Parameter values represent user intentions, constants embody developer
knowledge, and variables are typically defined by the runtime environment and
workload. This interaction essentially illustrates how different roles jointly
determine software behavior. In this regard, we studied 705 configuration
parameters from 10 large-scale software systems. We reveal that a large portion
of configuration parameters interact with constants/variables after parsing. We
analyzed the interaction patterns and their effects on software runtime
behavior. Furthermore, we highlighted the risks of PCV interaction and
identified potential issues behind specific interaction patterns. Our findings
expose the "double edge" of PCV interaction, providing new insights and
motivating the development of new automated techniques to help users configure
software appropriately and assist developers in designing better
configurations.
</summary>
    <author>
      <name>Chaopeng Luo</name>
    </author>
    <author>
      <name>Yuanliang Zhang</name>
    </author>
    <author>
      <name>Haochen He</name>
    </author>
    <author>
      <name>Zhouyang Jia</name>
    </author>
    <author>
      <name>Teng Wang</name>
    </author>
    <author>
      <name>Shulin Zhou</name>
    </author>
    <author>
      <name>Si Zheng</name>
    </author>
    <author>
      <name>Shanshan Li</name>
    </author>
    <link href="http://arxiv.org/abs/2503.23774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.23774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.13994v1</id>
    <updated>2025-04-18T17:28:40Z</updated>
    <published>2025-04-18T17:28:40Z</published>
    <title>Terminal Lucidity: Envisioning the Future of the Terminal</title>
    <summary>  The Unix terminal, or just simply, the terminal, can be found being applied
in almost every facet of computing. It is available across all major platforms
and often integrated into other applications. Due to its ubiquity, even
marginal improvements to the terminal have the potential to make massive
improvements to productivity on a global scale. We believe that evolutionary
improvements to the terminal, in its current incarnation as windowed terminal
emulator, are possible and that developing a thorough understanding of issues
that current terminal users face is fundamental to knowing how the terminal
should evolve. In order to develop that understanding we have mined Unix and
Linux Stack Exchange using a fully-reproducible method which was able to
extract and categorize 91.0% of 1,489 terminal-related questions (from the full
set of nearly 240,000 questions) without manual intervention.
  We present an analysis, to our knowledge the first of its kind, of windowed
terminal-related questions posted over a 15-year period and viewed, in
aggregate, approximately 40 million times. As expected, given its longevity, we
find the terminal's many features being applied across a wide variety of use
cases. We find evidence that the terminal, as windowed terminal emulator, has
neither fully adapted to its now current graphical environment nor completely
untangled itself from features more suited to incarnations in previous
environments. We also find evidence of areas where we believe the terminal
could be extended along with other areas where it could be simplified.
Surprisingly, while many current efforts to improve the terminal include
improving the terminal's social and collaborative aspects, we find little
evidence of this as a prominent pain point.
</summary>
    <author>
      <name>Michael MacInnis</name>
    </author>
    <author>
      <name>Olga Baysal</name>
    </author>
    <author>
      <name>Michele Lanza</name>
    </author>
    <link href="http://arxiv.org/abs/2504.13994v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13994v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15499v1</id>
    <updated>2025-04-22T00:29:18Z</updated>
    <published>2025-04-22T00:29:18Z</published>
    <title>Guillotine: Hypervisors for Isolating Malicious AIs</title>
    <summary>  As AI models become more embedded in critical sectors like finance,
healthcare, and the military, their inscrutable behavior poses ever-greater
risks to society. To mitigate this risk, we propose Guillotine, a hypervisor
architecture for sandboxing powerful AI models -- models that, by accident or
malice, can generate existential threats to humanity. Although Guillotine
borrows some well-known virtualization techniques, Guillotine must also
introduce fundamentally new isolation mechanisms to handle the unique threat
model posed by existential-risk AIs. For example, a rogue AI may try to
introspect upon hypervisor software or the underlying hardware substrate to
enable later subversion of that control plane; thus, a Guillotine hypervisor
requires careful co-design of the hypervisor software and the CPUs, RAM, NIC,
and storage devices that support the hypervisor software, to thwart side
channel leakage and more generally eliminate mechanisms for AI to exploit
reflection-based vulnerabilities. Beyond such isolation at the software,
network, and microarchitectural layers, a Guillotine hypervisor must also
provide physical fail-safes more commonly associated with nuclear power plants,
avionic platforms, and other types of mission critical systems. Physical
fail-safes, e.g., involving electromechanical disconnection of network cables,
or the flooding of a datacenter which holds a rogue AI, provide defense in
depth if software, network, and microarchitectural isolation is compromised and
a rogue AI must be temporarily shut down or permanently destroyed.
</summary>
    <author>
      <name>James Mickens</name>
    </author>
    <author>
      <name>Sarah Radway</name>
    </author>
    <author>
      <name>Ravi Netravali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in the ACM SIGOPS 2025 Workshop on Hot Topics in
  Operating Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.15499v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15499v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.19058v1</id>
    <updated>2025-04-27T00:13:02Z</updated>
    <published>2025-04-27T00:13:02Z</published>
    <title>Scaling Data Center TCP to Terabits with Laminar</title>
    <summary>  Laminar is the first TCP stack designed for the reconfigurable match-action
table (RMT) architecture, widely used in high-speed programmable switches and
SmartNICs. Laminar reimagines TCP processing as a pipeline of simple
match-action operations, enabling line-rate performance with low latency and
minimal energy consumption, while maintaining compatibility with standard TCP
and POSIX sockets. Leveraging novel techniques like optimistic concurrency,
pseudo segment updates, and bump-in-the-wire processing, Laminar handles the
transport logic, including retransmission, reassembly, flow, and congestion
control, entirely within the RMT pipeline.
  We prototype Laminar on an Intel Tofino2 switch and demonstrate its
scalability to terabit speeds, its flexibility, and robustness to network
dynamics. Laminar reaches an unprecedented 25M pkts/sec with a single host core
for streaming workloads, enough to exceed 1.6Tbps with 8K MTU. Laminar delivers
RDMA-equivalent performance, saving up to 16 host CPU cores versus the TAS
kernel-bypass TCP stack with short RPC workloads, while achieving 1.3$\times$
higher peak throughput at 5$\times$ lower 99.99p tail latency. A key-value
store on Laminar doubles the throughput-per-watt versus TAS. Demonstrating
Laminar's flexibility, we implement TCP stack extensions, including a sequencer
API for a linearizable distributed shared log, a new congestion control
protocol, and delayed ACKs.
</summary>
    <author>
      <name>Rajath Shashidhara</name>
    </author>
    <author>
      <name>Antoine Kaufmann</name>
    </author>
    <author>
      <name>Simon Peter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.19058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.19058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.20412v2</id>
    <updated>2025-05-13T18:59:15Z</updated>
    <published>2025-04-29T04:18:51Z</published>
    <title>CrashFixer: A crash resolution agent for the Linux kernel</title>
    <summary>  Code large language models (LLMs) have shown impressive capabilities on a
multitude of software engineering tasks. In particular, they have demonstrated
remarkable utility in the task of code repair. However, common benchmarks used
to evaluate the performance of code LLMs are often limited to small-scale
settings. In this work, we build upon kGym, which shares a benchmark for
system-level Linux kernel bugs and a platform to run experiments on the Linux
kernel.
  This paper introduces CrashFixer, the first LLM-based software repair agent
that is applicable to Linux kernel bugs. Inspired by the typical workflow of a
kernel developer, we identify the key capabilities an expert developer
leverages to resolve a kernel crash. Using this as our guide, we revisit the
kGym platform and identify key system improvements needed to practically run
LLM-based agents at the scale of the Linux kernel (50K files and 20M lines of
code). We implement these changes by extending kGym to create an improved
platform - called kGymSuite, which will be open-sourced. Finally, the paper
presents an evaluation of various repair strategies for such complex kernel
bugs and showcases the value of explicitly generating a hypothesis before
attempting to fix bugs in complex systems such as the Linux kernel. We also
evaluated CrashFixer's capabilities on still open bugs, and found at least two
patch suggestions considered plausible to resolve the reported bug.
</summary>
    <author>
      <name>Alex Mathai</name>
    </author>
    <author>
      <name>Chenxi Huang</name>
    </author>
    <author>
      <name>Suwei Ma</name>
    </author>
    <author>
      <name>Jihwan Kim</name>
    </author>
    <author>
      <name>Hailie Mitchell</name>
    </author>
    <author>
      <name>Aleksandr Nogikh</name>
    </author>
    <author>
      <name>Petros Maniatis</name>
    </author>
    <author>
      <name>Franjo Ivančić</name>
    </author>
    <author>
      <name>Junfeng Yang</name>
    </author>
    <author>
      <name>Baishakhi Ray</name>
    </author>
    <link href="http://arxiv.org/abs/2504.20412v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.20412v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.11554v1</id>
    <updated>2025-05-15T16:40:14Z</updated>
    <published>2025-05-15T16:40:14Z</published>
    <title>Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems</title>
    <summary>  Memory bandwidth regulation and cache partitioning are widely used techniques
for achieving predictable timing in real-time computing systems. Combined with
partitioned scheduling, these methods require careful co-allocation of tasks
and resources to cores, as task execution times strongly depend on available
allocated resources. To address this challenge, this paper presents a 0-1
linear program for task-resource co-allocation, along with a multi-objective
heuristic designed to minimize resource usage while guaranteeing schedulability
under a preemptive EDF scheduling policy. Our heuristic employs a multi-layer
framework, where an outer layer explores resource allocations using
Pareto-pruned search, and an inner layer optimizes task allocation by solving a
knapsack problem using dynamic programming. To evaluate the performance of the
proposed optimization algorithm, we profile real-world benchmarks on an
embedded AMD UltraScale+ ZCU102 platform, with fine-grained resource
partitioning enabled by the Jailhouse hypervisor, leveraging cache set
partitioning and MemGuard for memory bandwidth regulation. Experiments based on
the benchmarking results show that the proposed 0-1 linear program outperforms
existing mixed-integer programs by finding more optimal solutions within the
same time limit. Moreover, the proposed multi-objective multi-layer heuristic
performs consistently better than the state-of-the-art multi-resource-task
co-allocation algorithm in terms of schedulability, resource usage, number of
non-dominated solutions, and computational efficiency.
</summary>
    <author>
      <name>Binqi Sun</name>
    </author>
    <author>
      <name>Zhihang Wei</name>
    </author>
    <author>
      <name>Andrea Bastoni</name>
    </author>
    <author>
      <name>Debayan Roy</name>
    </author>
    <author>
      <name>Mirco Theile</name>
    </author>
    <author>
      <name>Tomasz Kloda</name>
    </author>
    <author>
      <name>Rodolfo Pellizzoni</name>
    </author>
    <author>
      <name>Marco Caccamo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4230/LIPIcs.ECRTS.2025.7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4230/LIPIcs.ECRTS.2025.7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in the 37th Euromicro Conference on Real-Time Systems (ECRTS
  2025)</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.11554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.11554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.12770v1</id>
    <updated>2025-05-19T06:50:28Z</updated>
    <published>2025-05-19T06:50:28Z</published>
    <title>Testing Access-Control Configuration Changes for Web Applications</title>
    <summary>  Access-control misconfigurations are among the main causes of today's data
breaches in web applications. However, few techniques are available to support
automatic and systematic testing for access-control changes and detecting risky
changes to prevent severe consequences. As a result, those critical security
configurations often lack testing, or are tested manually in an ad hoc way.
  This paper advocates that tests should be made available for users to test
access-control configuration changes. The key challenges are such tests need to
be run with production environments (to reason end-to-end behavior) and need to
be performance-efficient. We present a new approach to create such tests, as a
mini test environment incorporating production program and data, called
ACtests. ACtests report the impacts of access-control changes, namely the
requests that were denied but would be allowed after a change, and vice versa.
Users can validate if the changed requests are intended or not and identify
potential security vulnerabilities.
  We evaluate ACtests with 193 public configurations of widely-used web
applications on Dockerhub. ACtests detect 168 new vulnerabilities from 72
configuration images. We report them to the image maintainers: 54 of them have
been confirmed and 44 have been fixed. We also conduct in-depth experiments
with five real-world deployed systems, including Wikipedia and a commercial
company's web proxy. Our results show that ACtests effectively and efficiently
detect all the change impacts.
</summary>
    <author>
      <name>Chengcheng Xiang</name>
    </author>
    <author>
      <name>Li Zhong</name>
    </author>
    <author>
      <name>Eric Mugnier</name>
    </author>
    <author>
      <name>Nathaniel Nguyen</name>
    </author>
    <author>
      <name>Yuanyuan Zhou</name>
    </author>
    <author>
      <name>Tianyin Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2505.12770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.12770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.18829v1</id>
    <updated>2025-05-24T18:56:00Z</updated>
    <published>2025-05-24T18:56:00Z</published>
    <title>LiteCUA: Computer as MCP Server for Computer-Use Agent on AIOS</title>
    <summary>  We present AIOS 1.0, a novel platform designed to advance computer-use agent
(CUA) capabilities through environmental contextualization. While existing
approaches primarily focus on building more powerful agent frameworks or
enhancing agent models, we identify a fundamental limitation: the semantic
disconnect between how language models understand the world and how computer
interfaces are structured. AIOS 1.0 addresses this challenge by transforming
computers into contextual environments that language models can natively
comprehend, implementing a Model Context Protocol (MCP) server architecture to
abstract computer states and actions. This approach effectively decouples
interface complexity from decision complexity, enabling agents to reason more
effectively about computing environments. To demonstrate our platform's
effectiveness, we introduce LiteCUA, a lightweight computer-use agent built on
AIOS 1.0 that achieves a 14.66% success rate on the OSWorld benchmark,
outperforming several specialized agent frameworks despite its simple
architecture. Our results suggest that contextualizing computer environments
for language models represents a promising direction for developing more
capable computer-use agents and advancing toward AI that can interact with
digital systems. The source code of LiteCUA is available at
https://github.com/agiresearch/LiteCUA, and it is also integrated into the AIOS
main branch as part of AIOS at https://github.com/agiresearch/AIOS.
</summary>
    <author>
      <name>Kai Mei</name>
    </author>
    <author>
      <name>Xi Zhu</name>
    </author>
    <author>
      <name>Hang Gao</name>
    </author>
    <author>
      <name>Shuhang Lin</name>
    </author>
    <author>
      <name>Yongfeng Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2505.18829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.18829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701037v3</id>
    <updated>2009-02-24T18:13:25Z</updated>
    <published>2007-01-06T11:36:50Z</published>
    <title>DMTCP: Transparent Checkpointing for Cluster Computations and the
  Desktop</title>
    <summary>  DMTCP (Distributed MultiThreaded CheckPointing) is a transparent user-level
checkpointing package for distributed applications. Checkpointing and restart
is demonstrated for a wide range of over 20 well known applications, including
MATLAB, Python, TightVNC, MPICH2, OpenMPI, and runCMS. RunCMS runs as a 680 MB
image in memory that includes 540 dynamic libraries, and is used for the CMS
experiment of the Large Hadron Collider at CERN. DMTCP transparently
checkpoints general cluster computations consisting of many nodes, processes,
and threads; as well as typical desktop applications. On 128 distributed cores
(32 nodes), checkpoint and restart times are typically 2 seconds, with
negligible run-time overhead. Typical checkpoint times are reduced to 0.2
seconds when using forked checkpointing. Experimental results show that
checkpoint time remains nearly constant as the number of nodes increases on a
medium-size cluster.
  DMTCP automatically accounts for fork, exec, ssh, mutexes/semaphores, TCP/IP
sockets, UNIX domain sockets, pipes, ptys (pseudo-terminals), terminal modes,
ownership of controlling terminals, signal handlers, open file descriptors,
shared open file descriptors, I/O (including the readline library), shared
memory (via mmap), parent-child process relationships, pid virtualization, and
other operating system artifacts. By emphasizing an unprivileged, user-space
approach, compatibility is maintained across Linux kernels from 2.6.9 through
the current 2.6.28. Since DMTCP is unprivileged and does not require special
kernel modules or kernel patches, DMTCP can be incorporated and distributed as
a checkpoint-restart module within some larger package.
</summary>
    <author>
      <name>Jason Ansel</name>
    </author>
    <author>
      <name>Kapil Arya</name>
    </author>
    <author>
      <name>Gene Cooperman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages; 2 figures, 8 plots, and 2 tables; description of DMTCP;
  Version 3: describing checkpointing both for distributed multi-threaded
  applications (including MPI), and interactive shell-like languages on
  desktop; Revised to reflect version published in IPDPS-09; Software at:
  http://dmtcp.sourceforge.net/</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0701037v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701037v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.6354v1</id>
    <updated>2012-12-27T11:55:43Z</updated>
    <published>2012-12-27T11:55:43Z</published>
    <title>LNOS - Live Network Operating System</title>
    <summary>  Operating Systems exists since existence of computers, and have been evolving
continuously from time to time. In this paper we have reviewed a relatively new
or unexplored topic of Live OS. From networking perspective, Live OS is used
for establishing Clusters, Firewalls and as Network security assessment tool
etc. Our proposed concept is that a Live OS can be established or configured
for an organizations specific network requirements with respect to their
servers. An important server failure due to hardware or software could take
time for remedy of the problem, so for that situation a preconfigured server in
the form of Live OS on CD/DVD/USB can be used as an immediate solution. In a
network of ten nodes, we stopped the server machine and with necessary
adjustments, Live OS replaced the server in less than five minutes. Live OS in
a network environment is a quick replacement of the services that are failed
due to server failure (hardware or software). It is a cost effective solution
for low budget networks. The life of Live OS starts when we boot it from
CD/DVD/USB and remains in action for that session. As soon as the machine is
rebooted, any work done for that session is gone, (in case we do not store any
information on permanent storage media). Live CD/DVD/USB is normally used on
systems where we do not have Operating Systems installed. A Live OS can also be
used on systems where we already have an installed OS. On the basis of
functionality a Live OS can be used for many purposes and has some typical
advantages that are not available on other operating systems. Vendors are
releasing different distributions of Live OS and is becoming their sole
identity in a particular domain like Networks, Security, Education or
Entertainment etc. There can be many aspects of Live OS, but Linux based Live
OS and their use in the field of networks is the main focus of this paper.
</summary>
    <author>
      <name>Sajjad Haider</name>
    </author>
    <author>
      <name>Mehboob Yasin</name>
    </author>
    <author>
      <name>Naveed Hussain</name>
    </author>
    <author>
      <name>Muhammad Imran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6th CIIT Workshop on Research in Computing (CWRC, 2007)</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.6354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.6354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.1459v1</id>
    <updated>2013-05-07T10:22:31Z</updated>
    <published>2013-05-07T10:22:31Z</published>
    <title>EURETILE 2010-2012 summary: first three years of activity of the
  European Reference Tiled Experiment</title>
    <summary>  This is the summary of first three years of activity of the EURETILE FP7
project 247846. EURETILE investigates and implements brain-inspired and
fault-tolerant foundational innovations to the system architecture of massively
parallel tiled computer architectures and the corresponding programming
paradigm. The execution targets are a many-tile HW platform, and a many-tile
simulator. A set of SW process - HW tile mapping candidates is generated by the
holistic SW tool-chain using a combination of analytic and bio-inspired
methods. The Hardware dependent Software is then generated, providing OS
services with maximum efficiency/minimal overhead. The many-tile simulator
collects profiling data, closing the loop of the SW tool chain. Fine-grain
parallelism inside processes is exploited by optimized intra-tile compilation
techniques, but the project focus is above the level of the elementary tile.
The elementary HW tile is a multi-processor, which includes a fault tolerant
Distributed Network Processor (for inter-tile communication) and ASIP
accelerators. Furthermore, EURETILE investigates and implements the innovations
for equipping the elementary HW tile with high-bandwidth, low-latency
brain-like inter-tile communication emulating 3 levels of connection hierarchy,
namely neural columns, cortical areas and cortex, and develops a dedicated
cortical simulation benchmark: DPSNN-STDP (Distributed Polychronous Spiking
Neural Net with synaptic Spiking Time Dependent Plasticity). EURETILE leverages
on the multi-tile HW paradigm and SW tool-chain developed by the FET-ACA SHAPES
Integrated Project (2006-2009).
</summary>
    <author>
      <name>Pier Stanislao Paolucci</name>
    </author>
    <author>
      <name>Iuliana Bacivarov</name>
    </author>
    <author>
      <name>Gert Goossens</name>
    </author>
    <author>
      <name>Rainer Leupers</name>
    </author>
    <author>
      <name>Frédéric Rousseau</name>
    </author>
    <author>
      <name>Christoph Schumacher</name>
    </author>
    <author>
      <name>Lothar Thiele</name>
    </author>
    <author>
      <name>Piero Vicini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.12837/2013T01</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.12837/2013T01" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">56 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.1459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.4; C.3; B.7.2; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.6037v1</id>
    <updated>2014-06-23T19:44:03Z</updated>
    <published>2014-06-23T19:44:03Z</published>
    <title>Preemptive Thread Block Scheduling with Online Structural Runtime
  Prediction for Concurrent GPGPU Kernels</title>
    <summary>  Recent NVIDIA Graphics Processing Units (GPUs) can execute multiple kernels
concurrently. On these GPUs, the thread block scheduler (TBS) uses the FIFO
policy to schedule their thread blocks. We show that FIFO leaves performance to
chance, resulting in significant loss of performance and fairness. To improve
performance and fairness, we propose use of the preemptive Shortest Remaining
Time First (SRTF) policy instead. Although SRTF requires an estimate of runtime
of GPU kernels, we show that such an estimate of the runtime can be easily
obtained using online profiling and exploiting a simple observation on GPU
kernels' grid structure. Specifically, we propose a novel Structural Runtime
Predictor. Using a simple Staircase model of GPU kernel execution, we show that
the runtime of a kernel can be predicted by profiling only the first few thread
blocks. We evaluate an online predictor based on this model on benchmarks from
ERCBench, and find that it can estimate the actual runtime reasonably well
after the execution of only a single thread block. Next, we design a thread
block scheduler that is both concurrent kernel-aware and uses this predictor.
We implement the SRTF policy and evaluate it on two-program workloads from
ERCBench. SRTF improves STP by 1.18x and ANTT by 2.25x over FIFO. When compared
to MPMax, a state-of-the-art resource allocation policy for concurrent kernels,
SRTF improves STP by 1.16x and ANTT by 1.3x. To improve fairness, we also
propose SRTF/Adaptive which controls resource usage of concurrently executing
kernels to maximize fairness. SRTF/Adaptive improves STP by 1.12x, ANTT by
2.23x and Fairness by 2.95x compared to FIFO. Overall, our implementation of
SRTF achieves system throughput to within 12.64% of Shortest Job First (SJF, an
oracle optimal scheduling policy), bridging 49% of the gap between FIFO and
SJF.
</summary>
    <author>
      <name>Sreepathi Pai</name>
    </author>
    <author>
      <name>R. Govindarajan</name>
    </author>
    <author>
      <name>Matthew J. Thazhuthaveetil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, full pre-review version of PACT 2014 poster</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.6037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.3; C.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00544v1</id>
    <updated>2017-08-01T22:48:06Z</updated>
    <published>2017-08-01T22:48:06Z</published>
    <title>Performance Measurements of Supercomputing and Cloud Storage Solutions</title>
    <summary>  Increasing amounts of data from varied sources, particularly in the fields of
machine learning and graph analytics, are causing storage requirements to grow
rapidly. A variety of technologies exist for storing and sharing these data,
ranging from parallel file systems used by supercomputers to distributed block
storage systems found in clouds. Relatively few comparative measurements exist
to inform decisions about which storage systems are best suited for particular
tasks. This work provides these measurements for two of the most popular
storage technologies: Lustre and Amazon S3. Lustre is an open-source, high
performance, parallel file system used by many of the largest supercomputers in
the world. Amazon's Simple Storage Service, or S3, is part of the Amazon Web
Services offering, and offers a scalable, distributed option to store and
retrieve data from anywhere on the Internet. Parallel processing is essential
for achieving high performance on modern storage systems. The performance tests
used span the gamut of parallel I/O scenarios, ranging from single-client,
single-node Amazon S3 and Lustre performance to a large-scale, multi-client
test designed to demonstrate the capabilities of a modern storage appliance
under heavy load. These results show that, when parallel I/O is used correctly
(i.e., many simultaneous read or write processes), full network bandwidth
performance is achievable and ranged from 10 gigabits/s over a 10 GigE S3
connection to 0.35 terabits/s using Lustre on a 1200 port 10 GigE switch. These
results demonstrate that S3 is well-suited to sharing vast quantities of data
over the Internet, while Lustre is well-suited to processing large quantities
of data locally.
</summary>
    <author>
      <name>Michael Jones</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>William Arcand</name>
    </author>
    <author>
      <name>David Bestor</name>
    </author>
    <author>
      <name>Bill Bergeron</name>
    </author>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Michael Houle</name>
    </author>
    <author>
      <name>Matthew Hubbell</name>
    </author>
    <author>
      <name>Peter Michaleas</name>
    </author>
    <author>
      <name>Andrew Prout</name>
    </author>
    <author>
      <name>Albert Reuther</name>
    </author>
    <author>
      <name>Siddharth Samsi</name>
    </author>
    <author>
      <name>Paul Monticiollo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HPEC.2017.8091073</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HPEC.2017.8091073" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, to appear in IEEE HPEC 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.03951v1</id>
    <updated>2018-03-11T12:09:27Z</updated>
    <published>2018-03-11T12:09:27Z</published>
    <title>The Secure Machine: Efficient Secure Execution On Untrusted Platforms</title>
    <summary>  In this work we present the Secure Machine, SeM for short, a CPU architecture
extension for secure computing. SeM uses a small amount of in-chip additional
hardware that monitors key communication channels inside the CPU chip, and only
acts when required. SeM provides confidentiality and integrity for a secure
program without trusting the platform software or any off-chip hardware. SeM
supports existing binaries of single- and multi-threaded applications running
on single- or multi-core, multi-CPU. The performance reduction caused by it is
only few percent, most of which is due to the memory encryption layer that is
commonly used in many secure architectures.
  We also developed SeM-Prepare, a software tool that automatically instruments
existing applications (binaries) with additional instructions so they can be
securely executed on our architecture without requiring any programming efforts
or the availability of the desired program`s source code.
  To enable secure data sharing in shared memory environments, we developed
Secure Distributed Shared Memory (SDSM), an efficient (time and memory)
algorithm for allowing thousands of compute nodes to share data securely while
running on an untrusted computing environment. SDSM shows a negligible
reduction in performance, and it requires negligible and hardware resources. We
developed Distributed Memory Integrity Trees, a method for enhancing single
node integrity trees for preserving the integrity of a distributed application
running on an untrusted computing environment. We show that our method is
applicable to existing single node integrity trees such as Merkle Tree, Bonsai
Merkle Tree, and Intel`s SGX memory integrity engine. All these building blocks
may be used together to form a practical secure system, and some can be used in
conjunction with other secure systems.
</summary>
    <author>
      <name>Ofir Shwartz</name>
    </author>
    <author>
      <name>Yitzhak Birk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A PhD thesis, to appear at the Technion's library</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.03951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.03951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.13245v2</id>
    <updated>2020-06-18T01:55:53Z</updated>
    <published>2019-07-30T22:07:59Z</published>
    <title>EnclaveDom: Privilege Separation for Large-TCB Applications in Trusted
  Execution Environments</title>
    <summary>  Trusted executions environments (TEEs) such as Intel(R) SGX provide
hardware-isolated execution areas in memory, called enclaves. By running only
the most trusted application components in the enclave, TEEs enable developers
to minimize the TCB of their applications thereby helping to protect sensitive
application data. However, porting existing applications to TEEs often requires
considerable refactoring efforts, as TEEs provide a restricted interface to
standard OS features. To ease development efforts, TEE application developers
often choose to run their unmodified application in a library OS container that
provides a full in-enclave OS interface. Yet, this large-TCB development
approach now leaves sensitive in-enclave data exposed to potential bugs or
vulnerabilities in third-party code imported into the application. Importantly,
because the TEE libOS and the application run in the same enclave address
space, even the libOS management data structures (e.g. file descriptor table)
may be vulnerable to attack, where in traditional OSes these data structures
may be protected via privilege isolation.
  We present EnclaveDom, a privilege separation system for large-TCB TEE
applications that partitions an enclave into tagged memory regions, and
enforces per-region access rules at the granularity of individual in-enclave
functions. EnclaveDom is implemented on Intel SGX using Memory Protection Keys
(MPK) for memory tagging. To evaluate the security and performance impact of
EnclaveDom, we integrated EnclaveDom with the Graphene-SGX library OS. While no
product or component can be absolutely secure, our prototype helps protect
internal libOS management data structures against tampering by
application-level code. At every libOS system call, EnclaveDom then only grants
access to those internal data structures which the syscall needs to perform its
task.
</summary>
    <author>
      <name>Marcela S. Melara</name>
    </author>
    <author>
      <name>Michael J. Freedman</name>
    </author>
    <author>
      <name>Mic Bowman</name>
    </author>
    <link href="http://arxiv.org/abs/1907.13245v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.13245v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.10666v2</id>
    <updated>2020-10-12T06:33:43Z</updated>
    <published>2019-12-23T07:58:04Z</published>
    <title>ARM Pointer Authentication based Forward-Edge and Backward-Edge Control
  Flow Integrity for Kernels</title>
    <summary>  Code reuse attacks are still big threats to software and system security.
Control flow integrity is a promising technique to defend against such attacks.
However, its effectiveness has been weakened due to the inaccurate control flow
graph and practical strategy to trade security for performance. In recent
years, CPU vendors have integrated hardware features as countermeasures. For
instance, ARM Pointer Authentication (PA in short) was introduced in ARMV8-A
architecture. It can efficiently generate an authentication code for an
address, which is encoded in the unused bits of the address. When the address
is de-referenced, the authentication code is checked to ensure its integrity.
Though there exist systems that adopt PA to harden user programs, how to
effectively use PA to protect OS kernels is still an open research question.
  In this paper, we shed lights on how to leverage PA to protect control flows,
including function pointers and return addresses, of Linux kernel.
Specifically, to protect function pointers, we embed authentication code into
them, track their propagation and verify their values when loading from memory
or branching to targets. To further defend against the pointer substitution
attack, we use the function pointer address as its context, and take a clean
design to propagate the address by piggybacking it into the pointer value. We
have implemented a prototype system with LLVM to identify function pointers,
add authentication code and verify function pointers by emitting new machine
instructions. We applied this system to Linux kernel, and solved numerous
practical issues, e.g., function pointer comparison and arithmetic operations.
The security analysis shows that our system can protect all function pointers
and return addresses in Linux kernel.
</summary>
    <author>
      <name>Yutian Yang</name>
    </author>
    <author>
      <name>Songbo Zhu</name>
    </author>
    <author>
      <name>Wenbo Shen</name>
    </author>
    <author>
      <name>Yajin Zhou</name>
    </author>
    <author>
      <name>Jiadong Sun</name>
    </author>
    <author>
      <name>Kui Ren</name>
    </author>
    <link href="http://arxiv.org/abs/1912.10666v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.10666v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.08364v1</id>
    <updated>2020-03-11T04:03:40Z</updated>
    <published>2020-03-11T04:03:40Z</published>
    <title>Dynamic Budget Management with Service Guarantees for Mixed-Criticality
  Systems</title>
    <summary>  Many existing studies on mixed-criticality (MC) scheduling assume that
low-criticality budgets for high-criticality applications are known apriori.
These budgets are primarily used as guidance to determine when the scheduler
should switch the system mode from low to high. Based on this key observation,
in this paper we propose a dynamic MC scheduling model under which
low-criticality budgets for individual high-criticality applications are
determined at runtime as opposed to being fixed offline. To ensure sufficient
budget for high-criticality applications at all times, we use offline
schedulability analysis to determine a system-wide total low-criticality budget
allocation for all the high-criticality applications combined. This total
budget is used as guidance in our model to determine the need for a
mode-switch. The runtime strategy then distributes this total budget among the
various applications depending on their execution requirement and with the
objective of postponing mode-switch as much as possible. We show that this
runtime strategy is able to postpone mode-switches for a longer time than any
strategy that uses a fixed low-criticality budget allocation for each
application. Finally, since we are able to control the total budget allocation
for high-criticality applications before mode-switch, we also propose
techniques to determine these budgets considering system-wide objectives such
as schedulability and service guarantee for low-criticality applications.
</summary>
    <author>
      <name>Xiaozhe Gu</name>
    </author>
    <author>
      <name>Arvind Easwaran</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/RTSS.2016.014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/RTSS.2016.014" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">\copyright 2016 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Real-Time Systems Symposium (RTSS), Porto, Portugal, 2016,
  pages 47-56</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.08364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.08364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.04750v1</id>
    <updated>2020-05-10T18:53:02Z</updated>
    <published>2020-05-10T18:53:02Z</published>
    <title>Exploiting Inter- and Intra-Memory Asymmetries for Data Mapping in
  Hybrid Tiered-Memories</title>
    <summary>  Modern computing systems are embracing hybrid memory comprising of DRAM and
non-volatile memory (NVM) to combine the best properties of both memory
technologies, achieving low latency, high reliability, and high density. A
prominent characteristic of DRAM-NVM hybrid memory is that it has NVM access
latency much higher than DRAM access latency. We call this inter-memory
asymmetry. We observe that parasitic components on a long bitline are a major
source of high latency in both DRAM and NVM, and a significant factor
contributing to high-voltage operations in NVM, which impact their reliability.
We propose an architectural change, where each long bitline in DRAM and NVM is
split into two segments by an isolation transistor. One segment can be accessed
with lower latency and operating voltage than the other. By introducing tiers,
we enable non-uniform accesses within each memory type (which we call
intra-memory asymmetry), leading to performance and reliability trade-offs in
DRAM-NVM hybrid memory. We extend existing NVM-DRAM OS in three ways. First, we
exploit both inter- and intra-memory asymmetries to allocate and migrate memory
pages between the tiers in DRAM and NVM. Second, we improve the OS's page
allocation decisions by predicting the access intensity of a newly-referenced
memory page in a program and placing it to a matching tier during its initial
allocation. This minimizes page migrations during program execution, lowering
the performance overhead. Third, we propose a solution to migrate pages between
the tiers of the same memory without transferring data over the memory channel,
minimizing channel occupancy and improving performance. Our overall approach,
which we call MNEME, to enable and exploit asymmetries in DRAM-NVM hybrid
tiered memory improves both performance and reliability for both single-core
and multi-programmed workloads.
</summary>
    <author>
      <name>Shihao Song</name>
    </author>
    <author>
      <name>Anup Das</name>
    </author>
    <author>
      <name>Nagarajan Kandasamy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3381898.3397215</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3381898.3397215" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 29 figures, accepted at ACM SIGPLAN International Symposium
  on Memory Management</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.04750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.04750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.04753v1</id>
    <updated>2020-05-10T19:07:08Z</updated>
    <published>2020-05-10T19:07:08Z</published>
    <title>Improving Phase Change Memory Performance with Data Content Aware Access</title>
    <summary>  A prominent characteristic of write operation in Phase-Change Memory (PCM) is
that its latency and energy are sensitive to the data to be written as well as
the content that is overwritten. We observe that overwriting unknown memory
content can incur significantly higher latency and energy compared to
overwriting known all-zeros or all-ones content. This is because all-zeros or
all-ones content is overwritten by programming the PCM cells only in one
direction, i.e., using either SET or RESET operations, not both. In this paper,
we propose data content aware PCM writes (DATACON), a new mechanism that
reduces the latency and energy of PCM writes by redirecting these requests to
overwrite memory locations containing all-zeros or all-ones. DATACON operates
in three steps. First, it estimates how much a PCM write access would benefit
from overwriting known content (e.g., all-zeros, or all-ones) by
comprehensively considering the number of set bits in the data to be written,
and the energy-latency trade-offs for SET and RESET operations in PCM. Second,
it translates the write address to a physical address within memory that
contains the best type of content to overwrite, and records this translation in
a table for future accesses. We exploit data access locality in workloads to
minimize the address translation overhead. Third, it re-initializes unused
memory locations with known all-zeros or all-ones content in a manner that does
not interfere with regular read and write accesses. DATACON overwrites unknown
content only when it is absolutely necessary to do so. We evaluate DATACON with
workloads from state-of-the-art machine learning applications, SPEC CPU2017,
and NAS Parallel Benchmarks. Results demonstrate that DATACON significantly
improves system performance and memory system energy consumption compared to
the best of performance-oriented state-of-the-art techniques.
</summary>
    <author>
      <name>Shihao Song</name>
    </author>
    <author>
      <name>Anup Das</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <author>
      <name>Nagarajan Kandasamy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3381898.3397210</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3381898.3397210" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 21 figures, accepted at ACM SIGPLAN International Symposium
  on Memory Management (ISMM)</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.04753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.04753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.10333v2</id>
    <updated>2021-04-22T08:57:18Z</updated>
    <published>2020-05-20T19:54:38Z</published>
    <title>A Way Around UMIP and Descriptor-Table Exiting via TSX-based
  Side-Channel</title>
    <summary>  Nowadays, in operating systems, numerous protection mechanisms prevent or
limit the user-mode applicationsto access the kernels internal information.
This is regularlycarried out by software-based defenses such as Address Space
Layout Randomization (ASLR) and Kernel ASLR(KASLR). They play pronounced roles
when the security of sandboxed applications such as Web-browser are
considered.Armed with arbitrary write access in the kernel memory, if these
protections are bypassed, an adversary could find a suitable where to write in
order to get an elevation of privilege or code execution in ring 0. In this
paper, we introduce a reliable method based on Transactional Synchronization
Extensions (TSX) side-channel leakage to reveal the address of the Global
Descriptor Table (GDT) and Interrupt Descriptor Table (IDT). We indicate that
by detecting these addresses, one could execute instructions to sidestep the
Intels User-Mode InstructionPrevention (UMIP) and the Hypervisor-based
mitigation and, consequently, neutralized them. The introduced method is
successfully performed after the most recent patches for Meltdown and Spectre.
Moreover, the implementation of the proposed approach on different platforms,
including the latest releases of Microsoft Windows, Linux, and, Mac OSX with
the latest 9th generation of Intel processors, shows that the proposed
mechanism is independent from the Operating System implementation. We
demonstrate that a combinationof this method with call-gate mechanism
(available in modernprocessors) in a chain of events will eventually lead toa
system compromise despite the limitations of a super-secure sandboxed
environment in the presence of Windows proprietary Virtualization Based
Security (VBS). Finally, we suggest the software-based mitigation to avoid
these issues with an acceptable overhead cost.
</summary>
    <author>
      <name>Mohammad Sina Karvandi</name>
    </author>
    <author>
      <name>Saleh Khalaj Monfared</name>
    </author>
    <author>
      <name>Mohammad Sina Kiarostami</name>
    </author>
    <author>
      <name>Dara Rahmati</name>
    </author>
    <author>
      <name>Saeid Gorgin</name>
    </author>
    <link href="http://arxiv.org/abs/2005.10333v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.10333v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.12133v2</id>
    <updated>2020-06-23T01:28:19Z</updated>
    <published>2020-06-22T10:37:40Z</published>
    <title>Optimizing Placement of Heap Memory Objects in Energy-Constrained Hybrid
  Memory Systems</title>
    <summary>  Main memory (DRAM) significantly impacts the power and energy utilization of
the overall server system. Non-Volatile Memory (NVM) devices, such as Phase
Change Memory and Spin-Transfer Torque RAM, are suitable candidates for main
memory to reduce energy consumption. But unlike DRAM, NVMs access latencies are
higher than DRAM and NVM writes are more energy sensitive than DRAM write
operations. Thus, Hybrid Main Memory Systems (HMMS) employing DRAM and NVM have
been proposed to reduce the overall energy depletion of main memory while
optimizing the performance of NVM. This paper proposes eMap, an optimal heap
memory object placement planner in HMMS. eMap considers the object-level access
patterns and energy consumption at the application level and provides an ideal
placement strategy for each object to augment performance and energy
utilization. eMap is equipped with two modules, eMPlan and eMDyn. Specifically,
eMPlan is a static placement planner which provides one time placement policies
for memory object to meet the energy budget while eMDyn is a runtime placement
planner to consider the change in energy limiting constraint during the runtime
and shuffles the memory objects by taking into account the access patterns as
well as the migration cost in terms of energy and performance. The evaluation
shows that our proposed solution satisfies both the energy limiting constraint
and the performance. We compare our methodology with the state-of-the-art
memory object classification and allocation (MOCA) framework. Our extensive
evaluation shows that our proposed solution, eMPlan meets the energy constraint
with 4.17 times less costly and reducing the energy consumption up to 14% with
the same performance. eMDyn also satisfies the performance and energy
requirement while considering the migration cost in terms of time and energy.
</summary>
    <author>
      <name>Taeuk Kim</name>
    </author>
    <author>
      <name>Safdar Jamil</name>
    </author>
    <author>
      <name>Joongeon Park</name>
    </author>
    <author>
      <name>Youngjae Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2006.12133v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.12133v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.04552v2</id>
    <updated>2021-03-04T16:14:42Z</updated>
    <published>2020-07-09T04:45:54Z</published>
    <title>IOCA: High-Speed I/O-Aware LLC Management for Network-Centric
  Multi-Tenant Platform</title>
    <summary>  In modern server CPUs, last-level cache (LLC) is a critical hardware resource
that exerts significant influence on the performance of the workloads, and how
to manage LLC is a key to the performance isolation and QoS in the cloud with
multi-tenancy. In this paper, we argue that besides CPU cores, high-speed
network I/O is also important for LLC management. This is because of an Intel
architectural innovation -- Data Direct I/O (DDIO) -- that directly injects the
inbound I/O traffic to (part of) the LLC instead of the main memory. We
summarize two problems caused by DDIO and show that (1) the default DDIO
configuration may not always achieve optimal performance, (2) DDIO can decrease
the performance of non-I/O workloads which share LLC with it by as high as 32%.
  We then present IOCA, the first LLC management mechanism for network-centric
platforms that treats the I/O as the first-class citizen. IOCA monitors and
analyzes the performance of the cores, LLC, and DDIO using CPU's hardware
performance counters, and adaptively adjusts the number of LLC ways for DDIO or
the tenants that demand more LLC capacity. In addition, IOCA dynamically
chooses the tenants that share its LLC resource with DDIO, to minimize the
performance interference by both the tenants and the I/O. Our experiments with
multiple microbenchmarks and real-world applications in two major end-host
network models demonstrate that IOCA can effectively reduce the performance
degradation caused by DDIO, with minimal overhead.
</summary>
    <author>
      <name>Yifan Yuan</name>
    </author>
    <author>
      <name>Mohammad Alian</name>
    </author>
    <author>
      <name>Yipeng Wang</name>
    </author>
    <author>
      <name>Ilia Kurakin</name>
    </author>
    <author>
      <name>Ren Wang</name>
    </author>
    <author>
      <name>Charlie Tai</name>
    </author>
    <author>
      <name>Nam Sung Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by the 48th IEEE/ACM International Symposium on Computer
  Architecture (ISCA'21). The title is "Don't Forget the I/O When Allocating
  Your LLC"</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.04552v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.04552v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.11118v1</id>
    <updated>2021-12-21T11:45:13Z</updated>
    <published>2021-12-21T11:45:13Z</published>
    <title>Toolset for Collecting Shell Commands and Its Application in Hands-on
  Cybersecurity Training</title>
    <summary>  When learning cybersecurity, operating systems, or networking, students
perform practical tasks using a broad range of command-line tools. Collecting
and analyzing data about the command usage can reveal valuable insights into
how students progress and where they make mistakes. However, few learning
environments support recording and inspecting command-line inputs, and setting
up an efficient infrastructure for this purpose is challenging. To aid
engineering and computing educators, we share the design and implementation of
an open-source toolset for logging commands that students execute on Linux
machines. Compared to basic solutions, such as shell history files, the
toolset's added value is threefold. 1) Its configuration is automated so that
it can be easily used in classes on different topics. 2) It collects metadata
about the command execution, such as a timestamp, hostname, and IP address. 3)
Data are instantly forwarded to central storage in a unified, semi-structured
format. This enables automated processing, both in real-time and post hoc, to
enhance the instructors' understanding of student actions. The toolset works
independently of the teaching content, the training network's topology, or the
number of students working in parallel. We demonstrated the toolset's value in
two learning environments at four training sessions. Over two semesters, 50
students played educational cybersecurity games using a Linux command-line
interface. Each training session lasted approximately two hours, during which
we recorded 4439 shell commands. The semi-automated data analysis revealed
solution patterns, used tools, and misconceptions of students. Our insights
from creating the toolset and applying it in teaching practice are relevant for
instructors, researchers, and developers of learning environments. We provide
the software and data resulting from this work so that others can use them.
</summary>
    <author>
      <name>Valdemar Švábenský</name>
    </author>
    <author>
      <name>Jan Vykopal</name>
    </author>
    <author>
      <name>Daniel Tovarňák</name>
    </author>
    <author>
      <name>Pavel Čeleda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/FIE49875.2021.9637052</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/FIE49875.2021.9637052" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE FIE 2021 conference, 9 pages, 5 figure, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.11118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.11118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.04910v3</id>
    <updated>2023-02-06T20:18:16Z</updated>
    <published>2022-03-09T17:44:56Z</published>
    <title>GPU-Initiated On-Demand High-Throughput Storage Access in the BaM System
  Architecture</title>
    <summary>  Graphics Processing Units (GPUs) have traditionally relied on the host CPU to
initiate access to the data storage. This approach is well-suited for GPU
applications with known data access patterns that enable partitioning of their
dataset to be processed in a pipelined fashion in the GPU. However, emerging
applications such as graph and data analytics, recommender systems, or graph
neural networks, require fine-grained, data-dependent access to storage. CPU
initiation of storage access is unsuitable for these applications due to high
CPU-GPU synchronization overheads, I/O traffic amplification, and long CPU
processing latencies. GPU-initiated storage removes these overheads from the
storage control path and, thus, can potentially support these applications at
much higher speed. However, there is a lack of systems architecture and
software stack that enable efficient GPU-initiated storage access. This work
presents a novel system architecture, BaM, that fills this gap. BaM features a
fine-grained software cache to coalesce data storage requests while minimizing
I/O traffic amplification. This software cache communicates with the storage
system via high-throughput queues that enable the massive number of concurrent
threads in modern GPUs to make I/O requests at a high rate to fully utilize the
storage devices and the system interconnect. Experimental results show that BaM
delivers 1.0x and 1.49x end-to-end speed up for BFS and CC graph analytics
benchmarks while reducing hardware costs by up to 21.7x over accessing the
graph data from the host memory. Furthermore, BaM speeds up data-analytics
workloads by 5.3x over CPU-initiated storage access on the same hardware.
</summary>
    <author>
      <name>Zaid Qureshi</name>
    </author>
    <author>
      <name>Vikram Sharma Mailthody</name>
    </author>
    <author>
      <name>Isaac Gelado</name>
    </author>
    <author>
      <name>Seung Won Min</name>
    </author>
    <author>
      <name>Amna Masood</name>
    </author>
    <author>
      <name>Jeongmin Park</name>
    </author>
    <author>
      <name>Jinjun Xiong</name>
    </author>
    <author>
      <name>CJ Newburn</name>
    </author>
    <author>
      <name>Dmitri Vainbrand</name>
    </author>
    <author>
      <name>I-Hsin Chung</name>
    </author>
    <author>
      <name>Michael Garland</name>
    </author>
    <author>
      <name>William Dally</name>
    </author>
    <author>
      <name>Wen-mei Hwu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3575693.3575748</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3575693.3575748" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an extension to the published conference paper at ASPLOS'23:
  https://dl.acm.org/doi/abs/10.1145/3575693.3575748</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ASPLOS 2023: Proceedings of the 28th ACM International Conference
  on Architectural Support for Programming Languages and Operating Systems,
  Volume 2</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2203.04910v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.04910v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.13396v1</id>
    <updated>2022-03-25T00:24:44Z</updated>
    <published>2022-03-25T00:24:44Z</published>
    <title>HetSched: Quality-of-Mission Aware Scheduling for Autonomous Vehicle
  SoCs</title>
    <summary>  Systems-on-Chips (SoCs) that power autonomous vehicles (AVs) must meet
stringent performance and safety requirements prior to deployment. With
increasing complexity in AV applications, the system needs to meet these
real-time demands of multiple safety-critical applications simultaneously. A
typical AV-SoC is a heterogeneous multiprocessor consisting of accelerators
supported by general-purpose cores. Such heterogeneity, while needed for
power-performance efficiency, complicates the art of task scheduling.
  In this paper, we demonstrate that hardware heterogeneity impacts the
scheduler's effectiveness and that optimizing for only the real-time aspect of
applications is not sufficient in AVs. Therefore, a more holistic approach is
required -- one that considers global Quality-of-Mission (QoM) metrics, as
defined in the paper. We then propose HetSched, a multi-step scheduler that
leverages dynamic runtime information about the underlying heterogeneous
hardware platform, along with the applications' real-time constraints and the
task traffic in the system to optimize overall mission performance. HetSched
proposes two scheduling policies: MSstat and MSdyn and scheduling optimizations
like task pruning, hybrid heterogeneous ranking and rank update. HetSched
improves overall mission performance on average by 4.6x, 2.6x and 2.6x when
compared against CPATH, ADS and 2lvl-EDF (state-of-the-art real-time schedulers
built for heterogeneous systems), respectively, and achieves an average of
53.3% higher hardware utilization, while meeting 100% critical deadlines for
real-world applications of autonomous vehicles. Furthermore, when used as part
of an SoC design space exploration loop, in comparison to prior schedulers,
HetSched reduces the number of processing elements required by an SoC to safely
complete AV's missions by 35% on average while achieving 2.7x lower
energy-mission time product.
</summary>
    <author>
      <name>Aporva Amarnath</name>
    </author>
    <author>
      <name>Subhankar Pal</name>
    </author>
    <author>
      <name>Hiwot Kassa</name>
    </author>
    <author>
      <name>Augusto Vega</name>
    </author>
    <author>
      <name>Alper Buyuktosunoglu</name>
    </author>
    <author>
      <name>Hubertus Franke</name>
    </author>
    <author>
      <name>John-David Wellman</name>
    </author>
    <author>
      <name>Ronald Dreslinski</name>
    </author>
    <author>
      <name>Pradip Bose</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 11 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.13396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.13396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.13934v2</id>
    <updated>2022-09-05T15:34:47Z</updated>
    <published>2022-03-25T23:28:43Z</published>
    <title>GraphBLAS on the Edge: Anonymized High Performance Streaming of Network
  Traffic</title>
    <summary>  Long range detection is a cornerstone of defense in many operating domains
(land, sea, undersea, air, space, ..,). In the cyber domain, long range
detection requires the analysis of significant network traffic from a variety
of observatories and outposts. Construction of anonymized hypersparse traffic
matrices on edge network devices can be a key enabler by providing significant
data compression in a rapidly analyzable format that protects privacy.
GraphBLAS is ideally suited for both constructing and analyzing anonymized
hypersparse traffic matrices. The performance of GraphBLAS on an Accolade
Technologies edge network device is demonstrated on a near worse case traffic
scenario using a continuous stream of CAIDA Telescope darknet packets. The
performance for varying numbers of traffic buffers, threads, and processor
cores is explored. Anonymized hypersparse traffic matrices can be constructed
at a rate of over 50,000,000 packets per second; exceeding a typical 400
Gigabit network link. This performance demonstrates that anonymized hypersparse
traffic matrices are readily computable on edge network devices with minimal
compute resources and can be a viable data product for such devices.
</summary>
    <author>
      <name>Michael Jones</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Daniel Andersen</name>
    </author>
    <author>
      <name>Aydin Buluc</name>
    </author>
    <author>
      <name>Chansup Byun</name>
    </author>
    <author>
      <name>K Claffy</name>
    </author>
    <author>
      <name>Timothy Davis</name>
    </author>
    <author>
      <name>William Arcand</name>
    </author>
    <author>
      <name>Jonathan Bernays</name>
    </author>
    <author>
      <name>David Bestor</name>
    </author>
    <author>
      <name>William Bergeron</name>
    </author>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Micheal Houle</name>
    </author>
    <author>
      <name>Matthew Hubbell</name>
    </author>
    <author>
      <name>Hayden Jananthan</name>
    </author>
    <author>
      <name>Anna Klein</name>
    </author>
    <author>
      <name>Chad Meiners</name>
    </author>
    <author>
      <name>Lauren Milechin</name>
    </author>
    <author>
      <name>Julie Mullen</name>
    </author>
    <author>
      <name>Sandeep Pisharody</name>
    </author>
    <author>
      <name>Andrew Prout</name>
    </author>
    <author>
      <name>Albert Reuther</name>
    </author>
    <author>
      <name>Antonio Rosa</name>
    </author>
    <author>
      <name>Siddharth Samsi</name>
    </author>
    <author>
      <name>Jon Sreekanth</name>
    </author>
    <author>
      <name>Doug Stetson</name>
    </author>
    <author>
      <name>Charles Yee</name>
    </author>
    <author>
      <name>Peter Michaleas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HPEC55821.2022.9926332</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HPEC55821.2022.9926332" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE HPEC, Outstanding Paper Award, 8 pages, 8 figures, 1
  table, 70 references. arXiv admin note: text overlap with arXiv:2108.06653,
  arXiv:2008.00307, arXiv:2203.10230</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.13934v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.13934v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.10400v2</id>
    <updated>2023-04-06T16:56:04Z</updated>
    <published>2022-02-21T17:53:01Z</published>
    <title>GenStore: A High-Performance and Energy-Efficient In-Storage Computing
  System for Genome Sequence Analysis</title>
    <summary>  Read mapping is a fundamental, yet computationally-expensive step in many
genomics applications. It is used to identify potential matches and differences
between fragments (called reads) of a sequenced genome and an already known
genome (called a reference genome). To address the computational challenges in
genome analysis, many prior works propose various approaches such as filters
that select the reads that must undergo expensive computation, efficient
heuristics, and hardware acceleration. While effective at reducing the
computation overhead, all such approaches still require the costly movement of
a large amount of data from storage to the rest of the system, which can
significantly lower the end-to-end performance of read mapping in conventional
and emerging genomics systems.
  We propose GenStore, the first in-storage processing system designed for
genome sequence analysis that greatly reduces both data movement and
computational overheads of genome sequence analysis by exploiting low-cost and
accurate in-storage filters. GenStore leverages hardware/software co-design to
address the challenges of in-storage processing, supporting reads with 1)
different read lengths and error rates, and 2) different degrees of genetic
variation. Through rigorous analysis of read mapping processes, we meticulously
design low-cost hardware accelerators and data/computation flows inside a NAND
flash-based SSD. Our evaluation using a wide range of real genomic datasets
shows that GenStore, when implemented in three modern SSDs, significantly
improves the read mapping performance of state-of-the-art software (hardware)
baselines by 2.07-6.05$\times$ (1.52-3.32$\times$) for read sets with high
similarity to the reference genome and 1.45-33.63$\times$ (2.70-19.2$\times$)
for read sets with low similarity to the reference genome.
</summary>
    <author>
      <name>Nika Mansouri Ghiasi</name>
    </author>
    <author>
      <name>Jisung Park</name>
    </author>
    <author>
      <name>Harun Mustafa</name>
    </author>
    <author>
      <name>Jeremie Kim</name>
    </author>
    <author>
      <name>Ataberk Olgun</name>
    </author>
    <author>
      <name>Arvid Gollwitzer</name>
    </author>
    <author>
      <name>Damla Senol Cali</name>
    </author>
    <author>
      <name>Can Firtina</name>
    </author>
    <author>
      <name>Haiyu Mao</name>
    </author>
    <author>
      <name>Nour Almadhoun Alserr</name>
    </author>
    <author>
      <name>Rachata Ausavarungnirun</name>
    </author>
    <author>
      <name>Nandita Vijaykumar</name>
    </author>
    <author>
      <name>Mohammed Alser</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at ASPLOS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.10400v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.10400v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.03322v1</id>
    <updated>2022-05-06T15:56:40Z</updated>
    <published>2022-05-06T15:56:40Z</published>
    <title>Private delegated computations using strong isolation</title>
    <summary>  Sensitive computations are now routinely delegated to third-parties. In
response, Confidential Computing technologies are being introduced to
microprocessors, offering a protected processing environment, which we
generically call an isolate, providing confidentiality and integrity guarantees
to code and data hosted within -- even in the face of a privileged attacker.
Isolates, with an attestation protocol, permit remote third-parties to
establish a trusted "beachhead" containing known code and data on an otherwise
untrusted machine. Yet, the rise of these technologies introduces many new
problems, including: how to ease provisioning of computations safely into
isolates; how to develop distributed systems spanning multiple classes of
isolate; and what to do about the billions of "legacy" devices without support
for Confidential Computing?
  Tackling the problems above, we introduce Veracruz, a framework that eases
the design and implementation of complex privacy-preserving, collaborative,
delegated computations among a group of mutually mistrusting principals.
Veracruz supports multiple isolation technologies and provides a common
programming model and attestation protocol across all of them, smoothing
deployment of delegated computations over supported technologies. We
demonstrate Veracruz in operation, on private in-cloud object detection on
encrypted video streaming from a video camera. In addition to supporting
hardware-backed isolates -- like AWS Nitro Enclaves and Arm Confidential
Computing Architecture Realms -- Veracruz also provides pragmatic "software
isolates" on Armv8-A devices without hardware Confidential Computing
capability, using the high-assurance seL4 microkernel and our IceCap framework.
</summary>
    <author>
      <name>Mathias Brossard</name>
    </author>
    <author>
      <name>Guilhem Bryant</name>
    </author>
    <author>
      <name>Basma El Gaabouri</name>
    </author>
    <author>
      <name>Xinxin Fan</name>
    </author>
    <author>
      <name>Alexandre Ferreira</name>
    </author>
    <author>
      <name>Edmund Grimley-Evans</name>
    </author>
    <author>
      <name>Christopher Haster</name>
    </author>
    <author>
      <name>Evan Johnson</name>
    </author>
    <author>
      <name>Derek Miller</name>
    </author>
    <author>
      <name>Fan Mo</name>
    </author>
    <author>
      <name>Dominic P. Mulligan</name>
    </author>
    <author>
      <name>Nick Spinale</name>
    </author>
    <author>
      <name>Eric van Hensbergen</name>
    </author>
    <author>
      <name>Hugo J. M. Vincent</name>
    </author>
    <author>
      <name>Shale Xiong</name>
    </author>
    <link href="http://arxiv.org/abs/2205.03322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.03322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.02878v2</id>
    <updated>2023-05-28T06:05:47Z</updated>
    <published>2022-06-06T20:09:20Z</published>
    <title>TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory</title>
    <summary>  The increasing demand for memory in hyperscale applications has led to memory
becoming a large portion of the overall datacenter spend. The emergence of
coherent interfaces like CXL enables main memory expansion and offers an
efficient solution to this problem. In such systems, the main memory can
constitute different memory technologies with varied characteristics. In this
paper, we characterize memory usage patterns of a wide range of datacenter
applications across the server fleet of Meta. We, therefore, demonstrate the
opportunities to offload colder pages to slower memory tiers for these
applications. Without efficient memory management, however, such systems can
significantly degrade performance.
  We propose a novel OS-level application-transparent page placement mechanism
(TPP) for CXL-enabled memory. TPP employs a lightweight mechanism to identify
and place hot/cold pages to appropriate memory tiers. It enables a proactive
page demotion from local memory to CXL-Memory. This technique ensures a memory
headroom for new page allocations that are often related to request processing
and tend to be short-lived and hot. At the same time, TPP can promptly promote
performance-critical hot pages trapped in the slow CXL-Memory to the fast local
memory, while minimizing both sampling overhead and unnecessary migrations. TPP
works transparently without any application-specific knowledge and can be
deployed globally as a kernel release.
  We evaluate TPP in the production server fleet with early samples of new x86
CPUs with CXL 1.1 support. TPP makes a tiered memory system performant as an
ideal baseline (&lt;1% gap) that has all the memory in the local tier. It is 18%
better than today's Linux, and 5-17% better than existing solutions including
NUMA Balancing and AutoTiering. Most of the TPP patches have been merged in the
Linux v5.18 release.
</summary>
    <author>
      <name>Hasan Al Maruf</name>
    </author>
    <author>
      <name>Hao Wang</name>
    </author>
    <author>
      <name>Abhishek Dhanotia</name>
    </author>
    <author>
      <name>Johannes Weiner</name>
    </author>
    <author>
      <name>Niket Agarwal</name>
    </author>
    <author>
      <name>Pallab Bhattacharya</name>
    </author>
    <author>
      <name>Chris Petersen</name>
    </author>
    <author>
      <name>Mosharaf Chowdhury</name>
    </author>
    <author>
      <name>Shobhit Kanaujia</name>
    </author>
    <author>
      <name>Prakash Chauhan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3582016.3582063</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3582016.3582063" rel="related"/>
    <link href="http://arxiv.org/abs/2206.02878v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.02878v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.01849v1</id>
    <updated>2022-07-05T07:27:04Z</updated>
    <published>2022-07-05T07:27:04Z</published>
    <title>Learnings from an Under the Hood Analysis of an Object Storage Node IO
  Stack</title>
    <summary>  Conventional object-stores are built on top of traditional OS storage stack,
where I/O requests typically transfers through multiple hefty and redundant
layers. The complexity of object management has grown dramatically with the
ever increasing requirements of performance, consistency and fault-tolerance
from storage subsystems. Simply stated, more number of intermediate layers are
encountered in the I/O data path, with each passing layer adding its own syntax
and semantics. Thereby increasing the overheads of request processing. In this
paper, through comprehensive under-the-hood analysis of an object-storage node,
we characterize the impact of object-store (and user-application) workloads on
the OS I/O stack and its subsequent rippling effect on the underlying
object-storage devices (OSD). We observe that the legacy architecture of the OS
based I/O storage stack coupled with complex data management policies leads to
a performance mismatch between what an end-storage device is capable of
delivering and what it actually delivers in a production environment.
Therefore, the gains derived from developing faster storage devices is often
nullified. These issues get more pronounced in highly concurrent and
multiplexed cloud environments. Owing to the associated issues of
object-management and the vulnerabilities of the OS I/O software stacks, we
discuss the potential of a new class of storage devices, known as
Object-Drives. Samsung Key-Value SSD (KV-SSD) [1] and Seagate Kinetic Drive [2]
are classic industrial implementations of object-drives, where host data
management functionalities can be offloaded to the storage device. This leads
towards the simplification of the over-all storage stack. Based on our
analysis, we believe object-drives can alleviate object-stores from highly
taxing overheads of data management with 20-38% time-savings over traditional
Operating Systems (OS) stack.
</summary>
    <author>
      <name>Pratik Mishra</name>
    </author>
    <author>
      <name>Rekha Pitchumani</name>
    </author>
    <author>
      <name>Yang Suk Kee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.48550/arXiv.2207.01849</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.48550/arXiv.2207.01849" rel="related"/>
    <link href="http://arxiv.org/abs/2207.01849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.01849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.05676v2</id>
    <updated>2022-09-02T15:26:13Z</updated>
    <published>2022-05-29T09:19:27Z</published>
    <title>HyperDbg: Reinventing Hardware-Assisted Debugging (Extended Version)</title>
    <summary>  Software analysis, debugging, and reverse engineering have a crucial impact
in today's software industry. Efficient and stealthy debuggers are especially
relevant for malware analysis. However, existing debugging platforms fail to
address a transparent, effective, and high-performance low-level debugger due
to their detectable fingerprints, complexity, and implementation restrictions.
In this paper, we present HyperDbg, a new hypervisor-assisted debugger for
high-performance and stealthy debugging of user and kernel applications. To
accomplish this, HyperDbg relies on state-of-the-art hardware features
available in today's CPUs, such as VT-x and extended page tables. In contrast
to other widely used existing debuggers, we design HyperDbg using a custom
hypervisor, making it independent of OS functionality or API. We propose
hardware-based instruction-level emulation and OS-level API hooking via
extended page tables to increase the stealthiness. Our results of the dynamic
analysis of 10,853 malware samples show that HyperDbg's stealthiness allows
debugging on average 22% and 26% more samples than WinDbg and x64dbg,
respectively. Moreover, in contrast to existing debuggers, HyperDbg is not
detected by any of the 13 tested packers and protectors. We improve the
performance over other debuggers by deploying a VMX-compatible script engine,
eliminating unnecessary context switches. Our experiment on three concrete
debugging scenarios shows that compared to WinDbg as the only kernel debugger,
HyperDbg performs step-in, conditional breaks, and syscall recording, 2.98x,
1319x, and 2018x faster, respectively. We finally show real-world applications,
such as a 0-day analysis, structure reconstruction for reverse engineering,
software performance analysis, and code-coverage analysis.
</summary>
    <author>
      <name>Mohammad Sina Karvandi</name>
    </author>
    <author>
      <name>MohammadHossein Gholamrezaei</name>
    </author>
    <author>
      <name>Saleh Khalaj Monfared</name>
    </author>
    <author>
      <name>Soroush Meghdadizanjani</name>
    </author>
    <author>
      <name>Behrooz Abbassi</name>
    </author>
    <author>
      <name>Ali Amini</name>
    </author>
    <author>
      <name>Reza Mortazavi</name>
    </author>
    <author>
      <name>Saeid Gorgin</name>
    </author>
    <author>
      <name>Dara Rahmati</name>
    </author>
    <author>
      <name>Michael Schwarz</name>
    </author>
    <link href="http://arxiv.org/abs/2207.05676v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.05676v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.02915v2</id>
    <updated>2023-01-12T12:10:34Z</updated>
    <published>2023-01-07T18:35:08Z</published>
    <title>SFP: Providing System Call Flow Protection against Software and Fault
  Attacks</title>
    <summary>  With the improvements in computing technologies, edge devices in the
Internet-of-Things have become more complex. The enabler technology for these
complex systems are powerful application core processors with operating system
support, such as Linux. While the isolation of applications through the
operating system increases the security, the interface to the kernel poses a
new threat. Different attack vectors, including fault attacks and memory
vulnerabilities, exploit the kernel interface to escalate privileges and take
over the system.
  In this work, we present SFP, a mechanism to protect the execution of system
calls against software and fault attacks providing integrity to user-kernel
transitions. SFP provides system call flow integrity by a two-step linking
approach, which links the system call and its origin to the state of
control-flow integrity. A second linking step within the kernel ensures that
the right system call is executed in the kernel. Combining both linking steps
ensures that only the correct system call is executed at the right location in
the program and cannot be skipped. Furthermore, SFP provides dynamic CFI
instrumentation and a new CFI checking policy at the edge of the kernel to
verify the control-flow state of user programs before entering the kernel. We
integrated SFP into FIPAC, a CFI protection scheme exploiting ARM pointer
authentication. Our prototype is based on a custom LLVM-based toolchain with an
instrumented runtime library combined with a custom Linux kernel to protect
system calls. The evaluation of micro- and macrobenchmarks based on SPEC 2017
show an average runtime overhead of 1.9 % and 20.6 %, which is only an increase
of 1.8 % over plain control-flow protection. This small impact on the
performance shows the efficiency of SFP for protecting all system calls and
providing integrity for the user-kernel transitions.
</summary>
    <author>
      <name>Robert Schilling</name>
    </author>
    <author>
      <name>Pascal Nasahl</name>
    </author>
    <author>
      <name>Martin Unterguggenberger</name>
    </author>
    <author>
      <name>Stefan Mangard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at HASP22</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.02915v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.02915v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.03976v3</id>
    <updated>2023-03-07T16:16:33Z</updated>
    <published>2023-02-08T10:15:07Z</published>
    <title>Parma: Confidential Containers via Attested Execution Policies</title>
    <summary>  Container-based technologies empower cloud tenants to develop highly portable
software and deploy services in the cloud at a rapid pace. Cloud privacy,
meanwhile, is important as a large number of container deployments operate on
privacy-sensitive data, but challenging due to the increasing frequency and
sophistication of attacks. State-of-the-art confidential container-based
designs leverage process-based trusted execution environments (TEEs), but face
security and compatibility issues that limits their practical deployment. We
propose Parma, an architecture that provides lift-and-shift deployment of
unmodified containers while providing strong security protection against a
powerful attacker who controls the untrusted host and hypervisor. Parma
leverages VM-level isolation to execute a container group within a unique
VM-based TEE. Besides container integrity and user data confidentiality and
integrity, Parma also offers container attestation and execution integrity
based on an attested execution policy. Parma execution policies provide an
inductive proof over all future states of the container group. This proof,
which is established during initialization, forms a root of trust that can be
used for secure operations within the container group without requiring any
modifications of the containerized workflow itself (aside from the inclusion of
the execution policy.) We evaluate Parma on AMD SEV-SNP processors by running a
diverse set of workloads demonstrating that workflows exhibit 0-26% additional
overhead in performance over running outside the enclave, with a mean 13%
overhead on SPEC2017, while requiring no modifications to their program code.
Adding execution policies introduces less than 1% additional overhead.
Furthermore, we have deployed Parma as the underlying technology driving
Confidential Containers on Azure Container Instances.
</summary>
    <author>
      <name>Matthew A. Johnson</name>
    </author>
    <author>
      <name>Stavros Volos</name>
    </author>
    <author>
      <name>Ken Gordon</name>
    </author>
    <author>
      <name>Sean T. Allen</name>
    </author>
    <author>
      <name>Christoph M. Wintersteiger</name>
    </author>
    <author>
      <name>Sylvan Clebsch</name>
    </author>
    <author>
      <name>John Starks</name>
    </author>
    <author>
      <name>Manuel Costa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.03976v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.03976v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.10901v1</id>
    <updated>2023-03-20T06:38:10Z</updated>
    <published>2023-03-20T06:38:10Z</published>
    <title>E2C: A Visual Simulator to Reinforce Education of Heterogeneous
  Computing Systems</title>
    <summary>  With the increasing popularity of accelerator technologies (e.g., GPUs and
TPUs) and the emergence of domain-specific computing via ASICs and FPGA, the
matter of heterogeneity and understanding its ramifications on the performance
has become more critical than ever before. However, it is challenging to
effectively educate students about the potential impacts of heterogeneity on
the performance of distributed systems; and on the logic of resource allocation
methods to efficiently utilize the resources. Making use of the real
infrastructure for benchmarking the performance of heterogeneous machines, for
different applications, with respect to different objectives, and under various
workload intensities is cost- and time-prohibitive. To reinforce the quality of
learning about various dimensions of heterogeneity, and to decrease the
widening gap in education, we develop an open-source simulation tool, called
E2C, that can help students researchers to study any type of heterogeneous (or
homogeneous) computing system and measure its performance under various
configurations. E2C is equipped with an intuitive graphical user interface
(GUI) that enables its users to easily examine system-level solutions
(scheduling, load balancing, scalability, etc.) in a controlled environment
within a short time. E2C is a discrete event simulator that offers the
following features: (i) simulating a heterogeneous computing system; (ii)
implementing a newly developed scheduling method and plugging it into the
system, (iii) measuring energy consumption and other output-related metrics;
and (iv) powerful visual aspects to ease the learning curve for students. We
used E2C as an assignment in the Distributed and Cloud Computing course. Our
anonymous survey study indicates that students rated E2C with the score of 8.7
out of 10 for its usefulness in understanding the concepts of scheduling in
heterogeneous computing.
</summary>
    <author>
      <name>Ali Mokhtari</name>
    </author>
    <author>
      <name>Drake Rawls</name>
    </author>
    <author>
      <name>Tony Huynh</name>
    </author>
    <author>
      <name>Jeremiah Green</name>
    </author>
    <author>
      <name>Mohsen Amini Salehi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Edupar '23, as part of IPDPS '23 Conference. arXiv admin
  note: text overlap with arXiv:2212.11333</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.10901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.10901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.16463v2</id>
    <updated>2023-06-25T23:41:55Z</updated>
    <published>2023-03-29T05:33:11Z</published>
    <title>Remote attestation of SEV-SNP confidential VMs using e-vTPMs</title>
    <summary>  Trying to address the security challenges of a cloud-centric software
deployment paradigm, silicon and cloud vendors are introducing confidential
computing - an umbrella term aimed at providing hardware and software
mechanisms for protecting cloud workloads from the cloud provider and its
software stack. Today, Intel SGX, AMD SEV, Intel TDX, etc., provide a way to
shield cloud applications from the cloud provider through encryption of the
application's memory below the hardware boundary of the CPU, hence requiring
trust only in the CPU vendor. Unfortunately, existing hardware mechanisms do
not automatically enable the guarantee that a protected system was not tampered
with during configuration and boot time. Such a guarantee relies on a hardware
RoT, i.e., an integrity-protected location that can store measurements in a
trustworthy manner, extend them, and authenticate the measurement logs to the
user.
  In this work, we design and implement a virtual TPM that virtualizes the
hardware RoT without requiring trust in the cloud provider. To ensure the
security of a vTPM in a provider-controlled environment, we leverage unique
isolation properties of the SEV-SNP hardware that allows us to execute secure
services as part of the enclave environment protected from the cloud provider.
We further develop a novel approach to vTPM state management where the vTPM
state is not preserved across reboots. Specifically, we develop a stateless
ephemeral vTPM that supports remote attestation without any persistent state on
the host. This allows us to pair each confidential VM with a private instance
of a vTPM completely isolated from the provider-controlled environment and
other VMs. We built our prototype entirely on open-source components. Though
our work is AMD-specific, a similar approach could be used to build remote
attestation protocols on other trusted execution environments.
</summary>
    <author>
      <name>Vikram Narayanan</name>
    </author>
    <author>
      <name>Claudio Carvalho</name>
    </author>
    <author>
      <name>Angelo Ruocco</name>
    </author>
    <author>
      <name>Gheorghe Almási</name>
    </author>
    <author>
      <name>James Bottomley</name>
    </author>
    <author>
      <name>Mengmei Ye</name>
    </author>
    <author>
      <name>Tobin Feldman-Fitzthum</name>
    </author>
    <author>
      <name>Daniele Buono</name>
    </author>
    <author>
      <name>Hubertus Franke</name>
    </author>
    <author>
      <name>Anton Burtsev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3627106.3627112</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3627106.3627112" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 39th Annual Computer Security Applications
  Conference (ACSAC 2023). 732-743</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2303.16463v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.16463v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.02959v1</id>
    <updated>2023-10-04T16:50:23Z</updated>
    <published>2023-10-04T16:50:23Z</published>
    <title>Co-Optimizing Cache Partitioning and Multi-Core Task Scheduling: Exploit
  Cache Sensitivity or Not?</title>
    <summary>  Cache partitioning techniques have been successfully adopted to mitigate
interference among concurrently executing real-time tasks on multi-core
processors. Considering that the execution time of a cache-sensitive task
strongly depends on the cache available for it to use, co-optimizing cache
partitioning and task allocation improves the system's schedulability. In this
paper, we propose a hybrid multi-layer design space exploration technique to
solve this multi-resource management problem. We explore the interplay between
cache partitioning and schedulability by systematically interleaving three
optimization layers, viz., (i) in the outer layer, we perform a breadth-first
search combined with proactive pruning for cache partitioning; (ii) in the
middle layer, we exploit a first-fit heuristic for allocating tasks to cores;
and (iii) in the inner layer, we use the well-known recurrence relation for the
schedulability analysis of non-preemptive fixed-priority (NP-FP) tasks in a
uniprocessor setting. Although our focus is on NP-FP scheduling, we evaluate
the flexibility of our framework in supporting different scheduling policies
(NP-EDF, P-EDF) by plugging in appropriate analysis methods in the inner layer.
Experiments show that, compared to the state-of-the-art techniques, the
proposed framework can improve the real-time schedulability of NP-FP task sets
by an average of 15.2% with a maximum improvement of 233.6% (when tasks are
highly cache-sensitive) and a minimum of 1.6% (when cache sensitivity is low).
For such task sets, we found that clustering similar-period (or mutually
compatible) tasks often leads to higher schedulability (on average 7.6%) than
clustering by cache sensitivity. In our evaluation, the framework also achieves
good results for preemptive and dynamic-priority scheduling policies.
</summary>
    <author>
      <name>Binqi Sun</name>
    </author>
    <author>
      <name>Debayan Roy</name>
    </author>
    <author>
      <name>Tomasz Kloda</name>
    </author>
    <author>
      <name>Andrea Bastoni</name>
    </author>
    <author>
      <name>Rodolfo Pellizzoni</name>
    </author>
    <author>
      <name>Marco Caccamo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to be published in IEEE Real-Time Systems Symposium (RTSS), 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.02959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.02959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.04158v3</id>
    <updated>2024-01-05T12:37:22Z</updated>
    <published>2023-10-06T11:15:20Z</published>
    <title>Victima: Drastically Increasing Address Translation Reach by Leveraging
  Underutilized Cache Resources</title>
    <summary>  Address translation is a performance bottleneck in data-intensive workloads
due to large datasets and irregular access patterns that lead to frequent
high-latency page table walks (PTWs). PTWs can be reduced by using (i) large
hardware TLBs or (ii) large software-managed TLBs. Unfortunately, both
solutions have significant drawbacks: increased access latency, power and area
(for hardware TLBs), and costly memory accesses, the need for large contiguous
memory blocks, and complex OS modifications (for software-managed TLBs). We
present Victima, a new software-transparent mechanism that drastically
increases the translation reach of the processor by leveraging the
underutilized resources of the cache hierarchy. The key idea of Victima is to
repurpose L2 cache blocks to store clusters of TLB entries, thereby providing
an additional low-latency and high-capacity component that backs up the
last-level TLB and thus reduces PTWs. Victima has two main components. First, a
PTW cost predictor (PTW-CP) identifies costly-to-translate addresses based on
the frequency and cost of the PTWs they lead to. Second, a TLB-aware cache
replacement policy prioritizes keeping TLB entries in the cache hierarchy by
considering (i) the translation pressure (e.g., last-level TLB miss rate) and
(ii) the reuse characteristics of the TLB entries. Our evaluation results show
that in native (virtualized) execution environments Victima improves average
end-to-end application performance by 7.4% (28.7%) over the baseline four-level
radix-tree-based page table design and by 6.2% (20.1%) over a state-of-the-art
software-managed TLB, across 11 diverse data-intensive workloads. Victima (i)
is effective in both native and virtualized environments, (ii) is completely
transparent to application and system software, and (iii) incurs very small
area and power overheads on a modern high-end CPU.
</summary>
    <author>
      <name>Konstantinos Kanellopoulos</name>
    </author>
    <author>
      <name>Hong Chul Nam</name>
    </author>
    <author>
      <name>F. Nisa Bostanci</name>
    </author>
    <author>
      <name>Rahul Bera</name>
    </author>
    <author>
      <name>Mohammad Sadrosadati</name>
    </author>
    <author>
      <name>Rakesh Kumar</name>
    </author>
    <author>
      <name>Davide-Basilio Bartolini</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3613424.3614276</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3613424.3614276" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in 56th IEEE/ACM International Symposium on
  Microarchitecture (MICRO), 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.04158v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.04158v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.04875v1</id>
    <updated>2023-10-07T17:16:34Z</updated>
    <published>2023-10-07T17:16:34Z</published>
    <title>Prompt-to-OS (P2OS): Revolutionizing Operating Systems and
  Human-Computer Interaction with Integrated AI Generative Models</title>
    <summary>  In this paper, we present a groundbreaking paradigm for human-computer
interaction that revolutionizes the traditional notion of an operating system.
  Within this innovative framework, user requests issued to the machine are
handled by an interconnected ecosystem of generative AI models that seamlessly
integrate with or even replace traditional software applications. At the core
of this paradigm shift are large generative models, such as language and
diffusion models, which serve as the central interface between users and
computers. This pioneering approach leverages the abilities of advanced
language models, empowering users to engage in natural language conversations
with their computing devices. Users can articulate their intentions, tasks, and
inquiries directly to the system, eliminating the need for explicit commands or
complex navigation. The language model comprehends and interprets the user's
prompts, generating and displaying contextual and meaningful responses that
facilitate seamless and intuitive interactions.
  This paradigm shift not only streamlines user interactions but also opens up
new possibilities for personalized experiences. Generative models can adapt to
individual preferences, learning from user input and continuously improving
their understanding and response generation. Furthermore, it enables enhanced
accessibility, as users can interact with the system using speech or text,
accommodating diverse communication preferences.
  However, this visionary concept raises significant challenges, including
privacy, security, trustability, and the ethical use of generative models.
Robust safeguards must be in place to protect user data and prevent potential
misuse or manipulation of the language model.
  While the full realization of this paradigm is still far from being achieved,
this paper serves as a starting point for envisioning this transformative
potential.
</summary>
    <author>
      <name>Gabriele Tolomei</name>
    </author>
    <author>
      <name>Cesare Campagnano</name>
    </author>
    <author>
      <name>Fabrizio Silvestri</name>
    </author>
    <author>
      <name>Giovanni Trappolini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure. Accepted at IEEE CogMI 2023 (IEEE International
  Conference on Cognitive Machine Intelligence)</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.04875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.04875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.19699v3</id>
    <updated>2024-03-07T20:51:18Z</updated>
    <published>2023-10-30T16:21:49Z</published>
    <title>Optimizing Logical Execution Time Model for Both Determinism and Low
  Latency</title>
    <summary>  The Logical Execution Time (LET) programming model has recently received
considerable attention, particularly because of its timing and dataflow
determinism. In LET, task computation appears always to take the same amount of
time (called the task's LET interval), and the task reads (resp. writes) at the
beginning (resp. end) of the interval. Compared to other communication
mechanisms, such as implicit communication and Dynamic Buffer Protocol (DBP),
LET performs worse on many metrics, such as end-to-end latency (including
reaction time and data age) and time disparity jitter. Compared with the
default LET setting, the flexible LET (fLET) model shrinks the LET interval
while still guaranteeing schedulability by introducing the virtual offset to
defer the read operation and using the virtual deadline to move up the write
operation. Therefore, fLET has the potential to significantly improve the
end-to-end timing performance while keeping the benefits of deterministic
behavior on timing and dataflow.
  To fully realize the potential of fLET, we consider the problem of optimizing
the assignments of its virtual offsets and deadlines. We propose new
abstractions to describe the task communication pattern and new optimization
algorithms to explore the solution space efficiently. The algorithms leverage
the linearizability of communication patterns and utilize symbolic operations
to achieve efficient optimization while providing a theoretical guarantee. The
framework supports optimizing multiple performance metrics and guarantees
bounded suboptimality when optimizing end-to-end latency. Experimental results
show that our optimization algorithms improve upon the default LET and its
existing extensions and significantly outperform implicit communication and DBP
in terms of various metrics, such as end-to-end latency, time disparity, and
its jitter.
</summary>
    <author>
      <name>Sen Wang</name>
    </author>
    <author>
      <name>Dong Li</name>
    </author>
    <author>
      <name>Ashrarul H. Sifat</name>
    </author>
    <author>
      <name>Shao-Yu Huang</name>
    </author>
    <author>
      <name>Xuanliang Deng</name>
    </author>
    <author>
      <name>Changhee Jung</name>
    </author>
    <author>
      <name>Ryan Williams</name>
    </author>
    <author>
      <name>Haibo Zeng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted in RTAS'24</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.19699v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.19699v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.05531v1</id>
    <updated>2023-12-09T10:45:54Z</updated>
    <published>2023-12-09T10:45:54Z</published>
    <title>KEN: Kernel Extensions using Natural Language</title>
    <summary>  The ability to modify and extend an operating system is an important feature
for improving a system's security, reliability, and performance. The extended
Berkeley Packet Filters (eBPF) ecosystem has emerged as the standard mechanism
for extending the Linux kernel and has recently been ported to Windows. eBPF
programs inject new logic into the kernel that the system will execute before
or after existing logic. While the eBPF ecosystem provides a flexible mechanism
for kernel extension, it is difficult for developers to write eBPF programs
today. An eBPF developer must have deep knowledge of the internals of the
operating system to determine where to place logic and cope with programming
limitations on the control flow and data accesses of their eBPF program
enforced by the eBPF verifier. This paper presents KEN, an alternative
framework that alleviates the difficulty of writing an eBPF program by allowing
Kernel Extensions to be written in Natural language. KEN uses recent advances
in large language models (LLMs) to synthesize an eBPF program given a user's
English language prompt. To ensure that LLM's output is semantically equivalent
to the user's prompt, KEN employs a combination of LLM-empowered program
comprehension, symbolic execution, and a series of feedback loops. KEN's key
novelty is the combination of these techniques. In particular, the system uses
symbolic execution in a novel structure that allows it to combine the results
of program synthesis and program comprehension and build on the recent success
that LLMs have shown for each of these tasks individually. To evaluate KEN, we
developed a new corpus of natural language prompts for eBPF programs. We show
that KEN produces correct eBPF programs on 80% which is an improvement of a
factor of 2.67 compared to an LLM-empowered program synthesis baseline.
</summary>
    <author>
      <name>Yusheng Zheng</name>
    </author>
    <author>
      <name>Yiwei Yang</name>
    </author>
    <author>
      <name>Maolin Chen</name>
    </author>
    <author>
      <name>Andrew Quinn</name>
    </author>
    <link href="http://arxiv.org/abs/2312.05531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.05531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.13429v1</id>
    <updated>2024-02-20T23:45:37Z</updated>
    <published>2024-02-20T23:45:37Z</published>
    <title>Everything You Always Wanted to Know About Storage Compressibility of
  Pre-Trained ML Models but Were Afraid to Ask</title>
    <summary>  As the number of pre-trained machine learning (ML) models is growing
exponentially, data reduction tools are not catching up. Existing data
reduction techniques are not specifically designed for pre-trained model (PTM)
dataset files. This is largely due to a lack of understanding of the patterns
and characteristics of these datasets, especially those relevant to data
reduction and compressibility.
  This paper presents the first, exhaustive analysis to date of PTM datasets on
storage compressibility. Our analysis spans different types of data reduction
and compression techniques, from hash-based data deduplication, data similarity
detection, to dictionary-coding compression. Our analysis explores these
techniques at three data granularity levels, from model layers, model chunks,
to model parameters. We draw new observations that indicate that modern data
reduction tools are not effective when handling PTM datasets. There is a
pressing need for new compression methods that take into account PTMs' data
characteristics for effective storage reduction.
  Motivated by our findings, we design ELF, a simple yet effective,
error-bounded, lossy floating-point compression method. ELF transforms
floating-point parameters in such a way that the common exponent field of the
transformed parameters can be completely eliminated to save storage space. We
develop Elves, a compression framework that integrates ELF along with several
other data reduction methods. Elves uses the most effective method to compress
PTMs that exhibit different patterns. Evaluation shows that Elves achieves an
overall compression ratio of $1.52\times$, which is $1.31\times$, $1.32\times$
and $1.29\times$ higher than a general-purpose compressor (zstd), an
error-bounded lossy compressor (SZ3), and the uniform model quantization,
respectively, with negligible model accuracy loss.
</summary>
    <author>
      <name>Zhaoyuan Su</name>
    </author>
    <author>
      <name>Ammar Ahmed</name>
    </author>
    <author>
      <name>Zirui Wang</name>
    </author>
    <author>
      <name>Ali Anwar</name>
    </author>
    <author>
      <name>Yue Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper presents the first, exhaustive analysis to date of PTM
  datasets on storage compressibility. Motivated by our findings, we design
  ELF, a simple yet effective, error-bounded, lossy floating-point compression
  method</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.13429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.13429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.04635v2</id>
    <updated>2025-03-27T01:00:37Z</updated>
    <published>2024-03-07T16:21:02Z</published>
    <title>Virtuoso: Enabling Fast and Accurate Virtual Memory Research via an
  Imitation-based Operating System Simulation Methodology</title>
    <summary>  The unprecedented growth in data demand from emerging applications has turned
virtual memory (VM) into a major performance bottleneck. Researchers explore
new hardware/OS co-designs to optimize VM across diverse applications and
systems. To evaluate such designs, researchers rely on various simulation
methodologies to model VM components.Unfortunately, current simulation tools
(i) either lack the desired accuracy in modeling VM's software components or
(ii) are too slow and complex to prototype and evaluate schemes that span
across the hardware/software boundary.
  We introduce Virtuoso, a new simulation framework that enables quick and
accurate prototyping and evaluation of the software and hardware components of
the VM subsystem. The key idea of Virtuoso is to employ a lightweight userspace
OS kernel, called MimicOS, that (i) accelerates simulation time by imitating
only the desired kernel functionalities, (ii) facilitates the development of
new OS routines that imitate real ones, using an accessible high-level
programming interface, (iii) enables accurate and flexible evaluation of the
application- and system-level implications of VM after integrating Virtuoso to
a desired architectural simulator.
  We integrate Virtuoso into five diverse architectural simulators, each
specializing in different aspects of system design, and heavily enrich it with
multiple state-of-the-art VM schemes. Our validation shows that Virtuoso ported
on top of Sniper, a state-of-the-art microarchitectural simulator, models the
memory management unit of a real high-end server-grade page fault latency of a
real Linux kernel with high accuracy . Consequently, Virtuoso models the IPC
performance of a real high-end server-grade CPU with 21% higher accuracy than
the baseline version of Sniper. The source code of Virtuoso is freely available
at https://github.com/CMU-SAFARI/Virtuoso.
</summary>
    <author>
      <name>Konstantinos Kanellopoulos</name>
    </author>
    <author>
      <name>Konstantinos Sgouras</name>
    </author>
    <author>
      <name>F. Nisa Bostanci</name>
    </author>
    <author>
      <name>Andreas Kosmas Kakolyris</name>
    </author>
    <author>
      <name>Berkin Kerim Konar</name>
    </author>
    <author>
      <name>Rahul Bera</name>
    </author>
    <author>
      <name>Mohammad Sadrosadati</name>
    </author>
    <author>
      <name>Rakesh Kumar</name>
    </author>
    <author>
      <name>Nandita Vijaykumar</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <link href="http://arxiv.org/abs/2403.04635v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.04635v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.09555v1</id>
    <updated>2024-06-28T15:47:25Z</updated>
    <published>2024-06-28T15:47:25Z</published>
    <title>A parallel evolutionary algorithm to optimize dynamic memory managers in
  embedded systems</title>
    <summary>  For the last thirty years, several Dynamic Memory Managers (DMMs) have been
proposed. Such DMMs include first fit, best fit, segregated fit and buddy
systems. Since the performance, memory usage and energy consumption of each DMM
differs, software engineers often face difficult choices in selecting the most
suitable approach for their applications. This issue has special impact in the
field of portable consumer embedded systems, that must execute a limited amount
of multimedia applications (e.g., 3D games, video players and signal processing
software, etc.), demanding high performance and extensive memory usage at a low
energy consumption. Recently, we have developed a novel methodology based on
genetic programming to automatically design custom DMMs, optimizing
performance, memory usage and energy consumption. However, although this
process is automatic and faster than state-of-the-art optimizations, it demands
intensive computation, resulting in a time consuming process. Thus, parallel
processing can be very useful to enable to explore more solutions spending the
same time, as well as to implement new algorithms. In this paper we present a
novel parallel evolutionary algorithm for DMMs optimization in embedded
systems, based on the Discrete Event Specification (DEVS) formalism over a
Service Oriented Architecture (SOA) framework. Parallelism significantly
improves the performance of the sequential exploration algorithm. On the one
hand, when the number of generations are the same in both approaches, our
parallel optimization framework is able to reach a speed-up of 86.40x when
compared with other state-of-the-art approaches. On the other, it improves the
global quality (i.e., level of performance, low memory usage and low energy
consumption) of the final DMM obtained in a 36.36% with respect to two
well-known general-purpose DMMs and two state-of-the-art optimization
methodologies.
</summary>
    <author>
      <name>José L. Risco-Martín</name>
    </author>
    <author>
      <name>David Atienza</name>
    </author>
    <author>
      <name>J. Manuel Colmenar</name>
    </author>
    <author>
      <name>Oscar Garnica</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.parco.2010.07.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.parco.2010.07.001" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Parallel Computing, 36(10-11), pp. 572-590, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2407.09555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.09555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.18306v1</id>
    <updated>2024-07-25T18:00:08Z</updated>
    <published>2024-07-25T18:00:08Z</published>
    <title>Design and demonstration of an operating system for executing
  applications on quantum network nodes</title>
    <summary>  The goal of future quantum networks is to enable new internet applications
that are impossible to achieve using solely classical communication. Up to now,
demonstrations of quantum network applications and functionalities on quantum
processors have been performed in ad-hoc software that was specific to the
experimental setup, programmed to perform one single task (the application
experiment) directly into low-level control devices using expertise in
experimental physics. Here, we report on the design and implementation of the
first architecture capable of executing quantum network applications on quantum
processors in platform-independent high-level software. We demonstrate the
architecture's capability to execute applications in high-level software, by
implementing it as a quantum network operating system -- QNodeOS -- and
executing test programs including a delegated computation from a client to a
server on two quantum network nodes based on nitrogen-vacancy (NV) centers in
diamond. We show how our architecture allows us to maximize the use of quantum
network hardware, by multitasking different applications on a quantum network
for the first time. Our architecture can be used to execute programs on any
quantum processor platform corresponding to our system model, which we
illustrate by demonstrating an additional driver for QNodeOS for a trapped-ion
quantum network node based on a single $^{40}\text{Ca}^+$ atom. Our
architecture lays the groundwork for computer science research in the domain of
quantum network programming, and paves the way for the development of software
that can bring quantum network technology to society.
</summary>
    <author>
      <name>Carlo Delle Donne</name>
    </author>
    <author>
      <name>Mariagrazia Iuliano</name>
    </author>
    <author>
      <name>Bart van der Vecht</name>
    </author>
    <author>
      <name>Guilherme Maciel Ferreira</name>
    </author>
    <author>
      <name>Hana Jirovská</name>
    </author>
    <author>
      <name>Thom van der Steenhoven</name>
    </author>
    <author>
      <name>Axel Dahlberg</name>
    </author>
    <author>
      <name>Matt Skrzypczyk</name>
    </author>
    <author>
      <name>Dario Fioretto</name>
    </author>
    <author>
      <name>Markus Teller</name>
    </author>
    <author>
      <name>Pavel Filippov</name>
    </author>
    <author>
      <name>Alejandro Rodríguez-Pardo Montblanch</name>
    </author>
    <author>
      <name>Julius Fischer</name>
    </author>
    <author>
      <name>Benjamin van Ommen</name>
    </author>
    <author>
      <name>Nicolas Demetriou</name>
    </author>
    <author>
      <name>Dominik Leichtle</name>
    </author>
    <author>
      <name>Luka Music</name>
    </author>
    <author>
      <name>Harold Ollivier</name>
    </author>
    <author>
      <name>Ingmar te Raa</name>
    </author>
    <author>
      <name>Wojciech Kozlowski</name>
    </author>
    <author>
      <name>Tim Taminiau</name>
    </author>
    <author>
      <name>Przemysław Pawełczak</name>
    </author>
    <author>
      <name>Tracy Northup</name>
    </author>
    <author>
      <name>Ronald Hanson</name>
    </author>
    <author>
      <name>Stephanie Wehner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures, supplementary materials (48 pages, 24 figures,
  11 tables)</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.18306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.18306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.01241v3</id>
    <updated>2024-10-04T11:32:08Z</updated>
    <published>2024-09-02T13:14:50Z</published>
    <title>CyberCortex.AI: An AI-based Operating System for Autonomous Robotics and
  Complex Automation</title>
    <summary>  The underlying framework for controlling autonomous robots and complex
automation applications are Operating Systems (OS) capable of scheduling
perception-and-control tasks, as well as providing real-time data communication
to other robotic peers and remote cloud computers. In this paper, we introduce
CyberCortex AI, a robotics OS designed to enable heterogeneous AI-based
robotics and complex automation applications. CyberCortex AI is a decentralized
distributed OS which enables robots to talk to each other, as well as to High
Performance Computers (HPC) in the cloud. Sensory and control data from the
robots is streamed towards HPC systems with the purpose of training AI
algorithms, which are afterwards deployed on the robots. Each functionality of
a robot (e.g. sensory data acquisition, path planning, motion control, etc.) is
executed within a so-called DataBlock of Filters shared through the internet,
where each filter is computed either locally on the robot itself, or remotely
on a different robotic system. The data is stored and accessed via a so-called
Temporal Addressable Memory (TAM), which acts as a gateway between each
filter's input and output. CyberCortex AI has two main components: i) the
CyberCortex AI inference system, which is a real-time implementation of the
DataBlock running on the robots' embedded hardware, and ii) the CyberCortex AI
dojo, which runs on an HPC computer in the cloud, and it is used to design,
train and deploy AI algorithms. We present a quantitative and qualitative
performance analysis of the proposed approach using two collaborative robotics
applications: i) a forest fires prevention system based on an Unitree A1 legged
robot and an Anafi Parrot 4K drone, as well as ii) an autonomous driving system
which uses CyberCortex AI for collaborative perception and motion control.
</summary>
    <author>
      <name>Sorin Grigorescu</name>
    </author>
    <author>
      <name>Mihai Zaha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/rob.22426</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/rob.22426" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Field Robotics, August 2024, pp. 1-19</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2409.01241v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.01241v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.01129v1</id>
    <updated>2024-11-02T04:11:05Z</updated>
    <published>2024-11-02T04:11:05Z</published>
    <title>Mewz: Lightweight Execution Environment for WebAssembly with High
  Isolation and Portability using Unikernels</title>
    <summary>  Cloud computing requires isolation and portability for workloads. Cloud
vendors must isolate each user's resources from others to prevent them from
attacking other users or the whole system. Users may want to move their
applications to different environments, for instance other cloud, on-premise
servers, or edge devices. Virtual machines (VMs) and containers are widely used
to achieve these requirements. However, there are two problems with combined
use of VMs and containers. First, container images depend on host operating
systems and CPU architectures. Users need to manage different container images
for each platform to run the same codes on different OSes and ISAs. Second,
performance is degraded by the overheads of both VMs and containers. Previous
researches have solved each of these problems separately, but no solution
solves both problems simultaneously. Therefore, execution environments of
applications on cloud are required to be more lightweight and portable while
ensuring isolation is required. We propose a new system that combines
WebAssembly (Wasm) and unikernels. Wasm is a portable binary format, so it can
be run on any host operating systems and architectures. Unikernels are kernels
statically linked with applications, which reduces the overhead of guest
kernel. In this approach, users deploy applications as a Wasm binary and it
runs as a unikernel on cloud. To realize this system, we propose a mechanism to
convert a Wasm binary into a unikernel image with the Wasm AoT-compiled to
native code. We developed a unikernel with Wasm System Interface (WASI) API and
an Ahead-of-Time (AoT) compiler that converts Wasm to native code. We evaluated
the performance of the system by running a simple HTTP server compiled into
Wasm and native code. The performance was improved by 30\% compared to running
it with an existing Wasm runtime on Linux on a virtual machine.
</summary>
    <author>
      <name>Soichiro Ueda</name>
    </author>
    <author>
      <name>Ai Nozaki</name>
    </author>
    <author>
      <name>Daisuke Kotani</name>
    </author>
    <author>
      <name>Yasuo Okabe</name>
    </author>
    <link href="http://arxiv.org/abs/2411.01129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.01129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.15734v1</id>
    <updated>2025-02-05T14:12:33Z</updated>
    <published>2025-02-05T14:12:33Z</published>
    <title>Cache-Craft: Managing Chunk-Caches for Efficient Retrieval-Augmented
  Generation</title>
    <summary>  Retrieval-Augmented Generation (RAG) is often used with Large Language Models
(LLMs) to infuse domain knowledge or user-specific information. In RAG, given a
user query, a retriever extracts chunks of relevant text from a knowledge base.
These chunks are sent to an LLM as part of the input prompt. Typically, any
given chunk is repeatedly retrieved across user questions. However, currently,
for every question, attention-layers in LLMs fully compute the key values (KVs)
repeatedly for the input chunks, as state-of-the-art methods cannot reuse
KV-caches when chunks appear at arbitrary locations with arbitrary contexts.
Naive reuse leads to output quality degradation. This leads to potentially
redundant computations on expensive GPUs and increases latency. In this work,
we propose Cache-Craft, a system for managing and reusing precomputed KVs
corresponding to the text chunks (we call chunk-caches) in RAG-based systems.
We present how to identify chunk-caches that are reusable, how to efficiently
perform a small fraction of recomputation to fix the cache to maintain output
quality, and how to efficiently store and evict chunk-caches in the hardware
for maximizing reuse while masking any overheads. With real production
workloads as well as synthetic datasets, we show that Cache-Craft reduces
redundant computation by 51% over SOTA prefix-caching and 75% over full
recomputation. Additionally, with continuous batching on a real production
workload, we get a 1.6X speed up in throughput and a 2X reduction in end-to-end
response latency over prefix-caching while maintaining quality, for both the
LLaMA-3-8B and LLaMA-3-70B models.
</summary>
    <author>
      <name>Shubham Agarwal</name>
    </author>
    <author>
      <name>Sai Sundaresan</name>
    </author>
    <author>
      <name>Subrata Mitra</name>
    </author>
    <author>
      <name>Debabrata Mahapatra</name>
    </author>
    <author>
      <name>Archit Gupta</name>
    </author>
    <author>
      <name>Rounak Sharma</name>
    </author>
    <author>
      <name>Nirmal Joshua Kapu</name>
    </author>
    <author>
      <name>Tong Yu</name>
    </author>
    <author>
      <name>Shiv Saini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at SIGMOD 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.15734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.15734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.14603v2</id>
    <updated>2025-04-25T05:14:14Z</updated>
    <published>2025-04-20T13:04:43Z</published>
    <title>UFO2: The Desktop AgentOS</title>
    <summary>  Recent Computer-Using Agents (CUAs), powered by multimodal large language
models (LLMs), offer a promising direction for automating complex desktop
workflows through natural language. However, most existing CUAs remain
conceptual prototypes, hindered by shallow OS integration, fragile
screenshot-based interaction, and disruptive execution.
  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs
into practical, system-level automation. UFO2 features a centralized HostAgent
for task decomposition and coordination, alongside a collection of
application-specialized AppAgent equipped with native APIs, domain-specific
knowledge, and a unified GUI--API action layer. This architecture enables
robust task execution while preserving modularity and extensibility. A hybrid
control detection pipeline fuses Windows UI Automation (UIA) with vision-based
parsing to support diverse interface styles. Runtime efficiency is further
enhanced through speculative multi-action planning, reducing per-step LLM
overhead. Finally, a Picture-in-Picture (PiP) interface enables automation
within an isolated virtual desktop, allowing agents and users to operate
concurrently without interference.
  We evaluate UFO2 across over 20 real-world Windows applications,
demonstrating substantial improvements in robustness and execution accuracy
over prior CUAs. Our results show that deep OS integration unlocks a scalable
path toward reliable, user-aligned desktop automation.
</summary>
    <author>
      <name>Chaoyun Zhang</name>
    </author>
    <author>
      <name>He Huang</name>
    </author>
    <author>
      <name>Chiming Ni</name>
    </author>
    <author>
      <name>Jian Mu</name>
    </author>
    <author>
      <name>Si Qin</name>
    </author>
    <author>
      <name>Shilin He</name>
    </author>
    <author>
      <name>Lu Wang</name>
    </author>
    <author>
      <name>Fangkai Yang</name>
    </author>
    <author>
      <name>Pu Zhao</name>
    </author>
    <author>
      <name>Chao Du</name>
    </author>
    <author>
      <name>Liqun Li</name>
    </author>
    <author>
      <name>Yu Kang</name>
    </author>
    <author>
      <name>Zhao Jiang</name>
    </author>
    <author>
      <name>Suzhen Zheng</name>
    </author>
    <author>
      <name>Rujia Wang</name>
    </author>
    <author>
      <name>Jiaxu Qian</name>
    </author>
    <author>
      <name>Minghua Ma</name>
    </author>
    <author>
      <name>Jian-Guang Lou</name>
    </author>
    <author>
      <name>Qingwei Lin</name>
    </author>
    <author>
      <name>Saravan Rajmohan</name>
    </author>
    <author>
      <name>Dongmei Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The source code of UFO2 is publicly available at
  https://github.com/microsoft/UFO/, with comprehensive documentation provided
  at https://microsoft.github.io/UFO/</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.14603v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.14603v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.01603v1</id>
    <updated>2025-05-02T21:53:29Z</updated>
    <published>2025-05-02T21:53:29Z</published>
    <title>Unlocking True Elasticity for the Cloud-Native Era with Dandelion</title>
    <summary>  Elasticity is fundamental to cloud computing, as it enables quickly
allocating resources to match the demand of each workload as it arrives, rather
than pre-provisioning resources to meet performance objectives. However, even
serverless platforms -- which boot sandboxes in 10s to 100s of milliseconds --
are not sufficiently elastic to avoid over-provisioning expensive resources.
Today's FaaS platforms rely on pre-provisioning many idle sandboxes in memory
to reduce the occurrence of slow, cold starts. A key obstacle for high
elasticity is booting a guest OS and configuring features like networking in
sandboxes, which are required to expose an isolated POSIX-like interface to
user functions. Our key insight is that redesigning the interface for
applications in the cloud-native era enables co-designing a much more efficient
and elastic execution system. Now is a good time to rethink cloud abstractions
as developers are building applications to be cloud-native. Cloud-native
applications typically consist of user-provided compute logic interacting with
cloud services (for storage, AI inference, query processing, etc) exposed over
REST APIs. Hence, we propose Dandelion, an elastic cloud platform with a
declarative programming model that expresses applications as DAGs of pure
compute functions and higher-level communication functions. Dandelion can
securely execute untrusted user compute functions in lightweight sandboxes that
cold start in hundreds of microseconds, since pure functions do not rely on
extra software environments such as a guest OS. Dandelion makes it practical to
boot a sandbox on-demand for each request, decreasing performance variability
by two to three orders of magnitude compared to Firecracker and reducing
committed memory by 96% on average when running the Azure Functions trace.
</summary>
    <author>
      <name>Tom Kuchler</name>
    </author>
    <author>
      <name>Pinghe Li</name>
    </author>
    <author>
      <name>Yazhuo Zhang</name>
    </author>
    <author>
      <name>Lazar Cvetković</name>
    </author>
    <author>
      <name>Boris Goranov</name>
    </author>
    <author>
      <name>Tobias Stocker</name>
    </author>
    <author>
      <name>Leon Thomm</name>
    </author>
    <author>
      <name>Simone Kalbermatter</name>
    </author>
    <author>
      <name>Tim Notter</name>
    </author>
    <author>
      <name>Andrea Lattuada</name>
    </author>
    <author>
      <name>Ana Klimovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.01603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.01603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
