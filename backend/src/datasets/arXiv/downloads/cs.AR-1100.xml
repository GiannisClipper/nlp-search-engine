<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acs.AR%26id_list%3D%26start%3D0%26max_results%3D1100" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:cs.AR&amp;id_list=&amp;start=0&amp;max_results=1100</title>
  <id>http://arxiv.org/api/n85iJlJduEIHo4hfTWe0x36taC8</id>
  <updated>2025-05-27T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">6367</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1100</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/cs/0111032v1</id>
    <updated>2001-11-09T19:08:57Z</updated>
    <published>2001-11-09T19:08:57Z</published>
    <title>SNS Timing System</title>
    <summary>  This poster describes the timing system being designed for Spallation Neutron
Source being built at Oak Ridge National lab.
</summary>
    <author>
      <name>B. oerter</name>
    </author>
    <author>
      <name>R. Nelson</name>
    </author>
    <author>
      <name>T. Shea</name>
    </author>
    <author>
      <name>C. Sibley</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">eConf C011127 (2001) FRAT001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0111032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0111032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0602096v1</id>
    <updated>2006-02-27T18:59:39Z</updated>
    <published>2006-02-27T18:59:39Z</published>
    <title>Difficulties in the Implementation of Quantum Computers</title>
    <summary>  This paper reviews various engineering hurdles facing the field of quantum
computing. Specifically, problems related to decoherence, state preparation,
error correction, and implementability of gates are considered.
</summary>
    <author>
      <name>Abhilash Ponnath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0602096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0602096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0508038v1</id>
    <updated>2005-08-04T17:35:38Z</updated>
    <published>2005-08-04T17:35:38Z</published>
    <title>Quantum Algorithm Processor For Finding Exact Divisors</title>
    <summary>  Wiring diagrams are given for a quantum algorithm processor in CMOS to
compute, in parallel, all divisors of an n-bit integer. Lines required in a
wiring diagram are proportional to n. Execution time is proportional to the
square of n.
</summary>
    <author>
      <name>John Robert Burger</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0508038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0508038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.7.1; C.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.5364v1</id>
    <updated>2011-01-27T19:20:42Z</updated>
    <published>2011-01-27T19:20:42Z</published>
    <title>RISC and CISC</title>
    <summary>  Comparison of RISC &amp; CISC in details, encompassing the addressing modes,
evolution, definitions and characteristics. Pre - RISC design is also
elaborated. Both the architectures are explained with the help of example.
Analysis is made based on performance.
</summary>
    <author>
      <name>Farhat Masood</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computing Research Repository - CORR, vol. abs/1101.5, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1101.5364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.5364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00254v1</id>
    <updated>2018-03-01T08:58:21Z</updated>
    <published>2018-03-01T08:58:21Z</published>
    <title>45-year CPU evolution: one law and two equations</title>
    <summary>  Moore's law and two equations allow to explain the main trends of CPU
evolution since MOS technologies have been used to implement microprocessors.
</summary>
    <author>
      <name>Daniel Etiemble</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LRI</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Second Workshop on Pioneering Processor Paradigms, Feb 2018,
  Vienne, Austria</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.00254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.00151v1</id>
    <updated>2020-02-29T01:52:00Z</updated>
    <published>2020-02-29T01:52:00Z</published>
    <title>A Compiler Infrastructure for FPGA and ASIC Development</title>
    <summary>  This whitepaper proposes a unified framework for hardware design tools to
ease the development and inter-operability of said tools. By creating a large
ecosystem of hardware development tools across vendors, academia, and the open
source community, we hope to significantly increase much need productivity in
hardware design.
</summary>
    <author>
      <name>John Demme</name>
    </author>
    <link href="http://arxiv.org/abs/2003.00151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.00151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.09787v1</id>
    <updated>2020-09-21T12:06:59Z</updated>
    <published>2020-09-21T12:06:59Z</published>
    <title>A high-performance MEMRISTOR-based Smith-Waterman DNA sequence alignment
  Using FPNI structure</title>
    <summary>  This paper aims to present a new re-configuration sequencing method for
difference of read lengths that may take place as input data in which is
crucial drawbacks lay impact on DNA sequencing methods.
</summary>
    <author>
      <name>Mahdi Taheri</name>
    </author>
    <author>
      <name>Hamed Zandevakili</name>
    </author>
    <author>
      <name>Ali Mahani</name>
    </author>
    <link href="http://arxiv.org/abs/2009.09787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.06876v1</id>
    <updated>2021-02-13T07:05:39Z</updated>
    <published>2021-02-13T07:05:39Z</published>
    <title>Hardware Architecture of Wireless Power Transfer, RFID, and WIPT Systems</title>
    <summary>  In this work, we provide an overview of the hardware architecture of wireless
power transfer (WPT), RFID, and wireless information and power transfer (WIPT)
systems. The historical milestones and structure differences among WPT, RFID,
and WIPT are introduced.
</summary>
    <author>
      <name>Yu Luo</name>
    </author>
    <author>
      <name>Lina Pu</name>
    </author>
    <link href="http://arxiv.org/abs/2102.06876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.06876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.18024v1</id>
    <updated>2023-03-31T13:02:17Z</updated>
    <published>2023-03-31T13:02:17Z</published>
    <title>Proceedings of the 3rd Workshop on Open-Source Design Automation (OSDA),
  2023</title>
    <summary>  This volume represents the proceedings of the 3rd Workshop on Open-Source
Design Automation (OSDA) 2023, co-hosted with Design, Automation, and Test in
Europe (DATE) conference in Antwerp, Belgium, April 17, 2023.
</summary>
    <author>
      <name>Christian Krieg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 articles, presented at OSDA 2023, see also https://osda.ws/r/aw0ZG</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.18024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.18024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9912010v1</id>
    <updated>1999-12-18T02:19:42Z</updated>
    <published>1999-12-18T02:19:42Z</published>
    <title>Scalability Terminology: Farms, Clones, Partitions, Packs, RACS and RAPS</title>
    <summary>  Defines a vocabulary for scaleable systems: Geoplexes, Farms, Clones, RACS,
RAPS, clones, partitions, and packs and dicusses the design tradeoffs of using
clones, partitons, and packs.
</summary>
    <author>
      <name>Bill Devlin</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Bill Laing</name>
    </author>
    <author>
      <name>George Spix</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MSword version of file at
  http://research.microsoft.com/~gray/papers/MS_TR_99_85_Scalability_Terminolog
  y.doc</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9912010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9912010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0407019v2</id>
    <updated>2012-04-12T10:53:59Z</updated>
    <published>2004-07-08T06:52:37Z</published>
    <title>Stochastic fuzzy controller</title>
    <summary>  A standard approach to building a fuzzy controller based on stochastic logic
uses binary random signals with an average (expected value of a random
variable) in the range [0, 1]. A different approach is presented, founded on a
representation of the membership functions with the probability density
functions.
</summary>
    <author>
      <name>Franc Jurkovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Withdrawn by the author</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0407019v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0407019v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0409025v1</id>
    <updated>2004-09-13T11:08:09Z</updated>
    <published>2004-09-13T11:08:09Z</published>
    <title>Topics in asynchronous systems</title>
    <summary>  In the paper we define and characterize the asynchronous systems from the
point of view of their autonomy, determinism, order, non-anticipation, time
invariance, symmetry, stability and other important properties. The study is
inspired by the models of the asynchronous circuits.
</summary>
    <author>
      <name>Serban E. Vlad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Analele Universitatii din Oradea, Fascicola Matematica, TOM X,
  2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0409025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0409025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412040v1</id>
    <updated>2004-12-09T22:10:48Z</updated>
    <published>2004-12-09T22:10:48Z</published>
    <title>Data-stationary Architecture to Execute Quantum Algorithms Classically</title>
    <summary>  This paper presents a data stationary architecture in which each word has an
attached address field. Address fields massively update in parallel to record
data interchanges. Words do not move until memory is read for post processing.
A sea of such cells can test large-scale quantum algorithms, although other
programming is possible.
</summary>
    <author>
      <name>J. R. Burger</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0412040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703128v2</id>
    <updated>2007-04-08T08:29:51Z</updated>
    <published>2007-03-26T01:51:25Z</published>
    <title>Physarum machine: Implementation of Kolmogorov-Uspensky machine in
  biological substrat</title>
    <summary>  We implement Kolmogorov-Uspensky machine on a plasmodium of true slime mold
{\em Physarum polycephalum}. We provide experimental findings on realization of
the machine instructions, illustrate basic operations, and elements of
programming.
</summary>
    <author>
      <name>Andrew Adamatzky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0129626407003150</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0129626407003150" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First presented at Los Alamos "Unconventional Computation 2007"
  conference, Santa Fe, NM, 20-23 March 2007</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Paralllel Processing Letters Vol. 17, No. 4 (December 2007) pp.
  455-467</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0703128v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703128v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4832v1</id>
    <updated>2007-10-25T12:10:19Z</updated>
    <published>2007-10-25T12:10:19Z</published>
    <title>SystemC Analysis of a New Dynamic Power Management Architecture</title>
    <summary>  This paper presents a new dynamic power management architecture of a System
on Chip. The Power State Machine describing the status of the core follows the
recommendations of the ACPI standard. The algorithm controls the power states
of each block on the basis of battery status, chip temperature and a user
defined task priority.
</summary>
    <author>
      <name>Massimo Conti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.2584v2</id>
    <updated>2009-01-20T15:23:07Z</updated>
    <published>2008-08-19T12:31:07Z</published>
    <title>On Transformations of Load-Store Maurer Instruction Set Architecture</title>
    <summary>  In this paper, we study how certain conditions can affect the transformations
on the states of the memory of a strict load-store Maurer ISA, when half of the
data memory serves as the part of the operating unit.
</summary>
    <author>
      <name>Tie Hou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures; Corrected way of citing references</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.2584v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.2584v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.3871v1</id>
    <updated>2008-12-19T18:14:54Z</updated>
    <published>2008-12-19T18:14:54Z</published>
    <title>Decting Errors in Reversible Circuits With Invariant Relationships</title>
    <summary>  Reversible logic is experience renewed interest as we are approach the limits
of CMOS technologies. While physical implementations of reversible gates have
yet to materialize, it is safe to assume that they will rely on faulty
individual components. In this work we present a present a method to provide
fault tolerance to a reversible circuit based on invariant relationships.
</summary>
    <author>
      <name>Nuno Alves</name>
    </author>
    <link href="http://arxiv.org/abs/0812.3871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.3871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.4694v1</id>
    <updated>2010-01-26T14:11:18Z</updated>
    <published>2010-01-26T14:11:18Z</published>
    <title>VLSI Architectures for WIMAX Channel Decoders</title>
    <summary>  This chapter describes the main architectures proposed in the literature to
implement the channel decoders required by the WiMax standard, namely
convolutional codes, turbo codes (both block and convolutional) and LDPC. Then
it shows a complete design of a convolutional turbo code encoder/decoder system
for WiMax.
</summary>
    <author>
      <name>Maurizio Martina</name>
    </author>
    <author>
      <name>Guido Masera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the book "WIMAX, New Developments", M. Upena, D. Dalal,
  Y. Kosta (Ed.), ISBN978-953-7619-53-4</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.4694v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.4694v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.2729v1</id>
    <updated>2010-08-16T18:14:06Z</updated>
    <published>2010-08-16T18:14:06Z</published>
    <title>Asynchronous logic circuits and sheaf obstructions</title>
    <summary>  This article exhibits a particular encoding of logic circuits into a sheaf
formalism. The central result of this article is that there exists strictly
more information available to a circuit designer in this setting than exists in
static truth tables, but less than exists in event-level simulation. This
information is related to the timing behavior of the logic circuits, and
thereby provides a ``bridge'' between static logic analysis and detailed
simulation.
</summary>
    <author>
      <name>Michael Robinson</name>
    </author>
    <link href="http://arxiv.org/abs/1008.2729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.2729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.3681v1</id>
    <updated>2011-06-18T19:07:44Z</updated>
    <published>2011-06-18T19:07:44Z</published>
    <title>SoC Software Components Diagnosis Technology</title>
    <summary>  A novel approach to evaluation of hardware and software testability,
represented in the form of register transfer graph, is proposed. Instances of
making of software graph models for their subsequent testing and diagnosis are
shown.
</summary>
    <author>
      <name>Svetlana Chumachenko</name>
    </author>
    <author>
      <name>Wajeb Gharibi</name>
    </author>
    <author>
      <name>Anna Hahanova</name>
    </author>
    <author>
      <name>Aleksey Sushanov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/EWDTS.2008.5580135</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/EWDTS.2008.5580135" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of IEEE, East-West Design &amp; Test, Symposium
  (EWDTS'08),Lviv, Ukraine, October 9 - 12, 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1106.3681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.3681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.6865v1</id>
    <updated>2011-10-31T17:13:25Z</updated>
    <published>2011-10-31T17:13:25Z</published>
    <title>FPGA implementation of short critical path CORDIC-based approximation of
  the eight-point DCT</title>
    <summary>  This paper presents an efficient approach for multiplierless implementation
for eight-point DCT approximation, which based on coordinate rotation digital
computer (CORDIC) algorithm. The main design objective is to make critical path
of corresponding circuits shorter and reduce the combinational delay of
proposed scheme.
</summary>
    <author>
      <name>Maxim Vashkevich</name>
    </author>
    <author>
      <name>Marek Parfieniuk</name>
    </author>
    <author>
      <name>Alexander Petrovsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.6865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.6865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.5349v1</id>
    <updated>2012-03-22T10:48:33Z</updated>
    <published>2012-03-22T10:48:33Z</published>
    <title>LOCKE Detailed Specification Tables</title>
    <summary>  This document shows the detailed specification of LOCKE coherence protocol
for each cache controller, using a table-based technique. This representation
provides clear, concise visual information yet includes sufficient detail
(e.g., transient states) arguably lacking in the traditional, graphical form of
state diagrams.
</summary>
    <author>
      <name>Lucia G. Menezo</name>
    </author>
    <author>
      <name>Valentin Puente</name>
    </author>
    <author>
      <name>Jose-Angel Gregorio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.5349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.5349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.0995v1</id>
    <updated>2012-08-05T09:22:06Z</updated>
    <published>2012-08-05T09:22:06Z</published>
    <title>Design and implementation of a digital clock showing digits in Bangla
  font using microcontroller AT89C4051</title>
    <summary>  In this paper, a digital clock is designed where the microcontroller is used
for timing controller and the font of the Bangla digits are designed, and
programmed within the microcontroller. The design is cost effective, simple and
easy for maintenance.
</summary>
    <author>
      <name>Nasif Muslim</name>
    </author>
    <author>
      <name>Md. Tanvir Adnan</name>
    </author>
    <author>
      <name>Mohammad Zahidul Kabir</name>
    </author>
    <author>
      <name>Md. Humayun Kabir</name>
    </author>
    <author>
      <name>Sheikh Mominul Islam</name>
    </author>
    <link href="http://arxiv.org/abs/1208.0995v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.0995v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.5459v1</id>
    <updated>2013-09-21T10:33:36Z</updated>
    <published>2013-09-21T10:33:36Z</published>
    <title>Advances in computer architecture</title>
    <summary>  In the past, efforts were taken to improve the performance of a processor via
frequency scaling. However, industry has reached the limits of increasing the
frequency and therefore concurrent execution of instructions on multiple cores
seems the only possible option. It is not enough to provide concurrent
execution by the hardware, software also have to introduce concurrency in order
to exploit the parallelism.
</summary>
    <author>
      <name>Irfan Uddin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.5459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.5459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.4423v2</id>
    <updated>2015-02-27T05:40:38Z</updated>
    <published>2014-08-18T18:43:54Z</published>
    <title>Proceedings of the First International Workshop on FPGAs for Software
  Programmers (FSP 2014)</title>
    <summary>  This volume contains the papers accepted at the First International Workshop
on FPGAs for Software Programmers (FSP 2014), held in Munich, Germany,
September 1st, 2014. FSP 2014 was co-located with the International Conference
on Field Programmable Logic and Applications (FPL).
</summary>
    <author>
      <name>Frank Hannig</name>
    </author>
    <author>
      <name>Dirk Koch</name>
    </author>
    <author>
      <name>Daniel Ziener</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Website of the workshop: https://www12.cs.fau.de/ws/fsp2014/</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.4423v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.4423v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.6498v1</id>
    <updated>2014-11-24T16:00:47Z</updated>
    <published>2014-11-24T16:00:47Z</published>
    <title>Correction to the 2005 paper: "Digit Selection for SRT Division and
  Square Root"</title>
    <summary>  It has been pointed out by counterexamples in a 2013 paper in the IEEE
Transactions on Computers [1], that there is an error in the previously ibid.\
in 2005 published paper [2] on the construction of valid digit selection tables
for SRT type division and square root algorithms. The error has been corrected,
and new results found on selection constants for maximally redundant digit
sets.
</summary>
    <author>
      <name>Peter Kornerup</name>
    </author>
    <link href="http://arxiv.org/abs/1411.6498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.07420v1</id>
    <updated>2015-01-29T11:44:47Z</updated>
    <published>2015-01-29T11:44:47Z</published>
    <title>Tejas Simulator : Validation against Hardware</title>
    <summary>  In this report we show results that validate the Tejas architectural
simulator against native hardware. We report mean error rates of 11.45% and
18.77% for the SPEC2006 and Splash2 benchmark suites respectively. These error
rates are competitive and in most cases better than the numbers reported by
other contemporary simulators.
</summary>
    <author>
      <name>Smruti R. Sarangi</name>
    </author>
    <author>
      <name>Rajshekar Kalayappan</name>
    </author>
    <author>
      <name>Prathmesh Kallurkar</name>
    </author>
    <author>
      <name>Seep Goel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.07420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.07420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06320v1</id>
    <updated>2015-08-25T22:17:22Z</updated>
    <published>2015-08-25T22:17:22Z</published>
    <title>Proceedings of the Second International Workshop on FPGAs for Software
  Programmers (FSP 2015)</title>
    <summary>  This volume contains the papers accepted at the Second International Workshop
on FPGAs for Software Programmers (FSP 2015), held in London, United Kingdom,
September 1st, 2015. FSP 2015 was co-located with the International Conference
on Field Programmable Logic and Applications (FPL).
</summary>
    <author>
      <name>Frank Hannig</name>
    </author>
    <author>
      <name>Dirk Koch</name>
    </author>
    <author>
      <name>Daniel Ziener</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Website of the workshop: https://www12.cs.fau.de/ws/fsp2015/</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.06320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06452v1</id>
    <updated>2016-06-21T07:25:05Z</updated>
    <published>2016-06-21T07:25:05Z</published>
    <title>Reliability-Aware Overlay Architectures for FPGAs: Features and Design
  Challenges</title>
    <summary>  The FPGA overlay architectures have been mainly proposed to improve design
productivity, circuit portability and system debugging. In this paper, we
address the use of overlay architectures for building fault tolerant SRAM-based
FPGA systems and discuss the main features and design challenges of a
reliability-aware overlay architecture.
</summary>
    <author>
      <name>Mihalis Psarakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 2nd International Workshop on Overlay Architectures for
  FPGAs (OLAF 2016) arXiv:1605.08149</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.01894v1</id>
    <updated>2017-02-07T06:55:03Z</updated>
    <published>2017-02-07T06:55:03Z</published>
    <title>CAAD: Computer Architecture for Autonomous Driving</title>
    <summary>  We describe the computing tasks involved in autonomous driving, examine
existing autonomous driving computing platform implementations. To enable
autonomous driving, the computing stack needs to simultaneously provide high
performance, low power consumption, and low thermal dissipation, at low cost.
We discuss possible approaches to design computing platforms that will meet
these needs.
</summary>
    <author>
      <name>Shaoshan Liu</name>
    </author>
    <author>
      <name>Jie Tang</name>
    </author>
    <author>
      <name>Zhe Zhang</name>
    </author>
    <author>
      <name>Jean-Luc Gaudiot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures, accepted by IEEE Computer Magazine</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.01894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07280v1</id>
    <updated>2017-05-20T08:51:40Z</updated>
    <published>2017-05-20T08:51:40Z</published>
    <title>The Effect of Temperature on Amdahl Law in 3D Multicore Era</title>
    <summary>  This work studies the influence of temperature on performance and scalability
of 3D Chip Multiprocessors (CMP) from Amdahl law perspective. We find that 3D
CMP may reach its thermal limit before reaching its maximum power. We show that
a high level of parallelism may lead to high peak temperatures even in small
scale 3D CMPs, thus limiting 3D CMP scalability and calling for different,
in-memory computing architectures.
</summary>
    <author>
      <name>Leonid Yavits</name>
    </author>
    <author>
      <name>Amir Morad</name>
    </author>
    <author>
      <name>Ran Ginosar</name>
    </author>
    <link href="http://arxiv.org/abs/1705.07280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.09937v1</id>
    <updated>2017-05-28T13:00:11Z</updated>
    <published>2017-05-28T13:00:11Z</published>
    <title>Sparse Matrix Multiplication on CAM Based Accelerator</title>
    <summary>  Sparse matrix multiplication is an important component of linear algebra
computations. In this paper, an architecture based on Content Addressable
Memory (CAM) and Resistive Content Addressable Memory (ReCAM) is proposed for
accelerating sparse matrix by sparse vector and matrix multiplication in CSR
format. Using functional simulation, we show that the proposed ReCAM-based
accelerator exhibits two orders of magnitude higher power efficiency as
compared to existing sparse matrix-vector multiplication implementations.
</summary>
    <author>
      <name>Leonid Yavits</name>
    </author>
    <author>
      <name>Ran Ginosar</name>
    </author>
    <link href="http://arxiv.org/abs/1705.09937v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09937v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03251v1</id>
    <updated>2017-06-10T16:40:04Z</updated>
    <published>2017-06-10T16:40:04Z</published>
    <title>Proposal for a High Precision Tensor Processing Unit</title>
    <summary>  This whitepaper proposes the design and adoption of a new generation of
Tensor Processing Unit which has the performance of Google's TPU, yet performs
operations on wide precision data. The new generation TPU is made possible by
implementing arithmetic circuits which compute using a new general purpose,
fractional arithmetic based on the residue number system.
</summary>
    <author>
      <name>Eric B. Olsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.03251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.04614v1</id>
    <updated>2017-12-13T05:48:44Z</updated>
    <published>2017-12-13T05:48:44Z</published>
    <title>Applying the Residue Number System to Network Inference</title>
    <summary>  This work explores the lesser studied objective of optimizing the
multiply-and-accumulates executed during evaluation of the network. In
particular, we propose using the Residue Number System (RNS) as the internal
number representation across all layer evaluations, allowing us to explore
usage of the more power-efficient RNS multipliers and adders. Using results
from simulation of our RNS arithmetic block implementations, we show
theoretical power advantages of using RNS for an end-to-end evaluator.
</summary>
    <author>
      <name>Mohamed Abdelhamid</name>
    </author>
    <author>
      <name>Skanda Koppula</name>
    </author>
    <link href="http://arxiv.org/abs/1712.04614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05215v2</id>
    <updated>2022-03-11T08:36:18Z</updated>
    <published>2018-01-16T11:42:56Z</published>
    <title>Trends in Processor Architecture</title>
    <summary>  This paper presents an overview of the main trends in processor architecture.
It starts with an analysis of the past evolution of processors and the main
driving forces behind it, and then it focuses on a description of the main
architectural features of current processors. Finally, it presents a discussion
on some promising directions for future evolution of processor architectures.
</summary>
    <author>
      <name>Antonio Gonzalez</name>
    </author>
    <link href="http://arxiv.org/abs/1801.05215v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05215v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.00532v1</id>
    <updated>2020-05-31T14:31:30Z</updated>
    <published>2020-05-31T14:31:30Z</published>
    <title>How to extend the Single-Processor Paradigm to the Explicitly
  Many-Processor Approach</title>
    <summary>  The computing paradigm invented for processing a small amount of data on a
single segregated processor cannot meet the challenges set by the present-day
computing demands. The paper proposes a new computing paradigm (extending the
old one to use several processors explicitly) and discusses some questions of
its possible implementation. Some advantages of the implemented approach,
illustrated with the results of a loosely-timed simulator, are presented.
</summary>
    <author>
      <name>János Végh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.00532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.00532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.01516v1</id>
    <updated>2021-01-05T14:06:43Z</updated>
    <published>2021-01-05T14:06:43Z</published>
    <title>Best CNTFET Ternary Adders?</title>
    <summary>  The MUX implementation of ternary half adders and full adders using
predecessor and successor functions lead to the most efficient efficient
implementation using the smallest transistor count. These designs are compared
with the binary implementation of the corresponding half adders and full adders
using the MUX technique or the typical complementary CMOS circuit style. The
transistor count ratio between ternary and binary implementations is always
greater than the information ratio ($log_2(3)/log_2(2)$ = 1.585) between
ternary and binary wires.
</summary>
    <author>
      <name>Daniel Etiemble</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.01516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.01516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.01176v1</id>
    <updated>2021-03-01T18:16:14Z</updated>
    <published>2021-03-01T18:16:14Z</published>
    <title>Layering the monitoring action for improved flexibility and overhead
  control: work-in-progress</title>
    <summary>  With the diffusion of complex heterogeneous platforms and their need of
characterization, monitoring the system gained increasing interest. This work
proposes a framework to build custom and modular monitoring systems, flexible
enough to face the heterogeneity of modern platforms, offering a predictable
HW/SW impact.
</summary>
    <author>
      <name>Giacomo Valente</name>
    </author>
    <author>
      <name>Tiziana Fanni</name>
    </author>
    <author>
      <name>Carlo Sau</name>
    </author>
    <author>
      <name>Francesco Di Battista</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CODESISSS51650.2020.9244018</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CODESISSS51650.2020.9244018" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2020 International Conference on Hardware/Software Codesign and
  System Synthesis (CODES+ISSS), September 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2103.01176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.01176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.09849v1</id>
    <updated>2021-10-19T11:06:06Z</updated>
    <published>2021-10-19T11:06:06Z</published>
    <title>Holistic Hardware Security Assessment Framework: A Microarchitectural
  Perspective</title>
    <summary>  Our goal is to enable holistic hardware security evaluation from the
microarchitectural point of view. To achieve this, we propose a framework that
categorizes threat models based on the microarchitectural components being
targeted, and provides a generic security metric that can be used to assess the
vulnerability of components, as well as the system as a whole.
</summary>
    <author>
      <name>Tochukwu Idika</name>
    </author>
    <author>
      <name>Ismail Akturk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in the program of Energy-Secure System Architectures (ESSA)
  Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.09849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.09849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.06584v1</id>
    <updated>2021-11-12T07:01:44Z</updated>
    <published>2021-11-12T07:01:44Z</published>
    <title>Elastic Silicon Interconnects: Abstracting Communication in Accelerator
  Design</title>
    <summary>  Communication is an important part of accelerator design, though it is under
researched and under developed. Today, designers often face relatively
low-level communication tools requiring them to design straightforward but
error-prone plumbing. In this paper, we argue that raising the level of
abstraction could yield correctness, productivity, and performance benefits not
only for RTL-level designers but also for high level language developers.
</summary>
    <author>
      <name>John Demme</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Microsoft</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2111.06584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.06584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.13306v1</id>
    <updated>2021-12-26T01:58:04Z</updated>
    <published>2021-12-26T01:58:04Z</published>
    <title>Asynchronous Memory Access Unit for General Purpose Processors</title>
    <summary>  In future data centers, applications will make heavy use of far memory
(including disaggregated memory pools and NVM). The access latency of far
memory is more widely distributed than that of local memory accesses. This
makes the efficiency of traditional blocking load/store in most general-purpose
processors decrease in this scenario. Therefore, this work proposes an in-core
asynchronous memory access unit.
</summary>
    <author>
      <name>Luming Wang</name>
    </author>
    <author>
      <name>Xu Zhang</name>
    </author>
    <author>
      <name>Tianyue Lu</name>
    </author>
    <author>
      <name>Mingyu Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2112.13306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.13306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.02085v2</id>
    <updated>2022-04-23T20:19:27Z</updated>
    <published>2022-04-05T09:53:19Z</published>
    <title>High-throughput Pairwise Alignment with the Wavefront Algorithm using
  Processing-in-Memory</title>
    <summary>  We show that the wavefront algorithm can achieve higher pairwise read
alignment throughput on a UPMEM PIM system than on a server-grade
multi-threaded CPU system.
</summary>
    <author>
      <name>Safaa Diab</name>
    </author>
    <author>
      <name>Amir Nassereldine</name>
    </author>
    <author>
      <name>Mohammed Alser</name>
    </author>
    <author>
      <name>Juan Gómez Luna</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <author>
      <name>Izzat El Hajj</name>
    </author>
    <link href="http://arxiv.org/abs/2204.02085v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.02085v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10435v1</id>
    <updated>2017-11-20T16:33:31Z</updated>
    <published>2017-11-20T16:33:31Z</published>
    <title>LP-Based Power Grid Enhancement Methodology</title>
    <summary>  In this paper, we explored the opportunity to enhance power grid robustness
after routing stage, and propose a linear programming based algorithm that
maximizes the improvement of power grid strengthening with given available
routing resource. We further discussed some techniques to leverage tradeoffs
between runtime and optimality of the solutions. Experimental results show
substantial power integrity improvement with "zero cost".
</summary>
    <author>
      <name>Tapio Bohn</name>
    </author>
    <author>
      <name>Paul Salmi</name>
    </author>
    <author>
      <name>Albert Milner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.10435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.03147v1</id>
    <updated>2018-09-10T06:06:16Z</updated>
    <published>2018-09-10T06:06:16Z</published>
    <title>Is Leakage Power a Linear Function of Temperature?</title>
    <summary>  In this work, we present a study of the leakage power modeling techniques
commonly used in the architecture community. We further provide an analysis of
the error in leakage power estimation using the various modeling techniques. We
strongly believe that this study will help researchers determine an appropriate
leakage model to use in their work, based on the desired modeling accuracy and
speed.
</summary>
    <author>
      <name>Hameedah Sultan</name>
    </author>
    <author>
      <name>Shashank Varshney</name>
    </author>
    <author>
      <name>Smruti R Sarangi</name>
    </author>
    <link href="http://arxiv.org/abs/1809.03147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.03147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.06559v1</id>
    <updated>2019-09-14T08:36:32Z</updated>
    <published>2019-09-14T08:36:32Z</published>
    <title>Instructional Level Parallelism</title>
    <summary>  This paper is a review of the developments in Instruction level parallelism.
It takes into account all the changes made in speeding up the execution. The
various drawbacks and dependencies due to pipelining are discussed and various
solutions to overcome them are also incorporated. It goes ahead in the last
section to explain where is the new research leading us.
</summary>
    <author>
      <name>Taposh Dutta-Roy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Branch Prediction, Exceptions, Instructional level parallelism,
  pipelining</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.06559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.06559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.10154v1</id>
    <updated>2019-09-23T04:40:37Z</updated>
    <published>2019-09-23T04:40:37Z</published>
    <title>Implementation of Goldschmidt's Algorithm with hardware reduction</title>
    <summary>  Division algorithms have been developed to reduce latency and to improve the
efficiency of the processors. Floating point division is considered as a high
latency operation. This papers looks into one such division algorithm, examines
the hardware block diagram and suggests an alternative path which may be cost
effective.
</summary>
    <author>
      <name>Taposh Dutta Roy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Goldschmidt's Algorithm, 3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.10154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.10154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.12195v1</id>
    <updated>2019-09-26T15:35:28Z</updated>
    <published>2019-09-26T15:35:28Z</published>
    <title>Appearances of the Birthday Paradox in High Performance Computing</title>
    <summary>  We give an elementary statistical analysis of two High Performance Computing
issues, processor cache mapping and network port mapping. In both cases we find
that, as in the birthday paradox, random assignment leads to more frequent
coincidences than one expects a priori. Since these correspond to contention
for limited resources, this phenomenon has important consequences for
performance.
</summary>
    <author>
      <name>Victor Eijkhout</name>
    </author>
    <author>
      <name>Margaret Myers</name>
    </author>
    <author>
      <name>John McCalpin</name>
    </author>
    <link href="http://arxiv.org/abs/1909.12195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.12195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.02951v1</id>
    <updated>2022-09-07T06:23:09Z</updated>
    <published>2022-09-07T06:23:09Z</published>
    <title>Democratizing Domain-Specific Computing</title>
    <summary>  In the past few years, domain-specific accelerators (DSAs), such as Google's
Tensor Processing Units, have shown to offer significant performance and energy
efficiency over general-purpose CPUs. An important question is whether typical
software developers can design and implement their own customized DSAs, with
affordability and efficiency, to accelerate their applications. This article
presents our answer to this question.
</summary>
    <author>
      <name>Yuze Chi</name>
    </author>
    <author>
      <name>Weikang Qiao</name>
    </author>
    <author>
      <name>Atefeh Sohrabizadeh</name>
    </author>
    <author>
      <name>Jie Wang</name>
    </author>
    <author>
      <name>Jason Cong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in CACM'22</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.02951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.02951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.08324v1</id>
    <updated>2023-02-14T13:40:31Z</updated>
    <published>2023-02-14T13:40:31Z</published>
    <title>A Bit-Parallel Deterministic Stochastic Multiplier</title>
    <summary>  This paper presents a novel bit-parallel deterministic stochastic multiplier,
which improves the area-energy-latency product by up to 10.6$\times$10$^4$,
while improving the computational error by 32.2\%, compared to three prior
stochastic multipliers.
</summary>
    <author>
      <name>Sairam Sri Vatsavai</name>
    </author>
    <author>
      <name>Ishan Thakkar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To Appear at IEEE ISQED 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.08324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.08324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.09952v1</id>
    <updated>2023-04-19T20:06:08Z</updated>
    <published>2023-04-19T20:06:08Z</published>
    <title>Baugh-Wooley Multiplication for the RISCV Processor</title>
    <summary>  This article describes an efficient way to implement the multiplication
instructions for a RISCV processor. Instead of using three predefined IP blocks
for signed, unsigned and mixed multiplication, this article presents a novel
extension to the Baugh-Wooley multiplication algorithm which reduces area and
power consumption with roughly a factor three.
</summary>
    <author>
      <name>Franc Grootjen</name>
    </author>
    <author>
      <name>Nikolai Schauer</name>
    </author>
    <link href="http://arxiv.org/abs/2304.09952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.09952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.10426v2</id>
    <updated>2024-01-04T03:39:14Z</updated>
    <published>2023-12-16T11:45:53Z</published>
    <title>Branch Prediction in Hardcaml for a RISC-V 32im CPU</title>
    <summary>  Accurate branch prediction is a critical part of high performance instruction
stream processing. In this paper, I present a hardware implementation of branch
prediction for a RV32IM CPU, starting with static decode stage predictions and
culminating in the use of BATAGE. In addition, I detail my experience writing
the RTL in Hardcaml, a hardware description library for the functional
programming language OCaml.
</summary>
    <author>
      <name>Alex Saveau</name>
    </author>
    <link href="http://arxiv.org/abs/2312.10426v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.10426v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.14578v1</id>
    <updated>2023-12-22T10:14:44Z</updated>
    <published>2023-12-22T10:14:44Z</published>
    <title>CPS Workshop 2023 Proceedings</title>
    <summary>  These proceedings contain the contributions to the CPS workshop 2023
(http://www.cpsschool.eu/cps-workshop/). The CPS Workshop 2023 is an initiative
of the CPS Summer School 2023 community to offer participants close contact
with leading experts in the field and the opportunity to present and discuss
their ideas in a dynamic and friendly setting.
</summary>
    <author>
      <name>Christian Pilato</name>
    </author>
    <author>
      <name>Francesca Palumbo</name>
    </author>
    <link href="http://arxiv.org/abs/2312.14578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.14578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.09464v1</id>
    <updated>2024-01-08T13:14:41Z</updated>
    <published>2024-01-08T13:14:41Z</published>
    <title>Floating Point HUB Adder for RISC-V Sargantana Processor</title>
    <summary>  HUB format is an emerging technique to improve the hardware and time
requirement when round to nearest is needed. On the other hand, RISC-V is an
open-source ISA that many companies currently use in their designs. This paper
presents a tailored floating point HUB adder implemented in the Sargantana
RISC-V processor.
</summary>
    <author>
      <name>Gerardo Bandera</name>
    </author>
    <author>
      <name>Javier Salamero</name>
    </author>
    <author>
      <name>Miquel Moreto</name>
    </author>
    <author>
      <name>Julio Villalba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RISC-V Summit Europe, Barcelona, 5-9th June 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.09464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.09464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.17796v1</id>
    <updated>2024-06-12T14:17:21Z</updated>
    <published>2024-06-12T14:17:21Z</published>
    <title>Hypervisor Extension for a RISC-V Processor</title>
    <summary>  This paper describes our experience implementing a Hypervisor extension for a
64-bit RISC-V processor. We describe the design process and the main required
parts with a brief explanation of each one.
</summary>
    <author>
      <name>Jaume Gauchola</name>
    </author>
    <author>
      <name>JuanJosé Costa</name>
    </author>
    <author>
      <name>Enric Morancho</name>
    </author>
    <author>
      <name>Ramon Canal</name>
    </author>
    <author>
      <name>Xavier Carril</name>
    </author>
    <author>
      <name>Max Doblas</name>
    </author>
    <author>
      <name>Beatriz Otero</name>
    </author>
    <author>
      <name>Alex Pajuelo</name>
    </author>
    <author>
      <name>Eva Rodríguez</name>
    </author>
    <author>
      <name>Javier Salamero</name>
    </author>
    <author>
      <name>Javier Verdú</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RISC-V Summit Europe 2023, June 5-9, 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.17796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.17796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.20351v1</id>
    <updated>2024-12-29T04:48:33Z</updated>
    <published>2024-12-29T04:48:33Z</published>
    <title>Wavelet Based Frequency Detection Using FPGAs</title>
    <summary>  In the realm of signal processing, frequency and spectrum detection are
fundamental tasks that can be computationally intensive. This project leverages
the power of FPGAs to perform wavelet analysis on an input signal. The goal is
to detect the presence of a specific frequency component - in this case, 6 kHz.
Our experiments demonstrate that wavelet-based spectral detection is both
possible, and easily implemented using an FPGA.
</summary>
    <author>
      <name>Caleb Hill</name>
    </author>
    <author>
      <name>Darshika G. Perera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.20351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.20351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.00153v1</id>
    <updated>2025-01-31T20:44:10Z</updated>
    <published>2025-01-31T20:44:10Z</published>
    <title>Theoretical complexity analysis of many-cores on a single chip</title>
    <summary>  When a single core is scaled up to m cores occupying the same chip area and
executing the same (parallelizable) task, achievable speedup is square-root m,
power is reduced by square-root m and energy is reduced by m. Thus, many-core
architectures can efficiently outperform architectures of a single core and a
small-count multi-core.
</summary>
    <author>
      <name>Ran Ginosar</name>
    </author>
    <link href="http://arxiv.org/abs/2502.00153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.00153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07511v1</id>
    <updated>2016-11-12T19:14:05Z</updated>
    <published>2016-11-12T19:14:05Z</published>
    <title>Can Broken Multicore Hardware be Mended?</title>
    <summary>  A suggestion is made for mending multicore hardware, which has been diagnosed
as broken.
</summary>
    <author>
      <name>János Végh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 figures; a Viewpoint</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.07511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809007v1</id>
    <updated>1998-09-02T17:30:05Z</updated>
    <published>1998-09-02T17:30:05Z</published>
    <title>Locally Served Network Computers</title>
    <summary>  NCs are the natural evolution of PCs, ubiquitous computers everywhere. The
current vision of NCs requires two improbable developments: (1) inexpensive
high-bandwidth WAN links to the Internet, and (2) inexpensive centralized
servers. The large NC bandwidth requirements will force each home or office to
have a local server LAN attached to the NCs. These servers will be much less
expensive to purchase and manage than a centralized solution. Centralized staff
are expensive and unresponsive.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Original document at:
  http://research.microsoft.com/~gray/NC_Servers.doc</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Middleware Spectra, 11.2, 1997</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9809007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0207012v1</id>
    <updated>2002-07-04T04:30:30Z</updated>
    <published>2002-07-04T04:30:30Z</published>
    <title>Synthesis of Low-Power Digital Circuits Derived from Binary Decision
  Diagrams</title>
    <summary>  This paper introduces a novel method for synthesizing digital circuits
derived from Binary Decision Diagrams (BDDs) that can yield to reduction in
power dissipation. The power reduction is achieved by decreasing the switching
activity in a circuit while paying close attention to information measures as
an optimization criterion. We first present the technique of efficient
BDD-based computation of information measures which are used to guide the power
optimization procedures. Using this technique, we have developed an algorithm
of BDD reordering which leads to reducing the power consumption of the circuits
derived from BDDs. Results produced by the synthesis on the ISCAS benchmark
circuits are very encouraging.
</summary>
    <author>
      <name>Denis V. Popel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, 1 table, ECCTD'01</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ECCTD 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0207012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0207012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.6.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0207014v1</id>
    <updated>2002-07-04T04:55:42Z</updated>
    <published>2002-07-04T04:55:42Z</published>
    <title>On the Information Engine of Circuit Design</title>
    <summary>  This paper addresses a new approach to find a spectrum of information
measures for the process of digital circuit synthesis. We consider the problem
from the information engine point of view. The circuit synthesis as a whole and
different steps of the design process (an example of decision diagram is given)
are presented via such measurements as entropy, logical work and information
vitality. We also introduce new information measures to provide better
estimates of synthesis criteria. We show that the basic properties of
information engine, such as the conservation law of information flow and the
equilibrium law of information can be formulated.
</summary>
    <author>
      <name>Denis V. Popel</name>
    </author>
    <author>
      <name>Nawar Al-Hakeem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure, 2 tables, MWSCAS'02</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">MWSCAS 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0207014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0207014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.6.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307035v1</id>
    <updated>2003-07-13T13:01:19Z</updated>
    <published>2003-07-13T13:01:19Z</published>
    <title>Adaptive Domain Model: Dealing With Multiple Attributes of Self-Managing
  Distributed Object Systems</title>
    <summary>  Self-managing software has emerged as modern systems have become more
complex. Some of the distributed object systems may contain thousands of
objects deployed on tens or even hundreds hosts. Development and support of
such systems often costs a lot. To solve this issue the systems, which are
capable supporting multiple self-managing attributes, should be created. In the
paper, the Adaptive domain concept is introduced as an extension to the basic
domain concept to support a generic adaptation environment for building
distributed object systems with multiple self-managing attributes.
</summary>
    <author>
      <name>Pavel Motuzenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0307035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0407032v1</id>
    <updated>2004-07-13T08:24:37Z</updated>
    <published>2004-07-13T08:24:37Z</published>
    <title>Exposing Software Defined Radio Functionality To Native Operating System
  Applications via Virtual Devices</title>
    <summary>  Many reconfigurable platforms require that applications be written
specifically to take advantage of the reconfigurable hardware. In a PC-based
environment, this presents an undesirable constraint in that the many already
available applications cannot leverage on such hardware. Greatest benefit can
only be derived from reconfigurable devices if even native OS applications can
transparently utilize reconfigurable devices as they would normal full-fledged
hardware devices. This paper presents how Proteus Virtual Devices are used to
expose reconfigurable hardware in a transparent manner for use by typical
native OS applications.
</summary>
    <author>
      <name>Darran Nathan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0407032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0407032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0411075v1</id>
    <updated>2004-11-20T06:31:32Z</updated>
    <published>2004-11-20T06:31:32Z</published>
    <title>A Self-Reconfigurable Computing Platform Hardware Architecture</title>
    <summary>  Field Programmable Gate Arrays (FPGAs) have recently been increasingly used
for highly-parallel processing of compute intensive tasks. This paper
introduces an FPGA hardware platform architecture that is PC-based, allows for
fast reconfiguration over the PCI bus, and retains a simple physical hardware
design. The design considerations are first discussed, then the resulting
system architecture designed is illustrated. Finally, experimental results on
the FPGA resources utilized for this design are presented.
</summary>
    <author>
      <name>Andreas Weisensee</name>
    </author>
    <author>
      <name>Darran Nathan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0411075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0411075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412093v1</id>
    <updated>2004-12-20T11:55:30Z</updated>
    <published>2004-12-20T11:55:30Z</published>
    <title>ScotGrid: A Prototype Tier 2 Centre</title>
    <summary>  ScotGrid is a prototype regional computing centre formed as a collaboration
between the universities of Durham, Edinburgh and Glasgow as part of the UK's
national particle physics grid, GridPP. We outline the resources available at
the three core sites and our optimisation efforts for our user communities. We
discuss the work which has been conducted in extending the centre to embrace
new projects both from particle physics and new user communities and explain
our methodology for doing this.
</summary>
    <author>
      <name>A. Earl</name>
    </author>
    <author>
      <name>P. Clark</name>
    </author>
    <author>
      <name>S. Thorn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 diagrams. Presented at Computing for High Energy and
  Nuclear Physics 2004 (CHEP '04). Interlaken, Switzerland, September 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0412093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0512104v1</id>
    <updated>2005-12-28T19:59:10Z</updated>
    <published>2005-12-28T19:59:10Z</published>
    <title>Reversible CAM Processor Modeled After Quantum Computer Behavior</title>
    <summary>  Proposed below is a reversible digital computer modeled after the natural
behavior of a quantum system. Using approaches usually reserved for idealized
quantum computers, the Reversible CAM, or State Vector Parallel (RSVP)
processor can easily find keywords in an unstructured database (that is, it can
solve a needle in a haystack problem). The RSVP processor efficiently solves a
SAT (Satisfiability of Boolean Formulae) problem; also it can aid in the
solution of a GP (Global Properties of Truth Table) problem. The power delay
product of the RSVP processor is exponentially lower than that of a standard
CAM programmed to perform similar operations.
</summary>
    <author>
      <name>John Robert Burger</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0512104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0512104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.2; C.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0602078v2</id>
    <updated>2006-02-23T23:59:29Z</updated>
    <published>2006-02-21T21:58:06Z</published>
    <title>Associative Memory For Reversible Programming and Charge Recovery</title>
    <summary>  Presented below is an interesting type of associative memory called toggle
memory based on the concept of T flip flops, as opposed to D flip flops. Toggle
memory supports both reversible programming and charge recovery. Circuits
designed using the principles delineated below permit matchlines to charge and
discharge with near zero energy dissipation. The resulting lethargy is
compensated by the massive parallelism of associative memory. Simulation
indicates over 33x reduction in energy dissipation using a sinusoidal power
supply at 2 MHz, assuming realistic 50 nm MOSFET models.
</summary>
    <author>
      <name>John Robert Burger</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0602078v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0602078v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.2; C.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603115v1</id>
    <updated>2006-03-29T11:48:29Z</updated>
    <published>2006-03-29T11:48:29Z</published>
    <title>Implementation of float-float operators on graphics hardware</title>
    <summary>  The Graphic Processing Unit (GPU) has evolved into a powerful and flexible
processor. The latest graphic processors provide fully programmable vertex and
pixel processing units that support vector operations up to single
floating-point precision. This computational power is now being used for
general-purpose computations. However, some applications require higher
precision than single precision. This paper describes the emulation of a 44-bit
floating-point number format and its corresponding operations. An
implementation is presented along with performance and accuracy results.
</summary>
    <author>
      <name>Guillaume Da Graçca</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LP2A</arxiv:affiliation>
    </author>
    <author>
      <name>David Defour</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LP2A</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/cs/0603115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605125v1</id>
    <updated>2006-05-26T18:43:13Z</updated>
    <published>2006-05-26T18:43:13Z</published>
    <title>Combinational Logic Circuit Design with the Buchberger Algorithm</title>
    <summary>  We detail a procedure for the computation of the polynomial form of an
electronic combinational circuit from the design equations in a truth table.
The method uses the Buchberger algorithm rather than current traditional
methods based on search algorithms. We restrict the analysis to a single
output, but the procedure can be generalized to multiple outputs. The procedure
is illustrated with the design of a simple arithmetic and logic unit with two
3-bit operands and two control bits.
</summary>
    <author>
      <name>Germain Drolet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Electrical &amp; Computer Engineering, Royal Military College of Canada</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0605125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605142v1</id>
    <updated>2006-05-30T15:04:18Z</updated>
    <published>2006-05-30T15:04:18Z</published>
    <title>Intégration de la synthèse mémoire dans l'outil de
  synthèse d'architecture GAUT Low Power</title>
    <summary>  The systems supporting signal and image applications process large amount of
data. That involves an intensive use of the memory which becomes the bottleneck
of systems. Memory limits performances and represents a significant proportion
of total consumption. In the development high level synthesis tool called GAUT
Low Power, we are interested in the synthesis of the memory unit. In this work,
we integrate the data storage and data transfert to constraint the high level
synthesis of the datapath's execution unit.
</summary>
    <author>
      <name>Gwenolé Corre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Nathalie Julien</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Senn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JFAAA'02 (Journ\'{e}es Francophone Ad\'{e}quation Algorithme
  Architecture), Tunisie (2002)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0605142v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605142v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605144v1</id>
    <updated>2006-05-30T15:09:18Z</updated>
    <published>2006-05-30T15:09:18Z</published>
    <title>A Memory Aware High Level Synthesis Too</title>
    <summary>  We introduce a new approach to take into account the memory architecture and
the memory mapping in High- Level Synthesis for data intensive applications. We
formalize the memory mapping as a set of constraints for the synthesis, and
defined a Memory Constraint Graph and an accessibility criterion to be used in
the scheduling step. We use a memory mapping file to include those memory
constraints in our HLS tool GAUT. It is possible, with the help of GAUT, to
explore a wide range of solutions, and to reach a good tradeoff between time,
power-consumption, and area.
</summary>
    <author>
      <name>Gwenolé Corre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Nathalie Julien</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Senn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISBN 0-7695-2097-9</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Symposium on VLSI (2004) 279-280</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0605144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605145v1</id>
    <updated>2006-05-30T15:09:32Z</updated>
    <published>2006-05-30T15:09:32Z</published>
    <title>Memory Aware High-Level Synthesis for Embedded Systems</title>
    <summary>  We introduce a new approach to take into account the memory architecture and
the memory mapping in the High- Level Synthesis of Real-Time embedded systems.
We formalize the memory mapping as a set of constraints used in the scheduling
step. We use a memory mapping file to include those memory constraints in our
HLS tool GAUT. Our scheduling algorithm exhibits a relatively low complexity
that permits to tackle complex designs in a reasonable time. Finally, we show
how to explore, with the help of GAUT, a wide range of solutions, and to reach
a good tradeoff between time, power-consumption, and area.
</summary>
    <author>
      <name>Gwenolé Corre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Senn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Nathalie Julien</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IADIS conference on Applied Computing, Portugal (2004) 499-506</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0605145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702062v1</id>
    <updated>2007-02-11T09:52:12Z</updated>
    <published>2007-02-11T09:52:12Z</published>
    <title>Noise Limited Computational Speed</title>
    <summary>  In modern transistor based logic gates, the impact of noise on computation
has become increasingly relevant since the voltage scaling strategy, aimed at
decreasing the dissipated power, has increased the probability of error due to
the reduced switching threshold voltages. In this paper we discuss the role of
noise in a two state model that mimic the dynamics of standard logic gates and
show that the presence of the noise sets a fundamental limit to the computing
speed. An optimal idle time interval that minimizes the error probability, is
derived.
</summary>
    <author>
      <name>Luca Gammaitoni</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.2817968</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.2817968" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">L. Gammaitoni, Applied Physics Letters, 11/2007, Volume 91, p.3,
  (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0702062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.1.3; B.7.0; B.8.1; B.8.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.1496v1</id>
    <updated>2007-08-10T18:12:52Z</updated>
    <published>2007-08-10T18:12:52Z</published>
    <title>A Light-Based Device for Solving the Hamiltonian Path Problem</title>
    <summary>  In this paper we suggest the use of light for performing useful computations.
Namely, we propose a special device which uses light rays for solving the
Hamiltonian path problem on a directed graph. The device has a graph-like
representation and the light is traversing it following the routes given by the
connections between nodes. In each node the rays are uniquely marked so that
they can be easily identified. At the destination node we will search only for
particular rays that have passed only once through each node. We show that the
proposed device can solve small and medium instances of the problem in
reasonable time.
</summary>
    <author>
      <name>Mihai Oltean</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/11839132</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/11839132" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, Unconventional Computation conference, 2006</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNCS 4135, Unconventional Computation conference, pp. 217-227,
  2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0708.1496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.1496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.1512v1</id>
    <updated>2007-08-10T20:01:24Z</updated>
    <published>2007-08-10T20:01:24Z</published>
    <title>Solving the Hamiltonian path problem with a light-based computer</title>
    <summary>  In this paper we propose a special computational device which uses light rays
for solving the Hamiltonian path problem on a directed graph. The device has a
graph-like representation and the light is traversing it by following the
routes given by the connections between nodes. In each node the rays are
uniquely marked so that they can be easily identified. At the destination node
we will search only for particular rays that have passed only once through each
node. We show that the proposed device can solve small and medium instances of
the problem in reasonable time.
</summary>
    <author>
      <name>Mihai Oltean</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11047-007-9042-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11047-007-9042-z" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, Natural Computing journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Natural Computing, Springer, Vol 6, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0708.1512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.1512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.3443v1</id>
    <updated>2007-10-18T06:57:52Z</updated>
    <published>2007-10-18T06:57:52Z</published>
    <title>DPA on quasi delay insensitive asynchronous circuits: formalization and
  improvement</title>
    <summary>  The purpose of this paper is to formally specify a flow devoted to the design
of Differential Power Analysis (DPA) resistant QDI asynchronous circuits. The
paper first proposes a formal modeling of the electrical signature of QDI
asynchronous circuits. The DPA is then applied to the formal model in order to
identify the source of leakage of this type of circuits. Finally, a complete
design flow is specified to minimize the information leakage. The relevancy and
efficiency of the approach is demonstrated using the design of an AES
crypto-processor.
</summary>
    <author>
      <name>G. F. Bouesse</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TIMA</arxiv:affiliation>
    </author>
    <author>
      <name>M. Renaudin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TIMA</arxiv:affiliation>
    </author>
    <author>
      <name>S. Dumont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TIMA</arxiv:affiliation>
    </author>
    <author>
      <name>F. Germain</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DATE.2005.124</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DATE.2005.124" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.3443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.3443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4632v1</id>
    <updated>2007-10-25T08:07:52Z</updated>
    <published>2007-10-25T08:07:52Z</published>
    <title>Hardware Support for Arbitrarily Complex Loop Structures in Embedded
  Applications</title>
    <summary>  In this paper, the program control unit of an embedded RISC processor is
enhanced with a novel zero-overhead loop controller (ZOLC) supporting arbitrary
loop structures with multiple-entry/exit nodes. The ZOLC has been incorporated
to an open RISC processor core to evaluate the performance of the proposed unit
for alternative configurations of the selected processor. It is proven that
speed improvements of 8.4% to 48.2% are feasible for the used benchmarks.
</summary>
    <author>
      <name>Nikolaos Kavvadias</name>
    </author>
    <author>
      <name>Spiridon Nikolaidis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4634v1</id>
    <updated>2007-10-25T08:08:50Z</updated>
    <published>2007-10-25T08:08:50Z</published>
    <title>A Probabilistic Collocation Method Based Statistical Gate Delay Model
  Considering Process Variations and Multiple Input Switching</title>
    <summary>  Since the advent of new nanotechnologies, the variability of gate delay due
to process variations has become a major concern. This paper proposes a new
gate delay model that includes impact from both process variations and multiple
input switching. The proposed model uses orthogonal polynomial based
probabilistic collocation method to construct a delay analytical equation from
circuit timing performance. From the experimental results, our approach has
less that 0.2% error on the mean delay of gates and less than 3% error on the
standard deviation.
</summary>
    <author>
      <name>Y. Satish Kumar</name>
    </author>
    <author>
      <name>Jun Li</name>
    </author>
    <author>
      <name>Claudio Talarico</name>
    </author>
    <author>
      <name>Janet Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DATE.2005.31</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DATE.2005.31" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4636v1</id>
    <updated>2007-10-25T08:09:23Z</updated>
    <published>2007-10-25T08:09:23Z</published>
    <title>Why Systems-on-Chip Needs More UML like a Hole in the Head</title>
    <summary>  Let's be clear from the outset: SoC can most certainly make use of UML; SoC
just doesn't need more UML, or even all of it. The advent of model mappings,
coupled with marks that indicate which mapping rule to apply, enable a major
simplification of the use of UML in SoC.
</summary>
    <author>
      <name>Stephen J. Mellor</name>
    </author>
    <author>
      <name>John R. Wolfe</name>
    </author>
    <author>
      <name>Campbell Mccausland</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4645v1</id>
    <updated>2007-10-25T08:19:34Z</updated>
    <published>2007-10-25T08:19:34Z</published>
    <title>At-Speed Logic BIST for IP Cores</title>
    <summary>  This paper describes a flexible logic BIST scheme that features high fault
coverage achieved by fault-simulation guided test point insertion, real
at-speed test capability for multi-clock designs without clock frequency
manipulation, and easy physical implementation due to the use of a low-speed SE
signal. Application results of this scheme to two widely used IP cores are also
reported.
</summary>
    <author>
      <name>B. Cheon</name>
    </author>
    <author>
      <name>E. Lee</name>
    </author>
    <author>
      <name>L. -T. Wang</name>
    </author>
    <author>
      <name>X. Wen</name>
    </author>
    <author>
      <name>P. Hsu</name>
    </author>
    <author>
      <name>J. Cho</name>
    </author>
    <author>
      <name>J. Park</name>
    </author>
    <author>
      <name>H. Chao</name>
    </author>
    <author>
      <name>S. Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4646v1</id>
    <updated>2007-10-25T08:20:27Z</updated>
    <published>2007-10-25T08:20:27Z</published>
    <title>Fast Dynamic Memory Integration in Co-Simulation Frameworks for
  Multiprocessor System on-Chip</title>
    <summary>  In this paper is proposed a technique to integrate and simulate a dynamic
memory in a multiprocessor framework based on C/C++/SystemC. Using host
machine's memory management capabilities, dynamic data processing is supported
without compromising speed and accuracy of the simulation. A first prototype in
a shared memory context is presented.
</summary>
    <author>
      <name>O. Villa</name>
    </author>
    <author>
      <name>P. Schaumont</name>
    </author>
    <author>
      <name>I. Verbauwhede</name>
    </author>
    <author>
      <name>M. Monchiero</name>
    </author>
    <author>
      <name>G. Palermo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4657v1</id>
    <updated>2007-10-25T08:34:59Z</updated>
    <published>2007-10-25T08:34:59Z</published>
    <title>New Schemes for Self-Testing RAM</title>
    <summary>  This paper gives an overview of a new technique, named pseudo-ring testing
(PRT). PRT can be applied for testing wide type of random access memories
(RAM): bit- or word-oriented and single- or dual-port RAM's. An essential
particularity of the proposed methodology is the emulation of a linear
automaton over Galois field by memory own components.
</summary>
    <author>
      <name>Gh. Bodean</name>
    </author>
    <author>
      <name>D. Bodean</name>
    </author>
    <author>
      <name>A. Labunetz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4659v1</id>
    <updated>2007-10-25T08:35:37Z</updated>
    <published>2007-10-25T08:35:37Z</published>
    <title>Synchronization Processor Synthesis for Latency Insensitive Systems</title>
    <summary>  In this paper we present our contribution in terms of synchronization
processor for a SoC design methodology based on the theory of the latency
insensitive systems (LIS) of Carloni et al. Our contribution consists in IP
encapsulation into a new wrapper model which speed and area are optimized and
synthetizability guarantied. The main benefit of our approach is to preserve
the local IP performances when encapsulating them and reduce SoC silicon area.
</summary>
    <author>
      <name>Pierre Bomel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Emmanuel Boutillon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4659v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4659v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4667v1</id>
    <updated>2007-10-25T08:44:47Z</updated>
    <published>2007-10-25T08:44:47Z</published>
    <title>Integration, Verification and Layout of a Complex Multimedia SOC</title>
    <summary>  We present our experience of designing a single-chip controller for advanced
digital still camera from specification all the way to mass production. The
process involves collaboration with camera system designer, IP vendors, EDA
vendors, silicon wafer foundry, package and testing houses, and camera maker.
We also co-work with academic research groups to develop a JPEG codec IP and
memory BIST and SOC testing methodology. In this presentation, we cover the
problems encountered, our solutions, and lessons learned.
</summary>
    <author>
      <name>Chien-Liang Chen</name>
    </author>
    <author>
      <name>Jiing-Yuan Lin</name>
    </author>
    <author>
      <name>Youn-Long Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4669v1</id>
    <updated>2007-10-25T08:45:18Z</updated>
    <published>2007-10-25T08:45:18Z</published>
    <title>SOC Testing Methodology and Practice</title>
    <summary>  On a commercial digital still camera (DSC) controller chip we practice a
novel SOC test integration platform, solving real problems in test scheduling,
test IO reduction, timing of functional test, scan IO sharing, embedded memory
built-in self-test (BIST), etc. The chip has been fabricated and tested
successfully by our approach. Test results justify that short test integration
cost, short test time, and small area overhead can be achieved. To support SOC
testing, a memory BIST compiler and an SOC testing integration system have been
developed.
</summary>
    <author>
      <name>Cheng-Wen Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4670v1</id>
    <updated>2007-10-25T08:45:52Z</updated>
    <published>2007-10-25T08:45:52Z</published>
    <title>Evolutionary Optimization in Code-Based Test Compression</title>
    <summary>  We provide a general formulation for the code-based test compression problem
with fixed-length input blocks and propose a solution approach based on
Evolutionary Algorithms. In contrast to existing code-based methods, we allow
unspecified values in matching vectors, which allows encoding of arbitrary test
sets using a relatively small number of code-words. Experimental results for
both stuck-at and path delay fault test sets for ISCAS circuits demonstrate an
improvement compared to existing techniques.
</summary>
    <author>
      <name>Ilia Polian</name>
    </author>
    <author>
      <name>Alejandro Czutro</name>
    </author>
    <author>
      <name>Bernd Becker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4672v1</id>
    <updated>2007-10-25T08:47:40Z</updated>
    <published>2007-10-25T08:47:40Z</published>
    <title>Yield Enhancement of Digital Microfluidics-Based Biochips Using Space
  Redundancy and Local Reconfiguration</title>
    <summary>  As microfluidics-based biochips become more complex, manufacturing yield will
have significant influence on production volume and product cost. We propose an
interstitial redundancy approach to enhance the yield of biochips that are
based on droplet-based microfluidics. In this design method, spare cells are
placed in the interstitial sites within the microfluidic array, and they
replace neighboring faulty cells via local reconfiguration. The proposed design
method is evaluated using a set of concurrent real-life bioassays.
</summary>
    <author>
      <name>Fei Su</name>
    </author>
    <author>
      <name>Krishnendu Chakrabarty</name>
    </author>
    <author>
      <name>Vamsee K. Pamula</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4678v1</id>
    <updated>2007-10-25T09:02:53Z</updated>
    <published>2007-10-25T09:02:53Z</published>
    <title>CMOS-Based Biosensor Arrays</title>
    <summary>  CMOS-based sensor array chips provide new and attractive features as compared
to today's standard tools for medical, diagnostic, and biotechnical
applications. Examples for molecule- and cell-based approaches and related
circuit design issues are discussed.
</summary>
    <author>
      <name>R. Thewes</name>
    </author>
    <author>
      <name>C. Paulus</name>
    </author>
    <author>
      <name>M. Schienle</name>
    </author>
    <author>
      <name>F. Hofmann</name>
    </author>
    <author>
      <name>A. Frey</name>
    </author>
    <author>
      <name>R. Brederlow</name>
    </author>
    <author>
      <name>M. Augustyniak</name>
    </author>
    <author>
      <name>M. Jenkner</name>
    </author>
    <author>
      <name>B. Eversmann</name>
    </author>
    <author>
      <name>P. Schindler-Bauer</name>
    </author>
    <author>
      <name>M. Atzesberger</name>
    </author>
    <author>
      <name>B. Holzapfl</name>
    </author>
    <author>
      <name>G. Beer</name>
    </author>
    <author>
      <name>T. Haneder</name>
    </author>
    <author>
      <name>H. -C. Hanke</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4692v1</id>
    <updated>2007-10-25T09:11:49Z</updated>
    <published>2007-10-25T09:11:49Z</published>
    <title>Cantilever-Based Biosensors in CMOS Technology</title>
    <summary>  Single-chip CMOS-based biosensors that feature microcantilevers as transducer
elements are presented. The cantilevers are functionalized for the capturing of
specific analytes, e.g., proteins or DNA. The binding of the analyte changes
the mechanical properties of the cantilevers such as surface stress and
resonant frequency, which can be detected by an integrated Wheatstone bridge.
The monolithic integrated readout allows for a high signal-to-noise ratio,
lowers the sensitivity to external interference and enables autonomous device
operation.
</summary>
    <author>
      <name>K. -U. Kirstein</name>
    </author>
    <author>
      <name>Y. Li</name>
    </author>
    <author>
      <name>M. Zimmermann</name>
    </author>
    <author>
      <name>C. Vancura</name>
    </author>
    <author>
      <name>T. Volden</name>
    </author>
    <author>
      <name>W. H. Song</name>
    </author>
    <author>
      <name>J. Lichtenberg</name>
    </author>
    <author>
      <name>A. Hierlemannn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4706v1</id>
    <updated>2007-10-25T09:29:33Z</updated>
    <published>2007-10-25T09:29:33Z</published>
    <title>An Infrastructure to Functionally Test Designs Generated by Compilers
  Targeting FPGAs</title>
    <summary>  This paper presents an infrastructure to test the functionality of the
specific architectures output by a high-level compiler targeting dynamically
reconfigurable hardware. It results in a suitable scheme to verify the
architectures generated by the compiler, each time new optimization techniques
are included or changes in the compiler are performed. We believe this kind of
infrastructure is important to verify, by functional simulation, further
research techniques, as far as compilation to Field-Programmable Gate Array
(FPGA) platforms is concerned.
</summary>
    <author>
      <name>Rui Rodrigues</name>
    </author>
    <author>
      <name>Joao M. P. Cardoso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4711v1</id>
    <updated>2007-10-25T09:32:43Z</updated>
    <published>2007-10-25T09:32:43Z</published>
    <title>FPGA Architecture for Multi-Style Asynchronous Logic</title>
    <summary>  This paper presents a novel FPGA architecture for implementing various styles
of asynchronous logic. The main objective is to break the dependency between
the FPGA architecture dedicated to asynchronous logic and the logic style. The
innovative aspects of the architecture are described. Moreover the structure is
well suited to be rebuilt and adapted to fit with further asynchronous logic
evolutions thanks to the architecture genericity. A full-adder was implemented
in different styles of logic to show the architecture flexibility.
</summary>
    <author>
      <name>N. Huot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TIMA</arxiv:affiliation>
    </author>
    <author>
      <name>H. Dubreuil</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TIMA</arxiv:affiliation>
    </author>
    <author>
      <name>L. Fesquet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TIMA</arxiv:affiliation>
    </author>
    <author>
      <name>M. Renaudin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TIMA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4711v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4711v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4712v1</id>
    <updated>2007-10-25T09:33:14Z</updated>
    <published>2007-10-25T09:33:14Z</published>
    <title>An Accurate SER Estimation Method Based on Propagation Probability</title>
    <summary>  In this paper, we present an accurate but very fast soft error rate (SER)
estimation technique for digital circuits based on error propagation
probability (EPP) computation. Experiments results and comparison of the
results with the random simulation technique show that our proposed method is
on average within 6% of the random simulation method and four to five orders of
magnitude faster.
</summary>
    <author>
      <name>Ghazanfar Asadi</name>
    </author>
    <author>
      <name>Mehdi B. Tahoori</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4722v1</id>
    <updated>2007-10-25T09:36:56Z</updated>
    <published>2007-10-25T09:36:56Z</published>
    <title>Designer-Driven Topology Optimization for Pipelined Analog to Digital
  Converters</title>
    <summary>  This paper suggests a practical "hybrid" synthesis methodology which
integrates designer-derived analytical models for system-level description with
simulation-based models at the circuit level. We show how to optimize
stage-resolution to minimize the power in a pipelined ADC. Exploration (via
detailed synthesis) of several ADC configurations is used to show that a
4-3-2... resolution distribution uses the least power for a 13-bit 40 MSPS
converter in a 0.25 $\mu$m CMOS process.
</summary>
    <author>
      <name>Yu-Tsun Chien</name>
    </author>
    <author>
      <name>Dong Chen</name>
    </author>
    <author>
      <name>Jea-Hong Lou</name>
    </author>
    <author>
      <name>Gin-Kou Ma</name>
    </author>
    <author>
      <name>Rob A. Rutenbar</name>
    </author>
    <author>
      <name>Tamal Mukherjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4724v1</id>
    <updated>2007-10-25T09:37:45Z</updated>
    <published>2007-10-25T09:37:45Z</published>
    <title>Systematic Figure of Merit Computation for the Design of Pipeline ADC</title>
    <summary>  The emerging concept of SoC-AMS leads to research new top-down methodologies
to aid systems designers in sizing analog and mixed devices. This work applies
this idea to the high-level optimization of pipeline ADC. Considering a given
technology, it consists in comparing different configurations according to
their imperfections and their architectures without FFT computation or
time-consuming simulations. The final selection is based on a figure of merit.
</summary>
    <author>
      <name>L. Barrandon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IETR</arxiv:affiliation>
    </author>
    <author>
      <name>S. Crand</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IETR</arxiv:affiliation>
    </author>
    <author>
      <name>D. Houzet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IETR</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4731v1</id>
    <updated>2007-10-25T09:40:02Z</updated>
    <published>2007-10-25T09:40:02Z</published>
    <title>Leakage-Aware Interconnect for On-Chip Network</title>
    <summary>  On-chip networks have been proposed as the interconnect fabric for future
systems-on-chip and multi-processors on chip. Power is one of the main
constraints of these systems and interconnect consumes a significant portion of
the power budget. In this paper, we propose four leakage-aware interconnect
schemes. Our schemes achieve 10.13%~63.57% active leakage savings and
12.35%~95.96% standby leakage savings across schemes while the delay penalty
ranges from 0% to 4.69%.
</summary>
    <author>
      <name>Yuh-Fang Tsai</name>
    </author>
    <author>
      <name>Vijaykrishnan Narayaynan</name>
    </author>
    <author>
      <name>Yuan Xie</name>
    </author>
    <author>
      <name>Mary Jane Irwin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4731v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4731v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4733v1</id>
    <updated>2007-10-25T09:41:13Z</updated>
    <published>2007-10-25T09:41:13Z</published>
    <title>Smart Temperature Sensor for Thermal Testing of Cell-Based ICs</title>
    <summary>  In this paper we present a simple and efficient built-in temperature sensor
for thermal monitoring of standard-cell based VLSI circuits. The proposed smart
temperature sensor uses a ring-oscillator composed of complex gates instead of
inverters to optimize their linearity. Simulation results from a 0.18$\mu$m
CMOS technology show that the non-linearity error of the sensor can be reduced
when an adequate set of standard logic gates is selected.
</summary>
    <author>
      <name>S. A. Bota</name>
    </author>
    <author>
      <name>M. Rosales</name>
    </author>
    <author>
      <name>J. L. Rossello</name>
    </author>
    <author>
      <name>J. Segura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4736v1</id>
    <updated>2007-10-25T09:42:35Z</updated>
    <published>2007-10-25T09:42:35Z</published>
    <title>A New Embedded Measurement Structure for eDRAM Capacitor</title>
    <summary>  The embedded DRAM (eDRAM) is more and more used in System On Chip (SOC). The
integration of the DRAM capacitor process into a logic process is challenging
to get satisfactory yields. The specific process of DRAM capacitor and the low
capacitance value (~30F) of this device induce problems of process monitoring
and failure analysis. We propose a new test structure to measure the
capacitance value of each DRAM cell capacitor in a DRAM array. This concept has
been validated by simulation on a 0.18$\mu$m eDRAM technology.
</summary>
    <author>
      <name>L. Lopez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">L2MP</arxiv:affiliation>
    </author>
    <author>
      <name>J. M. Portal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">L2MP</arxiv:affiliation>
    </author>
    <author>
      <name>D. Nee</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ST-Rousset</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4748v1</id>
    <updated>2007-10-25T09:49:10Z</updated>
    <published>2007-10-25T09:49:10Z</published>
    <title>Systematic Transaction Level Modeling of Embedded Systems with SystemC</title>
    <summary>  This paper gives an overview of a transaction level modeling (TLM) design
flow for straightforward embedded system design with SystemC. The goal is to
systematically develop both application-specific HW and SW components of an
embedded system using the TLM approach, thus allowing for fast communication
architecture exploration, rapid prototyping and early embedded SW development.
To this end, we specify the lightweight transaction-based communication
protocol SHIP and present a methodology for automatic mapping of the
communication part of a system to a given architecture, including HW/SW
interfaces.
</summary>
    <author>
      <name>Wolfgang Klingauf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4754v1</id>
    <updated>2007-10-25T09:52:56Z</updated>
    <published>2007-10-25T09:52:56Z</published>
    <title>Design of a Virtual Component Neutral Network-on-Chip Transaction Layer</title>
    <summary>  Research studies have demonstrated the feasibility and advantages of
Network-on-Chip (NoC) over traditional bus-based architectures but have not
focused on compatibility communication standards. This paper describes a number
of issues faced when designing a VC-neutral NoC, i.e. compatible with standards
such as AHB 2.0, AXI, VCI, OCP, and various other proprietary protocols, and
how a layered approach to communication helps solve these issues.
</summary>
    <author>
      <name>Philippe Martin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4761v1</id>
    <updated>2007-10-25T09:55:04Z</updated>
    <published>2007-10-25T09:55:04Z</published>
    <title>Low-Cost Multi-Gigahertz Test Systems Using CMOS FPGAs and PECL</title>
    <summary>  This paper describes two research projects that develop new low-cost
techniques for testing devices with multiple high-speed (2 to 5 Gbps) signals.
Each project uses commercially available components to keep costs low, yet
achieves performance characteristics comparable to (and in some ways exceeding)
more expensive ATE. A common CMOS FPGA-based logic core provides flexibility,
adaptability, and communication with controlling computers while customized
positive emitter-coupled logic (PECL) achieves multi-gigahertz data rates with
about $\pm$25ps timing accuracy.
</summary>
    <author>
      <name>D. C. Keezer</name>
    </author>
    <author>
      <name>C. Gray</name>
    </author>
    <author>
      <name>A. Majid</name>
    </author>
    <author>
      <name>N. Taher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4762v1</id>
    <updated>2007-10-25T09:55:21Z</updated>
    <published>2007-10-25T09:55:21Z</published>
    <title>Area-Efficient Selective Multi-Threshold CMOS Design Methodology for
  Standby Leakage Power Reduction</title>
    <summary>  This paper presents a design flow for an improved selective
multi-threshold(Selective-MT) circuit. The Selective-MT circuit is improved so
that plural MT-cells can share one switch transistor. We propose the design
methodology from RTL(Register Transfer Level) to final layout with optimizing
switch transistor structure.
</summary>
    <author>
      <name>Takeshi Kitahara</name>
    </author>
    <author>
      <name>Naoyuki Kawabe</name>
    </author>
    <author>
      <name>Fimihiro Minami</name>
    </author>
    <author>
      <name>Katsuhiro Seta</name>
    </author>
    <author>
      <name>Toshiyuki Furusawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4763v1</id>
    <updated>2007-10-25T09:55:27Z</updated>
    <published>2007-10-25T09:55:27Z</published>
    <title>Logic Design for On-Chip Test Clock Generation - Implementation Details
  and Impact on Delay Test Quality</title>
    <summary>  This paper addresses delay test for SOC devices with high frequency clock
domains. A logic design for on-chip high-speed clock generation, implemented to
avoid expensive test equipment, is described in detail. Techniques for on-chip
clock generation, meant to reduce test vector count and to increase test
quality, are discussed. ATPG results for the proposed techniques are given.
</summary>
    <author>
      <name>Matthias Beck</name>
    </author>
    <author>
      <name>Olivier Barondeau</name>
    </author>
    <author>
      <name>Martin Kaibel</name>
    </author>
    <author>
      <name>Frank Poehl</name>
    </author>
    <author>
      <name>Xijiang Lin</name>
    </author>
    <author>
      <name>Ron Press</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4764v1</id>
    <updated>2007-10-25T09:55:48Z</updated>
    <published>2007-10-25T09:55:48Z</published>
    <title>Hotspot Prevention Through Runtime Reconfiguration in Network-On-Chip</title>
    <summary>  Many existing thermal management techniques focus on reducing the overall
power consumption of the chip, and do not address location-specific temperature
problems referred to as hotspots. We propose the use of dynamic runtime
reconfiguration to shift the hotspot-inducing computation periodically and make
the thermal profile more uniform. Our analysis shows that dynamic
reconfiguration is an effective technique in reducing hotspots for NoCs.
</summary>
    <author>
      <name>G. M. Link</name>
    </author>
    <author>
      <name>N. Vijaykrishnan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4814v1</id>
    <updated>2007-10-25T12:01:15Z</updated>
    <published>2007-10-25T12:01:15Z</published>
    <title>picoArray Technology: The Tool's Story</title>
    <summary>  This paper briefly describes the picoArray? architecture, and in particular
the deterministic internal communication fabric. The methods that have been
developed for debugging and verifying systems using devices from the picoArray
family are explained. In order to maximize the computational ability of these
devices, hardware debugging support has been kept to a minimum and the methods
and tools developed to take this into account.
</summary>
    <author>
      <name>Andrew Duller</name>
    </author>
    <author>
      <name>Daniel Towner</name>
    </author>
    <author>
      <name>Gajinder Panesar</name>
    </author>
    <author>
      <name>Alan Gray</name>
    </author>
    <author>
      <name>Will Robbins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4824v1</id>
    <updated>2007-10-25T12:04:43Z</updated>
    <published>2007-10-25T12:04:43Z</published>
    <title>FPGA based Agile Algorithm-On-Demand Co-Processor</title>
    <summary>  With growing computational needs of many real-world applications, frequently
changing specifications of standards, and the high design and NRE costs of
ASICs, an algorithm-agile FPGA based co-processor has become a viable
alternative. In this article, we report about the general design of an
algorith-agile co-processor and the proof-of-concept implementation.
</summary>
    <author>
      <name>R. Pradeep</name>
    </author>
    <author>
      <name>S. Vinay</name>
    </author>
    <author>
      <name>Sanjay Burman</name>
    </author>
    <author>
      <name>V. Kamakoti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4825v1</id>
    <updated>2007-10-25T12:05:33Z</updated>
    <published>2007-10-25T12:05:33Z</published>
    <title>Meeting the Embedded Design Needs of Automotive Applications</title>
    <summary>  The importance of embedded systems in driving innovation in automotive
applications continues to grow. Understanding the specific needs of developers
targeting this market is also helping to drive innovation in RISC core design.
This paper describes how a RISC instruction set architecture has evolved to
better meet those needs, and the key implementation features in two very
different RISC cores are used to demonstrate the challenges of designing for
real-time automotive systems.
</summary>
    <author>
      <name>Wayne Lyons</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4845v1</id>
    <updated>2007-10-25T12:21:19Z</updated>
    <published>2007-10-25T12:21:19Z</published>
    <title>Evaluation of SystemC Modelling of Reconfigurable Embedded Systems</title>
    <summary>  This paper evaluates the use of pin and cycle accurate SystemC models for
embedded system design exploration and early software development. The target
system is MicroBlaze VanillaNet Platform running MicroBlaze uClinux operating
system. The paper compares Register Transfer Level (RTL) Hardware Description
Language (HDL) simulation speed to the simulation speed of several different
SystemC models. It is shown that simulation speed of pin and cycle accurate
models can go up to 150 kHz, compared to 100 Hz range of HDL simulation.
Furthermore, utilising techniques that temporarily compromise cycle accuracy,
effective simulation speed of up to 500 kHz can be obtained.
</summary>
    <author>
      <name>Tero Rissa</name>
    </author>
    <author>
      <name>Adam Donlin</name>
    </author>
    <author>
      <name>Wayne Luk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4850v1</id>
    <updated>2007-10-25T12:24:59Z</updated>
    <published>2007-10-25T12:24:59Z</published>
    <title>Hardware Support for QoS-based Function Allocation in Reconfigurable
  Systems</title>
    <summary>  This contribution presents a new approach for allocating suitable
function-implementation variants depending on given quality-of-service
function-requirements for run-time reconfigurable multi-device systems. Our
approach adapts methodologies from the domain of knowledge-based systems which
can be used for doing run-time hardware/software resource usage optimizations.
</summary>
    <author>
      <name>Michael Ullmann</name>
    </author>
    <author>
      <name>Wansheng Jin</name>
    </author>
    <author>
      <name>Jurgen Becker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4850v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4850v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.2383v1</id>
    <updated>2007-11-15T11:55:30Z</updated>
    <published>2007-11-15T11:55:30Z</published>
    <title>Decoding the Golden Code: a VLSI design</title>
    <summary>  The recently proposed Golden code is an optimal space-time block code for 2 X
2 multiple-input multiple-output (MIMO) systems. The aim of this work is the
design of a VLSI decoder for a MIMO system coded with the Golden code. The
architecture is based on a rearrangement of the sphere decoding algorithm that
achieves maximum-likelihood (ML) decoding performance. Compared to other
approaces, the proposed solution exhibits an inherent flexibility in terms of
modulation schemes QAM modulation size and this makes our architecture
particularly suitable for adaptive modulation schemes.
</summary>
    <author>
      <name>Barbara Cerato</name>
    </author>
    <author>
      <name>Guido Masera</name>
    </author>
    <author>
      <name>Emanuele Viterbo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0711.2383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.2383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.7.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.2201v1</id>
    <updated>2008-01-15T15:44:28Z</updated>
    <published>2008-01-15T15:44:28Z</published>
    <title>Policies of System Level Pipeline Modeling</title>
    <summary>  Pipelining is a well understood and often used implementation technique for
increasing the performance of a hardware system. We develop several SystemC/C++
modeling techniques that allow us to quickly model, simulate, and evaluate
pipelines. We employ a small domain specific language (DSL) based on resource
usage patterns that automates the drudgery of boilerplate code needed to
configure connectivity in simulation models. The DSL is embedded directly in
the host modeling language SystemC/C++. Additionally we develop several
techniques for parameterizing a pipeline's behavior based on policies of
function, communication, and timing (performance modeling).
</summary>
    <author>
      <name>Ed Harcourt</name>
    </author>
    <link href="http://arxiv.org/abs/0801.2201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.2201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3441v1</id>
    <updated>2008-02-23T13:11:13Z</updated>
    <published>2008-02-23T13:11:13Z</published>
    <title>Efficient implementation of GALS systems over commercial synchronous
  FPGAs: a new approach</title>
    <summary>  The new vision presented is aimed to overcome the logic overhead issues that
previous works exhibit when applying GALS techniques to programmable logic
devices. The proposed new view relies in a 2-phase, bundled data parity based
protocol for data transfer and clock generation tasks. The ability of the
introduced methodology for smart real-time delay selection allows the
implementation of a variety of new methodologies for electromagnetic
interference mitigation and device environment changes adaptation.
</summary>
    <author>
      <name>Javier D. Garcia-Lasheras</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">English version of the paper presented in the Spanish Workshop on
  Reconfigurable Computing and Applications, Zaragoza (2007)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">"Implementacion eficiente de sistemas GALS sobre FPGAs", Jornadas
  de Computacion Reconfigurable y Aplicaciones (JCRA'07), Zaragoza (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.3441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.6.1; B.7.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.2602v2</id>
    <updated>2008-08-20T21:16:55Z</updated>
    <published>2008-08-19T14:44:05Z</published>
    <title>Easily testable logical networks based on a 'widened long flip-flop'</title>
    <summary>  The article describes an attempt to solve at once three basic problems
arising at testing a complex digital equipment for defects: 1) the problem of
an exponential increasing of the complexity of testing the equipment with the
complexity of the equipment; 2) the problem of testing of the tester; 3) the
problem of a mutual masking of defects. The proposed solution is nothing more
than using certain limitations for connections between usual logical gates.
Arbitrary multiple stuck-at-faults are supposed as defects.
</summary>
    <author>
      <name>Nick Stukach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">64 pages, including 35 figures and 10 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.2602v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.2602v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8; C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.4081v1</id>
    <updated>2009-01-26T19:54:27Z</updated>
    <published>2009-01-26T19:54:27Z</published>
    <title>Adaptive FPGA NoC-based Architecture for Multispectral Image Correlation</title>
    <summary>  An adaptive FPGA architecture based on the NoC (Network-on-Chip) approach is
used for the multispectral image correlation. This architecture must contain
several distance algorithms depending on the characteristics of spectral images
and the precision of the authentication. The analysis of distance algorithms is
required which bases on the algorithmic complexity, result precision, execution
time and the adaptability of the implementation. This paper presents the
comparison of these distance computation algorithms on one spectral database.
The result of a RGB algorithm implementation was discussed.
</summary>
    <author>
      <name>Linlin Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAHC</arxiv:affiliation>
    </author>
    <author>
      <name>Anne Claire Legrand</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAHC</arxiv:affiliation>
    </author>
    <author>
      <name>Virginie Fresse</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAHC</arxiv:affiliation>
    </author>
    <author>
      <name>Viktor Fischer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAHC</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0901.4081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.4081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.3832v1</id>
    <updated>2009-06-20T22:56:19Z</updated>
    <published>2009-06-20T22:56:19Z</published>
    <title>Hardware Trojan by Hot Carrier Injection</title>
    <summary>  This paper discusses how hot carrier injection (HCI) can be exploited to
create a trojan that will cause hardware failures. The trojan is produced not
via additional logic circuitry but by controlled scenarios that maximize and
accelerate the HCI effect in transistors. These scenarios range from
manipulating the manufacturing process to varying the internal voltage
distribution. This new type of trojan is difficult to test due to its gradual
hardware degradation mechanism. This paper describes the HCI effect, detection
techniques and discusses the possibility for maliciously induced HCI trojans.
</summary>
    <author>
      <name>Y. Shiyanovskii</name>
    </author>
    <author>
      <name>F. Wolff</name>
    </author>
    <author>
      <name>C. Papachristou</name>
    </author>
    <author>
      <name>D. Weyer</name>
    </author>
    <author>
      <name>W. Clay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.3832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.3832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.3834v1</id>
    <updated>2009-06-20T23:12:45Z</updated>
    <published>2009-06-20T23:12:45Z</published>
    <title>Exploiting Semiconductor Properties for Hardware Trojans</title>
    <summary>  This paper discusses the possible introduction of hidden reliability defects
during CMOS foundry fabrication processes that may lead to accelerated wearout
of the devices. These hidden defects or hardware Trojans can be created by
deviation from foundry design rules and processing parameters. The Trojans are
produced by exploiting time-based wearing mechanisms (HCI, NBTI, TDDB and EM)
and/or condition-based triggers (ESD, Latchup and Softerror). This class of
latent damage is difficult to test due to its gradual degradation nature. The
paper describes life-time expectancy results for various Trojan induced
scenarios. Semiconductor properties, processing and design parameters critical
for device reliability and Trojan creation are discussed.
</summary>
    <author>
      <name>Y. Shiyanovskii</name>
    </author>
    <author>
      <name>F. Wolff</name>
    </author>
    <author>
      <name>C. Papachristou</name>
    </author>
    <author>
      <name>D. Weyer</name>
    </author>
    <author>
      <name>W. Clay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.3834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.3834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1876v1</id>
    <updated>2009-09-10T07:29:52Z</updated>
    <published>2009-09-10T07:29:52Z</published>
    <title>Turbo NOC: a framework for the design of Network On Chip based turbo
  decoder architectures</title>
    <summary>  This work proposes a general framework for the design and simulation of
network on chip based turbo decoder architectures. Several parameters in the
design space are investigated, namely the network topology, the parallelism
degree, the rate at which messages are sent by processing nodes over the
network and the routing strategy. The main results of this analysis are: i) the
most suited topologies to achieve high throughput with a limited complexity
overhead are generalized de-Bruijn and generalized Kautz topologies; ii)
depending on the throughput requirements different parallelism degrees, message
injection rates and routing algorithms can be used to minimize the network area
overhead.
</summary>
    <author>
      <name>Maurizio Martina</name>
    </author>
    <author>
      <name>Guido Masera</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCSI.2010.2046257</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCSI.2010.2046257" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IEEE Trans. on Circuits and Systems I (submission date
  27 may 2009)</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.3736v1</id>
    <updated>2009-10-20T04:01:51Z</updated>
    <published>2009-10-20T04:01:51Z</published>
    <title>A Fault-tolerant Structure for Reliable Multi-core Systems Based on
  Hardware-Software Co-design</title>
    <summary>  To cope with the soft errors and make full use of the multi-core system, this
paper gives an efficient fault-tolerant hardware and software co-designed
architecture for multi-core systems. And with a not large number of test
patterns, it will use less than 33% hardware resources compared with the
traditional hardware redundancy (TMR) and it will take less than 50% time
compared with the traditional software redundancy (time redundant).Therefore,
it will be a good choice for the fault-tolerant architecture for the future
high-reliable multi-core systems.
</summary>
    <author>
      <name>Bingbing Xia</name>
    </author>
    <author>
      <name>Fei Qiao</name>
    </author>
    <author>
      <name>Huazhong Yang</name>
    </author>
    <author>
      <name>Hui Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0910.3736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.3736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.0838v1</id>
    <updated>2010-08-04T18:35:25Z</updated>
    <published>2010-08-04T18:35:25Z</published>
    <title>Associative control processor with a rigid structure</title>
    <summary>  The approach of applying associative processor for decision making problem
was proposed. It focuses on hardware implementations of fuzzy processing
systems, associativity as effective management basis of fuzzy processor. The
structural approach is being developed resulting in a quite simple and compact
parallel associative memory unit (PAMU). The memory cost and speed comparison
of processors with rigid and soft-variable structure is given. Also the example
PAMU flashing is considered.
</summary>
    <author>
      <name>Isa Magomedov</name>
    </author>
    <author>
      <name>Omar Khazamov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">DSTU Journal,2009,p. 445-453</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1008.0838v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.0838v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.3694v1</id>
    <updated>2010-08-22T11:13:34Z</updated>
    <published>2010-08-22T11:13:34Z</published>
    <title>Sorting Network for Reversible Logic Synthesis</title>
    <summary>  In this paper, we have introduced an algorithm to implement a sorting network
for reversible logic synthesis based on swapping bit strings. The algorithm
first constructs a network in terms of n*n Toffoli gates read from left to
right. The number of gates in the circuit produced by our algorithm is then
reduced by template matching and removing useless gates from the network. We
have also compared the efficiency of the proposed method with the existing
ones.
</summary>
    <author>
      <name>Md. Saiful Islam</name>
    </author>
    <author>
      <name>Md. Rafiqul Islam</name>
    </author>
    <author>
      <name>Abdullah Al Mahmud</name>
    </author>
    <author>
      <name>Muhammad Rezaul karim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 8 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference for Upcoming Engineers, Windsor
  University, Ontario, Canada, May 20-21, 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1008.3694v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.3694v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.4668v1</id>
    <updated>2010-08-27T09:05:49Z</updated>
    <published>2010-08-27T09:05:49Z</published>
    <title>BSSSN: Bit String Swapping Sorting Network for Reversible Logic
  Synthesis</title>
    <summary>  In this paper, we have introduced the notion of UselessGate and
ReverseOperation. We have also given an algorithm to implement a sorting
network for reversible logic synthesis based on swapping bit strings. The
network is constructed in terms of n*n Toffoli Gates read from left to right
and it has shown that there will be no more gates than the number of swappings
the algorithm requires. The gate complexity of the network is O(n2). The number
of gates in the network can be further reduced by template reduction technique
and removing UselessGate from the network.
</summary>
    <author>
      <name>Md. Saiful Islam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 11 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computer Science (IBAIS University), Vol. 1, No. 1, pp.
  94-99, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1008.4668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.4668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.2622v1</id>
    <updated>2010-09-14T10:48:56Z</updated>
    <published>2010-09-14T10:48:56Z</published>
    <title>On the Design and Analysis of Quaternary Serial and Parallel Adders</title>
    <summary>  Optimization techniques for decreasing the time and area of adder circuits
have been extensively studied for years mostly in binary logic system. In this
paper, we provide the necessary equations required to design a full adder in
quaternary logic system. We develop the equations for single-stage parallel
adder which works as a carry look-ahead adder. We also provide the design of a
logarithmic stage parallel adder which can compute the carries within log2(n)
time delay for n qudits. At last, we compare the designs and finally propose a
hybrid adder which combines the advantages of serial and parallel adder.
</summary>
    <author>
      <name>Anindya Das</name>
    </author>
    <author>
      <name>Ifat Jahangir</name>
    </author>
    <author>
      <name>Masud Hasan</name>
    </author>
    <link href="http://arxiv.org/abs/1009.2622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.2622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.4059v3</id>
    <updated>2021-08-18T22:41:16Z</updated>
    <published>2010-10-19T21:58:14Z</published>
    <title>Multiplierless Modules for Forward and Backward Integer Wavelet
  Transform</title>
    <summary>  This article is about the architecture of a lossless wavelet filter bank with
reprogrammable logic. It is based on second generation of wavelets with a
reduced of number of operations. A new basic structure for parallel
architecture and modules to forward and backward integer discrete wavelet
transform is proposed.
</summary>
    <author>
      <name>Vasil Kolev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/973620.973667</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/973620.973667" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, The ACM proceedings of CompSysTech 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/1010.4059v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.4059v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68-xx, 42Cxx, 65Txx, 54Hxx, 94Axx, 94-XX, 47AXX," scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.1; B.2; B.4; B.7; C.1; C.5; E.3; E.4; G.1; I.4; I.5; I.6; J.3;&#10;  J.6; J.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.0298v1</id>
    <updated>2011-04-02T07:14:38Z</updated>
    <published>2011-04-02T07:14:38Z</published>
    <title>High Speed Multiple Valued Logic Full Adder Using Carbon Nano Tube Field
  Effect Transistor</title>
    <summary>  High speed Full-Adder (FA) module is a critical element in designing high
performance arithmetic circuits. In this paper, we propose a new high speed
multiple-valued logic FA module. The proposed FA is constructed by 14
transistors and 3 capacitors, using carbon nano-tube field effect transistor
(CNFET) technology. Furthermore, our proposed technique has been examined in
different voltages (i.e., 0.65v and 0.9v). The observed results reveal power
consumption and power delay product (PDP) improvements compared to existing FA
counterparts
</summary>
    <author>
      <name>Ashkan Khatir</name>
    </author>
    <author>
      <name>Shaghayegh Abdolahzadegan</name>
    </author>
    <author>
      <name>Iman Mahmoudi</name>
    </author>
    <link href="http://arxiv.org/abs/1104.0298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.0298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.3677v1</id>
    <updated>2011-06-18T18:44:38Z</updated>
    <published>2011-06-18T18:44:38Z</published>
    <title>Pseudo-Ring Testing Schemes and Algorithms of RAM Built-In and Embedded
  Self-Testing</title>
    <summary>  Scan and ring schemes of the pseudo-ring memory selftesting are investigated.
Both schemes are based on emulation of the linear or nonlinear feedback shift
register by memory itself. Peculiarities of the pseudo-ring schemes
implementation for multi-port and embedded memories, and for register file are
described. It is shown that only small additional logic is required and allows
microcontrollers at-speed testing. Also, in this article,are given the a
posteriori values of some type of memories faults coverage when pseudo-ring
testing schemes are applied.
</summary>
    <author>
      <name>Diana Bodean</name>
    </author>
    <author>
      <name>Ghenadie Bodean</name>
    </author>
    <author>
      <name>Wajeb Gharibi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">14 th IEEE Symposium on Design and Diagnostics of Electronic
  Circuits and Systems, April 13-15, 2011, Cottbus, Germany</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1106.3677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.3677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.3924v1</id>
    <updated>2011-07-20T09:20:30Z</updated>
    <published>2011-07-20T09:20:30Z</published>
    <title>Reversible arithmetic logic unit</title>
    <summary>  Quantum computer requires quantum arithmetic. The sophisticated design of a
reversible arithmetic logic unit (reversible ALU) for quantum arithmetic has
been investigated in this letter. We provide explicit construction of
reversible ALU effecting basic arithmetic operations. By provided the
corresponding control unit, the proposed reversible ALU can combine the
classical arithmetic and logic operation in a reversible integrated system.
This letter provides actual evidence to prove the possibility of the
realization of reversible Programmable Logic Device (RPLD) using reversible
ALU.
</summary>
    <author>
      <name>Rigui zhou</name>
    </author>
    <author>
      <name>Yang shi</name>
    </author>
    <author>
      <name>Manqun Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.3924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.3924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.1549v1</id>
    <updated>2011-10-07T14:40:51Z</updated>
    <published>2011-10-07T14:40:51Z</published>
    <title>Power comparison of CMOS and adiabatic full adder circuit</title>
    <summary>  Full adders are important components in applications such as digital signal
processors (DSP) architectures and microprocessors. Apart from the basic
addition adders also used in performing useful operations such as subtraction,
multiplication, division, address calculation, etc. In most of these systems
the adder lies in the critical path that determines the overall performance of
the system. In this paper conventional complementary metal oxide semiconductor
(CMOS) and adiabatic adder circuits are analyzed in terms of power and
transistor count using 0.18UM technology.
</summary>
    <author>
      <name>Sunil Gavaskar Reddy</name>
    </author>
    <author>
      <name>Rajendra prasad</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/vlsic.2011.2306</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/vlsic.2011.2306" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of VLSI design &amp; Communication Systems
  (VLSICS) Vol.2, No.3, September 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1110.1549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.1549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.3535v1</id>
    <updated>2011-10-16T22:48:56Z</updated>
    <published>2011-10-16T22:48:56Z</published>
    <title>Multi-core processors - An overview</title>
    <summary>  Microprocessors have revolutionized the world we live in and continuous
efforts are being made to manufacture not only faster chips but also smarter
ones. A number of techniques such as data level parallelism, instruction level
parallelism and hyper threading (Intel's HT) already exists which have
dramatically improved the performance of microprocessor cores. This paper
briefs on evolution of multi-core processors followed by introducing the
technology and its advantages in today's world. The paper concludes by
detailing on the challenges currently faced by multi-core processors and how
the industry is trying to address these issues.
</summary>
    <author>
      <name>Balaji Venu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, Best Literature review</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.3535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.3535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.0954v1</id>
    <updated>2012-01-04T18:21:06Z</updated>
    <published>2012-01-04T18:21:06Z</published>
    <title>Information Analysis Infrastructure for Diagnosis</title>
    <summary>  A high-speed multiprocessor architecture for brain-like analyzing information
represented in analytic, graph- and table forms of associative relations to
search, recognize and make a decision in n-dimensional vector discrete space is
offered. Vector-logical process models of actual applications, where the
quality of solution is estimated by the proposed integral non-arithmetical
metric of the interaction between binary vectors, are described. The
theoretical proof of the metric for a vector logical space and the quality
criteria for estimating solutions is created.
</summary>
    <author>
      <name>Vladimir Hahanov</name>
    </author>
    <author>
      <name>Wajeb Gharibi</name>
    </author>
    <author>
      <name>Eugenia Litvinova</name>
    </author>
    <author>
      <name>Svetlana Chumachenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages; INFORMATION: An International Interdisciplinary Journal,
  Vol. 14, No. 7, July 2011. Tokyo. arXiv admin note: substantial text overlap
  with arXiv:1105.1973</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.0954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.0954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.2542v1</id>
    <updated>2012-01-12T12:23:33Z</updated>
    <published>2012-01-12T12:23:33Z</published>
    <title>An efficient FPGA implementation of MRI image filtering and tumor
  characterization using Xilinx system generator</title>
    <summary>  This paper presents an efficient architecture for various image filtering
algorithms and tumor characterization using Xilinx System Generator (XSG). This
architecture offers an alternative through a graphical user interface that
combines MATLAB, Simulink and XSG and explores important aspects concerned to
hardware implementation. Performance of this architecture implemented in
SPARTAN-3E Starter kit (XC3S500E-FG320) exceeds those of similar or greater
resources architectures. The proposed architecture reduces the resources
available on target device by 50%.
</summary>
    <author>
      <name>S. Allin Christe</name>
    </author>
    <author>
      <name>M. Vignesh</name>
    </author>
    <author>
      <name>A. Kandaswamy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages,14 figures,2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.2542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.2542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.0787v1</id>
    <updated>2012-03-04T23:43:07Z</updated>
    <published>2012-03-04T23:43:07Z</published>
    <title>A handy systematic method for data hazards detection in an instruction
  set of a pipelined microprocessor</title>
    <summary>  It is intended in this document to introduce a handy systematic method for
enumerating all possible data dependency cases that could occur between any two
instructions that might happen to be processed at the same time at different
stages of the pipeline. Given instructions of the instruction set, specific
information about operands of each instruction and when an instruction reads or
writes data, the method could be used to enumerate all possible data hazard
cases and to determine whether forwarding or stalling is suitable for resolving
each case.
</summary>
    <author>
      <name>Ahmed M. Mahran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 10 figures, 9 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.0787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.0787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.4464v1</id>
    <updated>2013-02-18T21:24:22Z</updated>
    <published>2013-02-18T21:24:22Z</published>
    <title>Dynamic Power Reduction in a Novel CMOS 5T-SRAM for Low-Power SoC</title>
    <summary>  This paper addresses a novel five-transistor (5T) CMOS SRAM design with high
performance and reliability in 65nm CMOS, and illustrates how it reduces the
dynamic power consumption in comparison with the conventional and low-power 6T
SRAM counterparts. This design can be used as cache memory in processors and
low-power portable devices. The proposed SRAM cell features ~13% area reduction
compared to a conventional 6T cell, and features a unique bit-line and negative
supply voltage biasing methodology and ground control architecture to enhance
performance, and suppress standby leakage power.
</summary>
    <author>
      <name>Hooman Jarollahi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">EIT</arxiv:affiliation>
    </author>
    <author>
      <name>Richard F. Hobson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, CDES'10 - The 2010 International Conference on Computer
  Design</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.4464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.4464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.6515v2</id>
    <updated>2013-04-09T15:39:24Z</updated>
    <published>2013-02-26T17:48:12Z</published>
    <title>Hybrid Crossbar Architecture for a Memristor Based Memory</title>
    <summary>  This paper describes a new memristor crossbar architecture that is proposed
for use in a high density cache design. This design has less than 10% of the
write energy consumption than a simple memristor crossbar. Also, it has up to 4
times the bit density of an STT-MRAM system and up to 11 times the bit density
of an SRAM architecture. The proposed architecture is analyzed using a detailed
SPICE analysis that accounts for the resistance of the wires in the memristor
structure. Additionally, the memristor model used in this work has been matched
to specific device characterization data to provide accurate results in terms
of energy, area, and timing.
</summary>
    <author>
      <name>Chris Yakopcic</name>
    </author>
    <author>
      <name>Tarek M. Taha</name>
    </author>
    <link href="http://arxiv.org/abs/1302.6515v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.6515v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.5081v1</id>
    <updated>2013-04-18T11:00:50Z</updated>
    <published>2013-04-18T11:00:50Z</published>
    <title>Open Tiled Manycore System-on-Chip</title>
    <summary>  Manycore System-on-Chip include an increasing amount of processing elements
and have become an important research topic for improvements of both hardware
and software. While research can be conducted using system simulators,
prototyping requires a variety of components and is very time consuming. With
the Open Tiled Manycore System-on-Chip (OpTiMSoC) we aim at building such an
environment for use in our and other research projects as prototyping platform.
  This paper describes the project goals and aspects of OpTiMSoC and summarizes
the current status and ideas.
</summary>
    <author>
      <name>Stefan Wallentowitz</name>
    </author>
    <author>
      <name>Philipp Wagner</name>
    </author>
    <author>
      <name>Michael Tempelmeier</name>
    </author>
    <author>
      <name>Thomas Wild</name>
    </author>
    <author>
      <name>Andreas Herkersdorf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.5081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.5081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3853v1</id>
    <updated>2013-07-15T08:44:19Z</updated>
    <published>2013-07-15T08:44:19Z</published>
    <title>Thermal analysis of 3D associative processor</title>
    <summary>  Thermal density and hot spots limit three-dimensional (3D) implementation of
massively-parallel SIMD processors and prohibit stacking DRAM dies above them.
This study proposes replacing SIMD by an Associative Processor (AP). AP
exhibits close to uniform thermal distribution with reduced hot spots.
Additionally, AP may outperform SIMD processor when the data set size is
sufficiently large, while dissipating less power. Comparative performance and
thermal analysis supported by simulation confirm that AP might be preferable
over SIMD for 3D implementation of large scale massively parallel processing
engines combined with 3D DRAM integration.
</summary>
    <author>
      <name>Leonid Yavits</name>
    </author>
    <author>
      <name>Amir Morad</name>
    </author>
    <author>
      <name>Ran Ginosar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1306.3109</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.3853v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3853v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.7154v2</id>
    <updated>2013-12-09T23:38:49Z</updated>
    <published>2013-07-26T20:07:04Z</published>
    <title>Fast Polar Decoders: Algorithm and Implementation</title>
    <summary>  Polar codes provably achieve the symmetric capacity of a memoryless channel
while having an explicit construction. This work aims to increase the
throughput of polar decoder hardware by an order of magnitude relative to the
state of the art successive-cancellation decoder. We present an algorithm,
architecture, and FPGA implementation of a gigabit-per-second polar decoder.
</summary>
    <author>
      <name>Gabi Sarkis</name>
    </author>
    <author>
      <name>Pascal Giard</name>
    </author>
    <author>
      <name>Alexander Vardy</name>
    </author>
    <author>
      <name>Claude Thibeault</name>
    </author>
    <author>
      <name>Warren J. Gross</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JSAC.2014.140514</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JSAC.2014.140514" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the IEEE Journal on Selected Areas in Communications
  (JSAC) on May 15th, 2013. 11 pages, 7 figures, 6 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Journal on Selected Areas in Communications, vol. 32, no. 5,
  May 2014, pp. 946-957</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.7154v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.7154v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.8319v1</id>
    <updated>2013-07-31T13:40:40Z</updated>
    <published>2013-07-31T13:40:40Z</published>
    <title>Allocating the chains of consecutive additions for optimal fixed-point
  data path synthesis</title>
    <summary>  Minimization of computational errors in the fixed-point data path is often
difficult task. Many signal processing algorithms use chains of consecutive
additions. The analyzing technique that can be applied to fixed-point data path
synthesis has been proposed. This technique takes advantage of allocating the
chains of consecutive additions in order to predict growing width of the data
path and minimize the design complexity and computational errors.
</summary>
    <author>
      <name>Ilya Y. Zhbannikov</name>
    </author>
    <author>
      <name>Gregory W. Donohoe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MWSCAS.2012.6292184</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MWSCAS.2012.6292184" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2012 IEEE 55th International Midwest Symposium on Circuits and
  Systems (MWSCAS)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.8319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.8319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.8401v1</id>
    <updated>2013-07-31T17:45:54Z</updated>
    <published>2013-07-31T17:45:54Z</published>
    <title>FpSynt: a fixed-point datapath synthesis tool for embedded systems</title>
    <summary>  Digital mobile systems must function with low power, small size and weight,
and low cost. High-performance desktop microprocessors, with built-in floating
point hardware, are not suitable in these cases. For embedded systems, it can
be advantageous to implement these calculations with fixed point arithmetic
instead. We present an automated fixed-point data path synthesis tool FpSynt
for designing embedded applications in fixed-point domain with sufficient
accuracy for most applications. FpSynt is available under the GNU General
Public License from the following GitHub repository:
http://github.com/izhbannikov/FPSYNT
</summary>
    <author>
      <name>Ilya Y. Zhbannikov</name>
    </author>
    <author>
      <name>Gregory W. Donohoe</name>
    </author>
    <link href="http://arxiv.org/abs/1307.8401v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.8401v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.0840v1</id>
    <updated>2013-08-04T19:06:44Z</updated>
    <published>2013-08-04T19:06:44Z</published>
    <title>Designing Parity Preserving Reversible Circuits</title>
    <summary>  Making a reversible circuit fault-tolerant is much more difficult than
classical circuit and there have been only a few works in the area of
parity-preserving reversible logic design. Moreover, all of these designs are
ad hoc, based on some pre-defined parity preserving reversible gates as
building blocks. In this paper, we for the first time propose a novel and
systematic approach towards parity preserving reversible circuits design. We
provide some related theoretical results and give two algorithms, one from
reversible specification to parity preserving reversible specification and
another from irreversible specification to parity preserving reversible
specification. We also evaluate the effectiveness of our approach by extensive
experimental results.
</summary>
    <author>
      <name>Goutam Paul</name>
    </author>
    <author>
      <name>Anupam Chattopadhyay</name>
    </author>
    <author>
      <name>Chander Chandak</name>
    </author>
    <link href="http://arxiv.org/abs/1308.0840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.0840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94C10, 81P68" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.3685v1</id>
    <updated>2013-09-14T16:43:05Z</updated>
    <published>2013-09-14T16:43:05Z</published>
    <title>On the Performance Potential of Speculative Execution based on Branch
  and Value Prediction</title>
    <summary>  Fluid Stochastic Petri Nets are used to capture the dynamic behavior of an
ILP processor, and discrete-event simulation is applied to assess the
performance potential of predictions and speculative execution in boosting the
performance of ILP processors that fetch, issue, execute and commit a large
number of instructions per cycle.
</summary>
    <author>
      <name>Pece Mitrevski</name>
    </author>
    <author>
      <name>Marjan Gusev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2298/FUEE0301083M</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2298/FUEE0301083M" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Scientific Journal Facta Universitatis, Series:
  Electronics and Energetics, Vol. 16, No. 1, ISSN: 0353-3670, pp. 83-91, 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1309.3685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.3685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.5507v1</id>
    <updated>2013-09-21T17:50:09Z</updated>
    <published>2013-09-21T17:50:09Z</published>
    <title>Microgrid - The microthreaded many-core architecture</title>
    <summary>  Traditional processors use the von Neumann execution model, some other
processors in the past have used the dataflow execution model. A combination of
von Neuman model and dataflow model is also tried in the past and the resultant
model is referred as hybrid dataflow execution model. We describe a hybrid
dataflow model known as the microthreading. It provides constructs for
creation, synchronization and communication between threads in an intermediate
language. The microthreading model is an abstract programming and machine model
for many-core architecture. A particular instance of this model is named as the
microthreaded architecture or the Microgrid. This architecture implements all
the concurrency constructs of the microthreading model in the hardware with the
management of these constructs in the hardware.
</summary>
    <author>
      <name>Irfan Uddin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.5507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.5507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.7818v1</id>
    <updated>2013-09-30T12:20:47Z</updated>
    <published>2013-09-30T12:20:47Z</published>
    <title>Partial Sums Generation Architecture for Successive Cancellation
  Decoding of Polar Codes</title>
    <summary>  Polar codes are a new family of error correction codes for which efficient
hardware architectures have to be defined for the encoder and the decoder.
Polar codes are decoded using the successive cancellation decoding algorithm
that includes partial sums computations. We take advantage of the recursive
structure of polar codes to introduce an efficient partial sums computation
unit that can also implements the encoder. The proposed architecture is
synthesized for several codelengths in 65nm ASIC technology. The area of the
resulting design is reduced up to 26% and the maximum working frequency is
improved by ~25%.
</summary>
    <author>
      <name>Guillaume Berhault</name>
    </author>
    <author>
      <name>Camille Leroux</name>
    </author>
    <author>
      <name>Christophe Jego</name>
    </author>
    <author>
      <name>Dominique Dallet</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SiPS.2013.6674541</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SiPS.2013.6674541" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Workshop on Signal Processing Systems (SiPS)(26
  April 2012). Accepted (28 June 2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.7818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.7818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.0100v1</id>
    <updated>2013-10-01T00:06:15Z</updated>
    <published>2013-10-01T00:06:15Z</published>
    <title>Technical report: Functional Constraint Extraction From Register
  Transfer Level for ATPG</title>
    <summary>  We proposed in "Functional Constraint Extraction From Register Transfer Level
for ATPG" that is currently submitted to TVLSI, an automatic functional
constraint extractor that can be applied on the RT level. These functional
constraints are used to generate pseudo functional test patterns with ATPG
tools. The patterns are then used to improve the verification process. This
technical report complements the work proposed as it contains the
implementation details of the proposed methodology and shows the detailed
intermediate and final results of the application of this methodology on a
concrete example.
</summary>
    <author>
      <name>Christelle Hobeika</name>
    </author>
    <author>
      <name>Claude Thibeault</name>
    </author>
    <author>
      <name>Jean-François Boland</name>
    </author>
    <link href="http://arxiv.org/abs/1310.0100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.0100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.7792v1</id>
    <updated>2013-10-29T13:03:38Z</updated>
    <published>2013-10-29T13:03:38Z</published>
    <title>Evaluating Cache Coherent Shared Virtual Memory for Heterogeneous
  Multicore Chips</title>
    <summary>  The trend in industry is towards heterogeneous multicore processors (HMCs),
including chips with CPUs and massively-threaded throughput-oriented processors
(MTTOPs) such as GPUs. Although current homogeneous chips tightly couple the
cores with cache-coherent shared virtual memory (CCSVM), this is not the
communication paradigm used by any current HMC. In this paper, we present a
CCSVM design for a CPU/MTTOP chip, as well as an extension of the pthreads
programming model, called xthreads, for programming this HMC. Our goal is to
evaluate the potential performance benefits of tightly coupling heterogeneous
cores with CCSVM.
</summary>
    <author>
      <name>Blake A. Hechtman</name>
    </author>
    <author>
      <name>Daniel J. Sorin</name>
    </author>
    <link href="http://arxiv.org/abs/1310.7792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.7792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.0170v1</id>
    <updated>2013-11-01T12:52:59Z</updated>
    <published>2013-11-01T12:52:59Z</published>
    <title>A Technique for Efficiently Managing SRAM-NVM Hybrid Cache</title>
    <summary>  In this paper, we present a SRAM-PCM hybrid cache design, along with a cache
replacement policy, named dead fast block (DFB) to manage the hybrid cache.
This design aims to leverage the best features of both SRAM and PCM devices.
Compared to a PCM-only cache, the hybrid cache with DFB policy provides
superior results on all relevant evaluation metrics, viz. cache lifetime,
performance and energy efficiency. Also, use of DFB policy for managing the
hybrid cache provides better results compared to LRU replacement policy on all
the evaluation metrics.
</summary>
    <author>
      <name>Sparsh Mittal</name>
    </author>
    <link href="http://arxiv.org/abs/1311.0170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.0170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.1667v1</id>
    <updated>2013-11-07T13:15:11Z</updated>
    <published>2013-11-07T13:15:11Z</published>
    <title>3D Cache Hierarchy Optimization</title>
    <summary>  3D integration has the potential to improve the scalability and performance
of Chip Multiprocessors (CMP). A closed form analytical solution for optimizing
3D CMP cache hierarchy is developed. It allows optimal partitioning of the
cache hierarchy levels into 3D silicon layers and optimal allocation of area
among cache hierarchy levels under constrained area and power budgets. The
optimization framework is extended by incorporating the impact of multithreaded
data sharing on the private cache miss rate. An analytical model for cache
access time as a function of cache size and a number of 3D partitions is
proposed and verified using CACTI simulation.
</summary>
    <author>
      <name>Leonid Yavits</name>
    </author>
    <author>
      <name>Amir Morad</name>
    </author>
    <author>
      <name>Ran Ginosar</name>
    </author>
    <link href="http://arxiv.org/abs/1311.1667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.1667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.2732v1</id>
    <updated>2014-01-13T07:19:52Z</updated>
    <published>2014-01-13T07:19:52Z</published>
    <title>Fault Detection for RC4 Algorithm and its Implementation on FPGA
  Platform</title>
    <summary>  In hardware implementation of a cryptographic algorithm, one may achieve
leakage of secret information by creating scopes to introduce controlled faulty
bit(s) even though the algorithm is mathematically a secured one. The technique
is very effective in respect of crypto processors embedded in smart cards. In
this paper few fault detecting architectures for RC4 algorithm are designed and
implemented on Virtex5(ML505, LX110t) FPGA board. The results indicate that the
proposed architectures can handle most of the faults without loss of throughput
consuming marginally additional hardware and power.
</summary>
    <author>
      <name>Rourab Paul</name>
    </author>
    <author>
      <name>Amlan Chakrabarti</name>
    </author>
    <author>
      <name>Ranjan Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published Book Title: Elsevier Science and Technology, ICCN 2013,
  Bangalore, Page(s): 224 - 232, Volume 3, DOI-03.elsevierst.2013.3.ICCN25,
  ISBN :9789351071044</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.2732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.2732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.4891v1</id>
    <updated>2014-01-20T13:20:58Z</updated>
    <published>2014-01-20T13:20:58Z</published>
    <title>The Design of a Network-On-Chip Architecture Based On An Avionic
  Protocol</title>
    <summary>  When the Network-On-Chip (NoC) paradigm was introduced, many researchers have
proposed many novelistic NoC architectures, tools and design strategies. In
this paper we introduce a new approach in the field of designing
Network-On-Chip (NoC). Our inspiration came from an avionic protocol which is
the AFDX protocol. The proposed NoC architecture is a switch centric
architecture, with exclusive shortcuts between hosts and utilizes the
flexibility, the reliability and the performances offered by AFDX.
</summary>
    <author>
      <name>Ahmed Ben Achballah</name>
    </author>
    <author>
      <name>Slim Ben Saoud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages World Symposium on Computer Applications &amp; Research WSCAR'
  2014, 18-20 January, Sousse, Tunisia</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.4891v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.4891v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2425v1</id>
    <updated>2014-02-11T10:26:20Z</updated>
    <published>2014-02-11T10:26:20Z</published>
    <title>Triple Patterning Lithography (TPL) Layout Decomposition using
  End-Cutting</title>
    <summary>  Triple patterning lithography (TPL) is one of the most promising techniques
in the 14nm logic node and beyond. However, traditional LELELE type TPL
technology suffers from native conflict and overlapping problems. Recently
LELEEC process was proposed to overcome the limitations, where the third mask
is used to generate the end-cuts. In this paper we propose the first study for
LELEEC layout decomposition. Conflict graphs and end-cut graphs are constructed
to extract all the geometrical relationships of input layout and end-cut
candidates. Based on these graphs, integer linear programming (ILP) is
formulated to minimize the conflict number and the stitch number.
</summary>
    <author>
      <name>Bei Yu</name>
    </author>
    <author>
      <name>Jhih-Rong Gao</name>
    </author>
    <author>
      <name>David Z. Pan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.2011355</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.2011355" rel="related"/>
    <link href="http://arxiv.org/abs/1402.2425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2435v1</id>
    <updated>2014-02-11T10:37:37Z</updated>
    <published>2014-02-11T10:37:37Z</published>
    <title>E-BLOW: E-Beam Lithography Overlapping aware Stencil Planning for MCC
  System</title>
    <summary>  Electron beam lithography (EBL) is a promising maskless solution for the
technology beyond 14nm logic node. To overcome its throughput limitation,
recently the traditional EBL system is extended into MCC system. %to further
improve the throughput. In this paper, we present E-BLOW, a tool to solve the
overlapping aware stencil planning (OSP) problems in MCC system. E-BLOW is
integrated with several novel speedup techniques, i.e., successive relaxation,
dynamic programming and KD-Tree based clustering, to achieve a good performance
in terms of runtime and solution quality. Experimental results show that,
compared with previous works, E-BLOW demonstrates better performance for both
conventional EBL system and MCC system.
</summary>
    <author>
      <name>Bei Yu</name>
    </author>
    <author>
      <name>Kun Yuan</name>
    </author>
    <author>
      <name>Jhih-Rong Gao</name>
    </author>
    <author>
      <name>David Z. Pan</name>
    </author>
    <link href="http://arxiv.org/abs/1402.2435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2442v1</id>
    <updated>2014-02-11T11:02:42Z</updated>
    <published>2014-02-11T11:02:42Z</published>
    <title>Self-Aligned Double Patterning Friendly Configuration for Standard Cell
  Library Considering Placement</title>
    <summary>  Self-aligned double patterning (SADP) has become a promising technique to
push pattern resolution limit to sub-22nm technology node. Although SADP
provides good overlay controllability, it encounters many challenges in
physical design stages to obtain conflict-free layout decomposition. In this
paper, we study the impact on placement by different standard cell layout
decomposition strategies. We propose a SADP friendly standard cell
configuration which provides pre-coloring results for standard cells. These
configurations are brought into the placement stage to help ensure layout
decomposability and save the extra effort for solving conflicts in later
stages.
</summary>
    <author>
      <name>Jhih-Rong Gao</name>
    </author>
    <author>
      <name>Bei Yu</name>
    </author>
    <author>
      <name>Ru Huang</name>
    </author>
    <author>
      <name>David Z. Pan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.2011660</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.2011660" rel="related"/>
    <link href="http://arxiv.org/abs/1402.2442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2462v1</id>
    <updated>2014-02-11T11:57:04Z</updated>
    <published>2014-02-11T11:57:04Z</published>
    <title>Floorplanning and Topology Generation for Application-Specific
  Network-on-Chip</title>
    <summary>  Network-on-chip (NoC) architectures have been proposed as a promising
alternative to classical bus-based communication architectures. In this paper,
we propose a two phases framework to solve application-specific NoCs topology
generation problem. At floorplanning phase, we carry out partition driven
floorplanning. At post-floorplanning phase, a heuristic method and a min-cost
max-flow algorithm is used to insert switches and network interfaces. Finally,
we allocate paths to minimize power consumption. The experimental results show
our algorithm is effective for power saving.
</summary>
    <author>
      <name>Bei Yu</name>
    </author>
    <author>
      <name>Sheqin Dong</name>
    </author>
    <author>
      <name>Song Chen</name>
    </author>
    <author>
      <name>Satoshi Goto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ASPDAC.2010.5419825</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ASPDAC.2010.5419825" rel="related"/>
    <link href="http://arxiv.org/abs/1402.2462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2894v1</id>
    <updated>2014-02-12T16:55:10Z</updated>
    <published>2014-02-12T16:55:10Z</published>
    <title>Multi-Voltage and Level-Shifter Assignment Driven Floorplanning</title>
    <summary>  As technology scales, low power design has become a significant requirement
for SOC designers. Among the existing techniques, Multiple-Supply Voltage (MSV)
is a popular and effective method to reduce both dynamic and static power.
Besides, level shifters consume area and delay, and should be considered during
floorplanning. In this paper, we present a new floorplanning system, called
MVLSAF, to solve multi-voltage and level shifter assignment problem. We use a
convex cost network flow algorithm to assign arbitrary number of legal working
voltages and a minimum cost flow algorithm to handle level-shifter assignment.
The experimental results show MVLSAF is effective.
</summary>
    <author>
      <name>Bei Yu</name>
    </author>
    <author>
      <name>Sheqin Dong</name>
    </author>
    <author>
      <name>Statoshi Goto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ASICON.2009.5351219</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ASICON.2009.5351219" rel="related"/>
    <link href="http://arxiv.org/abs/1402.2894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2906v1</id>
    <updated>2014-02-12T17:34:01Z</updated>
    <published>2014-02-12T17:34:01Z</published>
    <title>TRIAD: a triple patterning lithography aware detailed router</title>
    <summary>  TPL-friendly detailed routers require a systematic approach to detect TPL
conflicts. However, the complexity of conflict graph (CG) impedes directly
detecting TPL conflicts in CG. This work proposes a token graph-embedded
conflict graph (TECG) to facilitate the TPL conflict detection while
maintaining high coloring-flexibility. We then develop a TPL aware detailed
router (TRIAD) by applying TECG to a gridless router with the TPL stitch
generation. Compared to a greedy coloring approach, experimental results
indicate that TRIAD generates no conflicts and few stitches with shorter
wirelength at the cost of 2.41x of runtime.
</summary>
    <author>
      <name>Yen-Hung Lin</name>
    </author>
    <author>
      <name>Bei Yu</name>
    </author>
    <author>
      <name>David Z. Pan</name>
    </author>
    <author>
      <name>Yih-Lang Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2429384.2429408</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2429384.2429408" rel="related"/>
    <link href="http://arxiv.org/abs/1402.2906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.3150v1</id>
    <updated>2014-02-13T14:32:56Z</updated>
    <published>2014-02-13T14:32:56Z</published>
    <title>Lithography Hotspot Detection and Mitigation in Nanometer VLSI</title>
    <summary>  With continued feature size scaling, even state of the art semiconductor
manufacturing processes will often run into layouts with poor printability and
yield. Identifying lithography hotspots is important at both physical
verification and early physical design stages. While detailed lithography
simulations can be very accurate, they may be too computationally expensive for
full-chip scale and physical design inner loops. Meanwhile, pattern matching
and machine learning based hotspot detection methods can provide acceptable
quality and yet fast turn-around-time for full-chip scale physical verification
and design. In this paper, we discuss some key issues and recent results on
lithography hotspot detection and mitigation in nanometer VLSI.
</summary>
    <author>
      <name>Jhih-Rong Gao</name>
    </author>
    <author>
      <name>Bei Yu</name>
    </author>
    <author>
      <name>David Z. Pan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ASICON 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.3150v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.3150v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.2785v1</id>
    <updated>2014-03-12T01:26:16Z</updated>
    <published>2014-03-12T01:26:16Z</published>
    <title>State Dependent Statistical Timing Model for Voltage Scaled Circuits</title>
    <summary>  This paper presents a novel statistical state-dependent timing model for
voltage over scaled (VoS) logic circuits that accurately and rapidly finds the
timing distribution of output bits. Using this model erroneous VoS circuits can
be represented as error-free circuits combined with an error-injector. A case
study of a two point DFT unit employing the proposed model is presented and
compared to HSPICE circuit simulation. Results show an accurate match, with
significant speedup gains.
</summary>
    <author>
      <name>Aras Pirbadian</name>
    </author>
    <author>
      <name>Muhammad S. Khairy</name>
    </author>
    <author>
      <name>Ahmed M. Eltawil</name>
    </author>
    <author>
      <name>Fadi J. Kurdahi</name>
    </author>
    <link href="http://arxiv.org/abs/1403.2785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.2785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5885v1</id>
    <updated>2014-04-22T12:38:22Z</updated>
    <published>2014-04-22T12:38:22Z</published>
    <title>Hardware Efficient WiMAX Deinterleaver Capable of Address Generation for
  Random Interleaving Depths</title>
    <summary>  The variation in the prescribed modulation schemes and code rates for WiMAX
interleaver design, as defined by IEEE 802.16 standard, demands a plethora of
hardware if all the modulation schemes and code rates have to be unified into a
single electronic device. Add to this the complexities involved with the
algorithms and permutations of the WiMAX standard, invariably dependent on
floor function which is extremely hardware inefficient. This paper is an
attempt towards removing the complexities and excess hardware involvement in
the implementation of the permutations involved in Deinterleaver designs as
defined by IEEE 802.16
</summary>
    <author>
      <name>Omar Rafique</name>
    </author>
    <author>
      <name>Gangadharaiah S. L</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22315381/IJETT-V10P235</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22315381/IJETT-V10P235" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper contains seven figures and four tables spreab over four
  pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJETT, V10(4),187-190 April 2014.ISSN:2231-5381</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.5885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5929v1</id>
    <updated>2014-04-23T19:09:19Z</updated>
    <published>2014-04-23T19:09:19Z</published>
    <title>FPGA design of a cdma2000 turbo decoder</title>
    <summary>  This paper presents the FPGA hardware design of a turbo decoder for the
cdma2000 standard. The work includes a study and mathematical analysis of the
turbo decoding process, based on the MAX-Log-MAP algorithm. Results of decoding
for a packet size of two hundred fifty bits are presented, as well as an
analysis of area versus performance, and the key variables for hardware design
in turbo decoding.
</summary>
    <author>
      <name>Maribell Sacanamboy Franco</name>
    </author>
    <author>
      <name>Fabio G. Guerrero</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Spanish, 14 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.5929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.0413v1</id>
    <updated>2014-05-02T14:29:02Z</updated>
    <published>2014-05-02T14:29:02Z</published>
    <title>Multiplierless Approximate 4-point DCT VLSI Architectures for Transform
  Block Coding</title>
    <summary>  Two multiplierless algorithms are proposed for 4x4 approximate-DCT for
transform coding in digital video. Computational architectures for 1-D/2-D
realisations are implemented using Xilinx FPGA devices. CMOS synthesis at the
45 nm node indicate real-time operation at 1 GHz yielding 4x4 block rates of
125 MHz at less than 120 mW of dynamic power consumption.
</summary>
    <author>
      <name>F. M. Bayer</name>
    </author>
    <author>
      <name>R. J. Cintra</name>
    </author>
    <author>
      <name>A. Madanayake</name>
    </author>
    <author>
      <name>U. S. Potluri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1049/el.2013.1352</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1049/el.2013.1352" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, corrected Figure 1 (published paper in EL is
  incorrect)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronics Letters, vol. 49, no. 24, pp. 1532-1534, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.0413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.0413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.1565v1</id>
    <updated>2014-06-06T01:48:45Z</updated>
    <published>2014-06-06T01:48:45Z</published>
    <title>Modeling Algorithms in SystemC and ACL2</title>
    <summary>  We describe the formal language MASC, based on a subset of SystemC and
intended for modeling algorithms to be implemented in hardware. By means of a
special-purpose parser, an algorithm coded in SystemC is converted to a MASC
model for the purpose of documentation, which in turn is translated to ACL2 for
formal verification. The parser also generates a SystemC variant that is
suitable as input to a high-level synthesis tool. As an illustration of this
methodology, we describe a proof of correctness of a simple 32-bit radix-4
multiplier.
</summary>
    <author>
      <name>John W. O'Leary</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Intel Corp.</arxiv:affiliation>
    </author>
    <author>
      <name>David M. Russinoff</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Intel Corp</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.152.12</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.152.12" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings ACL2 2014, arXiv:1406.1238</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 152, 2014, pp. 145-162</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.1565v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.1565v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.4628v1</id>
    <updated>2014-06-18T07:50:50Z</updated>
    <published>2014-06-18T07:50:50Z</published>
    <title>An Efficient Synchronous Static Memory design for Embedded System</title>
    <summary>  Custom memory organization are challenging task in the area of VLSI design.
This study aims to design high speed and low power consumption memory for
embedded system. Synchronous SRAM has been proposed and analyzed using various
simulators. Xilinx simulator simulates the Synchronous SRAM memories which can
perform efficient read/write capability for embedded systems. Xinix tool also
provide the access time that required selecting a word and reading it.
Synchronous Static RAM which has easily read /writes capability and performs
scheduled read /writes operation in efficient manner.
</summary>
    <author>
      <name>Ravi Khatwal</name>
    </author>
    <author>
      <name>Manoj Kumar Jain</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/11187-6411 10.5120/11187-6411</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/11187-6411" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.5120/11187-6411" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Embeddded system, International Journal of Computer
  Applications(2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.4628v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.4628v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.5000v1</id>
    <updated>2014-06-19T10:40:23Z</updated>
    <published>2014-06-19T10:40:23Z</published>
    <title>Application Specific Cache Simulation Analysis for Application Specific
  Instruction set Processor</title>
    <summary>  An Efficient Simulation of application specific instruction-set processors
(ASIP) is a challenging onus in the area of VLSI design. This paper
reconnoiters the possibility of use of ASIP simulators for ASIP Simulation.
This proposed study allow as the simulation of the cache memory design with
various ASIP simulators like Simple scalar and VEX. In this paper we have
implemented the memory configuration according to desire application. These
simulators performs the cache related results such as cache name, sets, cache
associativity, cache block size, cache replacement policy according to specific
application.
</summary>
    <author>
      <name>Ravi Khatwal</name>
    </author>
    <author>
      <name>Manoj Kumar Jain</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/15782-4526</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/15782-4526" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ASIP simulation</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.5000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.5000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0147v1</id>
    <updated>2014-07-01T09:10:26Z</updated>
    <published>2014-07-01T09:10:26Z</published>
    <title>Application Specific Hardware Design Simulation for High Performance
  Embedded System</title>
    <summary>  Application specific simulation is challenging task in various real time high
performance embedded devices. In this study specific application is implemented
with the help of Xilinx. Xilinx provides SDK and XPS tools, XPS tools used for
develop complete hardware platform and SDK provides software platform for
application creation and verification. Xilinx XUP-5 board have been used and
implemented various specific Applications with hardware platform. In this study
the base instruction set with customized instructions, supported with specific
hardware resources are analyzed.
</summary>
    <author>
      <name>Ravi Khatwal</name>
    </author>
    <author>
      <name>Manoj Kumar Jain</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/16834-6599</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/16834-6599" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ijca,june(2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.0147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.5173v1</id>
    <updated>2014-07-19T11:42:48Z</updated>
    <published>2014-07-19T11:42:48Z</published>
    <title>An ECG-SoC with 535nW/channel lossless data compression for wearable
  sensors</title>
    <summary>  This paper presents a low power ECG recording Sys-tem-on-Chip (SoC) with
on-chip low complexity lossless ECG compression for data reduction in
wireless/ambulatory ECG sensor devices. The proposed algorithm uses a linear
slope predictor to estimate the ECG samples, and uses a novel low complexity
dynamic coding-packaging scheme to frame the resulting estimation error into
fixed-length 16-bit format. The proposed technique achieves an average
compression ratio of 2.25x on MIT/BIH ECG database. Implemented in 0.35 {\mu}m
process, the compressor uses 0.565 K gates/channel occupying 0.4 mm2 for
4-channel, and consumes 535 nW/channel at 2.4V for ECG sampled at 512 Hz. Small
size and ultra-low power consumption makes the proposed technique suitable for
wearable ECG sensor application.
</summary>
    <author>
      <name>C. J. Deepu</name>
    </author>
    <link href="http://arxiv.org/abs/1407.5173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.5173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.5401v1</id>
    <updated>2014-08-21T11:57:16Z</updated>
    <published>2014-08-21T11:57:16Z</published>
    <title>A Many-Core Overlay for High-Performance Embedded Computing on FPGAs</title>
    <summary>  In this work, we propose a configurable many-core overlay for
high-performance embedded computing. The size of internal memory, supported
operations and number of ports can be configured independently for each core of
the overlay. The overlay was evaluated with matrix multiplication, LU
decomposition and Fast-Fourier Transform (FFT) on a ZYNQ-7020 FPGA platform.
The results show that using a system-level many-core overlay avoids complex
hardware design and still provides good performance results.
</summary>
    <author>
      <name>Mário Véstias</name>
    </author>
    <author>
      <name>Horácio Neto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at First International Workshop on FPGAs for Software
  Programmers (FSP 2014) (arXiv:1408.4423)</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.5401v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5401v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.2088v1</id>
    <updated>2014-11-08T05:57:42Z</updated>
    <published>2014-11-08T05:57:42Z</published>
    <title>Energy Efficient Full Adder Cell Design With Using Carbon Nanotube Field
  Effect Transistors In 32 Nanometer Technology</title>
    <summary>  Full Adder is one of the critical parts of logical and arithmetic units. So,
presenting a low power full adder cell reduces the power consumption of the
entire circuit. Also, using Nano-scale transistors, because of their unique
characteristics will save energy consumption and decrease the chip area. In
this paper we presented a low power full adder cell by using carbon nanotube
field effect transistors (CNTFETs). Simulation results were carried out using
HSPICE based on the CNTFET model in 32 nanometer technology in Different values
of temperature and VDD.
</summary>
    <author>
      <name>Ali Ghorbani</name>
    </author>
    <author>
      <name>Ghazaleh Ghorbani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/vlsic.2014.5501</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/vlsic.2014.5501" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures, International Journal of VLSI design &amp;
  Communication Systems (VLSICS) Vol.5, No.5, October 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.2088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6043v1</id>
    <updated>2014-12-18T20:07:22Z</updated>
    <published>2014-12-18T20:07:22Z</published>
    <title>A 237 Gbps Unrolled Hardware Polar Decoder</title>
    <summary>  In this letter we present a new architecture for a polar decoder using a
reduced complexity successive cancellation decoding algorithm. This novel
fully-unrolled, deeply-pipelined architecture is capable of achieving a coded
throughput of over 237 Gbps for a (1024,512) polar code implemented using an
FPGA. This decoder is two orders of magnitude faster than state-of-the-art
polar decoders.
</summary>
    <author>
      <name>Pascal Giard</name>
    </author>
    <author>
      <name>Gabi Sarkis</name>
    </author>
    <author>
      <name>Claude Thibeault</name>
    </author>
    <author>
      <name>Warren J. Gross</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1049/el.2014.4432</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1049/el.2014.4432" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronics Lett., vol. 51, issue 10, May 2015, pp. 762-763</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1412.6043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.00579v1</id>
    <updated>2015-01-03T16:55:01Z</updated>
    <published>2015-01-03T16:55:01Z</published>
    <title>A Model Study of an All-Digital, Discrete-Time and Embedded Linear
  Regulator</title>
    <summary>  With an increasing number of power-states, finer- grained power management
and larger dynamic ranges of digital circuits, the integration of compact,
scalable linear-regulators embedded deep within logic blocks has become
important. While analog linear-regulators have traditionally been used in
digital ICs, the need for digitally implementable designs that can be
synthesized and embedded in digital functional units for ultra fine- grained
power management has emerged. This paper presents the circuit design and
control models of an all-digital, discrete-time linear regulator and explores
the parametric design space for transient response time and loop stability.
</summary>
    <author>
      <name>Saad Bin Nasir</name>
    </author>
    <author>
      <name>Arijit Raychowdhury</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.00579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.00579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.01237v1</id>
    <updated>2015-02-04T15:43:55Z</updated>
    <published>2015-02-04T15:43:55Z</published>
    <title>Running Identical Threads in C-Slow Retiming based Designs for
  Functional Failure Detection</title>
    <summary>  This paper shows the usage of C-Slow Retiming (CSR) in safety critical and
low power applications. CSR generates C copies of a design by reusing the given
logic resources in a time sliced fashion. When all C design copies are
stimulated with the same input values, then all C design copies should behave
the same way and will therefore create a redundant system. The paper shows that
this special method of using CSR offers great benefits when used in safety
critical and low power applications. Additional optimization techniques towards
reducing register count are shown and an on-the-fly recovery mechanism is
discussed.
</summary>
    <author>
      <name>Tobias Strauch</name>
    </author>
    <link href="http://arxiv.org/abs/1502.01237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.01237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.07055v1</id>
    <updated>2015-02-25T05:45:08Z</updated>
    <published>2015-02-25T05:45:08Z</published>
    <title>A Novel Architecture of Area Efficient FFT Algorithm for FPGA
  Implementation</title>
    <summary>  Fast Fourier transform (FFT) of large number of samples requires huge
hardware resources of field programmable gate arrays (FPGA), which needs more
area and power. In this paper, we present an area efficient architecture of FFT
processor that reuses the butterfly elements several times. The FFT processor
is simulated using VHDL and the results are validated on a Virtex-6 FPGA. The
proposed architecture outperforms the conventional architecture of a $N$-point
FFT processor in terms of area which is reduced by a factor of $log_N 2$ with
negligible increase in processing time.
</summary>
    <author>
      <name>Atin Mukherjee</name>
    </author>
    <author>
      <name>Amitabha Sinha</name>
    </author>
    <author>
      <name>Debesh Choudhury</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 10 figures; Accepted in ACM SIGARCH Computer Architecture
  News, December 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.07055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.07055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.07241v2</id>
    <updated>2015-02-26T06:00:09Z</updated>
    <published>2015-02-25T16:52:56Z</published>
    <title>Proceedings of the DATE Friday Workshop on Heterogeneous Architectures
  and Design Methods for Embedded Image Systems (HIS 2015)</title>
    <summary>  This volume contains the papers accepted at the DATE Friday Workshop on
Heterogeneous Architectures and Design Methods for Embedded Image Systems (HIS
2015), held in Grenoble, France, March 13, 2015. HIS 2015 was co-located with
the Conference on Design, Automation and Test in Europe (DATE).
</summary>
    <author>
      <name>Frank Hannig</name>
    </author>
    <author>
      <name>Dietmar Fey</name>
    </author>
    <author>
      <name>Anton Lokhmotov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Website of the workshop: https://www12.cs.fau.de/ws/his2015/</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.07241v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.07241v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.03193v2</id>
    <updated>2015-08-31T17:51:47Z</updated>
    <published>2015-06-10T07:14:45Z</published>
    <title>Accelerating Non-volatile/Hybrid Processor Cache Design Space
  Exploration for Application Specific Embedded Systems</title>
    <summary>  In this article, we propose a technique to accelerate nonvolatile or hybrid
of volatile and nonvolatile processor cache design space exploration for
application specific embedded systems. Utilizing a novel cache behavior
modeling equation and a new accurate cache miss prediction mechanism, our
proposed technique can accelerate NVM or hybrid FIFO processor cache design
space exploration for SPEC CPU 2000 applications up to 249 times compared to
the conventional approach.
</summary>
    <author>
      <name>Mohammad Shihabul Haque</name>
    </author>
    <author>
      <name>Ang Li</name>
    </author>
    <author>
      <name>Akash Kumar</name>
    </author>
    <author>
      <name>Qingsong Wei</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ASPDAC.2015.7059045</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ASPDAC.2015.7059045" rel="related"/>
    <link href="http://arxiv.org/abs/1506.03193v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.03193v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06832v1</id>
    <updated>2015-08-27T12:44:22Z</updated>
    <published>2015-08-27T12:44:22Z</published>
    <title>Designing Hardware/Software Systems for Embedded High-Performance
  Computing</title>
    <summary>  In this work, we propose an architecture and methodology to design
hardware/software systems for high-performance embedded computing on FPGA. The
hardware side is based on a many-core architecture whose design is generated
automatically given a set of architectural parameters. Both the architecture
and the methodology were evaluated running dense matrix multiplication and
sparse matrix-vector multiplication on a ZYNQ-7020 FPGA platform. The results
show that using a system-level design of the system avoids complex hardware
design and still provides good performance results.
</summary>
    <author>
      <name>Mário P. Véstias</name>
    </author>
    <author>
      <name>Rui Policarpo Duarte</name>
    </author>
    <author>
      <name>Horácio C. Neto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.06832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03575v1</id>
    <updated>2015-09-11T16:20:07Z</updated>
    <published>2015-09-11T16:20:07Z</published>
    <title>FPGA Implementation of High Speed Baugh-Wooley Multiplier using
  Decomposition Logic</title>
    <summary>  The Baugh-Wooley algorithm is a well-known iterative algorithm for performing
multiplication in digital signal processing applications. Decomposition logic
is used with Baugh-Wooley algorithm to enhance the speed and to reduce the
critical path delay. In this paper a high speed multiplier is designed and
implemented using decomposition logic and Baugh-Wooley algorithm. The result is
compared with booth multiplier. FPGA based architecture is presented and design
has been implemented using Xilinx 12.3 device.
</summary>
    <author>
      <name>Ananda Kiran</name>
    </author>
    <author>
      <name>Navdeep Prashar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, 2 tables, 3 equations</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.03575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01187v1</id>
    <updated>2016-03-03T17:17:56Z</updated>
    <published>2016-03-03T17:17:56Z</published>
    <title>A Dynamic Overlay Supporting Just-In-Time Assembly to Construct
  Customized Hardware Accelerators</title>
    <summary>  Barriers that prevent programmers from using FPGAs include the need to work
within vendor specific CAD tools, knowledge of hardware programming models, and
the requirement to pass each design through synthesis, place and route. In this
work, a dynamic overlay is designed to support Just- In-Time assembly by
composing hardware operators to construct full accelerators. The hardware
operators are pre-synthesized bit- streams and can be downloaded to Partially
Reconfigurable(PR) regions at runtime.
</summary>
    <author>
      <name>Zeyad Aklah</name>
    </author>
    <author>
      <name>Sen Ma</name>
    </author>
    <author>
      <name>David Andrews</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, extended abstract, 2nd International Workshop on Overlay
  Architectures for FPGAs (OLAF),Feburary 21, 2016 - Monterey, CA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.01187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03229v1</id>
    <updated>2016-05-10T22:18:06Z</updated>
    <published>2016-05-10T22:18:06Z</published>
    <title>CORDIC-based Architecture for Powering Computation in Fixed-Point
  Arithmetic</title>
    <summary>  We present a fixed point architecture (source VHDL code is provided) for
powering computation. The fully customized architecture, based on the expanded
hyperbolic CORDIC algorithm, allows for design space exploration to establish
trade-offs among design parameters (numerical format, number of iterations),
execution time, resource usage and accuracy. We also generate Pareto-optimal
realizations in the resource-accuracy space: this approach can produce optimal
hardware realizations that simultaneously satisfy resource and accuracy
requirements.
</summary>
    <author>
      <name>Nia Simmonds</name>
    </author>
    <author>
      <name>Joshua Mack</name>
    </author>
    <author>
      <name>Sam Bellestri</name>
    </author>
    <author>
      <name>Daniel Llamocca</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.03229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08149v1</id>
    <updated>2016-05-26T05:46:03Z</updated>
    <published>2016-05-26T05:46:03Z</published>
    <title>Proceedings of the 2nd International Workshop on Overlay Architectures
  for FPGAs (OLAF 2016)</title>
    <summary>  The 2nd International Workshop on Overlay Architectures for FPGAs (OLAF 2016)
was held on 21 Mar, 2016 as a co-located workshop at the 24th ACM/SIGDA
International Symposium on Field-Programmable Gate Arrays (FPGA 2016). This
year, the program committee selected 6 papers and 3 extended abstracts to be
presented at the workshop, which are subsequently collected in this online
volume.
</summary>
    <author>
      <name>Hayden Kwok-Hay So</name>
    </author>
    <author>
      <name>John Wawrzynek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2nd International Workshop on Overlay Architectures for FPGAs (OLAF
  2016) website: see http://olaf.eecs.berkeley.edu</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.08149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.0; C.1; B.5.2; B.6.3; B.7.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01037v1</id>
    <updated>2016-06-03T10:37:41Z</updated>
    <published>2016-06-03T10:37:41Z</published>
    <title>GRVI Phalanx: A Massively Parallel RISC-V FPGA Accelerator Accelerator</title>
    <summary>  GRVI is an FPGA-efficient RISC-V RV32I soft processor. Phalanx is a parallel
processor and accelerator array framework. Groups of processors and
accelerators form shared memory clusters. Clusters are interconnected with each
other and with extreme bandwidth I/O and memory devices by a 300- bit-wide
Hoplite NOC. An example Kintex UltraScale KU040 system has 400 RISC-V cores,
peak throughput of 100,000 MIPS, peak shared memory bandwidth of 600 GB/s, NOC
bisection bandwidth of 700 Gbps, and uses 13 W.
</summary>
    <author>
      <name>Jan Gray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 2nd International Workshop on Overlay Architectures for
  FPGAs (OLAF 2016) arXiv:1605.08149</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.01037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01980v2</id>
    <updated>2016-06-11T20:53:13Z</updated>
    <published>2016-06-07T00:15:16Z</published>
    <title>Open-source Hardware: Opportunities and Challenges</title>
    <summary>  Innovation in hardware is slowing due to rising costs of chip design and
diminishing benefits from Moore's law and Dennard scaling. Software innovation,
on the other hand, is flourishing, helped in good measure by a thriving
open-source ecosystem. We believe that open source can similarly help hardware
innovation, but has not yet due to several reasons. We identify these reasons
and how the industry, academia, and the hardware community at large can come
together to address them.
</summary>
    <author>
      <name>Gagan Gupta</name>
    </author>
    <author>
      <name>Tony Nowatzki</name>
    </author>
    <author>
      <name>Vinay Gangadhar</name>
    </author>
    <author>
      <name>Karthikeyan Sankaralingam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEEE Computer. IEEE copyright notice, and DOI to be
  added. 7 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.01980v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01980v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05094v1</id>
    <updated>2016-06-16T08:40:57Z</updated>
    <published>2016-06-16T08:40:57Z</published>
    <title>A 0.3-2.6 TOPS/W Precision-Scalable Processor for Real-Time Large-Scale
  ConvNets</title>
    <summary>  A low-power precision-scalable processor for ConvNets or convolutional neural
networks (CNN) is implemented in a 40nm technology. Its 256 parallel processing
units achieve a peak 102GOPS running at 204MHz. To minimize energy consumption
while maintaining throughput, this works is the first to both exploit the
sparsity of convolutions and to implement dynamic precision-scalability
enabling supply- and energy scaling. The processor is fully C-programmable,
consumes 25-288mW at 204 MHz and scales efficiency from 0.3-2.6 real TOPS/W.
This system hereby outperforms the state-of-the-art up to 3.9x in energy
efficiency.
</summary>
    <author>
      <name>Bert Moons</name>
    </author>
    <author>
      <name>Marian Verhelst</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at the Symposium on VLSI Circuits, 2016, Honolulu, HI, US</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07852v1</id>
    <updated>2016-06-24T23:20:50Z</updated>
    <published>2016-06-24T23:20:50Z</published>
    <title>FPMax: a 106GFLOPS/W at 217GFLOPS/mm2 Single-Precision FPU, and a
  43.7GFLOPS/W at 74.6GFLOPS/mm2 Double-Precision FPU, in 28nm UTBB FDSOI</title>
    <summary>  FPMax implements four FPUs optimized for latency or throughput workloads in
two precisions, fabricated in 28nm UTBB FDSOI. Each unit's parameters, e.g
pipeline stages, booth encoding etc., were optimized to yield 1.42ns latency at
110GLOPS/W (SP) and 1.39ns latency at 36GFLOPS/W (DP). At 100% activity,
body-bias control improves the energy efficiency by about 20%; at 10% activity
this saving is almost 2x.
  Keywords: FPU, energy efficiency, hardware generator, SOI
</summary>
    <author>
      <name>Jing Pu</name>
    </author>
    <author>
      <name>Sameh Galal</name>
    </author>
    <author>
      <name>Xuan Yang</name>
    </author>
    <author>
      <name>Ofer Shacham</name>
    </author>
    <author>
      <name>Mark Horowitz</name>
    </author>
    <link href="http://arxiv.org/abs/1606.07852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01585v1</id>
    <updated>2016-09-06T14:54:26Z</updated>
    <published>2016-09-06T14:54:26Z</published>
    <title>A Hardware-Efficient Approach to Computing the Rotation Matrix from a
  Quaternion</title>
    <summary>  In this paper, we have proposed a novel VLSI-oriented approach to computing
the rotation matrix entries from the quaternion coefficients. The advantage of
this approach is the complete elimination of multiplications and replacing them
by less costly squarings. Our approach uses Logan's identity, which proposes to
replace the calculation of the product of two numbers on summing the squares
via the Binomial Theorem. Replacing multiplications by squarings implies
reducing power consumption as well as decreases hardware circuit complexity.
</summary>
    <author>
      <name>Aleksandr Cariow</name>
    </author>
    <author>
      <name>Galina Cariowa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.01585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65Y04, 65Y05, 65Y10, 65Y20, 68M07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.7.1; C.1.2; C.1.4; C.3; F.2.1; G.1.0; I.3.1; I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.00751v1</id>
    <updated>2016-10-03T20:59:17Z</updated>
    <published>2016-10-03T20:59:17Z</published>
    <title>An overview about Networks-on-Chip with multicast suppor</title>
    <summary>  Modern System-on-Chip (SoC) platforms typically consist of multiple
processors and a communication interconnect between them. Network-on-Chip (NoC)
arises as a solution to interconnect these systems, which provides a scalable,
reusable, and an efficient interconnect. For these SoC platforms, multicast
communication is significantly used for parallel applications. Cache coherency
in distributed sharedmemory,clock synchronization, replication, or barrier
synchronization are examples of these requests. This paper presents an overview
of research on NoC with support for multicast communication and delineates the
major issues addressed so far by the scientific community in this investigation
area.
</summary>
    <author>
      <name>Marcelo Daniel Berejuck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">06 pages, 02 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.00751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.00751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01832v1</id>
    <updated>2016-10-06T12:05:14Z</updated>
    <published>2016-10-06T12:05:14Z</published>
    <title>Epiphany-V: A 1024 processor 64-bit RISC System-On-Chip</title>
    <summary>  This paper describes the design of a 1024-core processor chip in 16nm FinFet
technology. The chip ("Epiphany-V") contains an array of 1024 64-bit RISC
processors, 64MB of on-chip SRAM, three 136-bit wide mesh Networks-On-Chip, and
1024 programmable IO pins. The chip has taped out and is being manufactured by
TSMC.
  This research was developed with funding from the Defense Advanced Research
Projects Agency (DARPA). The views, opinions and/or findings expressed are
those of the author and should not be interpreted as representing the official
views or policies of the Department of Defense or the U.S. Government.
</summary>
    <author>
      <name>Andreas Olofsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.01832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.07501v1</id>
    <updated>2016-10-24T17:24:56Z</updated>
    <published>2016-10-24T17:24:56Z</published>
    <title>A 481pJ/decision 3.4M decision/s Multifunctional Deep In-memory
  Inference Processor using Standard 6T SRAM Array</title>
    <summary>  This paper describes a multi-functional deep in-memory processor for
inference applications. Deep in-memory processing is achieved by embedding
pitch-matched low-SNR analog processing into a standard 6T 16KB SRAM array in
65 nm CMOS. Four applications are demonstrated. The prototype achieves up to
5.6X (9.7X estimated for multi-bank scenario) energy savings with negligible
(&lt;1%) accuracy degradation in all four applications as compared to the
conventional architecture.
</summary>
    <author>
      <name>Mingu Kang</name>
    </author>
    <author>
      <name>Sujan Gonugondla</name>
    </author>
    <author>
      <name>Ameya Patil</name>
    </author>
    <author>
      <name>Naresh Shanbhag</name>
    </author>
    <link href="http://arxiv.org/abs/1610.07501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.07501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03380v1</id>
    <updated>2016-11-10T16:21:51Z</updated>
    <published>2016-11-10T16:21:51Z</published>
    <title>In-Storage Embedded Accelerator for Sparse Pattern Processing</title>
    <summary>  We present a novel architecture for sparse pattern processing, using flash
storage with embedded accelerators. Sparse pattern processing on large data
sets is the essence of applications such as document search, natural language
processing, bioinformatics, subgraph matching, machine learning, and graph
processing. One slice of our prototype accelerator is capable of handling up to
1TB of data, and experiments show that it can outperform C/C++ software
solutions on a 16-core system at a fraction of the power and cost; an optimized
version of the accelerator can match the performance of a 48-core server.
</summary>
    <author>
      <name>Sang-Woo Jun</name>
    </author>
    <author>
      <name>Huy T. Nguyen</name>
    </author>
    <author>
      <name>Vijay N. Gadepally</name>
    </author>
    <author>
      <name> Arvind</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HPEC.2016.7761588</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HPEC.2016.7761588" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE HPEC 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.03380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04277v1</id>
    <updated>2016-12-12T01:43:11Z</updated>
    <published>2016-12-12T01:43:11Z</published>
    <title>Copycat: A High Precision Real Time NAND Simulator</title>
    <summary>  In this paper, we describe the design and implementation of a high precision
real time NAND simulator called Copycat that runs on a commodity multi-core
desktop environment. This NAND simulator facilitates the development of
embedded flash memory management software such as the flash translation layer
(FTL). The simulator also allows a comprehensive fault injection for testing
the reliability of the FTL. Compared against a real FPGA implementation, the
simulator's response time deviation is under 0.28% on average, with a maximum
of 10.12%.
</summary>
    <author>
      <name>Juyong Shin</name>
    </author>
    <author>
      <name>Jongbo Bae</name>
    </author>
    <author>
      <name>Ansu Na</name>
    </author>
    <author>
      <name>Sang Lyul Min</name>
    </author>
    <link href="http://arxiv.org/abs/1612.04277v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04277v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04986v1</id>
    <updated>2016-12-15T08:51:02Z</updated>
    <published>2016-12-15T08:51:02Z</published>
    <title>HADES: Microprocessor Hazard Analysis via Formal Verification of
  Parameterized Systems</title>
    <summary>  HADES is a fully automated verification tool for pipeline-based
microprocessors that aims at flaws caused by improperly handled data hazards.
It focuses on single-pipeline microprocessors designed at the register transfer
level (RTL) and deals with read-after-write, write-after-write, and
write-after-read hazards. HADES combines several techniques, including
data-flow analysis, error pattern matching, SMT solving, and abstract regular
model checking. It has been successfully tested on several microprocessors for
embedded applications.
</summary>
    <author>
      <name>Lukáš Charvát</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Faculty of Information Technology, Brno University of Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Aleš Smrčka</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IT4Innovations Centre of Excellence, FIT, Brno University of Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Tomáš Vojnar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IT4Innovations Centre of Excellence, FIT, Brno University of Technology</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.233.9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.233.9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings MEMICS 2016, arXiv:1612.04037</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 233, 2016, pp. 87-93</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1612.04986v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04986v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08802v2</id>
    <updated>2019-03-05T15:06:01Z</updated>
    <published>2017-04-28T03:49:08Z</published>
    <title>Proceedings of the 3rd International Workshop on Overlay Architectures
  for FPGAs (OLAF 2017)</title>
    <summary>  The 3rd International Workshop on Overlay Architectures for FPGAs (OLAF 2017)
was held on 22 Feb, 2017 as a co-located workshop at the 25th ACM/SIGDA
International Symposium on Field-Programmable Gate Arrays (FPGA 2017). This
year, the program committee selected 3 papers and 3 extended abstracts to be
presented at the workshop, which are subsequently collected in this online
volume.
</summary>
    <author>
      <name>Hayden Kwok-Hay So</name>
    </author>
    <author>
      <name>John Wawrzynek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3rd International Workshop on Overlay Architectures for FPGAs (OLAF
  2017) website: see http://olaf.eecs.berkeley.edu</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08802v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08802v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.0; C.1; B.5.2; B.6.3; B.7.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04979v1</id>
    <updated>2017-05-14T15:39:59Z</updated>
    <published>2017-05-14T15:39:59Z</published>
    <title>Fast Statistical Timing Analysis for Circuits with Post-Silicon Tunable
  Clock Buffers</title>
    <summary>  Post-Silicon Tunable (PST) clock buffers are widely used in high performance
designs to counter process variations. By allowing delay compensation between
consecutive register stages, PST buffers can effectively improve the yield of
digital circuits. To date, the evaluation of manufacturing yield in the
presence of PST buffers is only possible using Monte Carlo simulation. In this
paper, we propose an alternative method based on graph transformations, which
is much faster, more than 1000 times, and computes a parametric minimum clock
period. It also identifies the gates which are most critical to the circuit
performance, therefore enabling a fast analysis-optimization flow.
</summary>
    <author>
      <name>Bing Li</name>
    </author>
    <author>
      <name>Ning Chen</name>
    </author>
    <author>
      <name>Ulf Schlichtmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCAD.2011.6105314</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCAD.2011.6105314" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/ACM International Conference on Computer-Aided Design (ICCAD),
  2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04982v1</id>
    <updated>2017-05-14T15:53:07Z</updated>
    <published>2017-05-14T15:53:07Z</published>
    <title>Post-Route Refinement for High-Frequency PCBs Considering Meander
  Segment Alleviation</title>
    <summary>  In this paper, we propose a post-processing framework which iteratively
refines the routing results from an existing PCB router by removing dense
meander segments. By swapping and detouring dense meander segments the proposed
method can effectively alleviate accumulating crosstalk noise, while respecting
pre-defined area constraints. Experimental results show more than 85% reduction
of the meander segments and hence the noise cost.
</summary>
    <author>
      <name> Tsun-Ming</name>
    </author>
    <author>
      <name>Tseng Bing Li</name>
    </author>
    <author>
      <name>Tsung-Yi Ho</name>
    </author>
    <author>
      <name>Ulf Schlichtmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2483028.2483123</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2483028.2483123" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Great Lake Symposium on VLSI (GLSVLSI), 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.06923v1</id>
    <updated>2017-05-19T10:36:49Z</updated>
    <published>2017-05-19T10:36:49Z</published>
    <title>MultiAmdahl: Optimal Resource Allocation in Heterogeneous Architectures</title>
    <summary>  Future multiprocessor chips will integrate many different units, each
tailored to a specific computation. When designing such a system, the chip
architect must decide how to distribute limited system resources such as area,
power, and energy among the computational units. We extend MultiAmdahl, an
analytical optimization technique for resource allocation in heterogeneous
architectures, for energy optimality under a variety of constant system power
scenarios. We conclude that reduction in constant system power should be met by
reallocating resources from general-purpose computing to heterogeneous
accelerator-dominated computing, to keep the overall energy consumption at a
minimum. We extend this conclusion to offer an intuition regarding
energy-optimal resource allocation in data center computing.
</summary>
    <author>
      <name>Leonid Yavits</name>
    </author>
    <author>
      <name>Amir Morad</name>
    </author>
    <author>
      <name>Uri Weiser</name>
    </author>
    <author>
      <name>Ran Ginosar</name>
    </author>
    <link href="http://arxiv.org/abs/1705.06923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.06923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07281v1</id>
    <updated>2017-05-20T08:54:52Z</updated>
    <published>2017-05-20T08:54:52Z</published>
    <title>Cache Hierarchy Optimization</title>
    <summary>  Power consumption, off-chip memory bandwidth, chip area and Network on Chip
(NoC) capacity are among main chip resources limiting the scalability of Chip
Multiprocessors (CMP). A closed form analytical solution for optimizing the CMP
cache hierarchy and optimally allocating area among hierarchy levels under such
constrained resources is developed. The optimization framework is extended by
incorporating the impact of data sharing on cache miss rate. An analytical
model for cache access time as a function of cache size is proposed and
verified using CACTI simulation.
</summary>
    <author>
      <name>Leonid Yavits</name>
    </author>
    <author>
      <name>Amir Morad</name>
    </author>
    <author>
      <name>Ran Ginosar</name>
    </author>
    <link href="http://arxiv.org/abs/1705.07281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03900v1</id>
    <updated>2017-08-13T12:56:29Z</updated>
    <published>2017-08-13T12:56:29Z</published>
    <title>Sensitivity Analysis of Core Specialization Techniques</title>
    <summary>  The instruction footprint of OS-intensive workloads such as web servers,
database servers, and file servers typically exceeds the size of the
instruction cache (32 KB). Consequently, such workloads incur a lot of i-cache
misses, which reduces their performance drastically. Several papers have
proposed to improve the performance of such workloads using core
specialization. In this scheme, tasks with different instruction footprints are
executed on different cores. In this report, we study the performance of five
state of the art core specialization techniques: SelectiveOffload [6], FlexSC
[8], DisAggregateOS [5], SLICC [2], and SchedTask [3] for different system
parameters. Our studies show that for a suite of 8 popular OS-intensive
workloads, SchedTask performs best for all evaluated configurations.
</summary>
    <author>
      <name>Prathmesh Kallurkar</name>
    </author>
    <author>
      <name>Smruti R. Sarangi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.03900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09597v1</id>
    <updated>2017-08-31T07:34:00Z</updated>
    <published>2017-08-31T07:34:00Z</published>
    <title>Advanced Datapath Synthesis using Graph Isomorphism</title>
    <summary>  This paper presents an advanced DAG-based algorithm for datapath synthesis
that targets area minimization using logic-level resource sharing. The problem
of identifying common specification logic is formulated using unweighted graph
isomorphism problem, in contrast to a weighted graph isomorphism using AIGs. In
the context of gate-level datapath circuits, our algorithm solves the un-
weighted graph isomorphism problem in linear time. The experiments are
conducted within an industrial synthesis flow that includes the complete
high-level synthesis, logic synthesis and placement and route procedures.
Experimental results show a significant runtime improvements compared to the
existing datapath synthesis algorithms.
</summary>
    <author>
      <name>Cunxi Yu</name>
    </author>
    <author>
      <name>Mihir Choudhury</name>
    </author>
    <author>
      <name>Andrew Sullivan</name>
    </author>
    <author>
      <name>Maciej Ciesielski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures. To appear in 2017 IEEE/ACM International
  Conference on Computer-Aided Design (ICCAD'17)</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.09597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.08088v1</id>
    <updated>2018-01-20T02:53:45Z</updated>
    <published>2018-01-20T02:53:45Z</published>
    <title>Pointer-Chase Prefetcher for Linked Data Structures</title>
    <summary>  Caches only exploit spatial and temporal locality in a set of address
referenced in a program. Due to dynamic construction of linked data-structures,
they are difficult to cache as the spatial locality between the nodes is highly
dependent on the data layout. Prefetching can play an important role in
improving the performance of linked data-structures. In this project, a pointer
chase mechanism along with compiler hints is adopted to design a prefetcher for
linked data-structures. The design is evaluated against the baseline of
processor with cache in terms of performance, area and energy
</summary>
    <author>
      <name>Nitish Kumar Srivastava</name>
    </author>
    <author>
      <name>Akshay Dilip Navalakha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 37 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.08088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.08088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06617v1</id>
    <updated>2018-03-18T07:18:58Z</updated>
    <published>2018-03-18T07:18:58Z</published>
    <title>Towards an Area-Efficient Implementation of a High ILP EDGE Soft
  Processor</title>
    <summary>  In-order scalar RISC architectures have been the dominant paradigm in FPGA
soft processor design for twenty years. Prior out-of-order superscalar
implementations have not exhibited competitive area or absolute performance.
This paper describes a new way to build fast and area-efficient out-of-order
superscalar soft processors by utilizing an Explicit Data Graph Execution
(EDGE) instruction set architecture. By carefully mapping the EDGE
microarchitecture, and in particular, its dataflow instruction scheduler, we
demonstrate the feasibility of an out-of-order FPGA architecture. Two scheduler
design alternatives are compared.
</summary>
    <author>
      <name>Jan Gray</name>
    </author>
    <author>
      <name>Aaron Smith</name>
    </author>
    <link href="http://arxiv.org/abs/1803.06617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06955v1</id>
    <updated>2018-03-19T14:36:12Z</updated>
    <published>2018-03-19T14:36:12Z</published>
    <title>AISC: Approximate Instruction Set Computer</title>
    <summary>  This paper makes the case for a single-ISA heterogeneous computing platform,
AISC, where each compute engine (be it a core or an accelerator) supports a
different subset of the very same ISA. An ISA subset may not be functionally
complete, but the union of the (per compute engine) subsets renders a
functionally complete, platform-wide single ISA. Tailoring the
microarchitecture of each compute engine to the subset of the ISA that it
supports can easily reduce hardware complexity. At the same time, the energy
efficiency of computing can improve by exploiting algorithmic noise tolerance:
by mapping code sequences that can tolerate (any potential inaccuracy induced
by) the incomplete ISA-subsets to the corresponding compute engines.
</summary>
    <author>
      <name>Alexandra Ferreron</name>
    </author>
    <author>
      <name>Jesus Alastruey-Benede</name>
    </author>
    <author>
      <name>Dario Suarez-Gracia</name>
    </author>
    <author>
      <name>Ulya R. Karpuzcu</name>
    </author>
    <link href="http://arxiv.org/abs/1803.06955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00261v1</id>
    <updated>2018-04-01T06:08:29Z</updated>
    <published>2018-04-01T06:08:29Z</published>
    <title>A Survey of Techniques for Dynamic Branch Prediction</title>
    <summary>  Branch predictor (BP) is an essential component in modern processors since
high BP accuracy can improve performance and reduce energy by decreasing the
number of instructions executed on wrong-path. However, reducing latency and
storage overhead of BP while maintaining high accuracy presents significant
challenges. In this paper, we present a survey of dynamic branch prediction
techniques. We classify the works based on key features to underscore their
differences and similarities. We believe this paper will spark further research
in this area and will be useful for computer architects, processor designers
and researchers.
</summary>
    <author>
      <name>Sparsh Mittal</name>
    </author>
    <link href="http://arxiv.org/abs/1804.00261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05442v1</id>
    <updated>2018-07-14T20:56:12Z</updated>
    <published>2018-07-14T20:56:12Z</published>
    <title>Deriving AOC C-Models from D&amp;V Languages for Single- or Multi-Threaded
  Execution Using C or C++</title>
    <summary>  The C language is getting more and more popular as a design and verification
language (DVL). SystemC, ParC [1] and Cx [2] are based on C. C-models of the
design and verification environment can also be generated from new DVLs (e.g.
Chisel [3]) or classical DVLs such as VHDL or Verilog. The execution of these
models is usually license free and presumably faster than their alternative
counterparts (simulators). This paper proposes activity-dependent, ordered,
cycle-accurate (AOC) C-models to speed up simulation time. It compares the
results with alternative concepts. The paper also examines the execution of the
AOC C-model on a multithreaded processor environment.
</summary>
    <author>
      <name>Tobias Strauch</name>
    </author>
    <link href="http://arxiv.org/abs/1807.05442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05446v1</id>
    <updated>2018-07-14T20:58:49Z</updated>
    <published>2018-07-14T20:58:49Z</published>
    <title>Timing Driven C-Slow Retiming on RTL for MultiCores on FPGAs</title>
    <summary>  In this paper C-Slow Retiming (CSR) on RTL is discussed. CSR multiplies the
functionality of cores by adding the same number of registers into each path.
The technique is ideal for FPGAs with their already existing registers.
Previously publications are limited to adding registers on netlist level, which
generates a lot of system verification problems and which is assumed to be the
major drawback to use this technology in the modern multicore times. The paper
shows how CSR can efficiently be done with timing driven automatic RTL
modification. The methodology provided with this paper can be used as guidance
for using CSR in high level synthesis (HLS). The paper shows the results of a
CSR-ed complex RISC core on RTL implemented on FPGAs.
</summary>
    <author>
      <name>Tobias Strauch</name>
    </author>
    <link href="http://arxiv.org/abs/1807.05446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03083v1</id>
    <updated>2018-08-09T11:01:09Z</updated>
    <published>2018-08-09T11:01:09Z</published>
    <title>Hardware realization of residue number system algorithms by Boolean
  functions minimization</title>
    <summary>  Residue number systems (RNS) represent numbers by their remainders modulo a
set of relatively prime numbers. This paper pro- poses an efficient hardware
implementation of modular multiplication and of the modulo function (X(mod P)),
based on Boolean minimiza- tion. We report experiments showing a performance
advantage up to 30 times for our approach vs. the results obtained by
state-of-art industrial tools.
</summary>
    <author>
      <name>Danila Gorodecky</name>
    </author>
    <author>
      <name>Tiziano Villa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 picture, 1 table, 4 plots; it is the same paper as for 13th
  International Workshop on Boolean Problems (Bremen, Germany) paper with a
  title "Efficient hardware realization of arithmetic operations for the
  residue number system by Boolean minimization"</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.12474v1</id>
    <updated>2018-11-05T14:59:12Z</updated>
    <published>2018-11-05T14:59:12Z</published>
    <title>Formally Verifying WARP-V, an Open-Source TL-Verilog RISC-V Core
  Generator</title>
    <summary>  Timing-abstract and transaction-level design using TL-Verilog have shown
significant productivity gains for logic design. In this work, we explored the
natural extension of transaction-level design methodology into formal
verification.
  WARP-V is a CPU core generator written in TL-Verilog. Our primary
verification vehicle for WARP-V was a formal verification framework for RISC-V,
called riscv-formal. The timing-abstract and transaction-level logic modeling
techniques of TL-Verilog greatly simplified the task of creating a harness
connecting the WARP-V model to the verification interface of riscv-formal.
Furthermore, the same harness works across all RISC-V configurations of WARP-V.
</summary>
    <author>
      <name>Steven Hoover</name>
    </author>
    <author>
      <name>Ákos Hadnagy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4-pages. Presented by \'Akos Hadnagy at open-source hardware
  conferences: ORConf 2018 and VSDOpen 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.12474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.12474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.09316v1</id>
    <updated>2019-01-27T04:50:02Z</updated>
    <published>2019-01-27T04:50:02Z</published>
    <title>Majority and Minority Voted Redundancy for Safety-Critical Applications</title>
    <summary>  A new majority and minority voted redundancy (MMR) scheme is proposed that
can provide the same degree of fault tolerance as N-modular redundancy (NMR)
but with fewer function units and a less sophisticated voting logic. Example
NMR and MMR circuits were implemented using a 32/28nm CMOS process and
compared. The results show that MMR circuits dissipate less power, occupy less
area, and encounter less critical path delay than the corresponding NMR
circuits while providing the same degree of fault tolerance. Hence the MMR is a
promising alternative to the NMR to efficiently implement high levels of
redundancy in safety-critical applications.
</summary>
    <author>
      <name>P Balasubramanian</name>
    </author>
    <author>
      <name>D L Maskell</name>
    </author>
    <author>
      <name>N E Mastorakis</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 61st MWSCAS 2018, pp. 1102-1105, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1901.09316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.09316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.06742v1</id>
    <updated>2019-02-17T20:34:42Z</updated>
    <published>2019-02-17T20:34:42Z</published>
    <title>Applicability of Partial Ternary Full Adder in Ternary Arithmetic Units</title>
    <summary>  This paper explores whether or not a complete ternary full adder, whose input
variables can independently be '0', '1', or '2', is indispensable in the
arithmetic blocks of adder, subtractor, and multiplier. Our investigations show
that none of the mentioned arithmetic units require a complete ternary full
adder. Instead, they can be designed by use of partial ternary full adder,
whose input carry never becomes '2'. Furthermore, some new ternary compressors
are proposed in this paper without the requirement of complete ternary full
adder. The usage of partial ternary full adder can help circuit designers to
simplify their designs, especially in transistor level.
</summary>
    <author>
      <name>Aida Ghorbani Asibelagh</name>
    </author>
    <author>
      <name>Reza Faghih Mirzaee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.06742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.06742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.11264v2</id>
    <updated>2019-04-26T17:21:00Z</updated>
    <published>2019-03-27T06:58:36Z</published>
    <title>A Novel Hierarchical Circuit LUT Model for SOI Technology for Rapid
  Prototyping</title>
    <summary>  This article is withdrawn because the co-authors are not in favor of
publication.
</summary>
    <author>
      <name>Sitansusekhar Roymohapatra</name>
    </author>
    <author>
      <name>Ganesh R. Gore</name>
    </author>
    <author>
      <name>Akanksha Yadav</name>
    </author>
    <author>
      <name>Mahesh B. Patil</name>
    </author>
    <author>
      <name>Krishnan S. Rengarajan</name>
    </author>
    <author>
      <name>Subhramanian S. Iyer</name>
    </author>
    <author>
      <name>Maryam Shojaei Baghini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article was uploaded by mistake without taking consent of
  co-authors including my Ph.D. guide. The co-authors are not in favor of
  publication of this article in arXiv. Hence I am withdrawing it. This is my
  mistake and I apologize for this</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.11264v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.11264v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.08239v1</id>
    <updated>2019-05-18T21:56:27Z</updated>
    <published>2019-05-18T21:56:27Z</published>
    <title>Low-power Programmable Processor for Fast Fourier Transform Based on
  Transport Triggered Architecture</title>
    <summary>  This paper describes a low-power processor tailored for fast Fourier
transform computations where transport triggering template is exploited. The
processor is software-programmable while retaining an energy-efficiency
comparable to existing fixed-function implementations. The power savings are
achieved by compressing the computation kernel into one instruction word. The
word is stored in an instruction loop buffer, which is more power-efficient
than regular instruction memory storage. The processor supports all
power-of-two FFT sizes from 64 to 16384 and given 1 mJ of energy, it can
compute 20916 transforms of size 1024.
</summary>
    <author>
      <name>Jakub Žádník</name>
    </author>
    <author>
      <name>Jarmo Takala</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2019.8682289</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2019.8682289" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, 1 table, ICASSP 2019 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.08239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.02217v1</id>
    <updated>2019-07-04T04:48:04Z</updated>
    <published>2019-07-04T04:48:04Z</published>
    <title>FusionAccel: A General Re-configurable Deep Learning Inference
  Accelerator on FPGA for Convolutional Neural Networks</title>
    <summary>  The deep learning accelerator is one of the methods to accelerate deep
learning network computations, which is mainly based on convolutional neural
network acceleration. To address the fact that concurrent convolutional neural
network accelerators are not solely open-source and the exclusiveness of
platforms, FusionAccel, a scalable convolutional neural network accelerator
hardware architecture with supporting software is proposed. It can adapt to
different network structures and can be reconstructed before compilation and
reconfigured at runtime. This paper realizes this RTL convolutional neural
network accelerator design and functional verifications on a Xilinx Spartan-6
FPGA. The result is identical to that of Caffe-CPU. Since the entire project is
based on RTL, it can be migrated to ASIC after replacing some FPGA-specific
IPs.
</summary>
    <author>
      <name>Shi Shi</name>
    </author>
    <link href="http://arxiv.org/abs/1907.02217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.02217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.06948v1</id>
    <updated>2019-07-16T11:48:15Z</updated>
    <published>2019-07-16T11:48:15Z</published>
    <title>Coprocessors: failures and successes</title>
    <summary>  The appearance and disappearance of coprocessors by integration into the CPU,
the success or failure of coprocessors are examined by summarizing their
characteristics from the mainframes of the 1960s. The coprocessors most
particularly reviewed are the IBM 360 and CDC-6600 I/O processors, the Intel
8087 math coprocessor, the Cell processor, the Intel Xeon Phi coprocessors, the
GPUs, the FPGAs, and the coprocessors of manycores SW26010 and Pezy SC-2 used
in high-ranked supercomputers in the TOP500 or Green500. The conditions for a
coprocessor to be viable in the medium or long-term are defined.
</summary>
    <author>
      <name>Daniel Etiemble</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures, Conf\'erence d'Informatique en Parall\'elisme,
  Architecture et Syst\`eme (COMPAS'2019), June 25-28, Anglet, France</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.06948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.06948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.09078v1</id>
    <updated>2019-07-22T01:52:36Z</updated>
    <published>2019-07-22T01:52:36Z</published>
    <title>Reconfigurable multiplier architecture based on memristor-cmos with
  higher flexibility</title>
    <summary>  Multiplication is an indispensable operation in most of digital signal
processing systems. Recently, many systems need to execute different types of
algorithms on a multiplier. Therefore, it needs complicated computation and
large area occupation. In this regard a fixed multiplier is inefficient and the
development of a reconfigurable multiplier becomes increasingly important. The
advent of memristor-CMOS hybrid circuits provides an opportunity for reducing
area occupation. This paper introduces memristor-CMOS based reconfigurable
multiplier which provides flexible multiplication according to various
bit-width. Performance of the proposed multiplier is estimated with some
applications and comparison with conventional multipliers, using memristor
SPICE model and proprietary 180-nm CMOS process.
</summary>
    <author>
      <name>Seungbum Baek</name>
    </author>
    <link href="http://arxiv.org/abs/1907.09078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.09078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.10826v1</id>
    <updated>2019-07-24T10:31:54Z</updated>
    <published>2019-07-24T10:31:54Z</published>
    <title>Performance Comparison of Quasi-Delay-Insensitive Asynchronous Adders</title>
    <summary>  In this technical note, we provide a comparison of the design metrics of
various quasi-delay-insensitive (QDI) asynchronous adders, where the adders
correspond to diverse architectures. QDI adders are robust, and the objective
of this technical note is to point to those QDI adders which are suitable for
low power/energy and less area. This information could be valuable for a
resource-constrained low power VLSI design scenario. Non-QDI adders are
excluded from the comparison since they are not robust although they may have
optimized design metrics. All the QDI adders were realized using a 32/28nm CMOS
process.
</summary>
    <author>
      <name>P Balasubramanian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1903.09433</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.10826v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.10826v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.06841v1</id>
    <updated>2019-08-19T14:50:54Z</updated>
    <published>2019-08-19T14:50:54Z</published>
    <title>Ternary circuits: why R=3 is not the Optimal Radix for Computation</title>
    <summary>  A demonstration that e=2.718 rounded to 3 is the best radix for computation
is disproved. The MOSFET-like CNTFET technology is used to compare inverters,
Nand, adders, multipliers, D Flip-Flops and SRAM cells. The transistor count
ratio between ternary and binary circuits is generally greater than the
log(3)/log(2) information ratio. The only exceptions concern a circuit approach
that combines two circuit drawbacks (an additional power supply and a circuit
conflict between transistors) and only when it implements circuits based on the
ternary inverter. For arithmetic circuits such as adders and multipliers, the
ternary circuits are always outperformed by the binary ones using the same
technology.
</summary>
    <author>
      <name>Daniel Etiemble</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 18 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.06841v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.06841v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.00976v2</id>
    <updated>2019-11-03T09:53:17Z</updated>
    <published>2019-10-01T08:37:19Z</published>
    <title>An efficient floating point multiplier design for high speed
  applications using Karatsuba algorithm and Urdhva-Tiryagbhyam algorithm</title>
    <summary>  Floating point multiplication is a crucial operation in high power computing
applications such as image processing, signal processing etc. And also
multiplication is the most time and power consuming operation. This paper
proposes an efficient method for IEEE 754 floating point multiplication which
gives a better implementation in terms of delay and power. A combination of
Karatsuba algorithm and Urdhva-Tiryagbhyam algorithm (Vedic Mathematics) is
used to implement unsigned binary multiplier for mantissa multiplication. The
multiplier is implemented using Verilog HDL, targeted on Spartan-3E and
Virtex-4 FPGA.
</summary>
    <author>
      <name>S Arish</name>
    </author>
    <author>
      <name>R. K. Sharma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICSPCom.2015.7150666</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICSPCom.2015.7150666" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1909.13318</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2015 International Conference on Signal Processing and
  Communication (ICSC)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.00976v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.00976v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.10234v1</id>
    <updated>2019-10-22T21:14:16Z</updated>
    <published>2019-10-22T21:14:16Z</published>
    <title>The Bitlet Model: Defining a Litmus Test for the Bitwise
  Processing-in-Memory Paradigm</title>
    <summary>  This paper describes an analytical modeling tool called Bitlet that can be
used, in a parameterized fashion, to understand the affinity of workloads to
processing-in-memory (PIM) as opposed to traditional computing. The tool
uncovers interesting trade-offs between operation complexity (cycles required
to perform an operation through PIM) and other key parameters, such as system
memory bandwidth, data transfer size, the extent of data alignment, and
effective memory capacity involved in PIM computations. Despite its simplicity,
the model has already proven useful. In the future, we intend to extend and
refine Bitlet to further increase its utility.
</summary>
    <author>
      <name>Kunal Korgaonkar</name>
    </author>
    <author>
      <name>Ronny Ronen</name>
    </author>
    <author>
      <name>Anupam Chattopadhyay</name>
    </author>
    <author>
      <name>Shahar Kvatinsky</name>
    </author>
    <link href="http://arxiv.org/abs/1910.10234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.06901v1</id>
    <updated>2019-12-14T18:28:53Z</updated>
    <published>2019-12-14T18:28:53Z</published>
    <title>Adaptive Multi-bit SRAM Topology Based Analog PUF</title>
    <summary>  Physically Unclonable Functions (PUFs) are lightweight cryptographic
primitives for generating unique signatures from minuscule manufacturing
variations. In this work, we present lightweight, area efficient and low power
adaptive multi-bit SRAM topology based Current Mirror Array (CMA) analog PUF
design for securing the sensor nodes, authentication and key generation. The
proposed Strong PUF increases the complexity of the machine learning attacks
thus making it difficult for the adversary. The design is based on scl180
library.
</summary>
    <author>
      <name>Sudarshan Sharma</name>
    </author>
    <author>
      <name>Dhruv Thapar</name>
    </author>
    <author>
      <name>Nikhil Bhelave</name>
    </author>
    <author>
      <name>Mrigank Sharad</name>
    </author>
    <link href="http://arxiv.org/abs/1912.06901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.06901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.00787v1</id>
    <updated>2020-01-24T15:23:02Z</updated>
    <published>2020-01-24T15:23:02Z</published>
    <title>Efficient Fault Injection based on Dynamic HDL Slicing Technique</title>
    <summary>  This work proposes a fault injection methodology where Hardware Description
Language (HDL) code slicing is exploited to prune fault injection locations,
thus enabling more efficient campaigns for safety mechanisms evaluation. In
particular, the dynamic HDL slicing technique provides for a highly collapsed
critical fault list and allows avoiding injections at redundant locations or
time-steps. Experimental results show that the proposed methodology integrated
into commercial tool flow doubles the simulation speed when comparing to the
state-of-the-art industrial-grade EDA tool flows.
</summary>
    <author>
      <name>Ahmet Cagri Bagbaba</name>
    </author>
    <author>
      <name>Maksim Jenihhin</name>
    </author>
    <author>
      <name>Jaan Raik</name>
    </author>
    <author>
      <name>Christian Sauer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IOLTS.2019.8854419</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IOLTS.2019.8854419" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2001.09982</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.00787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.00787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.05315v1</id>
    <updated>2020-03-11T14:06:04Z</updated>
    <published>2020-03-11T14:06:04Z</published>
    <title>Cycle-Accurate Evaluation of Software-Hardware Co-Design of Decimal
  Computation in RISC-V Ecosystem</title>
    <summary>  Software-hardware co-design solutions for decimal computation can provide
several Pareto points to development of embedded systems in terms of hardware
cost and performance. This paper demonstrates how to accurately evaluate such
co-design solutions using RISC-V ecosystem. In a software-hardware co-design
solution, a part of solution requires dedicated hardware. In our evaluation
framework, we develop new decimal oriented instructions supported by an
accelerator. The framework can realize cycle-accurate analysis for performance
as well as hardware overhead for co-design solutions for decimal computation.
The obtained performance result is compared with an estimation with dummy
functions.
</summary>
    <author>
      <name>Riaz-ul-haque Mian</name>
    </author>
    <author>
      <name>Michihiro Shintani</name>
    </author>
    <author>
      <name>Michiko Inoue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE double column format, 6 pages with 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.05315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.05315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W35" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.02206v1</id>
    <updated>2020-05-05T14:14:27Z</updated>
    <published>2020-05-05T14:14:27Z</published>
    <title>Best implementations of quaternary adders</title>
    <summary>  The implementation of a quaternary 1-digit adder composed of a 2-bit binary
adder, quaternary to binary decoders and binary to quaternary encoders is
compared with several recent implementations of quaternary adders. This simple
implementation outperforms all other implementations using only one power
supply. It is equivalent to the best other implementation using three power
supplies. The best quaternary adder using a 2-bit binary adder, the interface
circuits between quaternary and binary levels are just overhead compared to the
binary adder. This result shows that the quaternary approach for adders use
more transistors, more chip area and more power dissipation than the
corresponding binary ones.
</summary>
    <author>
      <name>Daniel Etiemble</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 25 figures, research report</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.02206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.02206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.02678v1</id>
    <updated>2020-05-06T09:23:49Z</updated>
    <published>2020-05-06T09:23:49Z</published>
    <title>Comparing quaternary and binary multipliers</title>
    <summary>  We compare the implementation of a 8x8 bit multiplier with two different
implementations of a 4x4 quaternary digit multiplier. Interfacing this binary
multiplier with quaternary to binary decoders and binary to quaternary encoders
leads to a 4x4 multiplier that outperforms the best direct implementation of a
4x4 quaternary multiplier. The far greater complexity of the 1-digit
multipliers and 1-digit adders used in this direct implementation compared to
the binary 1-bit multipliers and full adders cannot be compensated by the
reduced count of quaternary operators. As the best quaternary multiplier
includes the corresponding binary one, it means that there is no opportunity to
get less interconnects, less chip area, less power dissipation with the
quaternary multiplier.
</summary>
    <author>
      <name>Daniel Etiemble</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 15 figures, Research Report</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.02678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.02678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.05693v1</id>
    <updated>2020-06-10T07:24:19Z</updated>
    <published>2020-06-10T07:24:19Z</published>
    <title>A GPU Register File using Static Data Compression</title>
    <summary>  GPUs rely on large register files to unlock thread-level parallelism for high
throughput. Unfortunately, large register files are power hungry, making it
important to seek for new approaches to improve their utilization.
  This paper introduces a new register file organization for efficient
register-packing of narrow integer and floating-point operands designed to
leverage on advances in static analysis. We show that the hardware/software
co-designed register file organization yields a performance improvement of up
to 79%, and 18.6%, on average, at a modest output-quality degradation.
</summary>
    <author>
      <name>Alexandra Angerd</name>
    </author>
    <author>
      <name>Erik Sintorn</name>
    </author>
    <author>
      <name>Per Stenström</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3404397.3404431</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3404397.3404431" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICPP'20</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.05693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.05693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.07450v1</id>
    <updated>2020-06-12T20:07:06Z</updated>
    <published>2020-06-12T20:07:06Z</published>
    <title>A Unified Learning Platform for Dynamic Frequency Scaling in Pipelined
  Processors</title>
    <summary>  A machine learning (ML) design framework is proposed for dynamically
adjusting clock frequency based on propagation delay of individual
instructions. A Random Forest model is trained to classify propagation delays
in real-time, utilizing current operation type, current operands, and
computation history as ML features. The trained model is implemented in Verilog
as an additional pipeline stage within a baseline processor. The modified
system is simulated at the gate-level in 45 nm CMOS technology, exhibiting a
speed-up of 68% and energy reduction of 37% with coarse-grained ML
classification. A speed-up of 95% is demonstrated with finer granularities at
additional energy costs.
</summary>
    <author>
      <name>Arash Fouman Ajirlou</name>
    </author>
    <author>
      <name>Inna Partin-Vaisband</name>
    </author>
    <link href="http://arxiv.org/abs/2006.07450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.07450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.02242v1</id>
    <updated>2020-07-05T05:51:00Z</updated>
    <published>2020-07-05T05:51:00Z</published>
    <title>A Ring Router Microarchitecture for NoCs</title>
    <summary>  Network-on-Chip (NoC) has become a popular choice for connecting a large
number of processing cores in chip multiprocessor design. In a conventional NoC
design, most of the area in the router is occupied by the buffers and the
crossbar switch. These two components also consume the majority of the router's
power. Much of the research in NoC has been based on the conventional router
microarchitecture. We propose a novel router microarchitecture that treats the
router itself as a small network of the ring topology. It eliminates the large
crossbar switch in the conventional design. In addition, network latency is
much reduced. Simulation and circuit synthesis show that the proposed
microarchitecture can reduce the latency, area and power by 53%, 34% and 27%,
respectively, compared to the conventional design.
</summary>
    <author>
      <name>Wo-Tak Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2007.02242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.02242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.09109v2</id>
    <updated>2021-02-07T16:01:41Z</updated>
    <published>2020-07-17T16:38:46Z</published>
    <title>Klessydra-T: Designing Vector Coprocessors for Multi-Threaded
  Edge-Computing Cores</title>
    <summary>  Computation intensive kernels, such as convolutions, matrix multiplication
and Fourier transform, are fundamental to edge-computing AI, signal processing
and cryptographic applications. Interleaved-Multi-Threading (IMT) processor
cores are interesting to pursue energy efficiency and low hardware cost for
edge-computing, yet they need hardware acceleration schemes to run heavy
computational workloads. Following a vector approach to accelerate
computations, this study explores possible alternatives to implement vector
coprocessing units in RISC-V cores, showing the synergy between IMT and
data-level parallelism in the target workloads.
</summary>
    <author>
      <name>Abdallah Cheikh</name>
    </author>
    <author>
      <name>Stefano Sordillo</name>
    </author>
    <author>
      <name>Antonio Mastrandrea</name>
    </author>
    <author>
      <name>Francesco Menichelli</name>
    </author>
    <author>
      <name>Giuseppe Scotti</name>
    </author>
    <author>
      <name>Mauro Olivieri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MM.2021.3050962</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MM.2021.3050962" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final revision accepted for publication on IEEE Micro Journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Micro, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2007.09109v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.09109v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.01267v1</id>
    <updated>2020-12-02T15:22:01Z</updated>
    <published>2020-12-02T15:22:01Z</published>
    <title>Multivalued circuits and Interconnect issues</title>
    <summary>  Many papers have presented multi-valued circuits in various technologies as a
solution to reduce or solve interconnection issues in binary circuits. This
assumption is discussed. While 4-valued signaling could divide by two the
number of interconnects between building blocks, it turns out that circuit
designers use interconnect standards based on differential pairs such as PCIe,
Infiniband, RapidIO, NVLink, etc. Doubling the number of binary signals is a
better solution than using single-ended quaternary signals. The design of
quaternary basic gates, adders and multipliers are compared with the
corresponding binary ones.
  At each level, the transistor count ratio between quaternary and binary
circuits is greater than the x2 information ratio between base 4 and base 2.
Quaternary signaling is not a solution, either between or within circuit blocks .
</summary>
    <author>
      <name>Daniel Etiemble</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 12 figures, preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.01267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.01267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.02037v1</id>
    <updated>2020-12-03T16:22:10Z</updated>
    <published>2020-12-03T16:22:10Z</published>
    <title>Characteristics of Reversible Circuits for Error Detection</title>
    <summary>  In this work, we consider error detection via simulation for reversible
circuit architectures. We rigorously prove that reversibility augments the
performance of this simple error detection protocol to a considerable degree. A
single randomly generated input is guaranteed to unveil a single error with a
probability that only depends on the size of the error, not the size of the
circuit itself. Empirical studies confirm that this behavior typically extends
to multiple errors as well. In conclusion, reversible circuits offer
characteristics that reduce masking effects -- a desirable feature that is in
stark contrast to irreversible circuit architectures.
</summary>
    <author>
      <name>Lukas Burgholzer</name>
    </author>
    <author>
      <name>Robert Wille</name>
    </author>
    <author>
      <name>Richard Kueng</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.array.2022.100165</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.array.2022.100165" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.02037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.02037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.05419v2</id>
    <updated>2021-06-04T22:01:18Z</updated>
    <published>2020-12-10T02:31:57Z</published>
    <title>A Custom 7nm CMOS Standard Cell Library for Implementing TNN-based
  Neuromorphic Processors</title>
    <summary>  A set of highly-optimized custom macro extensions is developed for a 7nm CMOS
cell library for implementing Temporal Neural Networks (TNNs) that can mimic
brain-like sensory processing with extreme energy efficiency. A TNN prototype
(13,750 neurons and 315,000 synapses) for MNIST requires only 1.56mm2 die area
and consumes only 1.69mW.
</summary>
    <author>
      <name>Harideep Nair</name>
    </author>
    <author>
      <name>Prabhu Vellaisamy</name>
    </author>
    <author>
      <name>Santha Bhasuthkar</name>
    </author>
    <author>
      <name>John Paul Shen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work is dated and will be superseded by a forthcoming work</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.05419v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.05419v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.08071v1</id>
    <updated>2020-12-15T03:38:51Z</updated>
    <published>2020-12-15T03:38:51Z</published>
    <title>Optimization Techniques to Improve Inference Performance of a Forward
  Propagating Neural Network on an FPGA</title>
    <summary>  This paper describes an optimized implementation of a Forward Propagating
Classification Neural Network which has been previously trained. The
implementation described highlights a novel means of using Python scripts to
generate a Verilog hardware implementation. The characteristics of this
implementation include optimizations to scale input data, use selected addends
instead of multiplication functions, hardware friendly activation functions and
simplified output selection. Inference performance comparison of a 28x28 pixel
'hand-written' recognition NN between a software implementation on an Intel i7
vs a Xilinx FPGA will be detailed.
</summary>
    <author>
      <name>Matthew Joseph Adiletta</name>
    </author>
    <author>
      <name>Brian Flanagan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.08071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.08071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.08885v1</id>
    <updated>2021-01-21T23:17:42Z</updated>
    <published>2021-01-21T23:17:42Z</published>
    <title>User-Aware Power Management for Mobile Devices</title>
    <summary>  The power management techniques to extend battery lifespan is becoming
increasingly important due to longer user applications' running time in mobile
devices. Even when users do not use any applications, battery lifespan
decreases continually. It occurs because of service daemons of mobile platform
and network-based data synchronization operations. In this paper, we propose a
new power management system that recognizes the idle time of the device to
reduce the battery consumption of mobile devices.
</summary>
    <author>
      <name>Geunsik Lim</name>
    </author>
    <author>
      <name>Changwoo Min</name>
    </author>
    <author>
      <name>Dong Hyun Kang</name>
    </author>
    <author>
      <name>Young Ik Eom</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/GCCE.2013.6664780</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/GCCE.2013.6664780" rel="related"/>
    <link href="http://arxiv.org/abs/2101.08885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.08885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.09821v1</id>
    <updated>2021-01-24T23:01:42Z</updated>
    <published>2021-01-24T23:01:42Z</published>
    <title>A Survey of Novel Cache Hierarchy Designs for High Workloads</title>
    <summary>  Traditional on-die, three-level cache hierarchy design is very commonly used
but is also prone to latency, especially at the Level 2 (L2) cache. We discuss
three distinct ways of improving this design in order to have better
performance. Performance is especially important for systems with high
workloads. The first method proposes to eliminate L2 altogether while proposing
a new prefetching technique, the second method suggests increasing the size of
L2, while the last method advocates the implementation of optical caches. After
carefully contemplating the results in performance gains and the advantages and
disadvantages of each method, we found the last method to be the best of the
three.
</summary>
    <author>
      <name>Pranjal Singh Rajput</name>
    </author>
    <author>
      <name>Sonnya Dellarosa</name>
    </author>
    <author>
      <name>Kanya Satis</name>
    </author>
    <link href="http://arxiv.org/abs/2101.09821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.09821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.14808v1</id>
    <updated>2021-03-27T04:58:53Z</updated>
    <published>2021-03-27T04:58:53Z</published>
    <title>Reducing Load Latency with Cache Level Prediction</title>
    <summary>  High load latency that results from deep cache hierarchies and relatively
slow main memory is an important limiter of single-thread performance. Data
prefetch helps reduce this latency by fetching data up the hierarchy before it
is requested by load instructions. However, data prefetching has shown to be
imperfect in many situations. We propose cache-level prediction to complement
prefetchers. Our method predicts which memory hierarchy level a load will
access allowing the memory loads to start earlier, and thereby saves many
cycles. The predictor provides high prediction accuracy at the cost of just one
cycle added latency to L1 misses. Experimental results show speedup of 7.8\% on
generic, graph, and HPC applications over a baseline with aggressive
prefetchers.
</summary>
    <author>
      <name>Majid Jalili</name>
    </author>
    <author>
      <name>Mattan Erez</name>
    </author>
    <link href="http://arxiv.org/abs/2103.14808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.14808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.01448v1</id>
    <updated>2021-04-03T17:04:11Z</updated>
    <published>2021-04-03T17:04:11Z</published>
    <title>Compiler Infrastructure for Specializing Domain-Specific Memory
  Templates</title>
    <summary>  Specialized hardware accelerators are becoming important for more and more
applications. Thanks to specialization, they can achieve high performance and
energy efficiency but their design is complex and time consuming. This problem
is exacerbated when large amounts of data must be processed, like in modern big
data and machine learning applications. The designer has not only to optimize
the accelerator logic but also produce efficient memory architectures. To
simplify this process, we propose a multi-level compilation flow that
specializes a domain-specific memory template to match data, application, and
technology requirements.
</summary>
    <author>
      <name>Stephanie Soldavini</name>
    </author>
    <author>
      <name>Christian Pilato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for presentation at the 1st Workshop on Languages, Tools,
  and Techniques for Accelerator Design (LATTE'21)</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.01448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.01448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.03024v1</id>
    <updated>2021-04-07T09:56:42Z</updated>
    <published>2021-04-07T09:56:42Z</published>
    <title>Polynomial Circuit Verification using BDDs</title>
    <summary>  Verification is one of the central tasks during circuit design. While most of
the approaches have exponential worst-case behaviour, in the following
techniques are discussed for proving polynomial circuit verification based on
Binary Decision Diagrams (BDDs). It is shown that for circuits with specific
structural properties, like e.g. tree-like circuits, and circuits based on
multiplexers derived from BDDs complete formal verification can be carried out
in polynomial time and space.
</summary>
    <author>
      <name>Rolf Drechsler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.03024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.03024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W30, 68M07, 68W35" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.6.3; B.2.1; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04003v1</id>
    <updated>2021-04-08T18:36:42Z</updated>
    <published>2021-04-08T18:36:42Z</published>
    <title>AutoSVA: Democratizing Formal Verification of RTL Module Interactions</title>
    <summary>  Modern SoC design relies on the ability to separately verify IP blocks
relative to their own specifications. Formal verification (FV) using
SystemVerilog Assertions (SVA) is an effective method to exhaustively verify
blocks at unit-level. Unfortunately, FV has a steep learning curve and requires
engineering effort that discourages hardware designers from using it during RTL
module development. We propose AutoSVA, a framework to automatically generate
FV testbenches that verify liveness and safety of control logic involved in
module interactions. We demonstrate AutoSVA's effectiveness and efficiency on
deadlock-critical modules of widely-used open-source hardware projects.
</summary>
    <author>
      <name>Marcelo Orenes-Vera</name>
    </author>
    <author>
      <name>Aninda Manocha</name>
    </author>
    <author>
      <name>David Wentzlaff</name>
    </author>
    <author>
      <name>Margaret Martonosi</name>
    </author>
    <link href="http://arxiv.org/abs/2104.04003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.09798v1</id>
    <updated>2021-04-20T07:20:17Z</updated>
    <published>2021-04-20T07:20:17Z</published>
    <title>CoDR: Computation and Data Reuse Aware CNN Accelerator</title>
    <summary>  Computation and Data Reuse is critical for the resource-limited Convolutional
Neural Network (CNN) accelerators. This paper presents Universal Computation
Reuse to exploit weight sparsity, repetition, and similarity simultaneously in
a convolutional layer. Moreover, CoDR decreases the cost of weight memory
access by proposing a customized Run-Length Encoding scheme and the number of
memory accesses to the intermediate results by introducing an input and output
stationary dataflow. Compared to two recent compressed CNN accelerators with
the same area of 2.85 mm^2, CoDR decreases SRAM access by 5.08x and 7.99x, and
consumes 3.76x and 6.84x less energy.
</summary>
    <author>
      <name>Alireza Khadem</name>
    </author>
    <author>
      <name>Haojie Ye</name>
    </author>
    <author>
      <name>Trevor Mudge</name>
    </author>
    <link href="http://arxiv.org/abs/2104.09798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.09798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.14155v1</id>
    <updated>2021-04-29T07:32:43Z</updated>
    <published>2021-04-29T07:32:43Z</published>
    <title>Automated Design Space Exploration of CGRA Processing Element
  Architectures using Frequent Subgraph Analysis</title>
    <summary>  The architecture of a coarse-grained reconfigurable array (CGRA) processing
element (PE) has a significant effect on the performance and energy efficiency
of an application running on the CGRA. This paper presents an automated
approach for generating specialized PE architectures for an application or an
application domain. Frequent subgraphs mined from a set of applications are
merged to form a PE architecture specialized to that application domain. For
the image processing and machine learning domains, we generate specialized PEs
that are up to 10.5x more energy efficient and consume 9.1x less area than a
baseline PE.
</summary>
    <author>
      <name>Jackson Melchert</name>
    </author>
    <author>
      <name>Kathleen Feng</name>
    </author>
    <author>
      <name>Caleb Donovick</name>
    </author>
    <author>
      <name>Ross Daly</name>
    </author>
    <author>
      <name>Clark Barrett</name>
    </author>
    <author>
      <name>Mark Horowitz</name>
    </author>
    <author>
      <name>Pat Hanrahan</name>
    </author>
    <author>
      <name>Priyanka Raina</name>
    </author>
    <link href="http://arxiv.org/abs/2104.14155v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.14155v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.01139v1</id>
    <updated>2021-06-02T13:15:50Z</updated>
    <published>2021-06-02T13:15:50Z</published>
    <title>How Flexible is Your Computing System</title>
    <summary>  In literature computer architectures are frequently claimed to be highly
flexible, typically implying there exist trade-offs between flexibility and
performance or energy efficiency. Processor flexibility, however, is not very
sharply defined, and as such these claims can not be validated, nor can such
hypothetical relations be fully understood and exploited in the design of
computing systems. This paper is an attempt to introduce scientific rigour to
the notion of flexibility in computing systems.
</summary>
    <author>
      <name>Shihua Huang</name>
    </author>
    <author>
      <name>Luc Waeijen</name>
    </author>
    <author>
      <name>Henk Corporaal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Partial preprint pending peer review</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.01139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.01139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.01671v1</id>
    <updated>2021-06-03T08:20:40Z</updated>
    <published>2021-06-03T08:20:40Z</published>
    <title>Analyzing crosstalk error in the NISQ era</title>
    <summary>  Noisy Intermediate-Scale Quantum (NISQ) hardware has unavoidable noises, and
crosstalk error is a significant error source. When multiple quantum operations
are executed simultaneously, the quantum state can be corrupted due to the
crosstalk between gates during simultaneous operations, decreasing the circuit
fidelity. In this work, we first report on several protocols for characterizing
crosstalk. Then, we discuss different crosstalk mitigation methods from the
hardware and software perspectives. Finally, we perform crosstalk injection
experiments on the IBM quantum device and demonstrate the fidelity improvement
with the crosstalk mitigation method.
</summary>
    <author>
      <name>Siyuan Niu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIRMM</arxiv:affiliation>
    </author>
    <author>
      <name>Aida Todri-Sanial</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIRMM, CNRS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Computer Society Annual Symposium on VLSI 2021, Jul 2021,
  Tampa, Florida, United States</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.01671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.01671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.11376v2</id>
    <updated>2021-06-23T20:53:49Z</updated>
    <published>2021-06-21T19:21:33Z</published>
    <title>Content Addressable Parallel Processors on a FPGA</title>
    <summary>  In this short article, we report on the implementation of a Content
Addressable Parallel Processor using a FPGA. While Content addressable memories
have been implemented in FPGAs, to our knowledge this is the first
implementation in FPGA of Caxton C. Foster's vision of parallel processing,
particularly the notions of parallel write as well as the combining of output
values, which are usually missing in more typical CAM implementations, such as
the ones designed for network routing. The resulting CAPP is made accessible to
a host computer over a USB/UART interface, using a straightforward serial
protocol that is demonstrated using a Python-based driver.
</summary>
    <author>
      <name>Ayush Salik</name>
    </author>
    <author>
      <name>Manor Askenazi</name>
    </author>
    <author>
      <name>Edward Rietman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.11376v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.11376v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.04042v1</id>
    <updated>2021-08-09T13:52:26Z</updated>
    <published>2021-08-09T13:52:26Z</published>
    <title>Understanding Tool Synthesis Behavior and Safe Finite State Machine
  Design</title>
    <summary>  High-reliability design requires understanding synthesis tool behavior and
best practices. Detection and protection against illegal states and transitions
is important for critical Finite State Machines (FSMs) within high reliability
applications. Single Event Upsets (SEUs) probability is increasing with
decreasing circuit dimensions and voltage [1]. SEU handling must be analyzed
post optimization to ensure designed protections are still functional. In this
work the default behavior of three synthesis tools interacting with high
reliability FSMs is discussed. Post-synthesis netlists of test FSMs are
analyzed for optimization induced changes that affect reliability during a SEU.
Best practices are proposed to curtail aggressive optimizers.
</summary>
    <author>
      <name>Katie Liszewski</name>
    </author>
    <author>
      <name>Timothy McDonley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.04042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.04042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.7.3; B.7.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.03021v1</id>
    <updated>2021-07-19T10:04:50Z</updated>
    <published>2021-07-19T10:04:50Z</published>
    <title>Limited Associativity Makes Concurrent Software Caches a Breeze</title>
    <summary>  Software caches optimize the performance of diverse storage systems,
databases and other software systems. Existing works on software caches
automatically resort to fully associative cache designs. Our work shows that
limited associativity caches are a promising direction for concurrent software
caches. Specifically, we demonstrate that limited associativity enables simple
yet efficient realizations of multiple cache management schemes that can be
trivially parallelized. We show that the obtained hit ratio is usually similar
to fully associative caches of the same management policy, but the throughput
is improved by up to X5 compared to production-grade caching libraries,
especially in multi-threaded executions.
</summary>
    <author>
      <name>Dolev Adas</name>
    </author>
    <author>
      <name>Gil Einziger</name>
    </author>
    <author>
      <name>Roy Friedman</name>
    </author>
    <link href="http://arxiv.org/abs/2109.03021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.03021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.06526v3</id>
    <updated>2021-10-15T00:31:04Z</updated>
    <published>2021-10-13T06:41:10Z</published>
    <title>Practice Problems for Hardware Engineers</title>
    <summary>  This book is to help undergraduate and graduate students of electrical and
computer engineering disciplines with their job interviews. It may also be used
as a practice resource while taking courses in VLSI, logic and computer
architecture design. The first edition consists of more than 150 problems and
their solutions which the author has used in his VLSI, logic, and architectures
courses while teaching at USC. The author wishes this book to be available free
of charge, subject to the copyright policy on page 3.
</summary>
    <author>
      <name>Shahin Nazarian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">166 pages, Kindle Direct Publishing, Amazon 1st Edition</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Kindle Direct Publishing, Amazon Oct. 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.06526v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.06526v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.11521v1</id>
    <updated>2021-10-17T18:34:46Z</updated>
    <published>2021-10-17T18:34:46Z</published>
    <title>High Level Synthesis Implementation of a Three-dimensional Systolic
  Array Architecture for Matrix Multiplications on Intel Stratix 10 FPGAs</title>
    <summary>  In this paper, we consider the HLS implementation of a three-dimensional
systolic array architecture for matrix multiplication that targets specific
characteristics of Intel Stratix 10 FPGAs in order to produce designs that
achieve a high floating-point throughput using most of the DSPs at high
frequencies in a way that avoids the congestion of the routing fabric. The
investigated three-dimensional systolic array architecture is able to produce
hardware designs that use 99% of the available DSPs with maximum frequencies
that let us achieve performances above 3 TFLOPS.
</summary>
    <author>
      <name>Paolo Gorlani</name>
    </author>
    <author>
      <name>Christian Plessl</name>
    </author>
    <link href="http://arxiv.org/abs/2110.11521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.11521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.01947v1</id>
    <updated>2021-10-22T11:05:34Z</updated>
    <published>2021-10-22T11:05:34Z</published>
    <title>An Evaluation of WebAssembly and eBPF as Offloading Mechanisms in the
  Context of Computational Storage</title>
    <summary>  As the volume of data that needs to be processed continues to increase, we
also see renewed interests in near-data processing in the form of computational
storage, with eBPF (extended Berkeley Packet Filter) being proposed as a
vehicle for computation offloading. However, discussions in this regard have so
far ignored viable alternatives, and no convincing analysis has been provided.
As such, we qualitatively and quantitatively evaluated eBPF against
WebAssembly, a seemingly similar technology, in the context of computation
offloading. This report presents our findings.
</summary>
    <author>
      <name>Wenjun Huang</name>
    </author>
    <author>
      <name>Marcus Paradies</name>
    </author>
    <link href="http://arxiv.org/abs/2111.01947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.01947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.06436v1</id>
    <updated>2021-12-13T06:26:19Z</updated>
    <published>2021-12-13T06:26:19Z</published>
    <title>Software-Hardware Evolution and birth of Multicore Processors</title>
    <summary>  This paper presents a brief journey to the evolution of computer hardware and
software, and underlines that shift to multicore technology is natual part of
the evolution, and highlights the various laws governing the advancement of
computer industry. Looking to these, it appears that the HW-SW industry trend
can be represented by a mathematical model, for which future developments are
predictable. Finally, the paper establishes that future of computer industry
lies in more thrust in software to exploit parallelism available in software to
utilize the heterogeneity in multicore processors.
</summary>
    <author>
      <name>K. R. Chowdhary</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.06436v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.06436v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.01413v2</id>
    <updated>2022-03-09T12:07:44Z</updated>
    <published>2022-02-24T03:36:16Z</published>
    <title>A 915-1220 TOPS/W Hybrid In-Memory Computing based Image Restoration and
  Region Proposal Integrated Circuit for Neuromorphic Vision Sensors in 65nm
  CMOS</title>
    <summary>  In this work, we present a hybrid memory bit cell - collocated SRAM and DRAM
(CRAM) consisting of 11 transistors for in-memory computing (IMC) based image
restoration (IR) and region proposal (RP). A robust RP updated algorithm is
proposed to improve the performance. This work demonstrates IMC based global
parallel diffusion and column/row-wise projection to achieve a maximal energy
efficiency of 1220 TOPS/W for image restoration and 915 TOPS/W when combined
with region proposal.
</summary>
    <author>
      <name>Xueyong Zhang</name>
    </author>
    <author>
      <name>Arindam Basu</name>
    </author>
    <link href="http://arxiv.org/abs/2203.01413v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.01413v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.05742v1</id>
    <updated>2022-03-11T04:12:17Z</updated>
    <published>2022-03-11T04:12:17Z</published>
    <title>Bringing Source-Level Debugging Frameworks to Hardware Generators</title>
    <summary>  High-level hardware generators have significantly increased the productivity
of design engineers. They use software engineering constructs to reduce the
repetition required to express complex designs and enable more composability.
However, these benefits are undermined by a lack of debugging infrastructure,
requiring hardware designers to debug generated, usually incomprehensible, RTL
code. This paper describes a framework that connects modern software
source-level debugging frameworks to RTL created from hardware generators. Our
working prototype offers an Integrated Development Environment (IDE) experience
for generators such as RocketChip (Chisel), allowing designers to set
breakpoints in complex source code, relate RTL simulation state back to
source-level variables, and do forward and backward debugging, with almost no
simulation overhead (less than 5%).
</summary>
    <author>
      <name>Keyi Zhang</name>
    </author>
    <author>
      <name>Zain Asgar</name>
    </author>
    <author>
      <name>Mark Horowitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Design Automation Conference (DAC) 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.05742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.05742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.15561v1</id>
    <updated>2022-03-28T13:50:50Z</updated>
    <published>2022-03-28T13:50:50Z</published>
    <title>Algorithmic Improvement and GPU Acceleration of the GenASM Algorithm</title>
    <summary>  We improve on GenASM, a recent algorithm for genomic sequence alignment, by
significantly reducing its memory footprint and bandwidth requirement. Our
algorithmic improvements reduce the memory footprint by 24$\times$ and the
number of memory accesses by 12$\times$. We efficiently parallelize the
algorithm for GPUs, achieving a 4.1$\times$ speedup over a CPU implementation
of the same algorithm, a 62$\times$ speedup over minimap2's CPU-based KSW2 and
a 7.2$\times$ speedup over the CPU-based Edlib for long reads.
</summary>
    <author>
      <name>Joël Lindegger</name>
    </author>
    <author>
      <name>Damla Senol Cali</name>
    </author>
    <author>
      <name>Mohammed Alser</name>
    </author>
    <author>
      <name>Juan Gómez-Luna</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at the 21st IEEE International Workshop on High Performance
  Computational Biology (HiCOMB) 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.15561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.15561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.01679v1</id>
    <updated>2022-04-04T17:50:45Z</updated>
    <published>2022-04-04T17:50:45Z</published>
    <title>Predictable Sharing of Last-level Cache Partitions for Multi-core
  Safety-critical Systems</title>
    <summary>  Last-level cache (LLC) partitioning is a technique to provide temporal
isolation and low worst-case latency (WCL) bounds when cores access the shared
LLC in multicore safety-critical systems. A typical approach to cache
partitioning involves allocating a separate partition to a distinct core. A
central criticism of this approach is its poor utilization of cache storage.
Today's trend of integrating a larger number of cores exacerbates this issue
such that we are forced to consider shared LLC partitions for effective
deployments. This work presents an approach to share LLC partitions among
multiple cores while being able to provide low WCL bounds.
</summary>
    <author>
      <name>Zhuanhao Wu</name>
    </author>
    <author>
      <name>Hiren Patel</name>
    </author>
    <link href="http://arxiv.org/abs/2204.01679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.01679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.05107v1</id>
    <updated>2022-04-07T22:19:03Z</updated>
    <published>2022-04-07T22:19:03Z</published>
    <title>Challenges in implementing DDR3 memory interface on PCB systems: a
  methodology for interfacing DDR3 SDRAM DIMM to an FPGA</title>
    <summary>  Undoubtedly faster, larger and lower power per bit, but just how do you go
about interfacing a DDR3 SDRAM DIMM to an FPGA? The DDR3 standard addresses the
faster, more bandwidth and lower power per bit need, but it introduces new
design challenges in addition to challenges introduced by DDR2 ODT, slew rate
derating, etc. The DDR3 fly-by topology requirement means customers designing
DDR3 memories must now account for write leveling and read de-skew on the PCB.
This paper will cover modeling, simulation, and physical layout approaches
required to meet JEDEC-defined termination and tight timing requirements for
designing DDR3 memory interfaces on PCB systems.
</summary>
    <author>
      <name>Phil Murray</name>
    </author>
    <author>
      <name>Feras Al-Hawari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 10 figures, DesignCon 2008, published in 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.05107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.05107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1967v1</id>
    <updated>2011-05-10T15:03:05Z</updated>
    <published>2011-05-10T15:03:05Z</published>
    <title>Algebra-Logical Repair Method for FPGA Logic Blocks</title>
    <summary>  An algebra-logical repair method for FPGA functional logic blocks on the
basis of solving the coverage problem is proposed. It is focused on
implementation into Infrastructure IP for system-on-a chip and
system-in-package. A method is designed for providing the operability of FPGA
blocks and digital system as a whole. It enables to obtain exact and optimal
solution associated with the minimum number of spares needed to repair the FPGA
logic components with multiple faults.
</summary>
    <author>
      <name>Vladimir Hahanov</name>
    </author>
    <author>
      <name>Eugenia Litvinova</name>
    </author>
    <author>
      <name>Wajeb Gharibi</name>
    </author>
    <author>
      <name>Olesya Guz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Radioelectronics and Informatics, No.2, Vol. 45, pp.
  49-56, April -- June 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1105.1967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1973v1</id>
    <updated>2011-05-10T15:24:49Z</updated>
    <published>2011-05-10T15:24:49Z</published>
    <title>Brain-like infrastructure for embedded SoC diagnosis</title>
    <summary>  This article describes high-speed multiprocessor architecture for the
concurrent analyzing information represented in analytic, graph- and table
forms of associative relations to search, recognize and make a decision in
n-dimensional vector discrete space. Vector-logical process models of actual
applications,for which the quality of solution is estimated by the proposed
integral non-arithmetical metric of the interaction between Boolean vectors,
are described.
</summary>
    <author>
      <name>Vladimir Hahanov</name>
    </author>
    <author>
      <name>Wajeb Gharibi</name>
    </author>
    <author>
      <name>Olesya Guz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/AQTR.2010.5520841</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/AQTR.2010.5520841" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Automation Quality and Testing
  Robotics (AQTR), 28-30 May 2010, Cluj-Napoca, Romania</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1105.1973v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1973v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.2960v1</id>
    <updated>2011-05-15T19:03:25Z</updated>
    <published>2011-05-15T19:03:25Z</published>
    <title>Multi-Amdahl: Optimal Resource Sharing with Multiple Program Execution
  Segments</title>
    <summary>  This paper presents Multi-Amdahl, a resource allocation analytical tool for
heterogeneous systems. Our model includes multiple program execution segments,
where each one is accelerated by a specific hardware unit. The acceleration
speedup of the specific hardware unit is a function of a limited resource, such
as the unit area, power, or energy. Using the Lagrange theorem we discover the
optimal resource distribution between all specific units. We then illustrate
this general Multi-Amdahl technique using several examples of area and power
allocation among several cores and accelerators.
</summary>
    <author>
      <name>Tsahee Zidenberg</name>
    </author>
    <author>
      <name>Isaac Keslassy</name>
    </author>
    <author>
      <name>Uri Weiser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.2960v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.2960v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.0727v1</id>
    <updated>2011-12-04T06:48:53Z</updated>
    <published>2011-12-04T06:48:53Z</published>
    <title>Quantum Cost Efficient Reversible BCD Adder for Nanotechnology Based
  Systems</title>
    <summary>  Reversible logic allows low power dissipating circuit design and founds its
application in cryptography, digital signal processing, quantum and optical
information processing. This paper presents a novel quantum cost efficient
reversible BCD adder for nanotechnology based systems using PFAG gate. It has
been demonstrated that the proposed design offers less hardware complexity and
requires minimum number of garbage outputs than the existing counterparts. The
remarkable property of the proposed designs is that its quantum realization is
given in NMR technology.
</summary>
    <author>
      <name>Md. Saiful Islam</name>
    </author>
    <author>
      <name>Mohd. Zulfiquar Hafiz</name>
    </author>
    <author>
      <name>Zerina Begum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 12 figures, 1 table, submitted to IJCEE for possible
  publication</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer and Electrical Engineering,
  Vol.4, No.1, pp. 10-13, February 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1112.0727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.0727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.6338v1</id>
    <updated>2012-10-22T15:11:08Z</updated>
    <published>2012-10-22T15:11:08Z</published>
    <title>A Ternary Digital to Analog Converter with High Power Output and 170-dB
  Dynamic Range</title>
    <summary>  A prototype of a very high dynamic range 32-bits Digital to Analog Converter
(DAC) was designed and built for the purpose of direct auditory stimulus
generation. It provides signals from less than 100 nV up to 50 Watts peak power
output, driving a 32-Ohms earphone or speaker. The use of ternary cells makes
possible a 170 dB dynamic range that is basically limited by thermal noise
only.
</summary>
    <author>
      <name>Guido Stolfi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Sao Paulo</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.6338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.6338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94C99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.3109v2</id>
    <updated>2013-11-08T09:38:43Z</updated>
    <published>2013-06-13T14:02:13Z</published>
    <title>Computer Architecture with Associative Processor Replacing Last Level
  Cache and SIMD Accelerator</title>
    <summary>  This study presents a novel computer architecture where a last level cache
and a SIMD accelerator are replaced by an Associative Processor. Associative
Processor combines data storage and data processing and provides parallel
computational capabilities and data memory at the same time. An analytic
performance model of the new computer architecture is introduced. Comparative
analysis supported by simulation shows that this novel architecture may
outperform a conventional architecture comprising a SIMD coprocessor and a
shared last level cache while consuming less power.
</summary>
    <author>
      <name>Leonid Yavits</name>
    </author>
    <author>
      <name>Amir Morad</name>
    </author>
    <author>
      <name>Ran Ginosar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to a crucial error in
  equation 10</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.3109v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.3109v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.5501v1</id>
    <updated>2013-06-24T02:50:32Z</updated>
    <published>2013-06-24T02:50:32Z</published>
    <title>A Wrapper of PCI Express with FIFO Interfaces based on FPGA</title>
    <summary>  This paper proposes a PCI Express (PCIE) Wrapper core named PWrapper with
FIFO interfaces. Compared with other PCIE solutions, PWrapper has several
advantages such as flexibility, isolation of clock domain, etc. PWrapper is
implemented and verified on Vertex -5-FX70T which is a development board
provided by Xilinx Inc. Architecture of PWrapper and design of two key modules
are illustrated, which timing optimization methods have been adopted. Then we
explained the advantages and challenges of on-chip interfaces technology based
on FIFOs. The verification results show that PWrapper can achieve the speed of
1.8Gbps (Giga bits per second).
</summary>
    <author>
      <name>Hu Li</name>
    </author>
    <author>
      <name>Yuan`an Liu</name>
    </author>
    <author>
      <name>Dongming Yuan</name>
    </author>
    <author>
      <name>Hefei Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2012 International Conference on Industrial
  Control and Electronics Engineering, ICICEE 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1306.5501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.5501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.02354v1</id>
    <updated>2015-03-09T01:36:50Z</updated>
    <published>2015-03-09T01:36:50Z</published>
    <title>A General Scheme for Noise-Tolerant Logic Design Based on Probabilistic
  and DCVS Approaches</title>
    <summary>  In this paper, a general circuit scheme for noise-tolerant logic design based
on Markov Random Field theory and differential Cascade Voltage Switch technique
has been proposed, which is an extension of the work in [1-3], [4]. A block
with only four transistors has been successfully inserted to the original
circuit scheme from [3] and extensive simulation results show that our proposed
design can operate correctly with the input signal of 1 dB signal-noise-ratio.
When using the evaluation parameter from [5], the output value of our design
decreases by 76.5% on average than [3] which means that superior noise-immunity
could be obtained through our work.
</summary>
    <author>
      <name>Xinghua Yang</name>
    </author>
    <author>
      <name>Fei Qiao</name>
    </author>
    <author>
      <name>Qi Wei</name>
    </author>
    <author>
      <name>Huazhong Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.02354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.02354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01407v2</id>
    <updated>2017-11-07T03:33:29Z</updated>
    <published>2017-11-04T07:11:22Z</published>
    <title>Timing Aware Dummy Metal Fill Methodology</title>
    <summary>  In this paper, we analyzed parasitic coupling capacitance coming from dummy
metal fill and its impact on timing. Based on the modeling, we proposed two
approaches to minimize the timing impact from dummy metal fill. The first
approach applies more spacing between critical nets and metal fill, while the
second approach leverages the shielding effects of reference nets. Experimental
results show consistent improvement compared to traditional metal fill method.
</summary>
    <author>
      <name>Luis Charre</name>
    </author>
    <author>
      <name>Bruno Gravano</name>
    </author>
    <author>
      <name>Rémi Pôssas</name>
    </author>
    <author>
      <name>Chen Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.01407v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01407v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04172v1</id>
    <updated>2017-11-11T17:06:56Z</updated>
    <published>2017-11-11T17:06:56Z</published>
    <title>Depth First Always On Routing Trace Algorithm</title>
    <summary>  In this paper, we discussed current limitation in the
electronic-design-automotation (EDA) tool on tracing the always on routing. We
developed an algorithm to efficiently track the secondary power routing and
accurately estimate the routing quality using approximate voltage drop as the
criteria. The fast check can identify potential hotspot issues without going
through sign-off checks. It helps designers to capture issues at early stages
and fix the issues with less design effort. We also discussed some limitations
to our algorithm.
</summary>
    <author>
      <name>Anthony Kim</name>
    </author>
    <author>
      <name>Sung Hyun Chen</name>
    </author>
    <author>
      <name>Chen Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.04172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.09357v2</id>
    <updated>2019-08-19T17:48:47Z</updated>
    <published>2018-12-21T20:05:42Z</published>
    <title>A Complexity Reduction Method for Successive Cancellation List Decoding</title>
    <summary>  This brief introduces a hardware complexity reduction method for successive
cancellation list (SCL) decoders. Specifically, we propose to use a sorting
scheme so that L paths with smallest path metrics are also sorted according to
their path indexes for path pruning. We prove that such sorting scheme reduces
the input number of multiplexers in any hardware implementation of SCL decoding
from L to (L/2+1) without any changes in the decoding latency. We also propose
sorter architectures for the proposed sorting method. Field programmable gate
array (FPGA) implementations show that the proposed method achieves significant
gain in hardware consumptions of SCL decoder implementations, especially for
large list sizes and block lengths.
</summary>
    <author>
      <name>Onur Dizdar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.09357v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.09357v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.03468v1</id>
    <updated>2020-09-08T00:42:53Z</updated>
    <published>2020-09-08T00:42:53Z</published>
    <title>Quad-Core RSA Processor with Countermeasure Against Power Analysis
  Attacks</title>
    <summary>  Rivest-Shamir-Adleman (RSA) cryptosystem uses modular multiplication for
encryption and decryption. So, performance of RSA can be drastically improved
by optimizing modular multiplication. This paper proposes a new parallel,
high-radix Montgomery multiplier for 1024 bits multi-core RSA processor. Each
computation step operates in radix 4. The computation speed is increased by
more than 4 times. We also implement a True Random Number Generator based
resilience block to protect the coprocessor against power attacks.
</summary>
    <author>
      <name>Javad Bagherzadeh</name>
    </author>
    <author>
      <name>Vishishtha Bothra</name>
    </author>
    <author>
      <name>Disha Gujar</name>
    </author>
    <author>
      <name>Sugandha Gupta</name>
    </author>
    <author>
      <name>Jinal Shah</name>
    </author>
    <link href="http://arxiv.org/abs/2009.03468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.03468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.05329v1</id>
    <updated>2020-09-11T10:35:34Z</updated>
    <published>2020-09-11T10:35:34Z</published>
    <title>DMR-based Technique for Fault Tolerant AES S-box Architecture</title>
    <summary>  This paper presents a high-throughput fault-resilient hardware implementation
of AES S-box, called HFS-box. If a transient natural or even malicious fault in
each pipeline stage is detected, the corresponding error signal becomes high
and as a result, the control unit holds the output of our proposed DMR voter
till the fault effect disappears. The proposed low-cost HFS-box provides a high
capability of fault-tolerant against transient faults with any duration by
putting low area overhead, i.e. 137%, and low throughput degradation, i.e.
11.3%, on the original implementation.
</summary>
    <author>
      <name>Mahdi Taheri</name>
    </author>
    <author>
      <name>Saeideh Sheikhpour</name>
    </author>
    <author>
      <name>Mohammad Saeed Ansari</name>
    </author>
    <author>
      <name>Ali Mahani</name>
    </author>
    <link href="http://arxiv.org/abs/2009.05329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.05329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.09077v1</id>
    <updated>2020-09-18T21:04:59Z</updated>
    <published>2020-09-18T21:04:59Z</published>
    <title>Open-Source Synthesizable Analog Blocks for High-Speed Link Designs:
  20-GS/s 5b ENOB Analog-to-Digital Converter and 5-GHz Phase Interpolator</title>
    <summary>  Using digital standard cells and digital place-and-route (PnR) tools, we
created a 20 GS/s, 8-bit analog-to-digital converter (ADC) for use in
high-speed serial link applications with an ENOB of 5.6, a DNL of 0.96 LSB, and
an INL of 2.39 LSB, which dissipated 175 mW in 0.102 mm2 in a 16nm technology.
The design is entirely described by HDL so that it can be ported to other
processes with minimal effort and shared as open source.
</summary>
    <author>
      <name>Sung-Jin Kim</name>
    </author>
    <author>
      <name>Zachary Myers</name>
    </author>
    <author>
      <name>Steven Herbst</name>
    </author>
    <author>
      <name>ByongChan Lim</name>
    </author>
    <author>
      <name>Mark Horowitz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VLSICircuits18222.2020.9162800</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VLSICircuits18222.2020.9162800" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2020 IEEE Symposium on VLSI Circuits</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.09077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.13959v1</id>
    <updated>2020-11-27T19:05:49Z</updated>
    <published>2020-11-27T19:05:49Z</published>
    <title>Design Methodologies for Reliable and Energy-efficient PCM Systems</title>
    <summary>  Phase-change memory (PCM) is a scalable and low latency non-volatile memory
(NVM) technology that has been proposed to serve as storage class memory (SCM),
providing low access latency similar to DRAM and often approaching or exceeding
the capacity of SSD. The multilevel property of PCM also enables its adoption
in neuromorphic systems to build high-density synaptic storage. We investigate
and describe two significant bottlenecks of a PCM system. First, writing to PCM
cells incurs significantly higher latency and energy penalties than reading its
content. Second, high operating voltages of PCM impacts its reliable
operations. In this work, we propose methodologies to tackle the bottlenecks,
improving performance, reliability, energy consumption, and sustainability for
a PCM system.
</summary>
    <author>
      <name>Shihao Song</name>
    </author>
    <author>
      <name>Anup Das</name>
    </author>
    <link href="http://arxiv.org/abs/2011.13959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.13959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.00818v1</id>
    <updated>2021-01-27T18:14:02Z</updated>
    <published>2021-01-27T18:14:02Z</published>
    <title>Proceedings of the DATE Friday Workshop on System-level Design Methods
  for Deep Learning on Heterogeneous Architectures (SLOHA 2021)</title>
    <summary>  This volume contains the papers accepted at the first DATE Friday Workshop on
System-level Design Methods for Deep Learning on Heterogeneous Architectures
(SLOHA 2021), held virtually on February 5, 2021. SLOHA 2021 was co-located
with the Conference on Design, Automation and Test in Europe (DATE).
</summary>
    <author>
      <name>Frank Hannig</name>
    </author>
    <author>
      <name>Paolo Meloni</name>
    </author>
    <author>
      <name>Matteo Spallanzani</name>
    </author>
    <author>
      <name>Matthias Ziegler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Website of the workshop: https://www12.cs.fau.de/ws/sloha2021/</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.00818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.00818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.07169v1</id>
    <updated>2021-07-15T07:36:01Z</updated>
    <published>2021-07-15T07:36:01Z</published>
    <title>Arrow: A RISC-V Vector Accelerator for Machine Learning Inference</title>
    <summary>  In this paper we present Arrow, a configurable hardware accelerator
architecture that implements a subset of the RISC-V v0.9 vector ISA extension
aimed at edge machine learning inference. Our experimental results show that an
Arrow co-processor can execute a suite of vector and matrix benchmarks
fundamental to machine learning inference 2 - 78x faster than a scalar RISC
processor while consuming 20% - 99% less energy when implemented in a Xilinx
XC7A200T-1SBG484C FPGA.
</summary>
    <author>
      <name>Imad Al Assir</name>
    </author>
    <author>
      <name>Mohamad El Iskandarani</name>
    </author>
    <author>
      <name>Hadi Rayan Al Sandid</name>
    </author>
    <author>
      <name>Mazen A. R. Saghir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the Fifth Workshop on Computer Architecture Research
  with RISC-V (CARRV 2021), co-located with ISCA 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.07169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.07169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.5.1; C.1.4; C.3; C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.08997v1</id>
    <updated>2021-07-19T16:17:50Z</updated>
    <published>2021-07-19T16:17:50Z</published>
    <title>Dynamic Lockstep Processors for Applications with Functional Safety
  Relevance</title>
    <summary>  Lockstep processing is a recognized technique for helping to secure
functional-safety relevant processing against, for instance, single upset
errors that might cause faulty execution of code. Lockstepping processors does
however bind processing resources in a fashion not beneficial to architectures
and applications that would benefit from multi-core/-processors. We propose a
novel on-demand synchronizing of cores/processors for lock-step operation
featuring post-processing resource release, a concept that facilitates the
implementation of modularly redundant core/processor arrays. We discuss the
fundamentals of the design and some implementation notes on work achieved to
date.
</summary>
    <author>
      <name>Hans Dermot Doran</name>
    </author>
    <author>
      <name>Timo Lang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.08997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.08997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.04562v1</id>
    <updated>2021-12-28T02:06:43Z</updated>
    <published>2021-12-28T02:06:43Z</published>
    <title>Reduced Softmax Unit for Deep Neural Network Accelerators</title>
    <summary>  The Softmax activation layer is a very popular Deep Neural Network (DNN)
component when dealing with multi-class prediction problems. However, in DNN
accelerator implementations it creates additional complexities due to the need
for computation of the exponential for each of its inputs. In this brief we
propose a simplified version of the activation unit for accelerators, where
only a comparator unit produces the classification result, by choosing the
maximum among its inputs. Due to the nature of the activation function, we show
that this result is always identical to the classification produced by the
Softmax layer.
</summary>
    <author>
      <name>Raghuram S</name>
    </author>
    <link href="http://arxiv.org/abs/2201.04562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.04562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.11978v1</id>
    <updated>2022-01-28T08:00:40Z</updated>
    <published>2022-01-28T08:00:40Z</published>
    <title>Testable Array Multipliers for a Better Utilization of C-Testability and
  Bijectivity</title>
    <summary>  This paper presents a design for test (DFT)architecture for fast and scalable
testing of array multipliers (MULTs). Regardless of the MULT size, our proposed
testable architecture, without major changes in the original architecture,
requires only five test vectors. Test pattern generation (TPG) is done by
combining C-testability, bijectivity and deterministic TPG methods.
Experimental results show 100% fault coverage for single stuck-at faults. The
proposed method requires minor testability hardware insertion into the
multiplier with extra delay and area overhead of less than 0.5% for a 64-bit
multiplier.
</summary>
    <author>
      <name>Fatemeh Sheikh Shoaei</name>
    </author>
    <author>
      <name>Alireza Nahvy</name>
    </author>
    <author>
      <name>Zainalabedin Navabi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.11978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.11978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.00779v1</id>
    <updated>2022-05-02T09:57:17Z</updated>
    <published>2022-05-02T09:57:17Z</published>
    <title>Zebra: Memory Bandwidth Reduction for CNN Accelerators With Zero Block
  Regularization of Activation Maps</title>
    <summary>  The large amount of memory bandwidth between local buffer and external DRAM
has become the speedup bottleneck of CNN hardware accelerators, especially for
activation maps. To reduce memory bandwidth, we propose to learn pruning
unimportant blocks dynamically with zero block regularization of activation
maps (Zebra). This strategy has low computational overhead and could easily
integrate with other pruning methods for better performance. The experimental
results show that the proposed method can reduce 70\% of memory bandwidth for
Resnet-18 on Tiny-Imagenet within 1\% accuracy drops and 2\% accuracy gain with
the combination of Network Slimming.
</summary>
    <author>
      <name>Hsu-Tung Shih</name>
    </author>
    <author>
      <name>Tian-Sheuan Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, published in IEEE ISCAS 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.00779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.00779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.02103v1</id>
    <updated>2022-05-02T10:05:03Z</updated>
    <published>2022-05-02T10:05:03Z</published>
    <title>Efficient Accelerator for Dilated and Transposed Convolution with
  Decomposition</title>
    <summary>  Hardware acceleration for dilated and transposed convolution enables real
time execution of related tasks like segmentation, but current designs are
specific for these convolutional types or suffer from complex control for
reconfigurable designs. This paper presents a design that decomposes input or
weight for dilated and transposed convolutions respectively to skip redundant
computations and thus executes efficiently on existing dense CNN hardware as
well. The proposed architecture can cut down 87.8\% of the cycle counts to
achieve 8.2X speedup over a naive execution for the ENet case.
</summary>
    <author>
      <name>Kuo-Wei Chang</name>
    </author>
    <author>
      <name>Tian-Sheuan Chang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISCAS45731.2020.9180402</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISCAS45731.2020.9180402" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 12 figures, published in IEEE ISCAS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.02103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.02103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.02271v1</id>
    <updated>2022-05-02T10:17:37Z</updated>
    <published>2022-05-02T10:17:37Z</published>
    <title>VSCNN: Convolution Neural Network Accelerator With Vector Sparsity</title>
    <summary>  Hardware accelerator for convolution neural network (CNNs) enables real time
applications of artificial intelligence technology. However, most of the
accelerators only support dense CNN computations or suffers complex control to
support fine grained sparse networks. To solve above problem, this paper
presents an efficient CNN accelerator with 1-D vector broadcasted input to
support both dense network as well as vector sparse network with the same
hardware and low overhead. The presented design achieves 1.93X speedup over the
dense CNN computations.
</summary>
    <author>
      <name>Kuo-Wei Chang</name>
    </author>
    <author>
      <name>Tian-Sheuan Chang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISCAS.2019.8702471</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISCAS.2019.8702471" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 13 figures, published in IEEE ISCAS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.02271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.02271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.03997v1</id>
    <updated>2022-05-09T01:47:02Z</updated>
    <published>2022-05-09T01:47:02Z</published>
    <title>A Real Time Super Resolution Accelerator with Tilted Layer Fusion</title>
    <summary>  Deep learning based superresolution achieves high-quality results, but its
heavy computational workload, large buffer, and high external memory bandwidth
inhibit its usage in mobile devices. To solve the above issues, this paper
proposes a real-time hardware accelerator with the tilted layer fusion method
that reduces the external DRAM bandwidth by 92\% and just needs 102KB on-chip
memory. The design implemented with a 40nm CMOS process achieves
1920x1080@60fps throughput with 544.3K gate count when running at 600MHz; it
has higher throughput and lower area cost than previous designs.
</summary>
    <author>
      <name>An-Jung Huang</name>
    </author>
    <author>
      <name>Kai-Chieh Hsu</name>
    </author>
    <author>
      <name>Tian-Sheuan Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures, published in ISCAS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.03997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.03997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.07975v1</id>
    <updated>2022-05-11T08:21:34Z</updated>
    <published>2022-05-11T08:21:34Z</published>
    <title>Key-Value Stores on Flash Storage Devices: A Survey</title>
    <summary>  Key-value stores (KV) have become one of the main components of the modern
storage and data processing system stack. With the increasing need for timely
data analysis, performance becomes more and more critical. In the past, these
stores were frequently optimised to run on HDD and DRAM devices. However, the
last decade saw an increased interest in the use of flash devices because of
their attractive properties. Flash is cheaper than DRAM and yet has a lower
latency and higher throughput than HDDs. This literature survey aims to
highlight the changes proposed in the last decade to optimise key-value stores
for flash devices and predict what role these devices might play for key-value
stores in the future.
</summary>
    <author>
      <name>Krijn Doekemeijer</name>
    </author>
    <author>
      <name>Animesh Trivedi</name>
    </author>
    <link href="http://arxiv.org/abs/2205.07975v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.07975v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.4; A.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.09504v1</id>
    <updated>2022-05-19T12:21:11Z</updated>
    <published>2022-05-19T12:21:11Z</published>
    <title>Automatic Generation of Complete Polynomial Interpolation Hardware
  Design Space</title>
    <summary>  Hardware implementations of complex functions regularly deploy piecewise
polynomial approximations. This work determines the complete design space of
piecewise polynomial approximations meeting a given accuracy specification.
Knowledge of this design space determines the minimum number of regions
required to approximate the function accurately enough and facilitates the
generation of optimized hardware which is competitive against the state of the
art. Targeting alternative hardware technologies simply requires a modified
decision procedure to explore the space.
</summary>
    <author>
      <name>Bryce Orloski</name>
    </author>
    <author>
      <name>Samuel Coward</name>
    </author>
    <author>
      <name>Theo Drane</name>
    </author>
    <link href="http://arxiv.org/abs/2205.09504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.09504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.11244v1</id>
    <updated>2022-05-17T03:26:14Z</updated>
    <published>2022-05-17T03:26:14Z</published>
    <title>A Silicon Photonic Accelerator for Convolutional Neural Networks with
  Heterogeneous Quantization</title>
    <summary>  Parameter quantization in convolutional neural networks (CNNs) can help
generate efficient models with lower memory footprint and computational
complexity. But, homogeneous quantization can result in significant degradation
of CNN model accuracy. In contrast, heterogeneous quantization represents a
promising approach to realize compact, quantized models with higher inference
accuracies. In this paper, we propose HQNNA, a CNN accelerator based on
non-coherent silicon photonics that can accelerate both homogeneously quantized
and heterogeneously quantized CNN models. Our analyses show that HQNNA achieves
up to 73.8x better energy-per-bit and 159.5x better throughput-energy
efficiency than state-of-the-art photonic CNN accelerators.
</summary>
    <author>
      <name>Febin Sunny</name>
    </author>
    <author>
      <name>Mahdi Nikdast</name>
    </author>
    <author>
      <name>Sudeep Pasricha</name>
    </author>
    <link href="http://arxiv.org/abs/2205.11244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.11244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.06769v1</id>
    <updated>2022-06-14T12:02:59Z</updated>
    <published>2022-06-14T12:02:59Z</published>
    <title>Muntjac -- Open Source Multicore RV64 Linux-capable SoC</title>
    <summary>  Muntjac is an open-source collection of components which can be used to build
a multicore, Linux-capable system-on-chip. This includes a 64-bit RISC-V core,
a cache subsystem, and TileLink interconnect allowing cache-coherent multicore
configurations. Each component is easy to understand, verify, and extend, with
most being configurable enough to be useful across a wide range of
applications.
</summary>
    <author>
      <name>Xuan Guo</name>
    </author>
    <author>
      <name>Daniel Bates</name>
    </author>
    <author>
      <name>Robert Mullins</name>
    </author>
    <author>
      <name>Alex Bradbury</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in the First Workshop on Open-Source Computer
  Architecture Research</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.06769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.06769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.07308v1</id>
    <updated>2022-06-15T05:35:42Z</updated>
    <published>2022-06-15T05:35:42Z</published>
    <title>Cost-Aware Exploration for Chiplet-Based Architecture with Advanced
  Packaging Technologies</title>
    <summary>  The chiplet-based System-in-Package~(SiP) technology enables more design
flexibility via various inter-chiplet connection and heterogeneous integration.
However, it is not known how to convert such flexibility into cost efficiency,
which is critical when making a design decision. In this paper, we develop an
analytical cost model that can estimate the cost of the 2.5D chiplet-based SiP
systems under various interconnection options and technology nodes. We
conducted two case studies using our cost model to explore the cost
characteristics of the 2.5D chiplet-based SiP system. Based on the case
studies, we made several observations on the interposer selection, design
partition granularity, and technology node adoption for cost-efficient
chiplet-based SiP design.
</summary>
    <author>
      <name>Tianqi Tang</name>
    </author>
    <author>
      <name>Yuan Xie</name>
    </author>
    <link href="http://arxiv.org/abs/2206.07308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.07308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.01401v1</id>
    <updated>2022-07-04T13:31:59Z</updated>
    <published>2022-07-04T13:31:59Z</published>
    <title>Two New CNTFET Quaternary Full Adders for Carry-Propagate Adders</title>
    <summary>  In Carry Propagate Adders, carry propagation is the critical delay. For the
1-digit adders that they use, the most efficient scheme is to generate two
intermediate carries: C$_{out0}$ ($C_{in}$=0) and $C_{out1}$($C_{in}$=1). Then
multiplex them to produce the correct output according to $C_{in}$. For any
radix, the carry output has always a logical value 0 or 1. We show that using 0
and $V_{dd}$ levels for input and output carries instead of 0 and $V_{dd}$/3 in
quaternary full adders significantly reduce the carry propagation. We compare
such a quaternary full adder with binary full adders to implement N-digit carry
propagate adders.
</summary>
    <author>
      <name>Daniel Etiemble</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.01401v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.01401v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.0; B.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.04839v1</id>
    <updated>2022-07-11T13:14:09Z</updated>
    <published>2022-07-11T13:14:09Z</published>
    <title>Ternary and Quaternary CNTFET Full Adders are less efficient than the
  Binary Ones for Carry-Propagate Adders</title>
    <summary>  In Carry Propagate Adders, carry propagation is the critical delay. The most
efficient scheme is to generate Cout0 (Cin=0) and Cout1(Cin=1) and multiplex
the correct output according to Cin. For any radix, the carry output is always
0/1. We present two versions of ternary adders with Cin = (0V, Vdd/2) and Cin =
(0V, Vdd) and two versions of quaternary adders with Cin = (0V, Vdd/3) and Cin
= (0V, Vdd). Using full swing Vdd for Cin reduces the propagation delays for
ternary and quaternary adders. 6-bit, 4-trit and 3-quit CPAs are then compared.
</summary>
    <author>
      <name>Daniel Etiemble</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 39 figures. arXiv admin note: text overlap with
  arXiv:2207.01401</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.04839v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.04839v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.0; B.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.09070v1</id>
    <updated>2022-08-18T21:14:34Z</updated>
    <published>2022-08-18T21:14:34Z</published>
    <title>Electronic, Wireless, and Photonic Network-on-Chip Security: Challenges
  and Countermeasures</title>
    <summary>  Networks-on-chips (NoCs) are an integral part of emerging manycore computing
chips. They play a key role in facilitating communication among processing
cores and between cores and memory. To meet the aggressive performance and
energy-efficiency targets of machine learning and big data applications, NoCs
have been evolving to leverage emerging paradigms such as silicon photonics and
wireless communication. Increasingly, these NoC fabrics are becoming
susceptible to security vulnerabilities, such as from hardware trojans that can
snoop, corrupt, or disrupt information transfers on NoCs. This article surveys
the landscape of security challenges and countermeasures across electronic,
wireless, and photonic NoCs.
</summary>
    <author>
      <name>Sudeep Pasricha</name>
    </author>
    <author>
      <name>John Jose</name>
    </author>
    <author>
      <name>Sujay Deb</name>
    </author>
    <link href="http://arxiv.org/abs/2208.09070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.09070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.14756v1</id>
    <updated>2022-09-28T11:39:14Z</updated>
    <published>2022-09-28T11:39:14Z</published>
    <title>Unveiling the Real Performance of LPDDR5 Memories</title>
    <summary>  LPDDR5 is the latest low-power DRAM standard and expected to be used in
various application fields. The vendors have published promising peak
bandwidths up to 50 % higher than those of the predecessor LPDDR4. In this
paper we evaluate the best-case and worst-case real bandwidth utilization of
different LPDDR5 configurations and compare the results to corresponding LPDDR4
configurations. We also show that an upgrade from LPDDR4 to LPDDR5 does not
always bring a bandwidth advantage and that some LPDDR5 configurations should
be avoided for specific workloads.
</summary>
    <author>
      <name>Lukas Steiner</name>
    </author>
    <author>
      <name>Matthias Jung</name>
    </author>
    <author>
      <name>Michael Huonker</name>
    </author>
    <author>
      <name>Norbert Wehn</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3565053.3565062</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3565053.3565062" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM/IEEE International Symposium on Memory Systems (MEMSYS 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.14756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.14756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.04683v1</id>
    <updated>2022-10-10T13:32:23Z</updated>
    <published>2022-10-10T13:32:23Z</published>
    <title>End-to-End QoS for the Open Source Safety-Relevant RISC-V SELENE
  Platform</title>
    <summary>  This paper presents the end-to-end QoS approach to provide performance
guarantees followed in the SELENE platform, a high-performance RISC-V based
heterogeneous SoC for safety-related real-time systems. Our QoS approach
includes smart interconnect solutions for buses and NoCs, along with multicore
interference-aware statistics units to, cooperatively, achieve end-to-end QoS.
</summary>
    <author>
      <name>Pablo Andreu</name>
    </author>
    <author>
      <name>Carles Hernandez</name>
    </author>
    <author>
      <name>Tomas Picornell</name>
    </author>
    <author>
      <name>Pedro Lopez</name>
    </author>
    <author>
      <name>Sergi Alcaide</name>
    </author>
    <author>
      <name>Francisco Bas</name>
    </author>
    <author>
      <name>Pedro Benedicte</name>
    </author>
    <author>
      <name>Guillem Cabo</name>
    </author>
    <author>
      <name>Feng Chang</name>
    </author>
    <author>
      <name>Francisco Fuentes</name>
    </author>
    <author>
      <name>Jaume Abella</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, work presented on FORECAST workshop of HIPEAC
  2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.04683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.04683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.13107v1</id>
    <updated>2022-10-24T10:46:58Z</updated>
    <published>2022-10-24T10:46:58Z</published>
    <title>An Analytical Estimation of Spiking Neural Networks Energy Efficiency</title>
    <summary>  Spiking Neural Networks are a type of neural networks where neurons
communicate using only spikes. They are often presented as a low-power
alternative to classical neural networks, but few works have proven these
claims to be true. In this work, we present a metric to estimate the energy
consumption of SNNs independently of a specific hardware. We then apply this
metric on SNNs processing three different data types (static, dynamic and
event-based) representative of real-world applications. As a result, all of our
SNNs are 6 to 8 times more efficient than their FNN counterparts.
</summary>
    <author>
      <name>Edgar Lemaire</name>
    </author>
    <author>
      <name>Loic Cordone</name>
    </author>
    <author>
      <name>Andrea Castagnetti</name>
    </author>
    <author>
      <name>Pierre-Emmanuel Novac</name>
    </author>
    <author>
      <name>Jonathan Courtois</name>
    </author>
    <author>
      <name>Benoit Miramond</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-031-30105-6_48</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-031-30105-6_48" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for ICONIP 2022 Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.13107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.13107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.16471v1</id>
    <updated>2022-10-29T02:54:03Z</updated>
    <published>2022-10-29T02:54:03Z</published>
    <title>Fast Efficient Fixed-Size Memory Pool: No Loops and No Overhead</title>
    <summary>  In this paper, we examine a ready-to-use, robust, and computationally fast
fixed-size memory pool manager with no-loops and no-memory overhead that is
highly suited towards time-critical systems such as games. The algorithm
achieves this by exploiting the unused memory slots for bookkeeping in
combination with a trouble-free indexing scheme. We explain how it works in
amalgamation with straightforward step-by-step examples. Furthermore, we
compare just how much faster the memory pool manager is when compared with a
system allocator (e.g., malloc) over a range of allocations and sizes.
</summary>
    <author>
      <name>Ben Kenwright</name>
    </author>
    <link href="http://arxiv.org/abs/2210.16471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.16471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.04455v2</id>
    <updated>2022-11-28T17:48:25Z</updated>
    <published>2022-11-08T18:48:42Z</published>
    <title>Microprocessor Design with Dynamic Clock Source and Multi-Width
  Instructions</title>
    <summary>  This paper introduces a novel 32-bit microprocessor, based on the RISC-V
instruction set architecture, is designed,utilising a dynamic clock source to
achieve high efficiency, overcoming the limitations of hardware delays. In
addition, the microprocessor is also aimed to operate with both base (32-bit)
instructions and 16-bit compressed instructions. The testing of the design is
carried out using ModelSim with an ideal result.
</summary>
    <author>
      <name>Keyu Chen</name>
    </author>
    <author>
      <name>Xuyi Hu</name>
    </author>
    <author>
      <name>Robert Killey</name>
    </author>
    <link href="http://arxiv.org/abs/2211.04455v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.04455v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.11331v1</id>
    <updated>2022-11-21T10:33:13Z</updated>
    <published>2022-11-21T10:33:13Z</published>
    <title>BrainTTA: A 35 fJ/op Compiler Programmable Mixed-Precision
  Transport-Triggered NN SoC</title>
    <summary>  Recently, accelerators for extremely quantized deep neural network (DNN)
inference with operand widths as low as 1-bit have gained popularity due to
their ability to largely cut down energy cost per inference. In this paper, a
flexible SoC with mixed-precision support is presented. Contrary to the current
trend of fixed-datapath accelerators, this architecture makes use of a flexible
datapath based on a Transport-Triggered Architecture (TTA). The architecture is
fully programmable using C. The accelerator has a peak energy efficiency of
35/67/405 fJ/op (binary, ternary, and 8-bit precision) and a throughput of
614/307/77 GOPS, which is unprecedented for a programmable architecture.
</summary>
    <author>
      <name>Maarten Molendijk</name>
    </author>
    <author>
      <name>Floran de Putter</name>
    </author>
    <author>
      <name>Manil Gomony</name>
    </author>
    <author>
      <name>Pekka Jääskeläinen</name>
    </author>
    <author>
      <name>Henk Corporaal</name>
    </author>
    <link href="http://arxiv.org/abs/2211.11331v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.11331v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.11505v1</id>
    <updated>2023-01-27T02:48:21Z</updated>
    <published>2023-01-27T02:48:21Z</published>
    <title>Design of an FPGA-based USB 3.0 device controller</title>
    <summary>  The traditional USB 3.0 communication based on FPGA uses an external chip as
a USB PHY or a USB controller including a USB PHY. This paper realizes a USB
3.0 controller by using FPGA resources, in which FPGA logic realizes a serial
interface engine, and an FPGA internal transceiver is as a USB PHY. Used slices
percent after implementation is 4.59% in Kintex-7 325t. The test result shows
that the speed of USB 3.0 is more than 320 MB/s bulk-in and bulk-out transfers.
</summary>
    <author>
      <name>Zhe Ning</name>
    </author>
    <author>
      <name>Yunhua Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.11505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.11505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.06417v1</id>
    <updated>2023-01-13T21:04:16Z</updated>
    <published>2023-01-13T21:04:16Z</published>
    <title>Analog, In-memory Compute Architectures for Artificial Intelligence</title>
    <summary>  This paper presents an analysis of the fundamental limits on energy
efficiency in both digital and analog in-memory computing architectures, and
compares their performance to single instruction, single data (scalar) machines
specifically in the context of machine inference. The focus of the analysis is
on how efficiency scales with the size, arithmetic intensity, and bit precision
of the computation to be performed. It is shown that analog, in-memory
computing architectures can approach arbitrarily high energy efficiency as both
the problem size and processor size scales.
</summary>
    <author>
      <name>Patrick Bowen</name>
    </author>
    <author>
      <name>Guy Regev</name>
    </author>
    <author>
      <name>Nir Regev</name>
    </author>
    <author>
      <name>Bruno Pedroni</name>
    </author>
    <author>
      <name>Edward Hanson</name>
    </author>
    <author>
      <name>Yiran Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.06417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.06417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.09883v1</id>
    <updated>2023-02-20T10:17:59Z</updated>
    <published>2023-02-20T10:17:59Z</published>
    <title>Reducing the memory usage of Lattice-Boltzmann schemes with a DWT-based
  compression</title>
    <summary>  This paper presents a new solution to address the challenge of increasing
memory usage in high-performance computing simulations of Lattice-Bolzmann or
Finite-Volume schemes.Our approach utilises a lossy compression scheme based on
the Discrete Wavelet Transform (DWT) to achieve high compression ratios while
preserving the accuracy of the simulation.Our evaluation on two different
FV/LBM schemes demonstrates that the approach can reduce memory usage by
several orders of magnitude.
</summary>
    <author>
      <name>Clément Flint</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRMA, ICube, CAMUS</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Helluy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRMA, TONUS, UNISTRA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2302.09883v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.09883v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.01839v1</id>
    <updated>2023-03-03T10:46:19Z</updated>
    <published>2023-03-03T10:46:19Z</published>
    <title>Automating Constraint-Aware Datapath Optimization using E-Graphs</title>
    <summary>  Numerical hardware design requires aggressive optimization, where designers
exploit branch constraints, creating optimization opportunities that are valid
only on a sub-domain of input space. We developed an RTL optimization tool that
automatically learns the consequences of conditional branches and exploits that
knowledge to enable deep optimization. The tool deploys custom built program
analysis based on abstract interpretation theory, which when combined with a
data-structure known as an e-graph simplifies complex reasoning about program
properties. Our tool fully-automatically discovers known floating-point
architectures from the computer arithmetic literature and out-performs baseline
EDA tools, generating up to 33% faster and 41% smaller circuits.
</summary>
    <author>
      <name>Samuel Coward</name>
    </author>
    <author>
      <name>George A. Constantinides</name>
    </author>
    <author>
      <name>Theo Drane</name>
    </author>
    <link href="http://arxiv.org/abs/2303.01839v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.01839v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.07405v1</id>
    <updated>2023-03-07T17:43:20Z</updated>
    <published>2023-03-07T17:43:20Z</published>
    <title>Word-Level Structure Identification In FPGA Designs Using Cell Proximity
  Information</title>
    <summary>  Reverse engineering of FPGA based designs from the flattened LUT level
netlist to high level RTL helps in verification of the design or in
understanding legacy designs. We focus on flattened netlists for FPGA devices
from Xilinx 7 series and Zynq 7000. We propose a design element grouping
algorithm that makes use of the location information of the elements on the
physical device after place and route. The proposed grouping algorithm gives
clusters with average NMI of 0.73 for groupings including all element types.
The benchmarks chosen include a range of designs from communication, arithmetic
units, processors and DSP processing units.
</summary>
    <author>
      <name>Aparajithan Nathamuni-Venkatesan</name>
    </author>
    <author>
      <name>Ram-Venkat Narayanan</name>
    </author>
    <author>
      <name>Kishore Pula</name>
    </author>
    <author>
      <name>Sundarakumar Muthukumaran</name>
    </author>
    <author>
      <name>Ranga Vemuri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted into proceedings of VLSID2023 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.07405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.07405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.07406v1</id>
    <updated>2023-03-05T06:39:54Z</updated>
    <published>2023-03-05T06:39:54Z</published>
    <title>Infra-Red, In-Situ (IRIS) Inspection of Silicon</title>
    <summary>  This paper introduces the Infra-Red, In Situ (IRIS) inspection method, which
uses short-wave IR (SWIR) light to non-destructively "see through" the backside
of chips and image them with lightly modified conventional digital CMOS
cameras. With a ~1050 nm light source, IRIS is capable of constraining macro-
and meso-scale features of a chip. This hardens existing micro-scale self-test
verification techniques by ruling out the existence of extra circuitry that can
hide a hardware trojan with a test bypass. Thus, self-test techniques used in
conjunction with IRIS can ensure the correct construction of security-critical
hardware at all size scales.
</summary>
    <author>
      <name>Andrew 'bunnie' Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.07406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.07406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.10508v1</id>
    <updated>2023-03-18T22:00:50Z</updated>
    <published>2023-03-18T22:00:50Z</published>
    <title>Unraveling the Integration of Deep Machine Learning in FPGA CAD Flow: A
  Concise Survey and Future Insights</title>
    <summary>  This paper presents an overview of the integration of deep machine learning
(DL) in FPGA CAD design flow, focusing on high-level and logic synthesis,
placement, and routing. Our analysis identifies key research areas that require
more attention in FPGA CAD design, including the development of open-source
benchmarks optimized for end-to-end machine learning experiences and the
potential of knowledge-sharing among researchers and industry practitioners to
incorporate more intelligence in FPGA CAD decision-making steps. By providing
insights into the integration of deep machine learning in FPGA CAD flow, this
paper aims to inform future research directions in this exciting and rapidly
evolving field.
</summary>
    <author>
      <name>Behnam Ghavami</name>
    </author>
    <author>
      <name>Lesley Shannon</name>
    </author>
    <link href="http://arxiv.org/abs/2303.10508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.10508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.03079v1</id>
    <updated>2023-04-06T14:01:12Z</updated>
    <published>2023-04-06T14:01:12Z</published>
    <title>Spade: An Expression-Based HDL With Pipelines</title>
    <summary>  Spade is a new open source hardware description language (HDL) designed to
increase developer productivity without sacrificing the low-level control
offered by HDLs. It is a standalone language which takes inspiration from
modern software languages, and adds useful abstractions for common hardware
constructs. It also comes with a convenient set of tooling, such as a helpful
compiler, a build system with dependency management, tools for debugging, and
editor integration.
</summary>
    <author>
      <name>Frans Skarman</name>
    </author>
    <author>
      <name>Oscar Gustafsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 3rd Workshop on Open-Source Design Automation
  (OSDA), 2023 (arXiv:2303.18024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.03079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.03079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.12309v1</id>
    <updated>2023-04-06T21:27:52Z</updated>
    <published>2023-04-06T21:27:52Z</published>
    <title>Optimized Real-Time Assembly in a RISC Simulator</title>
    <summary>  Simulators for the RISC-V instruction set architecture (ISA) are useful for
teaching assembly language and modern CPU architecture concepts. The
Assembly/Simulation Platform for Illustration of RISC-V in Education (ASPIRE)
is an integrated RISC-V assembler and simulator used to illustrate these
concepts and evaluate algorithms to generate machine language code. In this
article, ASPIRE is introduced, selected features of the simulator that
interactively explain the RISC-V ISA as teaching aides are presented, then two
assembly algorithms are evaluated. Both assembly algorithms run in real time as
code is being edited in the simulator. The optimized algorithm performs
incremental assembly limited to only the portion of the program that is
changed. Both algorithms are then evaluated based on overall run-time
performance.
</summary>
    <author>
      <name>Marwan Shaban</name>
    </author>
    <author>
      <name>Adam J. Rocke</name>
    </author>
    <link href="http://arxiv.org/abs/2304.12309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.12309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.18328v1</id>
    <updated>2023-05-23T12:47:19Z</updated>
    <published>2023-05-23T12:47:19Z</published>
    <title>Open-Source GEMM Hardware Kernels Generator: Toward Numerically-Tailored
  Computations</title>
    <summary>  Many scientific computing problems can be reduced to Matrix-Matrix
Multiplications (MMM), making the General Matrix Multiply (GEMM) kernels in the
Basic Linear Algebra Subroutine (BLAS) of interest to the high-performance
computing community. However, these workloads have a wide range of numerical
requirements. Ill-conditioned linear systems require high-precision arithmetic
to ensure correct and reproducible results. In contrast, emerging workloads
such as deep neural networks, which can have millions up to billions of
parameters, have shown resilience to arithmetic tinkering and precision
lowering.
</summary>
    <author>
      <name>Louis Ledoux</name>
    </author>
    <author>
      <name>Marc Casas</name>
    </author>
    <link href="http://arxiv.org/abs/2305.18328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.18328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.19917v1</id>
    <updated>2023-05-31T14:51:08Z</updated>
    <published>2023-05-31T14:51:08Z</published>
    <title>ReDSEa: Automated Acceleration of Triangular Solver on Supercloud
  Heterogeneous Systems</title>
    <summary>  When utilized effectively, Supercloud heterogeneous systems have the
potential to significantly enhance performance. Our ReDSEa tool-chain automates
the mapping, load balancing, scheduling, parallelism, and overlapping processes
for the Triangular System Solver (TS) on a heterogeneous system consisting of a
Huawei Kunpeng ARM multi-core CPU and an Ascend 910 AI HW accelerator. We
propose an LLVM compiler tool-chain that a) leverages compiler analysis and b)
utilizes novel performance models exploring recursive, iterative, and blocked
computation models. Our tool-chain facilitates a speedup of up to 16x compared
to an optimized 48-core CPU-only implementation.
</summary>
    <author>
      <name>Georgios Zacharopoulos</name>
    </author>
    <author>
      <name>Ilias Bournias</name>
    </author>
    <author>
      <name>Verner Vlacic</name>
    </author>
    <author>
      <name>Lukas Cavigelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, SSH-S0C DAC 2023 Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.19917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.19917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.09552v1</id>
    <updated>2023-06-15T23:46:35Z</updated>
    <published>2023-06-15T23:46:35Z</published>
    <title>Retrospective: EIE: Efficient Inference Engine on Sparse and Compressed
  Neural Network</title>
    <summary>  EIE proposed to accelerate pruned and compressed neural networks, exploiting
weight sparsity, activation sparsity, and 4-bit weight-sharing in neural
network accelerators. Since published in ISCA'16, it opened a new design space
to accelerate pruned and sparse neural networks and spawned many
algorithm-hardware co-designs for model compression and acceleration, both in
academia and commercial AI chips. In retrospect, we review the background of
this project, summarize the pros and cons, and discuss new opportunities where
pruning, sparsity, and low precision can accelerate emerging deep learning
workloads.
</summary>
    <author>
      <name>Song Han</name>
    </author>
    <author>
      <name>Xingyu Liu</name>
    </author>
    <author>
      <name>Huizi Mao</name>
    </author>
    <author>
      <name>Jing Pu</name>
    </author>
    <author>
      <name>Ardavan Pedram</name>
    </author>
    <author>
      <name>Mark A. Horowitz</name>
    </author>
    <author>
      <name>William J. Dally</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invited retrospective paper at ISCA 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.09552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.09552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.03910v1</id>
    <updated>2023-07-08T06:02:12Z</updated>
    <published>2023-07-08T06:02:12Z</published>
    <title>A Survey of Spiking Neural Network Accelerator on FPGA</title>
    <summary>  Due to the ability to implement customized topology, FPGA is increasingly
used to deploy SNNs in both embedded and high-performance applications. In this
paper, we survey state-of-the-art SNN implementations and their applications on
FPGA. We collect the recent widely-used spiking neuron models, network
structures, and signal encoding formats, followed by the enumeration of related
hardware design schemes for FPGA-based SNN implementations. Compared with the
previous surveys, this manuscript enumerates the application instances that
applied the above-mentioned technical schemes in recent research. Based on
that, we discuss the actual acceleration potential of implementing SNN on FPGA.
According to our above discussion, the upcoming trends are discussed in this
paper and give a guideline for further advancement in related subjects.
</summary>
    <author>
      <name>Murat Isik</name>
    </author>
    <link href="http://arxiv.org/abs/2307.03910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.03910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.14252v1</id>
    <updated>2023-08-28T02:03:26Z</updated>
    <published>2023-08-28T02:03:26Z</published>
    <title>Key technologies and application for radar and smart video fusion in
  perimeter intrusion alarm system</title>
    <summary>  With the continuous development of modern science and technology, radar
detection, video surveillance and perimeter alarm system are more and more
widely used in the field of social security. This paper introduces video
surveillance and perimeter alarm in detail, mathematical modeling and key
technologies, analyzes their fusion and application status, and puts forward
suggestions combined with the development trend of intelligent security system
in the future.
</summary>
    <author>
      <name>Shujun Fu</name>
    </author>
    <author>
      <name>Shenghai Liao</name>
    </author>
    <author>
      <name>Jingjing Gao</name>
    </author>
    <author>
      <name>Shijing Song</name>
    </author>
    <author>
      <name>Zhonghua Man</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.14252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.14252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.03214v1</id>
    <updated>2023-08-30T03:18:34Z</updated>
    <published>2023-08-30T03:18:34Z</published>
    <title>Test Primitive:A Straightforward Method To Decouple March</title>
    <summary>  The academic community has made outstanding achievements in researching the
March algorithm. However, the current fault modeling method, which centers on
fault primitives, cannot be directly applied to analyzing the March algorithm.
This paper proposes a new test primitive. The test primitives, which decouple
the cell states from sensitization and detection operations, describe the
common features that must be possessed for the March algorithm to detect
corresponding faults, forming a highly flexible and scalable March algorithm
analysis unit. The theoretical analysis proves that the test primitives
demonstrate completeness, uniqueness, and conciseness. On this foundation, the
utilization of test primitives within the March analysis procedure is
elucidated.
</summary>
    <author>
      <name>Yindong Xiao</name>
    </author>
    <author>
      <name>Shanshan Lu</name>
    </author>
    <author>
      <name>Ensheng Wang</name>
    </author>
    <author>
      <name>Ruiqi Zhu</name>
    </author>
    <author>
      <name>Zhijian Dai</name>
    </author>
    <link href="http://arxiv.org/abs/2309.03214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.03214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.07077v1</id>
    <updated>2023-09-12T05:54:50Z</updated>
    <published>2023-09-12T05:54:50Z</published>
    <title>Optimized Implementation of Neuromorphic HATS Algorithm on FPGA</title>
    <summary>  In this paper, we present first-ever optimized hardware implementation of a
state-of-the-art neuromorphic approach Histogram of Averaged Time Surfaces
(HATS) algorithm to event-based object classification in FPGA for asynchronous
time-based image sensors (ATIS). Our Implementation achieves latency of 3.3 ms
for the N-CARS dataset samples and is capable of processing 2.94 Mevts/s.
Speed-up is achieved by using parallelism in the design and multiple Processing
Elements can be added. As development platform, Zynq-7000 SoC from Xilinx is
used. The tradeoff between Average Absolute Error and Resource Utilization for
fixed precision implementation is analyzed and presented. The proposed FPGA
implementation is $\sim$ 32 x power efficient compared to software
implementation.
</summary>
    <author>
      <name>Khushal Sethi</name>
    </author>
    <author>
      <name>Manan Suri</name>
    </author>
    <link href="http://arxiv.org/abs/2309.07077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.07077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.04371v1</id>
    <updated>2023-11-07T22:30:56Z</updated>
    <published>2023-11-07T22:30:56Z</published>
    <title>How Charles Babbage invented the Computer</title>
    <summary>  This paper provides an overview of the successive stages in the development
of Charles Babbage's Analytical Engine, based on the blueprints held in the
Babbage Papers Archive, accessible online through the Science Museum in London.
The first person to decipher these schematics was Allan Bromley, whose
contributions in the 1980s and 1990s significantly advanced our understanding
of Babbage's pioneering work. The Science Museum's digitization of the Babbage
Papers enables a chronological exploration of the evolution of Babbage's
machines. The focus is on the Analytical Engine, shedding light on its lesser
known but crucial transitional phases.
</summary>
    <author>
      <name>Raul Rojas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.04371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.04371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.hist-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.12235v1</id>
    <updated>2023-11-20T23:24:58Z</updated>
    <published>2023-11-20T23:24:58Z</published>
    <title>Improvements in Interlayer Pipelining of CNN Accelerators Using Genetic
  Algorithms</title>
    <summary>  Deploying Convolutional Neural Networks (CNNs) on edge platforms necessitates
efficient hardware acceleration. Any unnecessary data movement in such
accelerators can unacceptably degrade performance and efficiency. To address
this, we develop a layer fusion technique targeting CNNs, that reduces off-chip
data communication using a Genetic Algorithm (GA) applied to graph-based
topological sort. Results show a 1.8$\times$ increase in energy efficiency and
1.9$\times$ improvement in energy-delay product (EDP) for MobileNet-v3 on a
SIMBA-like mobile architecture. Our approach consistently improves workload
performance, averaging 1.4$\times$ improvement to EDP for SIMBA and
1.12$\times$ for Eyeriss.
</summary>
    <author>
      <name>Mark Horeni</name>
    </author>
    <author>
      <name>Siddharth Joshi</name>
    </author>
    <link href="http://arxiv.org/abs/2311.12235v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.12235v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.01455v1</id>
    <updated>2023-12-03T16:58:12Z</updated>
    <published>2023-12-03T16:58:12Z</published>
    <title>32-Bit RISC-V CPU Core on Logisim</title>
    <summary>  This project focuses on making a RISC-V CPU Core using the Logisim software.
RISC-V is significant because it will allow smaller device manufacturers to
build hardware without paying royalties and allow developers and researchers to
design and experiment with a proven and freely available instruction set
architecture. RISC-V is ideal for a variety of applications from IOTs to
Embedded systems such as disks, CPUs, Calculators, SOCs, etc. RISC-V(Reduced
Instruction Set Architecture) is an open standard instruction set architecture
(ISA) based on established reduced instruction set computer (RISC) principles.
Unlike most other ISA designs, the RISC-V ISA is provided under open source
licenses that do not require fees to use.
</summary>
    <author>
      <name>Siddesh D. Patil</name>
    </author>
    <author>
      <name>Premraj V. Jadhav</name>
    </author>
    <author>
      <name>Siddharth Sankhe</name>
    </author>
    <link href="http://arxiv.org/abs/2312.01455v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.01455v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.01832v1</id>
    <updated>2023-12-04T12:11:59Z</updated>
    <published>2023-12-04T12:11:59Z</published>
    <title>SPECRUN: The Danger of Speculative Runahead Execution in Processors</title>
    <summary>  Runahead execution is a continuously evolving microarchitectural technique
for processor performance. This paper introduces the first transient execution
attack on the runahead execution, called SPECRUN, which exploits the unresolved
branch prediction during runahead execution. We show that SPECRUN eliminates
the limitation on the number of transient instructions posed by the reorder
buffer size, enhancing the exploitability and harmfulness of the attack. We
concretely demonstrate a proof-of-concept attack that causes leaking secrets
from a victim process, validate the merit of SPECRUN, and design a secure
runahead execution scheme. This paper highlights the need to consider the
security of potential optimization techniques before implementing them in a
processor.
</summary>
    <author>
      <name>Chaoqun Shen</name>
    </author>
    <author>
      <name>Gang Qu</name>
    </author>
    <author>
      <name>Jiliang Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2312.01832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.01832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.06086v1</id>
    <updated>2023-12-11T03:24:18Z</updated>
    <published>2023-12-11T03:24:18Z</published>
    <title>HALO-CAT: A Hidden Network Processor with Activation-Localized CIM
  Architecture and Layer-Penetrative Tiling</title>
    <summary>  To address the 'memory wall' problem in NN hardware acceleration, we
introduce HALO-CAT, a software-hardware co-design optimized for Hidden Neural
Network (HNN) processing. HALO-CAT integrates Layer-Penetrative Tiling (LPT)
for algorithmic efficiency, reducing intermediate result sizes. Furthermore,
the architecture employs an activation-localized computing-in-memory approach
to minimize data movement. This design significantly enhances energy
efficiency, achieving a 14.2x reduction in activation memory capacity and a
17.8x decrease in energy consumption, with only a 1.5% loss in accuracy,
compared to traditional HNN processors.
</summary>
    <author>
      <name>Yung-Chin Chen</name>
    </author>
    <author>
      <name>Shimpei Ando</name>
    </author>
    <author>
      <name>Daichi Fujiki</name>
    </author>
    <author>
      <name>Shinya Takamaeda-Yamazaki</name>
    </author>
    <author>
      <name>Kentaro Yoshioka</name>
    </author>
    <link href="http://arxiv.org/abs/2312.06086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.06086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.09401v1</id>
    <updated>2023-12-14T23:45:55Z</updated>
    <published>2023-12-14T23:45:55Z</published>
    <title>Inter-Layer Scheduling Space Exploration for Multi-model Inference on
  Heterogeneous Chiplets</title>
    <summary>  To address increasing compute demand from recent multi-model workloads with
heavy models like large language models, we propose to deploy heterogeneous
chiplet-based multi-chip module (MCM)-based accelerators. We develop an
advanced scheduling framework for heterogeneous MCM accelerators that
comprehensively consider complex heterogeneity and inter-chiplet pipelining.
Our experiments using our framework on GPT-2 and ResNet-50 models on a
4-chiplet system have shown upto 2.2x and 1.9x increase in throughput and
energy efficiency, compared to a monolithic accelerator with an optimized
output-stationary dataflow.
</summary>
    <author>
      <name>Mohanad Odema</name>
    </author>
    <author>
      <name>Hyoukjun Kwon</name>
    </author>
    <author>
      <name>Mohammad Abdullah Al Faruque</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted poster abstract to the IBM IEEE AI Compute Symposium
  (AICS'23)</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.09401v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.09401v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.06885v1</id>
    <updated>2024-01-12T20:32:38Z</updated>
    <published>2024-01-12T20:32:38Z</published>
    <title>Accelerating Neural Networks for Large Language Models and Graph
  Processing with Silicon Photonics</title>
    <summary>  In the rapidly evolving landscape of artificial intelligence, large language
models (LLMs) and graph processing have emerged as transformative technologies
for natural language processing (NLP), computer vision, and graph-structured
data applications. However, the complex structures of these models pose
challenges for acceleration on conventional electronic platforms. In this
paper, we describe novel hardware accelerators based on silicon photonics to
accelerate transformer neural networks that are used in LLMs and graph neural
networks for graph data processing. Our analysis demonstrates that both
hardware accelerators achieve at least 10.2x throughput improvement and 3.8x
better energy efficiency over multiple state-of-the-art electronic hardware
accelerators designed for LLMs and graph processing.
</summary>
    <author>
      <name>Salma Afifi</name>
    </author>
    <author>
      <name>Febin Sunny</name>
    </author>
    <author>
      <name>Mahdi Nikdast</name>
    </author>
    <author>
      <name>Sudeep Pasricha</name>
    </author>
    <link href="http://arxiv.org/abs/2401.06885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.06885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.08625v1</id>
    <updated>2023-12-07T12:04:39Z</updated>
    <published>2023-12-07T12:04:39Z</published>
    <title>Conditional Flood Fill Method in Logic Synthesis</title>
    <summary>  In the field of Electronic Design Automation (EDA), logic synthesis plays a
pivotal role in optimizing hardware resources. Traditional logic synthesis
algorithms, such as the Quine-McCluskey method, face challenges in scalability
and efficiency, particularly for higher-dimension problems. This paper
introduces a novel heuristic algorithm based on Conditional Flood Fill Method
aimed at addressing these limitations. Our method employs count-based adjacent
element handling and introduces nine new theorems to guide the logic synthesis
process. Experimental results validate the efficacy of our approach, showing
significant improvements in computational efficiency and scalability compared
to existing algorithms. The algorithm holds potential for future advancements
in circuit development and Boolean function optimization.
</summary>
    <author>
      <name>Shitian Yang</name>
    </author>
    <author>
      <name>Junyue Jiang</name>
    </author>
    <author>
      <name>Yilai Liang</name>
    </author>
    <author>
      <name>Xiaoyang Chu</name>
    </author>
    <link href="http://arxiv.org/abs/2401.08625v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.08625v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.16387v1</id>
    <updated>2024-01-29T18:28:28Z</updated>
    <published>2024-01-29T18:28:28Z</published>
    <title>Green Adaptation of Real-Time Web Services for Industrial CPS within a
  Cloud Environment</title>
    <summary>  Managing energy efficiency under timing constraints is an interesting and big
challenge. This work proposes an accurate power model in data centers for
time-constrained servers in Cloud computing. This model, as opposed to previous
approaches, does not only consider the workload assigned to the processing
element, but also incorporates the need of considering the static power
consumption and, even more interestingly, its dependency with temperature. The
proposed model has been used in a multi-objective optimization environment in
which the Dynamic Voltage and Frequency Scaling (DVFS) and workload assignment
have been efficiently optimized.
</summary>
    <author>
      <name>Teresa Higuera</name>
    </author>
    <author>
      <name>José L. Risco-Martín</name>
    </author>
    <author>
      <name>Patricia Arroba</name>
    </author>
    <author>
      <name>José L. Ayala</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TII.2017.2693365</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TII.2017.2693365" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Industrial Informatics, 13(3), 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2401.16387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.16387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.10920v1</id>
    <updated>2024-01-25T21:21:38Z</updated>
    <published>2024-01-25T21:21:38Z</published>
    <title>Designing Silicon Brains using LLM: Leveraging ChatGPT for Automated
  Description of a Spiking Neuron Array</title>
    <summary>  Large language models (LLMs) have made headlines for synthesizing
correct-sounding responses to a variety of prompts, including code generation.
In this paper, we present the prompts used to guide ChatGPT4 to produce a
synthesizable and functional verilog description for the entirety of a
programmable Spiking Neuron Array ASIC. This design flow showcases the current
state of using ChatGPT4 for natural language driven hardware design. The
AI-generated design was verified in simulation using handcrafted testbenches
and has been submitted for fabrication in Skywater 130nm through Tiny Tapeout 5
using an open-source EDA flow.
</summary>
    <author>
      <name>Michael Tomlinson</name>
    </author>
    <author>
      <name>Joe Li</name>
    </author>
    <author>
      <name>Andreas Andreou</name>
    </author>
    <link href="http://arxiv.org/abs/2402.10920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.10920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.11691v1</id>
    <updated>2024-02-18T19:40:04Z</updated>
    <published>2024-02-18T19:40:04Z</published>
    <title>Stochastic Nonlinear Dynamical Modelling of SRAM Bitcells in Retention
  Mode</title>
    <summary>  SRAM bitcells in retention mode behave as autonomous stochastic nonlinear
dynamical systems. From observation of variability-aware transient noise
simulations, we provide an unidimensional model, fully characterizable by
conventional deterministic SPICE simulations, insightfully explaining the
mechanism of intrinsic noise-induced bit flips. The proposed model is exploited
to, first, explain the reported inaccuracy of existing closed-form
near-equilibrium formulas aimed at predicting the mean time to failure and,
secondly, to propose a closer estimate attractive in terms of CPU time.
</summary>
    <author>
      <name>Léopold Van Brandt</name>
    </author>
    <author>
      <name>Denis Flandre</name>
    </author>
    <author>
      <name>Jean-Charles Delvenne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted proceeding for invited talk at IEEE EDTM 2024, Bangalore,
  India</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.11691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.11691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.18089v1</id>
    <updated>2024-02-28T06:21:36Z</updated>
    <published>2024-02-28T06:21:36Z</published>
    <title>PIMSIM-NN: An ISA-based Simulation Framework for Processing-in-Memory
  Accelerators</title>
    <summary>  Processing-in-memory (PIM) has shown extraordinary potential in accelerating
neural networks. To evaluate the performance of PIM accelerators, we present an
ISA-based simulation framework including a dedicated ISA targeting neural
networks running on PIM architectures, a compiler, and a cycleaccurate
configurable simulator. Compared with prior works, this work decouples software
algorithms and hardware architectures through the proposed ISA, providing a
more convenient way to evaluate the effectiveness of software/hardware
optimizations. The simulator adopts an event-driven simulation approach and has
better support for hardware parallelism. The framework is open-sourced at
https://github.com/wangxy-2000/pimsim-nn.
</summary>
    <author>
      <name>Xinyu Wang</name>
    </author>
    <author>
      <name>Xiaotian Sun</name>
    </author>
    <author>
      <name>Yinhe Han</name>
    </author>
    <author>
      <name>Xiaoming Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2402.18089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.18089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.18746v1</id>
    <updated>2024-02-28T23:00:57Z</updated>
    <published>2024-02-28T23:00:57Z</published>
    <title>Accelerating Computer Architecture Simulation through Machine Learning</title>
    <summary>  This paper presents our approach to accelerate computer architecture
simulation by leveraging machine learning techniques. Traditional computer
architecture simulations are time-consuming, making it challenging to explore
different design choices efficiently. Our proposed model utilizes a combination
of application features and micro-architectural features to predict the
performance of an application. These features are derived from simulations of a
small portion of the application. We demonstrate the effectiveness of our
approach by building and evaluating a machine learning model that offers
significant speedup in architectural exploration. This model demonstrates the
ability to predict IPC values for the testing data with a root mean square
error of less than 0.1.
</summary>
    <author>
      <name>Wajid Ali</name>
    </author>
    <author>
      <name>Ayaz Akram</name>
    </author>
    <link href="http://arxiv.org/abs/2402.18746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.18746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.04189v1</id>
    <updated>2024-03-07T03:38:35Z</updated>
    <published>2024-03-07T03:38:35Z</published>
    <title>Silicon Photonic 2.5D Interposer Networks for Overcoming Communication
  Bottlenecks in Scale-out Machine Learning Hardware Accelerators</title>
    <summary>  Modern machine learning (ML) applications are becoming increasingly complex
and monolithic (single chip) accelerator architectures cannot keep up with
their energy efficiency and throughput demands. Even though modern digital
electronic accelerators are gradually adopting 2.5D architectures with multiple
smaller chiplets to improve scalability, they face fundamental limitations due
to a reliance on slow metallic interconnects. This paper outlines how optical
communication and computation can be leveraged in 2.5D platforms to realize
energy-efficient and high throughput 2.5D ML accelerator architectures.
</summary>
    <author>
      <name>Febin Sunny</name>
    </author>
    <author>
      <name>Ebadollah Taheri</name>
    </author>
    <author>
      <name>Mahdi Nikdast</name>
    </author>
    <author>
      <name>Sudeep Pasricha</name>
    </author>
    <link href="http://arxiv.org/abs/2403.04189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.04189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.18720v1</id>
    <updated>2024-03-27T16:11:23Z</updated>
    <published>2024-03-27T16:11:23Z</published>
    <title>Testing Resource Isolation for System-on-Chip Architectures</title>
    <summary>  Ensuring resource isolation at the hardware level is a crucial step towards
more security inside the Internet of Things. Even though there is still no
generally accepted technique to generate appropriate tests, it became clear
that tests should be generated at the system level. In this paper, we
illustrate the modeling aspects in test generation for resource isolation,
namely modeling the behavior and expressing the intended test scenario. We
present both aspects using the industrial standard PSS and an academic approach
based on conformance testing.
</summary>
    <author>
      <name>Philippe Ledent</name>
    </author>
    <author>
      <name>Radu Mateescu</name>
    </author>
    <author>
      <name>Wendelin Serwe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.399.7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.399.7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings MARS 2024, arXiv:2403.17862</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 399, 2024, pp. 129-168</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2403.18720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.18720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.13061v1</id>
    <updated>2024-04-11T20:29:15Z</updated>
    <published>2024-04-11T20:29:15Z</published>
    <title>FPGA Divide-and-Conquer Placement using Deep Reinforcement Learning</title>
    <summary>  This paper introduces the problem of learning to place logic blocks in
Field-Programmable Gate Arrays (FPGAs) and a learning-based method. In contrast
to previous search-based placement algorithms, we instead employ Reinforcement
Learning (RL) with the goal of minimizing wirelength. In addition to our
preliminary learning results, we also evaluated a novel decomposition to
address the nature of large search space when placing many blocks on a
chipboard. Empirical experiments evaluate the effectiveness of the learning and
decomposition paradigms on FPGA placement tasks.
</summary>
    <author>
      <name>Shang Wang</name>
    </author>
    <author>
      <name>Deepak Ranganatha Sastry Mamillapalli</name>
    </author>
    <author>
      <name>Tianpei Yang</name>
    </author>
    <author>
      <name>Matthew E. Taylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by ISEDA2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.13061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.13061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.00360v3</id>
    <updated>2024-07-08T18:21:49Z</updated>
    <published>2024-06-01T08:25:16Z</published>
    <title>L2R-CIPU: Efficient CNN Computation with Left-to-Right Composite Inner
  Product Units</title>
    <summary>  This paper proposes a composite inner-product computation unit based on
left-to-right (LR) arithmetic for the acceleration of convolution neural
networks (CNN) on hardware. The efficacy of the proposed L2R-CIPU method has
been shown on the VGG-16 network, and assessment is done on various performance
metrics. The L2R-CIPU design achieves 1.06x to 6.22x greater performance, 4.8x
to 15x more TOPS/W, and 4.51x to 53.45x higher TOPS/mm2 than prior
architectures.
</summary>
    <author>
      <name>Malik Zohaib Nisar</name>
    </author>
    <author>
      <name>Mohammad Sohail Ibrahim</name>
    </author>
    <author>
      <name>Muhammad Usman</name>
    </author>
    <author>
      <name>Jeong-A Lee</name>
    </author>
    <link href="http://arxiv.org/abs/2406.00360v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.00360v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.06536v1</id>
    <updated>2024-04-23T08:56:39Z</updated>
    <published>2024-04-23T08:56:39Z</published>
    <title>Apparate: Evading Memory Hierarchy with GodSpeed Wireless-on-Chip</title>
    <summary>  The rapid advancements in memory systems, CPU technology, and emerging
technologies herald a transformative potential in computing, promising to
revolutionize memory hierarchies. Innovations in DDR memory are delivering
unprecedented bandwidth, while advancements in on-chip wireless technology are
reducing size and increasing speed. The introduction of godspeed wireless
transceivers on chip, alongside near high-speed DRAM, is poised to directly
facilitate memory requests. This integration suggests the potential for
eliminating traditional memory hierarchies, offering a new paradigm in
computing efficiency and speed. These developments indicate a near-future where
computing systems are significantly more responsive and powerful, leveraging
direct, high-speed memory access mechanisms.
</summary>
    <author>
      <name>Nitesh Narayana GS</name>
    </author>
    <author>
      <name>Abhijit Das</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ASPLOS 2024, Wild and Crazy Ideas (WACI) session</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.06536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.06536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.09233v1</id>
    <updated>2024-06-13T15:33:54Z</updated>
    <published>2024-06-13T15:33:54Z</published>
    <title>C2HLSC: Can LLMs Bridge the Software-to-Hardware Design Gap?</title>
    <summary>  High Level Synthesis (HLS) tools offer rapid hardware design from C code, but
their compatibility is limited by code constructs. This paper investigates
Large Language Models (LLMs) for refactoring C code into HLS-compatible
formats. We present several case studies by using an LLM to rewrite C code for
NIST 800-22 randomness tests, a QuickSort algorithm and AES-128 into
HLS-synthesizable c. The LLM iteratively transforms the C code guided by user
prompts, implementing functions like streaming data and hardware-specific
signals. This evaluation demonstrates the LLM's potential to assist hardware
design refactoring regular C code into HLS synthesizable C code.
</summary>
    <author>
      <name>Luca Collini</name>
    </author>
    <author>
      <name>Siddharth Garg</name>
    </author>
    <author>
      <name>Ramesh Karri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LAD62341.2024.10691856</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LAD62341.2024.10691856" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at The First IEEE International Workshop on LLM-Aided Design</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.09233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.09233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.06751v1</id>
    <updated>2024-07-09T11:03:01Z</updated>
    <published>2024-07-09T11:03:01Z</published>
    <title>Laser Fault Injection Attacks against Radiation Tolerant TMR Registers</title>
    <summary>  Security requirements for the Internet of things (IoT), wireless sensor
nodes, and other wireless devices connected in a network for data exchange are
high. These devices are often subject to lab analysis with the objective to
reveal secret hidden information. One kind of attacks to reveal the
cryptographic key is to perform optical Fault Injection attacks. In this work,
we investigated the IHP radiation tolerant shift registers built of Triple
Modular Redundant flip-flops. In our experiments, we were able to inject
different transient faults into TMR registers.
</summary>
    <author>
      <name>Dmytro Petryk</name>
    </author>
    <author>
      <name>Zoya Dyka</name>
    </author>
    <author>
      <name>Ievgen Kabin</name>
    </author>
    <author>
      <name>Anselm Breitenreiter</name>
    </author>
    <author>
      <name>Jan Schaeffner</name>
    </author>
    <author>
      <name>Milos Krstic</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LATS57337.2022.9936987</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LATS57337.2022.9936987" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.06751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.06751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.08128v2</id>
    <updated>2025-01-18T07:17:30Z</updated>
    <published>2024-07-11T01:59:01Z</published>
    <title>Functional Type Expressions of Sequential Circuits with the Notion of
  Referring Forms</title>
    <summary>  This paper introduces the notion of referring forms as a new metric for
analyzing sequential circuits from a functional perspective. Sequential
circuits are modeled as causal stream functions, the outputs of which depend
solely on the past and current inputs. Referring forms are defined based on the
type expressions of functions and represent how a circuit refers to past
inputs. The key contribution of this study is identifying a universal property
in multiple clock domain circuits using referring forms. This theoretical
framework is expected to enhance the comprehension and analysis of sequential
circuits.
</summary>
    <author>
      <name>Shunji Nishimura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 figures, 2025 11th International Conference on Computing
  and Artificial Intelligence (ICCAI 2025): accepted</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.08128v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.08128v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.10312v1</id>
    <updated>2024-05-06T07:58:16Z</updated>
    <published>2024-05-06T07:58:16Z</published>
    <title>Effective Design Verification -- Constrained Random with Python and
  Cocotb</title>
    <summary>  Being the most widely used language across the world due to its simplicity
and with 35 keywords (v3.7), Python attracts both hardware and software
engineers. Python-based verification environment leverages open-source
libraries such as cocotb and cocotb-coverage that enables interfacing the
tesbenches with any available simulator and facilitating constrained
randomization, coverage respectively. These libraries significantly ease the
development of testbenches and have the potential to reduce the setup cost. The
goal of this paper is to assess the effectiveness of a Python-Cocotb
verification setup with design IPs and compare its features and performance
metrics with the current de-facto hardware verification language i.e.,
SystemVerilog.
</summary>
    <author>
      <name>Deepak Narayan Gadde</name>
    </author>
    <author>
      <name>Suruchi Kumari</name>
    </author>
    <author>
      <name>Aman Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in DVCon Europe 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.10312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.10312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.10317v1</id>
    <updated>2024-05-06T09:09:06Z</updated>
    <published>2024-05-06T09:09:06Z</published>
    <title>Towards Efficient Design Verification -- Constrained Random Verification
  using PyUVM</title>
    <summary>  Python, as a multi-paradigm language known for its ease of integration with
other languages, has gained significant attention among verification engineers
recently. A Python-based verification environment capitalizes on open-source
frameworks such as PyUVM providing Python-based UVM 1.2 implementation and
PyVSC facilitating constrained randomization and functional coverage. These
libraries play a pivotal role in expediting test development and hold promise
for reducing setup costs. The goal of this paper is to evaluate the
effectiveness of PyUVM verification testbenches across various design IPs,
aiming for a comprehensive comparison of their features and performance metrics
with the established SystemVerilog-UVM methodology.
</summary>
    <author>
      <name>Deepak Narayan Gadde</name>
    </author>
    <author>
      <name>Suruchi Kumari</name>
    </author>
    <author>
      <name>Aman Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in DVCon U.S. 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.10317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.10317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.19165v1</id>
    <updated>2024-07-27T04:17:38Z</updated>
    <published>2024-07-27T04:17:38Z</published>
    <title>HENNC: Hardware Engine for Artificial Neural Network-based Chaotic
  Oscillators</title>
    <summary>  This letter introduces a framework for the automatic generation of hardware
cores for Artificial Neural Network (ANN)-based chaotic oscillators. The
framework trains the model to approximate a chaotic system, then performs
design space exploration yielding potential hardware architectures for its
implementation. The framework then generates the corresponding synthesizable
High-Level Synthesis code and a validation testbench from a selected solution.
The hardware design primarily targets FPGAs. The proposed framework offers a
rapid hardware design process of candidate architectures superior to manually
designed works in terms of hardware cost and throughput. The source code is
available on GitHub.
</summary>
    <author>
      <name>Mobin Vaziri</name>
    </author>
    <author>
      <name>Shervin Vakili</name>
    </author>
    <author>
      <name>M. Mehdi Rahimifar</name>
    </author>
    <author>
      <name>J. M. Pierre Langlois</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.19165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.19165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.07228v1</id>
    <updated>2024-08-03T17:23:51Z</updated>
    <published>2024-08-03T17:23:51Z</published>
    <title>Bitwise Logic Using Phase Change Memory Devices Based on the Pinatubo
  Architecture</title>
    <summary>  This paper experimentally demonstrates a near-crossbar memory logic technique
called Pinatubo. Pinatubo, an acronym for Processing In Non-volatile memory
ArchiTecture for bUlk Bitwise Operations, facilitates the concurrent activation
of two or more rows, enabling bitwise operations such as OR, AND, XOR, and NOT
on the activated rows. We implement Pinatubo using phase change memory (PCM)
and compare our experimental results with the simulated data from the original
Pinatubo study. Our findings highlight a significant four-orders of magnitude
difference between resistance states, suggesting the robustness of the Pinatubo
architecture with PCM technology.
</summary>
    <author>
      <name>Noa Aflalo</name>
    </author>
    <author>
      <name>Eilam Yalon</name>
    </author>
    <author>
      <name>Shahar Kvatinsky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VLSID60093.2024.00103</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VLSID60093.2024.00103" rel="related"/>
    <link href="http://arxiv.org/abs/2408.07228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.07228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.10850v1</id>
    <updated>2024-08-20T13:44:41Z</updated>
    <published>2024-08-20T13:44:41Z</published>
    <title>Hardware Implementation of Projection-Aggregation Decoders for
  Reed-Muller Codes</title>
    <summary>  This paper presents the hardware implementation of two variants of
projection-aggregation-based decoding of Reed-Muller (RM) codes, namely unique
projection aggregation (UPA) and collapsed projection aggregation (CPA). Our
study focuses on introducing hardware architectures for both UPA and CPA.
Through thorough analysis and experimentation, we observe that the hardware
implementation of UPA exhibits superior resource usage and reduced energy
consumption compared to CPA for the vanilla IPA decoder. This finding
underscores a critical insight: software optimizations, in isolation, may not
necessarily translate into hardware cost-effectiveness.
</summary>
    <author>
      <name>Marzieh Hashemipour-Nazari</name>
    </author>
    <author>
      <name>Andrea Nardi-Dei</name>
    </author>
    <author>
      <name>Kees Goossens</name>
    </author>
    <author>
      <name>Alexios Balatsoukas-Stimming</name>
    </author>
    <link href="http://arxiv.org/abs/2408.10850v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.10850v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.15568v1</id>
    <updated>2024-08-28T06:50:09Z</updated>
    <published>2024-08-28T06:50:09Z</published>
    <title>Affordable HPC: Leveraging Small Clusters for Big Data and Graph
  Computing</title>
    <summary>  This study explores strategies for academic researchers to optimize
computational resources within limited budgets, focusing on building small,
efficient computing clusters. It delves into the comparative costs of
purchasing versus renting servers, guided by market research and economic
theories on tiered pricing. The paper offers detailed insights into the
selection and assembly of hardware components such as CPUs, GPUs, and
motherboards tailored to specific research needs. It introduces innovative
methods to mitigate the performance issues caused by PCIe switch bandwidth
limitations in order to enhance GPU task scheduling. Furthermore, a Graph
Neural Network (GNN) framework is proposed to analyze and optimize parallelism
in computing networks.
</summary>
    <author>
      <name>Ruilong Wu</name>
    </author>
    <author>
      <name>Yisu Wang</name>
    </author>
    <author>
      <name>Dirk Kutscher</name>
    </author>
    <link href="http://arxiv.org/abs/2408.15568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.15568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.11424v1</id>
    <updated>2024-09-12T17:53:37Z</updated>
    <published>2024-09-12T17:53:37Z</published>
    <title>LlamaF: An Efficient Llama2 Architecture Accelerator on Embedded FPGAs</title>
    <summary>  Large language models (LLMs) have demonstrated remarkable abilities in
natural language processing. However, their deployment on resource-constrained
embedded devices remains difficult due to memory and computational demands. In
this paper, we present an FPGA-based accelerator designed to improve LLM
inference performance on embedded FPGAs. We employ post-training quantization
to reduce model size and optimize for off-chip memory bandwidth. Our design
features asynchronous computation and a fully pipelined accelerator for
matrix-vector multiplication. Experiments of the TinyLlama 1.1B model on a
Xilinx ZCU102 platform show a 14.3-15.8x speedup and a 6.1x power efficiency
improvement over running exclusively on ZCU102 processing system (PS).
</summary>
    <author>
      <name>Han Xu</name>
    </author>
    <author>
      <name>Yutong Li</name>
    </author>
    <author>
      <name>Shihao Ji</name>
    </author>
    <link href="http://arxiv.org/abs/2409.11424v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.11424v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.17445v1</id>
    <updated>2024-09-26T00:34:23Z</updated>
    <published>2024-09-26T00:34:23Z</published>
    <title>The Interplay of Computing, Ethics, and Policy in Brain-Computer
  Interface Design</title>
    <summary>  Brain-computer interfaces (BCIs) connect biological neurons in the brain with
external systems like prosthetics and computers. They are increasingly
incorporating processing capabilities to analyze and stimulate neural activity,
and consequently, pose unique design challenges related to ethics, law, and
policy. For the first time, this paper articulates how ethical, legal, and
policy considerations can shape BCI architecture design, and how the decisions
that architects make constrain or expand the ethical, legal, and policy
frameworks that can be applied to them.
</summary>
    <author>
      <name>Muhammed Ugur</name>
    </author>
    <author>
      <name>Raghavendra Pradyumna Pothukuchi</name>
    </author>
    <author>
      <name>Abhishek Bhattacharjee</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 1st Workshop on Hot Topics in Ethical Computer Systems, April,
  2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2409.17445v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.17445v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.19337v1</id>
    <updated>2024-09-28T12:45:19Z</updated>
    <published>2024-09-28T12:45:19Z</published>
    <title>Developing Cost-Effective Drones for 5G Non-Terrestrial Network Research
  and Experimentation</title>
    <summary>  In this article, we describe the components and procedures for building a
drone ready for networking experimentation. In particular, our drone design
includes multiple technologies and elements such as 4G/5G connectivity for
real-time data transmission, a 360-degree camera for immersive vision and
AR/VR, precise GPS for navigation, and a powerful Linux-based system with GPU
for computer vision experiments and applications. Component selection and
assembly techniques are included, along with software integration for a smooth,
seamless operation of advanced edge applications.
</summary>
    <author>
      <name>Carlos de Quinto Cáceres</name>
    </author>
    <author>
      <name>Andrés Navarro</name>
    </author>
    <author>
      <name>Alejandro Leonardo García Navarro</name>
    </author>
    <author>
      <name>Tomás Martínez</name>
    </author>
    <author>
      <name>Gabriel Otero</name>
    </author>
    <author>
      <name>José Alberto Hernández</name>
    </author>
    <link href="http://arxiv.org/abs/2409.19337v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.19337v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.01442v1</id>
    <updated>2024-10-02T11:51:43Z</updated>
    <published>2024-10-02T11:51:43Z</published>
    <title>Using a Performance Model to Implement a Superscalar CVA6</title>
    <summary>  A performance model of CVA6 RISC-V processor is built to evaluate performance
related modifications before implementing them in RTL. Its accuracy is 99.2% on
CoreMark. This model is used to evaluate a superscalar feature for CVA6. During
design phase, the model helped detecting and fixing performance bugs. The
superscalar feature resulted in a CVA6 performance improvement of 40% on
CoreMark.
</summary>
    <author>
      <name>Côme Allart</name>
    </author>
    <author>
      <name>Jean-Roch Coulon</name>
    </author>
    <author>
      <name>André Sintzoff</name>
    </author>
    <author>
      <name>Olivier Potin</name>
    </author>
    <author>
      <name>Jean-Baptiste Rigaud</name>
    </author>
    <link href="http://arxiv.org/abs/2410.01442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.01442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.22595v1</id>
    <updated>2024-10-29T23:17:42Z</updated>
    <published>2024-10-29T23:17:42Z</published>
    <title>Systolic Array Data Flows for Efficient Matrix Multiplication in Deep
  Neural Networks</title>
    <summary>  The paper discusses how Systolic Arrays can improve matrix multiplication for
deep neural networks (DNNs). With AI models like OpenAI's GPT now containing
trillions of parameters, the need for efficient matrix multiplication is more
critical than ever. In this paper, the three main systolic array data flows:
Weight Stationary (WS), Input Stationary (IS), and Output Stationary (OS) are
discussed. Each data flow's energy consumption and efficiency across various
matrix sizes are calculated using the SCALE-Sim simulator. The results show
that selecting the right data flow for specific matrix configurations can
drastically reduce energy consumption. The conclusions provide helpful insights
into optimizing hardware for AI and machine learning applications, offering
potential improvements in designing energy-efficient DNN accelerators.
</summary>
    <author>
      <name>Tejas Raja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.22595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.22595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.03149v1</id>
    <updated>2024-11-05T14:48:16Z</updated>
    <published>2024-11-05T14:48:16Z</published>
    <title>Hardware for converting floating-point to the microscaling (MX) format</title>
    <summary>  This paper proposes hardware converters for the microscaling format
(MX-format), a reduced representation of floating-point numbers. We present an
algorithm and a memory-free hardware model for converting 32 single-precision
floating-point numbers to MX-format. The proposed model supports six different
types of MX-format: E5M2, E4M3, E3M2, E2M3, E2M1, and INT8. The conversion
process consists of three steps: calculating the maximum absolute value among
32 inputs, generating a shared scale, and producing 32 outputs in the selected
MX-format type. The hardware converters were implemented in FPGA, and
experimental results demonstrate.
</summary>
    <author>
      <name>Danila Gorodecky</name>
    </author>
    <author>
      <name>Leonel Sousa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">this 6-page paper was never published or submitted anywhere.
  Submitted 05.11.2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.03149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.03149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.07721v1</id>
    <updated>2024-11-12T11:28:02Z</updated>
    <published>2024-11-12T11:28:02Z</published>
    <title>Web-Based Simulator of Superscalar RISC-V Processors</title>
    <summary>  Mastering computational architectures is essential for developing fast and
power-efficient programs. Our advanced simulator empowers both IT students and
professionals to grasp the fundamentals of superscalar RISC-V processors, HW/SW
co-design and HPC optimization techniques. With customizable processor and
memory architecture, full C compiler support, and detailed runtime statistics,
this tool offers a comprehensive learning experience. Enjoy the convenience of
a modern, web-based GUI to enhance your understanding and skills.
</summary>
    <author>
      <name>Jiri Jaros</name>
    </author>
    <author>
      <name>Michal Majer</name>
    </author>
    <author>
      <name>Jakub Horky</name>
    </author>
    <author>
      <name>Jan Vavra</name>
    </author>
    <link href="http://arxiv.org/abs/2411.07721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.07721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.12983v1</id>
    <updated>2024-11-20T02:20:09Z</updated>
    <published>2024-11-20T02:20:09Z</published>
    <title>Veryl: A New Hardware Description Language as an Altarnative to
  SystemVerilog</title>
    <summary>  Veryl, a hardware description language based on SystemVerilog, offers
optimized syntax tailored for logic design, ensuring synthesizability and
simplifying common constructs. It prioritizes interoperability with
SystemVerilog, allowing for smooth integration with existing projects while
maintaining high readability. Additionally, Veryl includes a comprehensive set
of development support tools, such as package managers and real-time checkers,
to boost productivity and streamline the design process. These features empower
designers to conduct high-quality hardware design efficiently.
</summary>
    <author>
      <name>Naoya Hatta</name>
    </author>
    <author>
      <name>Taichi Ishitani</name>
    </author>
    <author>
      <name>Ryota Shioya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures, Published in DVCon Japan 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.12983v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.12983v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.6.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.17847v1</id>
    <updated>2024-11-26T20:00:54Z</updated>
    <published>2024-11-26T20:00:54Z</published>
    <title>SoftmAP: Software-Hardware Co-design for Integer-Only Softmax on
  Associative Processors</title>
    <summary>  Recent research efforts focus on reducing the computational and memory
overheads of Large Language Models (LLMs) to make them feasible on
resource-constrained devices. Despite advancements in compression techniques,
non-linear operators like Softmax and Layernorm remain bottlenecks due to their
sensitivity to quantization. We propose SoftmAP, a software-hardware co-design
methodology that implements an integer-only low-precision Softmax using
In-Memory Compute (IMC) hardware. Our method achieves up to three orders of
magnitude improvement in the energy-delay product compared to A100 and RTX3090
GPUs, making LLMs more deployable without compromising performance.
</summary>
    <author>
      <name>Mariam Rakka</name>
    </author>
    <author>
      <name>Jinhao Li</name>
    </author>
    <author>
      <name>Guohao Dai</name>
    </author>
    <author>
      <name>Ahmed Eltawil</name>
    </author>
    <author>
      <name>Mohammed E. Fouda</name>
    </author>
    <author>
      <name>Fadi Kurdahi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in DATE 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.17847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.17847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.18308v1</id>
    <updated>2024-11-27T12:58:06Z</updated>
    <published>2024-11-27T12:58:06Z</published>
    <title>CXL-Interference: Analysis and Characterization in Modern Computer
  Systems</title>
    <summary>  Compute Express Link (CXL) is a promising technology that addresses memory
and storage challenges. Despite its advantages, CXL faces performance threats
from external interference when co-existing with current memory and storage
systems. This interference is under-explored in existing research. To address
this, we develop CXL-Interplay, systematically characterizing and analyzing
interference from memory and storage systems. To the best of our knowledge, we
are the first to characterize CXL interference on real CXL hardware. We also
provide reverse-reasoning analysis with performance counters and kernel
functions. In the end, we propose and evaluate mitigating solutions.
</summary>
    <author>
      <name>Shunyu Mao</name>
    </author>
    <author>
      <name>Jiajun Luo</name>
    </author>
    <author>
      <name>Yixin Li</name>
    </author>
    <author>
      <name>Jiapeng Zhou</name>
    </author>
    <author>
      <name>Weidong Zhang</name>
    </author>
    <author>
      <name>Zheng Liu</name>
    </author>
    <author>
      <name>Teng Ma</name>
    </author>
    <author>
      <name>Shuwen Deng</name>
    </author>
    <link href="http://arxiv.org/abs/2411.18308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.18308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.20391v1</id>
    <updated>2024-12-29T08:04:54Z</updated>
    <published>2024-12-29T08:04:54Z</published>
    <title>Open-Source Heterogeneous SoCs for AI: The PULP Platform Experience</title>
    <summary>  Since 2013, the PULP (Parallel Ultra-Low Power) Platform project has been one
of the most active and successful initiatives in designing research IPs and
releasing them as open-source. Its portfolio now ranges from processor cores to
network-on-chips, peripherals, SoC templates, and full hardware accelerators.
In this article, we focus on the PULP experience designing heterogeneous AI
acceleration SoCs - an endeavour encompassing SoC architecture definition;
development, verification, and integration of acceleration IPs; front- and
back-end VLSI design; testing; development of AI deployment software.
</summary>
    <author>
      <name>Francesco Conti</name>
    </author>
    <author>
      <name>Angelo Garofalo</name>
    </author>
    <author>
      <name>Davide Rossi</name>
    </author>
    <author>
      <name>Giuseppe Tagliavini</name>
    </author>
    <author>
      <name>Luca Benini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprinted submitted to IEEE Solid-State Circuits Magazine</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.20391v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.20391v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.20393v1</id>
    <updated>2024-12-29T08:09:35Z</updated>
    <published>2024-12-29T08:09:35Z</published>
    <title>A Novel FPGA-based CNN Hardware Accelerator: Optimization for
  Convolutional Layers using Karatsuba Ofman Multiplier</title>
    <summary>  A new architecture of CNN hardware accelerator is presented. Convolutional
Neural Networks (CNNs) are a subclass of neural networks that have demonstrated
outstanding performance in a variety of computer vision applications, including
object detection, image classification, and many more.Convolution, a
mathematical operation that consists of multiplying, shifting and adding a set
of input values by a set of learnable parameters known as filters or kernels,
which is the fundamental component of a CNN.The Karatsuba Ofman multiplier is
known for its ability to perform high-speed multiplication with less hardware
resources compared to traditional multipliers. This article examines the usage
of the Karatsuba Ofman Multiplier method on FPGA in the prominent CNN designs
AlexNet, VGG16, and VGG19.
</summary>
    <author>
      <name>Amit Sarkar</name>
    </author>
    <link href="http://arxiv.org/abs/2412.20393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.20393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.00642v1</id>
    <updated>2024-12-31T20:37:20Z</updated>
    <published>2024-12-31T20:37:20Z</published>
    <title>Enabling New HDLs with Agents</title>
    <summary>  Large Language Models (LLMs) based agents are transforming the programming
language landscape by facilitating learning for beginners, enabling code
generation, and optimizing documentation workflows. Hardware Description
Languages (HDLs), with their smaller user community, stand to benefit
significantly from the application of LLMs as tools for learning new HDLs. This
paper investigates the challenges and solutions of enabling LLMs for HDLs,
particularly for HDLs that LLMs have not been previously trained on. This work
introduces HDLAgent, an AI agent optimized for LLMs with limited knowledge of
various HDLs. It significantly enhances off-the-shelf LLMs.
</summary>
    <author>
      <name>Mark Zakharov</name>
    </author>
    <author>
      <name>Farzaneh Rabiei Kashanaki</name>
    </author>
    <author>
      <name>Jose Renau</name>
    </author>
    <link href="http://arxiv.org/abs/2501.00642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.00642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.18113v2</id>
    <updated>2025-02-02T20:58:07Z</updated>
    <published>2025-01-30T03:32:14Z</published>
    <title>Adding MFMA Support to gem5</title>
    <summary>  In this work we have enhanced gem5's GPU model support to add Matrix Core
Engines (MCEs). Specifically, on the AMD MI200 and MI300 GPUs that gem5
supports, these MCEs perform Matrix Fused Multiply Add (MFMA) instructions for
a variety of precisions. By adding this support, our changes enable running
state-of-the-art ML workloads in gem5, as well as examining how MCE
optimizations impact the behavior of future systems.
</summary>
    <author>
      <name>Marco Kurzynski</name>
    </author>
    <author>
      <name>Matthew D. Sinclair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.18113v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.18113v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.00057v1</id>
    <updated>2025-01-29T23:43:08Z</updated>
    <published>2025-01-29T23:43:08Z</published>
    <title>CuLD: Current-Limiting Differential Reading Circuit for Current-Based
  Compute-in-Memory</title>
    <summary>  This paper proposes a circuit configuration that addresses the issue of
deviation in the multiply-accumulate (MAC) results when numerous word lines are
simultaneously opened in current-based compute-in-memory (CiM) circuits. The
proposed circuit solves this problem by automatically shrinking the product
value according to the degree of parallelism. This circuit configuration is
effective for circuit methods that calculate MAC through time integration of
charge.
</summary>
    <author>
      <name>Seiji Uenohara</name>
    </author>
    <author>
      <name>Satoshi Awamura</name>
    </author>
    <author>
      <name>Norio Hattori</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 10 figures, 2 tables, conference(2025 Symposium on VLSI
  Technology and Circuits)</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.00057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.00057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.00286v1</id>
    <updated>2025-02-01T02:54:33Z</updated>
    <published>2025-02-01T02:54:33Z</published>
    <title>Late Breaking Results: Leveraging Approximate Computing for Carbon-Aware
  DNN Accelerators</title>
    <summary>  The rapid growth of Machine Learning (ML) has increased demand for DNN
hardware accelerators, but their embodied carbon footprint poses significant
environmental challenges. This paper leverages approximate computing to design
sustainable accelerators by minimizing the Carbon Delay Product (CDP). Using
gate-level pruning and precision scaling, we generate area-aware approximate
multipliers and optimize the accelerator design with a genetic algorithm.
Results demonstrate reduced embodied carbon while meeting performance and
accuracy requirements.
</summary>
    <author>
      <name>Aikaterini Maria Panteleaki</name>
    </author>
    <author>
      <name>Konstantinos Balaskas</name>
    </author>
    <author>
      <name>Georgios Zervakis</name>
    </author>
    <author>
      <name>Hussam Amrouch</name>
    </author>
    <author>
      <name>Iraklis Anagnostopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in DATE 2025 - Late Breaking Results</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.00286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.00286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.00322v1</id>
    <updated>2025-03-01T03:18:12Z</updated>
    <published>2025-03-01T03:18:12Z</published>
    <title>T-REX: A 68-567 μs/token, 0.41-3.95 μJ/token Transformer
  Accelerator with Reduced External Memory Access and Enhanced Hardware
  Utilization in 16nm FinFET</title>
    <summary>  This work introduces novel training and post-training compression schemes to
reduce external memory access during transformer model inference. Additionally,
a new control flow mechanism, called dynamic batching, and a novel buffer
architecture, termed a two-direction accessible register file, further reduce
external memory access while improving hardware utilization.
</summary>
    <author>
      <name>Seunghyun Moon</name>
    </author>
    <author>
      <name>Mao Li</name>
    </author>
    <author>
      <name>Gregory Chen</name>
    </author>
    <author>
      <name>Phil Knag</name>
    </author>
    <author>
      <name>Ram Krishnamurthy</name>
    </author>
    <author>
      <name>Mingoo Seok</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE ISSCC 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.00322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.00322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.11654v1</id>
    <updated>2025-01-29T14:49:18Z</updated>
    <published>2025-01-29T14:49:18Z</published>
    <title>Enhanced LPDDR4X PHY in 12 nm FinFET</title>
    <summary>  The demand for memory technologies with high bandwidth, low power
consumption, and enhanced reliability has led to the emergence of LPDDR4X DRAM
memory. However, power efficiency and reliability depend not only on the memory
device but also on its interfacing. To enable advanced monitoring of LPDDR4X
DRAM devices and interface tuning, we propose a LPDDR4X PHY implemented in 12
nm FinFET technology. A RISC-V subsystem offers software-controlled DRAM
interface access as well as external interfaces to connect additional sensors
for monitoring temperature and current consumption of LPDDR4X DRAM devices.
</summary>
    <author>
      <name>Johannes Feldmann</name>
    </author>
    <author>
      <name>Jan Lappas</name>
    </author>
    <author>
      <name>Mohammadreza Esmaeilpour</name>
    </author>
    <author>
      <name>Hussien Abdo</name>
    </author>
    <author>
      <name>Christian Weis</name>
    </author>
    <author>
      <name>Norbert Wehn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RISC-V Summit Europe, Munich, 24-28th June 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.11654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.11654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.14708v1</id>
    <updated>2025-03-18T20:16:50Z</updated>
    <published>2025-03-18T20:16:50Z</published>
    <title>NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel
  16</title>
    <summary>  This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm
heterogeneous multicore RISC-V SoC for sparse and dense machine learning
kernels with both near-core and near-memory accelerators. A prototype chip runs
at 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.
The effectiveness of the design is demonstrated by running inference on a
sparse language model, ReLU-Llama.
</summary>
    <author>
      <name>Viansa Schmulbach</name>
    </author>
    <author>
      <name>Jason Kim</name>
    </author>
    <author>
      <name>Ethan Gao</name>
    </author>
    <author>
      <name>Lucy Revina</name>
    </author>
    <author>
      <name>Nikhil Jha</name>
    </author>
    <author>
      <name>Ethan Wu</name>
    </author>
    <author>
      <name>Borivoje Nikolic</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HCS61935.2024.10665203</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HCS61935.2024.10665203" rel="related"/>
    <link href="http://arxiv.org/abs/2503.14708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.14708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.18070v1</id>
    <updated>2025-03-23T13:40:20Z</updated>
    <published>2025-03-23T13:40:20Z</published>
    <title>Semicustom Frontend VLSI Design and Analysis of a 32-bit Brent-Kung
  Adder in Cadence Suite</title>
    <summary>  Adders are fundamental components in digital circuits, playing a crucial role
in arithmetic operations within computing systems and many other applications.
This paper focuses on the design and simulation of a 32-bit Brent-Kung parallel
prefix adder, which is recognized for its efficient carry propagation and
logarithmic delay characteristics. The Brent-Kung architecture balances
computational speed and hardware complexity, making it suitable for high-speed
digital applications. The design is implemented using Verilog HDL and simulated
using Cadence Design Suite tools, including NCLaunch and Genus, to evaluate its
performance in terms of scalability, speed, and functional working. Comparative
analysis with traditional adder architectures highlights the advantages of the
Brent-Kung adder for modern digital systems.
</summary>
    <author>
      <name>Yashvardhan Singh</name>
    </author>
    <link href="http://arxiv.org/abs/2503.18070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.18070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.20590v1</id>
    <updated>2025-03-26T14:41:40Z</updated>
    <published>2025-03-26T14:41:40Z</published>
    <title>Dual-Issue Execution of Mixed Integer and Floating-Point Workloads on
  Energy-Efficient In-Order RISC-V Cores</title>
    <summary>  To meet the computational requirements of modern workloads under tight energy
constraints, general-purpose accelerator architectures have to integrate an
ever-increasing number of extremely area- and energy-efficient processing
elements (PEs). In this context, single-issue in-order cores are commonplace,
but lean dual-issue cores could boost PE IPC, especially for the common case of
mixed integer and floating-point workloads. We develop the COPIFT methodology
and RISC-V ISA extensions to enable low-cost and flexible dual-issue execution
of mixed integer and floating-point instruction sequences. On such kernels, our
methodology achieves speedups of 1.47x, reaching a peak 1.75 instructions per
cycle, and 1.37x energy improvements on average, over optimized RV32G
baselines.
</summary>
    <author>
      <name>Luca Colagrande</name>
    </author>
    <author>
      <name>Luca Benini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at DAC 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.20590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.20590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.01972v1</id>
    <updated>2025-03-24T15:39:27Z</updated>
    <published>2025-03-24T15:39:27Z</published>
    <title>Efficient Trace for RISC-V: Design, Evaluation, and Integration in CVA6</title>
    <summary>  In this work, we present the design and evaluation of a Processor Tracing
System compliant with the RISC-V Efficient Trace specification for Instruction
Branch Tracing. We integrate our system into the host domain of a
state-of-the-art edge architecture based on CVA6. The proposed Tracing System
introduces a total overhead of 9.2% in terms of resource utilization on a
Xilinx VCU118 FPGA on the CVA6 subsystem while achieving an average compression
rate of 95.1% on platform-specific tests, compared to tracing each full opcode
instruction.
</summary>
    <author>
      <name>Umberto Laghi</name>
    </author>
    <author>
      <name>Simone Manoni</name>
    </author>
    <author>
      <name>Emanuele Parisi</name>
    </author>
    <author>
      <name>Andrea Bartolini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures, 1 table, accepted at RISC-V Summit Europe 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.01972v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.01972v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.03722v1</id>
    <updated>2025-03-30T15:42:19Z</updated>
    <published>2025-03-30T15:42:19Z</published>
    <title>WebRISC-V: A 64-bit RISC-V Pipeline Simulator for Computer Architecture
  Classes</title>
    <summary>  WebRISC-V is a web-based educational tool designed to simulate the pipelined
execution of assembly programs according to the RV64IM specifications (64-bit
RISC-V processor). The tool allows users to investigate pipeline stalls,
understand the internal state of pipeline architectural blocks, and visualize
the cycle-by-cycle execution of instructions. WebRISC-V executes directly in a
web browser, providing a detailed pipeline execution for RISC-V processors.
This paper describes the features of WebRISC-V, compares it with similar tools,
and provides an example of its usage in investigating the pipeline.
</summary>
    <author>
      <name>Roberto Giorgi</name>
    </author>
    <author>
      <name>Gianfranco Mariotti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RISC-V Summit Europe, Paris 12-15th May 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.03722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.03722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.10411v1</id>
    <updated>2025-04-14T16:58:46Z</updated>
    <published>2025-04-14T16:58:46Z</published>
    <title>FPGA-Optimized Hardware Accelerator for Fast Fourier Transform and
  Singular Value Decomposition in AI</title>
    <summary>  This research introduces an FPGA-based hardware accelerator to optimize the
Singular Value Decomposition (SVD) and Fast Fourier transform (FFT) operations
in AI models. The proposed design aims to improve processing speed and reduce
computational latency. Through experiments, we validate the performance
benefits of the hardware accelerator and show how well it handles FFT and SVD
operations. With its strong security and durability, the accelerator design
achieves significant speedups over software implementations, thanks to its
modules for data flow control, watermark embedding, FFT, and SVD.
</summary>
    <author>
      <name>Hong Ding</name>
    </author>
    <author>
      <name>Chia Chao Kang</name>
    </author>
    <author>
      <name>SuYang Xi</name>
    </author>
    <author>
      <name>Zehang Liu</name>
    </author>
    <author>
      <name>Xuan Zhang</name>
    </author>
    <author>
      <name>Yi Ding</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.10411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.10411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.16173v2</id>
    <updated>2025-04-24T12:04:11Z</updated>
    <published>2025-04-22T18:02:35Z</published>
    <title>FPGA-Based Neural Network Accelerators for Space Applications: A Survey</title>
    <summary>  Space missions are becoming increasingly ambitious, necessitating
high-performance onboard spacecraft computing systems. In response,
field-programmable gate arrays (FPGAs) have garnered significant interest due
to their flexibility, cost-effectiveness, and radiation tolerance potential.
Concurrently, neural networks (NNs) are being recognized for their capability
to execute space mission tasks such as autonomous operations, sensor data
analysis, and data compression. This survey serves as a valuable resource for
researchers aiming to implement FPGA-based NN accelerators in space
applications. By analyzing existing literature, identifying trends and gaps,
and proposing future research directions, this work highlights the potential of
these accelerators to enhance onboard computing systems.
</summary>
    <author>
      <name>Pedro Antunes</name>
    </author>
    <author>
      <name>Artur Podobas</name>
    </author>
    <link href="http://arxiv.org/abs/2504.16173v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.16173v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.19878v1</id>
    <updated>2025-04-28T15:07:36Z</updated>
    <published>2025-04-28T15:07:36Z</published>
    <title>FoldedHexaTorus: An Inter-Chiplet Interconnect Topology for
  Chiplet-based Systems using Organic and Glass Substrates</title>
    <summary>  Chiplet-based systems are rapidly gaining traction in the market. Two
packaging options for such systems are the established organic substrates and
the emerging glass substrates. These substrates are used to implement the
inter-chiplet interconnect (ICI), which is crucial for overall system
performance. To guide the development of ICIs, we introduce three design
principles for ICI network topologies on organic and glass substrates. Based on
our design principles, we propose the novel FoldedHexaTorus network topology.
Our evaluation shows that the FoldedHexaTorus achieves significantly higher
throughput than state-of-the-art topologies while maintaining low latency.
</summary>
    <author>
      <name>Patrick Iff</name>
    </author>
    <author>
      <name>Maciej Besta</name>
    </author>
    <author>
      <name>Torsten Hoefler</name>
    </author>
    <link href="http://arxiv.org/abs/2504.19878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.19878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.03750v2</id>
    <updated>2025-05-08T06:08:59Z</updated>
    <published>2025-04-17T05:56:14Z</published>
    <title>AI-Powered Agile Analog Circuit Design and Optimization</title>
    <summary>  Artificial intelligence (AI) techniques are transforming analog circuit
design by automating device-level tuning and enabling system-level
co-optimization. This paper integrates two approaches: (1) AI-assisted
transistor sizing using Multi-Objective Bayesian Optimization (MOBO) for direct
circuit parameter optimization, demonstrated on a linearly tunable
transconductor; and (2) AI-integrated circuit transfer function modeling for
system-level optimization in a keyword spotting (KWS) application, demonstrated
by optimizing an analog bandpass filter within a machine learning training
loop. The combined insights highlight how AI can improve analog performance,
reduce design iteration effort, and jointly optimize analog components and
application-level metrics.
</summary>
    <author>
      <name>Jinhai Hu</name>
    </author>
    <author>
      <name>Wang Ling Goh</name>
    </author>
    <author>
      <name>Yuan Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 5 figures, AI4X, 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.03750v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.03750v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.06687v1</id>
    <updated>2025-05-10T16:24:40Z</updated>
    <published>2025-05-10T16:24:40Z</published>
    <title>Extend IVerilog to Support Batch RTL Fault Simulation</title>
    <summary>  The advancement of functional safety has made RTL-level fault simulation
increasingly important to achieve iterative efficiency in the early stages of
design and to ensure compliance with functional safety standards. In this
paper, we extend IVerilog to support batch RTL fault simulation and integrate
the event-driven algorithm and the concurrent fault simulation algorithm.
Comparative experiments with a state-of-the-art commercial simulator and an
open-source RTL fault simulator demonstrate that our simulator achieves a
performance improvement of 2.2$\times$ and 3.4$\times$, respectively.
</summary>
    <author>
      <name>Jiaping Tang</name>
    </author>
    <author>
      <name>Jianan Mu</name>
    </author>
    <author>
      <name>Zizhen Liu</name>
    </author>
    <author>
      <name>Zhiteng Chao</name>
    </author>
    <author>
      <name>Jing Ye</name>
    </author>
    <author>
      <name>Huawei Li</name>
    </author>
    <link href="http://arxiv.org/abs/2505.06687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.06728v1</id>
    <updated>2025-05-10T18:35:18Z</updated>
    <published>2025-05-10T18:35:18Z</published>
    <title>Regular mixed-radix DFT matrix factorization for in-place FFT
  accelerators</title>
    <summary>  The generic vector memory based accelerator is considered which supports DIT
and DIF FFT with fixed datapath. The regular mixed-radix factorization of the
DFT matrix coherent with the accelerator architecture is proposed and the
correction proof is presented. It allows better understanding of architecture
requirements and simplifies the developing and proving correctness of more
complicated algorithms and conflict-free addressing schemes.
</summary>
    <author>
      <name>Sergey Salishev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, 2018 Systems of Signals Generating and Processing
  in the Field of on Board Communications. IEEE, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.06728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.13002v2</id>
    <updated>2025-05-20T07:34:45Z</updated>
    <published>2025-05-19T11:41:21Z</published>
    <title>PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for
  Processing-In-Memory (PIM) Architectures</title>
    <summary>  Dynamic memory allocation is essential in modern programming but remains
under-supported in current PIM devices. In this work, we first conduct a design
space exploration of PIM memory allocators, examining optimal metadata
placement and management strategies. Building on these insights, we propose
PIM-malloc, a fast and scalable allocator for real PIM hardware, improving
allocation performance by $66\times$. We further enhance this design with a
lightweight, per-PIM core hardware cache for dynamic allocation, achieving an
additional $31\%$ performance gain. Finally, we demonstrate the effectiveness
of PIM-malloc using a dynamic graph update workload, achieving a $28\times$
throughput increase.
</summary>
    <author>
      <name>Dongjae Lee</name>
    </author>
    <author>
      <name>Bongjoon Hyun</name>
    </author>
    <author>
      <name>Youngjin Kwon</name>
    </author>
    <author>
      <name>Minsoo Rhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.13002v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.13002v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405004v1</id>
    <updated>2004-05-03T20:25:00Z</updated>
    <published>2004-05-03T20:25:00Z</published>
    <title>Quantum Computers</title>
    <summary>  This research paper gives an overview of quantum computers - description of
their operation, differences between quantum and silicon computers, major
construction problems of a quantum computer and many other basic aspects. No
special scientific knowledge is necessary for the reader.
</summary>
    <author>
      <name>Archil Avaliani</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0405004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.0; C.0; K.4.0; I.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701165v1</id>
    <updated>2007-01-26T00:23:07Z</updated>
    <published>2007-01-26T00:23:07Z</published>
    <title>Petascale Computational Systems</title>
    <summary>  Computational science is changing to be data intensive. Super-Computers must
be balanced systems; not just CPU farms but also petascale IO and networking
arrays. Anyone building CyberInfrastructure should allocate resources to
support a balanced Tier-1 through Tier-3 design.
</summary>
    <author>
      <name>Gordon Bell</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Alex Szalay</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.4109v2</id>
    <updated>2012-07-24T00:58:43Z</updated>
    <published>2010-11-17T23:56:51Z</published>
    <title>Design and simulation of a sigma delta ADC</title>
    <summary>  In this report we describe the design and simulation of a Sigma Delta ADC in
Matlan/Simulink
</summary>
    <author>
      <name>Moslem Rashidi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to poor writing</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.4109v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.4109v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0411050v1</id>
    <updated>2004-11-16T01:49:22Z</updated>
    <published>2004-11-16T01:49:22Z</published>
    <title>Utilizing Reconfigurable Hardware Processors via Grid Services</title>
    <summary>  Computational grids typically consist of nodes utilizing ordinary processors
such as the Intel Pentium. Field Programmable Gate Arrays (FPGAs) are able to
perform certain compute-intensive tasks very well due to their inherent
parallel architecture, often resulting in orders of magnitude speedups. This
paper explores how FPGAs can be transparently exposed for remote use via grid
services, by integrating the Proteus Software Platform with the Globus Toolkit
3.0.
</summary>
    <author>
      <name>Darran Nathan</name>
    </author>
    <author>
      <name>Ralf Clemens</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0411050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0411050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611134v1</id>
    <updated>2006-11-27T19:45:40Z</updated>
    <published>2006-11-27T19:45:40Z</published>
    <title>Hard Disk Drive as a Magnetomechanical Logic Device</title>
    <summary>  We consider the conditions how two binary numbers can be superimposed on the
same track with the use of different recording magnetic fields. As a result the
average magnetization of longitudinal medium along the track can have three
states: -M, 0 and +M. Possibility to perform logic operations with these states
is considered. We demonstrate OR, AND, XOR and NOT operations and discuss a
modification of a recording device.
</summary>
    <author>
      <name>Vladimir L. Safonov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0611134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.0924v2</id>
    <updated>2011-05-31T15:20:48Z</updated>
    <published>2011-04-05T21:38:17Z</published>
    <title>ReveR: Software Simulator of Reversible Processor with Stack</title>
    <summary>  A software model of a reversible processor ReveR with the stack is discussed
in this paper. An architecture, the minimal set of elementary reversible
operations together with an implementation of the basic control flow structures
and procedures calls using simple assembler language are described.
</summary>
    <author>
      <name>Alexander Yu. Vlasov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LaTeX, 7 pages, no figures, 3 tables, v2: spelling and grammar
  corrected; project url http://friedmann.objectis.net/Members/vlasov/rever</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.0924v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.0924v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.0782v1</id>
    <updated>2012-01-03T22:15:22Z</updated>
    <published>2012-01-03T22:15:22Z</published>
    <title>Umgebungserfassungssystem fuer mobile Roboter (environment logging
  system for mobile autonomous robots)</title>
    <summary>  This diploma thesis describes the theoretical bases, the conception of the
module and the final result of the development process in application. for the
environment logging with a small mobile robot for interiors should be sketched
an economical alternative to the expensive laser scanners. the structure, color
or the material of the objects in the radius of action, as well as the
environment brightness and illuminating are to have thereby no influence on the
results of measurement.
</summary>
    <author>
      <name>Dirk Hesselbach</name>
    </author>
    <link href="http://arxiv.org/abs/1201.0782v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.0782v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0736v1</id>
    <updated>2014-09-02T14:53:09Z</updated>
    <published>2014-09-02T14:53:09Z</published>
    <title>On Delay Faults Affecting I/O Blocks of an SRAM-Based FPGA Due to
  Ionizing Radiations</title>
    <summary>  Experimental means to characterize delay faults induced by bit flips and SEUs
in I/O blocks of SRAM-based FPGAs are proposed. A delay fault up to 6.2ns
sensitized by an events chain is reported.
</summary>
    <author>
      <name>Fatima Zahra Tazi</name>
    </author>
    <author>
      <name>Claude Thibeault</name>
    </author>
    <author>
      <name>Yvon Savaria</name>
    </author>
    <author>
      <name>Simon Pichette</name>
    </author>
    <author>
      <name>Yves Audet</name>
    </author>
    <link href="http://arxiv.org/abs/1409.0736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.space-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.space-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.pop-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03057v1</id>
    <updated>2015-02-05T01:53:17Z</updated>
    <published>2015-02-05T01:53:17Z</published>
    <title>Circuit Level Modeling of Extra Combinational Delays in SRAM FPGAs Due
  to Transient Ionizing Radiation</title>
    <summary>  This paper presents a novel circuit level model that explains and confirms
the extra combinational delays in a SRAM-FPGA (Virtex-5) due to radiation,
which matches the experimental results by proton irradiation at TRIUMF.
</summary>
    <author>
      <name>Mostafa Darvishi</name>
    </author>
    <author>
      <name>Yves Audet</name>
    </author>
    <author>
      <name>Yves Blaquière</name>
    </author>
    <author>
      <name>Claude Thibeault</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNS.2014.2369424</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNS.2014.2369424" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2014 IEEE Nuclear and Space Radiation Effects Conference (NSREC),
  July 14-18, Paris, France</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.03057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.space-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.space-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06821v1</id>
    <updated>2015-08-27T12:03:57Z</updated>
    <published>2015-08-27T12:03:57Z</published>
    <title>ThreadPoolComposer - An Open-Source FPGA Toolchain for Software
  Developers</title>
    <summary>  This extended abstract presents ThreadPoolComposer, a high-level
synthesis-based development framework and meta-toolchain that provides a
uniform programming interface for FPGAs portable across multiple platforms.
</summary>
    <author>
      <name>Jens Korinth</name>
    </author>
    <author>
      <name>David de la Chevallerie</name>
    </author>
    <author>
      <name>Andreas Koch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.06821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00208v1</id>
    <updated>2017-02-01T11:04:34Z</updated>
    <published>2017-02-01T11:04:34Z</published>
    <title>Machines and Algorithms</title>
    <summary>  I discuss the evolution of computer architectures with a focus on QCD and
with reference to the interplay between architecture, engineering, data motion
and algorithms. New architectures are discussed and recent performance results
are displayed. I also review recent progress in multilevel solver and
integation algorithms.
</summary>
    <author>
      <name>Peter A Boyle</name>
    </author>
    <link href="http://arxiv.org/abs/1702.00208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-lat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-lat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11311v1</id>
    <updated>2018-07-30T12:23:10Z</updated>
    <published>2018-07-30T12:23:10Z</published>
    <title>Delay Monitor Circuit for Sensitive Nodes in SRAM-Based FPGA</title>
    <summary>  This paper presents a novel monitor circuit architecture and experiments
performed for detection of extra combinational delays in a high frequency
SRAM-Based FPGA on delay sensitive nodes due to transient ionizing radiation.
</summary>
    <author>
      <name>Mostafa Darvishi</name>
    </author>
    <author>
      <name>Yves Audet</name>
    </author>
    <author>
      <name>Yves Blaquiere</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Figures, 5 pages, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.space-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.space-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.04105v1</id>
    <updated>2020-12-07T23:10:51Z</updated>
    <published>2020-12-07T23:10:51Z</published>
    <title>The Tribes of Machine Learning and the Realm of Computer Architecture</title>
    <summary>  Machine learning techniques have influenced the field of computer
architecture like many other fields. This paper studies how the fundamental
machine learning techniques can be applied towards computer architecture
problems. We also provide a detailed survey of computer architecture research
that employs different machine learning methods. Finally, we present some
future opportunities and the outstanding challenges that need to be overcome to
exploit full potential of machine learning for computer architecture.
</summary>
    <author>
      <name>Ayaz Akram</name>
    </author>
    <author>
      <name>Jason Lowe-Power</name>
    </author>
    <link href="http://arxiv.org/abs/2012.04105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.04105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.08009v1</id>
    <updated>2021-04-16T10:07:28Z</updated>
    <published>2021-04-16T10:07:28Z</published>
    <title>Implementing CNN Layers on the Manticore Cluster-Based Many-Core
  Architecture</title>
    <summary>  This document presents implementations of fundamental convolutional neural
network (CNN) layers on the Manticore cluster-based many-core architecture and
discusses their characteristics and trade-offs.
</summary>
    <author>
      <name>Andreas Kurth</name>
    </author>
    <author>
      <name>Fabian Schuiki</name>
    </author>
    <author>
      <name>Luca Benini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report. 18 pages, 4 figures, 5 algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.08009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; C.1.4; F.2.1; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.01943v1</id>
    <updated>2021-09-04T22:10:07Z</updated>
    <published>2021-09-04T22:10:07Z</published>
    <title>Application Checkpoint and Power Study on Large Scale Systems</title>
    <summary>  Power efficiency is critical in high performance computing (HPC) systems. To
achieve high power efficiency on application level, it is vital importance to
efficiently distribute power used by application checkpoints. In this study, we
analyze the relation of application checkpoints and their power consumption.
The observations could guide the design of power management.
</summary>
    <author>
      <name>Yuping Fan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IIT technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.01943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.01943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.03032v1</id>
    <updated>2020-10-05T00:02:22Z</updated>
    <published>2020-10-05T00:02:22Z</published>
    <title>Symbolic Verification of Quantum Circuits</title>
    <summary>  This short note proposes a symbolic approach for representing and reasoning
about quantum circuits using complex, vector or matrix-valued Boolean
expressions. A major benefit of this approach is that it allows us to directly
borrow the existing techniques and tools for verification of classical logic
circuits in reasoning about quantum circuits.
</summary>
    <author>
      <name>Mingsheng Ying</name>
    </author>
    <author>
      <name>Zhengfeng Ji</name>
    </author>
    <link href="http://arxiv.org/abs/2010.03032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.03032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.01981v1</id>
    <updated>2022-06-04T12:45:06Z</updated>
    <published>2022-06-04T12:45:06Z</published>
    <title>Evaluation of Xilinx Deep Learning Processing Unit under Neutron
  Irradiation</title>
    <summary>  This paper studies the dependability of the Xilinx Deep-Learning Processing
Unit (DPU) under neutron irradiation. It analyses the impact of Single Event
Effects (SEEs) on the accuracy of the DPU running the resnet50 model on a
Xilinx Ultrascale+ MPSoC.
</summary>
    <author>
      <name>D. Agiakatsikas</name>
    </author>
    <author>
      <name>N. Foutris</name>
    </author>
    <author>
      <name>A. Sari</name>
    </author>
    <author>
      <name>V. Vlagkoulis</name>
    </author>
    <author>
      <name>I. Souvatzoglou</name>
    </author>
    <author>
      <name>M. Psarakis</name>
    </author>
    <author>
      <name>M. Luján</name>
    </author>
    <author>
      <name>M. Kastriotou</name>
    </author>
    <author>
      <name>C. Cazzaniga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.01981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.01981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.16942v1</id>
    <updated>2022-11-30T12:24:35Z</updated>
    <published>2022-11-30T12:24:35Z</published>
    <title>ALARM: Active LeArning of Rowhammer Mitigations</title>
    <summary>  Rowhammer is a serious security problem of contemporary dynamic random-access
memory (DRAM) where reads or writes of bits can flip other bits. DRAM
manufacturers add mitigations, but don't disclose details, making it difficult
for customers to evaluate their efficacy. We present a tool, based on active
learning, that automatically infers parameter of Rowhammer mitigations against
synthetic models of modern DRAM.
</summary>
    <author>
      <name>Amir Naseredini</name>
    </author>
    <author>
      <name>Martin Berger</name>
    </author>
    <author>
      <name>Matteo Sammartino</name>
    </author>
    <author>
      <name>Shale Xiong</name>
    </author>
    <link href="http://arxiv.org/abs/2211.16942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.16942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.08744v1</id>
    <updated>2023-02-17T07:50:35Z</updated>
    <published>2023-02-17T07:50:35Z</published>
    <title>Tensorized Optical Multimodal Fusion Network</title>
    <summary>  We propose the first tensorized optical multimodal fusion network
architecture with a self-attention mechanism and low-rank tensor fusion.
Simulation results show $51.3 \times$ less hardware requirement and $3.7\times
10^{13}$ MAC/J energy efficiency.
</summary>
    <author>
      <name>Yequan Zhao</name>
    </author>
    <author>
      <name>Xian Xiao</name>
    </author>
    <author>
      <name>Geza Kurczveil</name>
    </author>
    <author>
      <name>Raymond G. Beausoleil</name>
    </author>
    <author>
      <name>Zheng Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CLEO 2023 Novel Applications in Integrated Photonics</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.08744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.08744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.02936v1</id>
    <updated>2024-03-05T13:03:31Z</updated>
    <published>2024-03-05T13:03:31Z</published>
    <title>AdAM: Adaptive Fault-Tolerant Approximate Multiplier for Edge DNN
  Accelerators</title>
    <summary>  In this paper, we propose an architecture of a novel adaptive fault-tolerant
approximate multiplier tailored for ASIC-based DNN accelerators.
</summary>
    <author>
      <name>Mahdi Taheri</name>
    </author>
    <author>
      <name>Natalia Cherezova</name>
    </author>
    <author>
      <name>Samira Nazari</name>
    </author>
    <author>
      <name>Ahsan Rafiq</name>
    </author>
    <author>
      <name>Ali Azarpeyvand</name>
    </author>
    <author>
      <name>Tara Ghasempouri</name>
    </author>
    <author>
      <name>Masoud Daneshtalab</name>
    </author>
    <author>
      <name>Jaan Raik</name>
    </author>
    <author>
      <name>Maksim Jenihhin</name>
    </author>
    <link href="http://arxiv.org/abs/2403.02936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.02936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.11425v1</id>
    <updated>2024-05-19T02:11:30Z</updated>
    <published>2024-05-19T02:11:30Z</published>
    <title>Enabling full-speed random access to the entire memory on the A100 GPU</title>
    <summary>  We describe some features of the A100 memory architecture. In particular, we
give a technique to reverse-engineer some hardware layout information. Using
this information, we show how to avoid TLB issues to obtain full-speed random
HBM access to the entire memory, as long as we constrain any particular thread
to a reduced access window of less than 64GB.
</summary>
    <author>
      <name>Alden Walker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.11425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.11425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; B.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.00971v1</id>
    <updated>2024-12-01T21:24:23Z</updated>
    <published>2024-12-01T21:24:23Z</published>
    <title>Star arboricity relaxed book thickness of $K_n$</title>
    <summary>  A book embedding of the complete graph $K_n$ needs $\lceil \frac{n}{2}
\rceil$ pages and the page-subgraphs can be chosen to be spanning paths (for
$n$ even) and one spanning star for $n$ odd. We show that all page-subgraphs
can be chosen to be {\rm star forests} by including one extra {\rm cross-cap}
page or two new ordinary pages.
</summary>
    <author>
      <name>Paul C. Kainen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.00971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.00971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05C62, 05C15, 05C38, 05C12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9810011v1</id>
    <updated>1998-10-12T10:11:05Z</updated>
    <published>1998-10-12T10:11:05Z</published>
    <title>Flysig: Dataflow Oriented Delay-Insensitive Processor for Rapid
  Prototyping of Signal Processing</title>
    <summary>  As the one-chip integration of HW-modules designed by different companies
becomes more and more popular reliability of a HW-design and evaluation of the
timing behavior during the prototype stage are absolutely necessary. One way to
guarantee reliability is the use of robust design styles, e.g.,
delay-insensitivity. For early timing evaluation two aspects must be
considered: a) The timing needs to be proportional to technology variations and
b) the implemented architecture should be identical for prototype and target.
The first can be met also by delay-insensitive implementation. The latter one
is the key point. A unified architecture is needed for prototyping as well as
implementation. Our new approach to rapid prototyping of signal processing
tasks is based on a configurable, delay-insensitive implemented processor
called Flysig. In essence, the Flysig processor can be understood as a complex
FPGA where the CLBs are substituted by bit-serial operators. In this paper the
general concept is detailed and first experimental results are given for
demonstration of the main advantages: delay-insensitive design style, direct
correspondence between prototyping and target architecture, high performance
and reasonable shortening of the design cycle.
</summary>
    <author>
      <name>Wolfram Hardt</name>
    </author>
    <author>
      <name>Bernd Kleinjohann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IWRSP.1998.676682</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IWRSP.1998.676682" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nineth IEEE International Workshop on Rapid System Prototyping
  1998, Belgium, IEEE Computer Society Press</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9810011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9810011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; C.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0010007v1</id>
    <updated>2000-10-02T17:09:06Z</updated>
    <published>2000-10-02T17:09:06Z</published>
    <title>Towards a Theory of Cache-Efficient Algorithms</title>
    <summary>  We describe a model that enables us to analyze the running time of an
algorithm in a computer with a memory hierarchy with limited associativity, in
terms of various cache parameters. Our model, an extension of Aggarwal and
Vitter's I/O model, enables us to establish useful relationships between the
cache complexity and the I/O complexity of computations. As a corollary, we
obtain cache-optimal algorithms for some fundamental problems like sorting,
FFT, and an important subclass of permutations in the single-level cache model.
We also show that ignoring associativity concerns could lead to inferior
performance, by analyzing the average-case cache behavior of mergesort. We
further extend our model to multiple levels of cache with limited associativity
and present optimal algorithms for matrix transpose and sorting. Our techniques
may be used for systematic exploitation of the memory hierarchy starting from
the algorithm design stage, and dealing with the hitherto unresolved problem of
limited associativity.
</summary>
    <author>
      <name>Sandeep Sen</name>
    </author>
    <author>
      <name>Siddhartha Chatterjee</name>
    </author>
    <author>
      <name>Neeraj Dumir</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0010007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0010007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.3.2;C.0;F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0103025v1</id>
    <updated>2001-03-29T14:30:18Z</updated>
    <published>2001-03-29T14:30:18Z</published>
    <title>The Anatomy of the Grid - Enabling Scalable Virtual Organizations</title>
    <summary>  "Grid" computing has emerged as an important new field, distinguished from
conventional distributed computing by its focus on large-scale resource
sharing, innovative applications, and, in some cases, high-performance
orientation. In this article, we define this new field. First, we review the
"Grid problem," which we define as flexible, secure, coordinated resource
sharing among dynamic collections of individuals, institutions, and
resources-what we refer to as virtual organizations. In such settings, we
encounter unique authentication, authorization, resource access, resource
discovery, and other challenges. It is this class of problem that is addressed
by Grid technologies. Next, we present an extensible and open Grid
architecture, in which protocols, services, application programming interfaces,
and software development kits are categorized according to their roles in
enabling resource sharing. We describe requirements that we believe any such
mechanisms must satisfy, and we discuss the central role played by the
intergrid protocols that enable interoperability among different Grid systems.
Finally, we discuss how Grid technologies relate to other contemporary
technologies, including enterprise integration, application service provider,
storage service provider, and peer-to-peer computing. We maintain that Grid
concepts and technologies complement and have much to contribute to these other
approaches.
</summary>
    <author>
      <name>Ian Foster</name>
    </author>
    <author>
      <name>Carl Kesselman</name>
    </author>
    <author>
      <name>Steven Tuecke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0103025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0103025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.4; C.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0111029v1</id>
    <updated>2001-11-09T15:08:12Z</updated>
    <published>2001-11-09T15:08:12Z</published>
    <title>Versatile Data Acquisition and Controls for Epics Using Vme-Based Fpgas</title>
    <summary>  Field-Programmable Gate Arrays (FPGAs) have provided Thomas Jefferson
National Accelerator Facility (Jefferson Lab) with versatile VME-based data
acquisition and control interfaces with minimal development times. FPGA designs
have been used to interface to VME and provide control logic for numerous
systems. The building blocks of these logic designs can be tailored to the
individual needs of each system and provide system operators with read-backs
and controls via a VME interface to an EPICS based computer. This versatility
allows the system developer to choose components and define operating
parameters and options that are not readily available commercially. Jefferson
Lab has begun developing standard FPGA libraries that result in quick turn
around times and inexpensive designs.
</summary>
    <author>
      <name>T. Allison</name>
    </author>
    <author>
      <name>R. Flood</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, ICALEPCS 2001, T. Allison and R. Foold, Jefferson Lab</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">eConf C011127:TUAP053,2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0111029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0111029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.6.0;B.1.0;B.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0111030v1</id>
    <updated>2001-11-09T16:04:17Z</updated>
    <published>2001-11-09T16:04:17Z</published>
    <title>A Dual Digital Signal Processor VME Board For Instrumentation And
  Control Applications</title>
    <summary>  A Dual Digital Signal Processing VME Board was developed for the Continuous
Electron Beam Accelerator Facility (CEBAF) Beam Current Monitor (BCM) system at
Jefferson Lab. It is a versatile general-purpose digital signal processing
board using an open architecture, which allows for adaptation to various
applications. The base design uses two independent Texas Instrument (TI)
TMS320C6711, which are 900 MFLOPS floating-point digital signal processors
(DSP). Applications that require a fixed point DSP can be implemented by
replacing the baseline DSP with the pin-for-pin compatible TMS320C6211. The
design can be manufactured with a reduced chip set without redesigning the
printed circuit board. For example it can be implemented as a single-channel
DSP with no analog I/O.
</summary>
    <author>
      <name>H. Dong</name>
    </author>
    <author>
      <name>R. Flood</name>
    </author>
    <author>
      <name>C. Hovater</name>
    </author>
    <author>
      <name>J. Musson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 PDF pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">eConf C011127:THAP049,2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0111030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0111030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405015v1</id>
    <updated>2004-05-05T01:56:22Z</updated>
    <published>2004-05-05T01:56:22Z</published>
    <title>A High-Level Reconfigurable Computing Platform Software Frameworks</title>
    <summary>  Reconfigurable computing refers to the use of processors, such as Field
Programmable Gate Arrays (FPGAs), that can be modified at the hardware level to
take on different processing tasks. A reconfigurable computing platform
describes the hardware and software base on top of which modular extensions can
be created, depending on the desired application. Such reconfigurable computing
platforms can take on varied designs and implementations, according to the
constraints imposed and features desired by the scope of applications. This
paper introduces a PC-based reconfigurable computing platform software
frameworks that is flexible and extensible enough to abstract the different
hardware types and functionality that different PCs may have. The requirements
of the software platform, architectural issues addressed, rationale behind the
decisions made, and frameworks design implemented are discussed.
</summary>
    <author>
      <name>Darran Nathan</name>
    </author>
    <author>
      <name>Kelvin Lim Mun Kit</name>
    </author>
    <author>
      <name>Kelly Choo Hon Min</name>
    </author>
    <author>
      <name>Philip Wong Jit Chin</name>
    </author>
    <author>
      <name>Andreas Weisensee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0405015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0503066v1</id>
    <updated>2005-03-24T12:36:07Z</updated>
    <published>2005-03-24T12:36:07Z</published>
    <title>A Practical Approach for Circuit Routing on Dynamic Reconfigurable
  Devices</title>
    <summary>  Management of communication by on-line routing in new FPGAs with a large
amount of logic resources and partial reconfigurability is a new challenging
problem. A Network-on-Chip
  (NoC) typically uses packet routing mechanism, which has often unsafe data
transfers, and network interface overhead. In this paper, circuit routing for
such dynamic NoCs is investigated, and a practical 1-dimensional network with
an efficient routing algorithm is proposed and implemented. Also, this concept
has been extended to the 2-dimensional case. The implementation results show
the low area overhead and high performance of this network.
</summary>
    <author>
      <name>Ali Ahmadinia</name>
    </author>
    <author>
      <name>Christophe Bobda</name>
    </author>
    <author>
      <name>Ji Ding</name>
    </author>
    <author>
      <name>Mateusz Majer</name>
    </author>
    <author>
      <name>Juergen Teich</name>
    </author>
    <author>
      <name>Sandor P. Fekete</name>
    </author>
    <author>
      <name>Jan van der Veen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures, 2 tables, Latex, to appear in International
  Workshop on Rapid System Prototyping (RSP 2005)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0503066v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0503066v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.7; C.5; C.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505005v1</id>
    <updated>2005-05-02T01:10:04Z</updated>
    <published>2005-05-02T01:10:04Z</published>
    <title>Defragmenting the Module Layout of a Partially Reconfigurable Device</title>
    <summary>  Modern generations of field-programmable gate arrays (FPGAs) allow for
partial reconfiguration. In an online context, where the sequence of modules to
be loaded on the FPGA is unknown beforehand, repeated insertion and deletion of
modules leads to progressive fragmentation of the available space, making
defragmentation an important issue. We address this problem by propose an
online and an offline component for the defragmentation of the available space.
We consider defragmenting the module layout on a reconfigurable device. This
corresponds to solving a two-dimensional strip packing problem. Problems of
this type are NP-hard in the strong sense, and previous algorithmic results are
rather limited. Based on a graph-theoretic characterization of feasible
packings, we develop a method that can solve two-dimensional defragmentation
instances of practical size to optimality. Our approach is validated for a set
of benchmark instances.
</summary>
    <author>
      <name>Jan van der Veen</name>
    </author>
    <author>
      <name>Sandor P. Fekete</name>
    </author>
    <author>
      <name>Ali Ahmadinia</name>
    </author>
    <author>
      <name>Christophe Bobda</name>
    </author>
    <author>
      <name>Frank Hannig</name>
    </author>
    <author>
      <name>Juergen Teich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 11 figures, 1 table, Latex, to appear in "Engineering of
  Reconfigurable Systems and Algorithms" as a "Distinguished Paper"</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.7; C.5; C.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0508116v1</id>
    <updated>2005-08-25T19:04:22Z</updated>
    <published>2005-08-25T19:04:22Z</published>
    <title>Quantum Algorithm Processors to Reveal Hamiltonian Cycles</title>
    <summary>  Quantum computer versus quantum algorithm processor in CMOS are compared to
find (in parallel) all Hamiltonian cycles in a graph with m edges and n
vertices, each represented by k bits. A quantum computer uses quantum states
analogous to CMOS registers. With efficient initialization, number of CMOS
registers is proportional to (n-1)! Number of qubits in a quantum computer is
approximately proportional to kn+2mn in the approach below. Using CMOS, the
bits per register is about proportional to kn, which is less since bits can be
irreversibly reset. In either concept, number of gates, or operations to
identify Hamiltonian cycles is proportional to kmn. However, a quantum computer
needs an additional exponentially large number of operations to accomplish a
probabilistic readout. In contrast, CMOS is deterministic and readout is
comparable to ordinary memory.
</summary>
    <author>
      <name>John Robert Burger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0508116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0508116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.7.1; C.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0510039v1</id>
    <updated>2005-10-14T22:03:45Z</updated>
    <published>2005-10-14T22:03:45Z</published>
    <title>DyNoC: A Dynamic Infrastructure for Communication in Dynamically
  Reconfigurable Devices</title>
    <summary>  A new paradigm to support the communication among modules dynamically placed
on a reconfigurable device at run-time is presented. Based on the network on
chip (NoC) infrastructure, we developed a dynamic communication infrastructure
as well as routing methodologies capable to handle routing in a NoC with
obstacles created by dynamically placed components. We prove the unrestricted
reachability of components and pins, the deadlock-freeness and we finally show
the feasibility of our approach by means on real life example applications.
</summary>
    <author>
      <name>Christophe Bobda</name>
    </author>
    <author>
      <name>Ali Ahmadinia</name>
    </author>
    <author>
      <name>Mateusz Majer</name>
    </author>
    <author>
      <name>Juergen Teich</name>
    </author>
    <author>
      <name>Sandor P. Fekete</name>
    </author>
    <author>
      <name>Jan van der Veen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures, 1 table, Latex, to appear in 15th International
  Conference on Field-Programmable Logic and Application</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0510039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0510039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.7; C.5; C.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603091v1</id>
    <updated>2006-03-23T06:44:34Z</updated>
    <published>2006-03-23T06:44:34Z</published>
    <title>A New Reversible TSG Gate and Its Application For Designing Efficient
  Adder Circuits</title>
    <summary>  In the recent years, reversible logic has emerged as a promising technology
having its applications in low power CMOS, quantum computing, nanotechnology,
and optical computing. The classical set of gates such as AND, OR, and EXOR are
not reversible. This paper proposes a new 4 * 4 reversible gate called TSG
gate. The proposed gate is used to design efficient adder units. The most
significant aspect of the proposed gate is that it can work singly as a
reversible full adder i.e reversible full adder can now be implemented with a
single gate only. The proposed gate is then used to design reversible ripple
carry and carry skip adders. It is demonstrated that the adder architectures
designed using the proposed gate are much better and optimized, compared to
their existing counterparts in literature; in terms of number of reversible
gates and garbage outputs. Thus, this paper provides the initial threshold to
building of more complex system which can execute more complicated operations
using reversible logic.
</summary>
    <author>
      <name>Himanshu Thapliyal</name>
    </author>
    <author>
      <name>M. B Srinivas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages: Published in 7th International Symposium on Representations
  and Methodology of Future Computing Technologies(RM 2005), Tokyo, Japan,
  September 5-6, 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0603091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605039v4</id>
    <updated>2006-05-29T06:33:06Z</updated>
    <published>2006-05-09T05:45:52Z</published>
    <title>Fast and Generalized Polynomial Time Memory Consistency Verification</title>
    <summary>  The problem of verifying multi-threaded execution against the memory
consistency model of a processor is known to be an NP hard problem. However
polynomial time algorithms exist that detect almost all failures in such
execution. These are often used in practice for microprocessor verification. We
present a low complexity and fully parallelized algorithm to check program
execution against the processor consistency model. In addition our algorithm is
general enough to support a number of consistency models without any
degradation in performance. An implementation of this algorithm is currently
used in practice to verify processors in the post silicon stage for multiple
architectures.
</summary>
    <author>
      <name>Amitabha Roy</name>
    </author>
    <author>
      <name>Stephan Zeisset</name>
    </author>
    <author>
      <name>Charles J. Fleckenstein</name>
    </author>
    <author>
      <name>John C. Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the proceedings of Computer Aided Verification (CAV)
  2006</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0605039v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605039v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605143v1</id>
    <updated>2006-05-30T15:07:51Z</updated>
    <published>2006-05-30T15:07:51Z</published>
    <title>High-level synthesis under I/O Timing and Memory constraints</title>
    <summary>  The design of complex Systems-on-Chips implies to take into account
communication and memory access constraints for the integration of dedicated
hardware accelerator. In this paper, we present a methodology and a tool that
allow the High-Level Synthesis of DSP algorithm, under both I/O timing and
memory constraints. Based on formal models and a generic architecture, this
tool helps the designer to find a reasonable trade-off between both the
required I/O timing behavior and the internal memory access parallelism of the
circuit. The interest of our approach is demonstrated on the case study of a
FFT algorithm.
</summary>
    <author>
      <name>Philippe Coussy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Gwenolé Corre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Bomel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Senn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Symposium on Circuits And Systems (2005) 680-683</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0605143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605146v1</id>
    <updated>2006-05-30T15:10:57Z</updated>
    <published>2006-05-30T15:10:57Z</published>
    <title>Synthèse Comportementale Sous Contraintes de Communication et de
  Placement Mémoire pour les composants du TDSI</title>
    <summary>  The design of complex Digital Signal Processing systems implies to minimize
architectural cost and to maximize timing performances while taking into
account communication and memory accesses constraints for the integration of
dedicated hardware accelerator. Unfortunately, the traditional Matlab/ Simulink
design flows gather not very flexible hardware blocs. In this paper, we present
a methodology and a tool that permit the High-Level Synthesis of DSP
applications, under both I/O timing and memory constraints. Based on formal
models and a generic architecture, our tool GAUT helps the designer in finding
a reasonable trade-off between the circuit's performance and its architectural
complexity. The efficiency of our approach is demonstrated on the case study of
a FFT algorithm.
</summary>
    <author>
      <name>Gwenolé Corre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Coussy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Bomel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Senn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">GRETSI'05 (Colloque sur le Traitement du Signal et de l'Image),
  Belgique (2005) 779-782</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0605146v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605146v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0609028v1</id>
    <updated>2006-09-07T14:18:41Z</updated>
    <published>2006-09-07T14:18:41Z</published>
    <title>VLSI Implementation of RSA Encryption System Using Ancient Indian Vedic
  Mathematics</title>
    <summary>  This paper proposes the hardware implementation of RSA encryption/decryption
algorithm using the algorithms of Ancient Indian Vedic Mathematics that have
been modified to improve performance. The recently proposed hierarchical
overlay multiplier architecture is used in the RSA circuitry for multiplication
operation. The most significant aspect of the paper is the development of a
division architecture based on Straight Division algorithm of Ancient Indian
Vedic Mathematics and embedding it in RSA encryption/decryption circuitry for
improved efficiency. The coding is done in Verilog HDL and the FPGA synthesis
is done using Xilinx Spartan library. The results show that RSA circuitry
implemented using Vedic division and multiplication is efficient in terms of
area/speed compared to its implementation using conventional multiplication and
division architectures
</summary>
    <author>
      <name>Himanshu Thapliyal</name>
    </author>
    <author>
      <name>M. B Srinivas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages: Proceedings of SPIE -- Volume 5837 VLSI Circuits and Systems
  II, Jose F. Lopez, Francisco V. Fernandez, Jose Maria Lopez-Villegas, Jose M.
  de la Rosa, Editors, June 2005, pp. 888-892</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0609028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0609028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0609029v1</id>
    <updated>2006-09-07T14:25:21Z</updated>
    <published>2006-09-07T14:25:21Z</published>
    <title>Reversible Programmable Logic Array (RPLA) using Fredkin &amp; Feynman Gates
  for Industrial Electronics and Applications</title>
    <summary>  In recent years, reversible logic has emerged as a promising computing
paradigm having application in low power CMOS, quantum computing,
nanotechnology, and optical computing. The classical set of gates such as AND,
OR, and EXOR are not reversible. In this paper, the authors have proposed
reversible programmable logic array (RPLA) architecture using reversible
Fredkin and Feynman gates. The proposed RPLA has n inputs and m outputs and can
realize m functions of n variables. In order to demonstrate the design of RPLA,
a 3 input RPLA is designed which can perform any 28 functions using the
combination of 8 min terms (23). Furthermore, the application of the designed 3
input RPLA is shown by implementing the full adder and full subtractor
functions through it.
</summary>
    <author>
      <name>Himanshu Thapliyal</name>
    </author>
    <author>
      <name>Hamid R. Arabnia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Proceedings of the International Conference on Embedded
  Systems and Applications(ESA'06),Las Vegas, U.S.A, June 2006(CSREA Press)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0609029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0609029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0609036v1</id>
    <updated>2006-09-08T05:36:20Z</updated>
    <published>2006-09-08T05:36:20Z</published>
    <title>Reduced Area Low Power High Throughput BCD Adders for IEEE 754r Format</title>
    <summary>  IEEE 754r is the ongoing revision to the IEEE 754 floating point standard and
a major enhancement to the standard is the addition of decimal format. Firstly,
this paper proposes novel two transistor AND and OR gates. The proposed AND
gate has no power supply, thus it can be referred as the Powerless AND gate.
Similarly, the proposed two transistor OR gate has no ground and can be
referred as Groundless OR. Secondly for IEEE 754r format, two novel BCD adders
called carry skip and carry look-ahead BCD adders are also proposed in this
paper. In order to design the carry look-ahead BCD adder, a novel 4 bit carry
look-ahead adder called NCLA is proposed which forms the basic building block
of the proposed carry look-ahead BCD adder. Finally, the proposed two
transistors AND and OR gates are used to provide the optimized small area low
power high throughput circuitries of the proposed BCD adders.
</summary>
    <author>
      <name>Himanshu Thapliyal</name>
    </author>
    <author>
      <name>Hamid R. Arabnia</name>
    </author>
    <author>
      <name>M. B Srinivas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages;Published in Proceedings of the 11th International CSI
  Computer Conference (CSICC'06), Tehran, Jan 24-26, 2006, pp.59-64</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0609036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0609036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.1692v1</id>
    <updated>2007-06-12T13:45:50Z</updated>
    <published>2007-06-12T13:45:50Z</published>
    <title>A Methodology for Efficient Space-Time Adapter Design Space Exploration:
  A Case Study of an Ultra Wide Band Interleaver</title>
    <summary>  This paper presents a solution to efficiently explore the design space of
communication adapters. In most digital signal processing (DSP) applications,
the overall architecture of the system is significantly affected by
communication architecture, so the designers need specifically optimized
adapters. By explicitly modeling these communications within an effective
graph-theoretic model and analysis framework, we automatically generate an
optimized architecture, named Space-Time AdapteR (STAR). Our design flow inputs
a C description of Input/Output data scheduling, and user requirements
(throughput, latency, parallelism...), and formalizes communication constraints
through a Resource Constraints Graph (RCG). The RCG properties enable an
efficient architecture space exploration in order to synthesize a STAR
component. The proposed approach has been tested to design an industrial data
mixing block example: an Ultra-Wideband interleaver.
</summary>
    <author>
      <name>Cyrille Chavet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER, STM</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Coussy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Pascal Urard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">STM</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISBN:1-4244-0921-7</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the IEEE International Symposium on Circuits and
  Systems (ISCAS) (28/05/2007) 2946</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0706.1692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.1692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.2732v1</id>
    <updated>2007-06-19T14:18:57Z</updated>
    <published>2007-06-19T14:18:57Z</published>
    <title>A Design Methodology for Space-Time Adapter</title>
    <summary>  This paper presents a solution to efficiently explore the design space of
communication adapters. In most digital signal processing (DSP) applications,
the overall architecture of the system is significantly affected by
communication architecture, so the designers need specifically optimized
adapters. By explicitly modeling these communications within an effective
graph-theoretic model and analysis framework, we automatically generate an
optimized architecture, named Space-Time AdapteR (STAR). Our design flow inputs
a C description of Input/Output data scheduling, and user requirements
(throughput, latency, parallelism...), and formalizes communication constraints
through a Resource Constraints Graph (RCG). The RCG properties enable an
efficient architecture space exploration in order to synthesize a STAR
component. The proposed approach has been tested to design an industrial data
mixing block example: an Ultra-Wideband interleaver.
</summary>
    <author>
      <name>Cyrille Chavet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Coussy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Pascal Urard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">STM</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISBN : 978-1-59593-606-8</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2007 ACM Great Lakes Symposium on VLSI
  (12/03/2007) 347</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0706.2732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.2732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.3009v1</id>
    <updated>2007-06-20T15:19:01Z</updated>
    <published>2007-06-20T15:19:01Z</published>
    <title>Application of a design space exploration tool to enhance interleaver
  generation</title>
    <summary>  This paper presents a methodology to efficiently explore the design space of
communication adapters. In most digital signal processing (DSP) applications,
the overall performance of the system is significantly affected by
communication architectures, as a consequence the designers need specifically
optimized adapters. By explicitly modeling these communications within an
effective graph-theoretic model and analysis framework, we automatically
generate an optimized architecture, named Space-Time AdapteR (STAR). Our design
flow inputs a C description of Input/Output data scheduling, and user
requirements (throughput, latency, parallelism...), and formalizes
communication constraints through a Resource Constraints Graph (RCG). Design
space exploration is then performed through associated tools, to synthesize a
STAR component under time-to-market constraints. The proposed approach has been
tested to design an industrial data mixing block example: an Ultra-Wideband
interleaver.
</summary>
    <author>
      <name>Cyrille Chavet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER, STM</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Coussy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Pascal Urard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">STM</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the European Signal Processing Conference
  (EUSIPCO-2007) (03/09/2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0706.3009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.3009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.1962v1</id>
    <updated>2007-08-14T21:41:55Z</updated>
    <published>2007-08-14T21:41:55Z</published>
    <title>Exact Cover with light</title>
    <summary>  We suggest a new optical solution for solving the YES/NO version of the Exact
Cover problem by using the massive parallelism of light. The idea is to build
an optical device which can generate all possible solutions of the problem and
then to pick the correct one. In our case the device has a graph-like
representation and the light is traversing it by following the routes given by
the connections between nodes. The nodes are connected by arcs in a special way
which lets us to generate all possible covers (exact or not) of the given set.
For selecting the correct solution we assign to each item, from the set to be
covered, a special integer number. These numbers will actually represent delays
induced to light when it passes through arcs. The solution is represented as a
subray arriving at a certain moment in the destination node. This will tell us
if an exact cover does exist or not.
</summary>
    <author>
      <name>Mihai Oltean</name>
    </author>
    <author>
      <name>Oana Muntean</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00354-008-0049-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00354-008-0049-5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 4 figures, New Generation Computing, accepted, 2007</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">New Generation Computing, Springer-Verlag, Vol. 26, Issue 4, pp.
  327-344, 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0708.1962v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.1962v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.1964v1</id>
    <updated>2007-08-14T21:46:32Z</updated>
    <published>2007-08-14T21:46:32Z</published>
    <title>Solving the subset-sum problem with a light-based device</title>
    <summary>  We propose a special computational device which uses light rays for solving
the subset-sum problem. The device has a graph-like representation and the
light is traversing it by following the routes given by the connections between
nodes. The nodes are connected by arcs in a special way which lets us to
generate all possible subsets of the given set. To each arc we assign either a
number from the given set or a predefined constant. When the light is passing
through an arc it is delayed by the amount of time indicated by the number
placed in that arc. At the destination node we will check if there is a ray
whose total delay is equal to the target value of the subset sum problem (plus
some constants).
</summary>
    <author>
      <name>Mihai Oltean</name>
    </author>
    <author>
      <name>Oana Muntean</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11047-007-9059-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11047-007-9059-3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures, Natural Computing, 2007</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Natural Computing, Springer-Verlag, Vol 8, Issue 2, pp. 321-331,
  2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0708.1964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.1964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.3789v1</id>
    <updated>2007-10-19T20:59:53Z</updated>
    <published>2007-10-19T20:59:53Z</published>
    <title>Frequency Analysis of Decoupling Capacitors for Three Voltage Supplies
  in SoC</title>
    <summary>  Reduction in power consumption has become a major criterion of design in
modern ICs. One such scheme to reduce power consumption by an IC is the use of
multiple power supplies for critical and non-critical paths. To maintain the
impedance of a power distribution system below a specified level, multiple
decoupling capacitors are placed at different levels of power grid hierarchy.
This paper describes about three-voltage supply power distribution systems. The
noise at one power supply can propagate to the other power supply, causing
power and signal integrity problems in the overall system. Effects such as
anti-resonance and remedies for these effects are studied. Impedance of the
three-voltage supply power distribution system is calculated in terms of
RLC-model of decoupling capacitors. Further the obtained impedance depends on
the frequency; hence brief frequency analysis of impedance is done.
</summary>
    <author>
      <name>Mohd Abubakr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 9 figures, Submitted to ICCSC 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/0710.3789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.3789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4630v1</id>
    <updated>2007-10-25T08:07:11Z</updated>
    <published>2007-10-25T08:07:11Z</published>
    <title>CAFFEINE: Template-Free Symbolic Model Generation of Analog Circuits via
  Canonical Form Functions and Genetic Programming</title>
    <summary>  This paper presents a method to automatically generate compact symbolic
performance models of analog circuits with no prior specification of an
equation template. The approach takes SPICE simulation data as input, which
enables modeling of any nonlinear circuits and circuit characteristics. Genetic
programming is applied as a means of traversing the space of possible symbolic
expressions. A grammar is specially designed to constrain the search to a
canonical form for functions. Novel evolutionary search operators are designed
to exploit the structure of the grammar. The approach generates a set of
symbolic models which collectively provide a tradeoff between error and model
complexity. Experimental results show that the symbolic models generated are
compact and easy to understand, making this an effective method for aiding
understanding in analog design. The models also demonstrate better prediction
quality than posynomials.
</summary>
    <author>
      <name>Trent Mcconaghy</name>
    </author>
    <author>
      <name>Tom Eeckelaert</name>
    </author>
    <author>
      <name>Georges Gielen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4630v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4630v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4638v1</id>
    <updated>2007-10-25T08:10:40Z</updated>
    <published>2007-10-25T08:10:40Z</published>
    <title>Buffer Insertion for Bridges and Optimal Buffer Sizing for Communication
  Sub-System of Systems-on-Chip</title>
    <summary>  We have presented an optimal buffer sizing and buffer insertion methodology
which uses stochastic models of the architecture and Continuous Time Markov
Decision Processes CTMDPs. Such a methodology is useful in managing the scarce
buffer resources available on chip as compared to network based data
communication which can have large buffer space. The modeling of this problem
in terms of a CT-MDP framework lead to a nonlinear formulation due to usage of
bridges in the bus architecture. We present a methodology to split the problem
into several smaller though linear systems and we then solve these subsystems.
</summary>
    <author>
      <name>Sankalp S. Kallakuri</name>
    </author>
    <author>
      <name>Alex Doboli</name>
    </author>
    <author>
      <name>Eugene A. Feinberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4638v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4638v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4639v1</id>
    <updated>2007-10-25T08:11:06Z</updated>
    <published>2007-10-25T08:11:06Z</published>
    <title>Modeling the Non-Linear Behavior of Library Cells for an Accurate Static
  Noise Analysis</title>
    <summary>  In signal integrity analysis, the joint effect of propagated noise through
library cells, and of the noise injected on a quiet net by neighboring
switching nets through coupling capacitances, must be considered in order to
accurately estimate the overall noise impact on design functionality and
performances. In this work the impact of the cell non-linearity on the noise
glitch waveform is analyzed in detail, and a new macromodel that allows to
accurately and efficiently modeling the non-linear effects of the victim driver
in noise analysis is presented. Experimental results demonstrate the
effectiveness of our method, and confirm that existing noise analysis
approaches based on linear superposition of the propagated and
crosstalk-injected noise can be highly inaccurate, thus impairing the sign-off
functional verification phase.
</summary>
    <author>
      <name>Cristiano Forzan</name>
    </author>
    <author>
      <name>Davide Pandini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4643v1</id>
    <updated>2007-10-25T08:14:40Z</updated>
    <published>2007-10-25T08:14:40Z</published>
    <title>Generic Pipelined Processor Modeling and High Performance Cycle-Accurate
  Simulator Generation</title>
    <summary>  Detailed modeling of processors and high performance cycle-accurate
simulators are essential for today's hardware and software design. These
problems are challenging enough by themselves and have seen many previous
research efforts. Addressing both simultaneously is even more challenging, with
many existing approaches focusing on one over another. In this paper, we
propose the Reduced Colored Petri Net (RCPN) model that has two advantages:
first, it offers a very simple and intuitive way of modeling pipelined
processors; second, it can generate high performance cycle-accurate simulators.
RCPN benefits from all the useful features of Colored Petri Nets without
suffering from their exponential growth in complexity. RCPN processor models
are very intuitive since they are a mirror image of the processor pipeline
block diagram. Furthermore, in our experiments on the generated cycle-accurate
simulators for XScale and StrongArm processor models, we achieved an order of
magnitude (~15 times) speedup over the popular SimpleScalar ARM simulator.
</summary>
    <author>
      <name>Mehrdad Reshadi</name>
    </author>
    <author>
      <name>Nikil Dutt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DATE.2005.166</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DATE.2005.166" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4644v1</id>
    <updated>2007-10-25T08:18:14Z</updated>
    <published>2007-10-25T08:18:14Z</published>
    <title>Cycle Accurate Binary Translation for Simulation Acceleration in Rapid
  Prototyping of SoCs</title>
    <summary>  In this paper, the application of a cycle accurate binary translator for
rapid prototyping of SoCs will be presented. This translator generates code to
run on a rapid prototyping system consisting of a VLIW processor and FPGAs. The
generated code is annotated with information that triggers cycle generation for
the hardware in parallel to the execution of the translated program. The VLIW
processor executes the translated program whereas the FPGAs contain the
hardware for the parallel cycle generation and the bus interface that adapts
the bus of the VLIW processor to the SoC bus of the emulated processor core.
</summary>
    <author>
      <name>Jurgen Schnerr</name>
    </author>
    <author>
      <name>Oliver Bringmann</name>
    </author>
    <author>
      <name>Wolfgang Rosenstiel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4649v1</id>
    <updated>2007-10-25T08:24:02Z</updated>
    <published>2007-10-25T08:24:02Z</published>
    <title>Stochastic Power Grid Analysis Considering Process Variations</title>
    <summary>  In this paper, we investigate the impact of interconnect and device process
variations on voltage fluctuations in power grids. We consider random
variations in the power grid's electrical parameters as spatial stochastic
processes and propose a new and efficient method to compute the stochastic
voltage response of the power grid. Our approach provides an explicit
analytical representation of the stochastic voltage response using orthogonal
polynomials in a Hilbert space. The approach has been implemented in a
prototype software called OPERA (Orthogonal Polynomial Expansions for Response
Analysis). Use of OPERA on industrial power grids demonstrated speed-ups of up
to two orders of magnitude. The results also show a significant variation of
about $\pm$ 35% in the nominal voltage drops at various nodes of the power
grids and demonstrate the need for variation-aware power grid analysis.
</summary>
    <author>
      <name>Praveen Ghanta</name>
    </author>
    <author>
      <name>Sarma Vrudhula</name>
    </author>
    <author>
      <name>Rajendran Panda</name>
    </author>
    <author>
      <name>Janet Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4652v1</id>
    <updated>2007-10-25T08:31:15Z</updated>
    <published>2007-10-25T08:31:15Z</published>
    <title>Locality-Aware Process Scheduling for Embedded MPSoCs</title>
    <summary>  Utilizing on-chip caches in embedded multiprocessor-system-on-a-chip (MPSoC)
based systems is critical from both performance and power perspectives. While
most of the prior work that targets at optimizing cache behavior are performed
at hardware and compilation levels, operating system (OS) can also play major
role as it sees the global access pattern information across applications. This
paper proposes a cache-conscious OS process scheduling strategy based on data
reuse. The proposed scheduler implements two complementary approaches. First,
the processes that do not share any data between them are scheduled at
different cores if it is possible to do so. Second, the processes that could
not be executed at the same time (due to dependences) but share data among each
other are mapped to the same processor core so that they share the cache
contents. Our experimental results using this new data locality aware OS
scheduling strategy are promising, and show significant improvements in task
completion times.
</summary>
    <author>
      <name>Mahmut Kandemir</name>
    </author>
    <author>
      <name>Guilin Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4653v1</id>
    <updated>2007-10-25T08:32:08Z</updated>
    <published>2007-10-25T08:32:08Z</published>
    <title>Simultaneous Reduction of Dynamic and Static Power in Scan Structures</title>
    <summary>  Power dissipation during test is a major challenge in testing integrated
circuits. Dynamic power has been the dominant part of power dissipation in CMOS
circuits, however, in future technologies the static portion of power
dissipation will outreach the dynamic portion. This paper proposes an efficient
technique to reduce both dynamic and static power dissipation in scan
structures. Scan cell outputs which are not on the critical path(s) are
multiplexed to fixed values during scan mode. These constant values and primary
inputs are selected such that the transitions occurred on non-multiplexed scan
cells are suppressed and the leakage current during scan mode is decreased. A
method for finding these vectors is also proposed. Effectiveness of this
technique is proved by experiments performed on ISCAS89 benchmark circuits.
</summary>
    <author>
      <name>Shervin Sharifi</name>
    </author>
    <author>
      <name>Javid Jaffari</name>
    </author>
    <author>
      <name>Mohammad Hosseinabady</name>
    </author>
    <author>
      <name>Ali Afzali-Kusha</name>
    </author>
    <author>
      <name>Zainalabedin Navabi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4654v1</id>
    <updated>2007-10-25T08:33:14Z</updated>
    <published>2007-10-25T08:33:14Z</published>
    <title>Modeling Interconnect Variability Using Efficient Parametric Model Order
  Reduction</title>
    <summary>  Assessing IC manufacturing process fluctuations and their impacts on IC
interconnect performance has become unavoidable for modern DSM designs.
However, the construction of parametric interconnect models is often hampered
by the rapid increase in computational cost and model complexity. In this paper
we present an efficient yet accurate parametric model order reduction algorithm
for addressing the variability of IC interconnect performance. The efficiency
of the approach lies in a novel combination of low-rank matrix approximation
and multi-parameter moment matching. The complexity of the proposed parametric
model order reduction is as low as that of a standard Krylov subspace method
when applied to a nominal system. Under the projection-based framework, our
algorithm also preserves the passivity of the resulting parametric models.
</summary>
    <author>
      <name>Peng Li</name>
    </author>
    <author>
      <name>Frank Liu</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Lawrence T. Pileggi</name>
    </author>
    <author>
      <name>Sani R. Nassif</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4656v1</id>
    <updated>2007-10-25T08:34:32Z</updated>
    <published>2007-10-25T08:34:32Z</published>
    <title>A Memory Hierarchical Layer Assigning and Prefetching Technique to
  Overcome the Memory Performance/Energy Bottleneck</title>
    <summary>  The memory subsystem has always been a bottleneck in performance as well as
significant power contributor in memory intensive applications. Many
researchers have presented multi-layered memory hierarchies as a means to
design energy and performance efficient systems. However, most of the previous
work do not explore trade-offs systematically. We fill this gap by proposing a
formalized technique that takes into consideration data reuse, limited lifetime
of the arrays of an application and application specific prefetching
opportunities, and performs a thorough trade-off exploration for different
memory layer sizes. This technique has been implemented on a prototype tool,
which was tested successfully using nine real-life applications of industrial
relevance. Following this approach we have able to reduce execution time up to
60%, and energy consumption up to 70%.
</summary>
    <author>
      <name>Minas Dasygenis</name>
    </author>
    <author>
      <name>Erik Brockmeyer</name>
    </author>
    <author>
      <name>Bart Durinck</name>
    </author>
    <author>
      <name>Francky Catthoor</name>
    </author>
    <author>
      <name>Dimitrios Soudris</name>
    </author>
    <author>
      <name>Antonios Thanailakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4660v1</id>
    <updated>2007-10-25T08:36:55Z</updated>
    <published>2007-10-25T08:36:55Z</published>
    <title>Thermal-Aware Task Allocation and Scheduling for Embedded Systems</title>
    <summary>  Temperature affects not only the reliability but also the performance, power,
and cost of the embedded system. This paper proposes a thermal-aware task
allocation and scheduling algorithm for embedded systems. The algorithm is used
as a sub-routine for hardware/software co-synthesis to reduce the peak
temperature and achieve a thermally even distribution while meeting real time
constraints. The paper investigates both power-aware and thermal-aware
approaches to task allocation and scheduling. The experimental results show
that the thermal-aware approach outperforms the power-aware schemes in terms of
maximal and average temperature reductions. To the best of our knowledge, this
is the first task allocation and scheduling algorithm that takes temperature
into consideration.
</summary>
    <author>
      <name>W. -L. Hung</name>
    </author>
    <author>
      <name>Y. Xie</name>
    </author>
    <author>
      <name>N. Vijaykrishnan</name>
    </author>
    <author>
      <name>M. Kandemir</name>
    </author>
    <author>
      <name>M. J. Irwin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4663v1</id>
    <updated>2007-10-25T08:41:06Z</updated>
    <published>2007-10-25T08:41:06Z</published>
    <title>Statistical Modeling of Pipeline Delay and Design of Pipeline under
  Process Variation to Enhance Yield in sub-100nm Technologies</title>
    <summary>  Operating frequency of a pipelined circuit is determined by the delay of the
slowest pipeline stage. However, under statistical delay variation in sub-100nm
technology regime, the slowest stage is not readily identifiable and the
estimation of the pipeline yield with respect to a target delay is a
challenging problem. We have proposed analytical models to estimate yield for a
pipelined design based on delay distributions of individual pipe stages. Using
the proposed models, we have shown that change in logic depth and imbalance
between the stage delays can improve the yield of a pipeline. A statistical
methodology has been developed to optimally design a pipeline circuit for
enhancing yield. Optimization results show that, proper imbalance among the
stage delays in a pipeline improves design yield by 9% for the same area and
performance (and area reduction by about 8.4% under a yield constraint) over a
balanced design.
</summary>
    <author>
      <name>Animesh Datta</name>
    </author>
    <author>
      <name>Swarup Bhunia</name>
    </author>
    <author>
      <name>Saibal Mukhopadhyay</name>
    </author>
    <author>
      <name>Nilanjan Banerjee</name>
    </author>
    <author>
      <name>Kaushik Roy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4665v1</id>
    <updated>2007-10-25T08:43:31Z</updated>
    <published>2007-10-25T08:43:31Z</published>
    <title>New Perspectives and Opportunities From the Wild West of Microelectronic
  Biochips</title>
    <summary>  Application of Microelectronic to bioanalysis is an emerging field which
holds great promise. From the standpoint of electronic and system design,
biochips imply a radical change of perspective, since new, completely different
constraints emerge while other usual constraints can be relaxed. While
electronic parts of the system can rely on the usual established design-flow,
fluidic and packaging design, calls for a new approach which relies
significantly on experiments. We hereby make some general considerations based
on our experience in the development of biochips for cell analysis.
</summary>
    <author>
      <name>Nicolo Manaresi</name>
    </author>
    <author>
      <name>Gianni Medoro</name>
    </author>
    <author>
      <name>Melanie Abonnenc</name>
    </author>
    <author>
      <name>Vincent Auger</name>
    </author>
    <author>
      <name>Paul Vulto</name>
    </author>
    <author>
      <name>Aldo Romani</name>
    </author>
    <author>
      <name>Luigi Altomare</name>
    </author>
    <author>
      <name>Marco Tartagni</name>
    </author>
    <author>
      <name>Roberto Guerrieri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4673v1</id>
    <updated>2007-10-25T08:48:12Z</updated>
    <published>2007-10-25T08:48:12Z</published>
    <title>Design of Fault-Tolerant and Dynamically-Reconfigurable Microfluidic
  Biochips</title>
    <summary>  Microfluidics-based biochips are soon expected to revolutionize clinical
diagnosis, DNA sequencing, and other laboratory procedures involving molecular
biology. Most microfluidic biochips are based on the principle of continuous
fluid flow and they rely on permanently-etched microchannels, micropumps, and
microvalves. We focus here on the automated design of "digital" droplet-based
microfluidic biochips. In contrast to continuous-flow systems, digital
microfluidics offers dynamic reconfigurability; groups of cells in a
microfluidics array can be reconfigured to change their functionality during
the concurrent execution of a set of bioassays. We present a simulated
annealing-based technique for module placement in such biochips. The placement
procedure not only addresses chip area, but it also considers fault tolerance,
which allows a microfluidic module to be relocated elsewhere in the system when
a single cell is detected to be faulty. Simulation results are presented for a
case study involving the polymerase chain reaction.
</summary>
    <author>
      <name>Fei Su</name>
    </author>
    <author>
      <name>Krishnendu Chakrabarty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4684v1</id>
    <updated>2007-10-25T09:07:44Z</updated>
    <published>2007-10-25T09:07:44Z</published>
    <title>Reliability-Centric High-Level Synthesis</title>
    <summary>  Importance of addressing soft errors in both safety critical applications and
commercial consumer products is increasing, mainly due to ever shrinking
geometries, higher-density circuits, and employment of power-saving techniques
such as voltage scaling and component shut-down. As a result, it is becoming
necessary to treat reliability as a first-class citizen in system design. In
particular, reliability decisions taken early in system design can have
significant benefits in terms of design quality. Motivated by this observation,
this paper presents a reliability-centric high-level synthesis approach that
addresses the soft error problem. The proposed approach tries to maximize
reliability of the design while observing the bounds on area and performance,
and makes use of our reliability characterization of hardware components such
as adders and multipliers. We implemented the proposed approach, performed
experiments with several designs, and compared the results with those obtained
by a prior proposal.
</summary>
    <author>
      <name>S. Tosun</name>
    </author>
    <author>
      <name>N. Mansouri</name>
    </author>
    <author>
      <name>E. Arvas</name>
    </author>
    <author>
      <name>M. Kandemir</name>
    </author>
    <author>
      <name>Yuan Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4685v1</id>
    <updated>2007-10-25T09:08:39Z</updated>
    <published>2007-10-25T09:08:39Z</published>
    <title>Reliable System Specification for Self-Checking Data-Paths</title>
    <summary>  The design of reliable circuits has received a lot of attention in the past,
leading to the definition of several design techniques introducing fault
detection and fault tolerance properties in systems for critical
applications/environments. Such design methodologies tackled the problem at
different abstraction levels, from switch-level to logic, RT level, and more
recently to system level. Aim of this paper is to introduce a novel
system-level technique based on the redefinition of the operators functionality
in the system specification. This technique provides reliability properties to
the system data path, transparently with respect to the designer. Feasibility,
fault coverage, performance degradation and overheads are investigated on a FIR
circuit.
</summary>
    <author>
      <name>C. Bolchini</name>
    </author>
    <author>
      <name>F. Salice</name>
    </author>
    <author>
      <name>D. Sciuto</name>
    </author>
    <author>
      <name>L. Pomante</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4686v1</id>
    <updated>2007-10-25T09:08:40Z</updated>
    <published>2007-10-25T09:08:40Z</published>
    <title>Test Planning for Mixed-Signal SOCs with Wrapped Analog Cores</title>
    <summary>  Many SOCs today contain both digital and analog embedded cores. Even though
the test cost for such mixed-signal SOCs is significantly higher than that for
digital SOCs, most prior research in this area has focused exclusively on
digital cores. We propose a low-cost test development methodology for
mixed-signal SOCs that allows the analog and digital cores to be tested in a
unified manner, thereby minimizing the overall test cost. The analog cores in
the SOC are wrapped such that they can be accessed using a digital test access
mechanism (TAM). We evaluate the impact of the use of analog test wrappers on
area overhead and test time. To reduce area overhead, we present an analog test
wrapper optimization technique, which is then combined with TAM optimization in
a cost-oriented heuristic approach for test scheduling. We also demonstrate the
feasibility of using analog wrappers by presenting transistor-level simulations
for an analog wrapper and a representative core. We present experimental
results on test scheduling for an ITC'02 benchmark SOC that has been augmented
with five analog cores.
</summary>
    <author>
      <name>Anuja Sehgal</name>
    </author>
    <author>
      <name>Fang Liu</name>
    </author>
    <author>
      <name>Sule Ozev</name>
    </author>
    <author>
      <name>Krishnendu Chakrabarty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4687v1</id>
    <updated>2007-10-25T09:09:14Z</updated>
    <published>2007-10-25T09:09:14Z</published>
    <title>On-Chip Test Infrastructure Design for Optimal Multi-Site Testing of
  System Chips</title>
    <summary>  Multi-site testing is a popular and effective way to increase test throughput
and reduce test costs. We present a test throughput model, in which we focus on
wafer testing, and consider parameters like test time, index time,
abort-on-fail, and contact yield. Conventional multi-site testing requires
sufficient ATE resources, such as ATE channels, to allow to test multiple SOCs
in parallel. In this paper, we design and optimize on-chip DfT, in order to
maximize the test throughput for a given SOC and ATE. The on-chip DfT consists
of an E-RPCT wrapper, and, for modular SOCs, module wrappers and TAMs. We
present experimental results for a Philips SOC and several ITC'02 SOC Test
Benchmarks.
</summary>
    <author>
      <name>Sandeep Kumar Goel</name>
    </author>
    <author>
      <name>Erik Jan Marinissen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4688v1</id>
    <updated>2007-10-25T09:09:34Z</updated>
    <published>2007-10-25T09:09:34Z</published>
    <title>On the Optimal Design of Triple Modular Redundancy Logic for SRAM-based
  FPGAs</title>
    <summary>  Triple Modular Redundancy (TMR) is a suitable fault tolerant technique for
SRAM-based FPGA. However, one of the main challenges in achieving 100%
robustness in designs protected by TMR running on programmable platforms is to
prevent upsets in the routing from provoking undesirable connections between
signals from distinct redundant logic parts, which can generate an error in the
output. This paper investigates the optimal design of the TMR logic (e.g., by
cleverly inserting voters) to ensure robustness. Four different versions of a
TMR digital filter were analyzed by fault injection. Faults were randomly
inserted straight into the bitstream of the FPGA. The experimental results
presented in this paper demonstrate that the number and placement of voters in
the TMR design can directly affect the fault tolerance, ranging from 4.03% to
0.98% the number of upsets in the routing able to cause an error in the TMR
circuit.
</summary>
    <author>
      <name>F. Lima Kastensmidt</name>
    </author>
    <author>
      <name>L. Sterpone</name>
    </author>
    <author>
      <name>L. Carro</name>
    </author>
    <author>
      <name>M. Sonza Reorda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4688v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4688v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4691v1</id>
    <updated>2007-10-25T09:11:11Z</updated>
    <published>2007-10-25T09:11:11Z</published>
    <title>An O(bn^2) Time Algorithm for Optimal Buffer Insertion with b Buffer
  Types</title>
    <summary>  Buffer insertion is a popular technique to reduce the interconnect delay. The
classic buffer insertion algorithm of van Ginneken has time complexity O(n^2),
where n is the number of buffer positions. Lillis, Cheng and Lin extended van
Ginneken's algorithm to allow b buffer types in time O (b^2 n^2). For modern
design libraries that contain hundreds of buffers, it is a serious challenge to
balance the speed and performance of the buffer insertion algorithm. In this
paper, we present a new algorithm that computes the optimal buffer insertion in
O (bn^2) time. The reduction is achieved by the observation that the (Q, C)
pairs of the candidates that generate the new candidates must form a convex
hull. On industrial test cases, the new algorithm is faster than the previous
best buffer insertion algorithms by orders of magnitude.
</summary>
    <author>
      <name>Zhuo Li</name>
    </author>
    <author>
      <name>Weiping Shi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4693v1</id>
    <updated>2007-10-25T09:14:05Z</updated>
    <published>2007-10-25T09:14:05Z</published>
    <title>Memory Testing Under Different Stress Conditions: An Industrial
  Evaluation</title>
    <summary>  This paper presents the effectiveness of various stress conditions (mainly
voltage and frequency) on detecting the resistive shorts and open defects in
deep sub-micron embedded memories in an industrial environment. Simulation
studies on very-low voltage, high voltage and at-speed testing show the need of
the stress conditions for high quality products; i.e., low defect-per-million
(DPM) level, which is driving the semiconductor market today. The above test
conditions have been validated to screen out bad devices on real silicon (a
test-chip) built on CMOS 0.18 um technology. IFA (inductive fault analysis)
based simulation technique leads to an efficient fault coverage and DPM
estimator, which helps the customers upfront to make decisions on test
algorithm implementations under different stress conditions in order to reduce
the number of test escapes.
</summary>
    <author>
      <name>Ananta K. Majhi</name>
    </author>
    <author>
      <name>Mohamed Azimane</name>
    </author>
    <author>
      <name>Guido Gronthoud</name>
    </author>
    <author>
      <name>Maurice Lousberg</name>
    </author>
    <author>
      <name>Stefan Eichenberger</name>
    </author>
    <author>
      <name>Fred Bowen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4697v1</id>
    <updated>2007-10-25T09:16:49Z</updated>
    <published>2007-10-25T09:16:49Z</published>
    <title>Statistical Timing Based Optimization using Gate Sizing</title>
    <summary>  The increased dominance of intra-die process variations has motivated the
field of Statistical Static Timing Analysis (SSTA) and has raised the need for
SSTA-based circuit optimization. In this paper, we propose a new sensitivity
based, statistical gate sizing method. Since brute-force computation of the
change in circuit delay distribution to gate size change is computationally
expensive, we propose an efficient and exact pruning algorithm. The pruning
algorithm is based on a novel theory of perturbation bounds which are shown to
decrease as they propagate through the circuit. This allows pruning of gate
sensitivities without complete propagation of their perturbations. We apply our
proposed optimization algorithm to ISCAS benchmark circuits and demonstrate the
accuracy and efficiency of the proposed method. Our results show an improvement
of up to 10.5% in the 99-percentile circuit delay for the same circuit area,
using the proposed statistical optimizer and a run time improvement of up to
56x compared to the brute-force approach.
</summary>
    <author>
      <name>Aseem Agarwal</name>
    </author>
    <author>
      <name>Kaviraj Chopra</name>
    </author>
    <author>
      <name>David Blaauw</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4703v1</id>
    <updated>2007-10-25T09:27:22Z</updated>
    <published>2007-10-25T09:27:22Z</published>
    <title>A Way Memoization Technique for Reducing Power Consumption of Caches in
  Application Specific Integrated Processors</title>
    <summary>  This paper presents a technique for eliminating redundant cache-tag and
cache-way accesses to reduce power consumption. The basic idea is to keep a
small number of Most Recently Used (MRU) addresses in a Memory Address Buffer
(MAB) and to omit redundant tag and way accesses when there is a MAB-hit. Since
the approach keeps only tag and set-index values in the MAB, the energy and
area overheads are relatively small even for a MAB with a large number of
entries. Furthermore, the approach does not sacrifice the performance. In other
words, neither the cycle time nor the number of executed cycles increases. The
proposed technique has been applied to Fujitsu VLIW processor (FR-V) and its
power saving has been estimated using NanoSim. Experiments for 32kB 2-way set
associative caches show the power consumption of I-cache and D-cache can be
reduced by 40% and 50%, respectively.
</summary>
    <author>
      <name>Tohru Ishihara</name>
    </author>
    <author>
      <name>Farzan Fallah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4704v1</id>
    <updated>2007-10-25T09:28:05Z</updated>
    <published>2007-10-25T09:28:05Z</published>
    <title>Resource Sharing and Pipelining in Coarse-Grained Reconfigurable
  Architecture for Domain-Specific Optimization</title>
    <summary>  Coarse-grained reconfigurable architectures aim to achieve both goals of high
performance and flexibility. However, existing reconfigurable array
architectures require many resources without considering the specific
application domain. Functional resources that take long latency and/or large
area can be pipelined and/or shared among the processing elements. Therefore
the hardware cost and the delay can be effectively reduced without any
performance degradation for some application domains. We suggest such
reconfigurable array architecture template and design space exploration flow
for domain-specific optimization. Experimental results show that our approach
is much more efficient both in performance and area compared to existing
reconfigurable architectures.
</summary>
    <author>
      <name>Yoonjin Kim</name>
    </author>
    <author>
      <name>Mary Kiemb</name>
    </author>
    <author>
      <name>Chulsoo Park</name>
    </author>
    <author>
      <name>Jinyong Jung</name>
    </author>
    <author>
      <name>Kiyoung Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4704v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4704v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4707v1</id>
    <updated>2007-10-25T09:29:58Z</updated>
    <published>2007-10-25T09:29:58Z</published>
    <title>Energy- and Performance-Driven NoC Communication Architecture Synthesis
  Using a Decomposition Approach</title>
    <summary>  In this paper, we present a methodology for customized communication
architecture synthesis that matches the communication requirements of the
target application. This is an important problem, particularly for
network-based implementations of complex applications. Our approach is based on
using frequently encountered generic communication primitives as an alphabet
capable of characterizing any given communication pattern. The proposed
algorithm searches through the entire design space for a solution that
minimizes the system total energy consumption, while satisfying the other
design constraints. Compared to the standard mesh architecture, the customized
architecture generated by the newly proposed approach shows about 36%
throughput increase and 51% reduction in the energy required to encrypt 128
bits of data with a standard encryption algorithm.
</summary>
    <author>
      <name>Umit Y. Ogras</name>
    </author>
    <author>
      <name>Radu Marculescu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4709v1</id>
    <updated>2007-10-25T09:30:46Z</updated>
    <published>2007-10-25T09:30:46Z</published>
    <title>Analog and Digital Circuit Design in 65 nm CMOS: End of the Road?</title>
    <summary>  This special session adresses the problems that designers face when
implementing analog and digital circuits in nanometer technologies. An
introductory embedded tutorial will give an overview of the design problems at
hand : the leakage power and process variability and their implications for
digital circuits and memories, and the reducing supply voltages, the design
productivity and signal integrity problems for embedded analog blocks. Next, a
panel of experts from both industrial semiconductor houses and design
companies, EDA vendors and research institutes will present and discuss with
the audience their opinions on whether the design road ends at marker "65nm" or
not.
</summary>
    <author>
      <name>Georges Gielen</name>
    </author>
    <author>
      <name>Wim Dehaene</name>
    </author>
    <author>
      <name>Phillip Christie</name>
    </author>
    <author>
      <name>Dieter Draxelmayr</name>
    </author>
    <author>
      <name>Edmond Janssens</name>
    </author>
    <author>
      <name>Karen Maex</name>
    </author>
    <author>
      <name>Ted Vucurevich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4713v1</id>
    <updated>2007-10-25T09:33:36Z</updated>
    <published>2007-10-25T09:33:36Z</published>
    <title>Improving the Process-Variation Tolerance of Digital Circuits Using Gate
  Sizing and Statistical Techniques</title>
    <summary>  A new approach for enhancing the process-variation tolerance of digital
circuits is described. We extend recent advances in statistical timing analysis
into an optimization framework. Our objective is to reduce the performance
variance of a technology-mapped circuit where delays across elements are
represented by random variables which capture the manufacturing variations. We
introduce the notion of statistical critical paths, which account for both
means and variances of performance variation. An optimization engine is used to
size gates with a goal of reducing the timing variance along the statistical
critical paths. We apply a pair of nested statistical analysis methods
deploying a slower more accurate approach for tracking statistical critical
paths and a fast engine for evaluation of gate size assignments. We derive a
new approximation for the max operation on random variables which is deployed
for the faster inner engine. Circuit optimization is carried out using a
gain-based algorithm that terminates when constraints are satisfied or no
further improvements can be made. We show optimization results that demonstrate
an average of 72% reduction in performance variation at the expense of average
20% increase in design area.
</summary>
    <author>
      <name>Osama Neiroukh</name>
    </author>
    <author>
      <name>Xiaoyu Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4714v1</id>
    <updated>2007-10-25T09:33:38Z</updated>
    <published>2007-10-25T09:33:38Z</published>
    <title>Assertion-Based Design Exploration of DVS in Network Processor
  Architectures</title>
    <summary>  With the scaling of technology and higher requirements on performance and
functionality, power dissipation is becoming one of the major design
considerations in the development of network processors. In this paper, we use
an assertion-based methodology for system-level power/performance analysis to
study two dynamic voltage scaling (DVS) techniques, traffic-based DVS and
execution-based DVS, in a network processor model. Using the automatically
generated distribution analyzers, we analyze the power and performance
distributions and study their trade-offs for the two DVS policies with
different parameter settings such as threshold values and window sizes. We
discuss the optimal configurations of the two DVS policies under different
design requirements. By a set of experiments, we show that the assertion-based
trace analysis methodology is an efficient tool that can help a designer easily
compare and study optimal architectural configurations in a large design space.
</summary>
    <author>
      <name>Jia Yu</name>
    </author>
    <author>
      <name>Wei Wu</name>
    </author>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Harry Hsieh</name>
    </author>
    <author>
      <name>Jun Yang</name>
    </author>
    <author>
      <name>Felice Balarin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4715v1</id>
    <updated>2007-10-25T09:34:11Z</updated>
    <published>2007-10-25T09:34:11Z</published>
    <title>Circuit-Level Modeling for Concurrent Testing of Operational Defects due
  to Gate Oxide Breakdown</title>
    <summary>  As device sizes shrink and current densities increase, the probability of
device failures due to gate oxide breakdown (OBD) also increases. To provide
designs that are tolerant to such failures, we must investigate and understand
the manifestations of this physical phenomenon at the circuit and system level.
In this paper, we develop a model for operational OBD defects, and we explore
how to test for faults due to OBD. For a NAND gate, we derive the necessary
input conditions that excite and detect errors due to OBD defects at the gate
level. We show that traditional pattern generators fail to exercise all of
these defects. Finally, we show that these test patterns can be propagated and
justified for a combinational circuit in a manner similar to traditional ATPG.
</summary>
    <author>
      <name>Jonathan R. Carter</name>
    </author>
    <author>
      <name>Sule Ozev</name>
    </author>
    <author>
      <name>Daniel J. Sorin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4716v1</id>
    <updated>2007-10-25T09:34:19Z</updated>
    <published>2007-10-25T09:34:19Z</published>
    <title>Optimized Generation of Data-Path from C Codes for FPGAs</title>
    <summary>  FPGAs, as computing devices, offer significant speedup over microprocessors.
Furthermore, their configurability offers an advantage over traditional ASICs.
However, they do not yet enjoy high-level language programmability, as
microprocessors do. This has become the main obstacle for their wider
acceptance by application designers. ROCCC is a compiler designed to generate
circuits from C source code to execute on FPGAs, more specifically on CSoCs. It
generates RTL level HDLs from frequently executing kernels in an application.
In this paper, we describe ROCCC's system overview and focus on its data path
generation. We compare the performance of ROCCC-generated VHDL code with that
of Xilinx IPs. The synthesis result shows that ROCCC-generated circuit takes
around 2x ~ 3x area and runs at comparable clock rate.
</summary>
    <author>
      <name>Zhi Guo</name>
    </author>
    <author>
      <name>Betul Buyukkurt</name>
    </author>
    <author>
      <name>Walid Najjar</name>
    </author>
    <author>
      <name>Kees Vissers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4717v1</id>
    <updated>2007-10-25T09:35:03Z</updated>
    <published>2007-10-25T09:35:03Z</published>
    <title>Multi-Placement Structures for Fast and Optimized Placement in Analog
  Circuit Synthesis</title>
    <summary>  This paper presents the novel idea of multi-placement structures, for a fast
and optimized placement instantiation in analog circuit synthesis. These
structures need to be generated only once for a specific circuit topology. When
used in synthesis, these pre-generated structures instantiate various layout
floorplans for various sizes and parameters of a circuit. Unlike procedural
layout generators, they enable fast placement of circuits while keeping the
quality of the placements at a high level during a synthesis process. The fast
placement is a result of high speed instantiation resulting from the efficiency
of the multi-placement structure. The good quality of placements derive from
the extensive and intelligent search process that is used to build the
multi-placement structure. The target benchmarks of these structures are analog
circuits in the vicinity of 25 modules. An algorithm for the generation of such
multi-placement structures is presented. Experimental results show placement
execution times with an average of a few milliseconds making them usable during
layout-aware synthesis for optimized placements.
</summary>
    <author>
      <name>Raoul F. Badaoui</name>
    </author>
    <author>
      <name>Ranga Vemuri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4717v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4717v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4719v1</id>
    <updated>2007-10-25T09:36:21Z</updated>
    <published>2007-10-25T09:36:21Z</published>
    <title>Specification Test Compaction for Analog Circuits and MEMS</title>
    <summary>  Testing a non-digital integrated system against all of its specifications can
be quite expensive due to the elaborate test application and measurement setup
required. We propose to eliminate redundant tests by employing e-SVM based
statistical learning. Application of the proposed methodology to an operational
amplifier and a MEMS accelerometer reveal that redundant tests can be
statistically identified from a complete set of specification-based tests with
negligible error. Specifically, after eliminating five of eleven
specification-based tests for an operational amplifier, the defect escape and
yield loss is small at 0.6% and 0.9%, respectively. For the accelerometer,
defect escape of 0.2% and yield loss of 0.1% occurs when the hot and colt tests
are eliminated. For the accelerometer, this level of Compaction would reduce
test cost by more than half.
</summary>
    <author>
      <name>Sounil Biswas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">shawn</arxiv:affiliation>
    </author>
    <author>
      <name>Peng Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">shawn</arxiv:affiliation>
    </author>
    <author>
      <name>R. D.</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">shawn</arxiv:affiliation>
    </author>
    <author>
      <name> Blanton</name>
    </author>
    <author>
      <name>Larry T. Pileggi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4721v1</id>
    <updated>2007-10-25T09:36:53Z</updated>
    <published>2007-10-25T09:36:53Z</published>
    <title>IEEE 1149.4 Compatible ABMs for Basic RF Measurements</title>
    <summary>  An analogue testing standard IEEE 1149.4 is mainly targeted for low-frequency
testing. The problem studied in this paper is extending the standard also for
radio frequency testing. IEEE 1149.4 compatible measurement structures (ABMs)
developed in this study extract the information one is measuring from the radio
frequency signal and represent the result as a DC voltage level. The ABMs
presented in this paper are targeted for power and frequency measurements
operating in frequencies from 1 GHz to 2 GHz. The power measurement error
caused by temperature, supply voltage and process variations is roughly 2 dB
and the frequency measurement error is 0.1 GHz, respectively.
</summary>
    <author>
      <name>Pekka Syri</name>
    </author>
    <author>
      <name>Juha Hakkinen</name>
    </author>
    <author>
      <name>Markku Moilanen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4727v1</id>
    <updated>2007-10-25T09:38:14Z</updated>
    <published>2007-10-25T09:38:14Z</published>
    <title>Top-Down Design of a Low-Power Multi-Channel 2.5-Gbit/s/Channel Gated
  Oscillator Clock-Recovery Circuit</title>
    <summary>  We present a complete top-down design of a low-power multi-channel clock
recovery circuit based on gated current-controlled oscillators. The flow
includes several tools and methods used to specify block constraints, to design
and verify the topology down to the transistor level, as well as to achieve a
power consumption as low as 5mW/Gbit/s. Statistical simulation is used to
estimate the achievable bit error rate in presence of phase and frequency
errors and to prove the feasibility of the concept. VHDL modeling provides
extensive verification of the topology. Thermal noise modeling based on
well-known concepts delivers design parameters for the device sizing and
biasing. We present two practical examples of possible design improvements
analyzed and implemented with this methodology.
</summary>
    <author>
      <name>Paul Muller</name>
    </author>
    <author>
      <name>Armin Tajalli</name>
    </author>
    <author>
      <name>Mojtaba Atarodi</name>
    </author>
    <author>
      <name>Yusuf Leblebici</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4728v1</id>
    <updated>2007-10-25T09:38:43Z</updated>
    <published>2007-10-25T09:38:43Z</published>
    <title>Energy-Aware Routing for E-Textile Applications</title>
    <summary>  As the scale of electronic devices shrinks, "electronic textiles"
(e-textiles) will make possible a wide variety of novel applications which are
currently unfeasible. Due to the wearability concerns, low-power techniques are
critical for e-textile applications. In this paper, we address the issue of the
energy-aware routing for e-textile platforms and propose an efficient algorithm
to solve it. The platform we consider consists of dedicated components for
e-textiles, including computational modules, dedicated transmission lines and
thin-film batteries on fiber substrates. Furthermore, we derive an analytical
upper bound for the achievable number of jobs completed over all possible
routing strategies. From a practical standpoint, for the Advanced Encryption
Standard (AES) cipher, the routing technique we propose achieves about fifty
percent of this analytical upper bound. Moreover, compared to the
non-energy-aware counterpart, our routing technique increases the number of
encryption jobs completed by one order of magnitude.
</summary>
    <author>
      <name>Jung-Chun Kao</name>
    </author>
    <author>
      <name>Radu Marculescu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4729v1</id>
    <updated>2007-10-25T09:39:25Z</updated>
    <published>2007-10-25T09:39:25Z</published>
    <title>Modeling and Analysis of Loading Effect in Leakage of Nano-Scaled
  Bulk-CMOS Logic Circuits</title>
    <summary>  In nanometer scaled CMOS devices significant increase in the subthreshold,
the gate and the reverse biased junction band-to-band-tunneling (BTBT) leakage,
results in the large increase of total leakage power in a logic circuit.
Leakage components interact with each other in device level (through device
geometry, doping profile) and also in the circuit level (through node
voltages). Due to the circuit level interaction of the different leakage
components, the leakage of a logic gate strongly depends on the circuit
topology i.e. number and nature of the other logic gates connected to its input
and output. In this paper, for the first time, we have analyzed loading effect
on leakage and proposed a method to accurately estimate the total leakage in a
logic circuit, from its logic level description considering the impact of
loading and transistor stacking.
</summary>
    <author>
      <name>Saibal Mukhopadhyay</name>
    </author>
    <author>
      <name>Swarup Bhunia</name>
    </author>
    <author>
      <name>Kaushik Roy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4735v1</id>
    <updated>2007-10-25T09:42:30Z</updated>
    <published>2007-10-25T09:42:30Z</published>
    <title>Worst-Case and Average-Case Analysis of n-Detection Test Sets</title>
    <summary>  Test sets that detect each target fault n times (n-detection test sets) are
typically generated for restricted values of n due to the increase in test set
size with n. We perform both a worst-case analysis and an average-case analysis
to check the effect of restricting n on the unmodeled fault coverage of an
(arbitrary) n-detection test set. Our analysis is independent of any particular
test set or test generation approach. It is based on a specific set of target
faults and a specific set of untargeted faults. It shows that, depending on the
circuit, very large values of n may be needed to guarantee the detection of all
the untargeted faults. We discuss the implications of these results.
</summary>
    <author>
      <name>Irith Pomeranz</name>
    </author>
    <author>
      <name>Sudhakar M. Reddy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4735v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4735v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4738v1</id>
    <updated>2007-10-25T09:43:59Z</updated>
    <published>2007-10-25T09:43:59Z</published>
    <title>Exploring NoC Mapping Strategies: An Energy and Timing Aware Technique</title>
    <summary>  Complex applications implemented as Systems on Chip (SoCs) demand extensive
use of system level modeling and validation. Their implementation gathers a
large number of complex IP cores and advanced interconnection schemes, such as
hierarchical bus architectures or networks on chip (NoCs). Modeling
applications involves capturing its computation and communication
characteristics. Previously proposed communication weighted models (CWM)
consider only the application communication aspects. This work proposes a
communication dependence and computation model (CDCM) that can simultaneously
consider both aspects of an application. It presents a solution to the problem
of mapping applications on regular NoCs while considering execution time and
energy consumption. The use of CDCM is shown to provide estimated average
reductions of 40% in execution time, and 20% in energy consumption, for current
technologies.
</summary>
    <author>
      <name>Cesar Marcon</name>
    </author>
    <author>
      <name>Ney Calazans</name>
    </author>
    <author>
      <name>Fernando Moraes</name>
    </author>
    <author>
      <name>Altamiro Susin</name>
    </author>
    <author>
      <name>Igor Reis</name>
    </author>
    <author>
      <name>Fabiano Hessel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4742v1</id>
    <updated>2007-10-25T09:45:28Z</updated>
    <published>2007-10-25T09:45:28Z</published>
    <title>Hardware Accelerated Power Estimation</title>
    <summary>  In this paper, we present power emulation, a novel design paradigm that
utilizes hardware acceleration for the purpose of fast power estimation. Power
emulation is based on the observation that the functions necessary for power
estimation (power model evaluation, aggregation, etc.) can be implemented as
hardware circuits. Therefore, we can enhance any given design with "power
estimation hardware", map it to a prototyping platform, and exercise it with
any given test stimuli to obtain power consumption estimates. Our empirical
studies with industrial designs reveal that power emulation can achieve
significant speedups (10X to 500X) over state-of-the-art commercial
register-transfer level (RTL) power estimation tools.
</summary>
    <author>
      <name>Joel Coburn</name>
    </author>
    <author>
      <name>Srivaths Ravi</name>
    </author>
    <author>
      <name>Anand Raghunathan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4747v1</id>
    <updated>2007-10-25T09:48:22Z</updated>
    <published>2007-10-25T09:48:22Z</published>
    <title>An Efficient Transparent Test Scheme for Embedded Word-Oriented Memories</title>
    <summary>  Memory cores are usually the densest portion with the smallest feature size
in system-on-chip (SOC) designs. The reliability of memory cores thus has heavy
impact on the reliability of SOCs. Transparent test is one of useful technique
for improving the reliability of memories during life time. This paper presents
a systematic algorithm used for transforming a bit-oriented march test into a
transparent word-oriented march test. The transformed transparent march test
has shorter test complexity compared with that proposed in the previous works
[Theory of transparent BIST for RAMs, A transparent online memory test for
simultaneous detection of functional faults and soft errors in memories]. For
example, if a memory with 32-bit words is tested with March C-, time complexity
of the transparent word-oriented test transformed by the proposed scheme is
only about 56% or 19% time complexity of the transparent word-oriented test
converted by the scheme reported in [Theory of transparent BIST for RAMs] or [A
transparent online memory test for simultaneous detection of functional faults
and soft errors in memories], respectively.
</summary>
    <author>
      <name>Jin-Fu Li</name>
    </author>
    <author>
      <name>Tsu-Wei Tseng</name>
    </author>
    <author>
      <name>Chin-Long Wey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4751v1</id>
    <updated>2007-10-25T09:51:11Z</updated>
    <published>2007-10-25T09:51:11Z</published>
    <title>Influence of Memory Hierarchies on Predictability for Time Constrained
  Embedded Software</title>
    <summary>  Safety-critical embedded systems having to meet real-time constraints are
expected to be highly predictable in order to guarantee at design time that
certain timing deadlines will always be met. This requirement usually prevents
designers from utilizing caches due to their highly dynamic, thus hardly
predictable behavior. The integration of scratchpad memories represents an
alternative approach which allows the system to benefit from a performance gain
comparable to that of caches while at the same time maintaining predictability.
In this work, we compare the impact of scratchpad memories and caches on worst
case execution time (WCET) analysis results. We show that caches, despite
requiring complex techniques, can have a negative impact on the predicted WCET,
while the estimated WCET for scratchpad memories scales with the achieved
Performance gain at no extra analysis cost.
</summary>
    <author>
      <name>Lars Wehmeyer</name>
    </author>
    <author>
      <name>Peter Marwedel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4757v1</id>
    <updated>2007-10-25T09:53:37Z</updated>
    <published>2007-10-25T09:53:37Z</published>
    <title>Techniques for Fast Transient Fault Grading Based on Autonomous
  Emulation</title>
    <summary>  Very deep submicron and nanometer technologies have increased notably
integrated circuit (IC) sensitiveness to radiation. Soft errors are currently
appearing into ICs working at earth surface. Hardened circuits are currently
required in many applications where Fault Tolerance (FT) was not a requirement
in the very near past. The use of platform FPGAs for the emulation of
single-event upset effects (SEU) is gaining attention in order to speed up the
FT evaluation. In this work, a new emulation system for FT evaluation with
respect to SEU effects is proposed, providing shorter evaluation times by
performing all the evaluation process in the FPGA and avoiding emulator-host
communication bottlenecks.
</summary>
    <author>
      <name>Celia Lopez-Ongil</name>
    </author>
    <author>
      <name>Mario Garcia-Valderas</name>
    </author>
    <author>
      <name>Marta Portela-Garcia</name>
    </author>
    <author>
      <name>Luis Entrena-Arrontes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4757v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4757v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4759v1</id>
    <updated>2007-10-25T09:54:18Z</updated>
    <published>2007-10-25T09:54:18Z</published>
    <title>A Fast Concurrent Power-Thermal Model for Sub-100nm Digital ICs</title>
    <summary>  As technology scales down, the static power is expected to become a
significant fraction of the total power. The exponential dependence of static
power with the operating temperature makes the thermal profile estimation of
high-performance ICs a key issue to compute the total power dissipated in
next-generations. In this paper we present accurate and compact analytical
models to estimate the static power dissipation and the temperature of
operation of CMOS gates. The models are the fundamentals of a performance
estimation tool in which numerical procedures are avoided for any computation
to set a faster estimation and optimization. The models developed are compared
to measurements and SPICE simulations for a 0.12mm technology showing excellent
results.
</summary>
    <author>
      <name>J. L. Rossello</name>
    </author>
    <author>
      <name>V. Canals</name>
    </author>
    <author>
      <name>S. A. Bota</name>
    </author>
    <author>
      <name>A. Keshavarzi</name>
    </author>
    <author>
      <name>J. Segura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4760v1</id>
    <updated>2007-10-25T09:54:46Z</updated>
    <published>2007-10-25T09:54:46Z</published>
    <title>Low Power Oriented CMOS Circuit Optimization Protocol</title>
    <summary>  Low power oriented circuit optimization consists in selecting the best
alternative between gate sizing, buffer insertion and logic structure
transformation, for satisfying a delay constraint at minimum area cost. In this
paper we used a closed form model of delay in CMOS structures to define metrics
for a deterministic selection of the optimization alternative. The target is
delay constraint satisfaction with minimum area cost. We validate the design
space exploration method, defining maximum and minimum delay bounds on logical
paths. Then we adapt this method to a "constant sensitivity method" allowing to
size a circuit at minimum area under a delay constraint. An optimisation
protocol is finally defined to manage the trade-off performance constraint -
circuit structure. These methods are implemented in an optimization tool (POPS)
and validated by comparing on a 0.25$\mu$m process, the optimization efficiency
obtained on various benchmarks (ISCAS?85) to that resulting from an industrial
tool.
</summary>
    <author>
      <name>A. Verle</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIRMM</arxiv:affiliation>
    </author>
    <author>
      <name>X. Michel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIRMM</arxiv:affiliation>
    </author>
    <author>
      <name>N. Azemard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIRMM</arxiv:affiliation>
    </author>
    <author>
      <name>P. Maurine</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIRMM</arxiv:affiliation>
    </author>
    <author>
      <name>D. Auvergne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIRMM</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4795v1</id>
    <updated>2007-10-25T11:52:22Z</updated>
    <published>2007-10-25T11:52:22Z</published>
    <title>Test Time Reduction Reusing Multiple Processors in a Network-on-Chip
  Based Architecture</title>
    <summary>  The increasing complexity and the short life cycles of embedded systems are
pushing the current system-on-chip designs towards a rapid increasing on the
number of programmable processing units, while decreasing the gate count for
custom logic. Considering this trend, this work proposes a test planning method
capable of reusing available processors as test sources and sinks, and the
on-chip network as the test access mechanism. Experimental results are based on
ITC'02 benchmarks and on two open core processors compliant with MIPS and SPARC
instruction set. The results show that the cooperative use of both the on-chip
network and the embedded processors can increase the test parallelism and
reduce the test time without additional cost in area and pins.
</summary>
    <author>
      <name>Alexandre M. Amory</name>
    </author>
    <author>
      <name>Marcelo Lubaszewski</name>
    </author>
    <author>
      <name>Fernando G. Moraes</name>
    </author>
    <author>
      <name>Edson I. Moreno</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4796v1</id>
    <updated>2007-10-25T11:53:03Z</updated>
    <published>2007-10-25T11:53:03Z</published>
    <title>A Hybrid Prefetch Scheduling Heuristic to Minimize at Run-Time the
  Reconfiguration Overhead of Dynamically Reconfigurable Hardware</title>
    <summary>  Due to the emergence of highly dynamic multimedia applications there is a
need for flexible platforms and run-time scheduling support for embedded
systems. Dynamic Reconfigurable Hardware (DRHW) is a promising candidate to
provide this flexibility but, currently, not sufficient run-time scheduling
support to deal with the run-time reconfigurations exists. Moreover, executing
at run-time a complex scheduling heuristic to provide this support may generate
an excessive run-time penalty. Hence, we have developed a hybrid
design/run-time prefetch heuristic that schedules the reconfigurations at
run-time, but carries out the scheduling computations at design-time by
carefully identifying a set of near-optimal schedules that can be selected at
run-time. This approach provides run-time flexibility with a negligible
penalty.
</summary>
    <author>
      <name>Javier Resano</name>
    </author>
    <author>
      <name>Daniel Mozos</name>
    </author>
    <author>
      <name>Francky Catthoor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4801v1</id>
    <updated>2007-10-25T11:55:59Z</updated>
    <published>2007-10-25T11:55:59Z</published>
    <title>Behavioural Transformation to Improve Circuit Performance in High-Level
  Synthesis</title>
    <summary>  Early scheduling algorithms usually adjusted the clock cycle duration to the
execution time of the slowest operation. This resulted in large slack times
wasted in those cycles executing faster operations. To reduce the wasted times
multi-cycle and chaining techniques have been employed. While these techniques
have produced successful designs, its effectiveness is often limited due to the
area increment that may derive from chaining, and the extra latencies that may
derive from multicycling. In this paper we present an optimization method that
solves the time-constrained scheduling problem by transforming behavioural
specifications into new ones whose subsequent synthesis substantially improves
circuit performance. Our proposal breaks up some of the specification
operations, allowing their execution during several possibly unconsecutive
cycles, and also the calculation of several data-dependent operation fragments
in the same cycle. To do so, it takes into account the circuit latency and the
execution time of every specification operation. The experimental results
carried out show that circuits obtained from the optimized specification are on
average 60% faster than those synthesized from the original specification, with
only slight increments in the circuit area.
</summary>
    <author>
      <name>R. Ruiz-Sautua</name>
    </author>
    <author>
      <name>M. C. Molina</name>
    </author>
    <author>
      <name>J. M. Mendias</name>
    </author>
    <author>
      <name>R. Hermida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4801v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4801v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4806v1</id>
    <updated>2007-10-25T11:57:56Z</updated>
    <published>2007-10-25T11:57:56Z</published>
    <title>A VLSI Design Flow for Secure Side-Channel Attack Resistant ICs</title>
    <summary>  This paper presents a digital VLSI design flow to create secure, side-channel
attack (SCA) resistant integrated circuits. The design flow starts from a
normal design in a hardware description language such as VHDL or Verilog and
provides a direct path to a SCA resistant layout. Instead of a full custom
layout or an iterative design process with extensive simulations, a few key
modifications are incorporated in a regular synchronous CMOS standard cell
design flow. We discuss the basis for side-channel attack resistance and adjust
the library databases and constraints files of the synthesis and place &amp; route
procedures accordingly. Experimental results show that a DPA attack on a
regular single ended CMOS standard cell implementation of a module of the DES
algorithm discloses the secret key after 200 measurements. The same attack on a
secure version still does not disclose the secret key after more than 2000
measurements.
</summary>
    <author>
      <name>Kris Tiri</name>
    </author>
    <author>
      <name>Ingrid Verbauwhede</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4808v1</id>
    <updated>2007-10-25T11:59:15Z</updated>
    <published>2007-10-25T11:59:15Z</published>
    <title>Fast and Accurate Transaction Level Modeling of an Extended AMBA2.0 Bus
  Architecture</title>
    <summary>  Transaction Level Modeling (TLM) approach is used to meet the simulation
speed as well as cycle accuracy for large scale SoC performance analysis. We
implemented a transaction-level model of a proprietary bus called AHB+ which
supports an extended AMBA2.0 protocol. The AHB+ transaction-level model shows
353 times faster than pin-accurate RTL model while maintaining 97% of accuracy
on average. We also present the development procedure of TLM of a bus
architecture.
</summary>
    <author>
      <name>Young-Taek Kim</name>
    </author>
    <author>
      <name>Taehun Kim</name>
    </author>
    <author>
      <name>Youngduk Kim</name>
    </author>
    <author>
      <name>Chulho Shin</name>
    </author>
    <author>
      <name>Eui-Young Chung</name>
    </author>
    <author>
      <name>Kyu-Myung Choi</name>
    </author>
    <author>
      <name>Jeong-Taek Kong</name>
    </author>
    <author>
      <name>Soo-Kwan Eo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4809v1</id>
    <updated>2007-10-25T11:59:48Z</updated>
    <published>2007-10-25T11:59:48Z</published>
    <title>C Based Hardware Design for Wireless Applications</title>
    <summary>  The algorithms used in wireless applications are increasingly more
sophisticated and consequently more challenging to implement in hardware.
Traditional design flows require developing the micro architecture, coding the
RTL, and verifying the generated RTL against the original functional C or
MATLAB specification. This paper describes a C-based design flow that is well
suited for the hardware implementation of DSP algorithms commonly found in
wireless applications. The C design flow relies on guided synthesis to generate
the RTL directly from the untimed C algorithm. The specifics of the C-based
design flow are described using a simple DSP filtering algorithm consisting of
a forward adaptive equalizer, a 64-QAM slicer and an adaptive decision feedback
equalizer. The example illustrates some of the capabilities and advantages
offered by this flow.
</summary>
    <author>
      <name>Andres Takach</name>
    </author>
    <author>
      <name>Bryan Bowyer</name>
    </author>
    <author>
      <name>Thomas Bollaert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4812v1</id>
    <updated>2007-10-25T12:00:41Z</updated>
    <published>2007-10-25T12:00:41Z</published>
    <title>Area and Throughput Trade-Offs in the Design of Pipelined Discrete
  Wavelet Transform Architectures</title>
    <summary>  The JPEG2000 standard defines the discrete wavelet transform (DWT) as a
linear space-to-frequency transform of the image domain in an irreversible
compression. This irreversible discrete wavelet transform is implemented by FIR
filter using 9/7 Daubechies coefficients or a lifting scheme of factorizated
coefficients from 9/7 Daubechies coefficients. This work investigates the
tradeoffs between area, power and data throughput (or operating frequency) of
several implementations of the Discrete Wavelet Transform using the lifting
scheme in various pipeline designs. This paper shows the results of five
different architectures synthesized and simulated in FPGAs. It concludes that
the descriptions with pipelined operators provide the best area-power-operating
frequency trade-off over non-pipelined operators descriptions. Those
descriptions require around 40% more hardware to increase the maximum operating
frequency up to 100% and reduce power consumption to less than 50%. Starting
from behavioral HDL descriptions provide the best area-power-operating
frequency trade-off, improving hardware cost and maximum operating frequency
around 30% in comparison to structural descriptions for the same power
requirement.
</summary>
    <author>
      <name>Sandro V. Silva</name>
    </author>
    <author>
      <name>Sergio Bampi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4813v1</id>
    <updated>2007-10-25T12:00:48Z</updated>
    <published>2007-10-25T12:00:48Z</published>
    <title>Queue Management in Network Processors</title>
    <summary>  One of the main bottlenecks when designing a network processing system is
very often its memory subsystem. This is mainly due to the state-of-the-art
network links operating at very high speeds and to the fact that in order to
support advanced Quality of Service (QoS), a large number of independent queues
is desirable. In this paper we analyze the performance bottlenecks of various
data memory managers integrated in typical Network Processing Units (NPUs). We
expose the performance limitations of software implementations utilizing the
RISC processing cores typically found in most NPU architectures and we identify
the requirements for hardware assisted memory management in order to achieve
wire-speed operation at gigabit per second rates. Furthermore, we describe the
architecture and performance of a hardware memory manager that fulfills those
requirements. This memory manager, although it is implemented in a
reconfigurable technology, it can provide up to 6.2Gbps of aggregate
throughput, while handling 32K independent queues.
</summary>
    <author>
      <name>I. Papaefstathiou</name>
    </author>
    <author>
      <name>T. Orphanoudakis</name>
    </author>
    <author>
      <name>G. Kornaros</name>
    </author>
    <author>
      <name>C. Kachris</name>
    </author>
    <author>
      <name>I. Mavroidis</name>
    </author>
    <author>
      <name>A. Nikologiannis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4820v1</id>
    <updated>2007-10-25T12:04:11Z</updated>
    <published>2007-10-25T12:04:11Z</published>
    <title>ISEGEN: Generation of High-Quality Instruction Set Extensions by
  Iterative Improvement</title>
    <summary>  Customization of processor architectures through Instruction Set Extensions
(ISEs) is an effective way to meet the growing performance demands of embedded
applications. A high-quality ISE generation approach needs to obtain results
close to those achieved by experienced designers, particularly for complex
applications that exhibit regularity: expert designers are able to exploit
manually such regularity in the data flow graphs to generate high-quality ISEs.
In this paper, we present ISEGEN, an approach that identifies high-quality ISEs
by iterative improvement following the basic principles of the well-known
Kernighan-Lin (K-L) min-cut heuristic. Experimental results on a number of
MediaBench, EEMBC and cryptographic applications show that our approach matches
the quality of the optimal solution obtained by exhaustive search. We also show
that our ISEGEN technique is on average 20x faster than a genetic formulation
that generates equivalent solutions. Furthermore, the ISEs identified by our
technique exhibit 35% more speedup than the genetic solution on a large
cryptographic application (AES) by effectively exploiting its regular
structure.
</summary>
    <author>
      <name>Partha Biswas</name>
    </author>
    <author>
      <name>Sudarshan Banerjee</name>
    </author>
    <author>
      <name>Nikil Dutt</name>
    </author>
    <author>
      <name>Laura Pozzi</name>
    </author>
    <author>
      <name>Paolo Ienne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4820v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4820v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4826v1</id>
    <updated>2007-10-25T12:06:43Z</updated>
    <published>2007-10-25T12:06:43Z</published>
    <title>The Integration of On-Line Monitoring and Reconfiguration Functions
  using EDAA - European design and Automation Association1149.4 Into a Safety
  Critical Automotive Electronic Control Unit</title>
    <summary>  This paper presents an innovative application of EDAA - European design and
Automation Association 1149.4 and the Integrated Diagnostic Reconfiguration
(IDR) as tools for the implementation of an embedded test solution for an
Automotive Electronic Control Unit implemented as a fully integrated mixed
signal system. The paper described how the test architecture can be used for
fault avoidance with results from a hardware prototype presented. The paper
concludes that fault avoidance can be integrated into mixed signal electronic
systems to handle key failure modes.
</summary>
    <author>
      <name>C. Jeffrey</name>
    </author>
    <author>
      <name>R. Cutajar</name>
    </author>
    <author>
      <name>S. Prosser</name>
    </author>
    <author>
      <name>M. Lickess</name>
    </author>
    <author>
      <name>A. Richardson</name>
    </author>
    <author>
      <name>S. Riches</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4826v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4826v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4827v1</id>
    <updated>2007-10-25T12:07:17Z</updated>
    <published>2007-10-25T12:07:17Z</published>
    <title>Debug Support, Calibration and Emulation for Multiple Processor and
  Powertrain Control SoCs</title>
    <summary>  The introduction of complex SoCs with multiple processor cores presents new
development challenges, such that development support is now a decisive factor
when choosing a System-on-Chip (SoC). The presented developments support
strategy addresses the challenges using both architecture and technology
approaches. The Multi-Core Debug Support (MCDS) architecture provides flexible
triggering using cross triggers and a multiple core break and suspend switch.
Temporal trace ordering is guaranteed down to cycle level by on-chip time
stamping. The Package Sized-ICE (PSI) approach is a novel method of including
trace buffers, overlay memories, processing resources and communication
interfaces without changing device behavior. PSI requires no external emulation
box, as the debug host interfaces directly with the SoC using a standard
interface.
</summary>
    <author>
      <name>A. Mayer</name>
    </author>
    <author>
      <name>H. Siebert</name>
    </author>
    <author>
      <name>K. D. Mcdonald-Maier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4833v1</id>
    <updated>2007-10-25T12:11:11Z</updated>
    <published>2007-10-25T12:11:11Z</published>
    <title>Exploiting Real-Time FPGA Based Adaptive Systems Technology for
  Real-Time Sensor Fusion in Next Generation Automotive Safety Systems</title>
    <summary>  We present a system for the boresighting of sensors using inertial
measurement devices as the basis for developing a range of dynamic real-time
sensor fusion applications. The proof of concept utilizes a COTS FPGA platform
for sensor fusion and real-time correction of a misaligned video sensor. We
exploit a custom-designed 32-bit soft processor core and C-based design &amp;
synthesis for rapid, platform-neutral development. Kalman filter and sensor
fusion techniques established in advanced aviation systems are applied to
automotive vehicles with results exceeding typical industry requirements for
sensor alignment. Results of the static and the dynamic tests demonstrate that
using inexpensive accelerometers mounted on (or during assembly of) a sensor
and an Inertial Measurement Unit (IMU) fixed to a vehicle can be used to
compute the misalignment of the sensor to the IMU and thus vehicle. In some
cases the model predications and test results exceeded the requirements by an
order of magnitude with a 3-sigma or 99% confidence.
</summary>
    <author>
      <name>Steve Chappell</name>
    </author>
    <author>
      <name>Alistair Macarthur</name>
    </author>
    <author>
      <name>Dan Preston</name>
    </author>
    <author>
      <name>Dave Olmstead</name>
    </author>
    <author>
      <name>Bob Flint</name>
    </author>
    <author>
      <name>Chris Sullivan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4834v1</id>
    <updated>2007-10-25T12:12:03Z</updated>
    <published>2007-10-25T12:12:03Z</published>
    <title>Platform Based Design for Automotive Sensor Conditioning</title>
    <summary>  In this paper a general architecture suitable to interface several kinds of
sensors for automotive applications is presented. A platform based design
approach is pursued to improve system performance while minimizing
time-to-market.. The platform is composed by an analog front-end and a digital
section. The latter is based on a microcontroller core (8051 IP by Oregano)
plus a set of dedicated hardware dedicated to the complex signal processing
required for sensor conditioning. The microcontroller handles also the
communication with external devices (as a PC) for data output and fast
prototyping. A case study is presented concerning the conditioning of a Gyro
yaw rate sensor for automotive applications. Measured performance results
outperform current state-of-the-art commercial devices.
</summary>
    <author>
      <name>L. Fanucci</name>
    </author>
    <author>
      <name>A. Giambastiani</name>
    </author>
    <author>
      <name>F. Iozzi</name>
    </author>
    <author>
      <name>C. Marino</name>
    </author>
    <author>
      <name>A. Rocchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4838v1</id>
    <updated>2007-10-25T12:15:35Z</updated>
    <published>2007-10-25T12:15:35Z</published>
    <title>A 6bit, 1.2GSps Low-Power Flash-ADC in 0.13$μ$m Digital CMOS</title>
    <summary>  A 6bit flash-ADC with 1.2GSps, wide analog bandwidth and low power, realized
in a standard digital 0.13 $\mu$m CMOS copper technology is presented.
Employing capacitive interpolation gives various advantages when designing for
low power: no need for a reference resistor ladder, implicit sample-and-hold
operation, no edge effects in the interpolation network (as compared to
resistive interpolation), and a very low input capacitance of only 400fF, which
leads to an easily drivable analog converter interface. Operating at 1.2GSps
the ADC achieves an effective resolution bandwidth (ERBW) of 700MHz, while
consuming 160mW of power. At 600MSps we achieve an ERBW of 600MHz with only
90mW power consumption, both from a 1.5V supply. This corresponds to
outstanding Figure-of-Merit numbers (FoM) of 2.2 and 1.5pJ/convstep,
respectively. The module area is 0.12mm^2.
</summary>
    <author>
      <name>Christoph Sandner</name>
    </author>
    <author>
      <name>Martin Clara</name>
    </author>
    <author>
      <name>Andreas Santner</name>
    </author>
    <author>
      <name>Thomas Hartig</name>
    </author>
    <author>
      <name>Franz Kuttner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4838v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4838v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4839v1</id>
    <updated>2007-10-25T12:16:26Z</updated>
    <published>2007-10-25T12:16:26Z</published>
    <title>A 97mW 110MS/s 12b Pipeline ADC Implemented in 0.18$μ$m Digital CMOS</title>
    <summary>  A 12 bit Pipeline ADC fabricated in a 0.18 $\mu$m pure digital CMOS
technology is presented. Its nominal conversion rate is 110MS/s and the nominal
supply voltage is 1.8V. The effective number of bits is 10.4 when a 10MHz input
signal with 2V_{P-P} signal swing is applied. The occupied silicon area is
0.86mm^2 and the power consumption equals 97mW. A switched capacitor bias
current circuit scale the bias current automatically with the conversion rate,
which gives scaleable power consumption and full performance of the ADC from 20
to 140MS/s.
</summary>
    <author>
      <name>Terje N. Andersen</name>
    </author>
    <author>
      <name>Atle Briskemyr</name>
    </author>
    <author>
      <name>Frode Telsto</name>
    </author>
    <author>
      <name>Johnny Bjornsen</name>
    </author>
    <author>
      <name>Thomas E. Bonnerud</name>
    </author>
    <author>
      <name>Bjornar Hernes</name>
    </author>
    <author>
      <name>Oystein Moldsvor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4839v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4839v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4840v1</id>
    <updated>2007-10-25T12:17:29Z</updated>
    <published>2007-10-25T12:17:29Z</published>
    <title>Testing Logic Cores using a BIST P1500 Compliant Approach: A Case of
  Study</title>
    <summary>  In this paper we describe how we applied a BIST-based approach to the test of
a logic core to be included in System-on-a-chip (SoC) environments. The
approach advantages are the ability to protect the core IP, the simple test
interface (thanks also to the adoption of the P1500 standard), the possibility
to run the test at-speed, the reduced test time, and the good diagnostic
capabilities. The paper reports figures about the achieved fault coverage, the
required area overhead, and the performance slowdown, and compares the figures
with those for alternative approaches, such as those based on full scan and
sequential ATPG.
</summary>
    <author>
      <name>P. Bernardi</name>
    </author>
    <author>
      <name>G. Masera</name>
    </author>
    <author>
      <name>F. Quaglio</name>
    </author>
    <author>
      <name>M. Sonza Reorda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4842v1</id>
    <updated>2007-10-25T12:18:38Z</updated>
    <published>2007-10-25T12:18:38Z</published>
    <title>Using Mobilize Power Management IP for Dynamic &amp; Static Power Reduction
  in SoC at 130 nm</title>
    <summary>  At 130 nm and 90 nm, power consumption (both dynamic and static) has become a
barrier in the roadmap for SoC designs targeting battery powered, mobile
applications. This paper presents the results of dynamic and static power
reduction achieved implementing Tensilica's 32-bit Xtensa microprocessor core,
using Virtual Silicon's Power Management IP. Independent voltage islands are
created using Virtual Silicon's VIP PowerSaver standard cells by using voltage
level shifting cells and voltage isolation cells to implement power islands.
The VIP PowerSaver standard cells are characterized at 1.2V, 1.0V and 0.8V, to
accommodate voltage scaling. Power islands can also be turned off completely.
Designers can significantly lower both the dynamic power and the quiescent or
leakage power of their SoC designs, with very little impact on speed or area
using Virtual Silicon's VIP Gate Bias standard cells.
</summary>
    <author>
      <name>Dan Hillman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4843v1</id>
    <updated>2007-10-25T12:19:07Z</updated>
    <published>2007-10-25T12:19:07Z</published>
    <title>MultiNoC: A Multiprocessing System Enabled by a Network on Chip</title>
    <summary>  The MultiNoC system implements a programmable on-chip multiprocessing
platform built on top of an efficient, low area overhead intra-chip
interconnection scheme. The employed interconnection structure is a Network on
Chip, or NoC. NoCs are emerging as a viable alternative to increasing demands
on interconnection architectures, due to the following characteristics: (i)
energy efficiency and reliability; (ii) scalability of bandwidth, when compared
to traditional bus architectures; (iii) reusability; (iv) distributed routing
decisions. An external host computer feeds MultiNoC with application
instructions and data. After this initialization procedure, MultiNoC executes
some algorithm. After finishing execution of the algorithm, output data can be
read back by the host. Sequential or parallel algorithms conveniently adapted
to the MultiNoC structure can be executed. The main motivation to propose this
design is to enable the investigation of current trends to increase the number
of embedded processors in SoCs, leading to the concept of "sea of processors"
systems.
</summary>
    <author>
      <name>Aline Mello</name>
    </author>
    <author>
      <name>Leandro Moller</name>
    </author>
    <author>
      <name>Ney Calazans</name>
    </author>
    <author>
      <name>Fernando Moraes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4844v1</id>
    <updated>2007-10-25T12:20:27Z</updated>
    <published>2007-10-25T12:20:27Z</published>
    <title>A Partitioning Methodology for Accelerating Applications in Hybrid
  Reconfigurable Platforms</title>
    <summary>  In this paper, we propose a methodology for partitioning and mapping
computational intensive applications in reconfigurable hardware blocks of
different granularity. A generic hybrid reconfigurable architecture is
considered so as the methodology can be applicable to a large number of
heterogeneous reconfigurable platforms. The methodology mainly consists of two
stages, the analysis and the mapping of the application onto fine and
coarse-grain hardware resources. A prototype framework consisting of analysis,
partitioning and mapping tools has been also developed. For the coarse-grain
reconfigurable hardware, we use our previous-developed high-performance
coarse-grain data-path. In this work, the methodology is validated using two
real-world applications, an OFDM transmitter and a JPEG encoder. In the case of
the OFDM transmitter, a maximum clock cycles decrease of 82% relative to the
ones in an all fine-grain mapping solution is achieved. The corresponding
performance improvement for the JPEG is 43%.
</summary>
    <author>
      <name>M. D. Galanis</name>
    </author>
    <author>
      <name>A. Milidonis</name>
    </author>
    <author>
      <name>G. Theodoridis</name>
    </author>
    <author>
      <name>D. Soudris</name>
    </author>
    <author>
      <name>C. E. Goutis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.0838v2</id>
    <updated>2008-11-25T08:46:40Z</updated>
    <published>2007-11-06T11:16:34Z</published>
    <title>On the operating unit size of load/store architectures</title>
    <summary>  We introduce a strict version of the concept of a load/store instruction set
architecture in the setting of Maurer machines. We take the view that
transformations on the states of a Maurer machine are achieved by applying
threads as considered in thread algebra to the Maurer machine. We study how the
transformations on the states of the main memory of a strict load/store
instruction set architecture that can be achieved by applying threads depend on
the operating unit size, the cardinality of the instruction set, and the
maximal number of states of the threads.
</summary>
    <author>
      <name>J. A. Bergstra</name>
    </author>
    <author>
      <name>C. A. Middelburg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1017/S0960129509990314</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1017/S0960129509990314" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages; minor errors corrected, explanations added, references
  replaced</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Mathematical Structures in Computer Science, 20(3):395--417, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0711.0838v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.0838v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.0; F.1.1; F.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.2671v1</id>
    <updated>2007-11-16T20:18:32Z</updated>
    <published>2007-11-16T20:18:32Z</published>
    <title>Combined Integer and Variable Precision (CIVP) Floating Point
  Multiplication Architecture for FPGAs</title>
    <summary>  In this paper, we propose an architecture/methodology for making FPGAs
suitable for integer as well as variable precision floating point
multiplication. The proposed work will of great importance in applications
which requires variable precision floating point multiplication such as
multi-media processing applications. In the proposed architecture/methodology,
we propose the replacement of existing 18x18 bit and 25x18 bit dedicated
multipliers in FPGAs with dedicated 24x24 bit and 24x9 bit multipliers,
respectively. We have proved that our approach of providing the dedicated 24x24
bit and 24x9 bit multipliers in FPGAs will make them efficient for performing
integer as well as single precision, double precision, and Quadruple precision
floating point multiplications.
</summary>
    <author>
      <name>Himanshu Thapliyal</name>
    </author>
    <author>
      <name>Hamid R. Arabnia</name>
    </author>
    <author>
      <name>Rajnish Bajpai</name>
    </author>
    <author>
      <name>Kamal K. Sharma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Proceedings of the 2007 International Conference on
  Parallel and Distributed Processing Techniques and Applications (PDPTA'07),
  Las Vegas, U.S.A, June 2007, Volume 1, pp. 449-450.(CSREA Press)</arxiv:comment>
    <link href="http://arxiv.org/abs/0711.2671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.2671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.2674v1</id>
    <updated>2007-11-16T20:25:20Z</updated>
    <published>2007-11-16T20:25:20Z</published>
    <title>Partial Reversible Gates(PRG) for Reversible BCD Arithmetic</title>
    <summary>  IEEE 754r is the ongoing revision to the IEEE 754 floating point standard and
a major enhancement to the standard is the addition of decimal format.
Furthermore, in the recent years reversible logic has emerged as a promising
computing paradigm having its applications in low power CMOS, quantum
computing, nanotechnology, and optical computing. The major goal in reversible
logic is to minimize the number of reversible gates and garbage outputs. Thus,
this paper proposes the novel concept of partial reversible gates that will
satisfy the reversibility criteria for specific cases in BCD arithmetic. The
partial reversible gate is proposed to minimize the number of reversible gates
and garbage outputs, while designing the reversible BCD arithmetic circuits.
</summary>
    <author>
      <name>Himanshu Thapliyal</name>
    </author>
    <author>
      <name>Hamid R. Arabnia</name>
    </author>
    <author>
      <name>Rajnish Bajpai</name>
    </author>
    <author>
      <name>Kamal K. Sharma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Proceedings of the 2007 International Conference on
  Computer Design(CDES'07), Las Vegas, U.S.A, June 2007, pp. 90-91(CSREA Press)</arxiv:comment>
    <link href="http://arxiv.org/abs/0711.2674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.2674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.2640v1</id>
    <updated>2007-12-17T06:37:11Z</updated>
    <published>2007-12-17T06:37:11Z</published>
    <title>Optimal Memoryless Encoding for Low Power Off-Chip Data Buses</title>
    <summary>  Off-chip buses account for a significant portion of the total system power
consumed in embedded systems. Bus encoding schemes have been proposed to
minimize power dissipation, but none has been demonstrated to be optimal with
respect to any measure. In this paper, we give the first provably optimal and
explicit (polynomial-time constructible) families of memoryless codes for
minimizing bit transitions in off-chip buses. Our results imply that having
access to a clock does not make a memoryless encoding scheme that minimizes bit
transitions more powerful.
</summary>
    <author>
      <name>Yeow Meng Chee</name>
    </author>
    <author>
      <name>Charles J. Colbourn</name>
    </author>
    <author>
      <name>Alan C. H. Ling</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1233501.1233575</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1233501.1233575" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2006 IEEE/ACM international Conference on
  Computer-Aided Design (San Jose, California, November 05 - 09, 2006). ICCAD
  '06. ACM, New York, NY, 369-374</arxiv:comment>
    <link href="http://arxiv.org/abs/0712.2640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.2640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.4.3; B.m; E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.1765v1</id>
    <updated>2008-07-11T02:47:55Z</updated>
    <published>2008-07-11T02:47:55Z</published>
    <title>Archer: A Community Distributed Computing Infrastructure for Computer
  Architecture Research and Education</title>
    <summary>  This paper introduces Archer, a community-based computing resource for
computer architecture research and education. The Archer infrastructure
integrates virtualization and batch scheduling middleware to deliver
high-throughput computing resources aggregated from resources distributed
across wide-area networks and owned by different participating entities in a
seamless manner. The paper discusses the motivations leading to the design of
Archer, describes its core middleware components, and presents an analysis of
the functionality and performance of a prototype wide-area deployment running a
representative computer architecture simulation workload.
</summary>
    <author>
      <name>Renato Figueiredo</name>
    </author>
    <author>
      <name>P. Oscar Boykin</name>
    </author>
    <author>
      <name>Jose A. B. Fortes</name>
    </author>
    <author>
      <name>Tao Li</name>
    </author>
    <author>
      <name>Jie-Kwon Peir</name>
    </author>
    <author>
      <name>David Wolinsky</name>
    </author>
    <author>
      <name>Lizy John</name>
    </author>
    <author>
      <name>David Kaeli</name>
    </author>
    <author>
      <name>David Lilja</name>
    </author>
    <author>
      <name>Sally McKee</name>
    </author>
    <author>
      <name>Gokhan Memik</name>
    </author>
    <author>
      <name>Alain Roy</name>
    </author>
    <author>
      <name>Gary Tyson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-03354-4_7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-03354-4_7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures. Describes the Archer project,
  http://archer-project.org</arxiv:comment>
    <link href="http://arxiv.org/abs/0807.1765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.1765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.0; I.6.3; C.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.3732v1</id>
    <updated>2008-07-23T19:22:06Z</updated>
    <published>2008-07-23T19:22:06Z</published>
    <title>An adaptive embedded architecture for real-time Particle Image
  Velocimetry algorithms</title>
    <summary>  Particle Image Velocimetry (PIV) is a method of im-aging and analysing fields
of flows. The PIV tech-niques compute and display all the motion vectors of the
field in a resulting image. Speeds more than thou-sand vectors per second can
be required, each speed being environment-dependent. Essence of this work is to
propose an adaptive FPGA-based system for real-time PIV algorithms. The
proposed structure is ge-neric so that this unique structure can be re-used for
any PIV applications that uses the cross-correlation technique. The major
structure remains unchanged, adaptations only concern the number of processing
operations. The required speed (corresponding to the number of vector per
second) is obtained thanks to a parallel processing strategy. The image
processing designer duplicates the processing modules to distrib-ute the
operations. The result is a FPGA-based archi-tecture, which is easily adapted
to algorithm specifica-tions without any hardware requirement. The design flow
is fast and reliable.
</summary>
    <author>
      <name>Alain Aubert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAHC</arxiv:affiliation>
    </author>
    <author>
      <name>Nathalie Bochard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAHC</arxiv:affiliation>
    </author>
    <author>
      <name>Virginie Fresse</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAHC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14th European Signal Processing Conference - EUSIPCO 2006, Florence :
  Italie (2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/0807.3732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.3732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.1123v1</id>
    <updated>2009-01-08T20:14:00Z</updated>
    <published>2009-01-08T20:14:00Z</published>
    <title>A High Dynamic Range 3-Moduli-Set with Efficient Reverse Converter</title>
    <summary>  -Residue Number System (RNS) is a valuable tool for fast and parallel
arithmetic. It has a wide application in digital signal processing, fault
tolerant systems, etc. In this work, we introduce the 3-moduli set {2^n,
2^{2n}-1, 2^{2n}+1} and propose its residue to binary converter using the
Chinese Remainder Theorem. We present its simple hardware implementation that
mainly includes one Carry Save Adder (CSA) and a Modular Adder (MA). We compare
the performance and area utilization of our reverse converter to the reverse
converters of the moduli sets {2^n-1, 2^n, 2^n+1, 2^{2n}+1} and {2^n-1, 2^n,
2^n+1, 2^n-2^{(n+1)/2}+1, 2^n+2^{(n+1)/2}+1} that have the same dynamic range
and we demonstrate that our architecture is better in terms of performance and
area utilization. Also, we show that our reverse converter is faster than the
reverse converter of {2^n-1, 2^n, 2^n+1} for dynamic ranges like 8-bit, 16-bit,
32-bit and 64-bit however it requires more area.
</summary>
    <author>
      <name>Arash Hariri</name>
    </author>
    <author>
      <name>K. Navi</name>
    </author>
    <author>
      <name>Reza Rastegar</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers &amp; Mathematics with Applications (2008), Vol 55, No 4,
  660-668</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0901.1123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.1123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.4694v1</id>
    <updated>2009-01-29T14:46:48Z</updated>
    <published>2009-01-29T14:46:48Z</published>
    <title>Limit on the Addressability of Fault-Tolerant Nanowire Decoders</title>
    <summary>  Although prone to fabrication error, the nanowire crossbar is a promising
candidate component for next generation nanometer-scale circuits. In the
nanowire crossbar architecture, nanowires are addressed by controlling voltages
on the mesowires. For area efficiency, we are interested in the maximum number
of nanowires $N(m,e)$ that can be addressed by $m$ mesowires, in the face of up
to $e$ fabrication errors. Asymptotically tight bounds on $N(m,e)$ are
established in this paper. In particular, it is shown that $N(m,e) = \Theta(2^m
/ m^{e+1/2})$. Interesting observations are made on the equivalence between
this problem and the problem of constructing optimal EC/AUED codes,
superimposed distance codes, pooling designs, and diffbounded set systems.
Results in this paper also improve upon those in the EC/AUEC codes literature.
</summary>
    <author>
      <name>Yeow Meng Chee</name>
    </author>
    <author>
      <name>Alan C. H. Ling</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TC.2008.130</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TC.2008.130" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Computers, vol. 58, no. 1, pp. 60-68, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0901.4694v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.4694v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.3148v1</id>
    <updated>2009-04-21T00:34:43Z</updated>
    <published>2009-04-21T00:34:43Z</published>
    <title>CRT-Based High Speed Parallel Architecture for Long BCH Encoding</title>
    <summary>  BCH (Bose-Chaudhuri-Hocquenghen) error correcting codes ([1]-[2]) are now
widely used in communication systems and digital technology. Direct LFSR(linear
feedback shifted register)-based encoding of a long BCH code suffers from
serial-in and serial-out limitation and large fanout effect of some XOR gates.
This makes the LFSR-based encoders of long BCH codes cannot keep up with the
data transmission speed in some applications. Several parallel long parallel
encoders for long cyclic codes have been proposed in [3]-[8]. The technique for
eliminating the large fanout effect by J-unfolding method and some algebraic
manipulation was presented in [7] and [8] . In this paper we propose a
CRT(Chinese Remainder Theorem)-based parallel architecture for long BCH
encoding. Our novel technique can be used to eliminate the fanout bottleneck.
The only restriction on the speed of long BCH encoding of our CRT-based
architecture is $log_2N$, where $N$ is the length of the BCH code.
</summary>
    <author>
      <name>Hao Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0904.3148v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.3148v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3716v1</id>
    <updated>2010-01-21T04:34:07Z</updated>
    <published>2010-01-21T04:34:07Z</published>
    <title>A Multicore Processor based Real-Time System for Automobile management
  application</title>
    <summary>  In this paper we propose an Intelligent Management System which is capable of
managing the automobile functions using the rigorous real-time principles and a
multicore processor in order to realize higher efficiency and safety for the
vehicle. It depicts how various automobile functionalities can be fine grained
and treated to fit in real time concepts. It also shows how the modern
multicore processors can be of good use in organizing vast amounts of
correlated functions to be executed in real-time with excellent time
commitments. The modeling of the automobile tasks with real time commitments,
organizing appropriate scheduling for various real time tasks and the usage of
a multicore processor enables the system to realize higher efficiency and offer
better safety levels to the vehicle. The industry available real time operating
system is used for scheduling various tasks and jobs on the multicore
processor.
</summary>
    <author>
      <name>Vaidehi. M.</name>
    </author>
    <author>
      <name>T. R. Gopalakrishnan Nair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.3716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3781v1</id>
    <updated>2010-01-21T11:31:11Z</updated>
    <published>2010-01-21T11:31:11Z</published>
    <title>An Architectural Approach for Decoding and Distributing Functions in
  FPUs in a Functional Processor System</title>
    <summary>  The main goal of this research is to develop the concepts of a revolutionary
processor system called Functional Processor System. The fairly novel work
carried out in this proposal concentrates on decoding of function pipelines and
distributing it in FPUs as a part of scheduling approach. As the functional
programs are super-level programs that entails requirements only at functional
level, decoding of functions and distribution of functions in the heterogeneous
functional processor units are a challenge. We explored the possibilities of
segregation of the functions from the application program and distributing the
functions on the relevant FPUs by using address mapping techniques. Here we
pursue the perception of feeding the functions into the processor farm rather
than the processor fetching the instructions or functions and executing it.
This work is carried out at theoretical levels and it requires a long way to go
in the realization of this work in hardware perhaps with a large industrial
team with a pragmatic time frame.
</summary>
    <author>
      <name>T. R. Gopalakrishnan Nair</name>
    </author>
    <author>
      <name>R. Selva rani</name>
    </author>
    <author>
      <name>H. K. Krutthika</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.3781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.4493v1</id>
    <updated>2010-01-25T17:36:32Z</updated>
    <published>2010-01-25T17:36:32Z</published>
    <title>Maintaining Virtual Areas on FPGAs using Strip Packing with Delays</title>
    <summary>  Every year, the computing resources available on dynamically partially
reconfigurable devices increase enormously. In the near future, we expect many
applications to run on a single reconfigurable device. In this paper, we
present a concept for multitasking on dynamically partially reconfigurable
systems called virtual area management. We explain its advantages, show its
challenges, and discuss possible solutions. Furthermore, we investigate one
problem in more detail: Packing modules with time-varying resource requests.
This problem from the reconfigurable computing field results in a completely
new optimization problem not tackled before. ILP-based and heuristic approaches
are compared in an experimental study and the drawbacks and benefits discussed.
</summary>
    <author>
      <name>Josef Angermeier</name>
    </author>
    <author>
      <name>Sandor P. Fekete</name>
    </author>
    <author>
      <name>Tom Kamphans</name>
    </author>
    <author>
      <name>Nils Schweer</name>
    </author>
    <author>
      <name>Juergen Teich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 10 figures, 1 table, Latex, to appear in 17th Reconfigurable
  Architectures Workshop (RAW 2010)</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.4493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.4493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.1881v1</id>
    <updated>2010-02-09T15:21:20Z</updated>
    <published>2010-02-09T15:21:20Z</published>
    <title>Evaluation and Design Space Exploration of a Time-Division Multiplexed
  NoC on FPGA for Image Analysis Applications</title>
    <summary>  The aim of this paper is to present an adaptable Fat Tree NoC architecture
for Field Programmable Gate Array (FPGA) designed for image analysis
applications. Traditional NoCs (Network on Chip) are not optimal for dataflow
applications with large amount of data. On the opposite, point to point
communications are designed from the algorithm requirements but they are
expensives in terms of resource and wire. We propose a dedicated communication
architecture for image analysis algorithms. This communication mechanism is a
generic NoC infrastructure dedicated to dataflow image processing applications,
mixing circuit-switching and packet-switching communications. The complete
architecture integrates two dedicated communication architectures and reusable
IP blocks. Communications are based on the NoC concept to support the high
bandwidth required for a large number and type of data.
</summary>
    <author>
      <name>Linlin Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAHC</arxiv:affiliation>
    </author>
    <author>
      <name>Virginie Fresse</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAHC</arxiv:affiliation>
    </author>
    <author>
      <name>Mohammed Khalid</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">RCIM</arxiv:affiliation>
    </author>
    <author>
      <name>Dominique Houzet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GIPSA-lab</arxiv:affiliation>
    </author>
    <author>
      <name>Anne-Claire Legrand</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAHC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Eurasip Journal on Embedded Systems 2010 (2010) 542035</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.1881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.1881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.3990v1</id>
    <updated>2010-02-21T18:51:31Z</updated>
    <published>2010-02-21T18:51:31Z</published>
    <title>Static Address Generation Easing: a Design Methodology for Parallel
  Interleaver Architectures</title>
    <summary>  For high throughput applications, turbo-like iterative decoders are
implemented with parallel architectures. However, to be efficient parallel
architectures require to avoid collision accesses i.e. concurrent read/write
accesses should not target the same memory block. This consideration applies to
the two main classes of turbo-like codes which are Low Density Parity Check
(LDPC) and Turbo-Codes. In this paper we propose a methodology which finds a
collision-free mapping of the variables in the memory banks and which optimizes
the resulting interleaving architecture. Finally, we show through a pedagogical
example the interest of our approach compared to state-of-the-art techniques.
</summary>
    <author>
      <name>Cyrille Chavet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-Sticc, ST Microelectronics</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Coussy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-Sticc</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-Sticc</arxiv:affiliation>
    </author>
    <author>
      <name>Pascal Urard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ST Microelectronics</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.3990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.3990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.4465v1</id>
    <updated>2010-07-26T14:06:59Z</updated>
    <published>2010-07-26T14:06:59Z</published>
    <title>FPGA Implementation of a Reconfigurable Viterbi Decoder for WiMAX
  Receiver</title>
    <summary>  Field Programmable Gate Array technology (FPGA) is a highly configurable
option for implementing many sophisticated signal processing tasks in Software
Defined Radios (SDRs). Those types of radios are realized using highly
configurable hardware platforms. Convolutional codes are used in every robust
digital communication system and Viterbi algorithm is employed in wireless
communications to decode the convolutional codes. Such decoders are complex and
dissipate large amount of power. In this paper, a low power-reconfigurable
Viterbi decoder for WiMAX receiver is described using a VHDL code for FPGA
implementation. The proposed design is implemented on Xilinx Virtex-II Pro,
XC2vpx30 FPGA using the FPGA Advantage Pro package provided by Mentor Graphics
and ISE 10.1 by Xilinx.
</summary>
    <author>
      <name>Sherif Welsen Shaker</name>
    </author>
    <author>
      <name>Salwa Hussien Elramly</name>
    </author>
    <author>
      <name>Khaled Ali Shehata</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICM.2009.5418636</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICM.2009.5418636" rel="related"/>
    <link href="http://arxiv.org/abs/1007.4465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.4465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.3288v1</id>
    <updated>2010-08-19T12:25:44Z</updated>
    <published>2010-08-19T12:25:44Z</published>
    <title>Reversible Logic Synthesis of Fault Tolerant Carry Skip BCD Adder</title>
    <summary>  Reversible logic is emerging as an important research area having its
application in diverse fields such as low power CMOS design, digital signal
processing, cryptography, quantum computing and optical information processing.
This paper presents a new 4*4 parity preserving reversible logic gate, IG. The
proposed parity preserving reversible gate can be used to synthesize any
arbitrary Boolean function. It allows any fault that affects no more than a
single signal readily detectable at the circuit's primary outputs. It is shown
that a fault tolerant reversible full adder circuit can be realized using only
two IGs. The proposed fault tolerant full adder (FTFA) is used to design other
arithmetic logic circuits for which it is used as the fundamental building
block. It has also been demonstrated that the proposed design offers less
hardware complexity and is efficient in terms of gate count, garbage outputs
and constant inputs than the existing counterparts.
</summary>
    <author>
      <name>Md. Saiful Islam</name>
    </author>
    <author>
      <name>Zerina Begum</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3329/jbas.v32i2.2431</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3329/jbas.v32i2.2431" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures, 5 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bangladesh Academy of Science Journal, Vol. 32, No. 2, pp.
  193-200, December 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1008.3288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.3288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.3340v1</id>
    <updated>2010-08-19T16:12:11Z</updated>
    <published>2010-08-19T16:12:11Z</published>
    <title>Synthesis of Fault Tolerant Reversible Logic Circuits</title>
    <summary>  Reversible logic is emerging as an important research area having its
application in diverse fields such as low power CMOS design, digital signal
processing, cryptography, quantum computing and optical information processing.
This paper presents a new 4*4 universal reversible logic gate, IG. It is a
parity preserving reversible logic gate, that is, the parity of the inputs
matches the parity of the outputs. The proposed parity preserving reversible
gate can be used to synthesize any arbitrary Boolean function. It allows any
fault that affects no more than a single signal readily detectable at the
circuit's primary outputs. Finally, it is shown how a fault tolerant reversible
full adder circuit can be realized using only two IGs. It has also been
demonstrated that the proposed design offers less hardware complexity and is
efficient in terms of gate count, garbage outputs and constant inputs than the
existing counterparts.
</summary>
    <author>
      <name>Md. Saiful Islam</name>
    </author>
    <author>
      <name>Muhammad Mahbubur Rahman</name>
    </author>
    <author>
      <name>Zerina Begum</name>
    </author>
    <author>
      <name>Mohd. Zulfiquar Hafiz</name>
    </author>
    <author>
      <name>Abdullah Al Mahmud</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CAS-ICTD.2009.4960883</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CAS-ICTD.2009.4960883" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 9 figures, 7 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Testing and Diagnosis, 28-29
  April, 2009, Chengdu, China</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1008.3340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.3340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.3352v1</id>
    <updated>2010-08-19T17:07:51Z</updated>
    <published>2010-08-19T17:07:51Z</published>
    <title>Variable Block Carry Skip Logic using Reversible Gates</title>
    <summary>  Reversible circuits have applications in digital signal processing, computer
graphics, quantum computation and cryptography. In this paper, a generalized
k*k reversible gate family is proposed and a 3*3 gate of the family is
discussed. Inverter, AND, OR, NAND, NOR, and EXOR gates can be realized by this
gate. Implementation of a full-adder circuit using two such 3*3 gates is given.
This full-adder circuit contains only two reversible gates and produces no
extra garbage outputs. The proposed full-adder circuit is efficient in terms of
gate count, garbage outputs and quantum cost. A 4-bit carry skip adder is
designed using this full-adder circuit and a variable block carry skip adder is
discussed. Necessary equations required to evaluate these adder are presented.
</summary>
    <author>
      <name>Md. Rafiqul Islam</name>
    </author>
    <author>
      <name>Md. Saiful Islam</name>
    </author>
    <author>
      <name>Muhammad Rezaul Karim</name>
    </author>
    <author>
      <name>Abdullah Al Mahmud</name>
    </author>
    <author>
      <name>Hafiz Md. Hasan Babu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 7 figures, 3 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of 10th International Symposium on Integrated Circuits,
  Devices and Systems, Nanyang Technological University, Suntec, Singapore, pp
  9-12, 8-10 September, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1008.3352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.3352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.3357v1</id>
    <updated>2010-08-19T17:21:54Z</updated>
    <published>2010-08-19T17:21:54Z</published>
    <title>Building Toffoli Network for Reversible Logic Synthesis Based on
  Swapping Bit Strings</title>
    <summary>  In this paper, we have implemented and designed a sorting network for
reversible logic circuits synthesis in terms of n*n Toffoli gates. The
algorithm presented in this paper constructs a Toffoli Network based on
swapping bit strings. Reduction rules are then applied by simple template
matching and removing useless gates from the network. Random selection of bit
strings and reduction of control inputs are used to minimize both the number of
gates and gate width. The method produces near optimal results for up to
3-input 3-output circuits.
</summary>
    <author>
      <name>Hafiz Md. Hasaan Babu</name>
    </author>
    <author>
      <name>Md. Saiful Islam</name>
    </author>
    <author>
      <name>Md. Rafiqul Islam</name>
    </author>
    <author>
      <name>Lafifa Jamal</name>
    </author>
    <author>
      <name>Abu Ahmed Ferdaus</name>
    </author>
    <author>
      <name>Muhammad Rezaul Karim</name>
    </author>
    <author>
      <name>Abdullah Al Mahmud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dhaka University Journal of Science, Vol. 55, No. 2, pp. 153-156,
  2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1008.3357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.3357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.3452v2</id>
    <updated>2010-09-02T15:59:24Z</updated>
    <published>2010-08-20T07:53:24Z</published>
    <title>Memristor-based Circuits for Performing Basic Arithmetic Operations</title>
    <summary>  In almost all of the currently working circuits, especially in analog
circuits implementing signal processing applications, basic arithmetic
operations such as multiplication, addition, subtraction and division are
performed on values which are represented by voltages or currents. However, in
this paper, we propose a new and simple method for performing analog arithmetic
operations which in this scheme, signals are represented and stored through a
memristance of the newly found circuit element, i.e. memristor, instead of
voltage or current. Some of these operators such as divider and multiplier are
much simpler and faster than their equivalent voltage-based circuits and they
require less chip area. In addition, a new circuit is designed for programming
the memristance of the memristor with predetermined analog value. Presented
simulation results demonstrate the effectiveness and the accuracy of the
proposed circuits.
</summary>
    <author>
      <name>Farnood Merrikh-Bayat</name>
    </author>
    <author>
      <name>Saeed Bagheri Shouraki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5pages, 4 figures, Accepted in World Conference on Information
  Technology, turkey, 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1008.3452v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.3452v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.3533v1</id>
    <updated>2010-08-20T16:23:54Z</updated>
    <published>2010-08-20T16:23:54Z</published>
    <title>A Novel Quantum Cost Efficient Reversible Full Adder Gate in
  Nanotechnology</title>
    <summary>  Reversible logic has become one of the promising research directions in low
power dissipating circuit design in the past few years and has found its
applications in low power CMOS design, cryptography, optical information
processing and nanotechnology. This paper presents a novel and quantum cost
efficient reversible full adder gate in nanotechnology. This gate can work
singly as a reversible full adder unit and requires only one clock cycle. The
proposed gate is a universal gate in the sense that it can be used to
synthesize any arbitrary Boolean functions. It has been demonstrated that the
hardware complexity offered by the proposed gate is less than the existing
counterparts. The proposed reversible full adder gate also adheres to the
theoretical minimum established by the researchers.
</summary>
    <author>
      <name>Md. Saiful Islam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 12 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1008.3533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.3533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.1305v1</id>
    <updated>2010-09-07T14:26:09Z</updated>
    <published>2010-09-07T14:26:09Z</published>
    <title>Wideband Spectrum Sensing at Sub-Nyquist Rates</title>
    <summary>  We present a mixed analog-digital spectrum sensing method that is especially
suited to the typical wideband setting of cognitive radio (CR). The advantages
of our system with respect to current architectures are threefold. First, our
analog front-end is fixed and does not involve scanning hardware. Second, both
the analog-to-digital conversion (ADC) and the digital signal processing (DSP)
rates are substantially below Nyquist. Finally, the sensing resources are
shared with the reception path of the CR, so that the lowrate streaming samples
can be used for communication purposes of the device, besides the sensing
functionality they provide. Combining these advantages leads to a real time map
of the spectrum with minimal use of mobile resources. Our approach is based on
the modulated wideband converter (MWC) system, which samples sparse wideband
inputs at sub-Nyquist rates. We report on results of hardware experiments,
conducted on an MWC prototype circuit, which affirm fast and accurate spectrum
sensing in parallel to CR communication.
</summary>
    <author>
      <name>Moshe Mishali</name>
    </author>
    <author>
      <name>Yonina C. Eldar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.1305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.1305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.1796v1</id>
    <updated>2010-09-09T14:41:27Z</updated>
    <published>2010-09-09T14:41:27Z</published>
    <title>Power optimized programmable embedded controller</title>
    <summary>  Now a days, power has become a primary consideration in hardware design, and
is critical in computer systems especially for portable devices with high
performance and more functionality. Clock-gating is the most common technique
used for reducing processor's power. In this work clock gating technique is
applied to optimize the power of fully programmable Embedded Controller (PEC)
employing RISC architecture. The CPU designed supports i) smart instruction
set, ii) I/O port, UART iii) on-chip clocking to provide a range of frequencies
, iv) RISC as well as controller concepts. The whole design is captured using
VHDL and is implemented on FPGA chip using Xilinx .The architecture and clock
gating technique together is found to reduce the power consumption by 33.33% of
total power consumed by this chip.
</summary>
    <author>
      <name>M. Kamaraju</name>
    </author>
    <author>
      <name>K. Lal Kishore</name>
    </author>
    <author>
      <name>A. V. N. Tilak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcnc.2010.2409</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcnc.2010.2409" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages,11 figures,International Journal Publication</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Networks &amp; Communications
  (IJCNC),Vol.2, No.4, July 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.1796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.1796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.3819v1</id>
    <updated>2010-09-20T13:53:39Z</updated>
    <published>2010-09-20T13:53:39Z</published>
    <title>Fault Tolerant Variable Block Carry Skip Logic (VBCSL) using Parity
  Preserving Reversible Gates</title>
    <summary>  Reversible logic design has become one of the promising research directions
in low power dissipating circuit design in the past few years and has found its
application in low power CMOS design, digital signal processing and
nanotechnology. This paper presents the efficient design approaches of fault
tolerant carry skip adders (FTCSAs) and compares those designs with the
existing ones. Variable block carry skip logic (VBCSL) using the fault tolerant
full adders (FTFAs) has also been developed. The designs are minimized in terms
of hardware complexity, gate count, constant inputs and garbage outputs.
Besides of it, technology independent evaluation of the proposed designs
clearly demonstrates its superiority with the existing counterparts.
</summary>
    <author>
      <name>Md. Saiful Islam</name>
    </author>
    <author>
      <name>Muhammad Mahbubur Rahman</name>
    </author>
    <author>
      <name>Zerina Begum</name>
    </author>
    <author>
      <name>Mohd. Zulfiquar Hafiz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 16 figures, 2 tables, Accepted for publication in IJCEE,
  IACSIT, Singapore</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer and Electrical Engineering vol.
  3, no. 1, pp. 1-7, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.3819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.3819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4590v1</id>
    <updated>2010-09-23T12:01:38Z</updated>
    <published>2010-09-23T12:01:38Z</published>
    <title>A Unique 10 Segment Display for Bengali Numerals</title>
    <summary>  Segmented display is widely used for efficient display of alphanumeric
characters. English numerals are displayed by 7 segment and 16 segment display.
The segment size is uniform in this two display architecture. Display
architecture using 8, 10, 11, 18 segments have been proposed for Bengali
numerals 0...9 yet no display architecture is designed using segments of
uniform size and uniform power consumption. In this paper we have proposed a
uniform 10 segment architecture for Bengali numerals. This segment architecture
uses segments of uniform size and no bent segment is used.
</summary>
    <author>
      <name>Md. Abul Kalam Azad</name>
    </author>
    <author>
      <name>Rezwana Sharmeen</name>
    </author>
    <author>
      <name>Shabbir Ahmad</name>
    </author>
    <author>
      <name>S. M. Kamruzzaman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 Pages, International Conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 8th International Conference on Computer and Information
  Technology (ICCIT 2005), Dhaka, Bangladesh, pp. 97-99, Dec. 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.4590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4977v1</id>
    <updated>2010-09-25T06:17:02Z</updated>
    <published>2010-09-25T06:17:02Z</published>
    <title>Universal Numeric Segmented Display</title>
    <summary>  Segmentation display plays a vital role to display numerals. But in today's
world matrix display is also used in displaying numerals. Because numerals has
lots of curve edges which is better supported by matrix display. But as matrix
display is costly and complex to implement and also needs more memory, segment
display is generally used to display numerals. But as there is yet no proposed
compact display architecture to display multiple language numerals at a time,
this paper proposes uniform display architecture to display multiple language
digits and general mathematical expressions with higher accuracy and simplicity
by using a 18-segment display, which is an improvement over the 16 segment
display.
</summary>
    <author>
      <name>Md. Abul kalam Azad</name>
    </author>
    <author>
      <name>Rezwana Sharmeen</name>
    </author>
    <author>
      <name>S. M. Kamruzzaman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages, International Conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 7th International Conference on Computer and Information
  Technology (ICCIT-2004), Dhaka, Bangladesh, pp. 887-892, Dec. 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.4977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.6132v1</id>
    <updated>2010-09-09T04:43:52Z</updated>
    <published>2010-09-09T04:43:52Z</published>
    <title>Multi-standard programmable baseband modulator for next generation
  wireless communication</title>
    <summary>  Considerable research has taken place in recent times in the area of
parameterization of software defined radio (SDR) architecture. Parameterization
decreases the size of the software to be downloaded and also limits the
hardware reconfiguration time. The present paper is based on the design and
development of a programmable baseband modulator that perform the QPSK
modulation schemes and as well as its other three commonly used variants to
satisfy the requirement of several established 2G and 3G wireless communication
standards. The proposed design has been shown to be capable of operating at a
maximum data rate of 77 Mbps on Xilinx Virtex 2-Pro University field
programmable gate array (FPGA) board. The pulse shaping root raised cosine
(RRC) filter has been implemented using distributed arithmetic (DA) technique
in the present work in order to reduce the computational complexity, and to
achieve appropriate power reduction and enhanced throughput. The designed
multiplier-less programmable 32-tap FIR-based RRC filter has been found to
withstand a peak inter-symbol interference (ISI) distortion of -41 dBs
</summary>
    <author>
      <name>Indranil Hatai</name>
    </author>
    <author>
      <name>Indrajit Chakrabarti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcnc.2010.2406</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcnc.2010.2406" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Networks and Communications
  (IJCNC). Vol.2, No. 4, pp. 58-71, July 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.6132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.6132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.2919v1</id>
    <updated>2010-11-12T14:38:52Z</updated>
    <published>2010-11-12T14:38:52Z</published>
    <title>Hardware architectures for Successive Cancellation Decoding of Polar
  Codes</title>
    <summary>  The recently-discovered polar codes are widely seen as a major breakthrough
in coding theory. These codes achieve the capacity of many important channels
under successive cancellation decoding. Motivated by the rapid progress in the
theory of polar codes, we propose a family of architectures for efficient
hardware implementation of successive cancellation decoders. We show that such
decoders can be implemented with O(n) processing elements and O(n) memory
elements, while providing constant throughput. We also propose a technique for
overlapping the decoding of several consecutive codewords, thereby achieving a
significant speed-up factor. We furthermore show that successive cancellation
decoding can be implemented in the logarithmic domain, thereby eliminating the
multiplication and division operations and greatly reducing the complexity of
each processing element.
</summary>
    <author>
      <name>Camille Leroux</name>
    </author>
    <author>
      <name>Ido Tal</name>
    </author>
    <author>
      <name>Alexander Vardy</name>
    </author>
    <author>
      <name>Warren J. Gross</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICASSP 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.2919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.2919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.4157v2</id>
    <updated>2012-07-24T00:57:49Z</updated>
    <published>2010-11-18T09:12:50Z</published>
    <title>A full-custom ASIC design of a 8-bit, 25 MHz, Pipeline ADC using 0.35 um
  CMOS technology</title>
    <summary>  The purpose of this project was to design and implement a pipeline
Analog-to-Digital Converter using 0.35um CMOS technology. Initial requirements
of a 25-MHz conversion rate and 8-bits of resolution where the only given ones.
Although additional secondary goals such as low power consumption and small
area were stated. The architecture is based on a 1.5 bit per stage structure
utilizing digital correction for each stage [12]. A differential switched
capacitor circuit consisting of a cascade gm-C op-amp with 200MHz ft is used
for sampling and amplification in each stage [12]. Differential dynamic
comparators are used to implement the decision levels required for the 1.5-b
per stage structure. Correction of the pipeline is accomplished by using
digital correction circuit consist of D-latches and full-adders. Area and Power
consumption of whole design was 0.24mm2 and 35mW respectively. The maximum
sample rate at which the converter gave an adequate output was 33MHz.
</summary>
    <author>
      <name>Moslem Rashidi</name>
    </author>
    <author>
      <name>Mikael Hogrud</name>
    </author>
    <author>
      <name>Donatas Siaudinis</name>
    </author>
    <author>
      <name>Affaq Qamar</name>
    </author>
    <author>
      <name>Imran Khan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">the paper needs to be withdrawn because of copyright conflicts</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.4157v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.4157v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.0884v1</id>
    <updated>2011-02-04T11:40:14Z</updated>
    <published>2011-02-04T11:40:14Z</published>
    <title>A Simulation Experiment on a Built-In Self Test Equipped with
  Pseudorandom Test Pattern Generator and Multi-Input Shift Register (MISR)</title>
    <summary>  This paper investigates the impact of the changes of the characteristic
polynomials and initial loadings, on behaviour of aliasing errors of parallel
signature analyzer (Multi-Input Shift Register), used in an LFSR based digital
circuit testing technique. The investigation is carried-out through an
extensive simulation study of the effectiveness of the LFSR based digital
circuit testing technique. The results of the study show that when the
identical characteristic polynomials of order n are used in both pseudo-random
test-pattern generator, as well as in Multi-Input Shift Register (MISR)
signature analyzer (parallel type) then the probability of aliasing errors
remains unchanged due to the changes in the initial loadings of the
pseudo-random test-pattern generator.
</summary>
    <author>
      <name>A. Ahmad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages,2 figures, Journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International journal of VLSI Design &amp; Communication Systems,
  vol.1, no.4, pp. 1-12, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1102.0884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.0884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.1360v2</id>
    <updated>2011-03-08T12:13:20Z</updated>
    <published>2011-03-07T20:04:05Z</published>
    <title>A Secure Asynchronous FPGA Architecture, Experimental Results and Some
  Debug Feedback</title>
    <summary>  This article presents an asynchronous FPGA architecture for implementing
cryptographic algorithms secured against physical cryptanalysis. We discuss the
suitability of asynchronous reconfigurable architectures for such applications
before proceeding to model the side channel and defining our objectives. The
logic block architecture is presented in detail. We discuss several solutions
for the interconnect architecture, and how these solutions can be ported to
other flavours of interconnect (i.e. single driver). Next We discuss in detail
a high speed asynchronous configuration chain architecture used to configure
our asynchronous FPGA with simulation results, and we present a 3 X 3 prototype
FPGA fabricated in 65 nm CMOS. Lastly we present experiments to test the high
speed asynchronous configuration chain and evaluate how far our objectives have
been achieved with proposed solutions, and we conclude with emphasis on
complementary FPGA CAD algorithms, and the effect of CMOS variation on
Side-Channel Vulnerability.
</summary>
    <author>
      <name>Sumanta Chaudhuri</name>
    </author>
    <author>
      <name>Sylvain Guilley</name>
    </author>
    <author>
      <name>Philippe Hoogvorst</name>
    </author>
    <author>
      <name>Jean-Luc Danger</name>
    </author>
    <author>
      <name>Taha Beyrouthy</name>
    </author>
    <author>
      <name>Alin Razafindraibe</name>
    </author>
    <author>
      <name>Laurent Fesquet</name>
    </author>
    <author>
      <name>Marc Renaudin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 Pages, 23 figures. In detail description of a 3X3 Aysnchronous
  FPGA Tape-Out</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.1360v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.1360v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.3310v1</id>
    <updated>2011-04-17T12:15:42Z</updated>
    <published>2011-04-17T12:15:42Z</published>
    <title>Computer Arithmetic Preserving Hamming Distance of Operands in Operation
  Result</title>
    <summary>  The traditional approach to fault tolerant computing involves replicating
computation units and applying a majority vote operation on individual result
bits. This approach, however, has several limitations; the most severe is the
resource requirement. This paper presents a new method for fault tolerant
computing where for a given error rate, the hamming distance between correct
inputs and faulty inputs as well as the hamming distance between a correct
result and a faulty result is preserved throughout processing thereby enabling
correction of up to transient faults per computation cycle. The new method is
compared and contrasted with current protection methods and its cost /
performance is analyzed.
</summary>
    <author>
      <name>Shlomi Dolev</name>
    </author>
    <author>
      <name>Sergey Frenkel</name>
    </author>
    <author>
      <name>Dan Tamir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.3310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.3310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.2568v1</id>
    <updated>2011-06-13T22:29:08Z</updated>
    <published>2011-06-13T22:29:08Z</published>
    <title>HMTT: A Hybrid Hardware/Software Tracing System for Bridging Memory
  Trace's Semantic Gap</title>
    <summary>  Memory trace analysis is an important technology for architecture research,
system software (i.e., OS, compiler) optimization, and application performance
improvements. Hardware-snooping is an effective and efficient approach to
monitor and collect memory traces. Compared with software-based approaches,
memory traces collected by hardware-based approaches are usually lack of
semantic information, such as process/function/loop identifiers, virtual
address and I/O access. In this paper we propose a hybrid hardware/software
mechanism which is able to collect memory reference trace as well as semantic
information. Based on this mechanism, we designed and implemented a prototype
system called HMTT (Hybrid Memory Trace Tool) which adopts a DIMMsnooping
mechanism to snoop on memory bus and a software-controlled tracing mechanism to
inject semantic information into normal memory trace. To the best of our
knowledge, the HMTT system is the first hardware tracing system capable of
correlating memory trace with high-level events. Comprehensive validations and
evaluations show that the HMTT system has both hardware's (e.g., no distortion
or pollution) and software's advantages (e.g., flexibility and more
information).
</summary>
    <author>
      <name>Yungang Bao</name>
    </author>
    <author>
      <name>Jinyong Zhang</name>
    </author>
    <author>
      <name>Yan Zhu</name>
    </author>
    <author>
      <name>Dan Tang</name>
    </author>
    <author>
      <name>Yuan Ruan</name>
    </author>
    <author>
      <name>Mingyu Chen</name>
    </author>
    <author>
      <name>Jianping Fan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 papges, an extension version of ACM SIGMETRICS'08 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.2568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.2568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.0708v1</id>
    <updated>2011-09-04T12:28:46Z</updated>
    <published>2011-09-04T12:28:46Z</published>
    <title>A Novel Methodology for Thermal Analysis &amp; 3-Dimensional Memory
  Integration</title>
    <summary>  The semiconductor industry is reaching a fascinating confluence in several
evolutionary trends that will likely lead to a number of revolutionary changes
in the design, implementation, scaling, and the use of computer systems.
However, recently Moore's law has come to a stand-still since device scaling
beyond 65 nm is not practical. 2D integration has problems like memory latency,
power dissipation, and large foot-print. 3D technology comes as a solution to
the problems posed by 2D integration. The utilization of 3D is limited by the
problem of temperature crisis. It is important to develop an accurate power
profile extraction methodology to design 3D structure. In this paper, design of
3D integration of memory is considered and hence the static power dissipation
of the memory cell is analysed in transistor level and is used to accurately
model the inter-layer thermal effects for 3D memory stack. Subsequently,
packaging of the chip is considered and modelled using an architecture level
simulator. This modelling is intended to analyse the thermal effects of 3D
memory, its reliability and lifetime of the chip, with greater accuracy.
</summary>
    <author>
      <name>Annmol Cherian</name>
    </author>
    <author>
      <name>Ajay Augustine</name>
    </author>
    <author>
      <name>Jemy Jose</name>
    </author>
    <author>
      <name>Vinod Pangracious</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijait.2011.1403</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijait.2011.1403" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Information Technology (IJAIT)
  Vol. 1, No. 4, August 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.0708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.0708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.0752v1</id>
    <updated>2011-09-04T20:02:53Z</updated>
    <published>2011-09-04T20:02:53Z</published>
    <title>An improved distributed routing algorithm for Benes based optical NoC</title>
    <summary>  Integrated optical interconnect is believed to be one of the main
technologies to replace electrical wires. Optical Network-on-Chip (ONoC) has
attracted more attentions nowadays. Benes topology is a good choice for ONoC
for its rearrangeable non-blocking character, multistage feature and easy
scalability. Routing algorithm plays an important role in determining the
performance of ONoC. But traditional routing algorithms for Benes network are
not suitable for ONoC communication, we developed a new distributed routing
algorithm for Benes ONoC in this paper. Our algorithm selected the routing path
dynamically according to network condition and enables more path choices for
the message traveling in the network. We used OPNET to evaluate the performance
of our routing algorithm and also compared it with a well-known bit-controlled
routing algorithm. ETE delay and throughput were showed under different packet
length and network sizes. Simulation results show that our routing algorithm
can provide better performance for ONoC.
</summary>
    <author>
      <name>Jing Zhang</name>
    </author>
    <author>
      <name>Huaxi Gu</name>
    </author>
    <author>
      <name>Yintang Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.0752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.0752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.0755v1</id>
    <updated>2011-09-04T20:46:08Z</updated>
    <published>2011-09-04T20:46:08Z</published>
    <title>Intelligent Bees for QoS Routing in Networks-on-Chip</title>
    <summary>  Networks-on-Chip (NoCs) for future many-core processor platforms integrate
more and more heterogeneous components of different types and many real-time
and latency-sensitive applications can run on a single chip concurrently. The
reconfigurable FPGA and reconfigurable NoCs have emerged for the purpose of
reusability. Those types' traffics within NoCs exhibit diverse, burst, and
unpredictable communication patterns. QoS guaranteed mechanisms are necessary
to provide guaranteed throughput (GT) or guaranteed bandwidth (GB) performance
for NoCs. In this paper, we propose a QoS routing algorithm inspired by bees'
foraging behaviors to provide guaranteed bandwidth performance. Virtual
circuits and Spatial Division Multiplexing are employed to maintain available
paths for different type's traffics.
</summary>
    <author>
      <name>Peibo Xie</name>
    </author>
    <author>
      <name>Huaxi Gu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures, IEEE 2010 Second Pacific-Asia Conference on
  Circuits, Communications and System (PACCS)</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.0755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.0755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.3281v4</id>
    <updated>2012-04-26T11:52:33Z</updated>
    <published>2011-10-14T17:52:39Z</published>
    <title>Faster Energy Efficient Dadda Based Baugh-Wooley Multipliers</title>
    <summary>  In this work faster Baugh-Wooley multiplication has been achieved by using a
combination of two design techniques: partition of the partial products into
two parts for independent parallel column compression and acceleration of the
final addition using a hybrid adder proposed in this work. Based on the
proposed techniques 8, 16, 32 and 64-bit Dadda based Baugh-Wooley multipliers
has been developed and compared with the regular Baugh-Wooley multiplier. The
performance of the proposed multiplier is analyzed by evaluating the delay,
area and power, with 180 nm process technologies on interconnect and layout
using industry standard design and layout tools. The result analysis shows that
the 64-bit proposed multiplier is as much as 26.9% faster than the regular
Baugh-Wooley multiplier and requires only 2.21% more power. Also the
power-delay product of the proposed design is significantly lower than that of
the regular Baugh-Wooley multiplier.
</summary>
    <author>
      <name>B. Ramkumar</name>
    </author>
    <author>
      <name>V. Sreedeep</name>
    </author>
    <author>
      <name>Harish M Kittur</name>
    </author>
    <link href="http://arxiv.org/abs/1110.3281v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.3281v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.2.4; B.7.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.3376v2</id>
    <updated>2011-10-19T06:45:18Z</updated>
    <published>2011-10-15T03:54:27Z</published>
    <title>Faster and Low Power Twin Precision Multiplier</title>
    <summary>  In this work faster unsigned multiplication has been achieved by using a
combination of High Performance Multiplication [HPM] column reduction technique
and implementing a N-bit multiplier using 4 N/2-bit multipliers (recursive
multiplication) and acceleration of the final addition using a hybrid adder.
Low power has been achieved by using clock gating technique. Based on the
proposed technique 16 and 32-bit multipliers are developed. The performance of
the proposed multiplier is analyzed by evaluating the delay, area and power,
with TCBNPHP 90 nm process technology on interconnect and layout using Cadence
NC launch, RTL compiler and ENCOUNTER tools. The results show that the 32-bit
proposed multiplier is as much as 22% faster, occupies only 3% more area and
consumes 30% lesser power with respect to the recently reported twin precision
multiplier.
</summary>
    <author>
      <name>V. Sreedeep</name>
    </author>
    <author>
      <name>B. Ramkumar</name>
    </author>
    <author>
      <name>Harish M Kittur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 10 figures and 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.3376v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.3376v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.2.4; B.7.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.3584v1</id>
    <updated>2011-10-17T06:16:05Z</updated>
    <published>2011-10-17T06:16:05Z</published>
    <title>Optimal Final Carry Propagate Adder Design for Parallel Multipliers</title>
    <summary>  Based on the ASIC layout level simulation of 7 types of adder structures each
of four different sizes, i.e. a total of 28 adders, we propose expressions for
the width of each of the three regions of the final Carry Propagate Adder (CPA)
to be used in parallel multipliers. We also propose the types of adders to be
used in each region that would lead to the optimal performance of the hybrid
final adders in parallel multipliers. This work evaluates the complete
performance of the analyzed designs in terms of delay, area, power through
custom design and layout in 0.18 um CMOS process technology.
</summary>
    <author>
      <name>Ramkumar B.</name>
    </author>
    <author>
      <name>Harish M. Kittur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures, 2 tables, Submitted 0n 26 August 2011 to IEEE
  Transactions on VLSI Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.3584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.3584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.2.4; B.7.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.3655v1</id>
    <updated>2011-10-17T13:00:10Z</updated>
    <published>2011-10-17T13:00:10Z</published>
    <title>Accelerating Algorithms using a Dataflow Graph in a Reconfigurable
  System</title>
    <summary>  In this paper, the acceleration of algorithms using a design of a field
programmable gate array (FPGA) as a prototype of a static dataflow architecture
is discussed. The static dataflow architecture using operators interconnected
by parallel buses was implemented. Accelerating algorithms using a dataflow
graph in a reconfigurable system shows the potential for high computation
rates. The results of benchmarks implemented using the static dataflow
architecture are reported at the end of this paper.
</summary>
    <author>
      <name>Jorge Luiz e Silva</name>
    </author>
    <author>
      <name>Joelmir Jose Lopes</name>
    </author>
    <author>
      <name>Bruno de Abreu Silva</name>
    </author>
    <author>
      <name>Antonio Carlos Fernandes da Silva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 8 figures, 1 listing, 1 algorithm, 1 Table</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.3655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.3655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.0703v1</id>
    <updated>2011-11-03T01:10:43Z</updated>
    <published>2011-11-03T01:10:43Z</published>
    <title>Efficient Network for Non-Binary QC-LDPC Decoder</title>
    <summary>  This paper presents approaches to develop efficient network for non-binary
quasi-cyclic LDPC (QC-LDPC) decoders. By exploiting the intrinsic shifting and
symmetry properties of the check matrices, significant reduction of memory size
and routing complexity can be achieved. Two different efficient network
architectures for Class-I and Class-II non-binary QC-LDPC decoders have been
proposed, respectively. Comparison results have shown that for the code of the
64-ary (1260, 630) rate-0.5 Class-I code, the proposed scheme can save more
than 70.6% hardware required by shuffle network than the state-of-the-art
designs. The proposed decoder example for the 32-ary (992, 496) rate-0.5
Class-II code can achieve a 93.8% shuffle network reduction compared with the
conventional ones. Meanwhile, based on the similarity of Class-I and Class-II
codes, similar shuffle network is further developed to incorporate both classes
of codes at a very low cost.
</summary>
    <author>
      <name>Chuan Zhang</name>
    </author>
    <author>
      <name>Keshab K. Parhi</name>
    </author>
    <link href="http://arxiv.org/abs/1111.0703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.0703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.0704v1</id>
    <updated>2011-11-03T01:12:49Z</updated>
    <published>2011-11-03T01:12:49Z</published>
    <title>Reduced-Latency SC Polar Decoder Architectures</title>
    <summary>  Polar codes have become one of the most favorable capacity achieving error
correction codes (ECC) along with their simple encoding method. However, among
the very few prior successive cancellation (SC) polar decoder designs, the
required long code length makes the decoding latency high. In this paper,
conventional decoding algorithm is transformed with look-ahead techniques. This
reduces the decoding latency by 50%. With pipelining and parallel processing
schemes, a parallel SC polar decoder is proposed. Sub-structure sharing
approach is employed to design the merged processing element (PE). Moreover,
inspired by the real FFT architecture, this paper presents a novel input
generating circuit (ICG) block that can generate additional input signals for
merged PEs on-the-fly. Gate-level analysis has demonstrated that the proposed
design shows advantages of 50% decoding latency and twice throughput over the
conventional one with similar hardware cost.
</summary>
    <author>
      <name>Chuan Zhang</name>
    </author>
    <author>
      <name>Bo Yuan</name>
    </author>
    <author>
      <name>Keshab K. Parhi</name>
    </author>
    <link href="http://arxiv.org/abs/1111.0704v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.0704v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.3056v1</id>
    <updated>2011-11-13T19:28:01Z</updated>
    <published>2011-11-13T19:28:01Z</published>
    <title>Performance of Cache Memory Subsystems for Multicore Architectures</title>
    <summary>  Advancements in multi-core have created interest among many research groups
in finding out ways to harness the true power of processor cores. Recent
research suggests that on-board component such as cache memory plays a crucial
role in deciding the performance of multi-core systems. In this paper,
performance of cache memory is evaluated through the parameters such as cache
access time, miss rate and miss penalty. The influence of cache parameters over
execution time is also discussed. Results obtained from simulated studies of
multi-core environments with different instruction set architectures (ISA) like
ALPHA and X86 are produced.
</summary>
    <author>
      <name>N. Ramasubramanian</name>
    </author>
    <author>
      <name>Srinivas V. V.</name>
    </author>
    <author>
      <name>N. Ammasai Gounden</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsea.2011.1506</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsea.2011.1506" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science, Engineering and
  Applications (IJCSEA), Vol. 1, No. 5, pp. 59-71, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1111.3056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.3056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.4362v1</id>
    <updated>2011-11-18T13:40:16Z</updated>
    <published>2011-11-18T13:40:16Z</published>
    <title>Hardware Implementation of Successive Cancellation Decoders for Polar
  Codes</title>
    <summary>  The recently-discovered polar codes are seen as a major breakthrough in
coding theory; they provably achieve the theoretical capacity of discrete
memoryless channels using the low complexity successive cancellation (SC)
decoding algorithm. Motivated by recent developments in polar coding theory, we
propose a family of efficient hardware implementations for SC polar decoders.
We show that such decoders can be implemented with O(n) processing elements,
O(n) memory elements, and can provide a constant throughput for a given target
clock frequency. Furthermore, we show that SC decoding can be implemented in
the logarithm domain, thereby eliminating costly multiplication and division
operations and reducing the complexity of each processing element greatly. We
also present a detailed architecture for an SC decoder and provide logic
synthesis results confirming the linear growth in complexity of the decoder as
the code length increases.
</summary>
    <author>
      <name>Camille Leroux</name>
    </author>
    <author>
      <name>Alexandre J. Raymond</name>
    </author>
    <author>
      <name>Gabi Sarkis</name>
    </author>
    <author>
      <name>Ido Tal</name>
    </author>
    <author>
      <name>Alexander Vardy</name>
    </author>
    <author>
      <name>Warren J. Gross</name>
    </author>
    <link href="http://arxiv.org/abs/1111.4362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.4362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.7258v1</id>
    <updated>2011-11-30T18:07:44Z</updated>
    <published>2011-11-30T18:07:44Z</published>
    <title>A New Design for Array Multiplier with Trade off in Power and Area</title>
    <summary>  In this paper a low power and low area array multiplier with carry save adder
is proposed. The proposed adder eliminates the final addition stage of the
multiplier than the conventional parallel array multiplier. The conventional
and proposed multiplier both are synthesized with 16-T full adder. Among
Transmission Gate, Transmission Function Adder, 14-T, 16-T full adder shows
energy efficiency. In the proposed 4x4 multiplier to add carry bits with out
using Ripple Carry Adder (RCA) in the final stage, the carries given to the
input of the next left column input. Due to this the proposed multiplier shows
56 less transistor count, then cause trade off in power and area. The proposed
multiplier has shown 13.91% less power, 34.09% more speed and 59.91% less
energy consumption for TSMC 0.18nm technology at a supply voltage 2.0V than the
conventional multiplier.
</summary>
    <author>
      <name>Nirlakalla Ravi</name>
    </author>
    <author>
      <name>A. Satish</name>
    </author>
    <author>
      <name>T. Jayachandra Prasad</name>
    </author>
    <author>
      <name>T. Subba Rao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures; IJCSI International Journal of Computer Science
  Issues, Vol. 8, Issue 3, No. 2, May 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.7258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.7258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.1674v1</id>
    <updated>2012-01-09T01:00:44Z</updated>
    <published>2012-01-09T01:00:44Z</published>
    <title>Theoretical Modeling and Simulation of Phase-Locked Loop (PLL) for Clock
  Data Recovery (CDR)</title>
    <summary>  Modern communication and computer systems require rapid (Gbps), efficient and
large bandwidth data transfers. Agressive scaling of digital integrated systems
allow buses and communication controller circuits to be integrated with the
microprocessor on the same chip. The Peripheral Component Interconnect Express
(PCIe) protocol handles all communcation between the central processing unit
(CPU) and hardware devices. PCIe buses require efficient clock data recovery
circuits (CDR) to recover clock signals embedded in data during transmission.
This paper describes the theoretical modeling and simulation of a phase-locked
loop (PLL) used in a CDR circuit. A simple PLL architecture for a 5 GHz CDR
circuit is proposed and elaborated in this work. Simulations were carried out
using a Hardware Description Language, Verilog- AMS. The effect of jitter on
the proposed design is also simulated and evaluated in this work. It was found
that the proposed design is robust against both input and VCO jitter.
</summary>
    <author>
      <name>Zainab Ashari</name>
    </author>
    <author>
      <name>Anis Nurashikin Nordin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.1674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.1674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.2107v1</id>
    <updated>2012-01-10T16:46:57Z</updated>
    <published>2012-01-10T16:46:57Z</published>
    <title>Design and ASIC implementation of DUC/DDC for communication systems</title>
    <summary>  Communication systems use the concept of transmitting information using the
electrical distribution network as a communication channel. To enable the
transmission data signal modulated on a carrier signal is superimposed on the
electrical wires. Typical power lines are designed to handle 50/60 Hz of AC
power signal; however they can carry the signals up to 500 KHz frequency. This
work aims to aid transmission/reception of an audio signal in the spectrum from
300 Hz to 4000 Hz using PLCC on a tunable carrier frequency in the spectrum
from 200 KHz to 500 KHz. For digital amplitude modulation the sampling rate of
the carrier and the audio signal has to be matched. Tunable carrier generation
can be achieved with Direct Digital Synthesizers at a desired sampling rate.
DSP Sample rate conversion techniques are very useful to make the sampling
circuits to work on their own sampling rates which are fine for the
data/modulated-carrier signal's bandwidth. This also simplifies the complexity
of the sampling circuits. Digital Up Conversion (DUC) and Digital Down
Conversion (DDC) are DSP sample rate conversion techniques which refer to
increasing and decreasing the sampling rate of a signal respectively.
</summary>
    <author>
      <name>Naagesh S. Bhat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/vlsic.2011.2410</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/vlsic.2011.2410" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of VLSI design &amp; Communication Systems
  (VLSICS) Vol.2, No.4, December 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1201.2107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.2107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.0613v1</id>
    <updated>2012-02-03T06:47:39Z</updated>
    <published>2012-02-03T06:47:39Z</published>
    <title>A Resolution for Shared Memory Conflict in Multiprocessor
  System-on-a-Chip</title>
    <summary>  Now days, manufacturers are focusing on increasing the concurrency in
multiprocessor system-on-a-chip (MPSoC) architecture instead of increasing
clock speed, for embedded systems. Traditionally lock-based synchronization is
provided to support concurrency; as managing locks can be very difficult and
error prone. Transactional memories and lock based systems have been
extensively used to provide synchronization between multiple processors [1] in
general-purpose systems. It has been shown that locks have numerous
shortcomings over transactional memory in terms of power consumption, ease of
programming and performance. In this paper, we propose a new semaphore scheme
for synchronization in shared cache memory in an MPSoC. Moreover, we have
evaluated and compared our scheme with locks and transactions in terms of
energy consumption and cache miss rate using SimpleScalar functional simulator.
</summary>
    <author>
      <name>Shaily Mittal</name>
    </author>
    <author>
      <name> Nitin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 4, No 1, July 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.0613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.0613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.1536v1</id>
    <updated>2012-03-07T16:46:05Z</updated>
    <published>2012-03-07T16:46:05Z</published>
    <title>The Distributed Network Processor: a novel off-chip and on-chip
  interconnection network architecture</title>
    <summary>  One of the most demanding challenges for the designers of parallel computing
architectures is to deliver an efficient network infrastructure providing low
latency, high bandwidth communications while preserving scalability. Besides
off-chip communications between processors, recent multi-tile (i.e. multi-core)
architectures face the challenge for an efficient on-chip interconnection
network between processor's tiles. In this paper, we present a configurable and
scalable architecture, based on our Distributed Network Processor (DNP) IP
Library, targeting systems ranging from single MPSoCs to massive HPC platforms.
The DNP provides inter-tile services for both on-chip and off-chip
communications with a uniform RDMA style API, over a multi-dimensional direct
network with a (possibly) hybrid topology.
</summary>
    <author>
      <name>Andrea Biagioni</name>
    </author>
    <author>
      <name>Francesca Lo Cicero</name>
    </author>
    <author>
      <name>Alessandro Lonardo</name>
    </author>
    <author>
      <name>Pier Stanislao Paolucci</name>
    </author>
    <author>
      <name>Mersia Perra</name>
    </author>
    <author>
      <name>Davide Rossetti</name>
    </author>
    <author>
      <name>Carlo Sidore</name>
    </author>
    <author>
      <name>Francesco Simula</name>
    </author>
    <author>
      <name>Laura Tosoratto</name>
    </author>
    <author>
      <name>Piero Vicini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 11 figures, submitted to Hot Interconnect 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.1536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.1536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.4150v1</id>
    <updated>2012-03-19T16:18:54Z</updated>
    <published>2012-03-19T16:18:54Z</published>
    <title>Designing a WISHBONE Protocol Network Adapter for an Asynchronous
  Network-on-Chip</title>
    <summary>  The Scaling of microchip technologies, from micron to submicron and now to
deep sub-micron (DSM) range, has enabled large scale systems-on-chip (SoC). In
future deep submicron (DSM) designs, the interconnect effect will definitely
dominate performance. Network-on-Chip (NoC) has become a promising solution to
bus-based communication infrastructure limitations. NoC designs usually targets
Application Specific Integrated Circuits (ASICs), however, the fabrication
process costs a lot. Implementing a NoC on an FPGA does not only reduce the
cost but also decreases programming and verification cycles. In this paper, an
Asynchronous NoC has been implemented on a SPARTAN-3E\textregistered device.
The NoC supports basic transactions of both widely used on-chip interconnection
standards, the Open Core Protocol (OCP) and the WISHBONE Protocol. Although,
FPGA devices are synchronous in nature, it has been shown that they can be used
to prototype a Global Asynchronous Local Synchronous (GALS) systems, comprising
an Asynchronous NoC connecting IP cores operating in different clock domains.
</summary>
    <author>
      <name>Ahmed H. M. Soliman</name>
    </author>
    <author>
      <name>E. M. Saad</name>
    </author>
    <author>
      <name>M. El-Bably</name>
    </author>
    <author>
      <name>Hesham M. A. M. Keshk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures; ISSN (Online): 1694-0814</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 4, No 2, July 2011, 262-268</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.4150v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.4150v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2772v1</id>
    <updated>2012-04-12T17:07:58Z</updated>
    <published>2012-04-12T17:07:58Z</published>
    <title>Effect of Thread Level Parallelism on the Performance of Optimum
  Architecture for Embedded Applications</title>
    <summary>  According to the increasing complexity of network application and internet
traffic, network processor as a subset of embedded processors have to process
more computation intensive tasks. By scaling down the feature size and emersion
of chip multiprocessors (CMP) that are usually multi-thread processors, the
performance requirements are somehow guaranteed. As multithread processors are
the heir of uni-thread processors and there isn't any general design flow to
design a multithread embedded processor, in this paper we perform a
comprehensive design space exploration for an optimum uni-thread embedded
processor based on the limited area and power budgets. Finally we run multiple
threads on this architecture to find out the maximum thread level parallelism
(TLP) based on performance per power and area optimum uni-thread architecture.
</summary>
    <author>
      <name>Mehdi Alipour</name>
    </author>
    <author>
      <name>Hojjat Taghdisi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Embedded Systems and Applications (IJESA),
  http://airccse.org/journal/ijesa/current2012.html</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.2772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2809v1</id>
    <updated>2012-04-12T18:40:55Z</updated>
    <published>2012-04-12T18:40:55Z</published>
    <title>Performance-Optimum Superscalar Architecture for Embedded Applications</title>
    <summary>  Embedded applications are widely used in portable devices such as wireless
phones, personal digital assistants, laptops, etc. High throughput and real
time requirements are especially important in such data-intensive tasks.
Therefore, architectures that provide the required performance are the most
desirable. On the other hand, processor performance is severely related to the
average memory access delay, number of processor registers and also size of the
instruction window and superscalar parameters. Therefore, cache, register file
and superscalar parameters are the major architectural concerns in designing a
superscalar architecture for embedded processors. Although increasing cache and
register file size leads to performance improvements in high performance
embedded processors, the increased area, power consumption and memory delay are
the overheads of these techniques. This paper explores the effect of cache,
register file and superscalar parameters on the processor performance to
specify the optimum size of these parameters for embedded applications.
Experimental results show that although having bigger size of these parameters
is one of the performance improvement approaches in embedded processors,
however, by increasing the size of some parameters over a threshold value,
performance improvement is saturated and especially in cache size, increments
over this threshold value decrease the performance.
</summary>
    <author>
      <name>Mehdi Alipour</name>
    </author>
    <author>
      <name>Mostafa E. Salehi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">http://indianjournals.com/ijor.aspx 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.2809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.5407v1</id>
    <updated>2012-04-24T15:28:25Z</updated>
    <published>2012-04-24T15:28:25Z</published>
    <title>Reversible Programmable Logic Array (RPLA) using Feynman &amp; MUX Gates for
  Low Power Industrial Applications</title>
    <summary>  This paper present the research work directed towards the design of
reversible programmable logic array using very high speed integrated circuit
hardware description language (VHDL). Reversible logic circuits have
significant importance in bioinformatics, optical information processing, CMOS
design etc. In this paper the authors propose the design of new RPLA using
Feynman &amp; MUX gate.VHDL based codes of reversible gates with simulating results
are shown .This proposed RPLA may be further used to design any reversible
logic function or Boolean function (Adder, subtractor etc.) which dissipate
very low or ideally no heat.
</summary>
    <author>
      <name>Pradeep Singla</name>
    </author>
    <author>
      <name>Naveen Kr. Malik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages, 9 Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pradeep Singla and Naveen Kr. Malik. Article: Reversible
  Programmable Logic Array (RPLA) using Feynman &amp; MUX Gates for Low Power
  Industrial Applications. Proceedinggs of ICIAICT-2012,pp 411-419, March 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.5407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.5407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1860v1</id>
    <updated>2012-05-09T03:18:40Z</updated>
    <published>2012-05-09T03:18:40Z</published>
    <title>Wishbone bus Architecture - A Survey and Comparison</title>
    <summary>  The performance of an on-chip interconnection architecture used for
communication between IP cores depends on the efficiency of its bus
architecture. Any bus architecture having advantages of faster bus clock speed,
extra data transfer cycle, improved bus width and throughput is highly
desirable for a low cost, reduced time-to-market and efficient System-on-Chip
(SoC). This paper presents a survey of WISHBONE bus architecture and its
comparison with three other on-chip bus architectures viz. Advanced Micro
controller Bus Architecture (AMBA) by ARM, CoreConnect by IBM and Avalon by
Altera. The WISHBONE Bus Architecture by Silicore Corporation appears to be
gaining an upper edge over the other three bus architecture types because of
its special performance parameters like the use of flexible arbitration scheme
and additional data transfer cycle (Read-Modify-Write cycle). Moreover, its IP
Cores are available free for use requiring neither any registration nor any
agreement or license.
</summary>
    <author>
      <name>Mohandeep Sharma</name>
    </author>
    <author>
      <name>Dilip Kumar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/vlsic.2012.3210</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/vlsic.2012.3210" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International journal of VLSI Design &amp; Communication Systems
  (VLSICS) Vol.3, No.2 April 2012, 107-124</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1205.1860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1866v1</id>
    <updated>2012-05-09T04:15:39Z</updated>
    <published>2012-05-09T04:15:39Z</published>
    <title>Microcontroller Based Testing of Digital IP-Core</title>
    <summary>  Testing core based System on Chip is a challenge for the test engineers. To
test the complete SOC at one time with maximum fault coverage, test engineers
prefer to test each IP-core separately. At speed testing using external testers
is more expensive because of gigahertz processor. The purpose of this paper is
to develop cost efficient and flexible test methodology for testing digital
IP-cores . The prominent feature of the approach is to use microcontroller to
test IP-core. The novel feature is that there is no need of test pattern
generator and output response analyzer as microcontroller performs the function
of both. This approach has various advantages such as at speed testing, low
cost, less area overhead and greater flexibility since most of the testing
process is based on software.
</summary>
    <author>
      <name>Amandeep Singh</name>
    </author>
    <author>
      <name>Balwinder Singh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/vlsic.2012.3205</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/vlsic.2012.3205" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Microcontroller, FPGA, Testing, TMR, SOC</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.1866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1871v1</id>
    <updated>2012-05-09T05:23:13Z</updated>
    <published>2012-05-09T05:23:13Z</published>
    <title>Design Space Exploration to Find the Optimum Cache and Register File
  Size for Embedded Applications</title>
    <summary>  In the future, embedded processors must process more computation-intensive
network applications and internet traffic and packet-processing tasks become
heavier and sophisticated. Since the processor performance is severely related
to the average memory access delay and also the number of processor registers
affects the performance, cache and register file are two major parts in
designing embedded processor architecture. Although increasing cache and
register file size leads to performance improvement in embedded applications
and packet-processing tasks in high traffic networks with too much packets, the
increased area, power consumption and memory hierarchy delay are the overheads
of these techniques. Therefore, implementing these components in the optimum
size is of significant interest in the design of embedded processors. This
paper explores the effect of cache and register file size on the processor
performance to calculate the optimum size of these components for embedded
applications. Experimental results show that although having bigger cache and
register file is one of the performance improvement approaches in embedded
processors, however, by increasing the size of these parameters over a
threshold level, performance improvement is saturated and then, decreased.
</summary>
    <author>
      <name>Mehdi Alipour</name>
    </author>
    <author>
      <name>Mostafa E. Salehi</name>
    </author>
    <author>
      <name>Hesamodin shojaei baghini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 2011 International Conference on Embedded Systems and
  Applications, las vegas, nevada, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.1871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.2153v1</id>
    <updated>2012-05-10T04:04:45Z</updated>
    <published>2012-05-10T04:04:45Z</published>
    <title>Design and implementation of real time AES-128 on real time operating
  system for multiple FPGA communication</title>
    <summary>  Security is the most important part in data communication system, where more
randomization in secret keys increases the security as well as complexity of
the cryptography algorithms. As a result in recent dates these algorithms are
compensating with enormous memory spaces and large execution time on hardware
platform. Field programmable gate arrays (FPGAs), provide one of the major
alternative in hardware platform scenario due to its reconfiguration nature,
low price and marketing speed. In FPGA based embedded system we can use
embedded processor to execute particular algorithm with the inclusion of a real
time operating System (RTOS), where threads may reduce resource utilization and
time consumption. A process in the runtime is separated in different smaller
tasks which are executed by the scheduler to meet the real time dead line using
RTOS. In this paper we demonstrate the design and implementation of a 128-bit
Advanced Encryption Standard (AES) both symmetric key encryption and decryption
algorithm by developing suitable hardware and software design on Xilinx
Spartan- 3E (XC3S500E-FG320) device using an Xilkernel RTOS, the implementation
has been tested successfully The system is optimized in terms of execution
speed and hardware utilization.
</summary>
    <author>
      <name>Rourab Paul</name>
    </author>
    <author>
      <name>Sangeet Saha</name>
    </author>
    <author>
      <name>Suman Sau</name>
    </author>
    <author>
      <name>Amlan Chakrabarti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, IEMCON 12, Kolkata</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.2153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.2153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.2428v1</id>
    <updated>2012-05-11T05:06:47Z</updated>
    <published>2012-05-11T05:06:47Z</published>
    <title>Relaxed Half-Stochastic Belief Propagation</title>
    <summary>  Low-density parity-check codes are attractive for high throughput
applications because of their low decoding complexity per bit, but also because
all the codeword bits can be decoded in parallel. However, achieving this in a
circuit implementation is complicated by the number of wires required to
exchange messages between processing nodes. Decoding algorithms that exchange
binary messages are interesting for fully-parallel implementations because they
can reduce the number and the length of the wires, and increase logic density.
This paper introduces the Relaxed Half-Stochastic (RHS) decoding algorithm, a
binary message belief propagation (BP) algorithm that achieves a coding gain
comparable to the best known BP algorithms that use real-valued messages. We
derive the RHS algorithm by starting from the well-known Sum-Product algorithm,
and then derive a low-complexity version suitable for circuit implementation.
We present extensive simulation results on two standardized codes having
different rates and constructions, including low bit error rate results. These
simulations show that RHS can be an advantageous replacement for the existing
state-of-the-art decoding algorithms when targeting fully-parallel
implementations.
</summary>
    <author>
      <name>François Leduc-Primeau</name>
    </author>
    <author>
      <name>Saied Hemati</name>
    </author>
    <author>
      <name>Shie Mannor</name>
    </author>
    <author>
      <name>Warren J. Gross</name>
    </author>
    <link href="http://arxiv.org/abs/1205.2428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.2428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.4967v1</id>
    <updated>2012-05-22T16:26:23Z</updated>
    <published>2012-05-22T16:26:23Z</published>
    <title>Investigating Warp Size Impact in GPUs</title>
    <summary>  There are a number of design decisions that impact a GPU's performance. Among
such decisions deciding the right warp size can deeply influence the rest of
the design. Small warps reduce the performance penalty associated with branch
divergence at the expense of a reduction in memory coalescing. Large warps
enhance memory coalescing significantly but also increase branch divergence.
This leaves designers with two choices: use a small warps and invest in finding
new solutions to enhance coalescing or use large warps and address branch
divergence employing effective control-flow solutions. In this work our goal is
to investigate the answer to this question. We analyze warp size impact on
memory coalescing and branch divergence. We use our findings to study two
machines: a GPU using small warps but equipped with excellent memory coalescing
(SW+) and a GPU using large warps but employing an MIMD engine immune from
control-flow costs (LW+). Our evaluations show that building
coalescing-enhanced small warp GPUs is a better approach compared to pursuing a
control-flow enhanced large warp GPU.
</summary>
    <author>
      <name>Ahmad Lashgar</name>
    </author>
    <author>
      <name>Amirali Baniasadi</name>
    </author>
    <author>
      <name>Ahmad Khonsari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures, 2 tables, Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.4967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.4967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.2132v1</id>
    <updated>2012-06-11T08:50:37Z</updated>
    <published>2012-06-11T08:50:37Z</published>
    <title>RepTFD: Replay Based Transient Fault Detection</title>
    <summary>  The advances in IC process make future chip multiprocessors (CMPs) more and
more vulnerable to transient faults. To detect transient faults, previous
core-level schemes provide redundancy for each core separately. As a result,
they may leave transient faults in the uncore parts, which consume over 50%
area of a modern CMP, escaped from detection. This paper proposes RepTFD, the
first core-level transient fault detection scheme with 100% coverage. Instead
of providing redundancy for each core separately, RepTFD provides redundancy
for a group of cores as a whole. To be specific, it replays the execution of
the checked group of cores on a redundant group of cores. Through comparing the
execution results between the two groups of cores, all malignant transient
faults can be caught. Moreover, RepTFD adopts a novel pending period based
record-replay approach, which can greatly reduce the number of execution orders
that need to be enforced in the replay-run. Hence, RepTFD brings only 4.76%
performance overhead in comparison to the normal execution without
fault-tolerance according to our experiments on the RTL design of an industrial
CMP named Godson-3. In addition, RepTFD only consumes about 0.83% area of
Godson-3, while needing only trivial modifications to existing components of
Godson-3.
</summary>
    <author>
      <name>Lei Li</name>
    </author>
    <author>
      <name>Tianshi Chen</name>
    </author>
    <author>
      <name>Yunji Chen</name>
    </author>
    <author>
      <name>Ling Li</name>
    </author>
    <author>
      <name>Ruiyang Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.2132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.2132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.1187v1</id>
    <updated>2012-07-05T08:20:02Z</updated>
    <published>2012-07-05T08:20:02Z</published>
    <title>Dynamic Priority Queue: An SDRAM Arbiter With Bounded Access Latencies
  for Tight WCET Calculation</title>
    <summary>  This report introduces a shared resource arbitration scheme "DPQ - Dynamic
Priority Queue" which provides bandwidth guarantees and low worst case latency
to each master in an MPSoC. Being a non-trivial candidate for timing analysis,
SDRAM has been chosen as a showcase, but the approach is valid for any shared
resource arbitration.
  Due to its significant cost, data rate and physical size advantages, SDRAM is
a potential candidate for cost sensitive, safety critical and space conserving
systems. The variable access latency is a major drawback of SDRAM that induces
largely over estimated Worst Case Execution Time (WCET) bounds of applications.
In this report we present the DPQ together with an algorithm to predict the
shared SDRAM's worst case latencies. We use the approach to calculate WCET
bounds of six hardware tasks executing on an Altera Cyclone III FPGA with
shared DDR2 memory. The results show that the DPQ is a fair arbitration scheme
and produces low WCET bounds.
</summary>
    <author>
      <name>Hardik Shah</name>
    </author>
    <author>
      <name>Andreas Raabe</name>
    </author>
    <author>
      <name>Alois Knoll</name>
    </author>
    <link href="http://arxiv.org/abs/1207.1187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.1187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.1683v1</id>
    <updated>2012-07-06T16:52:24Z</updated>
    <published>2012-07-06T16:52:24Z</published>
    <title>Design and Development of Low Cost Multi-Channel USB Data</title>
    <summary>  This paper describes the design and development of low cost USB Data
Acquisition System (DAS) for the measurement of physical parameters. Physical
parameters such as temperature, humidity, light intensity etc., which are
generally slowly varying signals are sensed by respective sensors or integrated
sensors and converted into voltages. The DAS is designed using PIC18F4550
microcontroller, communicating with Personal Computer (PC) through USB
(Universal Serial Bus). The designed DAS has been tested with the application
program developed in Visual Basic, which allows online monitoring in graphical
as well as numerical display.
</summary>
    <author>
      <name>Nungleppam Monoranjan Singh</name>
    </author>
    <author>
      <name>Kanak Chandra Sarma</name>
    </author>
    <author>
      <name>Nungleppam Gopil Singh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/7452-0633</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/7452-0633" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,8 figures, published in International Journal of Computer
  Applications (IJCA)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications 48(18):47-51, June
  2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1207.1683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.1683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.2060v1</id>
    <updated>2012-07-09T14:25:51Z</updated>
    <published>2012-07-09T14:25:51Z</published>
    <title>Design of PIC12F675 Microcontroller Based Data Acquisition System for
  Slowly Varying Signals</title>
    <summary>  The present paper describes the design of a cost effective, better resolution
data acquisition system (DAS) which is compatible to most of the PC and
laptops. A low cost DAS has been designed using PIC12F675 having 4-channel
analog input with 10-bit resolution for the monitoring of slowly varying
signals. The DAS so designed is interfaced to the serial port of the PC.
Firmware is written in Basic using Oshonsoft PIC IDE and burn to the
microcontroller by using PICkit2 programmer. An application program is also
developed using Visual Basic 6 which allows to display the waveform of the
signal(s) and simultaneously the data also can be saved into the hard disk of
the computer for future use and analysis.
</summary>
    <author>
      <name>N. Monoranjan Singh</name>
    </author>
    <author>
      <name>K. C. Sarma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in the Journal of Instrument Society of India (ISOI)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Instrum. Soc. of India, Vol. 40 No. 1 March 2010, pp
  15-17</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1207.2060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.2060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.2739v1</id>
    <updated>2012-07-11T18:29:21Z</updated>
    <published>2012-07-11T18:29:21Z</published>
    <title>Low Cost PC Based Real Time Data Logging System Using PCs Parallel Port
  For Slowly Varying Signals</title>
    <summary>  A low cost PC based real time data logging system can be used in the
laboratories for the measurement, monitoring and storage of the data for slowly
varying signals in science and engineering stream. This can be designed and
interfaced to the PCs Parallel Port, which is common to all desktop computers
or Personal Computers (PCs). By the use of this data logging system one can
monitor, measure and store data for slowly varying signals, which is hard to
visualise the signal waveforms by ordinary CRO (Cathode Ray Oscilloscope) and
DSO (Digital Storage Oscilloscope). The data so stored can be used for further
study and analysis. It can be used for a wide range of applications to monitor
and store data of temperature, humidity, light intensity, ECG signals etc. with
proper signal conditioning circuitry.
</summary>
    <author>
      <name>N. Monoranjan Singh</name>
    </author>
    <author>
      <name>K. C. Sarma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in the Journal of Assam Science Society, December 2009</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Assam Sc. Soc. Vol. 50; No. 1,2; 36-41; December 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1207.2739v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.2739v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.2840v1</id>
    <updated>2012-07-12T03:57:37Z</updated>
    <published>2012-07-12T03:57:37Z</published>
    <title>Design and Performance Analysis of hybrid adders for high speed
  arithmetic circuit</title>
    <summary>  Adder cells using Gate Diffusion Technique (GDI) &amp; PTL-GDI technique are
described in this paper. GDI technique allows reducing power consumption,
propagation delay and low PDP (power delay product) whereas Pass Transistor
Logic (PTL) reduces the count of transistors used to make different logic
gates, by eliminating redundant transistors. Performance comparison with
various Hybrid Adder is been presented. In this paper, we propose two new
designs based on GDI &amp; PTL techniques, which is found to be much more power
efficient in comparison with existing design technique. Only 10 transistors are
used to implement the SUM &amp; CARRY function for both the designs. The SUM and
CARRY cell are implemented in a cascaded way i.e. firstly the XOR cell is
implemented and then using XOR as input SUM as well as CARRY cell is
implemented. For Proposed GDI adder the SUM as well as CARRY cell is designed
using GDI technique. On the other hand in Proposed PTL-GDI adder the SUM cell
is constructed using PTL technique and the CARRY cell is designed using GDI
technique. The advantages of both the designs are discussed. The significance
of these designs is substantiated by the simulation results obtained from
Cadence Virtuoso 180nm environment.
</summary>
    <author>
      <name>Rajkumar Sarma</name>
    </author>
    <author>
      <name>Veerati Raju</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 PAGES, 10 FIGURES</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.2840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.2840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.5138v1</id>
    <updated>2012-07-21T13:47:55Z</updated>
    <published>2012-07-21T13:47:55Z</published>
    <title>Ethernet Packet Processor for SoC Application</title>
    <summary>  As the demand for Internet expands significantly in numbers of users,
servers, IP addresses, switches and routers, the IP based network architecture
must evolve and change. The design of domain specific processors that require
high performance, low power and high degree of programmability is the
bottleneck in many processor based applications. This paper describes the
design of ethernet packet processor for system-on-chip (SoC) which performs all
core packet processing functions, including segmentation and reassembly,
packetization classification, route and queue management which will speedup
switching/routing performance. Our design has been configured for use with
multiple projects ttargeted to a commercial configurable logic device the
system is designed to support 10/100/1000 links with a speed advantage. VHDL
has been used to implement and simulated the required functions in FPGA.
</summary>
    <author>
      <name>Raja Jitendra Nayaka</name>
    </author>
    <author>
      <name>R. C. Biradar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The International Workshop Of Information Technology, Control And
  Automation (Itca-2012), Sl.No.27. Proceedings In Computer Science &amp;
  Information Technology (Cs &amp; It) Series</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.5138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.5138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Airccse Conferences" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.2374v2</id>
    <updated>2012-11-03T19:05:45Z</updated>
    <published>2012-08-11T18:05:59Z</published>
    <title>Dynamic Warp Resizing in High-Performance SIMT</title>
    <summary>  Modern GPUs synchronize threads grouped in a warp at every instruction. These
results in improving SIMD efficiency and makes sharing fetch and decode
resources possible. The number of threads included in each warp (or warp size)
affects divergence, synchronization overhead and the efficiency of memory
access coalescing. Small warps reduce the performance penalty associated with
branch and memory divergence at the expense of a reduction in memory
coalescing. Large warps enhance memory coalescing significantly but also
increase branch and memory divergence. Dynamic workload behavior, including
branch/memory divergence and coalescing, is an important factor in determining
the warp size returning best performance. Optimal warp size can vary from one
workload to another or from one program phase to the next. Based on this
observation, we propose Dynamic Warp Resizing (DWR). DWR takes innovative
microarchitectural steps to adjust warp size during runtime and according to
program characteristics. DWR outperforms static warp size decisions, up to 1.7X
to 2.28X, while imposing less than 1% area overhead. We investigate various
alternative configurations and show that DWR performs better for narrower SIMD
and larger caches.
</summary>
    <author>
      <name>Ahmad Lashgar</name>
    </author>
    <author>
      <name>Amirali Baniasadi</name>
    </author>
    <author>
      <name>Ahmad Khonsari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 Figures, 3 Lists, 1 Table, The extended version of ICCD
  2012 poster paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.2374v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.2374v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.3564v1</id>
    <updated>2012-09-17T07:14:15Z</updated>
    <published>2012-09-17T07:14:15Z</published>
    <title>Deadlock Recovery Technique in Bus Enhanced NoC Architecture</title>
    <summary>  Increase in the speed of processors has led to crucial role of communication
in the performance of systems. As a result, routing is taken into consideration
as one of the most important subjects of the Network on Chip architecture.
Routing algorithms to deadlock avoidance prevent packets route completely based
on network traffic condition by means of restricting the route of packets. This
action leads to less performance especially in non-uniform traffic patterns. On
the other hand True Fully Adoptive Routing algorithm provides routing of
packets completely based on traffic condition. However, deadlock detection and
recovery mechanisms are needed to handle deadlocks. Use of global bus beside
NoC as a parallel supportive environment, provide platform to offer advantages
of both features of bus and NoC. This bus is useful for broadcast and multicast
operations, sending delay sensitive signals, system management and other
services. In this research, we use this bus as an escaping path for deadlock
recovery technique. According to simulation results, this bus is suitable
platform for deadlock recovery technique.
</summary>
    <author>
      <name>Saeid Sharifian Nia</name>
    </author>
    <author>
      <name>Abbas Vafaei</name>
    </author>
    <author>
      <name>Hamid Shahimohamadi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.3564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.3564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0310v1</id>
    <updated>2012-12-03T08:28:13Z</updated>
    <published>2012-12-03T08:28:13Z</published>
    <title>Design and Implementation of Multistage Interconnection Networks for SoC
  Networks</title>
    <summary>  In this paper the focus is on a family of Interconnection Networks (INs)
known as Multistage Interconnection Networks (MINs). When it is exploited in
Network-on-Chip (NoC) architecture designs, smaller circuit area, lower power
consumption, less junctions and broader bandwidth can be achieved. Each MIN can
be considered as an alternative for an NoC architecture design for its simple
topology and easy scalability with low degree. This paper includes two major
contributions. First, it compares the performance of seven prominent MINs (i.e.
Omega, Butterfly, Flattened Butterfly, Flattened Baseline, Generalized Cube,
Bene\v{s} and Clos networks) based on 45nm-CMOS technology and under different
types of Synthetic and Trace-driven workloads. Second, a network called
Meta-Flattened Network (MFN), was introduced that can decrease the blocking
probability by means of reduction the number of hops and increase the
intermediate paths between stages. This is also led into significant decrease
in power consumption.
</summary>
    <author>
      <name>Mahsa Moazez</name>
    </author>
    <author>
      <name>Farshad Safaei</name>
    </author>
    <author>
      <name>Majid Rezazadeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 Pages, 14 Figures; International Journal of Computer Science,
  Engineering and Information Technology (IJCSEIT), Vol.2, No.5, October 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.0310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.1465v2</id>
    <updated>2013-05-31T11:09:16Z</updated>
    <published>2013-01-08T09:59:28Z</published>
    <title>A joint communication and application simulator for NoC-based SoCs</title>
    <summary>  NoCs have become a widespread paradigm in the system-on-chip design world,
not only for multi-purpose SoCs, but also for application-specific ICs. The
common approach in the NoC design world is to separate the design of the
interconnection from the design of the processing elements: this is well suited
for a large number of developments, but the need for joint application and NoC
design is not uncommon, especially in the application specific case. The
correlation between processing and communication tasks can be strong, and
separate or trace-based simulations fall often short of the desired precision.
In this work, the OMNET++ based JANoCS simulator is presented: concurrent
simulation of processing and communication allow cycle-accurate evaluation of
the system. Two cases of study are presented, showing both the need for joint
simulations and the effectiveness of JANoCS.
</summary>
    <author>
      <name>Carlo Condo</name>
    </author>
    <author>
      <name>Amer Baghdadi</name>
    </author>
    <author>
      <name>Guido Masera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Withdrawn, due to extended and revised version being published</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.1465v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.1465v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3281v1</id>
    <updated>2013-01-15T09:51:33Z</updated>
    <published>2013-01-15T09:51:33Z</published>
    <title>Reconfiguration Strategies for Online Hardware Multitasking in Embedded
  Systems</title>
    <summary>  An intensive use of reconfigurable hardware is expected in future embedded
systems. This means that the system has to decide which tasks are more suitable
for hardware execution. In order to make an efficient use of the FPGA it is
convenient to choose one that allows hardware multitasking, which is
implemented by using partial dynamic reconfiguration. One of the challenges for
hardware multitasking in embedded systems is the online management of the only
reconfiguration port of present FPGA devices. This paper presents different
online reconfiguration scheduling strategies which assign the reconfiguration
interface resource using different criteria: workload distribution or task
deadline. The online scheduling strategies presented take efficient and fast
decisions based on the information available at each moment. Experiments have
been made in order to analyze the performance and convenience of these
reconfiguration strategies.
</summary>
    <author>
      <name>Marcos Sanchez-Elez</name>
    </author>
    <author>
      <name>Sara Roman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/cseij.2012.2601</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/cseij.2012.2601" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Science &amp; Engineering: An International Journal (CSEIJ),
  Vol.2, No.6, December 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.1390v1</id>
    <updated>2013-02-06T15:00:35Z</updated>
    <published>2013-02-06T15:00:35Z</published>
    <title>MGSim - Simulation tools for multi-core processor architectures</title>
    <summary>  MGSim is an open source discrete event simulator for on-chip hardware
components, developed at the University of Amsterdam. It is intended to be a
research and teaching vehicle to study the fine-grained hardware/software
interactions on many-core and hardware multithreaded processors. It includes
support for core models with different instruction sets, a configurable
multi-core interconnect, multiple configurable cache and memory models, a
dedicated I/O subsystem, and comprehensive monitoring and interaction
facilities. The default model configuration shipped with MGSim implements
Microgrids, a many-core architecture with hardware concurrency management.
MGSim is furthermore written mostly in C++ and uses object classes to represent
chip components. It is optimized for architecture models that can be described
as process networks.
</summary>
    <author>
      <name>Mike Lankamp</name>
    </author>
    <author>
      <name>Raphael Poss</name>
    </author>
    <author>
      <name>Qiang Yang</name>
    </author>
    <author>
      <name>Jian Fu</name>
    </author>
    <author>
      <name>Irfan Uddin</name>
    </author>
    <author>
      <name>Chris R. Jesshope</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 22 figures, 4 listings, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.1390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.1390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.4463v1</id>
    <updated>2013-02-18T21:23:54Z</updated>
    <published>2013-02-18T21:23:54Z</published>
    <title>A Low-Power Content-Addressable-Memory Based on
  Clustered-Sparse-Networks</title>
    <summary>  A low-power Content-Addressable-Memory (CAM) is introduced employing a new
mechanism for associativity between the input tags and the corresponding
address of the output data. The proposed architecture is based on a recently
developed clustered-sparse-network using binary-weighted connections that
on-average will eliminate most of the parallel comparisons performed during a
search. Therefore, the dynamic energy consumption of the proposed design is
significantly lower compared to that of a conventional low-power CAM design.
Given an input tag, the proposed architecture computes a few possibilities for
the location of the matched tag and performs the comparisons on them to locate
a single valid match. A 0.13 um CMOS technology was used for simulation
purposes. The energy consumption and the search delay of the proposed design
are 9.5%, and 30.4% of that of the conventional NAND architecture respectively
with a 3.4% higher number of transistors.
</summary>
    <author>
      <name>Hooman Jarollahi</name>
    </author>
    <author>
      <name>Vincent Gripon</name>
    </author>
    <author>
      <name>Naoya Onizawa</name>
    </author>
    <author>
      <name>Warren J. Gross</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ASAP.2013.6567594</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ASAP.2013.6567594" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE ASAP 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.4463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.4463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.2175v1</id>
    <updated>2013-03-09T06:57:21Z</updated>
    <published>2013-03-09T06:57:21Z</published>
    <title>An efficient cntfet-based 7-input minority gate</title>
    <summary>  Complementary metal oxide semiconductor technology (CMOS) has been faced
critical challenges in nano-scale regime. CNTFET (Carbon Nanotube Field effect
transistor) technology is a promising alternative for CMOS technology. In this
paper, we proposed a novel 7-input minority gate in CNTFET technology that has
only 9 CNTFETs. Minority function is utilized in the voting systems for
decision making and also it is used in data mining. This proposed 7-input
minority gate is utilized less fewer transistors than the conventional CMOS
method which utilizes many transistors for implementing sum of products. By
means of this proposed 7-input minority gate, a 4-input NAND gate can be
implemented, which gets better the conventional design in terms of delay and
energy efficiency and has much more deriving power at its output.
</summary>
    <author>
      <name>Samira Shirinabadi Farahani</name>
    </author>
    <author>
      <name>Ronak Zarhoun</name>
    </author>
    <author>
      <name>Mohammad Hossein Moaiyeri</name>
    </author>
    <author>
      <name>Keivan Navi</name>
    </author>
    <link href="http://arxiv.org/abs/1303.2175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.2175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.4892v1</id>
    <updated>2013-03-20T10:17:54Z</updated>
    <published>2013-03-20T10:17:54Z</published>
    <title>On whether and how D-RISC and Microgrids can be kept relevant
  (self-assessment report)</title>
    <summary>  This report lays flat my personal views on D-RISC and Microgrids as of March
2013. It reflects the opinions and insights that I have gained from working on
this project during the period 2008-2013. This report is structed in two parts:
deconstruction and reconstruction. In the deconstruction phase, I review what I
believe are the fundamental motivation and goals of the D-RISC/Microgrids
enterprise, and identify what I judge are shortcomings: that the project did
not deliver on its expectations, that fundamental questions are left
unanswered, and that its original motivation may not even be relevant in
scientific research any more in this day and age. In the reconstruction phase,
I start by identifying the merits of the current D-RISC/Microgrids technology
and know-how taken at face value, re-motivate its existence from a different
angle, and suggest new, relevant research questions that could justify
continued scientific investment.
</summary>
    <author>
      <name>Raphael Poss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">45 pages, 5 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.4892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.4892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.5762v1</id>
    <updated>2013-03-21T22:53:24Z</updated>
    <published>2013-03-21T22:53:24Z</published>
    <title>Object-oriented approach to Rapid Custom Instruction design</title>
    <summary>  Due to continuous evolution of Systems-on-Chip (SoC), the complexity of their
design and development has augmented exponentially. To deal with the
ever-growing complexity of such embedded systems, we introduce, in this paper,
an object-oriented approach to rapid SoC design using auto-generation of
hardware custom instructions to simplify and accelerate the SoC design process.
In our approach, a Data Flow Graph (DFG) is adopted as a representation of the
arithmetic operation to convert it to a custom instruction. Then VHDL code will
be automatically generated. The input C code is automatically updated for
calling the new hardware components. To prove the effectiveness of the proposed
approach, a Java source code framework named Automatic Custom Architecture
generator (ACAgen) is developed. Experimental results on 3D sample application
validate our approach and demonstrate how the proposed framework facilitates
and accelerates the SoC design process at low costs.
</summary>
    <author>
      <name>Emna Kallel</name>
    </author>
    <author>
      <name>Yassine Aoudni</name>
    </author>
    <author>
      <name>Mohamed Abid</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/FTFC.2012.6231733</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/FTFC.2012.6231733" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.5762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.5762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.0835v1</id>
    <updated>2013-04-03T03:52:52Z</updated>
    <published>2013-04-03T03:52:52Z</published>
    <title>Improved Analytical Delay Models for RC-Coupled Interconnects</title>
    <summary>  As the process technologies scale into deep submicron region, crosstalk delay
is becoming increasingly severe, especially for global on-chip buses. To cope
with this problem, accurate delay models of coupled interconnects are needed.
In particular, delay models based on analytical approaches are desirable,
because they not only are largely transparent to technology, but also
explicitly establish the connections between delays of coupled interconnects
and transition patterns, thereby enabling crosstalk alleviating techniques such
as crosstalk avoidance codes (CACs). Unfortunately, existing analytical delay
models, such as the widely cited model in [1], have limited accuracy and do not
account for loading capacitance. In this paper, we propose analytical delay
models for coupled interconnects that address these disadvantages. By
accounting for more wires and eschewing the Elmore delay, our delay models
achieve better accuracy than the model in [1].
</summary>
    <author>
      <name>Feng Shi</name>
    </author>
    <author>
      <name>Xuebin Wu</name>
    </author>
    <author>
      <name>Zhiyuan Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.0835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.0835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7862v1</id>
    <updated>2013-04-30T04:14:53Z</updated>
    <published>2013-04-30T04:14:53Z</published>
    <title>A formalisation of XMAS</title>
    <summary>  Communication fabrics play a key role in the correctness and performance of
modern multi-core processors and systems-on-chip. To enable formal
verification, a recent trend is to use high-level micro-architectural models to
capture designers' intent about the communication and processing of messages.
Intel proposed the xMAS language to support the formal definition of executable
specifications of micro-architectures. We formalise the semantics of xMAS in
ACL2. Our formalisation represents the computation of the values of all wires
of a design. Our main function computes a set of possible routing targets for
each message and whether a message can make progress according to the current
network state. We prove several properties on the semantics, including
termination, non-emptiness of routing, and correctness of progress conditions.
Our current effort focuses on a basic subset of the entire xMAS language, which
includes queues, functions, and switches.
</summary>
    <author>
      <name>Bernard van Gastel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Open University of the Netherlands</arxiv:affiliation>
    </author>
    <author>
      <name>Julien Schmaltz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Open University of the Netherlands</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.114.9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.114.9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings ACL2 2013, arXiv:1304.7123</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 114, 2013, pp. 111-126</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.7862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3324v1</id>
    <updated>2013-07-12T04:57:14Z</updated>
    <published>2013-07-12T04:57:14Z</published>
    <title>Power efficient carry propagate adder</title>
    <summary>  Here we describe the design details and performance of proposed Carry
Propagate Adder based on GDI technique. GDI technique is power efficient
technique for designing digital circuit that consumes less power as compare to
most commonly used CMOS technique. GDI also has an advantage of minimum
propagation delay, minimum area required and less complexity for designing any
digital circuit. We designed Carry Propagate Adder using GDI technique and
compared its performance with CMOS technique in terms of area, delay and power
dissipation. Circuit designed using CADENCE EDA tool and simulated using
SPECTRE VIRTUOSO tool at 0.18m technology. Comparative performance result shows
that Carry Propagate Adder using GDI technique dissipated 55.6% less power as
compare to Carry Propagate Adder using CMOS technique.
</summary>
    <author>
      <name>Laxmi Kumre</name>
    </author>
    <author>
      <name>Ajay Somkuwar</name>
    </author>
    <author>
      <name>Ganga Agnihotri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 Pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of VLSI design &amp; Communication Systems
  (VLSICS) Vol.4, No.3, June 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.3324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3690v1</id>
    <updated>2013-07-14T03:31:10Z</updated>
    <published>2013-07-14T03:31:10Z</published>
    <title>Design of Parity Preserving Logic Based Fault Tolerant Reversible
  Arithmetic Logic Unit</title>
    <summary>  Reversible Logic is gaining significant consideration as the potential logic
design style for implementation in modern nanotechnology and quantum computing
with minimal impact on physical entropy .Fault Tolerant reversible logic is one
class of reversible logic that maintain the parity of the input and the
outputs. Significant contributions have been made in the literature towards the
design of fault tolerant reversible logic gate structures and arithmetic units,
however, there are not many efforts directed towards the design of fault
tolerant reversible ALUs. Arithmetic Logic Unit (ALU) is the prime performing
unit in any computing device and it has to be made fault tolerant. In this
paper we aim to design one such fault tolerant reversible ALU that is
constructed using parity preserving reversible logic gates. The designed ALU
can generate up to seven Arithmetic operations and four logical operations.
</summary>
    <author>
      <name>Rakshith Saligram</name>
    </author>
    <author>
      <name>Shrihari Shridhar Hegde</name>
    </author>
    <author>
      <name>Shashidhar A Kulkarni</name>
    </author>
    <author>
      <name>H. R. Bhagyalakshmi</name>
    </author>
    <author>
      <name>M. K. Venkatesha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 20 Figures, 8 Tables, Iinternational Journal of VLSI Design
  and Communication Systems, June 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.3690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.6406v1</id>
    <updated>2013-07-24T13:09:43Z</updated>
    <published>2013-07-24T13:09:43Z</published>
    <title>Relative Performance of a Multi-level Cache with Last-Level Cache
  Replacement: An Analytic Review</title>
    <summary>  Current day processors employ multi-level cache hierarchy with one or two
levels of private caches and a shared last-level cache (LLC). An efficient
cache replacement policy at LLC is essential for reducing the off-chip memory
transfer as well as conflict for memory bandwidth. Cache replacement techniques
for inclusive LLCs may not be efficient for multilevel cache as it can be
shared by enormous applications with varying access behavior, running
simultaneously. One application may dominate another by flooding of cache
requests and evicting the useful data of the other application. From the
performance point of view, an exclusive LLC make the replacement policies more
demanding, as compared to an inclusive LLC. This paper analyzes some of the
existing replacement techniques on the LLC with their performance assessment.
</summary>
    <author>
      <name>Bijay Paikaray</name>
    </author>
    <link href="http://arxiv.org/abs/1307.6406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.6021v1</id>
    <updated>2013-08-28T00:37:56Z</updated>
    <published>2013-08-28T00:37:56Z</published>
    <title>Selective Decoding in Associative Memories Based on Sparse-Clustered
  Networks</title>
    <summary>  Associative memories are structures that can retrieve previously stored
information given a partial input pattern instead of an explicit address as in
indexed memories. A few hardware approaches have recently been introduced for a
new family of associative memories based on Sparse-Clustered Networks (SCN)
that show attractive features. These architectures are suitable for
implementations with low retrieval latency, but are limited to small networks
that store a few hundred data entries. In this paper, a new hardware
architecture of SCNs is proposed that features a new data-storage technique as
well as a method we refer to as Selective Decoding (SD-SCN). The SD-SCN has
been implemented using a similar FPGA used in the previous efforts and achieves
two orders of magnitude higher capacity, with no error-performance penalty but
with the cost of few extra clock cycles per data access.
</summary>
    <author>
      <name>Hooman Jarollahi</name>
    </author>
    <author>
      <name>Naoya Onizawa</name>
    </author>
    <author>
      <name>Warren J. Gross</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/GlobalSIP.2013.6737140</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/GlobalSIP.2013.6737140" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Accepted in IEEE Global SIP 2013 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.6021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.6021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.2533v1</id>
    <updated>2013-09-10T14:50:29Z</updated>
    <published>2013-09-10T14:50:29Z</published>
    <title>Evaluation of the Performance/Energy Overhead in DSP Video Decoding and
  its Implications</title>
    <summary>  Video decoding is considered as one of the most compute and energy intensive
application in energy constrained mobile devices. Some specific processing
units, such as DSPs, are added to those devices in order to optimize the
performance and the energy consumption. However, in DSP video decoding, the
inter-processor communication overhead may have a considerable impact on the
performance and the energy consumption. In this paper, we propose to evaluate
this overhead and analyse its impact on the performance and the energy
consumption as compared to the GPP decoding. Our work revealed that the GPP can
be the best choice in many cases due to the a significant overhead in DSP
decoding which may represents 30% of the total decoding energy.
</summary>
    <author>
      <name>Yahia Benmoussa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Jalil Boukhobza</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Senn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Djamel Benazzouz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MSS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Annual Metting of the GDR SoC SiP, Lyon : France (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1309.2533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.2533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.3785v1</id>
    <updated>2013-09-15T18:32:21Z</updated>
    <published>2013-09-15T18:32:21Z</published>
    <title>Energy Saving Techniques for Phase Change Memory (PCM)</title>
    <summary>  In recent years, the energy consumption of computing systems has increased
and a large fraction of this energy is consumed in main memory. Towards this,
researchers have proposed use of non-volatile memory, such as phase change
memory (PCM), which has low read latency and power; and nearly zero leakage
power. However, the write latency and power of PCM are very high and this,
along with limited write endurance of PCM present significant challenges in
enabling wide-spread adoption of PCM. To address this, several
architecture-level techniques have been proposed. In this report, we review
several techniques to manage power consumption of PCM. We also classify these
techniques based on their characteristics to provide insights into them. The
aim of this work is encourage researchers to propose even better techniques for
improving energy efficiency of PCM based main memory.
</summary>
    <author>
      <name>Sparsh Mittal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Survey, phase change RAM (PCRAM)</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.3785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.3785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.5551v1</id>
    <updated>2013-09-22T02:01:28Z</updated>
    <published>2013-09-22T02:01:28Z</published>
    <title>Design space exploration in the microthreaded many-core architecture</title>
    <summary>  Design space exploration is commonly performed in embedded system, where the
architecture is a complicated piece of engineering. With the current trend of
many-core systems, design space exploration in general-purpose computers can no
longer be avoided. Microgrid is a complicated architecture, and therefor we
need to perform design space exploration. Generally, simulators are used for
the design space exploration of an architecture. Different simulators with
different levels of complexity, simulation time and accuracy are used.
Simulators with little complexity, low simulation time and reasonable accuracy
are desirable for the design space exploration of an architecture. These
simulators are referred as high-level simulators and are commonly used in the
design of embedded systems. However, the use of high-level simulation for
design space exploration in general-purpose computers is a relatively new area
of research.
</summary>
    <author>
      <name>Irfan Uddin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.5551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.5551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.5647v1</id>
    <updated>2013-09-22T20:44:49Z</updated>
    <published>2013-09-22T20:44:49Z</published>
    <title>A Cache-Coloring Based Technique for Saving Leakage Energy In
  Multitasking Systems</title>
    <summary>  There has been a significant increase in leakage energy dissipation of CMOS
circuits with each technology generation. Further, due to their large size,
last level caches (LLCs) spend a large fraction of their energy in the form of
leakage energy and hence, addressing this has become extremely important to
meet the challenges of chip power budget. For addressing this, several
techniques have been proposed. However, most of these techniques require
offline profiling and hence cannot be used for real-life systems which usually
run multitasking programs, with possible pre-emptions. In this paper, we
propose a dynamic profiling based technique for saving cache leakage energy in
multitasking systems. Our technique uses a small coloring-based profiling
cache, to estimate performance and energy consumption of multiple cache
configurations and then selects the best (least-energy) configuration among
them. Our technique uses non-intrusive profiling and saves energy despite
intra-task and inter-task variations; thus, it is suitable for multitasking
systems. Simulations performed using workloads from SPEC2006 suite show the
superiority of our technique over an existing cache energy saving technique.
With a 2MB baseline cache, the average saving in memory sub-system energy is
22.8%.
</summary>
    <author>
      <name>Sparsh Mittal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Cache leakage energy saving technique</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.5647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.5647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.7082v1</id>
    <updated>2013-09-26T22:18:43Z</updated>
    <published>2013-09-26T22:18:43Z</published>
    <title>A Cache Reconfiguration Approach for Saving Leakage and Refresh Energy
  in Embedded DRAM Caches</title>
    <summary>  In recent years, the size and leakage energy consumption of large last level
caches (LLCs) has increased. To address this, embedded DRAM (eDRAM) caches have
been considered which have lower leakage energy consumption; however eDRAM
caches consume a significant amount of energy in the form of refresh energy. In
this paper, we present a technique for saving both leakage and refresh energy
in eDRAM caches. We use dynamic cache reconfiguration approach to intelligently
turn-off part of the cache to save leakage energy and refresh only valid data
of the active (i.e. not turned-off) cache to save refresh energy. We evaluate
our technique using an x86-64 simulator and SPEC2006 benchmarks and compare it
with a recently proposed technique for saving refresh energy, named Refrint.
The experiments have shown that our technique provides better performance and
energy efficiency than Refrint. Using our technique, for a 2MB LLC and 40
micro-seconds eDRAM refresh period, the average saving in energy over eDRAM
baseline (which periodically refreshes all cache lines) is 22.8%.
</summary>
    <author>
      <name>Sparsh Mittal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Embedded DRAM (eDRAM) caches</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.7082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.7082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.7163v1</id>
    <updated>2013-09-27T09:09:37Z</updated>
    <published>2013-09-27T09:09:37Z</published>
    <title>A Low-Voltage, Low-Power 4-bit BCD Adder, designed using the Clock Gated
  Power Gating, and the DVT Scheme</title>
    <summary>  This paper proposes a Low-Power, Energy Efficient 4-bit Binary Coded Decimal
(BCD) adder design where the conventional 4-bit BCD adder has been modified
with the Clock Gated Power Gating Technique. Moreover, the concept of DVT
(Dual-vth) scheme has been introduced while designing the full adder blocks to
reduce the Leakage Power, as well as, to maintain the overall performance of
the entire circuit. The reported architecture of 4-bit BCD adder is designed
using 45 nm technology and it consumes 1.384 {\mu}Watt of Average Power while
operating with a frequency of 200 MHz, and a Supply Voltage (Vdd) of 1 Volt.
The results obtained from different simulation runs on SPICE, indicate the
superiority of the proposed design compared to the conventional 4-bit BCD
adder. Considering the product of Average Power and Delay, for the operating
frequency of 200 MHz, a fair 47.41 % reduction compared to the conventional
design has been achieved with this proposed scheme.
</summary>
    <author>
      <name>Dipankar Saha</name>
    </author>
    <author>
      <name>Subhramita Basak</name>
    </author>
    <author>
      <name>Sagar Mukherjee</name>
    </author>
    <author>
      <name>C. K. Sarkar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISPCC.2013.6663444</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISPCC.2013.6663444" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the proceedings of 2013 IEEE International Conference on
  Signal Processing, Computing and Control (ISPCC,13)</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.7163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.7163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.7321v1</id>
    <updated>2013-09-27T18:24:32Z</updated>
    <published>2013-09-27T18:24:32Z</published>
    <title>Recycled Error Bits: Energy-Efficient Architectural Support for Higher
  Precision Floating Point</title>
    <summary>  In this work, we provide energy-efficient architectural support for floating
point accuracy. Our goal is to provide accuracy that is far greater than that
provided by the processor's hardware floating point unit (FPU). Specifically,
for each floating point addition performed, we "recycle" that operation's
error: the difference between the finite-precision result produced by the
hardware and the result that would have been produced by an infinite-precision
FPU. We make this error architecturally visible such that it can be used, if
desired, by software. Experimental results on physical hardware show that
software that exploits architecturally recycled error bits can achieve accuracy
comparable to a 2B-bit FPU with performance and energy that are comparable to a
B-bit FPU.
</summary>
    <author>
      <name>Ralph Nathan</name>
    </author>
    <author>
      <name>Bryan Anthonio</name>
    </author>
    <author>
      <name>Shih-Lien Lu</name>
    </author>
    <author>
      <name>Helia Naeimi</name>
    </author>
    <author>
      <name>Daniel J. Sorin</name>
    </author>
    <author>
      <name>Xiaobai Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1309.7321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.7321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.1712v2</id>
    <updated>2015-01-09T14:11:44Z</updated>
    <published>2013-10-07T09:29:02Z</published>
    <title>Partial Sums Computation In Polar Codes Decoding</title>
    <summary>  Polar codes are the first error-correcting codes to provably achieve the
channel capacity but with infinite codelengths. For finite codelengths the
existing decoder architectures are limited in working frequency by the partial
sums computation unit. We explain in this paper how the partial sums
computation can be seen as a matrix multiplication. Then, an efficient hardware
implementation of this product is investigated. It has reduced logic resources
and interconnections. Formalized architectures, to compute partial sums and to
generate the bits of the generator matrix k^n, are presented. The proposed
architecture allows removing the multiplexing resources used to assigned to
each processing elements the required partial sums.
</summary>
    <author>
      <name>Guillaume Berhault</name>
    </author>
    <author>
      <name>Camille Leroux</name>
    </author>
    <author>
      <name>Christophe Jego</name>
    </author>
    <author>
      <name>Dominique Dallet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ISCAS 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.1712v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.1712v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.3356v1</id>
    <updated>2013-10-12T10:05:07Z</updated>
    <published>2013-10-12T10:05:07Z</published>
    <title>A Novel Reconfigurable Computing Architecture for Image Signal
  Processing Using Circuit-Switched NoC and Synchronous Dataflow Model</title>
    <summary>  In this paper, a novel reconfigurable architecture is proposed for
multifunctional image signal processing systems. A circuit-switched NoC is used
to provide interconnection because the non-TMD links ensure fixed throughput,
which is a desirable behavior for computational intensive image processing
algorithms compared with packet-switched NoC. Image processing algorithms are
modeled as synchronous dataflow graphs which provide a unified model for
general computing procedure. An image processing system is considered as
several temporally mutually exclusive algorithms. Thus, their dataflow graph
representations could be considered as a group and a merging algorithm could be
applied to generate a union graph while eliminating spatial redundancy for area
consumption optimization. After the union graph have been mapped and routed on
the NoC, the reconfigurable system could be configured to any of its target
image processing algorithms by properly setting the NoC topology. Experiments
show the demo reconfigurable system with two image processing applications cost
26.4% less hardware resource, compared with the non-reconfigurable
implementations.
</summary>
    <author>
      <name>Feitian Li</name>
    </author>
    <author>
      <name>Fei Qiao</name>
    </author>
    <author>
      <name>Qi Wei</name>
    </author>
    <author>
      <name>Huazhong Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISQED 2014,6 pages,7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.3356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.3356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.8494v1</id>
    <updated>2013-10-31T13:33:16Z</updated>
    <published>2013-10-31T13:33:16Z</published>
    <title>Using Cache-coloring to Mitigate Inter-set Write Variation in
  Non-volatile Caches</title>
    <summary>  In recent years, researchers have explored use of non-volatile devices such
as STT-RAM (spin torque transfer RAM) for designing on-chip caches, since they
provide high density and consume low leakage power. A common limitation of all
non-volatile devices is their limited write endurance. Further, since existing
cache management policies are write-variation unaware, excessive writes to a
few blocks may lead to a quick failure of the whole cache. We propose an
architectural technique for wear-leveling of non-volatile last level caches
(LLCs). Our technique uses cache-coloring approach which adds a
software-controlled mapping layer between groups of physical pages and cache
sets. Periodically the mapping is altered to ensure that write-traffic can be
spread uniformly to different sets of the cache to achieve wear-leveling.
Simulations performed with an x86-64 simulator and SPEC2006 benchmarks show
that our technique reduces the worst-case writes to cache blocks and thus
improves the cache lifetime by 4.07X.
</summary>
    <author>
      <name>Sparsh Mittal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">STT-RAM cache</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.8494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.8494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.0602v2</id>
    <updated>2013-11-05T02:59:59Z</updated>
    <published>2013-11-04T08:23:38Z</published>
    <title>Input-Output Logic based Fault-Tolerant Design Technique for SRAM-based
  FPGAs</title>
    <summary>  Effects of radiation on electronic circuits used in extra-terrestrial
applications and radiation prone environments need to be corrected. Since FPGAs
offer flexibility, the effects of radiation on them need to be studied and
robust methods of fault tolerance need to be devised. In this paper a new
fault-tolerant design strategy has been presented. This strategy exploits the
relation between changes in inputs and the expected change in output.
Essentially, it predicts whether or not a change in the output is expected and
thereby calculates the error. As a result this strategy reduces hardware and
time redundancy required by existing strategies like Duplication with
Comparison (DWC) and Triple Modular Redundancy (TMR). The design arising from
this strategy has been simulated and its robustness to fault-injection has been
verified. Simulations for a 16 bit multiplier show that the new design strategy
performs better than the state-of-the-art on critical factors such as hardware
redundancy, time redundancy and power consumption.
</summary>
    <author>
      <name>Aditya Srinivas Timmaraju</name>
    </author>
    <author>
      <name>Aniket Anand Deshmukh</name>
    </author>
    <author>
      <name>Mohammed Amir Khan</name>
    </author>
    <author>
      <name>Zafar Ali Khan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.0602v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.0602v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.2207v2</id>
    <updated>2014-08-08T20:03:48Z</updated>
    <published>2013-12-08T13:03:15Z</published>
    <title>A Cache Energy Optimization Technique for STT-RAM Last Level Cache</title>
    <summary>  Last level caches (LLCs) occupy a large chip-area and there size is expected
to grow further to offset the limitations of memory bandwidth and speed. Due to
high leakage consumption of SRAM device, caches designed with SRAM consume
large amount of energy. To address this, use of emerging technologies such as
spin torque transfer RAM (STT-RAM) has been investigated which have lower
leakage power dissipation. However, the high write latency and power of it may
lead to large energy consumption which present challenges in its use. In this
report, we propose a cache reconfiguration based technique for improving the
energy efficiency of STT-RAM based LLCs. Our technique dynamically adjusts the
active cache size to reduce the cache leakage energy consumption with minimum
performance loss. We choose a suitable value of STT-RAM retention time for
avoiding refresh overhead and gaining performance. Single-core simulations have
been performed using SPEC2006 benchmarks and Sniper x86-64 simulator. The
results show that while, compared to an STT-RAM LLC of similar area, an SRAM
LLC incurs nearly 100% loss in energy and 7.3% loss in performance; our
technique using STT-RAM cache saves 21.8% energy and incurs only 1.7% loss in
performance.
</summary>
    <author>
      <name>Sparsh Mittal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author for revising experiments</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.2207v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2207v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.2976v1</id>
    <updated>2013-12-10T21:39:01Z</updated>
    <published>2013-12-10T21:39:01Z</published>
    <title>A Survey of Network-On-Chip Tools</title>
    <summary>  Nowadays System-On-Chips (SoCs) have evolved considerably in term of
performances, reliability and integration capacity. The last advantage has
induced the growth of the number of cores or Intellectual Properties (IPs) in a
same chip. Unfortunately, this important number of IPs has caused a new issue
which is the intra-communication between the elements of a same chip. To
resolve this problem, a new paradigm has been introduced which is the
Network-On-Chip (NoC). Since the introduction of the NoC paradigm in the last
decade, new methodologies and approaches have been presented by research
community and many of them have been adopted by industrials. The literature
contains many relevant studies and surveys discussing NoC proposals and
contributions. However, few of them have discussed or proposed a comparative
study of NoC tools. The objective of this work is to establish a reliable
survey about available design, simulation or implementation NoC tools. We
collected an important amount of information and characteristics about NoC
dedicated tools that we will present throughout this survey. This study is
built around a respectable amount of references and we hope it will help
scientists.
</summary>
    <author>
      <name>Ahmed Ben Achballah</name>
    </author>
    <author>
      <name>Slim Ben Saoud</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14569/IJACSA.2013.040910</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14569/IJACSA.2013.040910" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure, 4 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Computer Science and
  Applications(IJACSA), 4(9), 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1312.2976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.0765v1</id>
    <updated>2014-01-04T02:06:23Z</updated>
    <published>2014-01-04T02:06:23Z</published>
    <title>A Survey of Techniques For Improving Energy Efficiency in Embedded
  Computing Systems</title>
    <summary>  Recent technological advances have greatly improved the performance and
features of embedded systems. With the number of just mobile devices now
reaching nearly equal to the population of earth, embedded systems have truly
become ubiquitous. These trends, however, have also made the task of managing
their power consumption extremely challenging. In recent years, several
techniques have been proposed to address this issue. In this paper, we survey
the techniques for managing power consumption of embedded systems. We discuss
the need of power management and provide a classification of the techniques on
several important parameters to highlight their similarities and differences.
This paper is intended to help the researchers and application-developers in
gaining insights into the working of power management techniques and designing
even more efficient high-performance embedded systems of tomorrow.
</summary>
    <author>
      <name>Sparsh Mittal</name>
    </author>
    <link href="http://arxiv.org/abs/1401.0765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.0765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.1003v1</id>
    <updated>2014-01-06T07:36:13Z</updated>
    <published>2014-01-06T07:36:13Z</published>
    <title>On the likelihood of multiple bit upsets in logic circuits</title>
    <summary>  Soft errors have a significant impact on the circuit reliability at nanoscale
technologies. At the architectural level, soft errors are commonly modeled by a
probabilistic bit-flip model. In developing such abstract fault models, an
important issue to consider is the likelihood of multiple bit errors caused by
particle strikes. This likelihood has been studied to a great extent in
memories, but has not been understood to the same extent in logic circuits. In
this paper, we attempt to quantify the likelihood that a single transient event
can cause multiple bit errors in logic circuits consisting of combinational
gates and flip-flops. In particular, we calculate the conditional probability
of multiple bit-flips given that a single bit flips as a result of the
transient. To calculate this conditional probability, we use a Monte Carlo
technique in which samples are generated using detailed post-layout circuit
simulations. Our experiments on the ISCAS'85 benchmarks and a few other
circuits indicate that, this conditional probability is quite significant and
can be as high as 0.31. Thus we conclude that multiple bit-flips must
necessarily be considered in order to obtain a realistic architectural fault
model for soft errors.
</summary>
    <author>
      <name>Nanditha P. Rao</name>
    </author>
    <author>
      <name>Shahbaz Sarik</name>
    </author>
    <author>
      <name>Madhav P. Desai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.1003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.1003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.3421v1</id>
    <updated>2014-01-15T03:25:41Z</updated>
    <published>2014-01-15T03:25:41Z</published>
    <title>Performance Evaluation of ECC in Single and Multi Processor
  Architectures on FPGA Based Embedded System</title>
    <summary>  Cryptographic algorithms are computationally costly and the challenge is more
if we need to execute them in resource constrained embedded systems. Field
Programmable Gate Arrays (FPGAs) having programmable logic de- vices and
processing cores, have proven to be highly feasible implementation platforms
for embedded systems providing lesser design time and reconfig- urability.
Design parameters like throughput, resource utilization and power requirements
are the key issues. The popular Elliptic Curve Cryptography (ECC), which is
superior over other public-key crypto-systems like RSA in many ways, such as
providing greater security for a smaller key size, is cho- sen in this work and
the possibilities of its implementation in FPGA based embedded systems for both
single and dual processor core architectures in- volving task parallelization
have been explored. This exploration, which is first of its kind considering
the other existing works, is a needed activity for evaluating the best possible
architectural environment for ECC implementa- tion on FPGA (Virtex4 XC4VFX12,
FF668, -10) based embedded platform.
</summary>
    <author>
      <name>Sruti Agarwal</name>
    </author>
    <author>
      <name>Sangeet Saha</name>
    </author>
    <author>
      <name>Rourab Paul</name>
    </author>
    <author>
      <name>Amlan Chakrabarti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published Book Title: Elsevier Science and Technology, ICCN 2013,
  Bangalore, Page(s): 140 - 147, Volume 3, 03.elsevierst.2013.3.ICCN16, ISBN
  :9789351071044, Paper
  link:-http://searchdl.org/index.php/book_series/view/917</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.3421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.3421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2415v2</id>
    <updated>2014-12-17T03:35:20Z</updated>
    <published>2014-02-11T10:01:15Z</published>
    <title>Reversible Squaring Circuit For Low Power Digital Signal Processing</title>
    <summary>  With the high demand of low power digital systems, energy dissipation in the
digital system is one of the limiting factors. Reversible logic is one of the
alternate to reduce heat/energy dissipation in the digital circuits and have a
very significant importance in bioinformatics, optical information processing,
CMOS design etc. In this paper the authors propose the design of new 2- bit
binary Squaring circuit used in most of the digital signal processing hardware
using Feynman &amp; MUX gate. The proposed squaring circuit having less garbage
outputs, constant inputs, Quantum cost and Total logical calculation i.e. less
delay as compared to the traditional method of squaring operation by reversible
multiplier. The simulating results and quantized results are also shown in the
paper which shows the greatest improvement in the design against the previous
methodology.
</summary>
    <author>
      <name>Pradeep Singla</name>
    </author>
    <author>
      <name>Devraj Gautam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to a crucial sign
  error in design fig. 3(b)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Electronics, Computer &amp; Communication
  Technology, Volume 4, Issue- 2, Jan-2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.2415v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2415v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2420v1</id>
    <updated>2014-02-11T10:10:39Z</updated>
    <published>2014-02-11T10:10:39Z</published>
    <title>L-Shape based Layout Fracturing for E-Beam Lithography</title>
    <summary>  Layout fracturing is a fundamental step in mask data preparation and e-beam
lithography (EBL) writing. To increase EBL throughput, recently a new L-shape
writing strategy is proposed, which calls for new L-shape fracturing, versus
the conventional rectangular fracturing. Meanwhile, during layout fracturing,
one must minimize very small/narrow features, also called slivers, due to
manufacturability concern. This paper addresses this new research problem of
how to perform L-shaped fracturing with sliver minimization. We propose two
novel algorithms. The first one, rectangular merging (RM), starts from a set of
rectangular fractures and merges them optimally to form L-shape fracturing. The
second algorithm, direct L-shape fracturing (DLF), directly and effectively
fractures the input layouts into L-shapes with sliver minimization. The
experimental results show that our algorithms are very effective.
</summary>
    <author>
      <name>Bei Yu</name>
    </author>
    <author>
      <name>Jhih-Rong Gao</name>
    </author>
    <author>
      <name>David Z. Pan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ASPDAC.2013.6509604</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ASPDAC.2013.6509604" rel="related"/>
    <link href="http://arxiv.org/abs/1402.2420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2459v1</id>
    <updated>2014-02-11T11:36:51Z</updated>
    <published>2014-02-11T11:36:51Z</published>
    <title>Layout decomposition for triple patterning lithography</title>
    <summary>  As minimum feature size and pitch spacing further decrease, triple patterning
lithography (TPL) is a possible 193nm extension along the paradigm of double
patterning lithography (DPL). However, there is very little study on TPL layout
decomposition. In this paper, we show that TPL layout decomposition is a more
difficult problem than that for DPL. We then propose a general integer linear
programming formulation for TPL layout decomposition which can simultaneously
minimize conflict and stitch numbers. Since ILP has very poor scalability, we
propose three acceleration techniques without sacrificing solution quality:
independent component computation, layout graph simplification, and bridge
computation. For very dense layouts, even with these speedup techniques, ILP
formulation may still be too slow. Therefore, we propose a novel vector
programming formulation for TPL decomposition, and solve it through effective
semidefinite programming (SDP) approximation. Experimental results show that
the ILP with acceleration techniques can reduce 82% runtime compared to the
baseline ILP. Using SDP based algorithm, the runtime can be further reduced by
42% with some tradeoff in the stitch number (reduced by 7%) and the conflict
(9% more). However, for very dense layouts, SDP based algorithm can achieve
140x speed-up even compared with accelerated ILP.
</summary>
    <author>
      <name>Bei Yu</name>
    </author>
    <author>
      <name>Kun Yuan</name>
    </author>
    <author>
      <name>Boyang Zhang</name>
    </author>
    <author>
      <name>Duo Ding</name>
    </author>
    <author>
      <name>David Z. Pan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCAD.2011.6105297</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCAD.2011.6105297" rel="related"/>
    <link href="http://arxiv.org/abs/1402.2459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2460v1</id>
    <updated>2014-02-11T11:49:24Z</updated>
    <published>2014-02-11T11:49:24Z</published>
    <title>Network flow-based simultaneous retiming and slack budgeting for low
  power design</title>
    <summary>  Low power design has become one of the most significant requirements when
CMOS technology entered the nanometer era. Therefore, timing budget is often
performed to slow down as many components as possible so that timing slacks can
be applied to reduce the power consumption while maintaining the performance of
the whole design. Retiming is a procedure that involves the relocation of
flip-flops (FFs) across logic gates to achieve faster clocking speed. In this
paper we show that the retiming and slack budgeting problem can be formulated
to a convex cost dual network flow problem. Both the theoretical analysis and
experimental results show the efficiency of our approach which can not only
reduce power consumption by 8.9%, but also speedup previous work by 500 times.
</summary>
    <author>
      <name>Bei Yu</name>
    </author>
    <author>
      <name>Sheqin Dong</name>
    </author>
    <author>
      <name>Yuchun Ma</name>
    </author>
    <author>
      <name>Tao Lin</name>
    </author>
    <author>
      <name>Yu Wang</name>
    </author>
    <author>
      <name>Song Chen</name>
    </author>
    <author>
      <name>Satoshi Goto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ASPDAC.2011.5722236</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ASPDAC.2011.5722236" rel="related"/>
    <link href="http://arxiv.org/abs/1402.2460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2536v1</id>
    <updated>2014-02-11T15:51:52Z</updated>
    <published>2014-02-11T15:51:52Z</published>
    <title>Design and Implementation of Bit Transition Counter</title>
    <summary>  In today VLSI system design, power consumption is gaining more attention as
compared to performance and area. This is due to battery life in portable
devices and operating frequency of the design. Power consumption mainly
consists of static power, dynamic power, leakage power and short circuit power.
Dynamic power is dominant among all which depends on many factors viz. power
supply, load capacitance and frequency. Switching activity also affects dynamic
power consumption of bus which is determined by calculating the number of bit
transitions on bus. The purpose of this paper is to design a bit transition
counter which can be used to calculate the switching activity of the circuit
nodes. The novel feature is that it can be inserted at any node of the circuit,
thus helpful for calculating power consumption of bus.
</summary>
    <author>
      <name>Amandeep Singh</name>
    </author>
    <author>
      <name>Balwinder Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal Paper</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Circuits and Systems: An International Journal (CSIJ), Vol. 1, No.
  1, January 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.2536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2635v1</id>
    <updated>2014-02-11T20:29:09Z</updated>
    <published>2014-02-11T20:29:09Z</published>
    <title>Methodology for standard cell compliance and detailed placement for
  triple patterning lithography</title>
    <summary>  As the feature size of semiconductor process further scales to sub-16nm
technology node, triple patterning lithography (TPL) has been regarded one of
the most promising lithography candidates. M1 and contact layers, which are
usually deployed within standard cells, are most critical and complex parts for
modern digital designs. Traditional design flow that ignores TPL in early
stages may limit the potential to resolve all the TPL conflicts. In this paper,
we propose a coherent framework, including standard cell compliance and
detailed placement to enable TPL friendly design. Considering TPL constraints
during early design stages, such as standard cell compliance, improves the
layout decomposability. With the pre-coloring solutions of standard cells, we
present a TPL aware detailed placement, where the layout decomposition and
placement can be resolved simultaneously. Our experimental results show that,
with negligible impact on critical path delay, our framework can resolve the
conflicts much more easily, compared with the traditional physical design flow
and followed layout decomposition.
</summary>
    <author>
      <name>Bei Yu</name>
    </author>
    <author>
      <name>Xiaoqing Xu</name>
    </author>
    <author>
      <name>Jhih-Rong Gao</name>
    </author>
    <author>
      <name>David Z. Pan</name>
    </author>
    <link href="http://arxiv.org/abs/1402.2635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2890v1</id>
    <updated>2014-02-12T16:43:46Z</updated>
    <published>2014-02-12T16:43:46Z</published>
    <title>A High-Performance Triple Patterning Layout Decomposer with Balanced
  Density</title>
    <summary>  Triple patterning lithography (TPL) has received more and more attentions
from industry as one of the leading candidate for 14nm/11nm nodes. In this
paper, we propose a high performance layout decomposer for TPL. Density
balancing is seamlessly integrated into all key steps in our TPL layout
decomposition, including density-balanced semi-definite programming (SDP),
density-based mapping, and density-balanced graph simplification. Our new TPL
decomposer can obtain high performance even compared to previous
state-of-the-art layout decomposers which are not balanced-density aware, e.g.,
by Yu et al. (ICCAD'11), Fang et al. (DAC'12), and Kuang et al. (DAC'13).
Furthermore, the balanced-density version of our decomposer can provide more
balanced density which leads to less edge placement error (EPE), while the
conflict and stitch numbers are still very comparable to our
non-balanced-density baseline.
</summary>
    <author>
      <name>Bei Yu</name>
    </author>
    <author>
      <name>Yen-Hung Lin</name>
    </author>
    <author>
      <name>Gerard Luk-Pat</name>
    </author>
    <author>
      <name>Duo Ding</name>
    </author>
    <author>
      <name>Kevin Lucas</name>
    </author>
    <author>
      <name>David Z. Pan</name>
    </author>
    <link href="http://arxiv.org/abs/1402.2890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2899v1</id>
    <updated>2014-02-12T17:03:22Z</updated>
    <published>2014-02-12T17:03:22Z</published>
    <title>GLOW: A global router for low-power thermal-reliable interconnect
  synthesis using photonic wavelength multiplexing</title>
    <summary>  In this paper, we examine the integration potential and explore the design
space of low power thermal reliable on-chip interconnect synthesis featuring
nanophotonics Wavelength Division Multiplexing (WDM). With the recent
advancements, it is foreseen that nanophotonics holds the promise to be
employed for future on-chip data signalling due to its unique power efficiency,
signal delay and huge multiplexing potential. However, there are major
challenges to address before feasible on-chip integration could be reached. In
this paper, we present GLOW, a hybrid global router to provide low power
opto-electronic interconnect synthesis under the considerations of thermal
reliability and various physical design constraints such as optical power,
delay and signal quality. GLOW is evaluated with testing cases derived from
ISPD07-08 global routing benchmarks. Compared with a greedy approach, GLOW
demonstrates around 23%-50% of total optical power reduction, revealing great
potential of on-chip WDM interconnect synthesis.
</summary>
    <author>
      <name>Duo Ding</name>
    </author>
    <author>
      <name>Bei Yu</name>
    </author>
    <author>
      <name>David Z. Pan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ASPDAC.2012.6165031</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ASPDAC.2012.6165031" rel="related"/>
    <link href="http://arxiv.org/abs/1402.2899v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2899v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2904v1</id>
    <updated>2014-02-12T17:25:05Z</updated>
    <published>2014-02-12T17:25:05Z</published>
    <title>EPIC: Efficient prediction of IC manufacturing hotspots with a unified
  meta-classification formulation</title>
    <summary>  In this paper we present EPIC, an efficient and effective predictor for IC
manufacturing hotspots in deep sub-wavelength lithography. EPIC proposes a
unified framework to combine different hotspot detection methods together, such
as machine learning and pattern matching, using mathematical
programming/optimization. EPIC algorithm has been tested on a number of
industry benchmarks under advanced manufacturing conditions. It demonstrates so
far the best capability in selectively combining the desirable features of
various hotspot detection methods (3.5-8.2% accuracy improvement) as well as
significant suppression of the detection noise (e.g., 80% false-alarm
reduction). These characteristics make EPIC very suitable for conducting high
performance physical verification and guiding efficient manufacturability
friendly physical design.
</summary>
    <author>
      <name>Duo Ding</name>
    </author>
    <author>
      <name>Bei Yu</name>
    </author>
    <author>
      <name>Joydeep Ghosh</name>
    </author>
    <author>
      <name>David Z. Pan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ASPDAC.2012.6164956</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ASPDAC.2012.6164956" rel="related"/>
    <link href="http://arxiv.org/abs/1402.2904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.3149v1</id>
    <updated>2014-02-13T14:32:26Z</updated>
    <published>2014-02-13T14:32:26Z</published>
    <title>Voltage and Level-Shifter Assignment Driven Floorplanning</title>
    <summary>  Low Power Design has become a significant requirement when the CMOS
technology entered the nanometer era. Multiple-Supply Voltage (MSV) is a
popular and effective method for both dynamic and static power reduction while
maintaining performance. Level shifters may cause area and Interconnect Length
Overhead (ILO), and should be considered at both floorplanning and
post-floorplanning stages. In this paper, we propose a two phases algorithm
framework, called VLSAF, to solve voltage and level shifter assignment problem.
At floorplanning phase, we use a convex cost network flow algorithm to assign
voltage and a minimum cost flow algorithm to handle level-shifter assignment.
At post-floorplanning phase, a heuristic method is adopted to redistribute
white spaces and calculate the positions and shapes of level shifters. The
experimental results show VLSAF is effective.
</summary>
    <author>
      <name>Bei Yu</name>
    </author>
    <author>
      <name>Sheqin Dong</name>
    </author>
    <author>
      <name>Song Chen</name>
    </author>
    <author>
      <name>Satoshi Goto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1587/transfun.E92.A.2990</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1587/transfun.E92.A.2990" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEICE Transactions on Fundamentals of Electronics, Communications and
  Computer Sciences, 2009</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEICE transactions on fundamentals of electronics, communications
  and computer sciences 92.12 (2009): 2990-2997</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.3149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.3149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.6005v1</id>
    <updated>2014-02-24T22:42:02Z</updated>
    <published>2014-02-24T22:42:02Z</published>
    <title>Open Cores for Digital Signal Processing</title>
    <summary>  This paper presents the design and implementation of three System on Chip
(SoC) cores, which implement the Digital Signal Processing (DSP) functions:
Finite Impulse Response (FIR) filter, Infinite Impulse Response (IIR) filter
and Fast Fourier Transform (FFT). The FIR filter core is based on the
symmetrical realization form, the IIR filter core is based on the Second Order
Sections (SOS) architecture and the FFT core is based on the Radix $2^2$ Single
Delay Feedback (R$2^2$SDF) architecture. The three cores are compatible with
the Wishbone SoC bus and they were described using generic and structural VHDL.
In system hardware verification was performed by using an OpenRisc-based SoC
synthesized on an Altera FPGA, the tests showed that the designed DSP cores are
suitable for building SoC based on the OpenRisc processor and the Wishbone bus.
</summary>
    <author>
      <name>Juan Camilo Valderrama-Cuervo</name>
    </author>
    <author>
      <name>Alexander López-Parrado</name>
    </author>
    <link href="http://arxiv.org/abs/1402.6005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.6005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.1928v1</id>
    <updated>2014-03-08T04:21:39Z</updated>
    <published>2014-03-08T04:21:39Z</published>
    <title>Five Modular Redundancy with Mitigation Technique to Recover the Error
  Module</title>
    <summary>  Hazard radiation can lead the system fault therefore Fault Tolerance is
required. Fault Tolerant is a system, which is designed to keep operations
running, despite the degradation in the specific module is happening. Many
fault tolerances have been developed to handle the problem, to find the most
robust and efficient in the possible technology. This paper will present the
Five Modular Redundancy (FMR) with Mitigation Technique to Recover the Error
Module. With Dynamic Partial Reconfiguration technology that have already
available today, such fault tolerance technique can be implemented
successfully. The project showed the robustness of the system is increased and
module which is error can be recovered immediately.
</summary>
    <author>
      <name> Haryono</name>
    </author>
    <author>
      <name>Jazi Eko Istiyanto</name>
    </author>
    <author>
      <name>Agus Harjoko</name>
    </author>
    <author>
      <name>Agfianto Eko Putra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Haryono, Istiyanto, J.E., Harjoko, A., Putra, A.E., 2014,
  International Journal of advanced studies in Computer Science and Engineering
  IJASCSE, Volume 3, Issue 2, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1403.1928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.1928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.4554v1</id>
    <updated>2014-03-18T18:02:25Z</updated>
    <published>2014-03-18T18:02:25Z</published>
    <title>A Flexible Design for Optimization of Hardware Architecture in
  Distributed Arithmetic based FIR Filters</title>
    <summary>  FIR filters are used in many performance/power critical applications such as
mobile communication devices, analogue to digital converters and digital signal
processing applications. Design of appropriate FIR filters usually causes the
order of filter to be increased. Synthesis and tape-out of high-order FIR
filters with reasonable delay, area and power has become an important challenge
for hardware designers. In many cases the complexity of high-order filters
causes the constraints of the total design could not be satisfied. In this
paper, efficient hardware architecture is proposed for distributed arithmetic
(DA) based FIR filters. The architecture is based on optimized combination of
Look-up Tables (LUTs) and compressors. The optimized system level solution is
obtained from a set of dynamic programming optimization algorithms. The
experiments show the proposed design educed the delay cost between 16%-62.5% in
comparison of previous optimized structures for DA-based architectures.
</summary>
    <author>
      <name>Fazel Sharifi</name>
    </author>
    <author>
      <name>Saba Amanollahi</name>
    </author>
    <author>
      <name>Mohammad Amin Taherkhani</name>
    </author>
    <author>
      <name>Omid Hashemipour</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">RadioElectronics &amp; Informatics 4 (2012) 25-30</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1403.4554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.4554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1311v3</id>
    <updated>2016-02-01T01:18:58Z</updated>
    <published>2014-03-25T21:54:56Z</published>
    <title>Comments on "IEEE 1588 Clock Synchronization using Dual Slave Clocks in
  a Slave"</title>
    <summary>  In the above letter, Chin and Chen proposed an IEEE 1588 clock
synchronization method based on dual slave clocks, where they claim that
multiple unknown parameters --- i.e., clock offset, clock skew, and
master-to-slave delay --- can be estimated with only one-way time transfers
using more equations than usual. This comment investigates Chin and Chen's dual
clock scheme with detailed models for a master and dual slave clocks and shows
that the formulation of multi-parameter estimation is invalid, which affirms
that it is impossible to distinguish the effect of delay from that of clock
offset at a slave even with dual slave clocks.
</summary>
    <author>
      <name>Kyeong Soo Kim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LCOMM.2014.2317738</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LCOMM.2014.2317738" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Communications Letters, vol. 18, no. 6, pp. 981-982, Jun.
  2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.1311v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1311v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1602v2</id>
    <updated>2014-04-23T08:53:35Z</updated>
    <published>2014-04-06T17:34:23Z</published>
    <title>A high-level model of embedded flash energy consumption</title>
    <summary>  The alignment of code in the flash memory of deeply embedded SoCs can have a
large impact on the total energy consumption of a computation. We investigate
the effect of code alignment in six SoCs and find that a large proportion of
this energy (up to 15% of total SoC energy consumption) can be saved by changes
to the alignment.
  A flexible model is created to predict the read-access energy consumption of
flash memory on deeply embedded SoCs, where code is executed in place. This
model uses the instruction level memory accesses performed by the processor to
calculate the flash energy consumption of a sequence of instructions. We derive
the model parameters for five SoCs and validate them. The error is as low as
5%, with a 11% average normalized RMS deviation overall.
  The scope for using this model to optimize code alignment is explored across
a range of benchmarks and SoCs. Analysis shows that over 30% of loops can be
better aligned. This can significantly reduce energy while increasing code size
by less than 4%. We conclude that this effect has potential as an effective
optimization, saving significant energy in deeply embedded SoCs.
</summary>
    <author>
      <name>James Pallister</name>
    </author>
    <author>
      <name>Kerstin Eder</name>
    </author>
    <author>
      <name>Simon J. Hollis</name>
    </author>
    <author>
      <name>Jeremy Bennett</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2656106.2656108</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2656106.2656108" rel="related"/>
    <link href="http://arxiv.org/abs/1404.1602v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1602v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.3162v1</id>
    <updated>2014-04-11T17:28:54Z</updated>
    <published>2014-04-11T17:28:54Z</published>
    <title>A Signal Processor for Gaussian Message Passing</title>
    <summary>  In this paper, we present a novel signal processing unit built upon the
theory of factor graphs, which is able to address a wide range of signal
processing algorithms. More specifically, the demonstrated factor graph
processor (FGP) is tailored to Gaussian message passing algorithms. We show how
to use a highly configurable systolic array to solve the message update
equations of nodes in a factor graph efficiently. A proper instruction set and
compilation procedure is presented. In a recursive least squares channel
estimation example we show that the FGP can compute a message update faster
than a state-ofthe- art DSP. The results demonstrate the usabilty of the FGP
architecture as a flexible HW accelerator for signal-processing and
communication systems.
</summary>
    <author>
      <name>Harald Kröll</name>
    </author>
    <author>
      <name>Stefan Zwicky</name>
    </author>
    <author>
      <name>Reto Odermatt</name>
    </author>
    <author>
      <name>Lukas Bruderer</name>
    </author>
    <author>
      <name>Andreas Burg</name>
    </author>
    <author>
      <name>Qiuting Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to the IEEE IEEE International Symposium on Circuits and
  Systems (ISCAS) 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.3162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.3162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.3877v1</id>
    <updated>2014-04-15T11:46:50Z</updated>
    <published>2014-04-15T11:46:50Z</published>
    <title>Design space exploration for image processing architectures on FPGA
  targets</title>
    <summary>  Due to the emergence of embedded applications in image and video processing,
communication and cryptography, improvement of pictorial information for better
human perception like deblurring, denoising in several fields such as satellite
imaging, medical imaging, mobile applications etc. are gaining importance for
renewed research. Behind such developments, the primary responsibility lies
with the advancement of semiconductor technology leading to FPGA based
programmable logic devices, which combines the advantages of both custom
hardware and dedicated DSP resources. In addition, FPGA provides powerful
reconfiguration feature and hence is an ideal target for rapid prototyping. We
have endeavoured to exploit exceptional features of FPGA technology in respect
to hardware parallelism leading to higher computational density and throughput,
and have observed better performances than those one can get just merely
porting the image processing software algorithms to hardware. In this paper, we
intend to present an elaborate review, based on our expertise and experiences,
on undertaking necessary transformation to an image processing software
algorithm including the optimization techniques that makes its operation in
hardware comparatively faster.
</summary>
    <author>
      <name>Chandrajit Pal</name>
    </author>
    <author>
      <name>Avik Kotal</name>
    </author>
    <author>
      <name>Asit Samanta</name>
    </author>
    <author>
      <name>Amlan Chakrabarti</name>
    </author>
    <author>
      <name>Ranjan Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc.International Doctoral Symposium on Applied Computation and
  Security Systems(ACSS) pp. 1-19, 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.3877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.3877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.4629v2</id>
    <updated>2014-04-18T14:43:40Z</updated>
    <published>2014-04-17T19:57:51Z</published>
    <title>A Survey of Methods For Analyzing and Improving GPU Energy Efficiency</title>
    <summary>  Recent years have witnessed a phenomenal growth in the computational
capabilities and applications of GPUs. However, this trend has also led to
dramatic increase in their power consumption. This paper surveys research works
on analyzing and improving energy efficiency of GPUs. It also provides a
classification of these techniques on the basis of their main research idea.
Further, it attempts to synthesize research works which compare energy
efficiency of GPUs with other computing systems, e.g. FPGAs and CPUs. The aim
of this survey is to provide researchers with knowledge of state-of-the-art in
GPU power management and motivate them to architect highly energy-efficient
GPUs of tomorrow.
</summary>
    <author>
      <name>Sparsh Mittal</name>
    </author>
    <author>
      <name>Jeffrey S. Vetter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted with minor revision in ACM Computing Survey Journal (impact
  factor 3.85, five year impact of 7.85)</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.4629v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.4629v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1; I.3.1; H.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.2907v1</id>
    <updated>2014-05-12T16:39:44Z</updated>
    <published>2014-05-12T16:39:44Z</published>
    <title>Massively Parallel Processor Architectures for Resource-aware Computing</title>
    <summary>  We present a class of massively parallel processor architectures called
invasive tightly coupled processor arrays (TCPAs). The presented processor
class is a highly parameterizable template, which can be tailored before
runtime to fulfill costumers' requirements such as performance, area cost, and
energy efficiency. These programmable accelerators are well suited for
domain-specific computing from the areas of signal, image, and video processing
as well as other streaming processing applications. To overcome future scaling
issues (e.g., power consumption, reliability, resource management, as well as
application parallelization and mapping), TCPAs are inherently designed in a
way to support self-adaptivity and resource awareness at hardware level. Here,
we follow a recently introduced resource-aware parallel computing paradigm
called invasive computing where an application can dynamically claim, execute,
and release resources. Furthermore, we show how invasive computing can be used
as an enabler for power management. Finally, we will introduce ideas on how to
realize fault-tolerant loop execution on such massively parallel architectures
through employing on-demand spatial redundancies at the processor array level.
</summary>
    <author>
      <name>Vahid Lari</name>
    </author>
    <author>
      <name>Alexandru Tanase</name>
    </author>
    <author>
      <name>Frank Hannig</name>
    </author>
    <author>
      <name>Jürgen Teich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.2907v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.2907v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.2909v1</id>
    <updated>2014-05-12T16:40:23Z</updated>
    <published>2014-05-12T16:40:23Z</published>
    <title>Emulated ASIC Power and Temperature Monitor System for FPGA Prototyping
  of an Invasive MPSoC Computing Architecture</title>
    <summary>  In this contribution the emulation of an ASIC temperature and power
monitoring system (TPMon) for FPGA prototyping is presented and tested to
control processor temperatures under different control targets and operating
strategies. The approach for emulating the power monitor is based on an
instruction-level energy model. For emulating the temperature monitor, a
thermal RC model is used. The monitoring system supplies an invasive MPSoC
computing architecture with hardware status information (power and temperature
data of the processors within the system). These data are required for
resource-aware load distribution. As a proof of concept different operating
strategies and control targets were evaluated for a 2-tile invasive MPSoC
computing system.
</summary>
    <author>
      <name>Elisabeth Glocker</name>
    </author>
    <author>
      <name>Qingqing Chen</name>
    </author>
    <author>
      <name>Asheque M. Zaidi</name>
    </author>
    <author>
      <name>Ulf Schlichtmann</name>
    </author>
    <author>
      <name>Doris Schmitt-Landsiedel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 1st Workshop on Resource Awareness and Adaptivity in
  Multi-Core Computing (Racing 2014) (arXiv:1405.2281)</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.2909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.2909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.4232v2</id>
    <updated>2014-05-20T09:02:30Z</updated>
    <published>2014-05-16T16:31:35Z</published>
    <title>Architectural Design of a RAM Arbiter</title>
    <summary>  Standard memory modules to store (and access) data are designed for use with
a single system accessing it. More complicated memory modules would be accessed
through a memory controller, which are also designed for one system. For
multiple systems to access a single memory module there must be some
facilitation that allows them to access the memory without overriding or
corrupting the access from the others. This was done with the use of a memory
arbiter, which controls the flow of traffic into the memory controller. The
arbiter has a set of rules to abide to in order to choose which system gets
through to the memory controller. In this project, a regular RAM module is
designed for use with one system. Furthermore, a memory arbiter is also
designed in Verilog that allows for more than one system to use a single RAM
module in a controlled and synchronized manner. The arbiter uses a fixed
priority scheme to avoid starvation of the system. In addition one of the major
problems associated with such systems i.e. The Address Clash Problem has been
nicely tackled and solved. The design is verified in simulation and validated
on a Xilinx ML605 evaluation board with a Virtex 6 FPGA.
</summary>
    <author>
      <name>Sourangsu Banerji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">121 pages, 87 figures, 1 table. Mini Project Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.4232v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4232v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.7662v1</id>
    <updated>2014-06-30T10:46:50Z</updated>
    <published>2014-06-30T10:46:50Z</published>
    <title>Selective Match-Line Energizer Content Addressable Memory(SMLE -CAM)</title>
    <summary>  A Content Addressable Memory (CAM) is a memory primarily designed for high
speed search operation. Parallel search scheme forms the basis of CAM, thus
power reduction is the challenge associated with a large amount of parallel
active circuits. We are presenting a novel algorithm and architecture described
as Selective Match-Line Energizer Content Addressable Memory (SMLE-CAM) which
energizes only those MLs (Match-Line) whose first three bits are conditionally
matched with corresponding first three search bit using special architecture
which comprises of novel XNOR-CAM cell and novel XOR-CAM cell. The rest of the
CAM chain is followed by NOR-CAM cell. The 256 X 144 bit SMLE-CAM is
implemented in TSMC 90 nm technology and its robustness across PVT variation is
verified. The post-layout simulation result shows, it has energy metric of
0.115 fJ/bit/search with search time 361.6 ps, the best reported so far. The
maximum operating frequency is 1GHz.
</summary>
    <author>
      <name>Mohammed Zackriya. V</name>
    </author>
    <author>
      <name>Harish M Kittur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 12 figures. Accepted for publication, International Journal
  of applied Engineering Research,Vol. 8 No. 19, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.7662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.7662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.7.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.2082v1</id>
    <updated>2014-07-08T13:46:42Z</updated>
    <published>2014-07-08T13:46:42Z</published>
    <title>FPGA Based Efficient Multiplier for Image Processing Applications Using
  Recursive Error Free Mitchell Log Multiplier and KOM Architecture</title>
    <summary>  The Digital Image processing applications like medical imaging, satellite
imaging, Biometric trait images etc., rely on multipliers to improve the
quality of image. However, existing multiplication techniques introduce errors
in the output with consumption of more time, hence error free high speed
multipliers has to be designed. In this paper we propose FPGA based Recursive
Error Free Mitchell Log Multiplier (REFMLM) for image Filters. The 2x2 error
free Mitchell log multiplier is designed with zero error by introducing error
correction term is used in higher order Karastuba-Ofman Multiplier (KOM)
Architectures. The higher order KOM multipliers is decomposed into number of
lower order multipliers using radix 2 till basic multiplier block of order 2x2
which is designed by error free Mitchell log multiplier. The 8x8 REFMLM is
tested for Gaussian filter to remove noise in fingerprint image. The Multiplier
is synthesized using Spartan 3 FPGA family device XC3S1500-5fg320. It is
observed that the performance parameters such as area utilization, speed, error
and PSNR are better in the case of proposed architecture compared to existing
architectures
</summary>
    <author>
      <name>Satish S Bhairannawar</name>
    </author>
    <author>
      <name>Rathan R</name>
    </author>
    <author>
      <name>Raja K B</name>
    </author>
    <author>
      <name>Venugopal K R</name>
    </author>
    <author>
      <name>L M Patnaik</name>
    </author>
    <link href="http://arxiv.org/abs/1407.2082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.2082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.0982v1</id>
    <updated>2014-08-05T14:19:21Z</updated>
    <published>2014-08-05T14:19:21Z</published>
    <title>Modeling and simulation of multiprocessor systems MPSoC by SystemC/TLM2</title>
    <summary>  The current manufacturing technology allows the integration of a complex
multiprocessor system on one piece of silicon (MPSoC for Multiprocessor
System-on- Chip). One way to manage the growing complexity of these systems is
to increase the level of abstraction and to address the system-level design. In
this paper, we focus on the implementation in SystemC language with TLM
(Transaction Level Model) to model an MPSOC platform. Our main contribution is
to define a comprehensive, fast and accurate method for designing and
evaluating performance for MPSoC systems. The studied MPSoC is composed of
MicroBlaze microprocessors, memory, a timer, a VGA and an interrupt handler
with two examples of software. This paper has two novel contributions: the
first is to develop this MPSOC at CABA and TLM for ISS (Instruction Set
Simulator), Native simulations and timed Programmer s View (PV+T); the second
is to show that with PV+T simulations we can achieve timing fidelity with
higher speeds than CABA simulations and have almost the same precision.
</summary>
    <author>
      <name>Abdelhakim Alali</name>
    </author>
    <author>
      <name>Ismail Assayad</name>
    </author>
    <author>
      <name>Mohamed Sadik</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 11,
  Issue 3, No 2, May 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.0982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.0982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.4744v2</id>
    <updated>2014-11-16T15:59:04Z</updated>
    <published>2014-09-16T19:39:49Z</published>
    <title>An Efficient List Decoder Architecture for Polar Codes</title>
    <summary>  Long polar codes can achieve the symmetric capacity of arbitrary binary-input
discrete memoryless channels under a low complexity successive cancelation (SC)
decoding algorithm. However, for polar codes with short and moderate code
length, the decoding performance of the SC algorithm is inferior. The cyclic
redundancy check (CRC) aided successive cancelation list (SCL) decoding
algorithm has better error performance than the SC algorithm for short or
moderate polar codes. In this paper, we propose an efficient list decoder
architecture for the CRC aided SCL algorithm, based on both algorithmic
reformulations and architectural techniques. In particular, an area efficient
message memory architecture is proposed to reduce the area of the proposed
decoder architecture. An efficient path pruning unit suitable for large list
size is also proposed. For a polar code of length 1024 and rate $\frac{1}{2}$,
when list size $L=2$ and 4, the proposed list decoder architecture is
implemented under a TSMC 90nm CMOS technology. Compared with the list decoders
in the literature, our decoder achieves 1.33 to 1.96 times hardware efficiency.
</summary>
    <author>
      <name>Jun Lin</name>
    </author>
    <author>
      <name>Zhiyuan Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, accepted by IEEE TVLSI Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.4744v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.4744v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.8018v1</id>
    <updated>2014-09-29T08:06:04Z</updated>
    <published>2014-09-29T08:06:04Z</published>
    <title>An ECG-on-Chip with 535-nW/Channel Integrated Lossless Data Compressor
  for Wireless Sensors</title>
    <summary>  This paper presents a low-power ECG recording system-on-chip (SoC) with
on-chip low-complexity lossless ECG compression for data reduction in
wireless/ambulatory ECG sensor devices. The chip uses a linear slope predictor
for data compression, and incorporates a novel low-complexity dynamic
coding-packaging scheme to frame the prediction error into fixed-length 16-bit
format. The proposed technique achieves an average compression ratio of 2.25x
on MIT/BIH ECG database. Implemented in a standard 0.35 um process, the
compressor uses 0.565K gates/channel occupying 0.4 mm2 for four channels, and
consumes 535 nW/channel at 2.4 V for ECG sampled at 512 Hz. Small size and
ultra-low power consumption makes the proposed technique suitable for wearable
ECG sensor applications.
</summary>
    <author>
      <name>C. J. Deepu</name>
    </author>
    <author>
      <name>X. Zhang</name>
    </author>
    <author>
      <name>W. -S. Liew</name>
    </author>
    <author>
      <name>D. L. T. Wong</name>
    </author>
    <author>
      <name>Y. Lian</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JSSC.2014.2349994</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JSSC.2014.2349994" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Journal of Solid-State Circuits, Nov 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1409.8018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.8018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.8020v1</id>
    <updated>2014-09-29T08:14:15Z</updated>
    <published>2014-09-29T08:14:15Z</published>
    <title>An ECG-on-Chip for Wearable Cardiac Monitoring Devices</title>
    <summary>  This paper describes a highly integrated, low power chip solution for ECG
signal processing in wearable devices. The chip contains an instrumentation
amplifier with programmable gain, a band-pass filter, a 12-bit SAR ADC, a novel
QRS detector, 8K on-chip SRAM, and relevant control circuitry and CPU
interfaces. The analog front end circuits accurately senses and digitizes the
raw ECG signal, which is then filtered to extract the QRS. The sampling
frequency used is 256 Hz. ECG samples are buffered locally on an asynchronous
FIFO and is read out using a faster clock, as and when it is required by the
host CPU via an SPI interface. The chip was designed and implemented in 0.35um
standard CMOS process. The analog core operates at 1V while the digital
circuits and SRAM operate at 3.3V. The chip total core area is 5.74 mm^2 and
consumes 9.6uW. Small size and low power consumption make this design suitable
for usage in wearable heart monitoring devices.
</summary>
    <author>
      <name>C. J. Deepu</name>
    </author>
    <author>
      <name>X. Y. Xu</name>
    </author>
    <author>
      <name>X. D. Zou</name>
    </author>
    <author>
      <name>L. B. Yao</name>
    </author>
    <author>
      <name>Y. Lian</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DELTA.2010.43</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DELTA.2010.43" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">5th IEEE International Symposium on Electronic Design Test and
  Applications 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1409.8020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.8020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.4460v2</id>
    <updated>2015-01-26T21:05:54Z</updated>
    <published>2014-10-16T15:07:54Z</published>
    <title>On Metric Sorting for Successive Cancellation List Decoding of Polar
  Codes</title>
    <summary>  We focus on the metric sorter unit of successive cancellation list decoders
for polar codes, which lies on the critical path in all current hardware
implementations of the decoder. We review existing metric sorter architectures
and we propose two new architectures that exploit the structure of the path
metrics in a log-likelihood ratio based formulation of successive cancellation
list decoding. Our synthesis results show that, for the list size of $L=32$,
our first proposed sorter is $14\%$ faster and $45\%$ smaller than existing
sorters, while for smaller list sizes, our second sorter has a higher delay in
return for up to $36\%$ reduction in the area.
</summary>
    <author>
      <name>Alexios Balatsoukas-Stimming</name>
    </author>
    <author>
      <name>Mani Bastani Parizi</name>
    </author>
    <author>
      <name>Andreas Burg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISCAS.2015.7169066</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISCAS.2015.7169066" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented in 2015 IEEE International Symposium on Circuits and
  Systems (ISCAS'2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.4460v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.4460v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.7560v1</id>
    <updated>2014-10-28T09:11:21Z</updated>
    <published>2014-10-28T09:11:21Z</published>
    <title>Multi Core SSL/TLS Security Processor Architecture Prototype Design with
  automated Preferential Algorithm in FPGA</title>
    <summary>  In this paper a pipelined architecture of a high speed network security
processor (NSP) for SSL,TLS protocol is implemented on a system on chip (SOC)
where hardware information of all encryption, hashing and key exchange
algorithms are stored in flash memory in terms of bit files, in contrary to
related works where all are actually implemented in hardware. The NSP finds
applications in e-commerce, virtual private network (VPN) and in other fields
that require data confidentiality. The motivation of the present work is to
dynamically execute applications with stipulated throughput within budgeted
hardware resource and power. A preferential algorithm choosing an appropriate
cipher suite is proposed, which is based on Efficient System Index (ESI) budget
comprising of power, throughput and resource given by the user. The bit files
of the chosen security algorithms are downloaded from the flash memory to the
partial region of field programmable gate array (FPGA). The proposed SOC
controls data communication between an application running in a system through
a PCI and the Ethernet interface of a network. Partial configuration feature is
used in ISE14.4 suite with ZYNQ 7z020-clg484 FPGA platform. The performances
</summary>
    <author>
      <name>Rourab Paul</name>
    </author>
    <author>
      <name>Amlan Chakrabarti</name>
    </author>
    <author>
      <name>Ranjan Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is Manuscript</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.7560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.7560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.8772v1</id>
    <updated>2014-10-30T08:29:11Z</updated>
    <published>2014-10-30T08:29:11Z</published>
    <title>Programming the Adapteva Epiphany 64-core Network-on-chip Coprocessor</title>
    <summary>  In the construction of exascale computing systems energy efficiency and power
consumption are two of the major challenges. Low-power high performance
embedded systems are of increasing interest as building blocks for large scale
high- performance systems. However, extracting maximum performance out of such
systems presents many challenges. Various aspects from the hardware
architecture to the programming models used need to be explored. The Epiphany
architecture integrates low-power RISC cores on a 2D mesh network and promises
up to 70 GFLOPS/Watt of processing efficiency. However, with just 32 KB of
memory per eCore for storing both data and code, and only low level inter-core
communication support, programming the Epiphany system presents several
challenges. In this paper we evaluate the performance of the Epiphany system
for a variety of basic compute and communication operations. Guided by this
data we explore strategies for implementing scientific applications on memory
constrained low-powered devices such as the Epiphany. With future systems
expected to house thousands of cores in a single chip, the merits of such
architectures as a path to exascale is compared to other competing systems.
</summary>
    <author>
      <name>Anish Varghese</name>
    </author>
    <author>
      <name>Bob Edwards</name>
    </author>
    <author>
      <name>Gaurav Mitra</name>
    </author>
    <author>
      <name>Alistair P. Rendell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, submitted to IJHPCA Journal special edition</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.8772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.8772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.2212v1</id>
    <updated>2014-11-09T09:26:43Z</updated>
    <published>2014-11-09T09:26:43Z</published>
    <title>Designing high-speed, low-power full adder cells based on carbon
  nanotube technology</title>
    <summary>  This article presents novel high speed and low power full adder cells based
on carbon nanotube field effect transistor (CNFET). Four full adder cells are
proposed in this article. First one (named CN9P4G) and second one (CN9P8GBUFF)
utilizes 13 and 17 CNFETs respectively. Third design that we named CN10PFS uses
only 10 transistors and is full swing. Finally, CN8P10G uses 18 transistors and
divided into two modules, causing Sum and Cout signals are produced in a
parallel manner. All inputs have been used straight, without inverting. These
designs also used the special feature of CNFET that is controlling the
threshold voltage by adjusting the diameters of CNFETs to achieve the best
performance and right voltage levels. All simulation performed using Synopsys
HSPICE software and the proposed designs are compared to other classical and
modern CMOS and CNFET-based full adder cells in terms of delay, power
consumption and power delay product.
</summary>
    <author>
      <name>Mehdi Masoudi</name>
    </author>
    <author>
      <name>Milad Mazaheri</name>
    </author>
    <author>
      <name>Aliakbar Rezaei</name>
    </author>
    <author>
      <name>Keivan Navi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/vlsic.2014.5503</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/vlsic.2014.5503" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 Pages, 13 Figures, 2 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.2212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.2917v1</id>
    <updated>2014-11-11T18:52:26Z</updated>
    <published>2014-11-11T18:52:26Z</published>
    <title>Fast Prefix Adders for Non-Uniform Input Arrival Times</title>
    <summary>  We consider the problem of constructing fast and small parallel prefix adders
for non-uniform input arrival times. This problem arises whenever the adder is
embedded into a more complex circuit, e. g. a multiplier.
  Most previous results are based on representing binary carry-propagate adders
as so-called parallel prefix graphs, in which pairs of generate and propagate
signals are combined using complex gates known as prefix gates. Adders
constructed in this model usually minimize the delay in terms of these prefix
gates. However, the delay in terms of logic gates can be worse by a factor of
two.
  In contrast, we aim to minimize the delay of the underlying logic circuit
directly. We prove a lower bound on the delay of a carry bit computation
achievable by any prefix carry bit circuit and develop an algorithm that
computes a prefix carry bit circuit with optimum delay up to a small additive
constant. Furthermore, we use this algorithm to construct a small parallel
prefix adder.
  Compared to existing algorithms we simultaneously improve the delay and size
guarantee, as well as the running time for constructing prefix carry bit and
adder circuits.
</summary>
    <author>
      <name>Stephan Held</name>
    </author>
    <author>
      <name>Sophie Spirkl</name>
    </author>
    <link href="http://arxiv.org/abs/1411.2917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.3492v1</id>
    <updated>2014-11-13T10:37:18Z</updated>
    <published>2014-11-13T10:37:18Z</published>
    <title>Evaluation of silicon consumption for a connectionless Network-on-Chip</title>
    <summary>  We present the design and evaluation of a predictable Network-on-Chip (NoC)
to interconnect processing units running multimedia applications with
variable-bit-rate. The design is based on a connectionless strategy in which
flits from different communication flows are interleaved in the same
communication channel between routers. Each flit carries routing information
used by routers to perform arbitration and scheduling of the corresponding
output communication channel. Analytic comparisons show that our approach keeps
average latency lower than a network based on resource reservation, when both
networks are working over 80% of offered load. We also evaluate the proposed
NoC on FPGA and ASIC technologies to understand the trade-off due to our
approach, in terms of silicon consumption.
</summary>
    <author>
      <name>Marcelo Daniel Berejuck</name>
    </author>
    <author>
      <name>Antônio Augusto Fröhlich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 figures and 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.3492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.3929v1</id>
    <updated>2014-11-14T15:07:54Z</updated>
    <published>2014-11-14T15:07:54Z</published>
    <title>Analog Signal Processing Solution for Image Alignment</title>
    <summary>  Imaging and Image sensors is a field that is continuously evolving. There are
new products coming into the market every day. Some of these have very severe
Size, Weight and Power constraints whereas other devices have to handle very
high computational loads. Some require both these conditions to be met
simultaneously. Current imaging architectures and digital image processing
solutions will not be able to meet these ever increasing demands. There is a
need to develop novel imaging architectures and image processing solutions to
address these requirements. In this work we propose analog signal processing as
a solution to this problem. The analog processor is not suggested as a
replacement to a digital processor but it will be used as an augmentation
device which works in parallel with the digital processor, making the system
faster and more efficient. In order to show the merits of analog processing the
highly computational Normalized Cross Correlation algorithm is implemented. We
propose two novel modifications to the algorithm and a new imaging architecture
which, significantly reduces the computation time.
</summary>
    <author>
      <name>Nihar Athreyas</name>
    </author>
    <author>
      <name>Zhiguo Lai</name>
    </author>
    <author>
      <name>Jai Gupta</name>
    </author>
    <author>
      <name>Dev Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Third International Conference on Advanced Information Technologies &amp;
  Applications (ICAITA-2014) November 7~8, Dubai, UAE ISBN : 978-1-921987-17-5</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.3929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.1140v1</id>
    <updated>2014-12-03T00:16:43Z</updated>
    <published>2014-12-03T00:16:43Z</published>
    <title>Sphynx: A Shared Instruction Cache Exporatory Study</title>
    <summary>  The Sphynx project was an exploratory study to discover what might be done to
improve the heavy replication of in- structions in independent instruction
caches for a massively parallel machine where a single program is executing
across all of the cores. While a machine with only many cores (fewer than 50)
might not have any issues replicating the instructions for each core, as we
approach the era where thousands of cores can be placed on one chip, the
overhead of instruction replication may become unacceptably large. We believe
that a large amount of sharing should be possible when the ma- chine is
configured for all of the threads to issue from the same set of instructions.
We propose a technique that allows sharing an instruction cache among a number
of independent processor cores to allow for inter-thread sharing and reuse of
instruction memory. While we do not have test cases to demonstrate the
potential magnitude of performance gains that could be achieved, the potential
for sharing reduces the die area required for instruction storage on chip.
</summary>
    <author>
      <name>Dong-hyeon Park</name>
    </author>
    <author>
      <name>Akhil Bagaria</name>
    </author>
    <author>
      <name>Fabiha Hannan</name>
    </author>
    <author>
      <name>Eric Storm</name>
    </author>
    <author>
      <name>Josef Spjut</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.1140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.1140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.3.2; C.1.2; C.1.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.2950v1</id>
    <updated>2014-12-09T13:14:29Z</updated>
    <published>2014-12-09T13:14:29Z</published>
    <title>Performance Enhancement of Routers in Networks-on-Chip Using Dynamic
  Virtual Channels Allocation</title>
    <summary>  This study proposes a new router architecture to improve the performance of
dynamic allocation of virtual channels. The proposed router is designed to
reduce the hardware complexity and to improve power and area consumption,
simultaneously. In the new structure of the proposed router, all of the
controlling components have been implemented sequentially inside the allocator
router modules. This optimizes communications between the controlling
components and eliminates the most of hardware overloads of modular
communications. Eliminating additional communications also reduces the hardware
complexity. In order to show the validity of the proposed design in real
hardware resources, the proposed router has been implemented onto a
Field-Programmable Gate Array (FPGA). Since the implementation of a
Network-on-Chip (NoC) requires certain amount of area on the chip, the
suggested approach is also able to reduce the demand of hardware resources. In
this method, the internal memory of the FPGA is used for implementing control
units. This memory is faster and can be used with specific patterns. The use of
the FPGA memory saves the hardware resources and allows the implementation of
NoC based FPGA.
</summary>
    <author>
      <name>Salman Onsori</name>
    </author>
    <author>
      <name>Farshad Safaei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 18 figures, Computer Applications: An International Journal
  (CAIJ), Vol.1, No.2, November 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.2950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.2950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3224v1</id>
    <updated>2014-12-10T08:43:27Z</updated>
    <published>2014-12-10T08:43:27Z</published>
    <title>Prophet: A Speculative Multi-threading Execution Model with
  Architectural Support Based on CMP</title>
    <summary>  Speculative multi-threading (SpMT) has been proposed as a perspective method
to exploit Chip Multiprocessors (CMP) hardware potential. It is a thread level
speculation (TLS) model mainly depending on software and hardware co-design.
This paper researches speculative thread-level parallelism of general purpose
programs and a speculative multi-threading execution model called Prophet is
presented. The architectural support for Prophet execution model is designed
based on CMP. In Prophet the inter-thread data dependency are predicted by
pre-computation slice (p-slice) to reduce RAW violation. Prophet
multi-versioning Cache system along with thread state control mechanism in
architectural support are utilized for buffering the speculative data, and a
snooping bus based cache coherence protocol is used to detect data dependence
violation. The simulation-based evaluation shows that the Prophet system could
achieve significant speedup for general-purpose programs.
</summary>
    <author>
      <name>Dong Zhaoyu</name>
    </author>
    <author>
      <name>Gao Bing</name>
    </author>
    <author>
      <name>Zhao Yinliang</name>
    </author>
    <author>
      <name>Song Shaolong</name>
    </author>
    <author>
      <name>Du Yanning</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/EmbeddedCom-ScalCom.2009.128</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/EmbeddedCom-ScalCom.2009.128" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.3224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3829v5</id>
    <updated>2016-01-11T09:01:44Z</updated>
    <published>2014-12-11T21:29:49Z</published>
    <title>A High-Throughput Energy-Efficient Implementation of
  Successive-Cancellation Decoder for Polar Codes Using Combinational Logic</title>
    <summary>  This paper proposes a high-throughput energy-efficient Successive
Cancellation (SC) decoder architecture for polar codes based on combinational
logic. The proposed combinational architecture operates at relatively low clock
frequencies compared to sequential circuits, but takes advantage of the high
degree of parallelism inherent in such architectures to provide a favorable
tradeoff between throughput and energy efficiency at short to medium block
lengths. At longer block lengths, the paper proposes a hybrid-logic SC decoder
that combines the advantageous aspects of the combinational decoder with the
low-complexity nature of sequential-logic decoders. Performance characteristics
on ASIC and FPGA are presented with a detailed power consumption analysis for
combinational decoders. Finally, the paper presents an analysis of the
complexity and delay of combinational decoders, and of the throughput gains
obtained by hybrid-logic decoders with respect to purely synchronous
architectures.
</summary>
    <author>
      <name>Onur Dizdar</name>
    </author>
    <author>
      <name>Erdal Arıkan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 10 figures, 8 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.3829v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3829v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5538v1</id>
    <updated>2014-12-17T19:39:56Z</updated>
    <published>2014-12-17T19:39:56Z</published>
    <title>Kickstarting High-performance Energy-efficient Manycore Architectures
  with Epiphany</title>
    <summary>  In this paper we introduce Epiphany as a high-performance energy-efficient
manycore architecture suitable for real-time embedded systems. This scalable
architecture supports floating point operations in hardware and achieves 50
GFLOPS/W in 28 nm technology, making it suitable for high performance streaming
applications like radio base stations and radar signal processing. Through an
efficient 2D mesh Network-on-Chip and a distributed shared memory model, the
architecture is scalable to thousands of cores on a single chip. An
Epiphany-based open source computer named Parallella was launched in 2012
through Kickstarter crowd funding and has now shipped to thousands of customers
around the world.
</summary>
    <author>
      <name>Andreas Olofsson</name>
    </author>
    <author>
      <name>Tomas Nordström</name>
    </author>
    <author>
      <name>Zain Ul-Abdin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(to appear in Asilomar Conference on signals, systems, and computers
  2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.5538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.07454v1</id>
    <updated>2015-02-26T06:21:58Z</updated>
    <published>2015-02-26T06:21:58Z</published>
    <title>Generation and Validation of Custom Multiplication IP Blocks from the
  Web</title>
    <summary>  Every CPU carries one or more arithmetical and logical units. One popular
operation that is performed by these units is multiplication. Automatic
generation of custom VHDL models for performing this operation, allows the
designer to achieve a time efficient design space exploration. Although these
units are heavily utilized in modern digital circuits and DSP, there is no
tool, accessible from the web, to generate the HDL description of such designs
for arbitrary and different input bitwidths. In this paper, we present our web
accessible tool to construct completely custom optimized multiplication units
together with random generated test vectors for their verification. Our novel
tool is one of the firsts web based EDA tools to automate the design of such
units and simultaneously provide custom testbenches to verify their
correctness. Our synthesized circuits on Xilinx Virtex 6 FPGA, operate up to
589 Mhz.
</summary>
    <author>
      <name>Minas Dasygenis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at DATE Friday Workshop on Heterogeneous Architectures and
  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.07454v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.07454v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03437v1</id>
    <updated>2015-04-14T07:12:24Z</updated>
    <published>2015-04-14T07:12:24Z</published>
    <title>Low-latency List Decoding Of Polar Codes With Double Thresholding</title>
    <summary>  For polar codes with short-to-medium code length, list successive
cancellation decoding is used to achieve a good error-correcting performance.
However, list pruning in the current list decoding is based on the sorting
strategy and its timing complexity is high. This results in a long decoding
latency for large list size. In this work, aiming at a low-latency list
decoding implementation, a double thresholding algorithm is proposed for a fast
list pruning. As a result, with a negligible performance degradation, the list
pruning delay is greatly reduced. Based on the double thresholding, a
low-latency list decoding architecture is proposed and implemented using a UMC
90nm CMOS technology. Synthesis results show that, even for a large list size
of 16, the proposed low-latency architecture achieves a decoding throughput of
220 Mbps at a frequency of 641 MHz.
</summary>
    <author>
      <name>YouZhe Fan</name>
    </author>
    <author>
      <name>Ji Chen</name>
    </author>
    <author>
      <name>ChenYang Xia</name>
    </author>
    <author>
      <name>Chi-ying Tsui</name>
    </author>
    <author>
      <name>Jie Jin</name>
    </author>
    <author>
      <name>Hui Shen</name>
    </author>
    <author>
      <name>Bin Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2015.7178128</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2015.7178128" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 figures, 1 table, to be presented in the 40th IEEE
  International Conference on Acoustics, Speech and Signal Processing (ICASSP)
  2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.03437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.04297v1</id>
    <updated>2015-04-16T16:36:14Z</updated>
    <published>2015-04-16T16:36:14Z</published>
    <title>MigrantStore: Leveraging Virtual Memory in DRAM-PCM Memory Architecture</title>
    <summary>  With the imminent slowing down of DRAM scaling, Phase Change Memory (PCM) is
emerging as a lead alternative for main memory technology. While PCM achieves
low energy due to various technology-specific advantages, PCM is significantly
slower than DRAM (especially for writes) and can endure far fewer writes before
wearing out. Previous work has proposed to use a large, DRAM-based hardware
cache to absorb writes and provide faster access. However, due to ineffectual
caching where blocks are evicted before sufficient number of accesses, hardware
caches incur significant overheads in energy and bandwidth, two key but scarce
resources in modern multicores. Because using hardware for detecting and
removing such ineffectual caching would incur additional hardware cost and
complexity, we leverage the OS virtual memory support for this purpose. We
propose a DRAM-PCM hybrid memory architecture where the OS migrates pages on
demand from the PCM to DRAM. We call the DRAM part of our memory as
MigrantStore which includes two ideas. First, to reduce the energy, bandwidth,
and wear overhead of ineffectual migrations, we propose migration hysteresis.
Second, to reduce the software overhead of good replacement policies, we
propose recently- accessed-page-id (RAPid) buffer, a hardware buffer to track
the addresses of recently-accessed MigrantStore pages.
</summary>
    <author>
      <name>Hamza Bin Sohail</name>
    </author>
    <author>
      <name>Balajee Vamanan</name>
    </author>
    <author>
      <name>T. N. Vijaykumar</name>
    </author>
    <link href="http://arxiv.org/abs/1504.04297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.04297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03899v1</id>
    <updated>2015-05-14T21:57:26Z</updated>
    <published>2015-05-14T21:57:26Z</published>
    <title>An Approach to Data Prefetching Using 2-Dimensional Selection Criteria</title>
    <summary>  We propose an approach to data memory prefetching which augments the standard
prefetch buffer with selection criteria based on performance and usage pattern
of a given instruction. This approach is built on top of a pattern matching
based prefetcher, specifically one which can choose between a stream, a stride,
or a stream followed by a stride. We track the most recently called
instructions to make a decision on the quantity of data to prefetch next. The
decision is based on the frequency with which these instructions are called and
the hit/miss rate of the prefetcher. In our approach, we separate the amount of
data to prefetch into three categories: a high degree, a standard degree and a
low degree. We ran tests on different values for the high prefetch degree,
standard prefetch degree and low prefetch degree to determine that the most
optimal combination was 1, 4, 8 lines respectively. The 2 dimensional selection
criteria improved the performance of the prefetcher by up to 9.5% over the
first data prefetching championship winner. Unfortunately performance also fell
by as much as 14%, but remained similar on average across all of the benchmarks
we tested.
</summary>
    <author>
      <name>Jean Sung</name>
    </author>
    <author>
      <name>Sebastian Krupa</name>
    </author>
    <author>
      <name>Andrew Fishberg</name>
    </author>
    <author>
      <name>Josef Spjut</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures, submitted to Second Data Prefetching Championship</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.03899v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03899v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.04339v1</id>
    <updated>2015-05-16T23:32:40Z</updated>
    <published>2015-05-16T23:32:40Z</published>
    <title>A 2.48Gb/s QC-LDPC Decoder Implementation on the NI USRP-2953R</title>
    <summary>  The increasing data rates expected to be of the order of Gb/s for future
wireless systems directly impact the throughput requirements of the modulation
and coding subsystems of the physical layer. In an effort to design a suitable
channel coding solution for 5G wireless systems, in this brief we present a
massively-parallel 2.48Gb/s Quasi-Cyclic Low-Density Parity-Check (QC-LDPC)
decoder implementation operating at 200MHz on the NI USRP-2953R, on a single
FPGA. The high-level description of the entire massively-parallel decoder was
translated to a Hardware Description Language (HDL), namely VHDL, using the
algorithmic compiler in the National Instruments LabVIEW Communication System
Design Suite (CSDS) in approximately 2 minutes. This implementation not only
demonstrates the scalability of our decoder architecture but also, the rapid
prototyping capability of the LabVIEW CSDS tools. As per our knowledge, at the
time of writing this paper, this is the fastest implementation of a standard
compliant QC-LDPC decoder on a USRP using an algorithmic compiler.
</summary>
    <author>
      <name>Swapnil Mhaske</name>
    </author>
    <author>
      <name>David Uliana</name>
    </author>
    <author>
      <name>Hojin Kee</name>
    </author>
    <author>
      <name>Tai Ly</name>
    </author>
    <author>
      <name>Ahsan Aziz</name>
    </author>
    <author>
      <name>Predrag Spasojevic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 figures, 5 pages. arXiv admin note: text overlap with
  arXiv:1503.02986</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.04339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.04339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.03181v2</id>
    <updated>2015-08-31T18:28:46Z</updated>
    <published>2015-06-10T06:14:33Z</published>
    <title>DEW: A Fast Level 1 Cache Simulation Approach for Embedded Processors
  with FIFO Replacement Policy</title>
    <summary>  Increasing the speed of cache simulation to obtain hit/miss rates en- ables
performance estimation, cache exploration for embedded sys- tems and energy
estimation. Previously, such simulations, particu- larly exact approaches, have
been exclusively for caches which uti- lize the least recently used (LRU)
replacement policy. In this paper, we propose a new, fast and exact cache
simulation method for the First In First Out(FIFO) replacement policy. This
method, called DEW, is able to simulate multiple level 1 cache configurations
(dif- ferent set sizes, associativities, and block sizes) with FIFO replace-
ment policy. DEW utilizes a binomial tree based representation of cache
configurations and a novel searching method to speed up sim- ulation over
single cache simulators like Dinero IV. Depending on different cache block
sizes and benchmark applications, DEW oper- ates around 8 to 40 times faster
than Dinero IV. Dinero IV compares 2.17 to 19.42 times more cache ways than DEW
to determine accu- rate miss rates.
</summary>
    <author>
      <name>Mohammad Shihabul Haque</name>
    </author>
    <author>
      <name>Jorgen Peddersen</name>
    </author>
    <author>
      <name>Andhi Janapsatya</name>
    </author>
    <author>
      <name>Sri Parameswaran</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DATE.2010.5457153</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DATE.2010.5457153" rel="related"/>
    <link href="http://arxiv.org/abs/1506.03181v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.03181v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.03182v2</id>
    <updated>2015-08-31T18:09:02Z</updated>
    <published>2015-06-10T06:26:02Z</published>
    <title>TRISHUL: A Single-pass Optimal Two-level Inclusive Data Cache Hierarchy
  Selection Process for Real-time MPSoCs</title>
    <summary>  Hitherto discovered approaches analyze the execution time of a real time
application on all the possible cache hierarchy setups to find the application
specific optimal two level inclusive data cache hierarchy to reduce cost, space
and energy consumption while satisfying the time deadline in real time
Multiprocessor Systems on Chip. These brute force like approaches can take
years to complete. Alternatively, memory access trace driven crude estimation
methods can find a cache hierarchy quickly by compromising the accuracy of
results. In this article, for the first time, we propose a fast and accurate
trace driven approach to find the optimal real time application specific two
level inclusive data cache hierarchy. Our proposed approach TRISHUL predicts
the optimal cache hierarchy performance first and then utilizes that
information to find the optimal cache hierarchy quickly. TRISHUL can suggest a
cache hierarchy, which has up to 128 times smaller size, up to 7 times faster
compared to the suggestion of the state of the art crude trace driven two level
inclusive cache hierarchy selection approach for the application traces
analyzed.
</summary>
    <author>
      <name>Mohammad Shihabul Haque</name>
    </author>
    <author>
      <name>Akash Kumar</name>
    </author>
    <author>
      <name>Yajun Ha</name>
    </author>
    <author>
      <name>Qiang Wu</name>
    </author>
    <author>
      <name>Shaobo Luo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ASPDAC.2013.6509615</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ASPDAC.2013.6509615" rel="related"/>
    <link href="http://arxiv.org/abs/1506.03182v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.03182v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.03186v2</id>
    <updated>2015-08-31T18:21:08Z</updated>
    <published>2015-06-10T06:57:55Z</published>
    <title>CIPARSim: Cache Intersection Property Assisted Rapid Single-pass FIFO
  Cache Simulation Technique</title>
    <summary>  In this paper, for the first time, we introduce a cache property called the
Intersection Property that helps to reduce singlepass simulation time in a
manner similar to inclusion property. An intersection property defines
conditions that if met, prove a particular element exists in larger caches,
thus avoiding further search time. We have discussed three such intersection
properties for caches using the FIFO replacement policy in this paper. A rapid
singlepass FIFO cache simulator CIPARSim has also been proposed. CIPARSim is
the first singlepass simulator dependent on the FIFO cache properties to reduce
simulation time significantly. CIPARSim simulation time was up to 5 times
faster compared to the state of the art singlepass FIFO cache simulator for the
cache configurations tested. CIPARSim produces the cache hit and miss rates of
an application accurately on various cache configurations. During simulation,
CIPARSim intersection properties alone predict up to 90% of the total hits,
reducing simulationtime immensely
</summary>
    <author>
      <name>Mohammad Shihabul Haque</name>
    </author>
    <author>
      <name>Jorgen Peddersen</name>
    </author>
    <author>
      <name>Sri Parameswaran</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCAD.2011.6105316</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCAD.2011.6105316" rel="related"/>
    <link href="http://arxiv.org/abs/1506.03186v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.03186v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06811v1</id>
    <updated>2015-08-27T11:37:36Z</updated>
    <published>2015-08-27T11:37:36Z</published>
    <title>Model-based Hardware Design for FPGAs using Folding Transformations
  based on Subcircuits</title>
    <summary>  We present a tool flow and results for a model-based hardware design for
FPGAs from Simulink descriptions which nicely integrates into existing
environments. While current commercial tools do not exploit some high-level
optimizations, we investigate the promising approach of using reusable
subcircuits for folding transformations to control embedded multiplier usage
and to optimize logic block usage. We show that resource improvements of up to
70% compared to the original model are possible, but it is also shown that
subcircuit selection is a critical task. While our tool flow provides good
results already, the investigation and optimization of subcircuit selection is
clearly identified as an additional keypoint to extend high-level control on
low-level FPGA mapping properties.
</summary>
    <author>
      <name>Konrad Möller</name>
    </author>
    <author>
      <name>Martin Kumm</name>
    </author>
    <author>
      <name>Charles-Frederic Müller</name>
    </author>
    <author>
      <name>Peter Zipf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.06811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.07123v1</id>
    <updated>2015-08-28T08:33:15Z</updated>
    <published>2015-08-28T08:33:15Z</published>
    <title>Proposal of ROS-compliant FPGA Component for Low-Power Robotic Systems</title>
    <summary>  In recent years, robots are required to be autonomous and their robotic
software are sophisticated. Robots have a problem of insufficient performance,
since it cannot equip with a high-performance microprocessor due to
battery-power operation. On the other hand, FPGA devices can accelerate
specific functions in a robot system without increasing power consumption by
implementing customized circuits. But it is difficult to introduce FPGA devices
into a robot due to large development cost of an FPGA circuit compared to
software. Therefore, in this study, we propose an FPGA component technology for
an easy integration of an FPGA into robots, which is compliant with ROS (Robot
Operating System). As a case study, we designed ROS-compliant FPGA component of
image labeling using Xilinx Zynq platform. The developed ROS-component FPGA
component performs 1.7 times faster compared to the ordinary ROS software
component.
</summary>
    <author>
      <name>Kazushi Yamashina</name>
    </author>
    <author>
      <name>Takeshi Ohkawa</name>
    </author>
    <author>
      <name>Kanemitsu Ootsu</name>
    </author>
    <author>
      <name>Takashi Yokota</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.07123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.07123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.07127v1</id>
    <updated>2015-08-28T08:45:35Z</updated>
    <published>2015-08-28T08:45:35Z</published>
    <title>Virtualization Architecture for NoC-based Reconfigurable Systems</title>
    <summary>  We propose a virtualization architecture for NoC-based reconfigurable
systems. The motivation of this work is to develop a service-oriented
architecture that includes Partial Reconfigurable Region as a Service (PRRaaS)
and Processing Element as a Service (PEaaS) for software applications.
According to the requirements of software applications, new PEs can be created
on-demand by (re)configuring the logic resource of the PRRs in the FPGA, while
the configured PEs can also be virtualized to support multiple application
tasks at the same time. As a result, such a two-level virtualization mechanism,
including the gate-level virtualization and the PE-level virtualization,
enables an SoC to be dynamically adapted to changing application requirements.
Therefore, more software applications can be performed, and system performance
can be further enhanced.
</summary>
    <author>
      <name>Chun-Hsian Huang</name>
    </author>
    <author>
      <name>Kwuan-Wei Tseng</name>
    </author>
    <author>
      <name>Chih-Cheng Lin</name>
    </author>
    <author>
      <name>Fang-Yu Lin</name>
    </author>
    <author>
      <name>Pao-Ann Hsiung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.07127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.07127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.07139v1</id>
    <updated>2015-08-28T09:19:57Z</updated>
    <published>2015-08-28T09:19:57Z</published>
    <title>Using System Hyper Pipelining (SHP) to Improve the Performance of a
  Coarse-Grained Reconfigurable Architecture (CGRA) Mapped on an FPGA</title>
    <summary>  The well known method C-Slow Retiming (CSR) can be used to automatically
convert a given CPU into a multithreaded CPU with independent threads. These
CPUs are then called streaming or barrel processors. System Hyper Pipelining
(SHP) adds a new flexibility on top of CSR by allowing a dynamic number of
threads to be executed and by enabling the threads to be stalled, bypassed and
reordered. SHP is now applied on the programming elements (PE) of a
coarse-grained reconfigurable architecture (CGRA). By using SHP, more
performance can be achieved per PE. Fork-Join operations can be implemented on
a PE using the flexibility provided by SHP to dynamically adjust the number of
threads per PE. Multiple threads can share the same data locally, which greatly
reduces the data traffic load on the CGRA's routing structure. The paper shows
the results of a CGRA using SHP-ed RISC-V cores as PEs implemented on a FPGA.
</summary>
    <author>
      <name>Tobias Strauch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.07139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.07139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.00040v1</id>
    <updated>2015-08-27T12:23:57Z</updated>
    <published>2015-08-27T12:23:57Z</published>
    <title>DSL-based Design Space Exploration for Temporal and Spatial Parallelism
  of Custom Stream Computing</title>
    <summary>  Stream computation is one of the approaches suitable for FPGA-based custom
computing due to its high throughput capability brought by pipelining with
regular memory access. To increase performance of iterative stream computation,
we can exploit both temporal and spatial parallelism by deepening and
duplicating pipelines, respectively. However, the performance is constrained by
several factors including available hardware resources on FPGA, an external
memory bandwidth, and utilization of pipeline stages, and therefore we need to
find the best mix of the different parallelism to achieve the highest
performance per power. In this paper, we present a domain-specific language
(DSL) based design space exploration for temporally and/or spatially parallel
stream computation with FPGA. We define a DSL where we can easily design a
hierarchical structure of parallel stream computation with abstract description
of computation. For iterative stream computation of fluid dynamics simulation,
we design hardware structures with a different mix of the temporal and spatial
parallelism. By measuring the performance and the power consumption, we find
the best among them.
</summary>
    <author>
      <name>Kentaro Sano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.00040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.00040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.02308v2</id>
    <updated>2016-03-14T13:20:42Z</updated>
    <published>2015-09-08T10:13:02Z</published>
    <title>Dissecting GPU Memory Hierarchy through Microbenchmarking</title>
    <summary>  Memory access efficiency is a key factor in fully utilizing the computational
power of graphics processing units (GPUs). However, many details of the GPU
memory hierarchy are not released by GPU vendors. In this paper, we propose a
novel fine-grained microbenchmarking approach and apply it to three generations
of NVIDIA GPUs, namely Fermi, Kepler and Maxwell, to expose the previously
unknown characteristics of their memory hierarchies. Specifically, we
investigate the structures of different GPU cache systems, such as the data
cache, the texture cache and the translation look-aside buffer (TLB). We also
investigate the throughput and access latency of GPU global memory and shared
memory. Our microbenchmark results offer a better understanding of the
mysterious GPU memory hierarchy, which will facilitate the software
optimization and modelling of GPU architectures. To the best of our knowledge,
this is the first study to reveal the cache properties of Kepler and Maxwell
GPUs, and the superiority of Maxwell in shared memory performance under bank
conflict.
</summary>
    <author>
      <name>Xinxin Mei</name>
    </author>
    <author>
      <name>Xiaowen Chu</name>
    </author>
    <link href="http://arxiv.org/abs/1509.02308v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.02308v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03721v1</id>
    <updated>2015-09-12T08:02:39Z</updated>
    <published>2015-09-12T08:02:39Z</published>
    <title>DReAM: Dynamic Re-arrangement of Address Mapping to Improve the
  Performance of DRAMs</title>
    <summary>  The initial location of data in DRAMs is determined and controlled by the
'address-mapping' and even modern memory controllers use a fixed and
run-time-agnostic address mapping. On the other hand, the memory access pattern
seen at the memory interface level will dynamically change at run-time. This
dynamic nature of memory access pattern and the fixed behavior of address
mapping process in DRAM controllers, implied by using a fixed address mapping
scheme, means that DRAM performance cannot be exploited efficiently. DReAM is a
novel hardware technique that can detect a workload-specific address mapping at
run-time based on the application access pattern which improves the performance
of DRAMs. The experimental results show that DReAM outperforms the best
evaluated address mapping on average by 9%, for mapping-sensitive workloads, by
2% for mapping-insensitive workloads, and up to 28% across all the workloads.
DReAM can be seen as an insurance policy capable of detecting which scenarios
are not well served by the predefined address mapping.
</summary>
    <author>
      <name>Mohsen Ghasempour</name>
    </author>
    <author>
      <name>Jim Garside</name>
    </author>
    <author>
      <name>Aamer Jaleel</name>
    </author>
    <author>
      <name>Mikel Luján</name>
    </author>
    <link href="http://arxiv.org/abs/1509.03721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.04618v1</id>
    <updated>2015-09-11T16:38:57Z</updated>
    <published>2015-09-11T16:38:57Z</published>
    <title>Cost Efficient Design of Reversible Adder Circuits for Low Power
  Applications</title>
    <summary>  A large amount of research is currently going on in the field of reversible
logic, which have low heat dissipation, low power consumption, which is the
main factor to apply reversible in digital VLSI circuit design. This paper
introduces reversible gate named as Inventive0 gate. The novel gate is
synthesis the efficient adder modules with minimum garbage output and gate
count. The Inventive0 gate capable of implementing a 4-bit ripple carry adder
and carry skip adders.It is presented that Inventive0 gate is much more
efficient and optimized approach as compared to their existing design, in terms
of gate count, garbage outputs and constant inputs. In addition, some popular
available reversible gates are implemented in the MOS transistor design the
implementation kept in mind for minimum MOS transistor count and are completely
reversible in behavior more precise forward and backward computation. Lesser
architectural complexity show that the novel designs are compact, fast as well
as low power.
</summary>
    <author>
      <name>Neeraj Kumar Misra</name>
    </author>
    <author>
      <name>Mukesh Kumar Kushwaha</name>
    </author>
    <author>
      <name>Subodh Wairya</name>
    </author>
    <author>
      <name>Amit Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 12 figures, journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.04618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.04618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.08111v3</id>
    <updated>2015-11-22T13:28:24Z</updated>
    <published>2015-09-27T17:32:42Z</published>
    <title>Automatic latency balancing in VHDL-implemented complex pipelined
  systems</title>
    <summary>  Balancing (equalization) of latency in parallel paths in the pipelined data
processing system is an important problem. Without that the data from different
paths arrive at the processing blocks in different clock cycles, and incorrect
results are produced. Manual correction of latencies is a tedious and
error-prone work. This paper presents an automatic method of latency
equalization in systems described in VHDL. The method is based on simulation
and is portable between different simulation and synthesis tools. The method
does not increase the complexity of the synthesized design comparing to the
solution based on manual latency adjustment. The example implementation of the
proposed methodology together with a simple design demonstrating its use is
available as an open source project under BSD license.
</summary>
    <author>
      <name>Wojciech M. Zabolotny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated bibliography. Small language corrections</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.08111v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.08111v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.09249v1</id>
    <updated>2015-09-30T16:36:32Z</updated>
    <published>2015-09-30T16:36:32Z</published>
    <title>In-Field Logic Repair of Deep Sub-Micron CMOS Processors</title>
    <summary>  Ultra Deep-Sub-Micron CMOS chips have to function correctly and reliably, not
only during their early post-fabrication life, but also for their entire life
span. In this paper, we present an architectural-level in-field repair
technique. The key idea is to trade area for reliability by adding repair
features to the system while keeping the power and the performance overheads as
low as possible. In the case of permanent faults, spare blocks will replace the
faulty blocks on the fly. Meanwhile by shutting down the main logic blocks,
partial threshold voltage recovery can be achieved which will alleviate the
ageing-related delays and timing issues. The technique can avoid fatal
shut-downs in the system and will decrease the down-time, hence the
availability of such a system will be preserved. We have implemented the
proposed idea on a pipelined processor core using a conventional ASIC design
flow. The simulation results show that by tolerating about 70% area overhead
and less than 18% power overhead we can dramatically increase the reliability
and decrease the downtime of the processor.
</summary>
    <author>
      <name>Massoud Mokhtarpour Ghahroodi</name>
    </author>
    <author>
      <name>Mark Zwolinski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.1.2435.6566</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.1.2435.6566" rel="related"/>
    <link href="http://arxiv.org/abs/1509.09249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.09249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04241v1</id>
    <updated>2015-10-14T19:09:21Z</updated>
    <published>2015-10-14T19:09:21Z</published>
    <title>A Clock Synchronizer for Repeaterless Low Swing On-Chip Links</title>
    <summary>  A clock synchronizing circuit for repeaterless low swing interconnects is
presented in this paper. The circuit uses a delay locked loop (DLL) to generate
multiple phases of the clock, of which the one closest to the center of the eye
is picked by a phase detector loop. The picked phase is then further fine tuned
by an analog voltage controlled delay to position the sampling clock at the
center of the eye. A clock domain transfer circuit then transfers the sampled
data to the receiver clock domain with a maximum latency of three clock cycles.
The proposed synchronizer has been designed and fabricated in 130 nm UMC MM
CMOS technology. The circuit consumes 1.4 mW from a 1.2 V supply at a data rate
of 1.3 Gbps. Further, the proposed synchronizer has been designed and simulated
in TSMC 65 nm CMOS technology. Post layout simulations show that the
synchronizer consumes 1.5 mW from a 1 V supply, at a data rate of 4 Gbps in
this technology.
</summary>
    <author>
      <name>Naveen Kadayinti</name>
    </author>
    <author>
      <name>Maryam Shojaei Baghini</name>
    </author>
    <author>
      <name>Dinesh K. Sharma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 25 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.06791v1</id>
    <updated>2015-10-23T00:14:10Z</updated>
    <published>2015-10-23T00:14:10Z</published>
    <title>Network-on-Chip with load balancing based on interleave of flits
  technique</title>
    <summary>  This paper presents the evaluation of a Network-on-Chip (NoC) that offers
load balancing for Systems-on-Chip (SoCs) dedicated for multimedia applications
that require high traffic of variable bitrate communication. The NoC is based
on a technique that allows the interleaving of flits from diferente flows in
the same communication channel, and keep the load balancing without a
centralized control in the network. For this purpose, all flits in the network
received extra bits, such that every flit carries routing information. The
routers use this extra information to perform arbitration and schedule the
flits to the corresponding output ports. Analytic comparisons and experimental
data show that the approach adopted in the network keeps average latency lower
for variable bitrate flows than a network based on resource reservation when
both networks are working over 80% of offered load.
</summary>
    <author>
      <name>Marcelo Daniel Berejuck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages. arXiv admin note: substantial text overlap with
  arXiv:1411.3492</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.06791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.06791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.01946v1</id>
    <updated>2015-11-05T23:00:13Z</updated>
    <published>2015-11-05T23:00:13Z</published>
    <title>SecureD: A Secure Dual Core Embedded Processor</title>
    <summary>  Security of embedded computing systems is becoming of paramount concern as
these devices become more ubiquitous, contain personal information and are
increasingly used for financial transactions. Security attacks targeting
embedded systems illegally gain access to the information in these devices or
destroy information. The two most common types of attacks embedded systems
encounter are code-injection and power analysis attacks. In the past, a number
of countermeasures, both hardware- and software-based, were proposed
individually against these two types of attacks. However, no single system
exists to counter both of these two prominent attacks in a processor based
embedded system. Therefore, this paper, for the first time, proposes a
hardware/software based countermeasure against both code-injection attacks and
power analysis based side-channel attacks in a dual core embedded system. The
proposed processor, named SecureD, has an area overhead of just 3.80% and an
average runtime increase of 20.0% when compared to a standard dual processing
system. The overhead were measured using a set of industry standard application
benchmarks, with two encryption and five other programs.
</summary>
    <author>
      <name>Roshan G. Ragel</name>
    </author>
    <author>
      <name>Jude A. Ambrose</name>
    </author>
    <author>
      <name>Sri Parameswaran</name>
    </author>
    <link href="http://arxiv.org/abs/1511.01946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.01946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06726v1</id>
    <updated>2015-11-20T19:10:00Z</updated>
    <published>2015-11-20T19:10:00Z</published>
    <title>Testable Design of Repeaterless Low Swing On-Chip Interconnect</title>
    <summary>  Repeaterless low swing interconnects use mixed signal circuits to achieve
high performance at low power. When these interconnects are used in large scale
and high volume digital systems their testability becomes very important. This
paper discusses the testability of low swing repeaterless on-chip interconnects
with equalization and clock synchronization. A capacitively coupled transmitter
with a weak driver is used as the transmitter. The receiver samples the low
swing input data at the center of the data eye and converts it to rail to rail
levels and also synchronizes the data to the receiver's clock domain. The
system is a mixed signal circuit and the digital components are all scan
testable. For the analog section, just a DC test has a fault coverage of 50% of
the structural faults. Simple techniques allow integration of the analog
components into the digital scan chain increasing the coverage to 74%. Finally,
a BIST with low overhead enhances the coverage to 95% of the structural faults.
The design and simulations have been done in UMC 130 nm CMOS technology.
</summary>
    <author>
      <name>Naveen Kadayinti</name>
    </author>
    <author>
      <name>Dinesh K. Sharma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Design Automation and Test in Europe (DATE) 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.06726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.08774v3</id>
    <updated>2016-07-27T19:34:11Z</updated>
    <published>2015-11-27T19:44:36Z</published>
    <title>Tardis 2.0: Optimized Time Traveling Coherence for Relaxed Consistency
  Models</title>
    <summary>  Cache coherence scalability is a big challenge in shared memory systems.
Traditional protocols do not scale due to the storage and traffic overhead of
cache invalidation. Tardis, a recently proposed coherence protocol, removes
cache invalidation using logical timestamps and achieves excellent scalability.
The original Tardis protocol, however, only supports the Sequential Consistency
(SC) memory model, limiting its applicability. Tardis also incurs extra network
traffic on some benchmarks due to renew messages, and has suboptimal
performance when the program uses spinning to communicate between threads.
  In this paper, we address these downsides of Tardis protocol and make it
significantly more practical. Specifically, we discuss the architectural,
memory system and protocol changes required in order to implement the TSO
consistency model on Tardis, and prove that the modified protocol satisfies
TSO. We also describe modifications for Partial Store Order (PSO) and Release
Consistency (RC). Finally, we propose optimizations for better leasing policies
and to handle program spinning. On a set of benchmarks, optimized Tardis
improves on a full-map directory protocol in the metrics of performance,
storage and network traffic, while being simpler to implement.
</summary>
    <author>
      <name>Xiangyao Yu</name>
    </author>
    <author>
      <name>Hongzhe Liu</name>
    </author>
    <author>
      <name>Ethan Zou</name>
    </author>
    <author>
      <name>Srinivas Devadas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.08774v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.08774v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.09074v1</id>
    <updated>2015-11-29T19:42:09Z</updated>
    <published>2015-11-29T19:42:09Z</published>
    <title>Digital LDO with Time-Interleaved Comparators for Fast Response and Low
  Ripple</title>
    <summary>  On-chip voltage regulation using distributed Digital Low Drop Out (LDO)
voltage regulators has been identified as a promising technique for efficient
power-management for emerging multi-core processors. Digital LDOs (DLDO) can
offer low voltage operation, faster transient response, and higher current
efficiency. Response time as well as output voltage ripple can be reduced by
increasing the speed of the dynamic comparators. However, the comparator offset
steeply increases for high clock frequencies, thereby leading to enhanced
variations in output voltage. In this work we explore the design of digital
LDOs with multiple dynamic comparators that can overcome this bottleneck. In
the proposed topology, we apply time-interleaved comparators with the same
voltage threshold and uniform current step in order to accomplish the
aforementioned features. Simulation based analysis shows that the DLDO with
time-interleaved comparators can achieve better overall performance in terms of
current efficiency, ripple and settling time. For a load step of 50mA, a DLDO
with 8 time-interleaved comparators could achieve an output ripple of less than
5mV, while achieving a settling time of less than 0.5us. Load current dependant
dynamic adjustment of clock frequency is proposed to maintain high current
efficiency of ~97%.
</summary>
    <author>
      <name>Sohail Ahasan</name>
    </author>
    <author>
      <name>Saurav Maji</name>
    </author>
    <author>
      <name>Kauhsik Roy</name>
    </author>
    <author>
      <name>Mrigank Sharad</name>
    </author>
    <link href="http://arxiv.org/abs/1511.09074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.09074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.00504v1</id>
    <updated>2015-12-01T22:32:21Z</updated>
    <published>2015-12-01T22:32:21Z</published>
    <title>Efficient Edge Detection on Low-Cost FPGAs</title>
    <summary>  Improving the efficiency of edge detection in embedded applications, such as
UAV control, is critical for reducing system cost and power dissipation. Field
programmable gate arrays (FPGA) are a good platform for making improvements
because of their specialised internal structure. However, current FPGA edge
detectors do not exploit this structure well. A new edge detection architecture
is proposed that is better optimised for FPGAs. The basis of the architecture
is the Sobel edge kernels that are shown to be the most suitable because of
their separability and absence of multiplications. Edge intensities are
calculated with a new 4:2 compressor that consists of two custom-designed 3:2
compressors. Addition speed is increased by breaking carry propagation chains
with look-ahead logic. Testing of the design showed it gives a 28% increase in
speed and 4.4% reduction in area over previous equivalent designs, which
demonstrated that it will lower the cost of edge detection systems, dissipate
less power and still maintain high-speed control.
</summary>
    <author>
      <name>Jamie Schiel</name>
    </author>
    <author>
      <name>Andrew Bainbridge-Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.00504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.00504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03128v2</id>
    <updated>2016-01-22T15:24:12Z</updated>
    <published>2015-12-10T02:34:13Z</published>
    <title>Partitioned Successive-Cancellation List Decoding of Polar Codes</title>
    <summary>  Successive-cancellation list (SCL) decoding is an algorithm that provides
very good error-correction performance for polar codes. However, its hardware
implementation requires a large amount of memory, mainly to store intermediate
results. In this paper, a partitioned SCL algorithm is proposed to reduce the
large memory requirements of the conventional SCL algorithm. The decoder tree
is broken into partitions that are decoded separately. We show that with
careful selection of list sizes and number of partitions, the proposed
algorithm can outperform conventional SCL while requiring less memory.
</summary>
    <author>
      <name>Seyyed Ali Hashemi</name>
    </author>
    <author>
      <name>Alexios Balatsoukas-Stimming</name>
    </author>
    <author>
      <name>Pascal Giard</name>
    </author>
    <author>
      <name>Claude Thibeault</name>
    </author>
    <author>
      <name>Warren J. Gross</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2016.7471817</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2016.7471817" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures, to appear at IEEE ICASSP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.03128v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03128v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00894v2</id>
    <updated>2016-01-07T12:12:42Z</updated>
    <published>2016-01-05T16:29:17Z</published>
    <title>Configurable memory systems for embedded many-core processors</title>
    <summary>  The memory system of a modern embedded processor consumes a large fraction of
total system energy. We explore a range of different configuration options and
show that a reconfigurable design can make better use of the resources
available to it than any fixed implementation, and provide large improvements
in both performance and energy consumption. Reconfigurability becomes
increasingly useful as resources become more constrained, so is particularly
relevant in the embedded space.
  For an optimised architectural configuration, we show that a configurable
cache system performs an average of 20% (maximum 70%) better than the best
fixed implementation when two programs are competing for the same resources,
and reduces cache miss rate by an average of 70% (maximum 90%). We then present
a case study of AES encryption and decryption, and find that a custom memory
configuration can almost double performance, with further benefits being
achieved by specialising the task of each core when parallelising the program.
</summary>
    <author>
      <name>Daniel Bates</name>
    </author>
    <author>
      <name>Alex Chadwick</name>
    </author>
    <author>
      <name>Robert Mullins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at HIP3ES, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.00894v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00894v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01463v1</id>
    <updated>2016-01-07T10:12:25Z</updated>
    <published>2016-01-07T10:12:25Z</published>
    <title>Design of a Low-Power 1.65 Gbps Data Channel for HDMI Transmitter</title>
    <summary>  This paper presents a design of low power data channel for application in
High Definition Multimedia Interface (HDMI) Transmitter circuit. The input is
10 bit parallel data and output is serial data at 1.65 Gbps. This circuit uses
only a single frequency of serial clock input. All other timing signals are
derived within the circuit from the serial clock. This design has dedicated
lines to disable and enable all its channels within two pixel-clock periods
only. A pair of disable and enable functions performed immediately after
power-on of the circuit serves as the reset function. The presented design is
immune to data-dependent switching spikes in supply current and pushes them in
the range of serial frequency and its multiples. Thus filtering requirements
are relaxed. The output stage uses a bias voltage of 2.8 volts for a receiver
pull-up voltage of 3.3 volts. The reported data channel is designed using UMC
180 nm CMOS Technology. The design is modifiable for other inter-board serial
interfaces like USB and LAN with different number of bits at the parallel
input.
</summary>
    <author>
      <name>Ajay Agrawal</name>
    </author>
    <author>
      <name>R. S. Gamad</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/vlsic.2015.6603</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/vlsic.2015.6603" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">TMDS, HDMI, USB, Gbps, data-dependent jitter, supply current, UMC180,
  low-power consumption, single serial clock</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of VLSI Design &amp; Communication Systems
  (VLSICS), December 2015, Volume 6, Number 6</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.01463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.06903v1</id>
    <updated>2016-01-26T06:36:03Z</updated>
    <published>2016-01-26T06:36:03Z</published>
    <title>Tiered-Latency DRAM (TL-DRAM)</title>
    <summary>  This paper summarizes the idea of Tiered-Latency DRAM, which was published in
HPCA 2013. The key goal of TL-DRAM is to provide low DRAM latency at low cost,
a critical problem in modern memory systems. To this end, TL-DRAM introduces
heterogeneity into the design of a DRAM subarray by segmenting the bitlines,
thereby creating a low-latency, low-energy, low-capacity portion in the
subarray (called the near segment), which is close to the sense amplifiers, and
a high-latency, high-energy, high-capacity portion, which is farther away from
the sense amplifiers. Thus, DRAM becomes heterogeneous with a small portion
having lower latency and a large portion having higher latency. Various
techniques can be employed to take advantage of the low-latency near segment
and this new heterogeneous DRAM substrate, including hardware-based caching and
software based caching and memory allocation of frequently used data in the
near segment. Evaluations with simple such techniques show significant
performance and energy-efficiency benefits.
</summary>
    <author>
      <name>Donghyuk Lee</name>
    </author>
    <author>
      <name>Yoongu Kim</name>
    </author>
    <author>
      <name>Vivek Seshadri</name>
    </author>
    <author>
      <name>Jamie Liu</name>
    </author>
    <author>
      <name>Lavanya Subramanian</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a summary of the original paper, entitled "Tiered-Latency
  DRAM: A Low Latency and Low Cost DRAM Architecture" which appears in HPCA
  2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.06903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.06903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01329v1</id>
    <updated>2016-02-03T15:02:05Z</updated>
    <published>2016-02-03T15:02:05Z</published>
    <title>Effect of Data Sharing on Private Cache Design in Chip Multiprocessors</title>
    <summary>  In multithreaded applications with high degree of data sharing, the miss rate
of private cache is shown to exhibit a compulsory miss component. It manifests
because at least some of the shared data originates from other cores and can
only be accessed in a shared cache. The compulsory component does not change
with the private cache size, causing its miss rate to diminish slower as the
cache size grows. As a result, the peak performance of a Chip Multiprocessor
(CMP) for workloads with high degree of data sharing is achieved with a smaller
private cache, compared to workloads with no data sharing. The CMP performance
can be improved by reassigning some of the constrained area or power resource
from private cache to core. Alternatively, the area or power budget of a CMP
can be reduced without a performance hit.
</summary>
    <author>
      <name>Leonid Yavits</name>
    </author>
    <author>
      <name>Amir Morad</name>
    </author>
    <author>
      <name>Ran Ginosar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.01329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01616v2</id>
    <updated>2016-08-29T06:24:25Z</updated>
    <published>2016-02-04T10:16:13Z</published>
    <title>FPGA Based Implementation of Deep Neural Networks Using On-chip Memory
  Only</title>
    <summary>  Deep neural networks (DNNs) demand a very large amount of computation and
weight storage, and thus efficient implementation using special purpose
hardware is highly desired. In this work, we have developed an FPGA based
fixed-point DNN system using only on-chip memory not to access external DRAM.
The execution time and energy consumption of the developed system is compared
with a GPU based implementation. Since the capacity of memory in FPGA is
limited, only 3-bit weights are used for this implementation, and training
based fixed-point weight optimization is employed. The implementation using
Xilinx XC7Z045 is tested for the MNIST handwritten digit recognition benchmark
and a phoneme recognition task on TIMIT corpus. The obtained speed is about one
quarter of a GPU based implementation and much better than that of a PC based
one. The power consumption is less than 5 Watt at the full speed operation
resulting in much higher efficiency compared to GPU based systems.
</summary>
    <author>
      <name>Jinhwan Park</name>
    </author>
    <author>
      <name>Wonyong Sung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in ICASSP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.01616v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01616v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.02517v1</id>
    <updated>2016-02-08T10:28:44Z</updated>
    <published>2016-02-08T10:28:44Z</published>
    <title>Energy Efficient Video Fusion with Heterogeneous CPU-FPGA Devices</title>
    <summary>  This paper presents a complete video fusion system with hardware acceleration
and investigates the energy trade-offs between computing in the CPU or the FPGA
device. The video fusion application is based on the Dual-Tree Complex Wavelet
Transforms (DT-CWT). In this work the transforms are mapped to a hardware
accelerator using high-level synthesis tools for the FPGA and also vectorized
code for the single instruction multiple data (SIMD) engine available in the
CPU. The accelerated system reduces computation time and energy by a factor of
2. Moreover, the results show a key finding that the FPGA is not always the
best choice for acceleration, and the SIMD engine should be selected when the
wavelet decomposition reduces the frame size below a certain threshold. This
dependency on workload size means that an adaptive system that intelligently
selects between the SIMD engine and the FPGA achieves the most energy and
performance efficiency point.
</summary>
    <author>
      <name>Jose Nunez-Yanez</name>
    </author>
    <author>
      <name>Tom Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at HIP3ES, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.02517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.02517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.03016v1</id>
    <updated>2016-02-09T15:04:11Z</updated>
    <published>2016-02-09T15:04:11Z</published>
    <title>FPGA Hardware Acceleration of Monte Carlo Simulations for the Ising
  Model</title>
    <summary>  A two-dimensional Ising model with nearest-neighbors ferromagnetic
interactions is implemented in a Field Programmable Gate Array (FPGA)
board.Extensive Monte Carlo simulations were carried out using an efficient
hardware representation of individual spins and a combined global-local LFSR
random number generator. Consistent results regarding the descriptive
properties of magnetic systems, like energy, magnetization and susceptibility
are obtained while a speed-up factor of approximately 6 times is achieved in
comparison to previous FPGA-based published works and almost $10^4$ times in
comparison to a standard CPU simulation. A detailed description of the logic
design used is given together with a careful analysis of the quality of the
random number generator used. The obtained results confirm the potential of
FPGAs for analyzing the statistical mechanics of magnetic systems.
</summary>
    <author>
      <name>Francisco Ortega-Zamorano</name>
    </author>
    <author>
      <name>Marcelo A. Montemurro</name>
    </author>
    <author>
      <name>Sergio A. Cannas</name>
    </author>
    <author>
      <name>José M. Jerez</name>
    </author>
    <author>
      <name>Leonardo Franco</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPDS.2015.2505725</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPDS.2015.2505725" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.03016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.03016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.03095v1</id>
    <updated>2016-02-09T17:59:24Z</updated>
    <published>2016-02-09T17:59:24Z</published>
    <title>OpenRISC System-on-Chip Design Emulation</title>
    <summary>  Recently the hardware emulation technique has emerged as a promising approach
to accelerating hardware verification/debugging process. To fully evaluate the
powerfulness of the emulation approach and demonstrate its potential impact, we
propose to emulate a system-on-chip (SoC) design using Mentor Graphics Veloce
emulation platform. This article presents our project setup and the results we
have achieved. The results are encouraging. ORPSoC emulation with Veloce has
more than ten times faster than hardware simulation. Our experimental results
demonstrate that Mentor Graphics Veloce has major advantages in emulation,
verification, and debugging of complicated real hardware designs, especially in
the context of SoC complexity. Through our three major tasks, we will
demonstrate that (1) Veloce can successfully emulate large-scale SoC designs;
(2) it has much better performance comparing to the state-of-the-art simulation
tools; (3) it can significantly accelerate the process of hardware verification
and debugging while maintaining full signal visibility.
</summary>
    <author>
      <name>Kai Cong</name>
    </author>
    <author>
      <name>Li Lei</name>
    </author>
    <author>
      <name>Zhenkun Yang</name>
    </author>
    <author>
      <name>Fei Xie</name>
    </author>
    <link href="http://arxiv.org/abs/1602.03095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.03095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04094v1</id>
    <updated>2016-03-10T06:32:36Z</updated>
    <published>2016-03-10T06:32:36Z</published>
    <title>Design and Implementation of an Improved Carry Increment Adder</title>
    <summary>  A complex digital circuit comprises of adder as a basic unit. The performance
of the circuit depends on the design of this basic adder unit. The speed of
operation of a circuit is one of the important performance criteria of many
digital circuits which ultimately depends on the delay of the basic adder unit.
Many research works have been devoted in improving the delay of the adder
circuit. In this paper we have proposed an improved carry increment adder (CIA)
that improves the delay performance of the circuit. The improvement is achieved
by incorporating carry look adder (CLA) in the design of CIA contrary to the
previous design of CIA that employs ripple carry adder (RCA). A simulation
study is carried out for comparative analysis. The coding is done in Verilog
hardware description language (HDL) and the simulation is carried out in Xilinx
ISE 13.1 environment.
</summary>
    <author>
      <name>Aribam Balarampyari Devi</name>
    </author>
    <author>
      <name>Manoj Kumar</name>
    </author>
    <author>
      <name>Romesh Laishram</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">vol.7,No.1,February 2016, pp 21-27</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.04094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07962v1</id>
    <updated>2016-03-25T17:11:33Z</updated>
    <published>2016-03-25T17:11:33Z</published>
    <title>Global versus Local Weak-Indication Self-Timed Function Blocks - A
  Comparative Analysis</title>
    <summary>  This paper analyzes the merits and demerits of global weak-indication
self-timed function blocks versus local weak-indication self-timed function
blocks, implemented using a delay-insensitive data code and adhering to 4-phase
return-to-zero handshaking. A self-timed ripple carry adder is considered as an
example function block for the analysis. The analysis shows that while global
weak-indication could help in optimizing the power, latency and area
parameters, local weak-indication facilitates the optimum performance in terms
of realizing the data-dependent cycle time that is characteristic of a
weak-indication self-timed design.
</summary>
    <author>
      <name>P Balasubramanian</name>
    </author>
    <author>
      <name>N E Mastorakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in the Book, Recent Advances in Circuits, Systems, Signal Processing
  and Communications, Included in ISI/SCI Web of Science and Web of Knowledge,
  Proceedings of 10th International Conference on Circuits, Systems, Signal and
  Telecommunications, pp. 86-97, 2016, Barcelona, Spain</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.07962v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07962v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07964v1</id>
    <updated>2016-03-25T17:14:55Z</updated>
    <published>2016-03-25T17:14:55Z</published>
    <title>Power, Delay and Area Comparisons of Majority Voters relevant to TMR
  Architectures</title>
    <summary>  N-modular redundancy (NMR) is commonly used to enhance the fault tolerance of
a circuit/system, when subject to a fault-inducing environment such as in space
or military systems, where upsets due to radiation phenomena, temperature
and/or other environmental conditions are anticipated. Triple Modular
Redundancy (TMR), which is a 3-tuple version of NMR, is widely preferred for
mission-control space, military, and aerospace, and safety-critical nuclear,
power, medical, and industrial control and automation systems. The TMR scheme
involves the two-times duplication of a simplex system hardware, with a
majority voter ensuring correctness provided at least two out of three copies
of the hardware remain operational. Thus the majority voter plays a pivotal
role in ensuring the correct operation of the TMR scheme. In this paper, a
number of standard-cell based majority voter designs relevant to TMR
architectures are presented, and their power, delay and area parameters are
estimated based on physical realization using a 32/28nm CMOS process.
</summary>
    <author>
      <name>P Balasubramanian</name>
    </author>
    <author>
      <name>N E Mastorakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in the Book, Recent Advances in Circuits, Systems, Signal Processing
  and Communications, Included in ISI/SCI Web of Science and Web of Knowledge,
  Proceedings of 10th International Conference on Circuits, Systems, Signal and
  Telecommunications, pp. 110-117, 2016, Barcelona, Spain</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.07964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09062v1</id>
    <updated>2016-03-30T07:44:23Z</updated>
    <published>2016-03-30T07:44:23Z</published>
    <title>FPGA Impementation of Erasure-Only Reed Solomon Decoders for Hybrid-ARQ
  Systems</title>
    <summary>  This paper presents the usage of the Reed Solomon Codes as the Forward Error
Correction (FEC) unit of the Hybrid Automatic Repeat Request (ARQ) methods.
Parametric and flexible FPGA implementation details of such Erasure-Only RS
decoders with high symbol lengths (e.g. GF(2^32)) have been presented. The
design is based on the GF(2m) multiplier logic core operating at a single clock
cycle, where the resource utilization and throughput are both directly
proportional to the number of these cores. For a fixed implementation, the
throughput inversely decreases with the number of erasures to be corrected.
Implementation in Zynq7020 SoC device of an example GF(2^32)-RS Decoder capable
of correcting 64-erasures with a single multiplier resulted in 1641-LUTs and
188-FFs achieving 15Mbps, whereas the design with 8 multipliers resulted in
6128-LUTs and 628-FFs achieving 100Mbps.
</summary>
    <author>
      <name>Cansu Sen</name>
    </author>
    <author>
      <name>Soner Yesil</name>
    </author>
    <author>
      <name>Ertugrul Kolagasioglu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Turkish</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04582v2</id>
    <updated>2016-05-23T04:53:45Z</updated>
    <published>2016-05-15T17:52:28Z</published>
    <title>A Foray into Efficient Mapping of Algorithms to Hardware Platforms on
  Heterogeneous Systems</title>
    <summary>  Heterogeneous computing can potentially offer significant performance and
performance per watt improvements over homogeneous computing, but the question
"what is the ideal mapping of algorithms to architectures?" remains an open
one. In the past couple of years new types of computing devices such as FPGAs
have come into general computing use. In this work we attempt to add to the
body of scientific knowledge by comparing Kernel performance and performance
per watt of seven key algorithms according to Berkley's dwarf taxonomy. We do
so using the Rodinia benchmark suite on three different high-end hardware
architecture representatives from the CPU, GPU and FPGA families. We find
results that support some distinct mappings between the architecture and
performance per watt. Perhaps the most interesting finding is that, for our
specific hardware representatives, FPGAs should be considered as alternatives
to GPUs and CPUs in several key algorithms: N-body simulations, dense linear
algebra and structured grid.
</summary>
    <author>
      <name>Oren Segal</name>
    </author>
    <author>
      <name>Nasibeh Nasiri</name>
    </author>
    <author>
      <name>Martin Margala</name>
    </author>
    <link href="http://arxiv.org/abs/1605.04582v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04582v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01607v1</id>
    <updated>2016-06-06T03:44:52Z</updated>
    <published>2016-06-06T03:44:52Z</published>
    <title>CG-OoO: Energy-Efficient Coarse-Grain Out-of-Order Execution</title>
    <summary>  We introduce the Coarse-Grain Out-of-Order (CG- OoO) general purpose
processor designed to achieve close to In-Order processor energy while
maintaining Out-of-Order (OoO) performance. CG-OoO is an energy-performance
proportional general purpose architecture that scales according to the program
load. Block-level code processing is at the heart of the this architecture;
CG-OoO speculates, fetches, schedules, and commits code at block-level
granularity. It eliminates unnecessary accesses to energy consuming tables, and
turns large tables into smaller and distributed tables that are cheaper to
access. CG-OoO leverages compiler-level code optimizations to deliver efficient
static code, and exploits dynamic instruction-level parallelism and block-level
parallelism. CG-OoO introduces Skipahead issue, a complexity effective, limited
out-of-order instruction scheduling model. Through the energy efficiency
techniques applied to the compiler and processor pipeline stages, CG-OoO closes
64% of the average energy gap between the In-Order and Out-of-Order baseline
processors at the performance of the OoO baseline. This makes CG-OoO 1.9x more
efficient than the OoO on the energy-delay product inverse metric.
</summary>
    <author>
      <name>Milad Mohammadi</name>
    </author>
    <author>
      <name>Tor M. Aamodt</name>
    </author>
    <author>
      <name>William J. Dally</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.01607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03248v1</id>
    <updated>2016-06-10T09:47:14Z</updated>
    <published>2016-06-10T09:47:14Z</published>
    <title>MAC: a novel systematically multilevel cache replacement policy for PCM
  memory</title>
    <summary>  The rapid development of multi-core system and increase of data-intensive
application in recent years call for larger main memory. Traditional DRAM
memory can increase its capacity by reducing the feature size of storage cell.
Now further scaling of DRAM faces great challenge, and the frequent refresh
operations of DRAM can bring a lot of energy consumption. As an emerging
technology, Phase Change Memory (PCM) is promising to be used as main memory.
It draws wide attention due to the advantages of low power consumption, high
density and nonvolatility, while it incurs finite endurance and relatively long
write latency. To handle the problem of write, optimizing the cache replacement
policy to protect dirty cache block is an efficient way. In this paper, we
construct a systematically multilevel structure, and based on it propose a
novel cache replacement policy called MAC. MAC can effectively reduce write
traffic to PCM memory with low hardware overhead. We conduct simulation
experiments on GEM5 to evaluate the performances of MAC and other related
works. The results show that MAC performs best in reducing the amount of writes
(averagely 25.12%) without increasing the program execution time.
</summary>
    <author>
      <name>Shenchen Ruan</name>
    </author>
    <author>
      <name>Haixia Wang</name>
    </author>
    <author>
      <name>Dongsheng Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04609v1</id>
    <updated>2016-06-15T01:26:24Z</updated>
    <published>2016-06-15T01:26:24Z</published>
    <title>High Throughput Neural Network based Embedded Streaming Multicore
  Processors</title>
    <summary>  With power consumption becoming a critical processor design issue,
specialized architectures for low power processing are becoming popular.
Several studies have shown that neural networks can be used for signal
processing and pattern recognition applications. This study examines the design
of memristor based multicore neural processors that would be used primarily to
process data directly from sensors. Additionally, we have examined the design
of SRAM based neural processors for the same task. Full system evaluation of
the multicore processors based on these specialized cores were performed taking
I/O and routing circuits into consideration. The area and power benefits were
compared with traditional multicore RISC processors. Our results show that the
memristor based architectures can provide an energy efficiency between three
and five orders of magnitude greater than that of RISC processors for the
benchmarks examined.
</summary>
    <author>
      <name>Raqibul Hasan</name>
    </author>
    <author>
      <name>Tarek M. Taha</name>
    </author>
    <author>
      <name>Chris Yakopcic</name>
    </author>
    <author>
      <name>David J. Mountain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. arXiv admin note: text overlap with arXiv:1603.07400</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05933v1</id>
    <updated>2016-06-20T00:57:30Z</updated>
    <published>2016-06-20T00:57:30Z</published>
    <title>Criticality Aware Multiprocessors</title>
    <summary>  Typically, a memory request from a processor may need to go through many
intermediate interconnect routers, directory node, owner node, etc before it is
finally serviced. Current multiprocessors do not give preference to any
particular memory request. But certain memory requests are more critical to
multiprocessor's performance than other requests. Example: memory requests from
critical sections, load request feeding into multiple dependent instructions,
etc. This knowledge can be used to improve the performance of current
multiprocessors by letting the ordering point and the interconnect routers
prioritize critical requests over non-critical ones. In this paper, we evaluate
using SIMICS/GEMS infrastructure. For lock-intensive microbenchmarks,
criticality-aware multiprocessors showed 5-15% performance improvement over
baseline multiprocessor. Criticality aware multiprocessor provides a new
direction for tapping performance in a shared memory multiprocessor and can
provide substantial speedup in lock intensive benchmarks.
</summary>
    <author>
      <name>Sandeep Navada</name>
    </author>
    <author>
      <name>Anil Krishna</name>
    </author>
    <link href="http://arxiv.org/abs/1606.05933v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05933v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06451v1</id>
    <updated>2016-06-21T07:22:12Z</updated>
    <published>2016-06-21T07:22:12Z</published>
    <title>High Level Synthesis with a Dataflow Architectural Template</title>
    <summary>  In this work, we present a new approach to high level synthesis (HLS), where
high level functions are first mapped to an architectural template, before
hardware synthesis is performed. As FPGA platforms are especially suitable for
implementing streaming processing pipelines, we perform transformations on
conventional high level programs where they are turned into multi-stage
dataflow engines [1]. This target template naturally overlaps slow memory data
accesses with computations and therefore has much better tolerance towards
memory subsystem latency. Using a state-of-the-art HLS tool for the actual
circuit generation, we observe up to 9x improvement in overall performance when
the dataflow architectural template is used as an intermediate compilation
target.
</summary>
    <author>
      <name>Shaoyi Cheng</name>
    </author>
    <author>
      <name>John Wawrzynek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 2nd International Workshop on Overlay Architectures for
  FPGAs (OLAF 2016) arXiv:1605.08149</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06454v1</id>
    <updated>2016-06-21T07:33:43Z</updated>
    <published>2016-06-21T07:33:43Z</published>
    <title>Soft GPGPUs for Embedded FPGAs: An Architectural Evaluation</title>
    <summary>  We present a customizable soft architecture which allows for the execution of
GPGPU code on an FPGA without the need to recompile the design. Issues related
to scaling the overlay architecture to multiple GPGPU multiprocessors are
considered along with application-class architectural optimizations. The
overlay architecture is optimized for FPGA implementation to support efficient
use of embedded block memories and DSP blocks. This architecture supports
direct CUDA compilation of integer computations to a binary which is executable
on the FPGA-based GPGPU. The benefits of our architecture are evaluated for a
collection of five standard CUDA benchmarks which are compiled using standard
GPGPU compilation tools. Speedups of 44x, on average, versus a MicroBlaze
microprocessor are achieved. We show dynamic energy savings versus a soft-core
processor of 80% on average. Application-customized versions of the soft GPGPU
can be used to further reduce dynamic energy consumption by an average of 14%.
</summary>
    <author>
      <name>Kevin Andryc</name>
    </author>
    <author>
      <name>Tedy Thomas</name>
    </author>
    <author>
      <name>Russell Tessier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 2nd International Workshop on Overlay Architectures for
  FPGAs (OLAF 2016) arXiv:1605.08149</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06454v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06454v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06457v1</id>
    <updated>2016-06-21T07:43:55Z</updated>
    <published>2016-06-21T07:43:55Z</published>
    <title>Enabling Effective FPGA Debug using Overlays: Opportunities and
  Challenges</title>
    <summary>  FPGAs are going mainstream. Major companies that were not traditionally
FPGA-focused are now seeking ways to exploit the benefits of reconfigurable
technology and provide it to their customers. In order to do so, a debug
ecosystem that provides for effective visibility into a working design and
quick debug turn-around times is essential. Overlays have the opportunity to
play a key role in this ecosystem. In this overview paper, we discuss how an
overlay fabric that allows the user to rapidly add debug instrumentation to a
design can be created and exploited. We discuss the requirements of such an
overlay and some of the research challenges and opportunities that need to be
addressed. To make our exposition concrete, we use two previously-published
examples of overlays that have been developed to implement debug
instrumentation.
</summary>
    <author>
      <name>Fatemeh Eslami</name>
    </author>
    <author>
      <name>Eddie Hung</name>
    </author>
    <author>
      <name>Steven J. E. Wilton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 2nd International Workshop on Overlay Architectures for
  FPGAs (OLAF 2016) arXiv:1605.08149</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06457v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06457v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06460v1</id>
    <updated>2016-06-21T07:52:00Z</updated>
    <published>2016-06-21T07:52:00Z</published>
    <title>An Area-Efficient FPGA Overlay using DSP Block based Time-multiplexed
  Functional Units</title>
    <summary>  Coarse grained overlay architectures improve FPGA design productivity by
providing fast compilation and software-like programmability. Throughput
oriented spatially configurable overlays typically suffer from area overheads
due to the requirement of one functional unit for each compute kernel
operation. Hence, these overlays have often been of limited size, supporting
only relatively small compute kernels while consuming considerable FPGA
resources. This paper examines the possibility of sharing the functional units
among kernel operations for reducing area overheads. We propose a linear
interconnected array of time-multiplexed FUs as an overlay architecture with
reduced instruction storage and interconnect resource requirements, which uses
a fully-pipelined, architecture-aware FU design supporting a fast context
switching time. The results presented show a reduction of up to 85% in FPGA
resource requirements compared to existing throughput oriented overlay
architectures, with an operating frequency which approaches the theoretical
limit for the FPGA device.
</summary>
    <author>
      <name>Xiangwei Li</name>
    </author>
    <author>
      <name>Abhishek Jain</name>
    </author>
    <author>
      <name>Douglas Maskell</name>
    </author>
    <author>
      <name>Suhaib A. Fahmy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 2nd International Workshop on Overlay Architectures for
  FPGAs (OLAF 2016) arXiv:1605.08149</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08686v1</id>
    <updated>2016-06-28T13:19:10Z</updated>
    <published>2016-06-28T13:19:10Z</published>
    <title>A Benes Based NoC Switching Architecture for Mixed Criticality Embedded
  Systems</title>
    <summary>  Multi-core, Mixed Criticality Embedded (MCE) real-time systems require high
timing precision and predictability to guarantee there will be no interference
between tasks. These guarantees are necessary in application areas such as
avionics and automotive, where task interference or missed deadlines could be
catastrophic, and safety requirements are strict. In modern multi-core systems,
the interconnect becomes a potential point of uncertainty, introducing major
challenges in proving behaviour is always within specified constraints,
limiting the means of growing system performance to add more tasks, or provide
more computational resources to existing tasks.
  We present MCENoC, a Network-on-Chip (NoC) switching architecture that
provides innovations to overcome this with predictable, formally verifiable
timing behaviour that is consistent across the whole NoC. We show how the
fundamental properties of Benes networks benefit MCE applications and meet our
architecture requirements. Using SystemVerilog Assertions (SVA), formal
properties are defined that aid the refinement of the specification of the
design as well as enabling the implementation to be exhaustively formally
verified. We demonstrate the performance of the design in terms of size,
throughput and predictability, and discuss the application level considerations
needed to exploit this architecture.
</summary>
    <author>
      <name>Steve Kerrison</name>
    </author>
    <author>
      <name>David May</name>
    </author>
    <author>
      <name>Kerstin Eder</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00064v2</id>
    <updated>2018-04-12T05:36:34Z</updated>
    <published>2016-06-30T22:10:13Z</published>
    <title>Maximizing CNN Accelerator Efficiency Through Resource Partitioning</title>
    <summary>  Convolutional neural networks (CNNs) are revolutionizing machine learning,
but they present significant computational challenges. Recently, many
FPGA-based accelerators have been proposed to improve the performance and
efficiency of CNNs. Current approaches construct a single processor that
computes the CNN layers one at a time; the processor is optimized to maximize
the throughput at which the collection of layers is computed. However, this
approach leads to inefficient designs because the same processor structure is
used to compute CNN layers of radically varying dimensions.
  We present a new CNN accelerator paradigm and an accompanying automated
design methodology that partitions the available FPGA resources into multiple
processors, each of which is tailored for a different subset of the CNN
convolutional layers. Using the same FPGA resources as a single large
processor, multiple smaller specialized processors increase computational
efficiency and lead to a higher overall throughput. Our design methodology
achieves 3.8x higher throughput than the state-of-the-art approach on
evaluating the popular AlexNet CNN on a Xilinx Virtex-7 FPGA. For the more
recent SqueezeNet and GoogLeNet, the speedups are 2.2x and 2.0x.
</summary>
    <author>
      <name>Yongming Shen</name>
    </author>
    <author>
      <name>Michael Ferdman</name>
    </author>
    <author>
      <name>Peter Milder</name>
    </author>
    <link href="http://arxiv.org/abs/1607.00064v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00064v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07766v2</id>
    <updated>2016-07-27T10:47:17Z</updated>
    <published>2016-07-26T15:53:54Z</published>
    <title>Uber: Utilizing Buffers to Simplify NoCs for Hundreds-Cores</title>
    <summary>  Approaching ideal wire latency using a network-on-chip (NoC) is an important
practical problem for many-core systems, particularly hundreds-cores. Although
other researchers have focused on optimizing large meshes, bypassing or
speculating router pipelines, or creating more intricate logarithmic
topologies, this paper proposes a balanced combination that trades queue
buffers for simplicity. Preliminary analysis of nine benchmarks from PARSEC and
SPLASH using execution-driven simulation shows that utilization rises from 2%
when connecting a single core per mesh port to at least 50%, as slack for delay
in concentrator and router queues is around 6x higher compared to the ideal
latency of just 20 cycles. That is, a 16-port mesh suffices because queueing is
the uncommon case for system performance. In this way, the mesh hop count is
bounded to three, as load becomes uniform via extended concentration, and ideal
latency is approached using conventional four-stage pipelines for the mesh
routers together with minor logarithmic edges. A realistic Uber is also
detailed, featuring the same performance as a 64-port mesh that employs
optimized router pipelines, improving the baseline by 12%. Ongoing work
develops techniques to better balance load by tuning the placement of cache
blocks, and compares Uber with bufferless routing.
</summary>
    <author>
      <name>Giorgos Passas</name>
    </author>
    <link href="http://arxiv.org/abs/1607.07766v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07766v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08523v1</id>
    <updated>2016-07-28T16:33:31Z</updated>
    <published>2016-07-28T16:33:31Z</published>
    <title>The Study of Transient Faults Propagation in Multithread Applications</title>
    <summary>  Whereas contemporary Error Correcting Codes (ECC) designs occupy a
significant fraction of total die area in chip-multiprocessors (CMPs),
approaches to deal with the vulnerability increase of CMP architecture against
Single Event Upsets (SEUs) and Multi-Bit Upsets (MBUs) are sought. In this
paper, we focus on reliability assessment of multithreaded applications running
on CMPs to propose an adaptive application-relevant architecture design to
accommodate the impact of both SEUs and MBUs in the entire CMP architecture.
This work concentrates on leveraging the intrinsic soft-error-immunity feature
of Spin-Transfer Torque RAM (STT-RAM) as an alternative for SRAM-based storage
and operation components. We target a specific portion of working set for
reallocation to improve the reliability level of the CMP architecture design. A
selected portion of instructions in multithreaded program which experience high
rate of referencing with the lowest memory modification are ideal candidate to
be stored and executed in STT-RAM based components. We argue about why we
cannot use STT-RAM for the global storage and operation counterparts and
describe the obtained resiliency compared to the baseline setup. In addition, a
detail study of the impact of SEUs and MBUs on multithreaded programs will be
presented in the Appendix.
</summary>
    <author>
      <name>Navid Khoshavi</name>
    </author>
    <author>
      <name>Armin Samiei</name>
    </author>
    <link href="http://arxiv.org/abs/1607.08523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01225v1</id>
    <updated>2016-08-03T15:40:01Z</updated>
    <published>2016-08-03T15:40:01Z</published>
    <title>Early Output Hybrid Input Encoded Asynchronous Full Adder and
  Relative-Timed Ripple Carry Adder</title>
    <summary>  This paper presents a new early output hybrid input encoded asynchronous full
adder designed using dual-rail and 1-of-4 delay-insensitive data codes. The
proposed full adder when cascaded to form a ripple carry adder (RCA)
necessitates the use of a small relative-timing assumption with respect to the
internal carries, which is independent of the RCA size. The forward latency of
the proposed hybrid input encoded full adder based RCA is data-dependent while
its reverse latency is the least equaling the propagation delay of just one
full adder. Compared to the best of the existing hybrid input encoded full
adders based 32-bit RCAs, the proposed early output hybrid input encoded full
adder based 32-bit RCA enables respective reductions in forward latency and
area by 7.9% and 5.6% whilst dissipating the same average power; in terms of
the theoretically computed cycle time, the latter reports a 10.9% reduction
compared to the former.
</summary>
    <author>
      <name>P Balasubramanian</name>
    </author>
    <author>
      <name>K Prasad</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 14th International Conference on Embedded
  Systems, Cyber-physical Systems, and Applications, pp. 62-65, July 25-28,
  2016, Las Vegas, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.01225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00306v1</id>
    <updated>2016-09-01T16:32:54Z</updated>
    <published>2016-09-01T16:32:54Z</published>
    <title>On-Chip Mechanisms to Reduce Effective Memory Access Latency</title>
    <summary>  This dissertation develops hardware that automatically reduces the effective
latency of accessing memory in both single-core and multi-core systems. To
accomplish this, the dissertation shows that all last level cache misses can be
separated into two categories: dependent cache misses and independent cache
misses. Independent cache misses have all of the source data that is required
to generate the address of the memory access available on-chip, while dependent
cache misses depend on data that is located off-chip. This dissertation
proposes that dependent cache misses are accelerated by migrating the
dependence chain that generates the address of the memory access to the memory
controller for execution. Independent cache misses are accelerated using a new
mode for runahead execution that only executes filtered dependence chains. With
these mechanisms, this dissertation demonstrates a 62% increase in performance
and a 19% decrease in effective memory access latency for a quad-core processor
on a set of high memory intensity workloads.
</summary>
    <author>
      <name>Milad Hashemi</name>
    </author>
    <link href="http://arxiv.org/abs/1609.00306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03897v3</id>
    <updated>2017-03-10T16:15:21Z</updated>
    <published>2016-09-13T15:33:06Z</published>
    <title>Design of a Ternary Edge-Triggered D Flip-Flap-Flop for Multiple-Valued
  Sequential Logic</title>
    <summary>  Development of large computerized systems requires both combinational and
sequential circuits. Registers and counters are two important examples of
sequential circuits, which are widely used in practical applications like CPUs.
The basic element of sequential logic is Flip-Flop, which stores an input value
and returns two outputs (Q and Q_bar). This paper presents an innovative
ternary D Flip-Flap-Flop, which offers circuit designers to customize their
design by eliminating one of the outputs if it is not required. This unique
feature of the new design leads to considerable power reduction in comparison
with the previously presented structures. The proposed design is simulated and
tested by HSPICE and 45 nm CMOS technology.
</summary>
    <author>
      <name>Reza Faghih Mirzaee</name>
    </author>
    <author>
      <name>Niloofar Farahani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages, 7 Figures, 5 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03897v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03897v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.3; B.6.1; B.7.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04913v1</id>
    <updated>2016-09-16T06:13:37Z</updated>
    <published>2016-09-16T06:13:37Z</published>
    <title>Design of an Optoelectronic State Machine with integrated BDD based
  Optical logic</title>
    <summary>  In this paper I demonstrate a novel design for an optoelectronic State
Machine which replaces input/output forming logic found in conventional state
machines with BDD based optical logic while still using solid state memory in
the form of flip-flops in order to store states. This type of logic makes use
of waveguides and ring resonators to create binary switches. These switches in
turn can be used to create combinational logic which can be used as
input/output forming logic for a state machine. Replacing conventional
combinational logic with BDD based optical logic allows for a faster range of
state machines that can certainly outperform conventional state machines as
propagation delays within the logic described are in the order of picoseconds
as opposed to nanoseconds in digital logic.
</summary>
    <author>
      <name>Macauley Coggins</name>
    </author>
    <link href="http://arxiv.org/abs/1609.04913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07234v1</id>
    <updated>2016-09-23T05:31:18Z</updated>
    <published>2016-09-23T05:31:18Z</published>
    <title>Reducing DRAM Access Latency by Exploiting DRAM Leakage Characteristics
  and Common Access Patterns</title>
    <summary>  DRAM-based memory is a critical factor that creates a bottleneck on the
system performance since the processor speed largely outperforms the DRAM
latency. In this thesis, we develop a low-cost mechanism, called ChargeCache,
which enables faster access to recently-accessed rows in DRAM, with no
modifications to DRAM chips. Our mechanism is based on the key observation that
a recently-accessed row has more charge and thus the following access to the
same row can be performed faster. To exploit this observation, we propose to
track the addresses of recently-accessed rows in a table in the memory
controller. If a later DRAM request hits in that table, the memory controller
uses lower timing parameters, leading to reduced DRAM latency. Row addresses
are removed from the table after a specified duration to ensure rows that have
leaked too much charge are not accessed with lower latency. We evaluate
ChargeCache on a wide variety of workloads and show that it provides
significant performance and energy benefits for both single-core and multi-core
systems.
</summary>
    <author>
      <name>Hasan Hassan</name>
    </author>
    <link href="http://arxiv.org/abs/1609.07234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08681v1</id>
    <updated>2016-09-27T21:56:17Z</updated>
    <published>2016-09-27T21:56:17Z</published>
    <title>Multi-Valued Routing Tracks for FPGAs in 28nm FDSOI Technology</title>
    <summary>  In this paper we present quaternary and ternary routing tracks for FPGAs, and
their implementation in 28nm FDSOI technology. We discuss the transistor level
design of multi-valued repeaters, multiplexers and translators, and specific
features of FDSOI technology which make it possible. Next we compare the
multi-valued routing architectures with equivalent single driver two-valued
routing architectures. We show that for long tracks, it is possible to achieve
upto 3x reduction in dynamic switching energy, upto 2x reduction in routing
wire area and 10% reduction in area dedicated to routing resources. The
multi-valued tracks are slightly more susceptible to process variation. We
present a layout method for multivalued standard cells and determine the layout
overhead.We conclude with various usage scenarios of these tracks.
</summary>
    <author>
      <name>Sumanta Chaudhuri</name>
    </author>
    <author>
      <name>Tarik Graba</name>
    </author>
    <author>
      <name>Yves Mathieu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper contains anonymous reviews at the end. 11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.08681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09603v1</id>
    <updated>2016-10-30T05:01:54Z</updated>
    <published>2016-10-30T05:01:54Z</published>
    <title>The Processing Using Memory Paradigm:In-DRAM Bulk Copy, Initialization,
  Bitwise AND and OR</title>
    <summary>  In existing systems, the off-chip memory interface allows the memory
controller to perform only read or write operations. Therefore, to perform any
operation, the processor must first read the source data and then write the
result back to memory after performing the operation. This approach consumes
high latency, bandwidth, and energy for operations that work on a large amount
of data. Several works have proposed techniques to process data near memory by
adding a small amount of compute logic closer to the main memory chips. In this
article, we describe two techniques proposed by recent works that take this
approach of processing in memory further by exploiting the underlying operation
of the main memory technology to perform more complex tasks. First, we describe
RowClone, a mechanism that exploits DRAM technology to perform bulk copy and
initialization operations completely inside main memory. We then describe a
complementary work that uses DRAM to perform bulk bitwise AND and OR operations
inside main memory. These two techniques significantly improve the performance
and energy efficiency of the respective operations.
</summary>
    <author>
      <name>Vivek Seshadri</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1605.06483</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.09603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02450v1</id>
    <updated>2016-11-08T09:43:03Z</updated>
    <published>2016-11-08T09:43:03Z</published>
    <title>PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution
  Neuron Networks</title>
    <summary>  Convolutional neural networks (CNNs) have been widely employed in many
applications such as image classification, video analysis and speech
recognition. Being compute-intensive, CNN computations are mainly accelerated
by GPUs with high power dissipations. Recently, studies were carried out
exploiting FPGA as CNN accelerator because of its reconfigurability and energy
efficiency advantage over GPU, especially when OpenCL-based high-level
synthesis tools are now available providing fast verification and
implementation flows. Previous OpenCL-based design only focused on creating a
generic framework to identify performance-related hardware parameters, without
utilizing FPGA's special capability of pipelining kernel functions to minimize
memory bandwidth requirement. In this work, we propose an FPGA accelerator with
a new architecture of deeply pipelined OpenCL kernels. Data reuse and task
mapping techniques are also presented to improve design efficiency. The
proposed schemes are verified by implementing two representative large-scale
CNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar
peak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks
compared to previous work. Our design is openly accessible and thus can be
reused to explore new architectures for neural network accelerators.
</summary>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Jianjing An</name>
    </author>
    <author>
      <name>Ke Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First Draft</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.02450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02792v1</id>
    <updated>2016-11-09T01:25:59Z</updated>
    <published>2016-11-09T01:25:59Z</published>
    <title>Non-volatile Hierarchical Temporal Memory: Hardware for Spatial Pooling</title>
    <summary>  Hierarchical Temporal Memory (HTM) is a biomimetic machine learning algorithm
imbibing the structural and algorithmic properties of the neocortex. Two main
functional components of HTM that enable spatio-temporal processing are the
spatial pooler and temporal memory. In this research, we explore a scalable
hardware realization of the spatial pooler closely coupled with the
mathematical formulation of spatial pooler. This class of neuromorphic
algorithms are advantageous in solving a subset of the future engineering
problems by extracting nonintuitive patterns in complex data. The proposed
architecture, Non-volatile HTM (NVHTM), leverages large-scale solid state flash
memory to realize a optimal memory organization, area and power envelope. A
behavioral model of NVHTM is evaluated against the MNIST dataset, yielding
91.98% classification accuracy. A full custom layout is developed to validate
the design in a TSMC 180nm process. The area and power profile of the spatial
pooler are 30.538mm2 and 64.394mW, respectively. This design is a
proof-of-concept that storage processing is a viable platform for large scale
HTM network models.
</summary>
    <author>
      <name>Lennard Streat</name>
    </author>
    <author>
      <name>Dhireesha Kudithipudi</name>
    </author>
    <author>
      <name>Kevin Gomez</name>
    </author>
    <link href="http://arxiv.org/abs/1611.02792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.02915v1</id>
    <updated>2015-10-17T04:27:53Z</updated>
    <published>2015-10-17T04:27:53Z</published>
    <title>Power Gating Structure for Reversible Programmable Logic Array</title>
    <summary>  Throughout the world, the numbers of researchers or hardware designer
struggle for the reducing of power dissipation in low power VLSI systems. This
paper presented an idea of using the power gating structure for reducing the
sub threshold leakage in the reversible system. This concept presented in the
paper is entirely new and presented in the literature of reversible logics. By
using the reversible logics for the digital systems, the energy can be saved up
to the gate level implementation. But at the physical level designing of the
reversible logics by the modern CMOS technology the heat or energy is
dissipated due the sub-threshold leakage at the time of inactivity or standby
mode. The Reversible Programming logic array (RPLA) is one of the important
parts of the low power industrial applications and in this paper the physical
design of the RPLA is presented by using the sleep transistor and the results
is shown with the help of TINA- PRO software. The results for the proposed
design is also compare with the CMOS design and shown that of 40.8% of energy
saving. The Transient response is also produces in the paper for the switching
activity and showing that the proposed design is much better that the modern
CMOS design of the RPLA.
</summary>
    <author>
      <name>Pradeep Singla</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14810/ecij.2015.4301</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14810/ecij.2015.4301" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 Pages, 9 figures, Electrical and Computer Engineering:
  International Journal, September 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.02915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.02915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.05415v1</id>
    <updated>2016-11-16T19:39:08Z</updated>
    <published>2016-11-16T19:39:08Z</published>
    <title>Multipliers: comparison of Fourier transformation based method and
  Synopsys design technique for up to 32 bits inputs in regular and saturation
  arithmetics</title>
    <summary>  The technique for hardware multiplication based upon Fourier transformation
has been introduced. The technique has the highest efficiency on multiplication
units with up to 8 bit range. Each multiplication unit is realized on base of
the minimized Boolean functions. Experimental data showed that this technique
the multiplication process speed up to 20% higher for 2-8 bit range of input
operands and up to 3% higher for 8-32 bit range of input operands than
analogues designed by Synopsys technique.
</summary>
    <author>
      <name>Danila Gorodecky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 12th International Workshop on Boolean Problems,
  Freiberg, Germany, Sept. 22-23, 2016. Edited by B. Steinbach. Freiberg
  University of Mining and Technology. P. 145-150. Changed title from
  "Multipliers design technique based disjunctive normal form minimizationa and
  Fourier transformation"; fixed some inaccuracies</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.05415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.05415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09452v2</id>
    <updated>2017-03-07T04:16:26Z</updated>
    <published>2016-11-29T01:28:58Z</published>
    <title>An Efficient Partial Sums Generator for Constituent Code based
  Successive Cancellation Decoding of Polar Codes</title>
    <summary>  This paper proposes the architecture of partial sum generator for constituent
codes based polar code decoder. Constituent codes based polar code decoder has
the advantage of low latency. However, no purposefully designed partial sum
generator design exists that can yield desired timing for the decoder. We first
derive the mathematical presentation with the partial sums set $\bm{\beta^c}$
which is corresponding to each constituent codes. From this, we concoct a
shift-register based partial sum generator. Next, the overall architecture and
design details are described, and the overhead compared with conventional
partial sum generator is evaluated. Finally, the implementation results with
both ASIC and FPGA technology and relevant discussions are presented.
</summary>
    <author>
      <name>Tiben Che</name>
    </author>
    <author>
      <name>Gwan Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to TCAS II</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.09452v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09452v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.10316v1</id>
    <updated>2016-11-30T19:09:07Z</updated>
    <published>2016-11-30T19:09:07Z</published>
    <title>Memory Controller Design Under Cloud Workloads</title>
    <summary>  This work studies the behavior of state-of-the-art memory controller designs
when executing scale-out workloads. It considers memory scheduling techniques,
memory page management policies, the number of memory channels, and the address
mapping scheme used. Experimental measurements demonstrate: 1)~Several recently
proposed memory scheduling policies are not a good match for these scale-out
workloads. 2)~The relatively simple First-Ready-First-Come-First-Served
(FR-FCFS) policy performs consistently better, and 3)~for most of the studied
workloads, the even simpler First-Come-First-Served scheduling policy is within
1\% of FR-FCFS. 4)~Increasing the number of memory channels offers negligible
performance benefits, e.g., performance improves by 1.7\% on average for
4-channels vs. 1-channel. 5)~77\%-90\% of DRAM rows activations are accessed
only once before closure. These observation can guide future development and
optimization of memory controllers for scale-out workloads.
</summary>
    <author>
      <name>Mostafa Mahmoud</name>
    </author>
    <author>
      <name>Andreas Moshovos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IISWC.2016.7581279</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IISWC.2016.7581279" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2016 IEEE International Symposium on Workload Characterization
  (IISWC)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.10316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.10316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.03182v1</id>
    <updated>2016-12-09T21:02:13Z</updated>
    <published>2016-12-09T21:02:13Z</published>
    <title>Arch2030: A Vision of Computer Architecture Research over the Next 15
  Years</title>
    <summary>  Application trends, device technologies and the architecture of systems drive
progress in information technologies. However, the former engines of such
progress - Moore's Law and Dennard Scaling - are rapidly reaching the point of
diminishing returns. The time has come for the computing community to boldly
confront a new challenge: how to secure a foundational future for information
technology's continued progress. The computer architecture community engaged in
several visioning exercises over the years. Five years ago, we released a white
paper, 21st Century Computer Architecture, which influenced funding programs in
both academia and industry. More recently, the IEEE Rebooting Computing
Initiative explored the future of computing systems in the architecture,
device, and circuit domains. This report stems from an effort to continue this
dialogue, reach out to the applications and devices/circuits communities, and
understand their trends and vision. We aim to identify opportunities where
architecture research can bridge the gap between the application and device
domains.
</summary>
    <author>
      <name>Luis Ceze</name>
    </author>
    <author>
      <name>Mark D. Hill</name>
    </author>
    <author>
      <name>Thomas F. Wenisch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A Computing Community Consortium (CCC) white paper, 7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.03182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.03182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04855v1</id>
    <updated>2016-12-14T21:52:33Z</updated>
    <published>2016-12-14T21:52:33Z</published>
    <title>A 700uW 1GS/s 4-bit Folding-Flash ADC in 65nm CMOS for Wideband Wireless
  Communications</title>
    <summary>  We present the design of a low-power 4-bit 1GS/s folding-flash ADC with a
folding factor of two. The design of a new unbalanced double-tail dynamic
comparator affords an ultra-low power operation and a high dynamic range.
Unlike the conventional approaches, this design uses a fully matched input
stage, an unbalanced latch stage, and a two-clock operation scheme. A
combination of these features yields significant reduction of the kick-back
noise, while allowing the design flexibility for adjusting the trip points of
the comparators. As a result, the ADC achieves SNDR of 22.3 dB at 100MHz and
21.8 dB at 500MHz (i.e. the Nyquist frequency). The maximum INL and DNL are
about 0.2 LSB. The converter consumes about 700uW from a 1-V supply yielding a
figure of merit of 65fJ/conversion step. These attributes make the proposed
folding-flash ADC attractive for the next-generation wireless applications.
</summary>
    <author>
      <name>Bayan Nasri</name>
    </author>
    <author>
      <name>Sunit P. Sebastian</name>
    </author>
    <author>
      <name>Kae-Dyi You</name>
    </author>
    <author>
      <name>RamKumar RanjithKumar</name>
    </author>
    <author>
      <name>Davood Shahrjerdi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to the the IEEE International Symposium of Circuits and
  Systems (ISCAS), 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.04855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05166v1</id>
    <updated>2016-12-15T17:55:06Z</updated>
    <published>2016-12-15T17:55:06Z</published>
    <title>A Novel RTL ATPG Model Based on Gate Inherent Faults (GIF-PO) of Complex
  Gates</title>
    <summary>  This paper starts with a comprehensive survey on RTL ATPG. It then proposes a
novel RTL ATPG model based on "Gate Inherent Faults" (GIF). These GIF are
extracted from each complex gate (adder, case-statement, etc.) of the RTL
source code individually. They are related to the internal logic paths of a
complex gate. They are not related to any net/signal in the RTL design. It is
observed, that when all GIF on RTL are covered (100%) and the same stimulus is
applied, then all gate level stuck-at faults of the netlist are covered (100%)
as well. The proposed RTL ATPG model is therefore synthesis independent. This
is shown on ITC'99 testcases. The applied semi-automatic test pattern
generation process is based on functional simulation.
</summary>
    <author>
      <name>Tobias Strauch</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05547v1</id>
    <updated>2016-12-16T16:42:28Z</updated>
    <published>2016-12-16T16:42:28Z</published>
    <title>Prototyping RISC Based, Reconfigurable Networking Applications in Open
  Source</title>
    <summary>  In the last decade we have witnessed a rapid growth in data center systems,
requiring new and highly complex networking devices. The need to refresh
networking infrastructure whenever new protocols or functions are introduced,
and the increasing costs that this entails, are of a concern to all data center
providers. New generations of Systems on Chip (SoC), integrating
microprocessors and higher bandwidth interfaces, are an emerging solution to
this problem. These devices permit entirely new systems and architectures that
can obviate the replacement of existing networking devices while enabling
seamless functionality change. In this work, we explore open source, RISC
based, SoC architectures with high performance networking capabilities. The
prototype architectures are implemented on the NetFPGA-SUME platform. Beyond
details of the architecture, we also describe the hardware implementation and
the porting of operating systems to the platform. The platform can be exploited
for the development of practical networking appliances, and we provide use case
examples.
</summary>
    <author>
      <name>Jong Hun Han</name>
    </author>
    <author>
      <name>Noa Zilberman</name>
    </author>
    <author>
      <name>Bjoern A. Zeeb</name>
    </author>
    <author>
      <name>Andreas Fiessler</name>
    </author>
    <author>
      <name>Andrew W. Moore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.08163v1</id>
    <updated>2016-12-24T10:55:46Z</updated>
    <published>2016-12-24T10:55:46Z</published>
    <title>Application-aware Retiming of Accelerators: A High-level Data-driven
  Approach</title>
    <summary>  Flexibility at hardware level is the main driving force behind adaptive
systems whose aim is to realise microarhitecture deconfiguration 'online'. This
feature allows the software/hardware stack to tolerate drastic changes of the
workload in data centres. With emerge of FPGA reconfigurablity this technology
is becoming a mainstream computing paradigm. Adaptivity is usually accompanied
by the high-level tools to facilitate multi-dimensional space exploration. An
essential aspect in this space is memory orchestration where on-chip and
off-chip memory distribution significantly influences the architecture in
coping with the critical spatial and timing constraints, e.g. Place and Route.
This paper proposes a memory smart technique for a particular class of adaptive
systems: Elastic Circuits which enjoy slack elasticity at fine level of
granularity. We explore retiming of a set of popular benchmarks via
investigating the memory distribution within and among accelerators. The area,
performance and power patterns are adopted by our high-level synthesis
framework, with respect to the behaviour of the input descriptions, to improve
the quality of the synthesised elastic circuits.
</summary>
    <author>
      <name>Ana Lava</name>
    </author>
    <author>
      <name>Mahdi Jelodari Mamaghani</name>
    </author>
    <author>
      <name>Siamak Mohammadi</name>
    </author>
    <author>
      <name>Steve Furber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures, submitted to IEEE Design and Test Journal -
  special issue on Accelerators in October 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.08163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.08163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.01630v1</id>
    <updated>2017-01-06T13:32:36Z</updated>
    <published>2017-01-06T13:32:36Z</published>
    <title>Reducing Competitive Cache Misses in Modern Processor Architectures</title>
    <summary>  The increasing number of threads inside the cores of a multicore processor,
and competitive access to the shared cache memory, become the main reasons for
an increased number of competitive cache misses and performance decline.
Inevitably, the development of modern processor architectures leads to an
increased number of cache misses. In this paper, we make an attempt to
implement a technique for decreasing the number of competitive cache misses in
the first level of cache memory. This technique enables competitive access to
the entire cache memory when there is a hit - but, if there are cache misses,
memory data (by using replacement techniques) is put in a virtual part given to
threads, so that competitive cache misses are avoided. By using a simulator
tool, the results show a decrease in the number of cache misses and performance
increase for up to 15%. The conclusion that comes out of this research is that
cache misses are a real challenge for future processor designers, in order to
hide memory latency.
</summary>
    <author>
      <name>Milcho Prisagjanec</name>
    </author>
    <author>
      <name>Pece Mitrevski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsit.2016.8605</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsit.2016.8605" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science &amp; Information Technology
  (IJCSIT), Vol 8, No 6, pp. 49-57, December 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.01630v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.01630v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.3.2; B.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03499v2</id>
    <updated>2017-02-14T07:58:57Z</updated>
    <published>2017-01-12T20:31:53Z</published>
    <title>VESPA: VIPT Enhancements for Superpage Accesses</title>
    <summary>  L1 caches are critical to the performance of modern computer systems. Their
design involves a delicate balance between fast lookups, high hit rates, low
access energy, and simplicity of implementation. Unfortunately, constraints
imposed by virtual memory make it difficult to satisfy all these attributes
today. Specifically, the modern staple of supporting virtual-indexing and
physical-tagging (VIPT) for parallel TLB-L1 lookups means that L1 caches are
usually grown with greater associativity rather than sets. This compromises
performance -- by degrading access times without significantly boosting hit
rates -- and increases access energy. We propose VIPT Enhancements for
SuperPage Accesses or VESPA in response. VESPA side-steps the traditional
problems of VIPT by leveraging the increasing ubiquity of superpages; since
superpages have more page offset bits, they can accommodate L1 cache
organizations with more sets than baseline pages can. VESPA dynamically adapts
to any OS distribution of page sizes to operate L1 caches with good access
times, hit rates, and energy, for both single- and multi-threaded workloads.
Since the hardware changes are modest, and there are no OS or application
changes, VESPA is readily-implementable.
  By superpages (also called huge or large pages) we refer to any page sizes
supported by the architecture bigger than baseline page size.
</summary>
    <author>
      <name>Mayank Parasar</name>
    </author>
    <author>
      <name>Abhishek Bhattacharjee</name>
    </author>
    <author>
      <name>Tushar Krishna</name>
    </author>
    <link href="http://arxiv.org/abs/1701.03499v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03499v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68-06" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03878v1</id>
    <updated>2017-01-14T04:03:50Z</updated>
    <published>2017-01-14T04:03:50Z</published>
    <title>HoLiSwap: Reducing Wire Energy in L1 Caches</title>
    <summary>  This paper describes HoLiSwap a method to reduce L1 cache wire energy, a
significant fraction of total cache energy, by swapping hot lines to the cache
way nearest to the processor. We observe that (i) a small fraction (&lt;3%) of
cache lines (hot lines) serve over 60% of the L1 cache accesses and (ii) the
difference in wire energy between the nearest and farthest cache subarray can
be over 6$\times$. Our method exploits this difference in wire energy to
dynamically identify hot lines and swap them to the nearest physical way in a
set-associative L1 cache. This provides up to 44% improvement in the wire
energy (1.82% saving in overall system energy) with no impact on the cache miss
rate and 0.13% performance drop. We also show that HoLiSwap can simplify
way-prediction.
</summary>
    <author>
      <name>Yatish Turakhia</name>
    </author>
    <author>
      <name>Subhasis Das</name>
    </author>
    <author>
      <name>Tor M. Aamodt</name>
    </author>
    <author>
      <name>William J. Dally</name>
    </author>
    <link href="http://arxiv.org/abs/1701.03878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.06741v3</id>
    <updated>2017-02-15T02:56:11Z</updated>
    <published>2017-01-24T06:06:25Z</published>
    <title>Variability-Aware Design for Energy Efficient Computational Artificial
  Intelligence Platform</title>
    <summary>  Portable computing devices, which include tablets, smart phones and various
types of wearable sensors, experienced a rapid development in recent years. One
of the most critical limitations for these devices is the power consumption as
they use batteries as the power supply. However, the bottleneck of the power
saving schemes in both hardware design and software algorithm is the huge
variability in power consumption. The variability is caused by a myriad of
factors, including the manufacturing process, the ambient environment
(temperature, humidity), the aging effects and etc. As the technology node
scaled down to 28nm and even lower, the variability becomes more severe. As a
result, a platform for variability characterization seems to be very necessary
and helpful.
</summary>
    <author>
      <name>Rhonda P. Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1701.06741v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.06741v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.07517v2</id>
    <updated>2017-02-15T16:36:28Z</updated>
    <published>2017-01-25T23:27:30Z</published>
    <title>Hardware Translation Coherence for Virtualized Systems</title>
    <summary>  To improve system performance, modern operating systems (OSes) often
undertake activities that require modification of virtual-to-physical page
translation mappings. For example, the OS may migrate data between physical
frames to defragment memory and enable superpages. The OS may migrate pages of
data between heterogeneous memory devices. We refer to all such activities as
page remappings. Unfortunately, page remappings are expensive. We show that
translation coherence is a major culprit and that systems employing
virtualization are especially badly affected by their overheads. In response,
we propose hardware translation invalidation and coherence or HATRIC, a readily
implementable hardware mechanism to piggyback translation coherence atop
existing cache coherence protocols. We perform detailed studies using KVM-based
virtualization, showing that HATRIC achieves up to 30% performance and 10%
energy benefits, for per-CPU area overheads of 2%. We also quantify HATRIC's
benefits on systems running Xen and find up to 33% performance improvements.
</summary>
    <author>
      <name>Zi Yan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Rutgers University</arxiv:affiliation>
    </author>
    <author>
      <name>Guilherme Cox</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Rutgers University</arxiv:affiliation>
    </author>
    <author>
      <name>Jan Vesely</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Rutgers University</arxiv:affiliation>
    </author>
    <author>
      <name>Abhishek Bhattacharjee</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Rutgers University</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1701.07517v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.07517v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08849v1</id>
    <updated>2017-01-30T22:09:23Z</updated>
    <published>2017-01-30T22:09:23Z</published>
    <title>Accurate Measurement of Power Consumption Overhead During FPGA Dynamic
  Partial Reconfiguration</title>
    <summary>  In the context of embedded systems design, two important challenges are still
under investigation. First, improve real-time data processing,
reconfigurability, scalability, and self-adjusting capabilities of hardware
components. Second, reduce power consumption through low-power design
techniques as clock gating, logic gating, and dynamic partial reconfiguration
(DPR) capabilities. Today, several application, e.g., cryptography,
Software-defined radio or aerospace missions exploit the benefits of DPR of
programmable logic devices. The DPR allows well defined reconfigurable FPGA
region to be modified during runtime. However, it introduces an overhead in
term of power consumption and time during the reconfiguration phase. In this
paper, we present an investigation of power consumption overhead of the DPR
process using a high-speed digital oscilloscope and the shunt resistor method.
Results in terms of reconfiguration time and power consumption overhead for
Virtex 5 FPGAs are shown.
</summary>
    <author>
      <name>Amor Nafkha</name>
    </author>
    <author>
      <name>Yves Louet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures, 2016 International Symposium on Wireless
  Communication Systems (ISWCS): Special sessions - Low Power Design Techniques
  for Embedded Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.08849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08877v2</id>
    <updated>2017-02-15T02:56:28Z</updated>
    <published>2017-01-31T00:14:32Z</published>
    <title>1.5 bit-per-stage 8-bit Pipelined CMOS A/D Converter for Neuromophic
  Vision Processor</title>
    <summary>  Neuromorphic vision processor is an electronic implementation of vision
algorithm processor on semiconductor. To image the world, a low-power CMOS
image sensor array is required in the vision processor. The image sensor array
is typically formed through photo diodes and analog to digital converter (ADC).
To achieve low power acquisition, a low-power mid-resolution ADC is necessary.
In this paper, a 1.8V, 8-bit, 166MS/s pipelined ADC was proposed in a 0.18 um
CMOS technology. The ADC used operational amplifier sharing architecture to
reduce power consumption and achieved maximum DNL of 0.24 LSB, maximum INL of
0.35 LSB, at a power consumption of 38.9mW. When input frequency is 10.4MHz, it
achieved an SNDR 45.9dB, SFDR 50dB, and an ENOB of 7.33 bit.
</summary>
    <author>
      <name>Yilei F. Li</name>
    </author>
    <author>
      <name>Li Du</name>
    </author>
    <link href="http://arxiv.org/abs/1701.08877v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08877v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.01067v1</id>
    <updated>2017-02-03T16:06:21Z</updated>
    <published>2017-02-03T16:06:21Z</published>
    <title>Sense Amplifier Comparator with Offset Correction for Decision Feedback
  Equalization based Receivers</title>
    <summary>  A decision feedback circuit with integrated offset compensation is presented
in this paper. The circuit is built around the sense amplifier comparator. The
feedback loop is closed around the first stage of the comparator resulting in
minimum loop latency. The feedback loop is implemented using a switched
capacitor network that picks from one of pre-computed voltages to be fed back.
The comparator's offset that is to be compensated for, is added in the same
path. Hence, an extra offset correction input is not required. The circuit is
used as a receiver for a 10 mm low swing interconnect implemented in UMC 130 nm
CMOS technology. The circuit is tested at a frequency of 1 GHz and it consumes
145 $\mu$A from a 1.2V supply at this frequency.
</summary>
    <author>
      <name>Naveen Kadayinti</name>
    </author>
    <author>
      <name>Dinesh Sharma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.01067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.01295v1</id>
    <updated>2017-02-04T14:37:38Z</updated>
    <published>2017-02-04T14:37:38Z</published>
    <title>Embedded Systems Architecture for SLAM Applications</title>
    <summary>  In recent years, we have observed a clear trend in the rapid rise of
autonomous vehicles, robotics, virtual reality, and augmented reality. The core
technology enabling these applications, Simultaneous Localization And Mapping
(SLAM), imposes two main challenges: first, these workloads are computationally
intensive and they often have real-time requirements; second, these workloads
run on battery-powered mobile devices with limited energy budget. In short, the
essence of these challenges is that performance should be improved while
simultaneously reducing energy consumption, two rather contradicting goals by
conventional wisdom. In this paper, we take a close look at state-of-the-art
Simultaneous Localization And Mapping (SLAM) workloads, especially how these
workloads behave on mobile devices. Based on the results, we propose a mobile
architecture to improve SLAM performance on mobile devices.
</summary>
    <author>
      <name>Jie Tang</name>
    </author>
    <author>
      <name>Shaoshan Liu</name>
    </author>
    <author>
      <name>Jean-Luc Gaudiot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.01295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02677v1</id>
    <updated>2017-04-10T00:44:42Z</updated>
    <published>2017-04-10T00:44:42Z</published>
    <title>Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware
  Cooperation</title>
    <summary>  Putting the DRAM on the same package with a processor enables several times
higher memory bandwidth than conventional off-package DRAM. Yet, the latency of
in-package DRAM is not appreciably lower than that of off-package DRAM. A
promising use of in-package DRAM is as a large cache. Unfortunately, most
previous DRAM cache designs mainly optimize for hit latency and do not consider
off-chip bandwidth efficiency as a first-class design constraint. Hence, as we
show in this paper, these designs are suboptimal for use with in-package DRAM.
  We propose a new DRAM cache design, Banshee, that optimizes for both in- and
off-package DRAM bandwidth efficiency without degrading access latency. The key
ideas are to eliminate the in-package DRAM bandwidth overheads due to costly
tag accesses through virtual memory mechanism and to incorporate a
bandwidth-aware frequency-based replacement policy that is biased to reduce
unnecessary traffic to off-package DRAM. Our extensive evaluation shows that
Banshee provides significant performance improvement and traffic reduction over
state-of-the-art latency-optimized DRAM cache designs.
</summary>
    <author>
      <name>Xiangyao Yu</name>
    </author>
    <author>
      <name>Christopher J. Hughes</name>
    </author>
    <author>
      <name>Nadathur Satish</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <author>
      <name>Srinivas Devadas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.02677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03991v1</id>
    <updated>2017-04-13T04:38:13Z</updated>
    <published>2017-04-13T04:38:13Z</published>
    <title>Architectural Techniques to Enable Reliable and Scalable Memory Systems</title>
    <summary>  High capacity and scalable memory systems play a vital role in enabling our
desktops, smartphones, and pervasive technologies like Internet of Things
(IoT). Unfortunately, memory systems are becoming increasingly prone to faults.
This is because we rely on technology scaling to improve memory density, and at
small feature sizes, memory cells tend to break easily. Today, memory
reliability is seen as the key impediment towards using high-density devices,
adopting new technologies, and even building the next Exascale supercomputer.
To ensure even a bare-minimum level of reliability, present-day solutions tend
to have high performance, power and area overheads. Ideally, we would like
memory systems to remain robust, scalable, and implementable while keeping the
overheads to a minimum. This dissertation describes how simple cross-layer
architectural techniques can provide orders of magnitude higher reliability and
enable seamless scalability for memory systems while incurring negligible
overheads.
</summary>
    <author>
      <name>Prashant J. Nair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD thesis, Georgia Institute of Technology (May 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03991v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03991v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05044v1</id>
    <updated>2017-04-17T17:41:17Z</updated>
    <published>2017-04-17T17:41:17Z</published>
    <title>A Study on Performance and Power Efficiency of Dense Non-Volatile Caches
  in Multi-Core Systems</title>
    <summary>  In this paper, we present a novel cache design based on Multi-Level Cell
Spin-Transfer Torque RAM (MLC STTRAM) that can dynamically adapt the set
capacity and associativity to use efficiently the full potential of MLC STTRAM.
We exploit the asymmetric nature of the MLC storage scheme to build cache lines
featuring heterogeneous performances, that is, half of the cache lines are
read-friendly, while the other is write-friendly. Furthermore, we propose to
opportunistically deactivate ways in underutilized sets to convert MLC to
Single-Level Cell (SLC) mode, which features overall better performance and
lifetime. Our ultimate goal is to build a cache architecture that combines the
capacity advantages of MLC and performance/energy advantages of SLC. Our
experiments show an improvement of 43% in total numbers of conflict misses, 27%
in memory access latency, 12% in system performance, and 26% in LLC access
energy, with a slight degradation in cache lifetime (about 7%) compared to an
SLC cache.
</summary>
    <author>
      <name>Amin Jadidi</name>
    </author>
    <author>
      <name>Mohammad Arjomand</name>
    </author>
    <author>
      <name>Mahmut T. Kandemir</name>
    </author>
    <author>
      <name>Chita R. Das</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3078505.3078547</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3078505.3078547" rel="related"/>
    <link href="http://arxiv.org/abs/1704.05044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08526v1</id>
    <updated>2017-04-27T12:07:52Z</updated>
    <published>2017-04-27T12:07:52Z</published>
    <title>An Efficient Reconfigurable FIR Digital Filter Using Modified Distribute
  Arithmetic Technique</title>
    <summary>  This paper provides modified Distributed Arithmetic based technique to
compute sum of products saving appreciable number of Multiply And accumulation
blocks and this consecutively reduces circuit size. In this technique
multiplexer based structure is used to reuse the blocks so as to reduce the
required memory locations. In this technique a Carry Look Ahead based adder
tree is used to have better area-delay product. Designing of FIR filter is done
using VHDL and synthesized using Xilinx 12.2 synthesis tool and ISIM simulator.
The power analysis is done using Xilinx Xpower analyzer. The proposed structure
requires nearly 42% less cells, 40% less LUT flip-flop pairs used, and also 2%
less power compared with existing structure.
</summary>
    <author>
      <name>Naveen S Naik</name>
    </author>
    <author>
      <name>Kiran A Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,4 figures, journal 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08526v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08526v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00218v1</id>
    <updated>2017-04-29T17:58:28Z</updated>
    <published>2017-04-29T17:58:28Z</published>
    <title>A floating point division unit based on Taylor-Series expansion
  algorithm and Iterative Logarithmic Multiplier</title>
    <summary>  Floating point division, even though being an infrequent operation in the
traditional sense, is indis- pensable when it comes to a range of
non-traditional applications such as K-Means Clustering and QR Decomposition
just to name a few. In such applications, hardware support for floating point
division would boost the performance of the entire system. In this paper, we
present a novel architecture for a floating point division unit based on the
Taylor-series expansion algorithm. We show that the Iterative Logarithmic
Multiplier is very well suited to be used as a part of this architecture. We
propose an implementation of the powering unit that can calculate an odd power
and an even power of a number simultaneously, meanwhile having little hardware
overhead when compared to the Iterative Logarithmic Multiplier.
</summary>
    <author>
      <name>Riyansh K. Karani</name>
    </author>
    <author>
      <name>Akash K. Rana</name>
    </author>
    <author>
      <name>Dhruv H. Reshamwala</name>
    </author>
    <author>
      <name>Kishore Saldanha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeCoM, CSITEC - 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.00218v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00218v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02610v1</id>
    <updated>2017-05-07T12:49:19Z</updated>
    <published>2017-05-07T12:49:19Z</published>
    <title>Static Timing Model Extraction for Combinational Circuits</title>
    <summary>  For large circuits, static timing analysis (STA) needs to be performed in a
hierarchical manner to achieve higher performance in arrival time propagation.
In hierarchical STA, efficient and accurate timing models of sub-modules need
to be created. We propose a timing model extraction method that significantly
reduces the size of timing models without losing any accuracy by removing
redundant timing information. Circuit components which do not contribute to the
delay of any input to output pair are removed. The proposed method is
deterministic. Compared to the original models, the numbers of edges and
vertices of the resulting timing models are reduced by 84% and 85% on average,
respectively, which are significantly more than the results achieved by other
methods.
</summary>
    <author>
      <name>Bing Li</name>
    </author>
    <author>
      <name>Christoph Knoth</name>
    </author>
    <author>
      <name>Walter Schneider</name>
    </author>
    <author>
      <name>Manuel Schmidt</name>
    </author>
    <author>
      <name>Ulf Schlichtmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-540-95948-9_16</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-540-95948-9_16" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Workshop on Power and Timing Modeling, Optimization and
  Simulation (PATMOS), 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.02610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W35 VLSI algorithms" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02732v1</id>
    <updated>2017-05-08T03:08:00Z</updated>
    <published>2017-05-08T03:08:00Z</published>
    <title>A Scalable, Low-Overhead Finite-State Machine Overlay for Rapid FPGA
  Application Development</title>
    <summary>  Productivity issues such as lengthy compilation and limited code reuse have
restricted usage of field-programmable gate arrays (FPGAs), despite significant
technical advantages. Recent work into overlays -- virtual coarse-grained
architectures implemented atop FPGAs -- has aimed to address these concerns
through abstraction, but have mostly focused on pipelined applications with
minimal control requirements. Although research has introduced overlays for
finite-state machines, those architectures suffer from limited scalability and
flexibility, which we address with a new overlay architecture using memory
decomposition on transitional logic. Although our overlay provides modest
average improvements of 15% to 29% fewer lookup tables for individual
finite-state machines, for the more common usage of an overlay supporting
different finite-state machines, our overlay achieves a 77% to 99% reduction in
lookup tables. In addition, our overlay reduces compilation time to tenths of a
second to enable rapid iterative-development methodologies.
</summary>
    <author>
      <name>David Wilson</name>
    </author>
    <author>
      <name>Greg Stitt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 3rd International Workshop on Overlay Architectures for
  FPGAs (OLAF 2017) arXiv:1704.08802</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.02732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02734v1</id>
    <updated>2017-05-08T03:18:33Z</updated>
    <published>2017-05-08T03:18:33Z</published>
    <title>Out-of-Order Dataflow Scheduling for FPGA Overlays</title>
    <summary>  We exploit floating-point DSPs in the Arria10 FPGA and multi-pumping feature
of the M20K RAMs to build a dataflow-driven soft processor fabric for large
graph workloads. In this paper, we introduce the idea of out-of-order node
scheduling across a large number of local nodes (thousands) per processor by
combining an efficient node tagging scheme along with leading-one detector
circuits. We use a static one-time node labeling algorithm to sort nodes based
on criticality to organize local memory inside each soft processor. This
translates to a small ~6% memory overhead. When compared to a memory-expensive
FIFO-based first-come-first-serve approach used in previous studies, we deliver
up to 50% performance improvement while eliminating the cost of the FIFOs. On
the Arria10 10AX115S board, we can create an overlay design of up to 300
processors connected by high bandwidth Hoplite NoC at frequencies up to 250MHz.
</summary>
    <author>
      <name> Siddhartha</name>
    </author>
    <author>
      <name>Nachiket Kapre</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 3rd International Workshop on Overlay Architectures for
  FPGAs (OLAF 2017) arXiv:1704.08802</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.02734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04627v1</id>
    <updated>2017-05-12T15:34:10Z</updated>
    <published>2017-05-12T15:34:10Z</published>
    <title>Sprinkler: Maximizing Resource Utilization in Many-Chip Solid State
  Disks</title>
    <summary>  Resource utilization is one of the emerging problems in many-chip SSDs. In
this paper, we propose Sprinkler, a novel device-level SSD controller, which
targets maximizing resource utilization and achieving high performance without
additional NAND flash chips. Specifically, Sprinkler relaxes parallelism
dependency by scheduling I/O requests based on internal resource layout rather
than the order imposed by the device-level queue. In addition, Sprinkler
improves flash-level parallelism and reduces the number of transactions (i.e.,
improves transactional-locality) by over-committing flash memory requests to
specific resources. Our extensive experimental evaluation using a
cycle-accurate large-scale SSD simulation framework shows that a many-chip SSD
equipped with our Sprinkler provides at least 56.6% shorter latency and 1.8 ~
2.2 times better throughput than the state-of-the-art SSD controllers. Further,
it improves overall resource utilization by 68.8% under different I/O request
patterns and provides, on average, 80.2% more flash-level parallelism by
reducing half of the flash memory requests at runtime.
</summary>
    <author>
      <name>Myoungsoo Jung</name>
    </author>
    <author>
      <name>Mahmut T. Kandemir</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HPCA.2014.6835961</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HPCA.2014.6835961" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is published at 20th IEEE International Symposium On High
  Performance Computer Architecture</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04975v1</id>
    <updated>2017-05-14T15:29:29Z</updated>
    <published>2017-05-14T15:29:29Z</published>
    <title>On Hierarchical Statistical Static Timing Analysis</title>
    <summary>  Statistical static timing analysis deals with the increasing variations in
manufacturing processes to reduce the pessimism in the worst case timing
analysis. Because of the correlation between delays of circuit components,
timing model generation and hierarchical timing analysis face more challenges
than in static timing analysis. In this paper, a novel method to generate
timing models for combinational circuits considering variations is proposed.
The resulting timing models have accurate input-output delays and are about 80%
smaller than the original circuits. Additionally, an accurate hierarchical
timing analysis method at design level using pre-characterized timing models is
proposed. This method incorporates the correlation between modules by replacing
independent random variables to improve timing accuracy. Experimental results
show that the correlation between modules strongly affects the delay
distribution of the hierarchical design and the proposed method has good
accuracy compared with Monte Carlo simulation, but is faster by three orders of
magnitude.
</summary>
    <author>
      <name>Bing Li</name>
    </author>
    <author>
      <name>Ning Chen</name>
    </author>
    <author>
      <name>Manuel Schmidt</name>
    </author>
    <author>
      <name>Walter Schneider</name>
    </author>
    <author>
      <name>Ulf Schlichtmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DATE.2009.5090869</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DATE.2009.5090869" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Design, Automation and Test in Europe (DATE) 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04975v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04975v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04976v1</id>
    <updated>2017-05-14T15:32:57Z</updated>
    <published>2017-05-14T15:32:57Z</published>
    <title>Timing Model Extraction for Sequential Circuits Considering Process
  Variations</title>
    <summary>  As semiconductor devices continue to scale down, process vari- ations become
more relevant for circuit design. Facing such variations, statistical static
timing analysis is introduced to model variations more accurately so that the
pessimism in tra- ditional worst case timing analysis is reduced. Because all
de- lays are modeled using correlated random variables, most statis- tical
timing methods are much slower than corner based timing analysis. To speed up
statistical timing analysis, we propose a method to extract timing models for
flip-flop and latch based sequential circuits respectively. When such a circuit
is used as a module in a hierarchical design, the timing model instead of the
original circuit is used for timing analysis. The extracted timing models are
much smaller than the original circuits. Ex- periments show that using
extracted timing models accelerates timing verification by orders of magnitude
compared to previ- ous approaches using flat netlists directly. Accuracy is
main- tained, however, with the mean and standard deviation of the clock period
both showing usually less than 1% error compared to Monte Carlo simulation on a
number of benchmark circuits.
</summary>
    <author>
      <name>Bing Li</name>
    </author>
    <author>
      <name>Ning Chen</name>
    </author>
    <author>
      <name>Ulf Schlichtmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1687399.1687463</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1687399.1687463" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/ACM International Conference on Computer-Aided Design (ICCAD),
  2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04981v1</id>
    <updated>2017-05-14T15:49:14Z</updated>
    <published>2017-05-14T15:49:14Z</published>
    <title>On Timing Model Extraction and Hierarchical Statistical Timing Analysis</title>
    <summary>  In this paper, we investigate the challenges to apply Statistical Static
Timing Analysis (SSTA) in hierarchical design flow, where modules supplied by
IP vendors are used to hide design details for IP protection and to reduce the
complexity of design and verification. For the three basic circuit types,
combinational, flip-flop-based and latch-controlled, we propose methods to
extract timing models which contain interfacing as well as compressed internal
constraints. Using these compact timing models the runtime of full-chip timing
analysis can be reduced, while circuit details from IP vendors are not exposed.
We also propose a method to reconstruct the correlation between modules during
full-chip timing analysis. This correlation can not be incorporated into timing
models because it depends on the layout of the corresponding modules in the
chip. In addition, we investigate how to apply the extracted timing models with
the reconstructed correlation to evaluate the performance of the complete
design. Experiments demonstrate that using the extracted timing models and
reconstructed correlation full-chip timing analysis can be several times faster
than applying the flattened circuit directly, while the accuracy of statistical
timing analysis is still well maintained.
</summary>
    <author>
      <name>Bing Li</name>
    </author>
    <author>
      <name>Ning Chen</name>
    </author>
    <author>
      <name>Yang Xu</name>
    </author>
    <author>
      <name>Ulf Schlichtmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCAD.2012.2228305</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCAD.2012.2228305" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Computer-Aided Design of Integrated Circuits
  and Systems 32(3), 367-380, March 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1705.04981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04983v1</id>
    <updated>2017-05-14T15:56:30Z</updated>
    <published>2017-05-14T15:56:30Z</published>
    <title>Post-Route Alleviation of Dense Meander Segments in High-Performance
  Printed Circuit Boards</title>
    <summary>  Length-matching is an important technique to balance delays of bus signals in
high-performance PCB routing. Existing routers, however, may generate dense
meander segments with small distance. Signals propagating across these meander
segments exhibit a speedup effect due to crosstalks between the segments of the
same wire, thus leading to mismatch of arrival times even with the same
physical wire length. In this paper, we propose a post-processing method to
enlarge the width and the distance of meander segments and distribute them more
evenly on the board so that the crosstalks can be reduced. In the proposed
framework, we model the sharing combinations of available routing areas after
removing dense meander segments from the initial routing, as well as the
generation of relaxed meander segments and their groups in subareas.
Thereafter, this model is transformed into an ILP problem and solved
efficiently. Experimental results show that the proposed method can extend the
width and the distance of meander segments about two times even under very
tight area constraints, so that the crosstalks and thus the speedup effect can
be alleviated effectively in high-performance PCB designs.
</summary>
    <author>
      <name>Tsun-Ming Tseng</name>
    </author>
    <author>
      <name>Bing Li</name>
    </author>
    <author>
      <name>Tsung-Yi Ho</name>
    </author>
    <author>
      <name>Ulf Schlichtmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCAD.2013.6691193</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCAD.2013.6691193" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/ACM International Conference on Computer-Aided Design (ICCAD),
  2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04983v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04983v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04986v1</id>
    <updated>2017-05-14T16:03:30Z</updated>
    <published>2017-05-14T16:03:30Z</published>
    <title>Statistical Timing Analysis and Criticality Computation for Circuits
  with Post-Silicon Clock Tuning Elements</title>
    <summary>  Post-silicon clock tuning elements are widely used in high-performance
designs to mitigate the effects of process variations and aging. Located on
clock paths to flip-flops, these tuning elements can be configured through the
scan chain so that clock skews to these flip-flops can be adjusted after man-
ufacturing. Owing to the delay compensation across consecutive register stages
enabled by the clock tuning elements, higher yield and enhanced robustness can
be achieved. These benefits are, nonetheless, attained by increasing die area
due to the inserted clock tuning elements. For balancing performance
improvement and area cost, an efficient timing analysis algorithm is needed to
evaluate the performance of such a circuit. So far this evaluation is only
possible by Monte Carlo simulation which is very timing- consuming. In this
paper, we propose an alternative method using graph transformation, which
computes a parametric minimum clock period and is more than 10 4 times faster
than Monte Carlo simulation while maintaining a good accuracy. This method also
identifies the gates that are critical to circuit performance, so that a fast
analysis-optimization flow becomes possible.
</summary>
    <author>
      <name>Bing Li</name>
    </author>
    <author>
      <name>Ulf Schlichtmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCAD.2015.2432143</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCAD.2015.2432143" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Computer-Aided Design of Integrated Circuits and
  Systems, November 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04986v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04986v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04990v1</id>
    <updated>2017-05-14T16:09:10Z</updated>
    <published>2017-05-14T16:09:10Z</published>
    <title>Sampling-based Buffer Insertion for Post-Silicon Yield Improvement under
  Process Variability</title>
    <summary>  At submicron manufacturing technology nodes process variations affect circuit
performance significantly. This trend leads to a large timing margin and thus
overdesign to maintain yield. To combat this pessimism, post-silicon clock
tuning buffers can be inserted into circuits to balance timing budgets of
critical paths with their neighbors. After manufacturing, these clock buffers
can be configured for each chip individually so that chips with timing failures
may be rescued to improve yield. In this paper, we propose a sampling-based
method to determine the proper locations of these buffers. The goal of this
buffer insertion is to reduce the number of buffers and their ranges, while
still maintaining a good yield improvement. Experimental results demonstrate
that our algorithm can achieve a significant yield improvement (up to 35%) with
only a small number of buffers.
</summary>
    <author>
      <name>Grace Li Zhang</name>
    </author>
    <author>
      <name>Bing Li</name>
    </author>
    <author>
      <name>Ulf Schlichtmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3850/9783981537079_0250</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3850/9783981537079_0250" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Design, Automation and Test in Europe (DATE), 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04992v1</id>
    <updated>2017-05-14T16:17:58Z</updated>
    <published>2017-05-14T16:17:58Z</published>
    <title>EffiTest: Efficient Delay Test and Statistical Prediction for
  Configuring Post-silicon Tunable Buffers</title>
    <summary>  At nanometer manufacturing technology nodes, process variations significantly
affect circuit performance. To combat them, post- silicon clock tuning buffers
can be deployed to balance timing bud- gets of critical paths for each
individual chip after manufacturing. The challenge of this method is that path
delays should be mea- sured for each chip to configure the tuning buffers
properly. Current methods for this delay measurement rely on path-wise
frequency stepping. This strategy, however, requires too much time from ex-
pensive testers. In this paper, we propose an efficient delay test framework
(EffiTest) to solve the post-silicon testing problem by aligning path delays
using the already-existing tuning buffers in the circuit. In addition, we only
test representative paths and the delays of other paths are estimated by
statistical delay prediction. Exper- imental results demonstrate that the
proposed method can reduce the number of frequency stepping iterations by more
than 94% with only a slight yield loss.
</summary>
    <author>
      <name>Grace Li Zhang</name>
    </author>
    <author>
      <name>Bing Li</name>
    </author>
    <author>
      <name>Ulf Schlichtmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2897937.2898017</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2897937.2898017" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM/IEEE Design Automation Conference (DAC), June 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04993v1</id>
    <updated>2017-05-14T16:21:03Z</updated>
    <published>2017-05-14T16:21:03Z</published>
    <title>PieceTimer: A Holistic Timing Analysis Framework Considering Setup/Hold
  Time Interdependency Using A Piecewise Model</title>
    <summary>  In static timing analysis, clock-to-q delays of flip-flops are considered as
constants. Setup times and hold times are characterized separately and also
used as constants. The characterized delays, setup times and hold times, are
ap- plied in timing analysis independently to verify the perfor- mance of
circuits. In reality, however, clock-to-q delays of flip-flops depend on both
setup and hold times. Instead of being constants, these delays change with
respect to different setup/hold time combinations. Consequently, the simple ab-
straction of setup/hold times and constant clock-to-q delays introduces
inaccuracy in timing analysis. In this paper, we propose a holistic method to
consider the relation between clock-to-q delays and setup/hold time
combinations with a piecewise linear model. The result is more accurate than
that of traditional timing analysis, and the incorporation of the
interdependency between clock-to-q delays, setup times and hold times may also
improve circuit performance.
</summary>
    <author>
      <name>Grace Li Zhang</name>
    </author>
    <author>
      <name>Bing Li</name>
    </author>
    <author>
      <name>Ulf Schlichtmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2966986.2967064</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2966986.2967064" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/ACM International Conference on Computer-Aided Design (ICCAD),
  November 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.06419v2</id>
    <updated>2017-09-14T14:28:11Z</updated>
    <published>2017-05-18T05:08:34Z</published>
    <title>SimpleSSD: Modeling Solid State Drives for Holistic System Simulation</title>
    <summary>  Existing solid state drive (SSD) simulators unfortunately lack hardware
and/or software architecture models. Consequently, they are far from capturing
the critical features of contemporary SSD devices. More importantly, while the
performance of modern systems that adopt SSDs can vary based on their numerous
internal design parameters and storage-level configurations, a full system
simulation with traditional SSD models often requires unreasonably long
runtimes and excessive computational resources. In this work, we propose
SimpleSSD, a highfidelity simulator that models all detailed characteristics of
hardware and software, while simplifying the nondescript features of storage
internals. In contrast to existing SSD simulators, SimpleSSD can easily be
integrated into publicly-available full system simulators. In addition, it can
accommodate a complete storage stack and evaluate the performance of SSDs along
with diverse memory technologies and microarchitectures. Thus, it facilitates
simulations that explore the full design space at different levels of system
abstraction.
</summary>
    <author>
      <name>Myoungsoo Jung</name>
    </author>
    <author>
      <name>Jie Zhang</name>
    </author>
    <author>
      <name>Ahmed Abulila</name>
    </author>
    <author>
      <name>Miryeong Kwon</name>
    </author>
    <author>
      <name>Narges Shahidi</name>
    </author>
    <author>
      <name>John Shalf</name>
    </author>
    <author>
      <name>Nam Sung Kim</name>
    </author>
    <author>
      <name>Mahmut Kandemir</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LCA.2017.2750658</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LCA.2017.2750658" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted at IEEE Computer Architecture Letters
  (CAL)</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.06419v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.06419v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07465v1</id>
    <updated>2017-05-21T15:56:04Z</updated>
    <published>2017-05-21T15:56:04Z</published>
    <title>Some Schemes for Implementation of Arithmetic Operations with Complex
  Numbers Using Squaring Units</title>
    <summary>  In this paper, new schemes for a squarer, multiplier and divider of complex
numbers are proposed. Traditional structural solutions for each of these
operations require the presence some number of general-purpose binary
multipliers. The advantage of our solutions is a removing of multiplications
through replacing them by less costly squarers. We use Logan's trick and
quarter square technique, which propose to replace the calculation of the
product of two real numbers by summing the squares. Replacing usual multipliers
on digital squares implies reducing power consumption as well as decreases
hardware circuit complexity. The squarer requiring less area and power as
compared to general-purpose multiplier, it is interesting to assess the use of
squarers to implementation of complex arithmetic.
</summary>
    <author>
      <name>Aleksandr Cariow</name>
    </author>
    <author>
      <name>Galina Cariowa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages. 3 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.07465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="15A23, 65Y20, 65F30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.1, G.1.0, I.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08009v1</id>
    <updated>2017-05-22T21:28:50Z</updated>
    <published>2017-05-22T21:28:50Z</published>
    <title>A Low-Power Accelerator for Deep Neural Networks with Enlarged Near-Zero
  Sparsity</title>
    <summary>  It remains a challenge to run Deep Learning in devices with stringent power
budget in the Internet-of-Things. This paper presents a low-power accelerator
for processing Deep Neural Networks in the embedded devices. The power
reduction is realized by avoiding multiplications of near-zero valued data. The
near-zero approximation and a dedicated Near-Zero Approximation Unit (NZAU) are
proposed to predict and skip the near-zero multiplications under certain
thresholds. Compared with skipping zero-valued computations, our design
achieves 1.92X and 1.51X further reduction of the total multiplications in
LeNet-5 and Alexnet respectively, with negligible lose of accuracy. In the
proposed accelerator, 256 multipliers are grouped into 16 independent
Processing Lanes (PL) to support up to 16 neuron activations simultaneously.
With the help of data pre-processing and buffering in each PL, multipliers can
be clock-gated in most of the time even the data is excessively streaming in.
Designed and simulated in UMC 65 nm process, the accelerator operating at 500
MHz is $&gt;$ 4X faster than the mobile GPU Tegra K1 in processing the
fully-connected layer FC8 of Alexnet, while consuming 717X less energy.
</summary>
    <author>
      <name>Yuxiang Huan</name>
    </author>
    <author>
      <name>Yifan Qin</name>
    </author>
    <author>
      <name>Yantian You</name>
    </author>
    <author>
      <name>Lirong Zheng</name>
    </author>
    <author>
      <name>Zhuo Zou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.08009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02344v1</id>
    <updated>2017-06-07T19:07:38Z</updated>
    <published>2017-06-07T19:07:38Z</published>
    <title>Energy-Efficient Hybrid Stochastic-Binary Neural Networks for
  Near-Sensor Computing</title>
    <summary>  Recent advances in neural networks (NNs) exhibit unprecedented success at
transforming large, unstructured data streams into compact higher-level
semantic information for tasks such as handwriting recognition, image
classification, and speech recognition. Ideally, systems would employ
near-sensor computation to execute these tasks at sensor endpoints to maximize
data reduction and minimize data movement. However, near- sensor computing
presents its own set of challenges such as operating power constraints, energy
budgets, and communication bandwidth capacities. In this paper, we propose a
stochastic- binary hybrid design which splits the computation between the
stochastic and binary domains for near-sensor NN applications. In addition, our
design uses a new stochastic adder and multiplier that are significantly more
accurate than existing adders and multipliers. We also show that retraining the
binary portion of the NN computation can compensate for precision losses
introduced by shorter stochastic bit-streams, allowing faster run times at
minimal accuracy losses. Our evaluation shows that our hybrid stochastic-binary
design can achieve 9.8x energy efficiency savings, and application-level
accuracies within 0.05% compared to conventional all-binary designs.
</summary>
    <author>
      <name>Vincent T. Lee</name>
    </author>
    <author>
      <name>Armin Alaghi</name>
    </author>
    <author>
      <name>John P. Hayes</name>
    </author>
    <author>
      <name>Visvesh Sathe</name>
    </author>
    <author>
      <name>Luis Ceze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, Design, Automata and Test in Europe (DATE) 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.02344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01697v1</id>
    <updated>2017-07-06T09:11:24Z</updated>
    <published>2017-07-06T09:11:24Z</published>
    <title>Pipelined Parallel FFT Architecture</title>
    <summary>  In this paper, an optimized efficient VLSI architecture of a pipeline Fast
Fourier transform (FFT) processor capable of producing the reverse output order
sequence is presented. Paper presents Radix-2 multipath delay architecture for
FFT calculation. The implementation of FFT in hardware is very critical because
for calculation of FFT number of butterfly operations i.e. number of
multipliers requires due to which hardware gets increased means indirectly cost
of hardware is automatically gets increased. Also multiplier operations are
slow that's why it limits the speed of operation of architecture. The optimized
VLSI implementation of FFT algorithm is presented in this paper. Here
architecture is pipelined to optimize it and to increase the speed of
operation. Also to increase the speed of operation 2 levels parallel processing
is used.
</summary>
    <author>
      <name>Tanaji U. Kamble</name>
    </author>
    <author>
      <name>B. G. Patil</name>
    </author>
    <author>
      <name>Rakhee S. Bhojakar</name>
    </author>
    <link href="http://arxiv.org/abs/1707.01697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.04657v1</id>
    <updated>2017-07-14T22:50:47Z</updated>
    <published>2017-07-14T22:50:47Z</published>
    <title>Variable Instruction Fetch Rate to Reduce Control Dependent Penalties</title>
    <summary>  In order to overcome the branch execution penalties of hard-to-predict
instruction branches, two new instruction fetch micro-architectural methods are
proposed in this paper. In addition, to compare performance of the two proposed
methods, different instruction fetch policy schemes of existing multi-branch
path architectures are evaluated. An improvement in Instructions Per Cycle
(IPC) of 29.4% on average over single-thread execution with gshare branch
predictor on SPEC 2000/2006 benchmark is shown. In this paper, wide pipeline
machines are simulated for evaluation purposes. The methods discussed in this
paper can be extended to High Performance Scientific Computing needs, if the
demands of IPC improvement are far more critical than $cost.
</summary>
    <author>
      <name>Aswin Ramachandran</name>
    </author>
    <author>
      <name>Louis Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 Figures, Disclosure: The work is a part of my PhD
  dissertation at OSU and not in anyway related to Intel</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.04657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.04657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.05399v2</id>
    <updated>2018-02-13T17:18:15Z</updated>
    <published>2017-07-17T21:19:28Z</published>
    <title>Performance Implications of NoCs on 3D-Stacked Memories: Insights from
  the Hybrid Memory Cube</title>
    <summary>  Memories that exploit three-dimensional (3D)-stacking technology, which
integrate memory and logic dies in a single stack, are becoming popular. These
memories, such as Hybrid Memory Cube (HMC), utilize a network-on-chip (NoC)
design for connecting their internal structural organizations. This novel usage
of NoC, in addition to aiding processing-in-memory capabilities, enables
numerous benefits such as high bandwidth and memory-level parallelism. However,
the implications of NoCs on the characteristics of 3D-stacked memories in terms
of memory access latency and bandwidth have not been fully explored. This paper
addresses this knowledge gap by (i) characterizing an HMC prototype on the
AC-510 accelerator board and revealing its access latency behaviors, and (ii)
by investigating the implications of such behaviors on system and software
designs.
</summary>
    <author>
      <name>Ramyad Hadidi</name>
    </author>
    <author>
      <name>Bahar Asgari</name>
    </author>
    <author>
      <name>Jeffrey Young</name>
    </author>
    <author>
      <name>Burhan Ahmad Mudassar</name>
    </author>
    <author>
      <name>Kartikay Garg</name>
    </author>
    <author>
      <name>Tushar Krishna</name>
    </author>
    <author>
      <name>Hyesoon Kim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISPASS.2018.00018</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISPASS.2018.00018" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2018 IEEE International Symposium on Performance Analysis of
  Systems and Software (ISPASS)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.05399v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05399v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06909v1</id>
    <updated>2017-07-21T14:17:07Z</updated>
    <published>2017-07-21T14:17:07Z</published>
    <title>Redundant Logic Insertion and Fault Tolerance Improvement in
  Combinational Circuits</title>
    <summary>  This paper presents a novel method to identify and insert redundant logic
into a combinational circuit to improve its fault tolerance without having to
replicate the entire circuit as is the case with conventional redundancy
techniques. In this context, it is discussed how to estimate the fault masking
capability of a combinational circuit using the truth-cum-fault enumeration
table, and then it is shown how to identify the logic that can introduced to
add redundancy into the original circuit without affecting its native
functionality and with the aim of improving its fault tolerance though this
would involve some trade-off in the design metrics. However, care should be
taken while introducing redundant logic since redundant logic insertion may
give rise to new internal nodes and faults on those may impact the fault
tolerance of the resulting circuit. The combinational circuit that is
considered and its redundant counterparts are all implemented in semi-custom
design style using a 32/28nm CMOS digital cell library and their respective
design metrics and fault tolerances are compared.
</summary>
    <author>
      <name>P Balasubramanian</name>
    </author>
    <author>
      <name>R T Naayagi</name>
    </author>
    <link href="http://arxiv.org/abs/1707.06909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06913v1</id>
    <updated>2017-07-21T14:22:36Z</updated>
    <published>2017-07-21T14:22:36Z</published>
    <title>Mathematical Estimation of Logical Masking Capability of
  Majority/Minority Gates Used in Nanoelectronic Circuits</title>
    <summary>  In nanoelectronic circuit synthesis, the majority gate and the inverter form
the basic combinational logic primitives. This paper deduces the mathematical
formulae to estimate the logical masking capability of majority gates, which
are used extensively in nanoelectronic digital circuit synthesis. The
mathematical formulae derived to evaluate the logical masking capability of
majority gates holds well for minority gates, and a comparison with the logical
masking capability of conventional gates such as NOT, AND/NAND, OR/NOR, and
XOR/XNOR is provided. It is inferred from this research work that the logical
masking capability of majority/minority gates is similar to that of XOR/XNOR
gates, and with an increase of fan-in the logical masking capability of
majority/minority gates also increases.
</summary>
    <author>
      <name>P Balasubramanian</name>
    </author>
    <author>
      <name>R T Naayagi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Circuits, System and Simulation, pp.
  18-23, London, UK (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.06913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08134v1</id>
    <updated>2017-07-25T18:03:20Z</updated>
    <published>2017-07-25T18:03:20Z</published>
    <title>Optimizing Scrubbing by Netlist Analysis for FPGA Configuration Bit
  Classification and Floorplanning</title>
    <summary>  Existing scrubbing techniques for SEU mitigation on FPGAs do not guarantee an
error-free operation after SEU recovering if the affected configuration bits do
belong to feedback loops of the implemented circuits. In this paper, we a)
provide a netlist-based circuit analysis technique to distinguish so-called
critical configuration bits from essential bits in order to identify
configuration bits which will need also state-restoring actions after a
recovered SEU and which not. Furthermore, b) an alternative classification
approach using fault injection is developed in order to compare both
classification techniques. Moreover, c) we will propose a floorplanning
approach for reducing the effective number of scrubbed frames and d),
experimental results will give evidence that our optimization methodology not
only allows to detect errors earlier but also to minimize the
Mean-Time-To-Repair (MTTR) of a circuit considerably. In particular, we show
that by using our approach, the MTTR for datapath-intensive circuits can be
reduced by up to 48.5% in comparison to standard approaches.
</summary>
    <author>
      <name>Bernhard Schmidt</name>
    </author>
    <author>
      <name>Daniel Ziener</name>
    </author>
    <author>
      <name>Jürgen Teich</name>
    </author>
    <author>
      <name>Christian Zöllner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.vlsi.2017.06.012</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.vlsi.2017.06.012" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Integration, the VLSI Journal 59C (2017) pp. 98-108</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.08134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09450v1</id>
    <updated>2017-07-29T02:12:36Z</updated>
    <published>2017-07-29T02:12:36Z</published>
    <title>Address Translation Design Tradeoffs for Heterogeneous Systems</title>
    <summary>  This paper presents a broad, pathfinding design space exploration of memory
management units (MMUs) for heterogeneous systems. We consider a variety of
designs, ranging from accelerators tightly coupled with CPUs (and using their
MMUs) to fully independent accelerators that have their own MMUs. We find that
regardless of the CPU-accelerator communication, accelerators should not rely
on the CPU MMU for any aspect of address translation, and instead must have its
own, local, fully-fledged MMU. That MMU, however, can and should be as
application-specific as the accelerator itself, as our data indicates that even
a 100% hit rate in a small, standard L1 Translation Lookaside Buffer (TLB)
presents a substantial accelerator performance overhead. Furthermore, we
isolate the benefits of individual MMU components (e.g., TLBs versus page table
walkers) and discover that their relative performance, area, and energy are
workload dependent, with their interplay resulting in different area-optimal
and energy-optimal configurations.
</summary>
    <author>
      <name>Yunsung Kim</name>
    </author>
    <author>
      <name>Guilherme Cox</name>
    </author>
    <author>
      <name>Martha A. Kim</name>
    </author>
    <author>
      <name>Abhishek Bhattacharjee</name>
    </author>
    <link href="http://arxiv.org/abs/1707.09450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05116v1</id>
    <updated>2017-09-15T09:03:42Z</updated>
    <published>2017-09-15T09:03:42Z</published>
    <title>A Streaming Accelerator for Deep Convolutional Neural Networks with
  Image and Feature Decomposition for Resource-limited System Applications</title>
    <summary>  Deep convolutional neural networks (CNN) are widely used in modern artificial
intelligence (AI) and smart vision systems but also limited by computation
latency, throughput, and energy efficiency on a resource-limited scenario, such
as mobile devices, internet of things (IoT), unmanned aerial vehicles (UAV),
and so on. A hardware streaming architecture is proposed to accelerate
convolution and pooling computations for state-of-the-art deep CNNs. It is
optimized for energy efficiency by maximizing local data reuse to reduce
off-chip DRAM data access. In addition, image and feature decomposition
techniques are introduced to optimize memory access pattern for an arbitrary
size of image and number of features within limited on-chip SRAM capacity. A
prototype accelerator was implemented in TSMC 65 nm CMOS technology with 2.3 mm
x 0.8 mm core area, which achieves 144 GOPS peak throughput and 0.8 TOPS/W peak
energy efficiency.
</summary>
    <author>
      <name>Yuan Du</name>
    </author>
    <author>
      <name>Li Du</name>
    </author>
    <author>
      <name>Yilei Li</name>
    </author>
    <author>
      <name>Junjie Su</name>
    </author>
    <author>
      <name>Mau-Chung Frank Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.05116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07241v1</id>
    <updated>2017-09-21T10:03:07Z</updated>
    <published>2017-09-21T10:03:07Z</published>
    <title>Satisfiability Modulo Theory based Methodology for Floorplanning in VLSI
  Circuits</title>
    <summary>  This paper proposes a Satisfiability Modulo Theory based formulation for
floorplanning in VLSI circuits. The proposed approach allows a number of fixed
blocks to be placed within a layout region without overlapping and at the same
time minimizing the area of the layout region. The proposed approach is
extended to allow a number of fixed blocks with ability to rotate and flexible
blocks (with variable width and height) to be placed within a layout without
overlap. Our target in all cases is reduction in area occupied on a chip which
is of vital importance in obtaining a good circuit design. Satisfiability
Modulo Theory combines the problem of Boolean satisfiability with domains such
as convex optimization. Satisfiability Modulo Theory provides a richer modeling
language than is possible with pure Boolean SAT formulas. We have conducted our
experiments on MCNC and GSRC benchmark circuits to calculate the total area
occupied, amount of deadspace and the total CPU time consumed while placing the
blocks without overlapping. The results obtained shows clearly that the amount
of dead space or wasted space is reduced if rotation is applied to the blocks.
</summary>
    <author>
      <name>Suchandra Banerjee</name>
    </author>
    <author>
      <name>Anand Ratna</name>
    </author>
    <author>
      <name>Suchismita Roy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.07241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07529v1</id>
    <updated>2017-09-21T22:14:01Z</updated>
    <published>2017-09-21T22:14:01Z</published>
    <title>Energy-Efficient Wireless Interconnection Framework for Multichip
  Systems with In-package Memory Stacks</title>
    <summary>  Multichip systems with memory stacks and various processing chips are at the
heart of platform based designs such as servers and embedded systems. Full
utilization of the benefits of these integrated multichip systems need a
seamless, and scalable in-package interconnection framework. However,
state-of-the-art inter-chip communication requires long wireline channels which
increases energy consumption and latency while decreasing data bandwidth. Here,
we propose the design of an energy-efficient, seamless wireless interconnection
network for multichip systems. We demonstrate with cycle-accurate simulations
that such a design reduces the energy consumption and latency while increasing
the bandwidth in comparison to modern multichip integration systems.
</summary>
    <author>
      <name>Md Shahriar Shamim</name>
    </author>
    <author>
      <name>M Meraj Ahmed</name>
    </author>
    <author>
      <name>Naseef Mansoor</name>
    </author>
    <author>
      <name>Amlan Ganguly</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in proceedings of the 30th IEEE International
  System-on-Chip (SoC) Conference, pp. 272-277, Munich, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.07529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.04347v1</id>
    <updated>2017-10-12T02:56:37Z</updated>
    <published>2017-10-12T02:56:37Z</published>
    <title>NeuroTrainer: An Intelligent Memory Module for Deep Learning Training</title>
    <summary>  This paper presents, NeuroTrainer, an intelligent memory module with
in-memory accelerators that forms the building block of a scalable architecture
for energy efficient training for deep neural networks. The proposed
architecture is based on integration of a homogeneous computing substrate
composed of multiple processing engines in the logic layer of a 3D memory
module. NeuroTrainer utilizes a programmable data flow based execution model to
optimize memory mapping and data re-use during different phases of training
operation. A programming model and supporting architecture utilizes the
flexible data flow to efficiently accelerate training of various types of DNNs.
The cycle level simulation and synthesized design in 15nm FinFET showspower
efficiency of 500 GFLOPS/W, and almost similar throughput for a wide range of
DNNs including convolutional, recurrent, multi-layer-perceptron, and mixed
(CNN+RNN) networks
</summary>
    <author>
      <name>Duckhwan Kim</name>
    </author>
    <author>
      <name>Taesik Na</name>
    </author>
    <author>
      <name>Sudhakar Yalamanchili</name>
    </author>
    <author>
      <name>Saibal Mukhopadhyay</name>
    </author>
    <link href="http://arxiv.org/abs/1710.04347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.04347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.05154v2</id>
    <updated>2017-10-17T15:08:39Z</updated>
    <published>2017-10-14T09:17:04Z</published>
    <title>High Throughput 2D Spatial Image Filters on FPGAs</title>
    <summary>  FPGAs are well established in the signal processing domain, where their
fine-grained programmable nature allows the inherent parallelism in these
applications to be exploited for enhanced performance. As architectures have
evolved, FPGA vendors have added more heterogeneous resources to allow
often-used functions to be implemented with higher performance, at lower power
and using less area. DSP blocks, for example, have evolved from basic
multipliers to support the multiply-accumulate operations that are the core of
many signal processing tasks. While more features were added to DSP blocks,
their structure and connectivity has been optimised primarily for
one-dimensional signal processing. Basic operations in image processing are
similar, but performed in a two-dimensional structure, and hence, many of the
optimisations in newer DSP blocks are not exploited when mapping image
processing algorithms to them. We present a detailed study of two-dimensional
spatial filter implementation on FPGAs, showing how to maximise performance
through exploitation of DSP block capabilities, while also presenting a lean
border pixel management policy.
</summary>
    <author>
      <name>Abdullah Al-Dujaili</name>
    </author>
    <author>
      <name>Suhaib A. Fahmy</name>
    </author>
    <link href="http://arxiv.org/abs/1710.05154v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.05154v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.05470v1</id>
    <updated>2017-10-16T02:33:49Z</updated>
    <published>2017-10-16T02:33:49Z</published>
    <title>Asynchronous Early Output Section-Carry Based Carry Lookahead Adder with
  Alias Carry Logic</title>
    <summary>  A new asynchronous early output section-carry based carry lookahead adder
(SCBCLA) with alias carry output logic is presented in this paper. To evaluate
the proposed SCBCLA with alias carry logic and to make a comparison with other
CLAs, a 32-bit addition operation is considered. Compared to the
weak-indication SCBCLA with alias logic, the proposed early output SCBCLA with
alias logic reports a 13% reduction in area without any increases in latency
and power dissipation. On the other hand, in comparison with the early output
recursive CLA (RCLA), the proposed early output SCBCLA with alias logic reports
a 16% reduction in latency while occupying almost the same area and dissipating
almost the same average power. All the asynchronous CLAs are
quasi-delay-insensitive designs which incorporate the delay-insensitive
dual-rail data encoding and adhere to the 4-phase return-to-zero handshaking.
The adders were realized and the simulations were performed based on a 32/28nm
CMOS process.
</summary>
    <author>
      <name>P Balasubramanian</name>
    </author>
    <author>
      <name>C Dang</name>
    </author>
    <author>
      <name>D L Maskell</name>
    </author>
    <author>
      <name>K Prasad</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">P. Balasubramanian, C. Dang, D.L. Maskell, K. Prasad,
  "Asynchronous Early Output Section-Carry Based Carry Lookahead Adder with
  Alias Carry Logic," Proc. 30th International Conference on Microelectronics,
  pp. 293-298, 2017, Serbia</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1710.05470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.05470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.05474v1</id>
    <updated>2017-10-16T02:42:59Z</updated>
    <published>2017-10-16T02:42:59Z</published>
    <title>Approximate Ripple Carry and Carry Lookahead Adders - A Comparative
  Analysis</title>
    <summary>  Approximate ripple carry adders (RCAs) and carry lookahead adders (CLAs) are
presented which are compared with accurate RCAs and CLAs for performing a
32-bit addition. The accurate and approximate RCAs and CLAs are implemented
using a 32/28nm CMOS process. Approximations ranging from 4- to 20-bits are
considered for the less significant adder bit positions. The simulation results
show that approximate RCAs report reductions in the power-delay product (PDP)
ranging from 19.5% to 82% than the accurate RCA for approximation sizes varying
from 4- to 20-bits. Also, approximate CLAs report reductions in PDP ranging
from 16.7% to 74.2% than the accurate CLA for approximation sizes varying from
4- to 20-bits. On average, for the approximation sizes considered, it is
observed that approximate CLAs achieve a 46.5% reduction in PDP compared to the
approximate RCAs. Hence, approximate CLAs are preferable over approximate RCAs
for the low power implementation of approximate computer arithmetic.
</summary>
    <author>
      <name>P Balasubramanian</name>
    </author>
    <author>
      <name>C Dang</name>
    </author>
    <author>
      <name>D L Maskell</name>
    </author>
    <author>
      <name>K Prasad</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">P. Balasubramanian, C. Dang, D.L. Maskell, K. Prasad, "Approximate
  Ripple Carry and Carry Lookahead Adders - A Comparative Analysis," Proc. 30th
  International Conference on Microelectronics, pp. 299-304, 2017, Serbia</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1710.05474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.05474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08270v2</id>
    <updated>2017-10-24T21:15:48Z</updated>
    <published>2017-10-23T13:45:17Z</published>
    <title>Amorphous Dynamic Partial Reconfiguration with Flexible Boundaries to
  Remove Fragmentation</title>
    <summary>  Dynamic partial reconfiguration (DPR) allows one region of an
field-programmable gate array (FPGA) fabric to be reconfigured without
affecting the operations on the rest of the fabric. To use an FPGA as a
dynamically shared compute resource, one could partition and manage an FPGA
fabric as multiple DPR partitions that can be independently reconfigured at
runtime with different application function units (AFUs). Unfortunately,
dividing a fabric into DPR partitions with fixed boundaries causes the
available fabric resources to become fragmented. An AFU of a given size cannot
be loaded unless a sufficiently large DPR partition was floorplanned at build
time. To overcome this inefficiency, we devised an "amorphous" DPR technique
that is compatible with current device and tool support but does not require
the DPR partition boundaries to be a priori fixed. A collection of AFU
bitstreams can be simultaneously loaded on the fabric if their footprints (the
actual area used by an AFU) in the fabric do not overlap. We verified the
feasibility of amorphous DPR on Xilinx Zynq System-on-Chip (SoC) FPGAs using
Vivado. We evaluated the benefits of amorphous DPR in the context of a
dynamically reconfigurable vision processing pipeline framework.
</summary>
    <author>
      <name>Marie Nguyen</name>
    </author>
    <author>
      <name>James C. Hoe</name>
    </author>
    <link href="http://arxiv.org/abs/1710.08270v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08270v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10290v2</id>
    <updated>2020-08-14T19:24:23Z</updated>
    <published>2017-10-27T18:29:32Z</published>
    <title>Using Vivado-HLS for Structural Design: a NoC Case Study</title>
    <summary>  There have been ample successful examples of applying Xilinx Vivado's
"function-to-module" high-level synthesis (HLS) where the subject is
algorithmic in nature. In this work, we carried out a design study to assess
the effectiveness of applying Vivado-HLS in structural design. We employed
Vivado-HLS to synthesize C functions corresponding to standalone
network-on-chip (NoC) routers as well as complete multi-endpoint NoCs.
Interestingly, we find that describing a complete NoC comprising router
submodules faces fundamental difficulties not present in describing the routers
as standalone modules. Ultimately, we succeeded in using Vivado-HLS to produce
router and NoC modules that are exact cycle- and bit-accurate replacements of
our reference RTL-based router and NoC modules. Furthermore, the routers and
NoCs resulting from HLS and RTL are comparable in resource utilization and
critical path delay. Our experience subjectively suggests that HLS is able to
simplify the design effort even though much of the structural details had to be
provided in the HLS description through a combination of coding discipline and
explicit pragmas. The C++ source code can be found at
https://github.com/zhipengzhaocmu/HLS_NoC.
</summary>
    <author>
      <name>Zhipeng Zhao</name>
    </author>
    <author>
      <name>James C. Hoe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A poster with the same title was presented at the 2017 International
  Symposium on Field Programmable Gate Arrays</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10290v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10290v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.01021v2</id>
    <updated>2017-12-05T01:45:29Z</updated>
    <published>2017-12-04T11:43:58Z</published>
    <title>An 826 MOPS, 210 uW/MHz Unum ALU in 65 nm</title>
    <summary>  To overcome the limitations of conventional floating-point number formats, an
interval arithmetic and variable-width storage format called universal number
(unum) has been recently introduced. This paper presents the first (to the best
of our knowledge) silicon implementation measurements of an
application-specific integrated circuit (ASIC) for unum floating-point
arithmetic. The designed chip includes a 128-bit wide unum arithmetic unit to
execute additions and subtractions, while also supporting lossless (for
intermediate results) and lossy (for external data movements) compression units
to exploit the memory usage reduction potential of the unum format. Our chip,
fabricated in a 65 nm CMOS process, achieves a maximum clock frequency of 413
MHz at 1.2 V with an average measured power of 210 uW/MHz.
</summary>
    <author>
      <name>Florian Glaser</name>
    </author>
    <author>
      <name>Stefan Mach</name>
    </author>
    <author>
      <name>Abbas Rahimi</name>
    </author>
    <author>
      <name>Frank K. Gürkaynak</name>
    </author>
    <author>
      <name>Qiuting Huang</name>
    </author>
    <author>
      <name>Luca Benini</name>
    </author>
    <link href="http://arxiv.org/abs/1712.01021v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.01021v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03411v1</id>
    <updated>2017-12-09T16:46:08Z</updated>
    <published>2017-12-09T16:46:08Z</published>
    <title>FPGA with Improved Routability and Robustness in 130nm CMOS with
  Open-Source CAD Targetability</title>
    <summary>  This paper outlines an FPGA VLSI design methodology that was used to realize
a fully functioning FPGA chip in 130nm CMOS with improved routability and
memory robustness. The architectural design space exploration and synthesis
capability were enabled by the Verilog-to-Routing CAD tool. The capabilities of
this tool were extended to enable bitstream generation and deployment. To
validate the architecture and bitstream implementation, a Chisel (Constructing
Hardware in the Embedded Scala Language) model of the FPGA was created to
rapidly verify the microarchitectural details of the device prior to schematic
design. A custom carrier board and configuration tool were used to verify
correct operational characteristics of the FPGA over various resource
utilizations and clock frequencies.
</summary>
    <author>
      <name>Guanshun Yu</name>
    </author>
    <author>
      <name>Tom Y. Cheng</name>
    </author>
    <author>
      <name>Blayne Kettlewell</name>
    </author>
    <author>
      <name>Harrison Liew</name>
    </author>
    <author>
      <name>Mingoo Seok</name>
    </author>
    <author>
      <name>Peter R. Kinget</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 11 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.03411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03478v1</id>
    <updated>2017-12-10T07:44:56Z</updated>
    <published>2017-12-10T07:44:56Z</published>
    <title>A Scalable High-Performance Priority Encoder Using 1D-Array to 2D-Array
  Conversion</title>
    <summary>  In our prior study of an L-bit priority encoder (PE), a so-called
one-directional-array to two-directional-array conversion method is deployed to
turn an L-bit input data into an MxN-bit matrix. Following this, an N-bit PE
and an M-bit PE are employed to obtain a row index and column index. From
those, the highest priority bit of L-bit input data is achieved. This brief
extends our previous work to construct a scalable architecture of
high-performance large-sized PEs. An optimum pair of (M, N) and look-ahead
signal are proposed to improve the overall PE performance significantly. The
evaluation is achieved by implementing a variety of PEs whose L varies from
4-bit to 4096-bit in 180-nm CMOS technology. According to post-place-and-route
simulation results, at PE size of 64 bits, 256 bits, and 2048 bits the
operating frequencies reach 649 MHz, 520 MHz, and 370 MHz, which are 1.2 times,
1.5 times, and 1.4 times, as high as state-of-the-art ones.
</summary>
    <author>
      <name>Xuan-Thuan Nguyen</name>
    </author>
    <author>
      <name>Hong-Thu Nguyen</name>
    </author>
    <author>
      <name>Cong-Kha Pham</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCSII.2017.2672865</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCSII.2017.2672865" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Circuits and Systems II: Express Briefs (
  Volume: 64, Issue: 9, Sept. 2017 )</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.03478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.04902v1</id>
    <updated>2017-12-13T18:14:18Z</updated>
    <published>2017-12-13T18:14:18Z</published>
    <title>The microarchitecture of a multi-threaded RISC-V compliant processing
  core family for IoT end-nodes</title>
    <summary>  Internet-of-Things end-nodes demand low power processing platforms
characterized by heterogeneous dedicated units, controlled by a processor core
running concurrent control threads. Such architecture scheme fits one of the
main target application domain of the RISC-V instruction set. We present an
open-source processing core compliant with RISC-V on the software side and with
the popular Pulpino processor platform on the hardware side, while supporting
interleaved multi-threading for IoT applications. The latter feature is a novel
contribution in this application domain. We report details about the
microarchitecture design along with performance data.
</summary>
    <author>
      <name>Abdallah Cheikh</name>
    </author>
    <author>
      <name>Gianmarco Cerutti</name>
    </author>
    <author>
      <name>Antonio Mastrandrea</name>
    </author>
    <author>
      <name>Francesco Menichelli</name>
    </author>
    <author>
      <name>Mauro Olivieri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-93082-4_12</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-93082-4_12" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ApplePies 2017. Lecture Notes in Electrical Engineering, vol 512.
  2019. Springer</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.04902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.06497v1</id>
    <updated>2017-12-18T16:14:49Z</updated>
    <published>2017-12-18T16:14:49Z</published>
    <title>HERO: Heterogeneous Embedded Research Platform for Exploring RISC-V
  Manycore Accelerators on FPGA</title>
    <summary>  Heterogeneous embedded systems on chip (HESoCs) co-integrate a standard host
processor with programmable manycore accelerators (PMCAs) to combine
general-purpose computing with domain-specific, efficient processing
capabilities. While leading companies successfully advance their HESoC
products, research lags behind due to the challenges of building a prototyping
platform that unites an industry-standard host processor with an open research
PMCA architecture. In this work we introduce HERO, an FPGA-based research
platform that combines a PMCA composed of clusters of RISC-V cores, implemented
as soft cores on an FPGA fabric, with a hard ARM Cortex-A multicore host
processor. The PMCA architecture mapped on the FPGA is silicon-proven,
scalable, configurable, and fully modifiable. HERO includes a complete software
stack that consists of a heterogeneous cross-compilation toolchain with support
for OpenMP accelerator programming, a Linux driver, and runtime libraries for
both host and PMCA. HERO is designed to facilitate rapid exploration on all
software and hardware layers: run-time behavior can be accurately analyzed by
tracing events, and modifications can be validated through fully automated hard
ware and software builds and executed tests. We demonstrate the usefulness of
HERO by means of case studies from our research.
</summary>
    <author>
      <name>Andreas Kurth</name>
    </author>
    <author>
      <name>Pirmin Vogel</name>
    </author>
    <author>
      <name>Alessandro Capotondi</name>
    </author>
    <author>
      <name>Andrea Marongiu</name>
    </author>
    <author>
      <name>Luca Benini</name>
    </author>
    <link href="http://arxiv.org/abs/1712.06497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.06497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09818v1</id>
    <updated>2017-12-28T10:46:05Z</updated>
    <published>2017-12-28T10:46:05Z</published>
    <title>Automated Formal Equivalence Verification of Pipelined Nested Loops in
  Datapath Designs</title>
    <summary>  In this paper, we present an efficient formal approach to check the
equivalence of synthesized RTL against the high-level specification in the
presence of pipelining transformations. To increase the scalability of our
proposed method, we dynamically divide the designs into several smaller parts
called segments by introducing cut-points. Then we employ Modular Horner
Expansion Diagram (M-HED) to check whether the specification and implementation
are equivalent or not. In an iterative manner, the equivalence checking for
each segment is performed. At each step, the equivalent nodes and those nodes
which have an impact on them are removed until the whole design is covered. Our
proposed method enables us to deal with the equivalence checking problem for
behaviorally synthesized designs even in the presence of pipelines for nested
loops. The empirical results demonstrate the efficiency and scalability of our
proposed method in terms of run-time and memory usage for several large designs
synthesized by a commercial behavioral synthesis tool. Average improvements in
terms of the memory usage and run time in comparison with SMT- and SAT-based
equivalence checking are 16.7x and 111.9x, respectively.
</summary>
    <author>
      <name>Payman Behnam</name>
    </author>
    <author>
      <name>Bijan Alizadeh</name>
    </author>
    <author>
      <name>Sajjad Taheri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 20 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.09818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00472v1</id>
    <updated>2018-01-01T16:50:09Z</updated>
    <published>2018-01-01T16:50:09Z</published>
    <title>Auto-Generation of Pipelined Hardware Designs for Polar Encoder</title>
    <summary>  This paper presents a general framework for auto-generation of pipelined
polar encoder architectures. The proposed framework could be well represented
by a general formula. Given arbitrary code length $N$ and the level of
parallelism $M$, the formula could specify the corresponding hardware
architecture. We have written a compiler which could read the formula and then
automatically generate its register-transfer level (RTL) description suitable
for FPGA or ASIC implementation. With this hardware generation system, one
could explore the design space and make a trade-off between cost and
performance. Our experimental results have demonstrated the efficiency of this
auto-generator for polar encoder architectures.
</summary>
    <author>
      <name>Zhiwei Zhong</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab of Efficient Architectures for Digital-communication and Signal-processing</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Mobile Communications Research Laboratory, Southeast University, Nanjing, China</arxiv:affiliation>
    </author>
    <author>
      <name>Xiaohu You</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Mobile Communications Research Laboratory, Southeast University, Nanjing, China</arxiv:affiliation>
    </author>
    <author>
      <name>Chuan Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab of Efficient Architectures for Digital-communication and Signal-processing</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Mobile Communications Research Laboratory, Southeast University, Nanjing, China</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1801.00472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03712v1</id>
    <updated>2018-01-11T11:15:31Z</updated>
    <published>2018-01-11T11:15:31Z</published>
    <title>A Software-defined SoC Memory Bus Bridge Architecture for Disaggregated
  Computing</title>
    <summary>  Disaggregation and rack-scale systems have the potential of drastically
decreasing TCO and increasing utilization of cloud datacenters, while
maintaining performance. While the concept of organising resources in separate
pools and interconnecting them together on demand is straightforward, its
materialisation can be radically different in terms of performance and scale
potential.
  In this paper, we present a memory bus bridge architecture which enables
communication between 100s of masters and slaves in todays complex
multiprocessor SoCs, that are physically intregrated in different chips and
even different mainboards. The bridge tightly couples serial transceivers and a
circuit network for chip-to-chip transfers. A key property of the proposed
bridge architecture is that it is software-defined and thus can be configured
at runtime, via a software control plane, to prepare and steer memory access
transactions to remote slaves. This is particularly important because it
enables datacenter orchestration tools to manage the disaggregated resource
allocation. Moreover, we evaluate a bridge prototype we have build for ARM AXI4
memory bus interconnect and we discuss application-level observed performance.
</summary>
    <author>
      <name>Dimitris Syrivelis</name>
    </author>
    <author>
      <name>Andrea Reale</name>
    </author>
    <author>
      <name>Kostas Katrinis</name>
    </author>
    <author>
      <name>Christian Pinto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3rd International Workshop on Advanced Interconnect Solutions and
  Technologies for Emerging Computing Systems (AISTECS 2018, part of HiPEAC
  2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.03712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05178v1</id>
    <updated>2018-01-16T09:38:46Z</updated>
    <published>2018-01-16T09:38:46Z</published>
    <title>Inter-thread Communication in Multithreaded, Reconfigurable Coarse-grain
  Arrays</title>
    <summary>  Traditional von Neumann GPGPUs only allow threads to communicate through
memory on a group-to-group basis. In this model, a group of producer threads
writes intermediate values to memory, which are read by a group of consumer
threads after a barrier synchronization. To alleviate the memory bandwidth
imposed by this method of communication, GPGPUs provide a small scratchpad
memory that prevents intermediate values from overloading DRAM bandwidth. In
this paper we introduce direct inter-thread communications for massively
multithreaded CGRAs, where intermediate values are communicated directly
through the compute fabric on a point-to-point basis. This method avoids the
need to write values to memory, eliminates the need for a dedicated scratchpad,
and avoids workgroup-global barriers. The paper introduces the programming
model (CUDA) and execution model extensions, as well as the hardware primitives
that facilitate the communication. Our simulations of Rodinia benchmarks
running on the new system show that direct inter-thread communication provides
an average speedup of 4.5x (13.5x max) and reduces system power by an average
of 7x (33x max), when compared to an equivalent Nvidia GPGPU.
</summary>
    <author>
      <name>Dani Voitsechov</name>
    </author>
    <author>
      <name>Yoav Etsion</name>
    </author>
    <link href="http://arxiv.org/abs/1801.05178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00580v1</id>
    <updated>2018-02-02T07:02:19Z</updated>
    <published>2018-02-02T07:02:19Z</published>
    <title>A Multi-Kernel Multi-Code Polar Decoder Architecture</title>
    <summary>  Polar codes have received increasing attention in the past decade, and have
been selected for the next generation of wireless communication standard. Most
research on polar codes has focused on codes constructed from a $2\times2$
polarization matrix, called binary kernel: codes constructed from binary
kernels have code lengths that are bound to powers of $2$. A few recent works
have proposed construction methods based on multiple kernels of different
dimensions, not only binary ones, allowing code lengths different from powers
of $2$. In this work, we design and implement the first multi-kernel successive
cancellation polar code decoder in literature. It can decode any code
constructed with binary and ternary kernels: the architecture, sized for a
maximum code length $N_{max}$, is fully flexible in terms of code length, code
rate and kernel sequence. The decoder can achieve frequency of more than $1$
GHz in $65$ nm CMOS technology, and a throughput of $615$ Mb/s. The area
occupation ranges between $0.11$ mm$^2$ for $N_{max}=256$ and $2.01$ mm$^2$ for
$N_{max}=4096$. Implementation results show an unprecedented degree of
flexibility: with $N_{max}=4096$, up to $55$ code lengths can be decoded with
the same hardware, along with any kernel sequence and code rate.
</summary>
    <author>
      <name>Gabriele Coppolino</name>
    </author>
    <author>
      <name>Carlo Condo</name>
    </author>
    <author>
      <name>Guido Masera</name>
    </author>
    <author>
      <name>Warren J. Gross</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05100v1</id>
    <updated>2018-02-12T05:13:16Z</updated>
    <published>2018-02-12T05:13:16Z</published>
    <title>SAPA: Self-Aware Polymorphic Architecture</title>
    <summary>  In this work, we introduce a Self-Aware Polymorphic Architecture (SAPA)
design approach to support emerging context-aware applications and mitigate the
programming challenges caused by the ever-increasing complexity and
heterogeneity of high performance computing systems. Through the SAPA design,
we examined the salient software-hardware features of adaptive computing
systems that allow for (1) the dynamic allocation of computing resources
depending on program needs (e.g., the amount of parallelism in the program) and
(2) automatic approximation to meet program and system goals (e.g., execution
time budget, power constraints and computation resiliency) without the
programming complexity of current multicore and many-core systems. The proposed
adaptive computer architecture framework applies machine learning algorithms
and control theory techniques to the application execution based on information
collected about the system runtime performance trade-offs. It has heterogeneous
reconfigurable cores with fast hardware-level migration capability,
self-organizing memory structures and hierarchies, an adaptive
application-aware network-on-chip, and a built-in hardware layer for dynamic,
autonomous resource management. Our prototyped architecture performs extremely
well on a large pool of applications.
</summary>
    <author>
      <name>Michel A. Kinsy</name>
    </author>
    <author>
      <name>Mihailo Isakov</name>
    </author>
    <author>
      <name>Alan Ehret</name>
    </author>
    <author>
      <name>Donato Kava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Boston Area Architecture 2018 Workshop (BARC18)</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.05100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.02490v1</id>
    <updated>2018-03-07T00:37:24Z</updated>
    <published>2018-03-07T00:37:24Z</published>
    <title>Adaptive 3D-IC TSV Fault Tolerance Structure Generation</title>
    <summary>  In three dimensional integrated circuits (3D-ICs), through silicon via (TSV)
is a critical technique in providing vertical connections. However, the yield
and reliability is one of the key obstacles to adopt the TSV based 3D-ICs
technology in industry. Various fault-tolerance structures using spare TSVs to
repair faulty functional TSVs have been proposed in literature for yield and
reliability enhancement, but a valid structure cannot always be found due to
the lack of effective generation methods for fault-tolerance structures. In
this paper, we focus on the problem of adaptive fault-tolerance structure
generation. Given the relations between functional TSVs and spare TSVs, we
first calculate the maximum number of tolerant faults in each TSV group. Then
we propose an integer linear programming (ILP) based model to construct
adaptive fault-tolerance struc- ture with minimal multiplexer delay overhead
and hardware cost. We further develop a speed-up technique through efficient
min-cost-max-flow (MCMF) model. All the proposed method- ologies are embedded
in a top-down TSV planning framework to form functional TSV groups and generate
adaptive fault- tolerance structures. Experimental results show that, compared
with state-of-the-art, the number of spare TSVs used for fault tolerance can be
effectively reduced.
</summary>
    <author>
      <name>Song Chen</name>
    </author>
    <author>
      <name>Qi Xu</name>
    </author>
    <author>
      <name>Bei Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Trans. on CAD</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.02490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.02490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.03790v1</id>
    <updated>2018-03-10T10:33:28Z</updated>
    <published>2018-03-10T10:33:28Z</published>
    <title>Towards a Multi-array Architecture for Accelerating Large-scale Matrix
  Multiplication on FPGAs</title>
    <summary>  Large-scale floating-point matrix multiplication is a fundamental kernel in
many scientific and engineering applications. Most existing work only focus on
accelerating matrix multiplication on FPGA by adopting a linear systolic array.
This paper towards the extension of this architecture by proposing a scalable
and highly configurable multi-array architecture. In addition, we propose a
work-stealing scheme to ensure the equality in the workload partition among
multiple linear arrays. Furthermore, an analytical model is developed to
determine the optimal design parameters. Experiments on a real-life
convolutional neural network (CNN) show that we can obtain the optimal
extension of the linear array architecture.
</summary>
    <author>
      <name>Junzhong Shen</name>
    </author>
    <author>
      <name>Yuran Qiao</name>
    </author>
    <author>
      <name>You Huang</name>
    </author>
    <author>
      <name>Mei Wen</name>
    </author>
    <author>
      <name>Chunyuan Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepet by IEEE International Symposium on
  Circuits and Systems (ISCAS 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.03790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.03790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.07613v1</id>
    <updated>2018-03-20T19:22:27Z</updated>
    <published>2018-03-20T19:22:27Z</published>
    <title>Integrating DRAM Power-Down Modes in gem5 and Quantifying their Impact</title>
    <summary>  Across applications, DRAM is a significant contributor to the overall system
power, with the DRAM access energy per bit up to three orders of magnitude
higher compared to on-chip memory accesses. To improve the power efficiency,
DRAM technology incorporates multiple power-down modes, each with different
trade-offs between achievable power savings and performance impact due to entry
and exit delay requirements. Accurate modeling of these low power modes and
entry and exit control is crucial to analyze the trade-offs across controller
configurations and workloads with varied memory access characteristics. To
address this, we integrate the power-down modes into the DRAM controller model
in the open-source simulator gem5. This is the first publicly available
full-system simulator with DRAM power-down modes, providing the research
community a tool for DRAM power analysis for a breadth of use cases. We
validate the power-down functionality with sweep tests, which trigger defined
memory access characteristics. We further evaluate the model with real HPC
workloads, illustrating the value of integrating low power functionality into a
full system simulator.
</summary>
    <author>
      <name>Radhika Jagtap</name>
    </author>
    <author>
      <name>Matthias Jung</name>
    </author>
    <author>
      <name>Wendy Elsasser</name>
    </author>
    <author>
      <name>Christian Weis</name>
    </author>
    <author>
      <name>Andreas Hansson</name>
    </author>
    <author>
      <name>Norbert Wehn</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3132402.3132444</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3132402.3132444" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of MEMSYS 2017, Alexandria, VA, USA, October 2,
  2017, 10 pages, ACM</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.07613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00776v1</id>
    <updated>2018-06-03T11:58:27Z</updated>
    <published>2018-06-03T11:58:27Z</published>
    <title>Supporting Superpages and Lightweight Page Migration in Hybrid Memory
  Systems</title>
    <summary>  Superpages have long been used to mitigate address translation overhead in
big memory systems. However, superpages often preclude lightweight page
migration, which is crucial for performance and energy efficiency in hybrid
memory systems composed of DRAM and non-volatile memory (NVM). In this paper,
we propose a novel memory management mechanism called \textit{Rainbow} to
bridge this fundamental conflict between superpages and lightweight page
migration. \textit{Rainbow} manages NVM at the superpage granularity, and uses
DRAM to cache frequently-accessed (hot) small pages in each superpage.
Correspondingly, \textit{Rainbow} utilizes split TLBs to support different page
sizes. By introducing an efficient hot page identification mechanism and a
novel NVM-to-DRAM address remapping mechanism, \textit{Rainbow} supports
lightweight page migration while without splintering superpages. Experimental
results show that Rainbow can significantly reduce applications' TLB misses by
99.8\%, and improve application performance (IPC) by up to 2.9X (43.0\% on
average) when compared to a state-of-the-art memory migration policy without
superpage support.
</summary>
    <author>
      <name>Xiaoyuan Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00779v1</id>
    <updated>2018-06-03T12:29:26Z</updated>
    <published>2018-06-03T12:29:26Z</published>
    <title>Gemini: Reducing DRAM Cache Hit Latency by Hybrid Mappings</title>
    <summary>  Die-stacked DRAM caches are increasingly advocated to bridge the performance
gap between on-chip Cache and main memory. It is essential to improve DRAM
cache hit rate and lower cache hit latency simultaneously. Prior DRAM cache
designs fall into two categories according to the data mapping polices:
set-associative and direct-mapped, achieving either one. In this paper, we
propose a partial direct-mapped die-stacked DRAM cache to achieve the both
objectives simultaneously, called Gemini, which is motivated by the following
observations: applying unified mapping policy to different blocks cannot
achieve high cache hit rate and low hit latency in terms of mapping structure.
Gemini cache classifies data into leading blocks and following blocks, and
places them with static mapping and dynamic mapping respectively in a unified
set-associative structure. Gemini also designs a replacement policy to balance
the different blocks miss penalty and the recency, and provides strategies to
mitigate cache thrashing due to block type transitions. Experimental results
demonstrate that Gemini cache can narrow the hit latency gap with direct-mapped
cache significantly, from 1.75X to 1.22X on average, and can achieve comparable
hit rate with set-associative cache. Compared with the state-of-the-art
baselines, i.e., enhanced Loh-Hill cache, Gemini improves the IPC by up to 20%
respectively.
</summary>
    <author>
      <name>Ye Chi</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02271v1</id>
    <updated>2018-05-25T14:20:54Z</updated>
    <published>2018-05-25T14:20:54Z</published>
    <title>Data-Dependent Clock Gating approach for Low Power Sequential System</title>
    <summary>  Power dissipation in the sequential systems of modern CPU integrated chips
(CPU-IC viz., Silicon Chip) is in discussion since the last decade. Researchers
have been cultivating many low power design methods to choose the best
potential candidate for reducing both static and dynamic power of a chip.
Though, clock gating (CG) has been an accepted technique to control dynamic
power dissipation, question still loiters on its credibility to handle the
static power of the system. Therefore in this paper, we have revisited the
popular CG schemes and found out some scope of improvisation to support the
simultaneous reduction of static and dynamic power dissipation. Our proposed CG
is simulated for 90nm CMOS using Cadence Virtuoso and has been tested on a
conventional Master-Slave Flip-flop at 5GHz clock with a power supply of
1.1Volt. This assignment clearly depicts its supremacy in terms of power and
timing metrics in comparison to the implementation of existing CG schemes.
</summary>
    <author>
      <name>Dhiraj Sarkar</name>
    </author>
    <author>
      <name>Pritam Bhattacharjee</name>
    </author>
    <author>
      <name>Alak Majumder</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08095v1</id>
    <updated>2018-06-21T07:35:37Z</updated>
    <published>2018-06-21T07:35:37Z</published>
    <title>Generic and Universal Parallel Matrix Summation with a Flexible
  Compression Goal for Xilinx FPGAs</title>
    <summary>  Bit matrix compression is a highly relevant operation in computer arithmetic.
Essentially being a multi-operand addition, it is the key operation behind fast
multiplication and many higher-level operations such as multiply-accumulate,
the computation of the dot product or the implementation of FIR filters.
Compressor implementations have been constantly evolving for greater efficiency
both in general and in the context of concrete applications or specific
implementation technologies. This paper is building on this history and
describes a generic implementation of a bit matrix compressor for Xilinx FPGAs,
which does not require a generator tool. It contributes FPGA-oriented metrics
for the evaluation of elementary parallel bit counters, a systematic analysis
and partial decomposition of previously proposed counters and a fully
implemented construction heuristic with a flexible compression target matching
the device capabilities. The generic implementation is agnostic of the aspect
ratio of the input matrix and can be used for multiplication the same way as it
can be for single-column population count operations.
</summary>
    <author>
      <name>Thomas B. Preußer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23919/FPL.2017.8056834</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23919/FPL.2017.8056834" rel="related"/>
    <link href="http://arxiv.org/abs/1806.08095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08862v1</id>
    <updated>2018-06-22T21:30:05Z</updated>
    <published>2018-06-22T21:30:05Z</published>
    <title>BISMO: A Scalable Bit-Serial Matrix Multiplication Overlay for
  Reconfigurable Computing</title>
    <summary>  Matrix-matrix multiplication is a key computational kernel for numerous
applications in science and engineering, with ample parallelism and data
locality that lends itself well to high-performance implementations. Many
matrix multiplication-dependent applications can use reduced-precision integer
or fixed-point representations to increase their performance and energy
efficiency while still offering adequate quality of results. However, precision
requirements may vary between different application phases or depend on input
data, rendering constant-precision solutions ineffective. We present BISMO, a
vectorized bit-serial matrix multiplication overlay for reconfigurable
computing. BISMO utilizes the excellent binary-operation performance of FPGAs
to offer a matrix multiplication performance that scales with required
precision and parallelism. We characterize the resource usage and performance
of BISMO across a range of parameters to build a hardware cost model, and
demonstrate a peak performance of 6.5 TOPS on the Xilinx PYNQ-Z1 board.
</summary>
    <author>
      <name>Yaman Umuroglu</name>
    </author>
    <author>
      <name>Lahiru Rasnayake</name>
    </author>
    <author>
      <name>Magnus Sjalander</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at FPL'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.08862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04013v1</id>
    <updated>2018-07-11T09:06:20Z</updated>
    <published>2018-07-11T09:06:20Z</published>
    <title>Medusa: A Scalable Interconnect for Many-Port DNN Accelerators and Wide
  DRAM Controller Interfaces</title>
    <summary>  To cope with the increasing demand and computational intensity of deep neural
networks (DNNs), industry and academia have turned to accelerator technologies.
In particular, FPGAs have been shown to provide a good balance between
performance and energy efficiency for accelerating DNNs. While significant
research has focused on how to build efficient layer processors, the
computational building blocks of DNN accelerators, relatively little attention
has been paid to the on-chip interconnects that sit between the layer
processors and the FPGA's DRAM controller.
  We observe a disparity between DNN accelerator interfaces, which tend to
comprise many narrow ports, and FPGA DRAM controller interfaces, which tend to
be wide buses. This mismatch causes traditional interconnects to consume
significant FPGA resources. To address this problem, we designed Medusa: an
optimized FPGA memory interconnect which transposes data in the interconnect
fabric, tailoring the interconnect to the needs of DNN layer processors.
Compared to a traditional FPGA interconnect, our design can reduce LUT and FF
use by 4.7x and 6.0x, and improves frequency by 1.8x.
</summary>
    <author>
      <name>Yongming Shen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stony Brook University</arxiv:affiliation>
    </author>
    <author>
      <name>Tianchu Ji</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stony Brook University</arxiv:affiliation>
    </author>
    <author>
      <name>Michael Ferdman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stony Brook University</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Milder</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Stony Brook University</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1807.04013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07023v2</id>
    <updated>2018-10-16T04:37:28Z</updated>
    <published>2018-07-18T16:23:26Z</published>
    <title>Cross-layer Optimization for High Speed Adders: A Pareto Driven Machine
  Learning Approach</title>
    <summary>  In spite of maturity to the modern electronic design automation (EDA) tools,
optimized designs at architectural stage may become sub-optimal after going
through physical design flow. Adder design has been such a long studied
fundamental problem in VLSI industry yet designers cannot achieve optimal
solutions by running EDA tools on the set of available prefix adder
architectures. In this paper, we enhance a state-of-the-art prefix adder
synthesis algorithm to obtain a much wider solution space in architectural
domain. On top of that, a machine learning-based design space exploration
methodology is applied to predict the Pareto frontier of the adders in physical
domain, which is infeasible by exhaustively running EDA tools for innumerable
architectural solutions. Considering the high cost of obtaining the true values
for learning, an active learning algorithm is utilized to select the
representative data during learning process, which uses less labeled data while
achieving better quality of Pareto frontier. Experimental results demonstrate
that our framework can achieve Pareto frontier of high quality over a wide
design space, bridging the gap between architectural and physical designs.
</summary>
    <author>
      <name>Yuzhe Ma</name>
    </author>
    <author>
      <name>Subhendu Roy</name>
    </author>
    <author>
      <name>Jin Miao</name>
    </author>
    <author>
      <name>Jiamin Chen</name>
    </author>
    <author>
      <name>Bei Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07023v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07023v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.2; I.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11396v1</id>
    <updated>2018-07-30T15:34:36Z</updated>
    <published>2018-07-30T15:34:36Z</published>
    <title>Standard Cell Library Design and Optimization Methodology for ASAP7 PDK</title>
    <summary>  Standard cell libraries are the foundation for the entire backend design and
optimization flow in modern application-specific integrated circuit designs. At
7nm technology node and beyond, standard cell library design and optimization
is becoming increasingly difficult due to extremely complex design constraints,
as described in the ASAP7 process design kit (PDK). Notable complexities
include discrete transistor sizing due to FinFETs, complicated design rules
from lithography and restrictive layout space from modern standard cell
architectures. The design methodology presented in this paper enables efficient
and high-quality standard cell library design and optimization with the ASAP7
PDK. The key techniques include exhaustive transistor sizing for cell timing
optimization, transistor placement with generalized Euler paths and back-end
design prototyping for library-level explorations.
</summary>
    <author>
      <name>Xiaoqing Xu</name>
    </author>
    <author>
      <name>Nishi Shah</name>
    </author>
    <author>
      <name>Andrew Evans</name>
    </author>
    <author>
      <name>Saurabh Sinha</name>
    </author>
    <author>
      <name>Brian Cline</name>
    </author>
    <author>
      <name>Greg Yeric</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 papes, ICCAD 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00650v2</id>
    <updated>2018-08-15T13:08:55Z</updated>
    <published>2018-08-02T03:20:01Z</published>
    <title>The BaseJump Manycore Accelerator Network</title>
    <summary>  The BaseJump Manycore Accelerator-Network is an open source mesh-based
On-Chip-Network which is designed leveraging the Bespoke Silicon Group's 20+
years of experience in designing manycore architectures. It has been used in
the 16nm 511-core RISC-V compatible Celerity chip Davidson et al. (2018),
forming the basis of both a 1 GHz 496-core RISC-V manycore and a 10-core
always-on low voltage complex. It was also used in the 180nm BSG Ten chip,
which featured ten cores and a mesh that extends over off-chip links to an
FPGA. To facilitate use by the open source community of the BaseJump Manycore
network, we explain the ideas, protocols, interfaces and potential uses of the
mesh network. We also show an example with source code that demonstrates how to
integrate user designs into the mesh network.
</summary>
    <author>
      <name>Shaolin Xie</name>
    </author>
    <author>
      <name>Michael Bedford Taylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report of Bespoke Silicon Group, University of Washington.
  http://bjump.org/manycore</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00650v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00650v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.04864v1</id>
    <updated>2018-08-14T19:07:46Z</updated>
    <published>2018-08-14T19:07:46Z</published>
    <title>Scale-Out Processors &amp; Energy Efficiency</title>
    <summary>  Scale-out workloads like media streaming or Web search serve millions of
users and operate on a massive amount of data, and hence, require enormous
computational power. As the number of users is increasing and the size of data
is expanding, even more computational power is necessary for powering up such
workloads. Data centers with thousands of servers are providing the
computational power necessary for executing scale-out workloads. As operating
data centers requires enormous capital outlay, it is important to optimize them
to execute scale-out workloads efficiently. Server processors contribute
significantly to the data center capital outlay, and hence, are a prime
candidate for optimizations. While data centers are constrained with power, and
power consumption is one of the major components contributing to the total cost
of ownership (TCO), a recently-introduced scale-out design methodology
optimizes server processors for data centers using performance per unit area.
In this work, we use a more relevant performance-per-power metric as the
optimization criterion for optimizing server processors and reevaluate the
scale-out design methodology. Interestingly, we show that a scale-out processor
that delivers the maximum performance per unit area, also delivers the highest
performance per unit power.
</summary>
    <author>
      <name>Pouya Esmaili-Dokht</name>
    </author>
    <author>
      <name>Mohammad Bakhshalipour</name>
    </author>
    <author>
      <name>Behnam Khodabandeloo</name>
    </author>
    <author>
      <name>Pejman Lotfi-Kamran</name>
    </author>
    <author>
      <name>Hamid Sarbazi-Azad</name>
    </author>
    <link href="http://arxiv.org/abs/1808.04864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.04864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06334v4</id>
    <updated>2019-08-02T00:45:12Z</updated>
    <published>2018-08-20T07:57:47Z</published>
    <title>Wrangling Rogues: Managing Experimental Post-Moore Architectures</title>
    <summary>  The Rogues Gallery is a new experimental testbed that is focused on tackling
"rogue" architectures for the Post-Moore era of computing. While some of these
devices have roots in the embedded and high-performance computing spaces,
managing current and emerging technologies provides a challenge for system
administration that are not always foreseen in traditional data center
environments.
  We present an overview of the motivations and design of the initial Rogues
Gallery testbed and cover some of the unique challenges that we have seen and
foresee with upcoming hardware prototypes for future post-Moore research.
Specifically, we cover the networking, identity management, scheduling of
resources, and tools and sensor access aspects of the Rogues Gallery and
techniques we have developed to manage these new platforms.
</summary>
    <author>
      <name>Will Powell</name>
    </author>
    <author>
      <name>Jason Riedy</name>
    </author>
    <author>
      <name>Jeffrey S. Young</name>
    </author>
    <author>
      <name>Thomas M. Conte</name>
    </author>
    <link href="http://arxiv.org/abs/1808.06334v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06334v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09651v1</id>
    <updated>2018-08-29T06:17:07Z</updated>
    <published>2018-08-29T06:17:07Z</published>
    <title>Implications of Integrated CPU-GPU Processors on Thermal and Power
  Management Techniques</title>
    <summary>  Heterogeneous processors with architecturally different cores (CPU and GPU)
integrated on the same die lead to new challenges and opportunities for thermal
and power management techniques because of shared thermal/power budgets between
these cores. In this paper, we show that new parallel programming paradigms
(e.g., OpenCL) for CPU-GPU processors create a tighter coupling between the
workload, the thermal/power management unit and the operating system. Using
detailed thermal and power maps of the die from infrared imaging, we
demonstrate that in contrast to traditional multi-core CPUs, heterogeneous
processors exhibit higher coupled behavior for dynamic voltage and frequency
scaling and workload scheduling, in terms of their effect on performance,
power, and temperature. Further, we show that by taking the differences in core
architectures and relative proximity of different computing cores on the die
into consideration, better scheduling schemes could be implemented to reduce
both the power density and peak temperature of the die. The findings presented
in the paper can be used to improve thermal and power efficiency of
heterogeneous CPU-GPU processors.
</summary>
    <author>
      <name>Kapil Dev</name>
    </author>
    <author>
      <name>Indrani Paul</name>
    </author>
    <author>
      <name>Wei Huang</name>
    </author>
    <author>
      <name>Yasuko Eckert</name>
    </author>
    <author>
      <name>Wayne Burleson</name>
    </author>
    <author>
      <name>Sherief Reda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.01115v1</id>
    <updated>2018-10-02T08:34:26Z</updated>
    <published>2018-10-02T08:34:26Z</published>
    <title>Performance Comparison of some Synchronous Adders</title>
    <summary>  This technical note compares the performance of some synchronous adders which
correspond to the following architectures: i) ripple carry adder (RCA), ii)
recursive carry lookahead adder (RCLA), iii) hybrid RCLA-RCA with the RCA used
in the least significant adder bit positions, iv) block carry lookahead adder
(BCLA), v) hybrid BCLA-RCA with the RCA used in the least significant adder bit
positions, and vi) non-uniform input partitioned carry select adders (CSLAs)
without and with the binary to excess-1 code (BEC) converter. The 32-bit
addition was considered as an example operation. The adder architectures
mentioned were implemented by targeting a typical case PVT specification (high
threshold voltage, supply voltage of 1.05V and operating temperature of 25
degrees Celsius) of the Synopsys 32/28nm CMOS technology. The comparison leads
to the following observations: i) the hybrid CCLA-RCA is preferable to the
other adders in terms of the speed, the power-delay product, and the
energy-delay product, ii) the non-uniform input partitioned CSLA without the
BEC converter is preferable to the other adders in terms of the area-delay
product, and iii) the RCA incorporating the full adder present in the standard
digital cell library is preferable to the other adders in terms of the
power-delay-area product.
</summary>
    <author>
      <name>P Balasubramanian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.01115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.01115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.06472v1</id>
    <updated>2018-10-15T15:44:27Z</updated>
    <published>2018-10-15T15:44:27Z</published>
    <title>Memory Vulnerability: A Case for Delaying Error Reporting</title>
    <summary>  To face future reliability challenges, it is necessary to quantify the risk
of error in any part of a computing system. To this goal, the Architectural
Vulnerability Factor (AVF) has long been used for chips. However, this metric
is used for offline characterisation, which is inappropriate for memory. We
survey the literature and formalise one of the metrics used, the Memory
Vulnerability Factor, and extend it to take into account false errors. These
are reported errors which would have no impact on the program if they were
ignored. We measure the False Error Aware MVF (FEA) and related metrics
precisely in a cycle-accurate simulator, and compare them with the effects of
injecting faults in a program's data, in native parallel runs. Our findings
show that MVF and FEA are the only two metrics that are safe to use at runtime,
as they both consistently give an upper bound on the probability of incorrect
program outcome. FEA gives a tighter bound than MVF, and is the metric that
correlates best with the incorrect outcome probability of all considered
metrics.
</summary>
    <author>
      <name>Luc Jaulmes</name>
    </author>
    <author>
      <name>Miquel Moretó</name>
    </author>
    <author>
      <name>Mateo Valero</name>
    </author>
    <author>
      <name>Marc Casas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IOLTS.2019.8854397</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IOLTS.2019.8854397" rel="related"/>
    <link href="http://arxiv.org/abs/1810.06472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.06472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.06885v1</id>
    <updated>2018-10-16T08:58:19Z</updated>
    <published>2018-10-16T08:58:19Z</published>
    <title>An Area Efficient 2D Fourier Transform Architecture for FPGA
  Implementation</title>
    <summary>  Two-dimensional Fourier transform plays a significant role in a variety of
image processing problems, such as medical image processing, digital
holography, correlation pattern recognition, hybrid digital optical processing,
optical computing etc. 2D spatial Fourier transformation involves large number
of image samples and hence it requires huge hardware resources of field
programmable gate arrays (FPGA). In this paper, we present an area efficient
architecture of 2D FFT processor that reuses the butterfly units multiple
times. This is achieved by using a control unit that sends back the previous
computed data of N/2 butterfly units to itself for {log_2(N) - 1} times. A RAM
controller is used to synchronize the flow of data samples between the
functional blocks.The 2D FFT processor is simulated by VHDL and the results are
verified on a Virtex-6 FPGA. The proposed method outperforms the conventional
NxN point 2D FFT in terms of area which is reduced by a factor of log_N(2) with
negligible increase in computation time.
</summary>
    <author>
      <name>Atin Mukherjee</name>
    </author>
    <author>
      <name>Debesh Choudhury</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 8 figures, 6 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE VLSI Circuits &amp; Systems Letters, Volume 4, Issue 3, August
  2018, pages 2-8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1810.06885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.06885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.07059v1</id>
    <updated>2018-10-16T15:02:12Z</updated>
    <published>2018-10-16T15:02:12Z</published>
    <title>On the Off-chip Memory Latency of Real-Time Systems: Is DDR DRAM Really
  the Best Option?</title>
    <summary>  Predictable execution time upon accessing shared memories in multi-core
real-time systems is a stringent requirement. A plethora of existing works
focus on the analysis of Double Data Rate Dynamic Random Access Memories (DDR
DRAMs), or redesigning its memory to provide predictable memory behavior. In
this paper, we show that DDR DRAMs by construction suffer inherent limitations
associated with achieving such predictability. These limitations lead to 1)
highly variable access latencies that fluctuate based on various factors such
as access patterns and memory state from previous accesses, and 2) overly
pessimistic latency bounds. As a result, DDR DRAMs can be ill-suited for some
real-time systems that mandate a strict predictable performance with tight
timing constraints. Targeting these systems, we promote an alternative off-chip
memory solution that is based on the emerging Reduced Latency DRAM (RLDRAM)
protocol, and propose a predictable memory controller (RLDC) managing accesses
to this memory. Comparing with the state-of-the-art predictable DDR
controllers, the proposed solution provides up to 11x less timing variability
and 6.4x reduction in the worst case memory latency.
</summary>
    <author>
      <name>Mohamed Hassan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in IEEE Real Time Systems Symposium (RTSS)</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.07059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.07059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.09661v1</id>
    <updated>2018-10-23T05:07:24Z</updated>
    <published>2018-10-23T05:07:24Z</published>
    <title>Criticality Aware Soft Error Mitigation in the Configuration Memory of
  SRAM based FPGA</title>
    <summary>  Efficient low complexity error correcting code(ECC) is considered as an
effective technique for mitigation of multi-bit upset (MBU) in the
configuration memory(CM)of static random access memory (SRAM) based Field
Programmable Gate Array (FPGA) devices. Traditional multi-bit ECCs have large
overhead and complex decoding circuit to correct adjacent multibit error. In
this work, we propose a simple multi-bit ECC which uses Secure Hash Algorithm
for error detection and parity based two dimensional Erasure Product Code for
error correction. Present error mitigation techniques perform error correction
in the CM without considering the criticality or the execution period of the
tasks allocated in different portion of CM. In most of the cases, error
correction is not done in the right instant, which sometimes either suspends
normal system operation or wastes hardware resources for less critical tasks.
In this paper,we advocate for a dynamic priority-based hardware scheduling
algorithm which chooses the tasks for error correction based on their area,
execution period and criticality. The proposed method has been validated in
terms of overhead due to redundant bits, error correction time and system
reliability
</summary>
    <author>
      <name>Swagata Mandal</name>
    </author>
    <author>
      <name>Sreetama Sarkar</name>
    </author>
    <author>
      <name>Wong Ming Ming</name>
    </author>
    <author>
      <name>Anupam Chattopadhyay</name>
    </author>
    <author>
      <name>Amlan Chakrabarti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.09661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.09661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.12573v1</id>
    <updated>2018-10-30T08:25:12Z</updated>
    <published>2018-10-30T08:25:12Z</published>
    <title>Architectural exploration of heterogeneous memory systems</title>
    <summary>  Heterogeneous systems appear as a viable design alternative for the dark
silicon era. In this paradigm, a processor chip includes several different
technological alternatives for implementing a certain logical block (e.g.,
core, on-chip memories) which cannot be used at the same time due to power
constraints. The programmer and compiler are then responsible for selecting
which of the alternatives should be used for maximizing performance and/or
energy efficiency for a given application. This paper presents an initial
approach for the exploration of different technological alternatives for the
implementation of on-chip memories. It hinges on a linear programming-based
model for theoretically comparing the performance offered by the available
alternatives, namely SRAM and STT-RAM scratchpads or caches. Experimental
results using a cycle-accurate simulation tool confirm that this is a viable
model for implementation into production compilers.
</summary>
    <author>
      <name>M. Horro</name>
    </author>
    <author>
      <name>G. Rodríguez</name>
    </author>
    <author>
      <name>J. Touriño</name>
    </author>
    <author>
      <name>M. T. Kandemir</name>
    </author>
    <link href="http://arxiv.org/abs/1810.12573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.12573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.01780v1</id>
    <updated>2018-11-05T15:14:35Z</updated>
    <published>2018-11-05T15:14:35Z</published>
    <title>Top-Down Transaction-Level Design with TL-Verilog</title>
    <summary>  Transaction-Level Verilog (TL-Verilog) is an emerging extension to
SystemVerilog that supports a new design methodology, called transaction-level
design. A transaction, in this methodology, is an entity that moves through
structures like pipelines, arbiters, and queues, A transaction might be a
machine instruction, a flit of a packet, or a memory read/write. Transaction
logic, like packet header decode or instruction execution, that operates on the
transaction can be placed anywhere along the transaction's flow. Tools produce
the logic to carry signals through their flows to stitch the transaction logic.
  We implemented a small library of TL-Verilog flow components, and we
illustrate the use of these components in a top-down design methodology. We
construct a hypothetical microarchitecture simply by instantiating components.
Within the flows created by these components, we add combinational transaction
logic, enabling verification activities and performance evaluation to begin. We
then refine the model by positioning the transaction logic within its flow to
produce a high-quality register-transfer-level (RTL) implementation.
</summary>
    <author>
      <name>Steven Hoover</name>
    </author>
    <author>
      <name>Ahmed Salman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages. 9 figures. Presented by Ahmed Salman at VSDOpen 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.01780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.01780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.08091v1</id>
    <updated>2018-11-20T06:51:42Z</updated>
    <published>2018-11-20T06:51:42Z</published>
    <title>JuxtaPiton: Enabling Heterogeneous-ISA Research with RISC-V and SPARC
  FPGA Soft-cores</title>
    <summary>  Energy efficiency has become an increasingly important concern in computer
architecture due to the end of Dennard scaling. Heterogeneity has been explored
as a way to achieve better energy efficiency and heterogeneous
microarchitecture chips have become common in the mobile setting.
  Recent research has explored using heterogeneous-ISA, heterogeneous
microarchitecture, general-purpose cores to achieve further energy efficiency
gains. However, there is no open-source hardware implementation of a
heterogeneous-ISA processor available for research, and effective research on
heterogeneous-ISA processors necessitates the emulation speed provided by FPGA
prototyping. This work describes our experiences creating JuxtaPiton by
integrating a small RISC-V core into the OpenPiton framework, which uses a
modified OpenSPARC T1 core. This is the first time a new core has been
integrated with the OpenPiton framework, and JuxtaPiton is the first
open-source, general-purpose, heterogeneous-ISA processor. JuxtaPiton inherits
all the capabilities of OpenPiton, including vital FPGA emulation
infrastructure which can boot full-stack Debian Linux. Using this
infrastructure, we investigate area and timing effects of using the new RISC-V
core on FPGA and the performance of the new core running microbenchmarks.
</summary>
    <author>
      <name>Katie Lim</name>
    </author>
    <author>
      <name>Jonathan Balkind</name>
    </author>
    <author>
      <name>David Wentzlaff</name>
    </author>
    <link href="http://arxiv.org/abs/1811.08091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.08091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.00092v1</id>
    <updated>2019-01-01T04:55:14Z</updated>
    <published>2019-01-01T04:55:14Z</published>
    <title>High Performance GNR Power Gating for Low-Voltage CMOS Circuits</title>
    <summary>  A robust power gating design using Graphene Nano-Ribbon Field Effect
Transistors (GNRFET) is proposed using 16nm technology. The Power Gating (PG)
structure is composed of GNRFET as a power switch and MOS power gated module.
The proposed structure resolves the main drawbacks of the traditional PG design
from the point of view increasing the propagation delay and wake-up time in low
voltage regions. GNRFET/MOSFET Conjunction (GMC) is employed to build various
structures of PG, GMCPG-SS and GMCPG-NS. In addition to exploiting it to build
two multi-mode PG structures. Circuit analysis for CMOS power gated logic
modules ISCAS85 benchmark of 16nm technology is used to evaluate the
performance of the proposed GNR power switch is compared to the traditional MOS
one. Leakage power, wake-up time and power delay product are used as
performance circuit parameters for the evaluation.
</summary>
    <author>
      <name>Hader E. El-hmaily</name>
    </author>
    <author>
      <name>Rabab Ezz-Eldin</name>
    </author>
    <author>
      <name>A. I. A. Galal</name>
    </author>
    <author>
      <name>Hesham F. A. Hamed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 13 Figures and 5 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.00092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.00092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94C99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.7.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.00370v2</id>
    <updated>2019-06-11T11:12:17Z</updated>
    <published>2019-01-02T14:05:48Z</published>
    <title>Optimizing Bit-Serial Matrix Multiplication for Reconfigurable Computing</title>
    <summary>  Matrix-matrix multiplication is a key computational kernel for numerous
applications in science and engineering, with ample parallelism and data
locality that lends itself well to high-performance implementations. Many
matrix multiplication-dependent applications can use reduced-precision integer
or fixed-point representations to increase their performance and energy
efficiency while still offering adequate quality of results. However, precision
requirements may vary between different application phases or depend on input
data, rendering constant-precision solutions ineffective. BISMO, a vectorized
bit-serial matrix multiplication overlay for reconfigurable computing,
previously utilized the excellent binary-operation performance of FPGAs to
offer a matrix multiplication performance that scales with required precision
and parallelism. We show how BISMO can be scaled up on Xilinx FPGAs using an
arithmetic architecture that better utilizes 6-LUTs. The improved BISMO
achieves a peak performance of 15.4 binary TOPS on the Ultra96 board with a
Xilinx UltraScale+ MPSoC.
</summary>
    <author>
      <name>Yaman Umuroglu</name>
    </author>
    <author>
      <name>Davide Conficconi</name>
    </author>
    <author>
      <name>Lahiru Rasnayake</name>
    </author>
    <author>
      <name>Thomas B. Preusser</name>
    </author>
    <author>
      <name>Magnus Sjalander</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3337929</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3337929" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invited paper at ACM TRETS as extension of FPL'18 paper
  arXiv:1806.08862</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.00370v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.00370v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.09315v1</id>
    <updated>2019-01-27T04:46:35Z</updated>
    <published>2019-01-27T04:46:35Z</published>
    <title>Asynchronous Early Output Block Carry Lookahead Adder with Improved
  Quality of Results</title>
    <summary>  A new asynchronous early output block carry lookahead adder (BCLA)
incorporating redundant carries is proposed. Compared to the best of existing
semi-custom asynchronous carry lookahead adders (CLAs) employing
delay-insensitive data encoding and following a 4-phase handshaking, the
proposed BCLA with redundant carries achieves 13% reduction in forward latency
and 14.8% reduction in cycle time compared to the best of the existing CLAs
featuring redundant carries with no area or power penalty. A hybrid variant
involving a ripple carry adder (RCA) in the least significant stages i.e.
BCLA-RCA is also considered that achieves a further 4% reduction in the forward
latency and a 2.4% reduction in the cycle time compared to the proposed BCLA
featuring redundant carries without area or power penalties.
</summary>
    <author>
      <name>P Balasubramanian</name>
    </author>
    <author>
      <name>D L Maskell</name>
    </author>
    <author>
      <name>N E Mastorakis</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 61st MWSCAS 2018, pp. 587-590, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1901.09315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.09315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.00484v1</id>
    <updated>2019-02-01T18:02:53Z</updated>
    <published>2019-02-01T18:02:53Z</published>
    <title>Hybrid Cell Assignment and Sizing for Power, Area, Delay Product
  Optimization of SRAM Arrays</title>
    <summary>  Memory accounts for a considerable portion of the total power budget and area
of digital systems. Furthermore, it is typically the performance bottleneck of
the processing units. Therefore, it is critical to optimize the memory with
respect to the product of power, area, and delay (PAD). We propose a hybrid
cell assignment method based on multi-sized and dual-Vth SRAM cells which
improves the PAD cost function by 34% compared to the conventional cell
assignment. We also utilize the sizing of SRAM cells for minimizing the Data
Retention Voltage (DRV), and voltages for the read and write operations in the
SRAM array. Experimental results in a 32nm technology show that combining the
proposed hybrid cell assignment and the cell sizing methods can lower PAD by up
to 41% when compared to the conventional cell design and assignment.
</summary>
    <author>
      <name>Ghasem Pasandi</name>
    </author>
    <author>
      <name>Raghav Mehta</name>
    </author>
    <author>
      <name>Massoud Pedram</name>
    </author>
    <author>
      <name>Shahin Nazarian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS II: EXPRESS BRIEF (DOI:
  10.1109/TCSII.2019.2896794)</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.00484v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.00484v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.03314v1</id>
    <updated>2019-02-08T22:33:18Z</updated>
    <published>2019-02-08T22:33:18Z</published>
    <title>Routing in Networks on Chip with Multiplicative Circulant Topology</title>
    <summary>  The development of multi-core processor systems is a demanded branch of
science and technology. The appearance of processors with dozens and hundreds
of cores poses to the developers the question of choosing the optimal topology
capable to provide efficient routing in a network with a large number of nodes.
In this paper, we consider the possibility of using multiplicative circulants
as a topology for networks-on-chip. A specialized routing algorithm for
networks with multiplicative circulant topology, taking into account topology
features and having a high scalability, has been developed.
</summary>
    <author>
      <name>Shchegoleva M. A.</name>
    </author>
    <author>
      <name>Romanov A. Yu.</name>
    </author>
    <author>
      <name>Lezhnev E. V.</name>
    </author>
    <author>
      <name>Amerikanov A. A</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-6596/1163/1/012027</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-6596/1163/1/012027" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 p., 4 fig., International Conference on Computer Simulation in
  Physics and beyond</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.03314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.00191v1</id>
    <updated>2019-03-01T08:07:24Z</updated>
    <published>2019-03-01T08:07:24Z</published>
    <title>MIPS-Core Application Specific Instruction-Set Processor for IDEA
  Cryptography - Comparison between Single-Cycle and Multi-Cycle Architectures</title>
    <summary>  A single-cycle processor completes the execution of an instruction in only
one clock cycle. However, its clock period is usually rather long. On the
contrary, although clock frequency is higher in a multi-cycle processor, it
takes several clock cycles to finish an instruction. Therefore, their runtime
efficiencies depend on which program is executed. This paper presents a new
processor for International Data Encryption Algorithm (IDEA) cryptography. The
new design is an Application Specific Instruction-set Processor (ASIP) in which
both general-purpose and special instructions are supported. It is a
single-cycle MIPS-core architecture, whose average Clocks Per Instruction (CPI)
is 1. Furthermore, a comparison is provided in this paper to show the
differences between the proposed single-cycle processor and another comparable
multi-cycle crypto processor. FPGA implementation results show that both
architectures have almost the same encoding/decoding throughput. However, the
previous processor consumes nearly twice as many resources as the new one does.
</summary>
    <author>
      <name>Ahmad Ahmadi</name>
    </author>
    <author>
      <name>Reza Faghih Mirzaee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 10 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.00191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.00191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.01314v1</id>
    <updated>2019-03-04T15:48:00Z</updated>
    <published>2019-03-04T15:48:00Z</published>
    <title>Denial-of-Service Attacks on Shared Cache in Multicore: Analysis and
  Prevention</title>
    <summary>  In this paper we investigate the feasibility of denial-of-service (DoS)
attacks on shared caches in multicore platforms. With carefully engineered
attacker tasks, we are able to cause more than 300X execution time increases on
a victim task running on a dedicated core on a popular embedded multicore
platform, regardless of whether we partition its shared cache or not. Based on
careful experimentation on real and simulated multicore platforms, we identify
an internal hardware structure of a non-blocking cache, namely the cache
writeback buffer, as a potential target of shared cache DoS attacks. We propose
an OS-level solution to prevent such DoS attacks by extending a
state-of-the-art memory bandwidth regulation mechanism. We implement the
proposed mechanism in Linux on a real multicore platform and show its
effectiveness in protecting against cache DoS attacks.
</summary>
    <author>
      <name>Michael G Bechtel</name>
    </author>
    <author>
      <name>Heechul Yun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published as a conference paper at RTAS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.01314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.01776v2</id>
    <updated>2019-03-09T12:44:11Z</updated>
    <published>2019-03-05T11:56:32Z</published>
    <title>FUSE: Fusing STT-MRAM into GPUs to Alleviate Off-Chip Memory Access
  Overheads</title>
    <summary>  In this work, we propose FUSE, a novel GPU cache system that integrates
spin-transfer torque magnetic random-access memory (STT-MRAM) into the on-chip
L1D cache. FUSE can minimize the number of outgoing memory accesses over the
interconnection network of GPU's multiprocessors, which in turn can
considerably improve the level of massive computing parallelism in GPUs.
Specifically, FUSE predicts a read-level of GPU memory accesses by extracting
GPU runtime information and places write-once-read-multiple (WORM) data blocks
into the STT-MRAM, while accommodating write-multiple data blocks over a small
portion of SRAM in the L1D cache. To further reduce the off-chip memory
accesses, FUSE also allows WORM data blocks to be allocated anywhere in the
STT-MRAM by approximating the associativity with the limited number of tag
comparators and I/O peripherals. Our evaluation results show that, in
comparison to a traditional GPU cache, our proposed heterogeneous cache reduces
the number of outgoing memory references by 32% across the interconnection
network, thereby improving the overall performance by 217% and reducing energy
cost by 53%.
</summary>
    <author>
      <name>Jie Zhang</name>
    </author>
    <author>
      <name>Myoungsoo Jung</name>
    </author>
    <author>
      <name>Mahmut Taylan Kandemir</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">HPCA 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.01776v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01776v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.02596v2</id>
    <updated>2019-04-15T23:28:48Z</updated>
    <published>2019-03-06T19:55:19Z</published>
    <title>Buddy Compression: Enabling Larger Memory for Deep Learning and HPC
  Workloads on GPUs</title>
    <summary>  GPUs offer orders-of-magnitude higher memory bandwidth than traditional
CPU-only systems. However, GPU device memory tends to be relatively small and
the memory capacity can not be increased by the user. This paper describes
Buddy Compression, a scheme to increase both the effective GPU memory capacity
and bandwidth while avoiding the downsides of conventional memory-expanding
strategies. Buddy Compression compresses GPU memory, splitting each compressed
memory entry between high-speed device memory and a slower-but-larger
disaggregated memory pool (or system memory). Highly-compressible memory
entries can thus be accessed completely from device memory, while
incompressible entries source their data using both on and off-device accesses.
Increasing the effective GPU memory capacity enables us to run
larger-memory-footprint HPC workloads and larger batch-sizes or models for DL
workloads than current memory capacities would allow. We show that our solution
achieves an average compression ratio of 2.2x on HPC workloads and 1.5x on DL
workloads, with a slowdown of just 1~2%.
</summary>
    <author>
      <name>Esha Choukse</name>
    </author>
    <author>
      <name>Michael Sullivan</name>
    </author>
    <author>
      <name>Mike O'Connor</name>
    </author>
    <author>
      <name>Mattan Erez</name>
    </author>
    <author>
      <name>Jeff Pool</name>
    </author>
    <author>
      <name>David Nellans</name>
    </author>
    <author>
      <name>Steve Keckler</name>
    </author>
    <link href="http://arxiv.org/abs/1903.02596v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.02596v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.04188v1</id>
    <updated>2019-03-11T09:36:06Z</updated>
    <published>2019-03-11T09:36:06Z</published>
    <title>Automated Circuit Approximation Method Driven by Data Distribution</title>
    <summary>  We propose an application-tailored data-driven fully automated method for
functional approximation of combinational circuits. We demonstrate how an
application-level error metric such as the classification accuracy can be
translated to a component-level error metric needed for an efficient and fast
search in the space of approximate low-level components that are used in the
application. This is possible by employing a weighted mean error distance
(WMED) metric for steering the circuit approximation process which is conducted
by means of genetic programming. WMED introduces a set of weights (calculated
from the data distribution measured on a selected signal in a given
application) determining the importance of each input vector for the
approximation process. The method is evaluated using synthetic benchmarks and
application-specific approximate MAC (multiply-and-accumulate) units that are
designed to provide the best trade-offs between the classification accuracy and
power consumption of two image classifiers based on neural networks.
</summary>
    <author>
      <name>Zdenek Vasicek</name>
    </author>
    <author>
      <name>Vojtech Mrazek</name>
    </author>
    <author>
      <name>Lukas Sekanina</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23919/DATE.2019.8714977</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23919/DATE.2019.8714977" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at Design, Automation and Test in Europe
  (DATE 2019). Florence, Italy</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.04188v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.04188v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.08680v1</id>
    <updated>2019-03-20T18:17:03Z</updated>
    <published>2019-03-20T18:17:03Z</published>
    <title>A 68 uW 31 kS/s Fully-Capacitive Noise-Shaping SAR ADC with 102 dB SNDR</title>
    <summary>  This paper presents a 17 bit analogue-to-digital converter that incorporates
mismatch and quantisation noise-shaping techniques into an energy-saving 10 bit
successive approximation quantiser to increase the dynamic range by another 42
dB. We propose a novel fully-capacitive topology which allows for high-speed
asynchronous conversion together with a background calibration scheme to reduce
the oversampling requirement by 10x compared to prior-art. A 0.18 um CMOS
technology is used to demonstrate preliminary simulation results together with
analytic measures that optimise parameter and topology selection. The proposed
system is able to achieve a FoMS of 183 dB for a maximum signal bandwidth of
15.6 kHz while dissipating 68 uW from a 1.8 V supply. A peak SNDR of 102 dB is
demonstrated for this rate with a 0.201 mm^2 area requirement.
</summary>
    <author>
      <name>Lieuwe B. Leene</name>
    </author>
    <author>
      <name>Shiva Letchumanan</name>
    </author>
    <author>
      <name>Timothy G. Constandinou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 figures, conference, iscas, ieee, accepted submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.08680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.08680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.03428v2</id>
    <updated>2019-11-04T18:40:45Z</updated>
    <published>2019-04-06T12:23:40Z</published>
    <title>Ring-Mesh: A Scalable and High-Performance Approach for Manycore
  Accelerators</title>
    <summary>  There are increasing number of works addressing the design challenges of
fast, scalable solutions for the growing number of new type of applications.
Recently, many of the solutions aimed at improving processing element
capabilities to speed up the execution of machine learning application domain.
However, only a few works focused on the interconnection subsystem as a
potential source of performance improvement. Wrapping many cores together offer
excellent parallelism, but it brings other challenges (e.g., adequate
interconnections). Scalable, power-aware interconnects are required to support
such a growing number of processing elements, as well as modern applications.
In this paper, we propose a scalable and energy efficient Network-on-Chip
architecture fusing the advantages of rings as well as the 2D-mesh without
using any bridge router to provide high-performance. A dynamic adaptation
mechanism allows to better adapt to the application requirements. Simulation
results show efficient power consumption (up to 141.3% saving for connecting
1024 cores), 2x (on average) throughput growth with better scalability (up to
1024 processing elements) compared to popular 2D-mesh while tested in multiple
statistical traffic pattern scenarios.
</summary>
    <author>
      <name>Somnath Mazumdar</name>
    </author>
    <author>
      <name>Alberto Scionti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11227-019-03072-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11227-019-03072-5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, Accepted to Journal of Supercomputing</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.03428v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.03428v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.05442v1</id>
    <updated>2019-04-10T21:07:21Z</updated>
    <published>2019-04-10T21:07:21Z</published>
    <title>The Cost of Application-Class Processing: Energy and Performance
  Analysis of a Linux-ready 1.7GHz 64bit RISC-V Core in 22nm FDSOI Technology</title>
    <summary>  The open-source RISC-V ISA is gaining traction, both in industry and
academia. The ISA is designed to scale from micro-controllers to server-class
processors. Furthermore, openness promotes the availability of various
open-source and commercial implementations. Our main contribution in this work
is a thorough power, performance, and efficiency analysis of the RISC-V ISA
targeting baseline "application class" functionality, i.e. supporting the Linux
OS and its application environment based on our open-source single-issue
in-order implementation of the 64 bit ISA variant (RV64GC) called Ariane. Our
analysis is based on a detailed power and efficiency analysis of the RISC-V ISA
extracted from silicon measurements and calibrated simulation of an Ariane
instance (RV64IMC) taped-out in GlobalFoundries 22 FDX technology. Ariane runs
at up to 1.7 GHz and achieves up to 40 Gop/sW peak efficiency. We give insight
into the interplay between functionality required for application-class
execution (e.g. virtual memory, caches, multiple modes of privileged operation)
and energy cost. Our analysis indicates that ISA heterogeneity and simpler
cores with a few critical instruction extensions (e.g. packed SIMD) can
significantly boost a RISC-V core's compute energy efficiency.
</summary>
    <author>
      <name>Florian Zaruba</name>
    </author>
    <author>
      <name>Luca Benini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TVLSI.2019.2926114</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TVLSI.2019.2926114" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, submitted to IEEE Transaction on Very Large Scale
  Integration (VLSI) Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.05442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.05782v1</id>
    <updated>2019-04-11T15:29:44Z</updated>
    <published>2019-04-11T15:29:44Z</published>
    <title>Accelerating Bulk Bit-Wise X(N)OR Operation in Processing-in-DRAM
  Platform</title>
    <summary>  With Von-Neumann computing architectures struggling to address
computationally- and memory-intensive big data analytic task today,
Processing-in-Memory (PIM) platforms are gaining growing interests. In this
way, processing-in-DRAM architecture has achieved remarkable success by
dramatically reducing data transfer energy and latency. However, the
performance of such system unavoidably diminishes when dealing with more
complex applications seeking bulk bit-wise X(N)OR- or addition operations,
despite utilizing maximum internal DRAM bandwidth and in-memory parallelism. In
this paper, we develop DRIM platform that harnesses DRAM as computational
memory and transforms it into a fundamental processing unit. DRIM uses the
analog operation of DRAM sub-arrays and elevates it to implement bit-wise
X(N)OR operation between operands stored in the same bit-line, based on a new
dual-row activation mechanism with a modest change to peripheral circuits such
sense amplifiers. The simulation results show that DRIM achieves on average 71x
and 8.4x higher throughput for performing bulk bit-wise X(N)OR-based operations
compared with CPU and GPU, respectively. Besides, DRIM outperforms recent
processing-in-DRAM platforms with up to 3.7x better performance.
</summary>
    <author>
      <name>Shaahin Angizi</name>
    </author>
    <author>
      <name>Deliang Fan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 9 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.05782v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05782v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.08233v1</id>
    <updated>2019-04-10T20:15:29Z</updated>
    <published>2019-04-10T20:15:29Z</published>
    <title>Performance Analysis of Linear Algebraic Functions using Reconfigurable
  Computing</title>
    <summary>  This paper introduces a new mapping of geometrical transformation on the
MorphoSys (M1) reconfigurable computing (RC) system. New mapping techniques for
some linear algebraic functions are recalled. A new mapping for geometrical
transformation operations is introduced and their performance on the M1 system
is evaluated. The translation and scaling transformation addressed in this
mapping employ some vector-vector and vector-scalar operations [6-7]. A
performance analysis study of the M1 RC system is also presented to evaluate
the efficiency of the algorithm execution. Numerical examples were simulated to
validate our results, using the MorphoSys mULATE program, which emulates M1
operations.
</summary>
    <author>
      <name>Issam Damaj</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">London South Bank University</arxiv:affiliation>
    </author>
    <author>
      <name>Hassan Diab</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">American University of Beirut</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1023/A:1020993510939</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1023/A:1020993510939" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 17 figures, 5 tables. arXiv admin note: substantial text
  overlap with arXiv:1904.04953; text overlap with arXiv:1904.06198</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Intl. Jrnl. of. Super. Comp. 24(2003) 91-107</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.08233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.4.4; B.5.1; B.5.2; B.6.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.09363v1</id>
    <updated>2019-04-19T23:03:04Z</updated>
    <published>2019-04-19T23:03:04Z</published>
    <title>Energy-Efficient Runtime Adaptable L1 STT-RAM Cache Design</title>
    <summary>  Much research has shown that applications have variable runtime cache
requirements. In the context of the increasingly popular Spin-Transfer Torque
RAM (STT-RAM) cache, the retention time, which defines how long the cache can
retain a cache block in the absence of power, is one of the most important
cache requirements that may vary for different applications. In this paper, we
propose a Logically Adaptable Retention Time STT-RAM (LARS) cache that allows
the retention time to be dynamically adapted to applications' runtime
requirements. LARS cache comprises of multiple STT-RAM units with different
retention times, with only one unit being used at a given time. LARS
dynamically determines which STT-RAM unit to use during runtime, based on
executing applications' needs. As an integral part of LARS, we also explore
different algorithms to dynamically determine the best retention time based on
different cache design tradeoffs. Our experiments show that by adapting the
retention time to different applications' requirements, LARS cache can reduce
the average cache energy by 25.31%, compared to prior work, with minimal
overheads.
</summary>
    <author>
      <name>Kyle Kuan</name>
    </author>
    <author>
      <name>Tosiron Adegbija</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCAD.2019.2912920</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCAD.2019.2912920" rel="related"/>
    <link href="http://arxiv.org/abs/1904.09363v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.09363v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.09495v2</id>
    <updated>2019-05-01T08:27:33Z</updated>
    <published>2019-04-20T20:31:15Z</published>
    <title>Development of routing algorithms in networks-on-chip based on ring
  circulant topologies</title>
    <summary>  This work is devoted to the study of communication subsystem of
networks-onchip (NoCs) development with an emphasis on their topologies. The
main characteristics of NoC topologies and the routing problem in NoCs with
various topologies are considered. It is proposed to use two-dimensional
circulant topologies for NoC design, since they have significantly better
characteristics than most common mesh and torus topologies, and, in contrast to
many other approaches to improving topologies, have a regular structure. The
emphasis is on using ring circulants which although in some cases have somewhat
worse characteristics than the optimal circulants, compensate by one-length
first generatrix in such graphs that greatly facilitate routing in them. The
paper considers three different approaches to routing in NoCs with ring
circulant topology: Table routing, Clockwise routing, and Adaptive routing. The
algorithms of routing are proposed, the results of synthesis of routers, based
on them, are presented, and the cost of chip resources for the implementation
of such communication subsystems in NoCs is estimated.
</summary>
    <author>
      <name>Aleksandr Yu. Romanov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.heliyon.2019.e01516</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.heliyon.2019.e01516" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 p., 10 fig</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Heliyon 5 (2019) e01516</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.09495v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.09495v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.10646v1</id>
    <updated>2019-04-24T05:26:39Z</updated>
    <published>2019-04-24T05:26:39Z</published>
    <title>Efficient FPGA Floorplanning for Partial Reconfiguration-Based
  Applications</title>
    <summary>  Partial Reconfiguration (PR) is a technique that allows reconfiguring the
FPGA chip at runtime. However, current design support tools require manual
floorplanning of the partial modules. Several approaches have been proposed in
this field, but only a few of them consider all aspects of PR, like the shape
and the aspect ratio of the reconfigurable region. Most of them are defined for
old FPGA architectures and have a high computational time. This paper
introduces an efficient automatic floorplanning algorithm, which takes into
account the heterogeneous architectures of modern FPGA families, as well as PR
constraints, introducing the aspect ratio constraint to optimize routing. The
algorithm generates possible placements of the partial modules, then applies a
recursive pseudo-bipartitioning heuristic search to find the best floorplan.
The experiments showed that the algorithm's performance is significantly better
than the one of other algorithms in this field.
</summary>
    <author>
      <name>Norbert Deak</name>
    </author>
    <author>
      <name>Octavian Creţ</name>
    </author>
    <author>
      <name>Horia Hedeşiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures, a one page summary published to FCCM 19</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.10646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.10646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.11560v1</id>
    <updated>2019-04-25T19:57:31Z</updated>
    <published>2019-04-25T19:57:31Z</published>
    <title>A Survey on Tiering and Caching in High-Performance Storage Systems</title>
    <summary>  Although every individual invented storage technology made a big step towards
perfection, none of them is spotless. Different data store essentials such as
performance, availability, and recovery requirements have not met together in a
single economically affordable medium, yet. One of the most influential factors
is price. So, there has always been a trade-off between having a desired set of
storage choices and the costs. To address this issue, a network of various
types of storing media is used to deliver the high performance of expensive
devices such as solid state drives and non-volatile memories, along with the
high capacity of inexpensive ones like hard disk drives. In software, caching
and tiering are long-established concepts for handling file operations and
moving data automatically within such a storage network and manage data backup
in low-cost media. Intelligently moving data around different devices based on
the needs is the key insight for this matter. In this survey, we discuss some
recent pieces of research that have been done to improve high-performance
storage systems with caching and tiering techniques.
</summary>
    <author>
      <name>Morteza Hoseinzadeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ph.D. Research Exam Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.11560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.11560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.04423v1</id>
    <updated>2019-05-11T02:15:44Z</updated>
    <published>2019-05-11T02:15:44Z</published>
    <title>Optimizing Routerless Network-on-Chip Designs: An Innovative
  Learning-Based Framework</title>
    <summary>  Machine learning applied to architecture design presents a promising
opportunity with broad applications. Recent deep reinforcement learning (DRL)
techniques, in particular, enable efficient exploration in vast design spaces
where conventional design strategies may be inadequate. This paper proposes a
novel deep reinforcement framework, taking routerless networks-on-chip (NoC) as
an evaluation case study. The new framework successfully resolves problems with
prior design approaches being either unreliable due to random searches or
inflexible due to severe design space restrictions. The framework learns
(near-)optimal loop placement for routerless NoCs with various design
constraints. A deep neural network is developed using parallel threads that
efficiently explore the immense routerless NoC design space with a Monte Carlo
search tree. Experimental results show that, compared with conventional mesh,
the proposed deep reinforcement learning (DRL) routerless design achieves a
3.25x increase in throughput, 1.6x reduction in packet latency, and 5x
reduction in power. Compared with the state-of-the-art routerless NoC, DRL
achieves a 1.47x increase in throughput, 1.18x reduction in packet latency, and
1.14x reduction in average hop count albeit with slightly more power overhead.
</summary>
    <author>
      <name>Ting-Ru Lin</name>
    </author>
    <author>
      <name>Drew Penney</name>
    </author>
    <author>
      <name>Massoud Pedram</name>
    </author>
    <author>
      <name>Lizhong Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.04423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.04423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.08624v1</id>
    <updated>2019-05-18T07:53:51Z</updated>
    <published>2019-05-18T07:53:51Z</published>
    <title>Performance Analysis of 6T and 9T SRAM</title>
    <summary>  The SRAM cell is made up of latch, which ensures that the cell data is
preserved as long as power is turned on and refresh operation is not required
for the SRAM cell. SRAM is widely used for on-chip cache memory in
microprocessors, game software, computers, workstations, portable handheld
devices due to high data speed, low power consumption, low voltage supply,
no-refresh needed. Therefore, to build a reliable cache/memory, the individual
cell (SRAM) must be designed to have high Static Noise Margin (SNM). In
sub-threshold region, conventional 6T-cell SRAM experiences poor read and write
ability, and reduction in the SNM at various fluctuation of the threshold
voltage, supply voltage down scaling, and technology scaling in nano-meter
ranges (180nm, 90nm, 45nm, 22nm, 16nm and 10nm). Thus, noise margin becomes
worse during read and write operations compared to hold operation which the
internal feedback operates independent of the access transistors. Due to these
limitations of the conventional 6T SRAM cell, we have proposed a 9T SRAM that
will drastically minimize these limitations; the extra three transistors added
to the 6T topology will improve the read, hold and write SNM. The design and
simulation results were carried out using Cadence Virtuoso to evaluate the
performance of 6T and 9T SRAM cells.
</summary>
    <author>
      <name>Apollos Ezeogu</name>
    </author>
    <link href="http://arxiv.org/abs/1905.08624v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08624v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.09822v3</id>
    <updated>2020-04-05T14:08:23Z</updated>
    <published>2019-05-23T06:17:39Z</published>
    <title>In-DRAM Bulk Bitwise Execution Engine</title>
    <summary>  Many applications heavily use bitwise operations on large bitvectors as part
of their computation. In existing systems, performing such bulk bitwise
operations requires the processor to transfer a large amount of data on the
memory channel, thereby consuming high latency, memory bandwidth, and energy.
In this paper, we describe Ambit, a recently-proposed mechanism to perform bulk
bitwise operations completely inside main memory. Ambit exploits the internal
organization and analog operation of DRAM-based memory to achieve low cost,
high performance, and low energy. Ambit exposes a new bulk bitwise execution
model to the host processor. Evaluations show that Ambit significantly improves
the performance of several applications that use bulk bitwise operations,
including databases.
</summary>
    <author>
      <name>Vivek Seshadri</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1605.06483,
  arXiv:1610.09603, arXiv:1611.09988</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.09822v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.09822v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.10336v1</id>
    <updated>2019-05-24T17:01:36Z</updated>
    <published>2019-05-24T17:01:36Z</published>
    <title>Polystore++: Accelerated Polystore System for Heterogeneous Workloads</title>
    <summary>  Modern real-time business analytic consist of heterogeneous workloads (e.g,
database queries, graph processing, and machine learning). These analytic
applications need programming environments that can capture all aspects of the
constituent workloads (including data models they work on and movement of data
across processing engines). Polystore systems suit such applications; however,
these systems currently execute on CPUs and the slowdown of Moore's Law means
they cannot meet the performance and efficiency requirements of modern
workloads. We envision Polystore++, an architecture to accelerate existing
polystore systems using hardware accelerators (e.g, FPGAs, CGRAs, and GPUs).
Polystore++ systems can achieve high performance at low power by identifying
and offloading components of a polystore system that are amenable to
acceleration using specialized hardware. Building a Polystore++ system is
challenging and introduces new research problems motivated by the use of
hardware accelerators (e.g, optimizing and mapping query plans across
heterogeneous computing units and exploiting hardware pipelining and
parallelism to improve performance). In this paper, we discuss these challenges
in detail and list possible approaches to address these problems.
</summary>
    <author>
      <name>Rekha Singhal</name>
    </author>
    <author>
      <name>Nathan Zhang</name>
    </author>
    <author>
      <name>Luigi Nardi</name>
    </author>
    <author>
      <name>Muhammad Shahbaz</name>
    </author>
    <author>
      <name>Kunle Olukotun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, Accepted in ICDCS 2019</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICDCS 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.10336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.10336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.01112v2</id>
    <updated>2019-07-18T17:44:05Z</updated>
    <published>2019-07-02T00:49:11Z</published>
    <title>On the Optimal Refresh Power Allocation for Energy-Efficient Memories</title>
    <summary>  Refresh is an important operation to prevent loss of data in dynamic
random-access memory (DRAM). However, frequent refresh operations incur
considerable power consumption and degrade system performance. Refresh power
cost is especially significant in high-capacity memory devices and
battery-powered edge/mobile applications. In this paper, we propose a
principled approach to optimizing the refresh power allocation. Given a model
for the bit error rate dependence on power, we formulate a convex optimization
problem to minimize the word mean squared error for a refresh power constraint;
hence we can guarantee the optimality of the obtained refresh power
allocations. In addition, we provide an integer programming problem to optimize
the discrete refresh interval assignments. For an 8-bit accessed word,
numerical results show that the optimized nonuniform refresh intervals reduce
the refresh power by 29% at a peak signal-to-noise ratio of 50dB compared to
the uniform assignment.
</summary>
    <author>
      <name>Yongjune Kim</name>
    </author>
    <author>
      <name>Won Ho Choi</name>
    </author>
    <author>
      <name>Cyril Guyot</name>
    </author>
    <author>
      <name>Yuval Cassuto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/GLOBECOM38437.2019.9013465</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/GLOBECOM38437.2019.9013465" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.01112v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01112v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.04504v1</id>
    <updated>2019-07-10T04:38:06Z</updated>
    <published>2019-07-10T04:38:06Z</published>
    <title>A Range Matching CAM for Hierarchical Defect Tolerance Technique in NRAM
  Structures</title>
    <summary>  Due to the small size of nanoscale devices, they are highly prone to process
disturbances which results in manufacturing defects. Some of the defects are
randomly distributed throughout the nanodevice layer. Other disturbances tend
to be local and lead to cluster defects caused by factors such as layer
misintegration and line width variations. In this paper, we propose a method
for identifying cluster defects from random ones. The motivation is to repair
the cluster defects using rectangular ranges in a range matching
content-addressable memory (RM-CAM) and random defects using triple-modular
redundancy (TMR). It is believed a combination of these two approaches is more
effective for repairing defects at high error rate with less resource. With the
proposed fault repairing technique, defect recovery results are examined for
different fault distribution scenarios. Also the mapping circuit structure
required for two conceptual 32*32 and 64*64 bit RAMs are presented and their
speed, power and transistor count are reported.
</summary>
    <author>
      <name>Hossein Pourmeidani</name>
    </author>
    <author>
      <name>Mehdi Habibi</name>
    </author>
    <link href="http://arxiv.org/abs/1907.04504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.07776v1</id>
    <updated>2019-07-17T21:14:24Z</updated>
    <published>2019-07-17T21:14:24Z</published>
    <title>CADS: Core-Aware Dynamic Scheduler for Multicore Memory Controllers</title>
    <summary>  Memory controller scheduling is crucial in multicore processors, where DRAM
bandwidth is shared. Since increased number of requests from multiple cores of
processors becomes a source of bottleneck, scheduling the requests efficiently
is necessary to utilize all the computing power these processors offer.
However, current multicore processors are using traditional memory controllers,
which are designed for single-core processors. They are unable to adapt to
changing characteristics of memory workloads that run simultaneously on
multiple cores. Existing schedulers may disrupt locality and bank parallelism
among data requests coming from different cores. Hence, novel memory
controllers that consider and adapt to the memory access characteristics, and
share memory resources efficiently and fairly are necessary. We introduce
Core-Aware Dynamic Scheduler (CADS) for multicore memory controller. CADS uses
Reinforcement Learning (RL) to alter its scheduling strategy dynamically at
runtime. Our scheduler utilizes locality among data requests from multiple
cores and exploits parallelism in accessing multiple banks of DRAM. CADS is
also able to share the DRAM while guaranteeing fairness to all cores accessing
memory. Using CADS policy, we achieve 20% better cycles per instruction (CPI)
in running memory intensive and compute intensive PARSEC parallel benchmarks
simultaneously, and 16% better CPI with SPEC 2006 benchmarks.
</summary>
    <author>
      <name>Eduardo Olmedo Sanchez</name>
    </author>
    <author>
      <name>Xian-He Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1907.07776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.07776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.08641v1</id>
    <updated>2019-07-19T18:25:38Z</updated>
    <published>2019-07-19T18:25:38Z</published>
    <title>PPAC: A Versatile In-Memory Accelerator for Matrix-Vector-Product-Like
  Operations</title>
    <summary>  Processing in memory (PIM) moves computation into memories with the goal of
improving throughput and energy-efficiency compared to traditional von
Neumann-based architectures. Most existing PIM architectures are either
general-purpose but only support atomistic operations, or are specialized to
accelerate a single task. We propose the Parallel Processor in Associative
Content-addressable memory (PPAC), a novel in-memory accelerator that supports
a range of matrix-vector-product (MVP)-like operations that find use in
traditional and emerging applications. PPAC is, for example, able to accelerate
low-precision neural networks, exact/approximate hash lookups, cryptography,
and forward error correction. The fully-digital nature of PPAC enables its
implementation with standard-cell-based CMOS, which facilitates automated
design and portability among technology nodes. To demonstrate the efficacy of
PPAC, we provide post-layout implementation results in 28nm CMOS for different
array sizes. A comparison with recent digital and mixed-signal PIM accelerators
reveals that PPAC is competitive in terms of throughput and energy-efficiency,
while accelerating a wide range of applications and simplifying development.
</summary>
    <author>
      <name>Oscar Castañeda</name>
    </author>
    <author>
      <name>Maria Bobbett</name>
    </author>
    <author>
      <name>Alexandra Gallyas-Sanhueza</name>
    </author>
    <author>
      <name>Christoph Studer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the IEEE International Conference on
  Application-specific Systems, Architectures and Processors (ASAP), 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.08641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.08641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.00314v1</id>
    <updated>2019-08-01T10:33:46Z</updated>
    <published>2019-08-01T10:33:46Z</published>
    <title>Towards Multidimensional Verification: Where Functional Meets
  Non-Functional</title>
    <summary>  Trends in advanced electronic systems' design have a notable impact on design
verification technologies. The recent paradigms of Internet-of-Things (IoT) and
Cyber-Physical Systems (CPS) assume devices immersed in physical environments,
significantly constrained in resources and expected to provide levels of
security, privacy, reliability, performance and low power features. In recent
years, numerous extra-functional aspects of electronic systems were brought to
the front and imply verification of hardware design models in multidimensional
space along with the functional concerns of the target system. However,
different from the software domain such a holistic approach remains
underdeveloped. The contributions of this paper are a taxonomy for
multidimensional hardware verification aspects, a state-of-the-art survey of
related research works and trends towards the multidimensional verification
concept. The concept is motivated by an example for the functional and power
verification dimensions.
</summary>
    <author>
      <name>Maksim Jenihhin</name>
    </author>
    <author>
      <name>Xinhui Lai</name>
    </author>
    <author>
      <name>Tara Ghasempouri</name>
    </author>
    <author>
      <name>Jaan Raik</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/NORCHIP.2018.8573495</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/NORCHIP.2018.8573495" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2018 IEEE Nordic Circuits and Systems Conference (NORCAS): NORCHIP
  and International Symposium of System-on-Chip (SoC)</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.00314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.00314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01261v1</id>
    <updated>2019-08-04T02:35:57Z</updated>
    <published>2019-08-04T02:35:57Z</published>
    <title>Analysis and Optimization of I/O Cache Coherency Strategies for SoC-FPGA
  Device</title>
    <summary>  Unlike traditional PCIe-based FPGA accelerators, heterogeneous SoC-FPGA
devices provide tighter integrations between software running on CPUs and
hardware accelerators. Modern heterogeneous SoC-FPGA platforms support multiple
I/O cache coherence options between CPUs and FPGAs, but these options can have
inadvertent effects on the achieved bandwidths depending on applications and
data access patterns. To provide the most efficient communications between CPUs
and accelerators, understanding the data transaction behaviors and selecting
the right I/O cache coherence method is essential. In this paper, we use Xilinx
Zynq UltraScale+ as the SoC platform to show how certain I/O cache coherence
method can perform better or worse in different situations, ultimately
affecting the overall accelerator performances as well. Based on our analysis,
we further explore possible software and hardware modifications to improve the
I/O performances with different I/O cache coherence options. With our proposed
modifications, the overall performance of SoC design can be averagely improved
by 20%.
</summary>
    <author>
      <name>Seung Won Min</name>
    </author>
    <author>
      <name>Sitao Huang</name>
    </author>
    <author>
      <name>Mohamed El-Hadedy</name>
    </author>
    <author>
      <name>Jinjun Xiong</name>
    </author>
    <author>
      <name>Deming Chen</name>
    </author>
    <author>
      <name>Wen-mei Hwu</name>
    </author>
    <link href="http://arxiv.org/abs/1908.01261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01806v2</id>
    <updated>2020-02-22T18:38:44Z</updated>
    <published>2019-08-05T19:01:58Z</published>
    <title>Addressing multiple bit/symbol errors in DRAM subsystem</title>
    <summary>  As DRAM technology continues to evolve towards smaller feature sizes and
increased densities, faults in DRAM subsystem are becoming more severe. Current
servers mostly use CHIPKILL based schemes to tolerate up-to one/two symbol
errors per DRAM beat. Multi-symbol errors arising due to faults in multiple
data buses and chips may not be detected by these schemes. In this paper, we
introduce Single Symbol Correction Multiple Symbol Detection (SSCMSD) - a novel
error handling scheme to correct single-symbol errors and detect multi-symbol
errors. Our scheme makes use of a hash in combination with Error Correcting
Code (ECC) to avoid silent data corruptions (SDCs). SSCMSD can also enhance the
capability of detecting errors in address bits. We employ 32-bit CRC along with
Reed-Solomon code to implement SSCMSD for a x4 based DDRx system. Our
simulations show that the proposed scheme effectively prevents SDCs in the
presence of multiple symbol errors. Our novel design enabled us to achieve this
without introducing additional READ latency. Also, we need 19 chips per rank
(storage overhead of 18.75 percent), 76 data bus-lines and additional
hash-logic at the memory controller.
</summary>
    <author>
      <name>Ravikiran Yeleswarapu</name>
    </author>
    <author>
      <name>Arun K. Somani</name>
    </author>
    <link href="http://arxiv.org/abs/1908.01806v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01806v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.02640v1</id>
    <updated>2019-08-07T14:00:08Z</updated>
    <published>2019-08-07T14:00:08Z</published>
    <title>Near-Memory Computing: Past, Present, and Future</title>
    <summary>  The conventional approach of moving data to the CPU for computation has
become a significant performance bottleneck for emerging scale-out
data-intensive applications due to their limited data reuse. At the same time,
the advancement in 3D integration technologies has made the decade-old concept
of coupling compute units close to the memory --- called near-memory computing
(NMC) --- more viable. Processing right at the "home" of data can significantly
diminish the data movement problem of data-intensive applications.
  In this paper, we survey the prior art on NMC across various dimensions
(architecture, applications, tools, etc.) and identify the key challenges and
open issues with future research directions. We also provide a glimpse of our
approach to near-memory computing that includes i) NMC specific
microarchitecture independent application characterization ii) a compiler
framework to offload the NMC kernels on our target NMC platform and iii) an
analytical model to evaluate the potential of NMC.
</summary>
    <author>
      <name>Gagandeep Singh</name>
    </author>
    <author>
      <name>Lorenzo Chelini</name>
    </author>
    <author>
      <name>Stefano Corda</name>
    </author>
    <author>
      <name>Ahsan Javed Awan</name>
    </author>
    <author>
      <name>Sander Stuijk</name>
    </author>
    <author>
      <name>Roel Jordans</name>
    </author>
    <author>
      <name>Henk Corporaal</name>
    </author>
    <author>
      <name>Albert-Jan Boonstra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.02640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.03664v1</id>
    <updated>2019-08-10T01:54:08Z</updated>
    <published>2019-08-10T01:54:08Z</published>
    <title>Work-in-Progress: A Simulation Framework for Domain-Specific
  System-on-Chips</title>
    <summary>  Heterogeneous system-on-chips (SoCs) have become the standard embedded
computing platforms due to their potential to deliver superior performance and
energy efficiency compared to homogeneous architectures. They can be
particularly suited to target a specific domain of applications. However, this
potential is contingent upon optimizing the SoC for the target domain and
utilizing its resources effectively at run-time. Cycle-accurate instruction set
simulators are not suitable for this optimization, since meaningful temperature
and power consumption evaluations require simulating seconds, if not minutes,
of workloads.
  This paper presents a system-level domain-specific SoC simulation (DS3)
framework to address this need. DS3 enables both design space exploration and
dynamic resource management for power-performance optimization for domain
applications with$~600\times$ speedup compared to commonly used gem5 simulator.
We showcase DS3 using five applications from wireless communications and radar
processing domain. DS3, as well as the reference applications, will be shared
as open-source software to stimulate research in this area.
</summary>
    <author>
      <name>Samet E. Arda</name>
    </author>
    <author>
      <name>Anish NK</name>
    </author>
    <author>
      <name>A. Alper Goksoy</name>
    </author>
    <author>
      <name>Joshua Mack</name>
    </author>
    <author>
      <name>Nirmal Kumbhare</name>
    </author>
    <author>
      <name>Anderson L. Sartor</name>
    </author>
    <author>
      <name>Ali Akoglu</name>
    </author>
    <author>
      <name>Radu Marculescu</name>
    </author>
    <author>
      <name>Umit Y. Ogras</name>
    </author>
    <link href="http://arxiv.org/abs/1908.03664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.03664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.06362v2</id>
    <updated>2020-11-30T23:27:51Z</updated>
    <published>2019-08-18T01:36:35Z</published>
    <title>Near Data Acceleration with Concurrent Host Access</title>
    <summary>  Near-data accelerators (NDAs) that are integrated with main memory have the
potential for significant power and performance benefits. Fully realizing these
benefits requires the large available memory capacity to be shared between the
host and the NDAs in a way that permits both regular memory access by some
applications and accelerating others with an NDA, avoids copying data, enables
collaborative processing, and simultaneously offers high performance for both
host and NDA. We identify and solve new challenges in this context: mitigating
row-locality interference from host to NDAs, reducing read/write-turnaround
overhead caused by fine-grain interleaving of host and NDA requests,
architecting a memory layout that supports the locality required for NDAs and
sophisticated address interleaving for host performance, and supporting both
packetized and traditional memory interfaces. We demonstrate our approach in a
simulated system that consists of a multi-core CPU and NDA-enabled DDR4 memory
modules. We show that our mechanisms enable effective and efficient concurrent
access using a set of microbenchmarks, and then demonstrate the potential of
the system for the important stochastic variance-reduced gradient (SVRG)
algorithm.
</summary>
    <author>
      <name>Benjamin Y. Cho</name>
    </author>
    <author>
      <name>Yongkee Kwon</name>
    </author>
    <author>
      <name>Sangkug Lym</name>
    </author>
    <author>
      <name>Mattan Erez</name>
    </author>
    <link href="http://arxiv.org/abs/1908.06362v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.06362v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.07299v1</id>
    <updated>2019-08-20T12:27:06Z</updated>
    <published>2019-08-20T12:27:06Z</published>
    <title>Comparing ternary and binary adders and multipliers</title>
    <summary>  While many papers have proposed implementations of ternary adders and ternary
multipliers, no comparisons have generally been done with the corresponding
binary ones. We compare the implementations of binary and ternary adders and
multipliers with the same computing capability according to the basic blocks
that are 1-bit and 1-trit adders and 1-bit and 1-trit multipliers. Then we
compare the complexity of these basic blocks by using the same CNTFET
technology to evaluate the overall complexity of N-bit adders and M-trit adders
on one side, and NxN bit multipliers and MxM trits multipliers with M = N/IR
(IR = log(3)/log(2) is the information ratio). While ternary adders and
multipliers have less input and output connections and use less basic building
blocks, the complexity of the ternary building blocks is too high and the
ternary adders and multipliers cannot compete with the binary ones.
</summary>
    <author>
      <name>Daniel Etiemble</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.07299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.07299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.09297v1</id>
    <updated>2019-08-25T10:37:29Z</updated>
    <published>2019-08-25T10:37:29Z</published>
    <title>4-Bit High-Speed Binary Ling Adder</title>
    <summary>  Binary addition is one of the most primitive and most commonly used
applications in computer arithmetic. A large variety of algorithms and
implementations have been proposed for binary addition. Huey Ling proposed a
simpler form of CLA equations which rely on adjacent pair bits. Along with bit
generate and bit propagate, we introduce another prefix bit, the half sum bit.
Ling adder increases the speed of n-bit binary addition, which is an upgrade
from the existing Carry-Look-Ahead adder. Several variants of the carry
look-ahead equations, like Ling carries, have been presented that simplify
carry computation and can lead to faster structures. Ling adders, make use of
Ling carry and propagate bits, in order to calculate the sum bit. As a result,
dependency on the previous bit addition is reduced; that is, ripple effect is
lowered. This paper provides a comparative study on the implementation of the
above mentioned high-speed adders.
</summary>
    <author>
      <name>Projjal Gupta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.15095.68002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.15095.68002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.09297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.09297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.09922v1</id>
    <updated>2019-08-26T21:04:37Z</updated>
    <published>2019-08-26T21:04:37Z</published>
    <title>Tvarak: Software-managed hardware offload for DAX NVM storage redundancy</title>
    <summary>  Tvarak efficiently implements system-level redundancy for direct-access (DAX)
NVM storage. Production storage systems complement device-level ECC (which
covers media errors) with system-checksums and cross-device parity. This
system-level redundancy enables detection of and recovery from data corruption
due to device firmware bugs (e.g., reading data from the wrong physical
location). Direct access to NVM penalizes software-only implementations of
system-level redundancy, forcing a choice between lack of data protection or
significant performance penalties. Offloading the update and verification of
system-level redundancy to Tvarak, a hardware controller co-located with the
last-level cache, enables efficient protection of data from such bugs in memory
controller and NVM DIMM firmware. Simulation-based evaluation with seven
data-intensive applications shows Tvarak's performance and energy efficiency.
For example, Tvarak reduces Redis set-only performance by only 3%, compared to
50% reduction for a state-of-the-art software-only approach.
</summary>
    <author>
      <name>Rajat Kateja</name>
    </author>
    <author>
      <name>Nathan Beckmann</name>
    </author>
    <author>
      <name>Gregory R. Ganger</name>
    </author>
    <link href="http://arxiv.org/abs/1908.09922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.09922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.09930v1</id>
    <updated>2019-08-26T21:30:44Z</updated>
    <published>2019-08-26T21:30:44Z</published>
    <title>Cyclic Sequence Generators as Program Counters for High-Speed FPGA-based
  Processors</title>
    <summary>  This paper compares the performance of conventional radix-2 program counters
with program counters based on Feedback Shift Registers (FSRs), a class of
cyclic sequence generator. FSR counters have constant time scaling with
bit-width, $N$, whereas FPGA-based radix-2 counters typically have $O(N)$
time-complexity due to the carry-chain. Program counter performance is measured
by synthesis of standalone counter circuits, as well as synthesis of three
FPGA-based processor designs modified to incorporate FSR program counters.
Hybrid counters, combining both an FSR and a radix-2 counter, are presented as
a solution to the potential cache-coherency issues of FSR program counters.
Results show that high-speed processor designs benefit more from FSR program
counters, allowing both greater operating frequency and the use of fewer logic
resources.
</summary>
    <author>
      <name>P. A. Suggate</name>
    </author>
    <author>
      <name>R. W. Ward</name>
    </author>
    <author>
      <name>T. C. A. Molteno</name>
    </author>
    <link href="http://arxiv.org/abs/1908.09930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.09930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.00098v1</id>
    <updated>2019-09-27T06:08:56Z</updated>
    <published>2019-09-27T06:08:56Z</published>
    <title>Analysis and Design of a 32nm FinFET Dynamic Latch Comparator</title>
    <summary>  Comparators have multifarious applications in various fields, especially used
in analog to digital converters. Over the years, we have seen many different
designs of single stage, dynamic latch type and double tail type comparators
based on CMOS technology, and all of them had to make the tradeoff between
power consumption and delay time. Meanwhile, to mitigate the short channel
effects of conventional CMOS based design, FinFET has emerged as the most
promising alternative by owning the tremendous gate control feature over the
channel region. In this paper, we have analyzed the performance of some recent
dynamic latch type comparators and proposed a new structure of dynamic latch
comparator; moreover, 32nm FinFET technology has been considered as the common
platform for all of the comparators circuit design. The proposed comparator has
shown impressive performance in case of power consumption, time delay, power
delay product and offset voltage while compared with the other recent
comparators through simulations with LTspice.
</summary>
    <author>
      <name>Mir Muntasir Hossain</name>
    </author>
    <author>
      <name>Satyendra N. Biswas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICAEE48663.2019.8975615</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICAEE48663.2019.8975615" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.00098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.00098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.00134v1</id>
    <updated>2019-09-30T22:18:22Z</updated>
    <published>2019-09-30T22:18:22Z</published>
    <title>Optimizing GPU Cache Policies for MI Workloads</title>
    <summary>  In recent years, machine intelligence (MI) applications have emerged as a
major driver for the computing industry. Optimizing these workloads is
important but complicated. As memory demands grow and data movement overheads
increasingly limit performance, determining the best GPU caching policy to use
for a diverse range of MI workloads represents one important challenge. To
study this, we evaluate 17 MI applications and characterize their behaviors
using a range of GPU caching strategies. In our evaluations, we find that the
choice of caching policy in GPU caches involves multiple performance trade-offs
and interactions, and there is no one-size-fits-all GPU caching policy for MI
workloads. Based on detailed simulation results, we motivate and evaluate a set
of cache optimizations that consistently match the performance of the best
static GPU caching policies.
</summary>
    <author>
      <name>Johnathan Alsop</name>
    </author>
    <author>
      <name>Matthew D. Sinclair</name>
    </author>
    <author>
      <name>Srikant Bharadwaj</name>
    </author>
    <author>
      <name>Alexandru Dutu</name>
    </author>
    <author>
      <name>Anthony Gutierrez</name>
    </author>
    <author>
      <name>Onur Kayiran</name>
    </author>
    <author>
      <name>Michael LeBeane</name>
    </author>
    <author>
      <name>Sooraj Puthoor</name>
    </author>
    <author>
      <name>Xianwei Zhang</name>
    </author>
    <author>
      <name>Tsung Tai Yeh</name>
    </author>
    <author>
      <name>Bradford M. Beckmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of short paper published in the 2019 IEEE
  International Symposium on Workload Characterization</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.00134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.00134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.00197v1</id>
    <updated>2019-10-01T04:19:40Z</updated>
    <published>2019-10-01T04:19:40Z</published>
    <title>UltraShare: FPGA-based Dynamic Accelerator Sharing and Allocation</title>
    <summary>  Despite all the available commercial and open-source frameworks to ease
deploying FPGAs in accelerating applications, the current schemes fail to
support sharing multiple accelerators among various applications. There are
three main features that an accelerator sharing scheme requires to support:
exploiting dynamic parallelism of multiple accelerators for a single
application, sharing accelerators among multiple applications, and providing a
non-blocking congestion-free environment for applications to invoke the
accelerators. In this paper, we developed a scalable fully functional hardware
controller, called UltraShare, with a supporting software stack that provides a
dynamic accelerator sharing scheme through an accelerators grouping mechanism.
UltraShare allows software applications to fully utilize FPGA accelerators in a
non-blocking congestion-free environment. Our experimental results for a simple
scenario of a combination of three streaming accelerators invocation show an
improvement of up to 8x in throughput of the accelerators by removing
accelerators idle times.
</summary>
    <author>
      <name>Siavash Rezaei</name>
    </author>
    <author>
      <name>Eli Bozorgzadeh</name>
    </author>
    <author>
      <name>Kanghee Kim</name>
    </author>
    <link href="http://arxiv.org/abs/1910.00197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.00197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.04436v1</id>
    <updated>2019-10-10T08:44:41Z</updated>
    <published>2019-10-10T08:44:41Z</published>
    <title>hlslib: Software Engineering for Hardware Design</title>
    <summary>  High-level synthesis (HLS) tools have brought FPGA development into the
mainstream, by allowing programmers to design architectures using familiar
languages such as C, C++, and OpenCL. While the move to these languages has
brought significant benefits, many aspects of traditional software engineering
are still unsupported, or not exploited by developers in practice. Furthermore,
designing reconfigurable architectures requires support for hardware
constructs, such as FIFOs and shift registers, that are not native to
CPU-oriented languages. To address this gap, we have developed hlslib, a
collection of software tools, plug-in hardware modules, and code samples,
designed to enhance the productivity of HLS developers. The goal of hlslib is
two-fold: first, create a community-driven arena of bleeding edge development,
which can move quicker, and provides more powerful abstractions than what is
provided by vendors; and second, collect a wide range of example codes, both
minimal proofs of concept, and larger, real-world applications, that can be
reused directly or inspire other work. hlslib is offered as an open source
library, containing CMake files, C++ headers, convenience scripts, and examples
codes, and is receptive to any contribution that can benefit HLS developers,
through general functionality or examples.
</summary>
    <author>
      <name>Johannes de Fine Licht</name>
    </author>
    <author>
      <name>Torsten Hoefler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages extended abstract accepted to H2RC'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.04436v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.04436v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.04683v1</id>
    <updated>2019-10-01T04:06:24Z</updated>
    <published>2019-10-01T04:06:24Z</published>
    <title>A Novel Low Power Non-Volatile SRAM Cell with Self Write Termination</title>
    <summary>  A non-volatile SRAM cell is proposed for low power applications using Spin
Transfer Torque-Magnetic Tunnel Junction (STT-MTJ) devices. This novel cell
offers non-volatile storage, thus allowing selected blocks of SRAM to be
switched off during standby operation. To further increase the power savings, a
write termination circuit is designed which detects completion of MTJ write and
closes the bidirectional current path for the MTJ. A reduction of 25.81% in the
number of transistors and a reduction of 2.95% in the power consumption is
achieved in comparison to prior work on write termination circuits.
</summary>
    <author>
      <name>Kanika Monga</name>
    </author>
    <author>
      <name>Akul Malhotra</name>
    </author>
    <author>
      <name>Nitin Chaturvedi</name>
    </author>
    <author>
      <name>S. Gurunayaranan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at: THE 10th INTERNATIONAL CONFERENCE ON COMPUTING,
  COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT), 2019 Other information: 4
  Pages, 2 figures, 2 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.04683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.04683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.05100v2</id>
    <updated>2019-10-26T08:53:37Z</updated>
    <published>2019-10-11T12:11:32Z</published>
    <title>Run-Time-Reconfigurable Multi-Precision Floating-Point Matrix Multiplier
  Intellectual Property Core on FPGA</title>
    <summary>  In todays world, high-power computing applications such as image processing,
digital signal processing, graphics, and robotics require enormous computing
power. These applications use matrix operations, especially matrix
multiplication. Multiplication operations require a lot of computational time
and are also complex in design. We can use field-programmable gate arrays as
low-cost hardware accelerators along with a low-cost general-purpose processor
instead of a high-cost application-specific processor for such applications. In
this work, we employ an efficient Strassens algorithm for matrix multiplication
and a highly efficient run-time-reconfigurable floating-point multiplier for
matrix element multiplication. The run-time-reconfigurable floating-point
multiplier is implemented with custom floating-point format for
variable-precision applications. A very efficient combination of Karatsuba
algorithm and Urdhva Tiryagbhyam algorithm is used to implement the binary
multiplier. This design can effectively adjust the power and delay requirements
according to different accuracy requirements by reconfiguring itself during run
time.
</summary>
    <author>
      <name>Arish S</name>
    </author>
    <author>
      <name>R. K. Sharma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00034-016-0335-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00034-016-0335-2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Circuits, Systems, and Signal Processing, March 2017, Volume 36,
  Issue 3</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.05100v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.05100v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.10794v1</id>
    <updated>2019-10-23T20:07:40Z</updated>
    <published>2019-10-23T20:07:40Z</published>
    <title>Sidebar: Scratchpad Based Communication Between CPUs and Accelerators</title>
    <summary>  Hardware accelerators for neural networks have shown great promise for both
performance and power. These accelerators are at their most efficient when
optimized for a fixed functionality. But this inflexibility limits the
longevity of the hardware itself as the underlying neural network algorithms
and structures undergo improvements and changes. We propose and evaluate a
flexible design paradigm for accelerators with a close coordination with host
processors. The relatively static matrix operations are implemented in
specialized accelerators while fast-evolving functions, such as activations,
are computed on the host processor. This architecture is enabled by a low
latency shared buffer we call Sidebar. Sidebar memory is shared between the
accelerator and host, exists outside of program address space and holds
intermediate data only. We show that a generalised DMA dependent flexible
accelerator design performs poorly in both perf and energy as compared to an
equivalent fixed function accelerator. Sidebar based accelerator design
achieves near identical performance and energy to equivalent fixed function
accelerator while still providing all the flexibility of computing activations
on the host processor.
</summary>
    <author>
      <name>Ayoosh Bansal</name>
    </author>
    <author>
      <name>Chance Coats</name>
    </author>
    <author>
      <name>Evan Lissoos</name>
    </author>
    <author>
      <name>Benjamin Schreiber</name>
    </author>
    <link href="http://arxiv.org/abs/1910.10794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.13683v1</id>
    <updated>2019-10-30T05:45:37Z</updated>
    <published>2019-10-30T05:45:37Z</published>
    <title>Scalable High Performance SDN Switch Architecture on FPGA for Core
  Networks</title>
    <summary>  Due to the increasing heterogeneity in network user requirements, dynamically
varying day to day network traffic patterns and delay in-network service
deployment, there is a huge demand for scalability and flexibility in modern
networking infrastructure, which in return has paved way for the introduction
of Software Defined Networking (SDN) in core networks. In this paper, we
present an FPGA-based switch that is fully compliant with OpenFlow; the
pioneering protocol for southbound interface of SDN. The switch architecture is
completely implemented on hardware. The design consists of an OpenFlow
Southbound agent which can process OpenFlow packets at a rate of 10Gbps. The
proposed architecture speed scales up to 400Gbps while it consumes only 60% of
resources on a Xilinx Virtex-7 featuring XC7VX485T FPGA.
</summary>
    <author>
      <name>Sasindu Wijeratne</name>
    </author>
    <author>
      <name>Ashen Ekanayake</name>
    </author>
    <author>
      <name>Sandaruwan Jayaweera</name>
    </author>
    <author>
      <name>Danuka Ravishan</name>
    </author>
    <author>
      <name>Ajith Pasqual</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.13683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.13683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03364v1</id>
    <updated>2019-11-08T16:43:54Z</updated>
    <published>2019-11-08T16:43:54Z</published>
    <title>AMOEBA: A Coarse Grained Reconfigurable Architecture for Dynamic GPU
  Scaling</title>
    <summary>  Different GPU applications exhibit varying scalability patterns with
network-on-chip (NoC), coalescing, memory and control divergence, and L1 cache
behavior. A GPU consists of several StreamingMulti-processors (SMs) that
collectively determine how shared resources are partitioned and accessed.
Recent years have seen divergent paths in SM scaling towards scale-up (fewer,
larger SMs) vs. scale-out (more, smaller SMs). However, neither scaling up nor
scaling out can meet the scalability requirement of all applications running on
a given GPU system, which inevitably results in performance degradation and
resource under-utilization for some applications. In this work, we investigate
major design parameters that influence GPU scaling. We then propose AMOEBA, a
solution to GPU scaling through reconfigurable SM cores. AMOEBA monitors and
predicts application scalability at run-time and adjusts the SM configuration
to meet program requirements. AMOEBA also enables dynamic creation of
heterogeneous SMs through independent fusing or splitting. AMOEBA is a
microarchitecture-based solution and requires no additional programming effort
or custom compiler support. Our experimental evaluations with application
programs from various benchmark suites indicate that AMOEBA is able to achieve
a maximum performance gain of 4.3x, and generates an average performance
improvement of 47% when considering all benchmarks tested.
</summary>
    <author>
      <name>Xianwei Cheng</name>
    </author>
    <author>
      <name>Hui Zhao</name>
    </author>
    <author>
      <name>Mahmut Kandemir</name>
    </author>
    <author>
      <name>Beilei Jiang</name>
    </author>
    <author>
      <name>Gayatri Mehta</name>
    </author>
    <link href="http://arxiv.org/abs/1911.03364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.05101v1</id>
    <updated>2019-11-12T19:10:18Z</updated>
    <published>2019-11-12T19:10:18Z</published>
    <title>Coordinated Management of DVFS and Cache Partitioning under QoS
  Constraints to Save Energy in Multi-Core Systems</title>
    <summary>  Reducing the energy expended to carry out a computational task is important.
In this work, we explore the prospects of meeting Quality-of-Service
requirements of tasks on a multi-core system while adjusting resources to
expend a minimum of energy. This paper considers, for the first time, a
QoS-driven coordinated resource management algorithm (RMA) that dynamically
adjusts the size of the per-core last-level cache partitions and the per-core
voltage-frequency settings to save energy while respecting QoS requirements of
every application in multi-programmed workloads run on multi-core systems. It
does so by doing configuration-space exploration across the spectrum of LLC
partition sizes and Dynamic Voltage Frequency Scaling (DVFS) settings at
runtime at negligible overhead. We show that the energy of 4-core and 8-core
systems can be reduced by up to 18% and 14%, respectively, compared to a
baseline with even distribution of cache resources and a fixed mid-range core
voltage-frequency setting. The energy savings can potentially reach 29% if the
QoS targets are relaxed to 40% longer execution time.
</summary>
    <author>
      <name>Mehrzad Nejat</name>
    </author>
    <author>
      <name>Madhavan Manivannan</name>
    </author>
    <author>
      <name>Miquel Pericas</name>
    </author>
    <author>
      <name>Per Stenstrom</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the Journal of Parallel and Distributed Computing (Nov
  2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.05101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.05101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.06859v1</id>
    <updated>2019-11-15T20:10:01Z</updated>
    <published>2019-11-15T20:10:01Z</published>
    <title>NeuMMU: Architectural Support for Efficient Address Translations in
  Neural Processing Units</title>
    <summary>  To satisfy the compute and memory demands of deep neural networks, neural
processing units (NPUs) are widely being utilized for accelerating deep
learning algorithms. Similar to how GPUs have evolved from a slave device into
a mainstream processor architecture, it is likely that NPUs will become first
class citizens in this fast-evolving heterogeneous architecture space. This
paper makes a case for enabling address translation in NPUs to decouple the
virtual and physical memory address space. Through a careful data-driven
application characterization study, we root-cause several limitations of prior
GPU-centric address translation schemes and propose a memory management unit
(MMU) that is tailored for NPUs. Compared to an oracular MMU design point, our
proposal incurs only an average 0.06% performance overhead.
</summary>
    <author>
      <name>Bongjoon Hyun</name>
    </author>
    <author>
      <name>Youngeun Kwon</name>
    </author>
    <author>
      <name>Yujeong Choi</name>
    </author>
    <author>
      <name>John Kim</name>
    </author>
    <author>
      <name>Minsoo Rhu</name>
    </author>
    <link href="http://arxiv.org/abs/1911.06859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.10349v3</id>
    <updated>2019-12-11T12:08:14Z</updated>
    <published>2019-11-23T11:06:22Z</published>
    <title>Arsenal of Hardware Prefetchers</title>
    <summary>  Hardware prefetching is one of the latency tolerance optimization techniques
that tolerate costly DRAM accesses. Though hardware prefetching is one of the
fundamental mechanisms prevalent on most of the commercial machines, there is
no prefetching technique that works well across all the access patterns and
different types of workloads. Through this paper, we propose Arsenal, a
prefetching framework which allows the advantages provided by different data
prefetchers to be combined, by dynamically selecting the best-suited prefetcher
for the current workload. Thus effectively improving the versatility of the
prefetching system. It bases on the classic Sandbox prefetcher that dynamically
adapts and utilizes multiple offsets for sequential prefetchers. We take it to
the next step by switching between prefetchers like Multi look Ahead Offset
Prefetching and Timing SKID Prefetcher on the run. Arsenal utilizes a
space-efficient pooling filter, Bloom filters, that keeps track of useful
prefetches of each of these component prefetchers and thus helps to maintain a
score for each of the component prefetchers. This approach is shown to provide
better speedup than anyone prefetcher alone. Arsenal provides a performance
improvement of 44.29% on the single-core mixes and 19.5% for some of the
selected 25 representative multi-core mixes.
</summary>
    <author>
      <name>Dishank Yadav</name>
    </author>
    <author>
      <name>Chaitanya Paikara</name>
    </author>
    <link href="http://arxiv.org/abs/1911.10349v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.10349v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.11768v1</id>
    <updated>2019-11-26T06:29:05Z</updated>
    <published>2019-11-26T06:29:05Z</published>
    <title>3D IC optimal layout design. A parallel and distributed topological
  approach</title>
    <summary>  The task of 3D ICs layout design involves the assembly of millions of
components taking into account many different requirements and constraints such
as topological, wiring or manufacturability ones. It is a NP-hard problem that
requires new non-deterministic and heuristic algorithms. Considering the time
complexity, the commonly applied Fiduccia-Mattheyses partitioning algorithm is
superior to any other local search method. Nevertheless, it can often miss to
reach a quasi-optimal solution in 3D spaces. The presented approach uses an
original 3D layout graph partitioning heuristics implemented with use of the
extremal optimization method. The goal is to minimize the total wire-length in
the chip. In order to improve the time complexity a parallel and distributed
Java implementation is applied. Inside one Java Virtual Machine separate
optimization algorithms are executed by independent threads. The work may also
be shared among different machines by means of The Java Remote Method
Invocation system.
</summary>
    <author>
      <name>Katarzyna Grzesiak-Kopeć</name>
    </author>
    <author>
      <name>Maciej Ogorzałek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.11768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.01664v1</id>
    <updated>2019-12-03T20:02:41Z</updated>
    <published>2019-12-03T20:02:41Z</published>
    <title>Understanding the Impact of On-chip Communication on DNN Accelerator
  Performance</title>
    <summary>  Deep Neural Networks have flourished at an unprecedented pace in recent
years. They have achieved outstanding accuracy in fields such as computer
vision, natural language processing, medicine or economics. Specifically,
Convolutional Neural Networks (CNN) are particularly suited to object
recognition or identification tasks. This, however, comes at a high
computational cost, prompting the use of specialized GPU architectures or even
ASICs to achieve high speeds and energy efficiency. ASIC accelerators
streamline the execution of certain dataflows amenable to CNN computation that
imply the constant movement of large amounts of data, thereby turning on-chip
communication into a critical function within the accelerator. This paper
studies the communication flows within CNN inference accelerators of edge
devices, with the aim to justify current and future decisions in the design of
the on-chip networks that interconnect their processing elements. Leveraging
this analysis, we then qualitatively discuss the potential impact of
introducing the novel paradigm of wireless on-chip network in this context.
</summary>
    <author>
      <name>Robert Guirado</name>
    </author>
    <author>
      <name>Hyoukjun Kwon</name>
    </author>
    <author>
      <name>Eduard Alarcón</name>
    </author>
    <author>
      <name>Sergi Abadal</name>
    </author>
    <author>
      <name>Tushar Krishna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICECS2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.01664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.01664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.10663v1</id>
    <updated>2019-12-23T07:52:37Z</updated>
    <published>2019-12-23T07:52:37Z</published>
    <title>SSR: A Stall Scheme Reducing Bubbles in Load-Use Hazard of RISC-V
  Pipeline</title>
    <summary>  Modern processors usually adopt pipeline structure and often load data from
memory. At that point, the load-use hazard will inevitably occur, which usually
stall the pipeline and reduce performance. This paper introduces and compares
two schemes to solve load-use hazard. One is the traditional scheme that detect
hazard between ID stage and EXE stage, which stalls the pipeline and insert
bubbles between the two instructions. In the scheme we proposed, we add a
simple bypass unit between EXE and MEM stage that disables the stall of
load-use hazard caused by the traditional scheme, which can reduce the bubble
between the two instructions. It's quite a considerable benefit in eliminating
bubbles especially in the long pipeline or programs of plenty load
instructions. The scheme was implemented in the open source RISC-V SoC
generator Rocket-chip and synthesized in SMIC 130-nm technology. The results
show that the performance of the latter scheme is increased by 6.9% in the
Dhrystone benchmark with the reasonable cost of area and power.
</summary>
    <author>
      <name>Dongchu Su</name>
    </author>
    <author>
      <name>Yong Li</name>
    </author>
    <author>
      <name>Bo Yuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The International Conference on Advanced Information Networking and
  Applications (AINA-2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.10663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.10663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.02072v1</id>
    <updated>2020-02-06T02:24:15Z</updated>
    <published>2020-02-06T02:24:15Z</published>
    <title>Fast FPGA emulation of analog dynamics in digitally-driven systems</title>
    <summary>  In this paper, we propose an architecture for FPGA emulation of mixed-signal
systems that achieves high accuracy at a high throughput. We represent the
analog output of a block as a superposition of step responses to changes in its
analog input, and the output is evaluated only when needed by the digital
subsystem. Our architecture is therefore intended for digitally-driven systems;
that is, those in which the inputs of analog dynamical blocks change only on
digital clock edges. We implemented a high-speed link transceiver design using
the proposed architecture on a Xilinx FPGA. This design demonstrates how our
approach breaks the link between simulation rate and time resolution that is
characteristic of prior approaches. The emulator is flexible, allowing for the
real-time adjustment of analog dynamics, clock jitter, and various design
parameters. We demonstrate that our architecture achieves 1% accuracy while
running 3 orders of magnitude faster than a comparable high-performance CPU
simulation.
</summary>
    <author>
      <name>Steven Herbst</name>
    </author>
    <author>
      <name>Byong Chan Lim</name>
    </author>
    <author>
      <name>Mark Horowitz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3240765.3240808</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3240765.3240808" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCAD '18: Proceedings of the International Conference on
  Computer-Aided Design</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.02072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.02072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.02094v1</id>
    <updated>2020-02-06T04:33:15Z</updated>
    <published>2020-02-06T04:33:15Z</published>
    <title>Low Overhead Online Data Flow Tracking for Intermittently Powered
  Non-volatile FPGAs</title>
    <summary>  Energy harvesting is an attractive way to power future IoT devices since it
can eliminate the need for battery or power cables. However, harvested energy
is intrinsically unstable. While FPGAs have been widely adopted in various
embedded systems, it is hard to survive unstable power since all the memory
components in FPGA are based on volatile SRAMs. The emerging non-volatile
memory based FPGAs provide promising potentials to keep configuration data on
the chip during power outages. Few works have considered implementing efficient
runtime intermediate data checkpoint on non-volatile FPGAs. To realize
accumulative computation under intermittent power on FPGA, this paper proposes
a low-cost design framework, Data-Flow-Tracking FPGA (DFT-FPGA), which utilizes
binary counters to track intermediate data flow. Instead of keeping all on-chip
intermediate data, DFT-FPGA only targets on necessary data that is labeled by
off-line analysis and identified by an online tracking system. The evaluation
shows that compared with state-of-the-art techniques, DFT-FPGA can realize
accumulative computing with less off-line workload and significantly reduce
online roll-back time and resource utilization.
</summary>
    <author>
      <name>Xinyi Zhang</name>
    </author>
    <author>
      <name>Clay Patterson</name>
    </author>
    <author>
      <name>Yongpan Liu</name>
    </author>
    <author>
      <name>Chengmo Yang</name>
    </author>
    <author>
      <name>Chun Jason Xue</name>
    </author>
    <author>
      <name>Jingtong Hu</name>
    </author>
    <link href="http://arxiv.org/abs/2002.02094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.02094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.03568v1</id>
    <updated>2020-02-10T05:54:26Z</updated>
    <published>2020-02-10T05:54:26Z</published>
    <title>RVCoreP : An optimized RISC-V soft processor of five-stage pipelining</title>
    <summary>  RISC-V is a RISC based open and loyalty free instruction set architecture
which has been developed since 2010, and can be used for cost-effective soft
processors on FPGAs. The basic 32-bit integer instruction set in RISC-V is
defined as RV32I, which is sufficient to support the operating system
environment and suits for embedded systems. In this paper, we propose an
optimized RV32I soft processor named RVCoreP adopting five-stage pipelining.
The processor applies three effective optimization methods to improve the
operating frequency. These methods are instruction fetch unit optimization
including pipelined branch prediction mechanism, ALU optimization, and data
alignment and sign-extension optimization for data memory output. We implement
RVCoreP in Verilog HDL and verify the behavior using Verilog simulation and an
actual Xilinx Atrix-7 FPGA board. We evaluate IPC (instructions per cycle),
operating frequency, hardware resource utilization, and processor performance.
From the evaluation results, we show that RVCoreP achieves 30.0% performance
improvement compared with VexRiscv, which is a high-performance and open source
RV32I processor selected from some related works.
</summary>
    <author>
      <name>Hiromu Miyazaki</name>
    </author>
    <author>
      <name>Takuto Kanamori</name>
    </author>
    <author>
      <name>Md Ashraful Islam</name>
    </author>
    <author>
      <name>Kenji Kise</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1587/transinf.2020PAP0015</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1587/transinf.2020PAP0015" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 9 figures, this paper is submitted to the Institute of
  Electronics, Information and Communication Engineers (IEICE)</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.03568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.03576v2</id>
    <updated>2020-03-27T09:11:00Z</updated>
    <published>2020-02-10T06:44:02Z</published>
    <title>A portable and Linux capable RISC-V computer system in Verilog HDL</title>
    <summary>  RISC-V is an open and royalty free instruction set architecture which has
been developed at the University of California, Berkeley. The processors using
RISC-V can be designed and released freely. Because of this, various processor
cores and system on chips (SoCs) have been released so far. However, there are
a few public RISC-V computer systems that are portable and can boot Linux
operating systems. In this paper, we describe a portable and Linux capable
RISC-V computer system targeting FPGAs in Verilog HDL. This system can be
implemented on an FPGA with fewer hardware resources, and can be implemented on
low cost FPGAs or customized by introducing an accelerator. This paper also
describes the knowledge obtained through the development of this RISC-V
computer system.
</summary>
    <author>
      <name>Junya Miura</name>
    </author>
    <author>
      <name>Hiromu Miyazaki</name>
    </author>
    <author>
      <name>Kenji Kise</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.03576v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03576v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.05455v1</id>
    <updated>2020-02-13T11:38:15Z</updated>
    <published>2020-02-13T11:38:15Z</published>
    <title>Functional Failure Rate Due to Single-Event Transients in Clock
  Distribution Networks</title>
    <summary>  With technology scaling, lower supply voltages, and higher operating
frequencies clock distribution networks become more and more vulnerable to
transients faults. These faults can cause circuit-wide effects and thus,
significantly contribute to the functional failure rate of the circuit. This
paper proposes a methodology to analyse how the functional behaviour is
affected by Single-Event Transients in the clock distribution network. The
approach is based on logic-level simulation and thus, only uses the
register-transfer level description of a design. Therefore, a fault model is
proposed which implements the main effects due to radiation-induced transients
in the clock network. This fault model enables the computation of the
functional failure rate caused by Single-Event Transients for each individual
clock buffer, as well as the complete network. Further, it allows the
identification of the most vulnerable flip-flops related to Single-Event
Transients in the clock network.
  The proposed methodology is applied in a practical example and a fault
injection campaign is performed. In order to evaluate the impact of
Single-Event Transients in clock distribution networks, the obtained functional
failure rate is compared to the error rate caused by Single-Event Upsets in the
sequential logic.
</summary>
    <author>
      <name>Thomas Lange</name>
    </author>
    <author>
      <name>Maximilien Glorieux</name>
    </author>
    <author>
      <name>Dan Alexandrescu</name>
    </author>
    <author>
      <name>Luca Sterpone</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DTIS.2019.8735052</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DTIS.2019.8735052" rel="related"/>
    <link href="http://arxiv.org/abs/2002.05455v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05455v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.07507v1</id>
    <updated>2020-02-18T11:55:19Z</updated>
    <published>2020-02-18T11:55:19Z</published>
    <title>Design of SEC-DED and SEC-DED-DAEC Codes of different lengths</title>
    <summary>  Reliability is an important requirement for both communication and storage
systems. Due to continuous scale down of technology multiple adjacent bits
error probability increases. The data may be corrupted due soft errors. Error
correction codes are used to detect and correct the errors. In this paper,
design of single error correction-double error detection (SEC-DED) and single
error correction-double error detection-double adjacent error correction
(SEC-DED-DAEC) codes of different data lengths have been proposed. Proposed
SEC-DED and SEC-DED-DAEC codes require lower delay and power compared to
existing coding schemes. Area complexity in terms of logic gates of proposed
and existing codes have been presented. ASIC-based synthesis results show a
notable reduction compared to existing SEC-DED codes. All the codec
architectures are synthesized on ASIC platform. Performances of different
SEC-DED-DAEC codes are tabulated in terms of area, power and delay.
</summary>
    <author>
      <name>Sayan Tripathi</name>
    </author>
    <author>
      <name>Jhilam Jana</name>
    </author>
    <author>
      <name>Jaydeb Bhaumik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.07507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.07507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.11289v1</id>
    <updated>2020-02-26T03:54:26Z</updated>
    <published>2020-02-26T03:54:26Z</published>
    <title>LORAX: Loss-Aware Approximations for Energy-Efficient Silicon Photonic
  Networks-on-Chip</title>
    <summary>  The approximate computing paradigm advocates for relaxing accuracy goals in
applications to improve energy-efficiency and performance. Recently, this
paradigm has been explored to improve the energy efficiency of silicon photonic
networks-on-chip (PNoCs). In this paper, we propose a novel framework (LORAX)
to enable more aggressive approximation during communication over silicon
photonic links in PNoCs. Given that silicon photonic interconnects have
significant power dissipation due to the laser sources that generate the
wavelengths for photonic communication, our framework attempts to reduce laser
power overheads while intelligently approximating communication such that
application output quality is not distorted beyond an acceptable limit. To the
best of our knowledge, this is the first work that considers loss-aware laser
power management and multilevel signaling to enable effective data
approximation and energy-efficiency in PNoCs. Simulation results show that our
framework can achieve up to 31.4% lower laser power consumption and up to 12.2%
better energy efficiency than the best known prior work on approximate
communication with silicon photonic interconnects, for the same application
output quality
</summary>
    <author>
      <name>Febin Sunny</name>
    </author>
    <author>
      <name>Asif Mirza</name>
    </author>
    <author>
      <name>Ishan Thakkar</name>
    </author>
    <author>
      <name>Sudeep Pasricha</name>
    </author>
    <author>
      <name>Nikdast Mahdi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted and accepted at GLSVLSI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.11289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.06727v1</id>
    <updated>2020-03-15T01:04:55Z</updated>
    <published>2020-03-15T01:04:55Z</published>
    <title>New Approximate Multiplier for Low Power Digital Signal Processing</title>
    <summary>  In this paper a low power multiplier is proposed. The proposed multiplier
utilizes Broken-Array Multiplier approximation method on the conventional
modified Booth multiplier. This method reduces the total power consumption of
multiplier up to 58% at the cost of a small decrease in output accuracy. The
proposed multiplier is compared with other approximate multipliers in terms of
power consumption and accuracy. Furthermore, to have a better evaluation of the
proposed multiplier efficiency, it has been used in designing a 30-tap low-pass
FIR filter and the power consumption and accuracy are compared with that of a
filter with conventional booth multipliers. The simulation results show a 17.1%
power reduction at the cost of only 0.4dB decrease in the output SNR.
</summary>
    <author>
      <name>Farzad Farshchi</name>
    </author>
    <author>
      <name>Muhammad Saeed Abrishami</name>
    </author>
    <author>
      <name>Sied Mehdi Fakhraie</name>
    </author>
    <link href="http://arxiv.org/abs/2003.06727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.09016v1</id>
    <updated>2020-03-19T21:00:09Z</updated>
    <published>2020-03-19T21:00:09Z</published>
    <title>DS3: A System-Level Domain-Specific System-on-Chip Simulation Framework</title>
    <summary>  Heterogeneous systems-on-chip (SoCs) are highly favorable computing platforms
due to their superior performance and energy efficiency potential compared to
homogeneous architectures. They can be further tailored to a specific domain of
applications by incorporating processing elements (PEs) that accelerate
frequently used kernels in these applications. However, this potential is
contingent upon optimizing the SoC for the target domain and utilizing its
resources effectively at runtime. To this end, system-level design - including
scheduling, power-thermal management algorithms and design space exploration
studies - plays a crucial role. This paper presents a system-level
domain-specific SoC simulation (DS3) framework to address this need. DS3
enables both design space exploration and dynamic resource management for
power-performance optimization of domain applications. We showcase DS3 using
six real-world applications from wireless communications and radar processing
domain. DS3, as well as the reference applications, is shared as open-source
software to stimulate research in this area.
</summary>
    <author>
      <name>Samet E. Arda</name>
    </author>
    <author>
      <name>Anish NK</name>
    </author>
    <author>
      <name>A. Alper Goksoy</name>
    </author>
    <author>
      <name>Nirmal Kumbhare</name>
    </author>
    <author>
      <name>Joshua Mack</name>
    </author>
    <author>
      <name>Anderson L. Sartor</name>
    </author>
    <author>
      <name>Ali Akoglu</name>
    </author>
    <author>
      <name>Radu Marculescu</name>
    </author>
    <author>
      <name>Umit Y. Ogras</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 20 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.09016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.09016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.09616v1</id>
    <updated>2020-03-21T09:53:31Z</updated>
    <published>2020-03-21T09:53:31Z</published>
    <title>Soft-Error and Hard-fault Tolerant Architecture and Routing Algorithm
  for Reliable 3D-NoC Systems</title>
    <summary>  Network-on-Chip (NoC) paradigm has been proposed as an auspicious solution to
handle the strict communication requirements between the increasingly large
number of cores on a single multi and many-core chips. However, NoC systems are
exposed to a variety of manufacturing, design and energetic particles factors
making them vulnerable to permanent (hard) faults and transient (soft) errors.
In this paper, we present a comprehensive soft error and hard fault tolerant
3D-NoC architecture, named 3D-Hard-Fault-Soft-Error-Tolerant-OASIS-NoC
(3D-FETO). With the aid of adaptive algorithms, 3D-FETO is capable of detecting
and recovering from soft errors occurring in the routing pipeline stages and is
leveraging on reconfigurable components to handle permanent faults occurrence
in links, input buffers, and crossbar. In-depth evaluation results show that
the 3D-FETO system is able to work around different kinds of hard faults and
soft errors while ensuring graceful performance degradation, minimizing the
additional hardware complexity and remaining power-efficient.
</summary>
    <author>
      <name>Khanh N. Dang</name>
    </author>
    <author>
      <name>Yuichi Okuyama</name>
    </author>
    <author>
      <name>Abderazek Ben Abdallah</name>
    </author>
    <link href="http://arxiv.org/abs/2003.09616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.09616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.09617v1</id>
    <updated>2020-03-21T09:57:53Z</updated>
    <published>2020-03-21T09:57:53Z</published>
    <title>Reliability Assessment and Quantitative Evaluation of Soft-Error
  Resilient 3D Network-on-Chip Systems</title>
    <summary>  Three-Dimensional Networks-on-Chips (3D-NoCs) have been proposed as an
auspicious solution, merging the high parallelism of the Network-on-Chip (NoC)
paradigm with the high-performance and low-power cost of 3D-ICs. However, as
technology scales down, the reliability issues are becoming more crucial,
especially for complex 3D-NoC which provides the communication requirements of
multi and many-core systems-on-chip. Reliability assessment is prominent for
early stages of the manufacturing process to prevent costly redesigns of a
target system. In this paper, we present an accurate reliability assessment and
quantitative evaluation of a soft-error resilient 3D-NoC based on a soft-error
resilient mechanism. The system can recover from transient errors occurring in
different pipeline stages of the router. Based on this analysis, the effects of
failures in the network's principal components are determined.
</summary>
    <author>
      <name>Khanh N Dang</name>
    </author>
    <author>
      <name>Michael Meyer</name>
    </author>
    <author>
      <name>Yuichi Okuyama</name>
    </author>
    <author>
      <name>Abderazek Ben Abdallah</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ATS.2016.37</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ATS.2016.37" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2016 IEEE 25th Asian Test Symposium (ATS)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.09617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.09617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.10472v1</id>
    <updated>2020-03-23T18:05:22Z</updated>
    <published>2020-03-23T18:05:22Z</published>
    <title>A distributed memory, local configuration technique for re-configurable
  logic designs</title>
    <summary>  The use and location of memory in integrated circuits plays a key factor in
their performance. Memory requires large physical area, access times limit
overall system performance and connectivity can result in large fan-out. Modern
FPGA systems and ASICs contain an area of memory used to set the operation of
the device from a series of commands set by a host. Implementing these settings
registers requires a level of care otherwise the resulting implementation can
result in a number of large fan-out nets that consume valuable resources
complicating the placement of timing critical pathways. This paper presents an
architecture for implementing and programming these settings registers in a
distributed method across an FPGA and how the presented architecture works in
both clock-domain crossing and dynamic partial re-configuration applications.
The design is compared to that of a `global' settings register architecture. We
implement the architectures using Intel FPGAs Quartus Prime software targeting
an Intel FPGA Cyclone V. It is shown that the distributed memory architecture
has a smaller resource cost (as small as 25% of the ALMs and 20% of the
registers) compared to the global memory architectures.
</summary>
    <author>
      <name>Alexander E. Beasley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 files, 19 images</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.10472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.10472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.11455v1</id>
    <updated>2020-03-25T15:48:54Z</updated>
    <published>2020-03-25T15:48:54Z</published>
    <title>Verification and Design Methods for the BrainScaleS Neuromorphic
  Hardware System</title>
    <summary>  This paper presents verification and implementation methods that have been
developed for the design of the BrainScaleS-2 65nm ASICs. The 2nd generation
BrainScaleS chips are mixed-signal devices with tight coupling between
full-custom analog neuromorphic circuits and two general purpose
microprocessors (PPU) with SIMD extension for on-chip learning and plasticity.
Simulation methods for automated analysis and pre-tapeout calibration of the
highly parameterizable analog neuron and synapse circuits and for
hardware-software co-development of the digital logic and software stack are
presented. Accelerated operation of neuromorphic circuits and highly-parallel
digital data buses between the full-custom neuromorphic part and the PPU
require custom methodologies to close the digital signal timing at the
interfaces. Novel extensions to the standard digital physical implementation
design flow are highlighted. We present early results from the first full-size
BrainScaleS-2 ASIC containing 512 neurons and 130K synapses, demonstrating the
successful application of these methods. An application example illustrates the
full functionality of the BrainScaleS-2 hybrid plasticity architecture.
</summary>
    <author>
      <name>Andreas Grübl</name>
    </author>
    <author>
      <name>Sebastian Billaudelle</name>
    </author>
    <author>
      <name>Benjamin Cramer</name>
    </author>
    <author>
      <name>Vitali Karasenko</name>
    </author>
    <author>
      <name>Johannes Schemmel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.11455v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.11455v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.13054v1</id>
    <updated>2020-03-29T15:25:20Z</updated>
    <published>2020-03-29T15:25:20Z</published>
    <title>Analytical Model of Memory-Bound Applications Compiled with High Level
  Synthesis</title>
    <summary>  The increasing demand of dedicated accelerators to improve energy efficiency
and performance has highlighted FPGAs as a promising option to deliver both.
However, programming FPGAs in hardware description languages requires long time
and effort to achieve optimal results, which discourages many programmers from
adopting this technology.
  High Level Synthesis tools improve the accessibility to FPGAs, but the
optimization process is still time expensive due to the large compilation time,
between minutes and days, required to generate a single bitstream. Whereas
placing and routing take most of this time, the RTL pipeline and memory
organization are known in seconds. This early information about the
organization of the upcoming bitstream is enough to provide an accurate and
fast performance model.
  This paper presents a performance analytical model for HLS designs focused on
memory bound applications. With a careful analysis of the generated memory
architecture and DRAM organization, the model predicts the execution time with
a maximum error of 9.2% for a set of representative applications. Compared with
previous works, our predictions reduce on average at least $2\times$ the
estimation error.
</summary>
    <author>
      <name>Maria A. Dávila-Guzmán</name>
    </author>
    <author>
      <name>Rubén Gran Tejero</name>
    </author>
    <author>
      <name>María Villarroya-Gaudó</name>
    </author>
    <author>
      <name>Darío Suárez Gracia</name>
    </author>
    <link href="http://arxiv.org/abs/2003.13054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.13054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.00243v1</id>
    <updated>2020-04-01T05:57:03Z</updated>
    <published>2020-04-01T05:57:03Z</published>
    <title>Efficient Implementation of Multi-Channel Convolution in Monolithic 3D
  ReRAM Crossbar</title>
    <summary>  Convolutional neural networks (CNNs) demonstrate promising accuracy in a wide
range of applications. Among all layers in CNNs, convolution layers are the
most computation-intensive and consume the most energy. As the maturity of
device and fabrication technology, 3D resistive random access memory (ReRAM)
receives substantial attention for accelerating large vector-matrix
multiplication and convolution due to its high parallelism and energy
efficiency benefits. However, implementing multi-channel convolution naively in
3D ReRAM will either produce incorrect results or exploit only partial
parallelism of 3D ReRAM. In this paper, we propose a 3D ReRAM-based convolution
accelerator architecture, which efficiently maps multi-channel convolution to
monolithic 3D ReRAM. Our design has two key principles. First, we exploit the
intertwined structure of 3D ReRAM to implement multi-channel convolution by
using a state-of-the-art convolution algorithm. Second, we propose a new
approach to efficiently implement negative weights by separating them from
non-negative weights using configurable interconnects. Our evaluation
demonstrates that our mapping scheme in 16-layer 3D ReRAM achieves a speedup of
5.79X, 927.81X, and 36.8X compared with a custom 2D ReRAM baseline and
state-of-the-art CPU and GPU. Our design also reduces energy consumption by
2.12X, 1802.64X, and 114.1X compared with the same baseline.
</summary>
    <author>
      <name>Sho Ko</name>
    </author>
    <author>
      <name>Yun Joon Soh</name>
    </author>
    <author>
      <name>Jishen Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/2004.00243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.00243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.01353v1</id>
    <updated>2020-04-03T03:17:14Z</updated>
    <published>2020-04-03T03:17:14Z</published>
    <title>Hardware Trojan with Frequency Modulation</title>
    <summary>  The use of third-party IP cores in implementing applications in FPGAs has
given rise to the threat of malicious alterations through the insertion of
hardware Trojans. To address this threat, it is important to predict the way
hardware Trojans are built and to identify their weaknesses. This paper
describes a logic family for implementing robust hardware Trojans, which can
evade the two major detection methods, namely unused-circuit identification and
side-channel analysis. This robustness is achieved by encoding information in
frequency rather than amplitude so that the Trojan trigger circuitry's state
will never stay constant during 'normal' operation. In addition, the power
consumption of Trojan circuits built using the proposed logic family can be
concealed with minimal design effort and supplementary hardware resources.
Defense measures against hardware Trojans with frequency modulation are
described.
</summary>
    <author>
      <name>Ash Luft</name>
    </author>
    <author>
      <name>Mihai Sima</name>
    </author>
    <author>
      <name>Michael McGuire</name>
    </author>
    <link href="http://arxiv.org/abs/2004.01353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.01353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.03804v1</id>
    <updated>2020-04-08T04:28:38Z</updated>
    <published>2020-04-08T04:28:38Z</published>
    <title>HybridDNN: A Framework for High-Performance Hybrid DNN Accelerator
  Design and Implementation</title>
    <summary>  To speedup Deep Neural Networks (DNN) accelerator design and enable effective
implementation, we propose HybridDNN, a framework for building high-performance
hybrid DNN accelerators and delivering FPGA-based hardware implementations.
Novel techniques include a highly flexible and scalable architecture with a
hybrid Spatial/Winograd convolution (CONV) Processing Engine (PE), a
comprehensive design space exploration tool, and a complete design flow to
fully support accelerator design and implementation. Experimental results show
that the accelerators generated by HybridDNN can deliver 3375.7 and 83.3 GOPS
on a high-end FPGA (VU9P) and an embedded FPGA (PYNQ-Z1), respectively, which
achieve a 1.8x higher performance improvement compared to the state-of-art
accelerator designs. This demonstrates that HybridDNN is flexible and scalable
and can target both cloud and embedded hardware platforms with vastly different
resource constraints.
</summary>
    <author>
      <name>Hanchen Ye</name>
    </author>
    <author>
      <name>Xiaofan Zhang</name>
    </author>
    <author>
      <name>Zhize Huang</name>
    </author>
    <author>
      <name>Gengsheng Chen</name>
    </author>
    <author>
      <name>Deming Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at Design Automation Conference 2020
  (DAC'20)</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.03804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.03804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.04509v2</id>
    <updated>2020-09-15T15:25:04Z</updated>
    <published>2020-04-09T12:12:05Z</published>
    <title>A Survey on Coarse-Grained Reconfigurable Architectures from a
  Performance Perspective</title>
    <summary>  With the end of both Dennard's scaling and Moore's law, computer users and
researchers are aggressively exploring alternative forms of computing in order
to continue the performance scaling that we have come to enjoy. Among the more
salient and practical of the post-Moore alternatives are reconfigurable
systems, with Coarse-Grained Reconfigurable Architectures (CGRAs) seemingly
capable of striking a balance between performance and programmability. In this
paper, we survey the landscape of CGRAs. We summarize nearly three decades of
literature on the subject, with a particular focus on the premise behind the
different CGRAs and how they have evolved. Next, we compile metrics of
available CGRAs and analyze their performance properties in order to understand
and discover knowledge gaps and opportunities for future CGRA research
specialized towards High-Performance Computing (HPC). We find that there are
ample opportunities for future research on CGRAs, in particular with respect to
size, functionality, support for parallel programming models, and to evaluate
more complex applications.
</summary>
    <author>
      <name>Artur Podobas</name>
    </author>
    <author>
      <name>Kentaro Sano</name>
    </author>
    <author>
      <name>Satoshi Matsuoka</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2020.3012084</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2020.3012084" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Access, 2020 (https://ieeexplore.ieee.org/document/9149601)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2004.04509v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.04509v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1; B.0; C.1; C.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.09858v1</id>
    <updated>2020-04-21T09:34:49Z</updated>
    <published>2020-04-21T09:34:49Z</published>
    <title>Towards a Hardware DSL Ecosystem : RubyRTL and Friends</title>
    <summary>  For several years, hardware design has been undergoing a surprising revival:
fueled by open source initiatives, various tools and architectures have
recently emerged. This resurgence also involves new hardware description
languages. Inspired by the Migen Python community, we present RubyRTL, a novel
internal domain-specific language for hardware design embedded in the Ruby
language. Ruby -- which is best known in the field of web design -- has proven
to be an excellent solution for the design of such DSLs, because of its
meta-programming features. This paper presents the main aspects of RubyRTL,
along with illustrating examples. We also propose a language-neutral
interchange format, named Sexpir, that allows to seamlessly exchange RTL
designs between Migen Python DSL and RubyRTL. This paves the way for
interactions between various agile communities in the field of open source
hardware design.
</summary>
    <author>
      <name>Jean-Christophe Le Lann</name>
    </author>
    <author>
      <name>Hannah Badier</name>
    </author>
    <author>
      <name>Florent Kermarrec</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Open Source Design Automation Workshop, in conjunction with DATE'2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.09858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.09858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.10341v1</id>
    <updated>2020-04-21T23:26:23Z</updated>
    <published>2020-04-21T23:26:23Z</published>
    <title>DRMap: A Generic DRAM Data Mapping Policy for Energy-Efficient
  Processing of Convolutional Neural Networks</title>
    <summary>  Many convolutional neural network (CNN) accelerators face performance- and
energy-efficiency challenges which are crucial for embedded implementations,
due to high DRAM access latency and energy. Recently, some DRAM architectures
have been proposed to exploit subarray-level parallelism for decreasing the
access latency. Towards this, we present a design space exploration methodology
to study the latency and energy of different mapping policies on different DRAM
architectures, and identify the pareto-optimal design choices. The results show
that the energy-efficient DRAM accesses can be achieved by a mapping policy
that orderly prioritizes to maximize the row buffer hits, bank- and
subarray-level parallelism.
</summary>
    <author>
      <name>Rachmad Vidya Wicaksana Putra</name>
    </author>
    <author>
      <name>Muhammad Abdullah Hanif</name>
    </author>
    <author>
      <name>Muhammad Shafique</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DAC18072.2020.9218672</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DAC18072.2020.9218672" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at the 57th Design Automation Conference (DAC), July 2020,
  San Francisco, CA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.10341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.10341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.10470v2</id>
    <updated>2020-11-05T14:35:31Z</updated>
    <published>2020-04-22T09:57:15Z</published>
    <title>Proactive Aging Mitigation in CGRAs through Utilization-Aware Allocation</title>
    <summary>  Resource balancing has been effectively used to mitigate the long-term aging
effects of Negative Bias Temperature Instability (NBTI) in multi-core and
Graphics Processing Unit (GPU) architectures. In this work, we investigate this
strategy in Coarse-Grained Reconfigurable Arrays (CGRAs) with a novel
application-to-CGRA allocation approach. By introducing important extensions to
the reconfiguration logic and the datapath, we enable the dynamic movement of
configurations throughout the fabric and allow overutilized Functional Units
(FUs) to recover from stress-induced NBTI aging. Implementing the approach in a
resource-constrained state-of-the-art CGRA reveals $2.2\times$ lifetime
improvement with negligible performance overheads and less than $10\%$ increase
in area.
</summary>
    <author>
      <name>Marcelo Brandalero</name>
    </author>
    <author>
      <name>Bernardo Neuhaus Lignati</name>
    </author>
    <author>
      <name>Antonio Carlos Schneider Beck</name>
    </author>
    <author>
      <name>Muhammad Shafique</name>
    </author>
    <author>
      <name>Michael Hübner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DAC18072.2020.9218586</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DAC18072.2020.9218586" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Please cite this as: M. Brandalero, B. N. Lignati, A. Carlos
  Schneider Beck, M. Shafique and M. H\"ubner, "Proactive Aging Mitigation in
  CGRAs through Utilization-Aware Allocation," 2020 57th ACM/IEEE Design
  Automation Conference (DAC), San Francisco, CA, USA, 2020, pp. 1-6, doi:
  10.1109/DAC18072.2020.9218586</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.10470v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.10470v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1; B.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.10483v1</id>
    <updated>2020-04-22T10:42:27Z</updated>
    <published>2020-04-22T10:42:27Z</published>
    <title>Using Libraries of Approximate Circuits in Design of Hardware
  Accelerators of Deep Neural Networks</title>
    <summary>  Approximate circuits have been developed to provide good tradeoffs between
power consumption and quality of service in error resilient applications such
as hardware accelerators of deep neural networks (DNN). In order to accelerate
the approximate circuit design process and to support a fair benchmarking of
circuit approximation methods, libraries of approximate circuits have been
introduced. For example, EvoApprox8b contains hundreds of 8-bit approximate
adders and multipliers. By means of genetic programming we generated an
extended version of the library in which thousands of 8- to 128-bit approximate
arithmetic circuits are included. These circuits form Pareto fronts with
respect to several error metrics, power consumption and other circuit
parameters. In our case study we show how a large set of approximate
multipliers can be used to perform a resilience analysis of a hardware
accelerator of ResNet DNN and to select the most suitable approximate
multiplier for a given application. Results are reported for various instances
of the ResNet DNN trained on CIFAR-10 benchmark problem.
</summary>
    <author>
      <name>Vojtech Mrazek</name>
    </author>
    <author>
      <name>Lukas Sekanina</name>
    </author>
    <author>
      <name>Zdenek Vasicek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/AICAS48895.2020.9073837</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/AICAS48895.2020.9073837" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at the 2nd IEEE International Conference on Artificial
  Intelligence Circuits and Systems (AICAS 2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.10483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.10483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.10502v1</id>
    <updated>2020-04-22T11:27:47Z</updated>
    <published>2020-04-22T11:27:47Z</published>
    <title>ApproxFPGAs: Embracing ASIC-Based Approximate Arithmetic Components for
  FPGA-Based Systems</title>
    <summary>  There has been abundant research on the development of Approximate Circuits
(ACs) for ASICs. However, previous studies have illustrated that ASIC-based ACs
offer asymmetrical gains in FPGA-based accelerators. Therefore, an AC that
might be pareto-optimal for ASICs might not be pareto-optimal for FPGAs. In
this work, we present the ApproxFPGAs methodology that uses machine learning
models to reduce the exploration time for analyzing the state-of-the-art
ASIC-based ACs to determine the set of pareto-optimal FPGA-based ACs. We also
perform a case-study to illustrate the benefits obtained by deploying these
pareto-optimal FPGA-based ACs in a state-of-the-art automation framework to
systematically generate pareto-optimal approximate accelerators that can be
deployed in FPGA-based systems to achieve high performance or low-power
consumption.
</summary>
    <author>
      <name>Bharath Srinivas Prabakaran</name>
    </author>
    <author>
      <name>Vojtech Mrazek</name>
    </author>
    <author>
      <name>Zdenek Vasicek</name>
    </author>
    <author>
      <name>Lukas Sekanina</name>
    </author>
    <author>
      <name>Muhammad Shafique</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DAC18072.2020.9218533</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DAC18072.2020.9218533" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for Publication at the 57th Design Automation Conference
  (DAC), July 2020, San Francisco, CA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.10502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.10502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.11080v1</id>
    <updated>2020-04-23T11:19:37Z</updated>
    <published>2020-04-23T11:19:37Z</published>
    <title>Using DSP Slices as Content-Addressable Update Queues</title>
    <summary>  Content-Addressable Memory (CAM) is a powerful abstraction for building
memory caches, routing tables and hazard detection logic. Without a native CAM
structure available on FPGA devices, their functionality must be emulated using
the structural primitives at hand. Such an emulation causes significant
overhead in the consumption of the underlying resources, typically
general-purpose fabric and on-chip block RAM (BRAM). This often motivates
mitigating trade-offs, such as the reduction of the associativity of memory
caches. This paper describes a technique to implement the hazard resolution in
a memory update queue that hides the off-chip memory readout latency of
read-modify-write cycles while guaranteeing the delivery of the full memory
bandwidth. The innovative use of DSP slices allows them to assume and combine
the functions of (a) the tag and data storage, (b) the tag matching, and (c)
the data update in this key-value storage scenario. The proposed approach
provides designers with extra flexibility by adding this resource type as
another option to implement CAM.
</summary>
    <author>
      <name>Thomas B. Preußer</name>
    </author>
    <author>
      <name>Monica Chiosa</name>
    </author>
    <author>
      <name>Alexander Weiss</name>
    </author>
    <author>
      <name>Gustavo Alonso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to FPL 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.11080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.11080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.13074v2</id>
    <updated>2020-11-23T20:12:39Z</updated>
    <published>2020-04-27T18:11:12Z</published>
    <title>The Case for Learning Application Behavior to Improve Hardware Energy
  Efficiency</title>
    <summary>  Computer applications are continuously evolving. However, significant
knowledge can be harvested from a set of applications and applied in the
context of unknown applications. In this paper, we propose to use the harvested
knowledge to tune hardware configurations. The goal of such tuning is to
maximize hardware efficiency (i.e., maximize an applications performance while
minimizing the energy consumption). Our proposed approach, called FORECASTER,
uses a deep learning model to learn what configuration of hardware resources
provides the optimal energy efficiency for a certain behavior of an
application. During the execution of an unseen application, the model uses the
learned knowledge to reconfigure hardware resources in order to maximize energy
efficiency. We have provided a detailed design and implementation of FORECASTER
and compared its performance against a prior state-of-the-art hardware
reconfiguration approach. Our results show that FORECASTER can save as much as
18.4% system power over the baseline set up with all resources. On average,
FORECASTER saves 16% system power over the baseline setup while sacrificing
less than 0.01% of overall performance. Compared to the prior scheme,
FORECASTER increases power savings by 7%.
</summary>
    <author>
      <name>Kevin Weston</name>
    </author>
    <author>
      <name>Vahid Jafanza</name>
    </author>
    <author>
      <name>Arnav Kansal</name>
    </author>
    <author>
      <name>Abhishek Taur</name>
    </author>
    <author>
      <name>Mohamed Zahran</name>
    </author>
    <author>
      <name>Abdullah Muzahid</name>
    </author>
    <link href="http://arxiv.org/abs/2004.13074v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.13074v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.02310v3</id>
    <updated>2020-10-27T18:40:15Z</updated>
    <published>2020-05-05T16:15:52Z</published>
    <title>Testing Compilers for Programmable Switches Through Switch Hardware
  Simulation</title>
    <summary>  Programmable switches have emerged as powerful and flexible alternatives to
fixed-function forwarding devices. But because of the unique hardware
constraints of network switches, the design and implementation of compilers
targeting these devices is tedious and error prone. Despite the important role
that compilers play in software development, there is a dearth of tools for
testing compilers for programmable network devices. We present Druzhba, a
programmable switch simulator used for testing compilers targeting programmable
packet-processing substrates. We show that we can model the low-level behavior
of a switch's programmable hardware. We further show how our machine model can
be used by compiler developers to target Druzhba as a compiler backend.
Generated machine code programs are fed into Druzhba and tested using a
fuzzing-based approach that allows compiler developers to test the correctness
of their compilers. Using a program-synthesis-based compiler as a case study,
we demonstrate how Druzhba has been successful in testing compiler-generated
machine code for our simulated switch pipeline instruction set.
</summary>
    <author>
      <name>Michael D. Wong</name>
    </author>
    <author>
      <name>Aatish Kishan Varma</name>
    </author>
    <author>
      <name>Anirudh Sivaraman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.02310v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.02310v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.4.4; C.2.0; D.2.5; D.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.02506v1</id>
    <updated>2020-05-05T21:28:48Z</updated>
    <published>2020-05-05T21:28:48Z</published>
    <title>LiteX: an open-source SoC builder and library based on Migen Python DSL</title>
    <summary>  LiteX is a GitHub-hosted SoC builder / IP library and utilities that can be
used to create SoCs and full FPGA designs. Besides being open-source and BSD
licensed, its originality lies in the fact that its IP components are entirely
described using Migen Python internal DSL, which simplifies its design in
depth. LiteX already supports various softcores CPUs and essential peripherals,
with no dependencies on proprietary IP blocks or generators. This paper
provides an overview of LiteX: two real SoC designs on FPGA are presented. They
both leverage the LiteX approach in terms of design entry, libraries and
integration capabilities. The first one is based on RISC-V core, while the
second is based on a LM32 core. In the second use case, we further demonstrate
the use of a fully open-source toolchain coupled with LiteX.
</summary>
    <author>
      <name>Florent Kermarrec</name>
    </author>
    <author>
      <name>Sébastien Bourdeauducq</name>
    </author>
    <author>
      <name>Jean-Christophe Le Lann</name>
    </author>
    <author>
      <name>Hannah Badier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, OSDA'2019 Open Source Hardware Design, colocated with
  DATE'19, Florence, Italy</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.02506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.02506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.02550v1</id>
    <updated>2020-05-06T01:29:15Z</updated>
    <published>2020-05-06T01:29:15Z</published>
    <title>A Post-Silicon Trace Analysis Approach for System-on-Chip Protocol Debug</title>
    <summary>  Reconstructing system-level behavior from silicon traces is a critical
problem in post-silicon validation of System-on-Chip designs. Current
industrial practice in this area is primarily manual, depending on
collaborative insights of the architects, designers, and validators. This paper
presents a trace analysis approach that exploits architectural models of the
system-level protocols to reconstruct design behavior from partially observed
silicon traces in the presence of ambiguous and noisy data. The output of the
approach is a set of all potential interpretations of a system's internal
executions abstracted to the system-level protocols. To support the trace
analysis approach, a companion trace signal selection framework guided by
system-level protocols is also presented, and its impacts on the complexity and
accuracy of the analysis approach are discussed. That approach and the
framework have been evaluated on a multi-core system-on-chip prototype that
implements a set of common industrial system-level protocols.
</summary>
    <author>
      <name>Yuting Cao</name>
    </author>
    <author>
      <name>Hao Zheng</name>
    </author>
    <author>
      <name>Sandip Ray</name>
    </author>
    <author>
      <name>Jin Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2005.02550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.02550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.5.3; B.7.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.08478v1</id>
    <updated>2020-05-18T06:28:45Z</updated>
    <published>2020-05-18T06:28:45Z</published>
    <title>Energy-Efficient On-Chip Networks through Profiled Hybrid Switching</title>
    <summary>  Virtual channel flow control is the de facto choice for modern
networks-on-chip to allow better utilization of the link bandwidth through
buffering and packet switching, which are also the sources of large power
footprint and long per-hop latency. On the other hand, bandwidth can be
plentiful for parallel workloads under virtual channel flow control. Thus,
dated but simpler flow controls such as circuit switching can be utilized to
improve the energy efficiency of modern networks-on-chip. In this paper, we
propose to utilize part of the link bandwidth under circuit switching so that
part of the traffic can be transmitted bufferlessly without routing. Our
evaluations reveal that this proposal leads to a reduction of energy per flit
by up to 32% while also provides very competitive latency per flit when
compared to networks under virtual channel flow control.
</summary>
    <author>
      <name>Yuan He</name>
    </author>
    <author>
      <name>Jinyu Jiao</name>
    </author>
    <author>
      <name>Thang Cao</name>
    </author>
    <author>
      <name>Masaaki Kondo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the 30th ACM Great Lakes Symposium on VLSI (GLSVLSI'20),
  Beijing, China</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.08478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.08478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.10866v1</id>
    <updated>2020-05-21T19:32:33Z</updated>
    <published>2020-05-21T19:32:33Z</published>
    <title>Stack up your chips: Betting on 3D integration to augment Moore's Law
  scaling</title>
    <summary>  3D integration, i.e., stacking of integrated circuit layers using parallel or
sequential processing is gaining rapid industry adoption with the slowdown of
Moore's law scaling. 3D stacking promises potential gains in performance, power
and cost but the actual magnitude of gains varies depending on end-application,
technology choices and design. In this talk, we will discuss some key
challenges associated with 3D design and how design-for-3D will require us to
break traditional silos of micro-architecture, circuit/physical design and
manufacturing technology to work across abstractions to enable the gains
promised by 3D technologies.
</summary>
    <author>
      <name>Saurabh Sinha</name>
    </author>
    <author>
      <name>Xiaoqing Xu</name>
    </author>
    <author>
      <name>Mudit Bhargava</name>
    </author>
    <author>
      <name>Shidhartha Das</name>
    </author>
    <author>
      <name>Brian Cline</name>
    </author>
    <author>
      <name>Greg Yeric</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 Figures, invited talk at S3S conference held in San Jose,
  October 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.10866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.10866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.11357v1</id>
    <updated>2020-05-22T19:24:52Z</updated>
    <published>2020-05-22T19:24:52Z</published>
    <title>Accelerate Cycle-Level Full-System Simulation of Multi-Core RISC-V
  Systems with Binary Translation</title>
    <summary>  It has always been difficult to balance the accuracy and performance of ISSs.
RTL simulators or systems such as gem5 are used to execute programs in a
cycle-accurate manner but are often prohibitively slow. In contrast, functional
simulators such as QEMU can run large benchmarks to completion in a reasonable
time yet capture few performance metrics and fail to model complex interactions
between multiple cores.
  This paper presents a novel multi-purpose simulator that exploits binary
translation to offer fast cycle-level full-system simulations. Its functional
simulation mode outperforms QEMU and, if desired, it is possible to switch
between functional and timing modes at run-time. Cycle-level simulations of
RISC-V multi-core processors are possible at more than 20 MIPS, a useful middle
ground in terms of accuracy and performance with simulation speeds nearly 100
times those of more detailed cycle-accurate models.
</summary>
    <author>
      <name>Xuan Guo</name>
    </author>
    <author>
      <name>Robert Mullins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in the Fourth Workshop on Computer Architecture
  Research with RISC-V</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.11357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.11357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.14691v1</id>
    <updated>2020-05-29T17:30:00Z</updated>
    <published>2020-05-29T17:30:00Z</published>
    <title>Dynamic Merge Point Prediction</title>
    <summary>  Despite decades of research, conditional branch mispredictions still pose a
significant problem for performance. Moreover, limit studies on infinite size
predictors show that many of the remaining branches are impossible to predict
by current strategies. Our work focuses on mitigating performance loss in the
face of impossible to predict branches. This paper presents a dynamic merge
point predictor, which uses instructions fetched on the wrong path of the
branch to dynamically detect the merge point. Our predictor locates the merge
point with an accuracy of 95%, even when faced with branches whose direction is
impossible to predict. Furthermore, we introduce a novel confidence-cost
system, which identifies costly hard-to-predict branches. Our complete system
replaces 58% of all branch mispredictions with a correct merge point
prediction, effectively reducing MPKI by 43%. This result demonstrates the
potential for dynamic merge point prediction to significantly improve
performance.
</summary>
    <author>
      <name>Stephen Pruett</name>
    </author>
    <author>
      <name>Yale Patt</name>
    </author>
    <link href="http://arxiv.org/abs/2005.14691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.14691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.01049v1</id>
    <updated>2020-06-01T16:21:19Z</updated>
    <published>2020-06-01T16:21:19Z</published>
    <title>Exceeding Conservative Limits: A Consolidated Analysis on Modern
  Hardware Margins</title>
    <summary>  Modern large-scale computing systems (data centers, supercomputers, cloud and
edge setups and high-end cyber-physical systems) employ heterogeneous
architectures that consist of multicore CPUs, general-purpose many-core GPUs,
and programmable FPGAs. The effective utilization of these architectures poses
several challenges, among which a primary one is power consumption. Voltage
reduction is one of the most efficient methods to reduce power consumption of a
chip. With the galloping adoption of hardware accelerators (i.e., GPUs and
FPGAs) in large datacenters and other large-scale computing infrastructures, a
comprehensive evaluation of the safe voltage reduction levels for each
different chip can be employed for efficient reduction of the total power. We
present a survey of recent studies in voltage margins reduction at the system
level for modern CPUs, GPUs and FPGAs. The pessimistic voltage guardbands
inserted by the silicon vendors can be exploited in all devices for significant
power savings. On average, voltage reduction can reach 12% in multicore CPUs,
20% in manycore GPUs and 39% in FPGAs.
</summary>
    <author>
      <name>George Papadimitriou</name>
    </author>
    <author>
      <name>Athanasios Chatzidimitriou</name>
    </author>
    <author>
      <name>Dimitris Gizopoulos</name>
    </author>
    <author>
      <name>Vijay Janapa Reddi</name>
    </author>
    <author>
      <name>Jingwen Leng</name>
    </author>
    <author>
      <name>Behzad Salami</name>
    </author>
    <author>
      <name>Osman S. Unsal</name>
    </author>
    <author>
      <name>Adrian Cristal Kestelman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TDMR.2020.2989813</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TDMR.2020.2989813" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in IEEE Transactions on Device and Materials
  Reliability</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.01049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.01049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.05696v1</id>
    <updated>2020-06-10T07:27:14Z</updated>
    <published>2020-06-10T07:27:14Z</published>
    <title>Unified Characterization Platform for Emerging NVM Technology: Neural
  Network Application Benchmarking Using off-the-shelf NVM Chips</title>
    <summary>  In this paper, we present a unified FPGA based electrical test-bench for
characterizing different emerging NonVolatile Memory (NVM) chips. In
particular, we present detailed electrical characterization and benchmarking of
multiple commercially available, off-the-shelf, NVM chips viz.: MRAM, FeRAM,
CBRAM, and ReRAM. We investigate important NVM parameters such as: (i) current
consumption patterns, (ii) endurance, and (iii) error characterization. The
proposed FPGA based testbench is then utilized for a Proof-of-Concept (PoC)
Neural Network (NN) image classification application. Four emerging NVM chips
are benchmarked against standard SRAM and Flash technology for the AI
application as active weight memory during inference mode.
</summary>
    <author>
      <name>Supriya Chakraborty</name>
    </author>
    <author>
      <name>Abhishek Gupta</name>
    </author>
    <author>
      <name>Manan Suri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at 2020 IEEE International Symposium on Circuits and Systems
  (ISCAS)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2020 IEEE International Symposium on Circuits and Systems (ISCAS)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2006.05696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.05696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.08026v1</id>
    <updated>2020-06-14T21:27:15Z</updated>
    <published>2020-06-14T21:27:15Z</published>
    <title>Architecture Support for FPGA Multi-tenancy in the Cloud</title>
    <summary>  Cloud deployments now increasingly provision FPGA accelerators as part of
virtual instances. While FPGAs are still essentially single-tenant, the growing
demand for hardware acceleration will inevitably lead to the need for methods
and architectures supporting FPGA multi-tenancy. In this paper, we propose an
architecture supporting space-sharing of FPGA devices among multiple tenants in
the cloud. The proposed architecture implements a network-on-chip (NoC)
designed for fast data movement and low hardware footprint. Prototyping the
proposed architecture on a Xilinx Virtex Ultrascale+ demonstrated near
specification maximum frequency for on-chip data movement and high throughput
in virtual instance access to hardware accelerators. We demonstrate similar
performance compared to single-tenant deployment while increasing FPGA
utilization ( we achieved 6x higher FPGA utilization with our case study),
which is one of the major goals of virtualization. Overall, our NoC
interconnect achieved about 2x higher maximum frequency than the
state-of-the-art and a bandwidth of 25.6 Gbps.
</summary>
    <author>
      <name>Joel Mandebi Mbongue</name>
    </author>
    <author>
      <name>Alex Shuping</name>
    </author>
    <author>
      <name>Pankaj Bhowmik</name>
    </author>
    <author>
      <name>Christophe Bobda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was accepted as a full-paper (8pages) at the 31st IEEE
  International Conference on Application-specific Systems, Architectures and
  Processors (ASAP 2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.08026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.08026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.08975v1</id>
    <updated>2020-06-16T08:07:21Z</updated>
    <published>2020-06-16T08:07:21Z</published>
    <title>ZnG: Architecting GPU Multi-Processors with New Flash for Scalable Data
  Analysis</title>
    <summary>  We propose ZnG, a new GPU-SSD integrated architecture, which can maximize the
memory capacity in a GPU and address performance penalties imposed by an SSD.
Specifically, ZnG replaces all GPU internal DRAMs with an ultra-low-latency SSD
to maximize the GPU memory capacity. ZnG further removes performance bottleneck
of the SSD by replacing its flash channels with a high-throughput flash network
and integrating SSD firmware in the GPU's MMU to reap the benefits of hardware
accelerations. Although flash arrays within the SSD can deliver high
accumulated bandwidth, only a small fraction of such bandwidth can be utilized
by GPU's memory requests due to mismatches of their access granularity. To
address this, ZnG employs a large L2 cache and flash registers to buffer the
memory requests. Our evaluation results indicate that ZnG can achieve 7.5x
higher performance than prior work.
</summary>
    <author>
      <name>Jie Zhang</name>
    </author>
    <author>
      <name>Myoungsoo Jung</name>
    </author>
    <link href="http://arxiv.org/abs/2006.08975v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.08975v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.13547v1</id>
    <updated>2020-06-24T08:14:11Z</updated>
    <published>2020-06-24T08:14:11Z</published>
    <title>Fetch-Directed Instruction Prefetching Revisited</title>
    <summary>  Prior work has observed that fetch-directed prefetching (FDIP) is highly
effective at covering instruction cache misses. The key to FDIP's effectiveness
is having a sufficiently large BTB to accommodate the application's branch
working set. In this work, we introduce several optimizations that
significantly extend the reach of the BTB within the available storage budget.
Our optimizations target nearly every source of storage overhead in each BTB
entry; namely, the tag, target address, and size fields.
  We observe that while most dynamic branch instances have short offsets, a
large number of branches has longer offsets or requires the use of full target
addresses. Based on this insight, we break-up the BTB into multiple smaller
BTBs, each storing offsets of different length. This enables a dramatic
reduction in storage for target addresses. We further compress tags to 16 bits
and avoid the use of the basic-block-oriented BTB advocated in prior FDIP
variants. The latter optimization eliminates the need to store the basic block
size in each BTB entry. Our final design, called FDIP-X, uses an ensemble of 4
BTBs and always outperforms conventional FDIP with a unified
basic-block-oriented BTB for equal storage budgets.
</summary>
    <author>
      <name>Truls Asheim</name>
    </author>
    <author>
      <name>Rakesh Kumar</name>
    </author>
    <author>
      <name>Boris Grot</name>
    </author>
    <link href="http://arxiv.org/abs/2006.13547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.13547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.14317v2</id>
    <updated>2020-11-26T05:31:39Z</updated>
    <published>2020-06-25T11:37:30Z</published>
    <title>A Fast Finite Field Multiplier for SIKE</title>
    <summary>  Various post-quantum cryptography algorithms have been recently proposed.
Supersingluar isogeny Diffie-Hellman key exchange (SIKE) is one of the most
promising candidates due to its small key size. However, the SIKE scheme
requires numerous finite field multiplications for its isogeny computation, and
hence suffers from slow encryption and decryption process. In this paper, we
propose a fast finite field multiplier design that performs multiplications in
GF(p) with high throughput and low latency. The design accelerates the
computation by adopting deep pipelining, and achieves high hardware utilization
through data interleaving. The proposed finite field multiplier demonstrates
4.48 times higher throughput than prior work based on the identical fast
multiplication algorithm and 1.43 times higher throughput than the
state-of-the-art fast finite field multiplier design aimed at SIKE.
</summary>
    <author>
      <name>Yeonsoo Jeon</name>
    </author>
    <author>
      <name>Dongsuk Jeon</name>
    </author>
    <link href="http://arxiv.org/abs/2006.14317v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.14317v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.16535v4</id>
    <updated>2022-07-25T02:22:01Z</updated>
    <published>2020-06-30T05:16:51Z</published>
    <title>RCP: A Low-overhead Reversible Coherence Protocol</title>
    <summary>  This paper proposes RCP, a new reversible coherence protocol that ensures
invisible speculative load execution (ISLE) with low overhead. RCP can be
combined with processor mechanisms that eliminate the effects of speculative
instructions on other instructions to achieve low overhead invisible
speculative execution (ISE). ISE provides protection that is at least as strong
as speculative privacy tracking (SPT) and stronger than speculative taint
tracking (STT). RCP is designed by systematically extending the existing
coherence protocol to incorporate speculative loads and states. The protocol is
implemented in Gem5 and verified with Murphi. The results show that RCP based
ISE incurs lower overhead than STT/SDO/SPT while providing similar/stronger
protection.
</summary>
    <author>
      <name>You Wu</name>
    </author>
    <author>
      <name>Xuehai Qian</name>
    </author>
    <link href="http://arxiv.org/abs/2006.16535v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.16535v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.00060v1</id>
    <updated>2020-06-30T18:49:33Z</updated>
    <published>2020-06-30T18:49:33Z</published>
    <title>TDO-CIM: Transparent Detection and Offloading for Computation In-memory</title>
    <summary>  Computation in-memory is a promising non-von Neumann approach aiming at
completely diminishing the data transfer to and from the memory subsystem.
Although a lot of architectures have been proposed, compiler support for such
architectures is still lagging behind. In this paper, we close this gap by
proposing an end-to-end compilation flow for in-memory computing based on the
LLVM compiler infrastructure. Starting from sequential code, our approach
automatically detects, optimizes, and offloads kernels suitable for in-memory
acceleration. We demonstrate our compiler tool-flow on the PolyBench/C
benchmark suite and evaluate the benefits of our proposed in-memory
architecture simulated in Gem5 by comparing it with a state-of-the-art von
Neumann architecture.
</summary>
    <author>
      <name>Kanishkan Vadivel</name>
    </author>
    <author>
      <name>Lorenzo Chelini</name>
    </author>
    <author>
      <name>Ali BanaGozar</name>
    </author>
    <author>
      <name>Gagandeep Singh</name>
    </author>
    <author>
      <name>Stefano Corda</name>
    </author>
    <author>
      <name>Roel Jordans</name>
    </author>
    <author>
      <name>Henk Corporaal</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23919/DATE48585.2020.9116464</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23919/DATE48585.2020.9116464" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of DATE2020 publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.00060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.00060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.01348v1</id>
    <updated>2020-07-02T19:21:05Z</updated>
    <published>2020-07-02T19:21:05Z</published>
    <title>Efficient Neural Network Deployment for Microcontroller</title>
    <summary>  Edge computing for neural networks is getting important especially for low
power applications and offline devices. TensorFlow Lite and PyTorch Mobile were
released for this purpose. But they mainly support mobile devices instead of
microcontroller level yet. Microcontroller support is an emerging area now.
There are many approaches to reduce network size and compute load like pruning,
binarization and layer manipulation i.e. operator reordering. This paper is
going to explore and generalize convolution neural network deployment for
microcontrollers with two novel optimization proposals offering memory saving
and compute efficiency in 2D convolutions as well as fully connected layers.
The first one is in-place max-pooling, if the stride is greater than or equal
to pooling kernel size. The second optimization is to use ping-pong buffers
between layers to reduce memory consumption significantly. The memory savings
and performance will be compared with CMSIS-NN framework developed for ARM
Cortex-M CPUs. The final purpose is to develop a tool consuming PyTorch model
with trained network weights, and it turns into an optimized inference
engine(forward pass) in C/C++ for low memory(kilobyte level) and limited
computing capable microcontrollers.
</summary>
    <author>
      <name>Hasan Unlu</name>
    </author>
    <link href="http://arxiv.org/abs/2007.01348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.01348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.01820v1</id>
    <updated>2020-07-02T07:55:08Z</updated>
    <published>2020-07-02T07:55:08Z</published>
    <title>A Machine Learning Pipeline Stage for Adaptive Frequency Adjustment</title>
    <summary>  A machine learning (ML) design framework is proposed for adaptively adjusting
clock frequency based on propagation delay of individual instructions. A random
forest model is trained to classify propagation delays in real time, utilizing
current operation type, current operands, and computation history as ML
features. The trained model is implemented in Verilog as an additional pipeline
stage within a baseline processor. The modified system is experimentally tested
at the gate level in 45 nm CMOS technology, exhibiting a speedup of 70% and
energy reduction of 30% with coarse-grained ML classification. A speedup of 89%
is demonstrated with finer granularities with 15.5% reduction in energy
consumption.
</summary>
    <author>
      <name>Arash Fouman Ajirlou</name>
    </author>
    <author>
      <name>Inna Partin-Vaisband</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures, 5 tables, IEEE transaction on computers. arXiv
  admin note: substantial text overlap with arXiv:2006.07450</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.01820v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.01820v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.07759v1</id>
    <updated>2020-07-15T15:34:53Z</updated>
    <published>2020-07-15T15:34:53Z</published>
    <title>Enabling Mixed-Precision Quantized Neural Networks in Extreme-Edge
  Devices</title>
    <summary>  The deployment of Quantized Neural Networks (QNN) on advanced
microcontrollers requires optimized software to exploit digital signal
processing (DSP) extensions of modern instruction set architectures (ISA). As
such, recent research proposed optimized libraries for QNNs (from 8-bit to
2-bit) such as CMSIS-NN and PULP-NN. This work presents an extension to the
PULP-NN library targeting the acceleration of mixed-precision Deep Neural
Networks, an emerging paradigm able to significantly shrink the memory
footprint of deep neural networks with negligible accuracy loss. The library,
composed of 27 kernels, one for each permutation of input feature maps,
weights, and output feature maps precision (considering 8-bit, 4-bit and
2-bit), enables efficient inference of QNN on parallel ultra-low-power (PULP)
clusters of RISC-V based processors, featuring the RV32IMCXpulpV2 ISA. The
proposed solution, benchmarked on an 8-cores GAP-8 PULP cluster, reaches peak
performance of 16 MACs/cycle on 8 cores, performing 21x to 25x faster than an
STM32H7 (powered by an ARM Cortex M7 processor) with 15x to 21x better energy
efficiency.
</summary>
    <author>
      <name>Nazareno Bruschi</name>
    </author>
    <author>
      <name>Angelo Garofalo</name>
    </author>
    <author>
      <name>Francesco Conti</name>
    </author>
    <author>
      <name>Giuseppe Tagliavini</name>
    </author>
    <author>
      <name>Davide Rossi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3387902.3394038</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3387902.3394038" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures, published in 17th ACM International Conference on
  Computing Frontiers (CF '20), May 11--13, 2020, Catania, Italy</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.07759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.07759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.08622v2</id>
    <updated>2020-09-11T08:23:10Z</updated>
    <published>2020-07-16T20:43:06Z</published>
    <title>Dagger: Towards Efficient RPCs in Cloud Microservices with Near-Memory
  Reconfigurable NICs</title>
    <summary>  Cloud applications are increasingly relying on hundreds of loosely-coupled
microservices to complete user requests that meet an applications end-to-end
QoS requirements. Communication time between services accounts for a large
fraction of the end-to-end latency and can introduce performance
unpredictability and QoS violations. This work presents our early work on
Dagger, a hardware acceleration platform for networking, designed specifically
with the unique qualities of microservices in mind. The Dagger architecture
relies on an FPGA-based NIC, closely coupled with the processor over a
configurable memory interconnect, designed to offload and accelerate RPC
stacks. Unlike the traditional cloud systems that use PCIe links as the NIC I/O
interface, we leverage memory-interconnected FPGAs as networking devices to
provide the efficiency, transparency, and programmability needed for
fine-grained microservices. We show that this considerably improves CPU
utilization and performance for cloud RPCs.
</summary>
    <author>
      <name>Nikita Lazarev</name>
    </author>
    <author>
      <name>Neil Adit</name>
    </author>
    <author>
      <name>Shaojie Xiang</name>
    </author>
    <author>
      <name>Zhiru Zhang</name>
    </author>
    <author>
      <name>Christina Delimitrou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.08622v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.08622v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.09363v2</id>
    <updated>2022-09-03T21:47:26Z</updated>
    <published>2020-07-18T08:23:25Z</published>
    <title>Design Space Exploration of Algorithmic Multi-Port Memories in
  High-Performance Application-Specific Accelerators</title>
    <summary>  Memory load/store instructions consume an important part in execution time
and energy consumption in domain-specific accelerators. For designing highly
parallel systems, available parallelism at each granularity is extracted from
the workloads. The maximal use of parallelism at each granularity in these
high-performance designs requires the utilization of multi-port memories.
Currently, true multiport designs are less popular because there is no inherent
EDA support for multiport memory beyond 2-ports, utilizing more ports requires
circuit-level implementation and hence a high design time. In this work, we
present a framework for Design Space Exploration of Algorithmic Multi-Port
Memories (AMM) in ASICs. We study different AMM designs in the literature,
discuss how we incorporate them in the Pre-RTL Aladdin Framework with different
memory depth, port configurations and banking structures. From our analysis on
selected applications from the MachSuite (accelerator benchmark suite), we
understand and quantify the potential use of AMMs (as true multiport memories)
for high performance in applications with low spatial locality in memory access
patterns.
</summary>
    <author>
      <name>Khushal Sethi</name>
    </author>
    <link href="http://arxiv.org/abs/2007.09363v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.09363v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.09490v1</id>
    <updated>2020-07-18T17:50:01Z</updated>
    <published>2020-07-18T17:50:01Z</published>
    <title>DeepDive: An Integrative Algorithm/Architecture Co-Design for Deep
  Separable Convolutional Neural Networks</title>
    <summary>  Deep Separable Convolutional Neural Networks (DSCNNs) have become the
emerging paradigm by offering modular networks with structural sparsity in
order to achieve higher accuracy with relatively lower operations and
parameters. However, there is a lack of customized architectures that can
provide flexible solutions that fit the sparsity of the DSCNNs. This paper
introduces DeepDive, which is a fully-functional, vertical co-design framework,
for power-efficient implementation of DSCNNs on edge FPGAs. DeepDive's
architecture supports crucial heterogeneous Compute Units (CUs) to fully
support DSCNNs with various convolutional operators interconnected with
structural sparsity. It offers an FPGA-aware training and online quantization
combined with modular synthesizable C++ CUs, customized for DSCNNs. The
execution results on Xilinx's ZCU102 FPGA board, demonstrate 47.4 and 233.3
FPS/Watt for MobileNet-V2 and a compact version of EfficientNet, respectively,
as two state-of-the-art depthwise separable CNNs. These comparisons showcase
how DeepDive improves FPS/Watt by 2.2$\times$ and 1.51$\times$ over Jetson Nano
high and low power modes, respectively. It also enhances FPS/Watt about
2.27$\times$ and 37.25$\times$ over two other FPGA implementations. The
DeepDive output for MobileNetV2 is available at
https://github.com/TeCSAR-UNCC/DeepDive.
</summary>
    <author>
      <name>Mohammadreza Baharani</name>
    </author>
    <author>
      <name>Ushma Sunil</name>
    </author>
    <author>
      <name>Kaustubh Manohar</name>
    </author>
    <author>
      <name>Steven Furgurson</name>
    </author>
    <author>
      <name>Hamed Tabkhi</name>
    </author>
    <link href="http://arxiv.org/abs/2007.09490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.09490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.09578v1</id>
    <updated>2020-07-19T03:37:41Z</updated>
    <published>2020-07-19T03:37:41Z</published>
    <title>NeuroMAX: A High Throughput, Multi-Threaded, Log-Based Accelerator for
  Convolutional Neural Networks</title>
    <summary>  Convolutional neural networks (CNNs) require high throughput hardware
accelerators for real time applications owing to their huge computational cost.
Most traditional CNN accelerators rely on single core, linear processing
elements (PEs) in conjunction with 1D dataflows for accelerating convolution
operations. This limits the maximum achievable ratio of peak throughput per PE
count to unity. Most of the past works optimize their dataflows to attain close
to a 100% hardware utilization to reach this ratio. In this paper, we introduce
a high throughput, multi-threaded, log-based PE core. The designed core
provides a 200% increase in peak throughput per PE count while only incurring a
6% increase in area overhead compared to a single, linear multiplier PE core
with same output bit precision. We also present a 2D weight broadcast dataflow
which exploits the multi-threaded nature of the PE cores to achieve a high
hardware utilization per layer for various CNNs. The entire architecture, which
we refer to as NeuroMAX, is implemented on Xilinx Zynq 7020 SoC at 200 MHz
processing clock. Detailed analysis is performed on throughput, hardware
utilization, area and power breakdown, and latency to show performance
improvement compared to previous FPGA and ASIC designs.
</summary>
    <author>
      <name>Mahmood Azhar Qureshi</name>
    </author>
    <author>
      <name>Arslan Munir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in ICCAD 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.09578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.09578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.09822v2</id>
    <updated>2020-10-20T21:51:33Z</updated>
    <published>2020-07-20T00:28:55Z</published>
    <title>UVMBench: A Comprehensive Benchmark Suite for Researching Unified
  Virtual Memory in GPUs</title>
    <summary>  The recent introduction of Unified Virtual Memory (UVM) in GPUs offers a new
programming model that allows GPUs and CPUs to share the same virtual memory
space, which shifts the complex memory management from programmers to GPU
driver/ hardware and enables kernel execution even when memory is
oversubscribed. Meanwhile, UVM may also incur considerable performance overhead
due to tracking and data migration along with special handling of page faults
and page table walk. As UVM is attracting significant attention from the
research community to develop innovative solutions to these problems, in this
paper, we propose a comprehensive UVM benchmark suite named UVMBench to
facilitate future research on this important topic. The proposed UVMBench
consists of 32 representative benchmarks from a wide range of application
domains. The suite also features unified programming implementation and diverse
memory access patterns across benchmarks, thus allowing thorough evaluation and
comparison with current state-of-the-art. A set of experiments have been
conducted on real GPUs to verify and analyze the benchmark suite behaviors
under various scenarios.
</summary>
    <author>
      <name>Yongbin Gu</name>
    </author>
    <author>
      <name>Wenxuan Wu</name>
    </author>
    <author>
      <name>Yunfan Li</name>
    </author>
    <author>
      <name>Lizhong Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.09822v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.09822v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.10451v1</id>
    <updated>2020-07-20T20:17:58Z</updated>
    <published>2020-07-20T20:17:58Z</published>
    <title>HPIPE: Heterogeneous Layer-Pipelined and Sparse-Aware CNN Inference for
  FPGAs</title>
    <summary>  We present both a novel Convolutional Neural Network (CNN) accelerator
architecture and a network compiler for FPGAs that outperforms all prior work.
Instead of having generic processing elements that together process one layer
at a time, our network compiler statically partitions available device
resources and builds custom-tailored hardware for each layer of a CNN. By
building hardware for each layer we can pack our controllers into fewer lookup
tables and use dedicated routing. These efficiencies enable our accelerator to
utilize 2x the DSPs and operate at more than 2x the frequency of prior work on
sparse CNN acceleration on FPGAs. We evaluate the performance of our
architecture on both sparse Resnet-50 and dense MobileNet Imagenet classifiers
on a Stratix 10 2800 FPGA. We find that the sparse Resnet-50 model has
throughput at a batch size of 1 of 4550 images/s, which is nearly 4x the
throughput of NVIDIA's fastest machine learning targeted GPU, the V100, and
outperforms all prior work on FPGAs.
</summary>
    <author>
      <name>Mathew Hall</name>
    </author>
    <author>
      <name>Vaughn Betz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 11 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.10451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.10451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.10454v1</id>
    <updated>2020-07-20T20:31:09Z</updated>
    <published>2020-07-20T20:31:09Z</published>
    <title>Exploiting Process Variations to Secure Photonic NoC Architectures from
  Snooping Attacks</title>
    <summary>  The compact size and high wavelength-selectivity of microring resonators
(MRs) enable photonic networks-on-chip (PNoCs) to utilize
dense-wavelength-division-multiplexing (DWDM) in their photonic waveguides, and
as a result, attain high bandwidth on-chip data transfers. Unfortunately, a
Hardware Trojan in a PNoC can manipulate the electrical driving circuit of its
MRs to cause the MRs to snoop data from the neighboring wavelength channels in
a shared photonic waveguide, which introduces a serious security threat. This
paper presents a framework that utilizes process variation-based authentication
signatures along with architecture-level enhancements to protect against
data-snooping Hardware Trojans during unicast as well as multicast transfers in
PNoCs. Evaluation results indicate that our framework can improve hardware
security across various PNoC architectures with minimal overheads of up to
14.2% in average latency and of up to 14.6% in energy-delay-product (EDP).
</summary>
    <author>
      <name>Sai Vineel Reddy Chittamuru</name>
    </author>
    <author>
      <name>Ishan G Thakkar</name>
    </author>
    <author>
      <name>Sudeep Pasricha</name>
    </author>
    <author>
      <name>Sairam Sri Vatsavai</name>
    </author>
    <author>
      <name>Varun Bhat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pre-Print: Accepted in IEEE TCAD Journal on July 16, 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.10454v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.10454v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.11976v2</id>
    <updated>2020-09-24T13:41:18Z</updated>
    <published>2020-07-13T07:31:02Z</published>
    <title>Comparative Analysis of Polynomial and Rational Approximations of
  Hyperbolic Tangent Function for VLSI Implementation</title>
    <summary>  Deep neural networks yield the state-of-the-art results in many computer
vision and human machine interface applications such as object detection,
speech recognition etc. Since, these networks are computationally expensive,
customized accelerators are designed for achieving the required performance at
lower cost and power. One of the key building blocks of these neural networks
is non-linear activation function such as sigmoid, hyperbolic tangent (tanh),
and ReLU. A low complexity accurate hardware implementation of the activation
function is required to meet the performance and area targets of the neural
network accelerators. Even though, various methods and implementations of tanh
activation function have been published, a comparative study is missing. This
paper presents comparative analysis of polynomial and rational methods and
their hardware implementation.
</summary>
    <author>
      <name>Mahesh Chandra</name>
    </author>
    <link href="http://arxiv.org/abs/2007.11976v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.11976v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.13516v1</id>
    <updated>2020-07-13T07:11:59Z</updated>
    <published>2020-07-13T07:11:59Z</published>
    <title>Hardware Implementation of Hyperbolic Tangent Function using Catmull-Rom
  Spline Interpolation</title>
    <summary>  Deep neural networks yield the state of the art results in many computer
vision and human machine interface tasks such as object recognition, speech
recognition etc. Since, these networks are computationally expensive,
customized accelerators are designed for achieving the required performance at
lower cost and power. One of the key building blocks of these neural networks
is non-linear activation function such as sigmoid, hyperbolic tangent (tanh),
and ReLU. A low complexity accurate hardware implementation of the activation
function is required to meet the performance and area targets of the neural
network accelerators. This paper presents an implementation of tanh function
using the Catmull-Rom spline interpolation. State of the art results are
achieved using this method with comparatively smaller logic area.
</summary>
    <author>
      <name>Mahesh Chandra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures. arXiv admin note: substantial text overlap with
  arXiv:2007.11976</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.13516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.13516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.13661v1</id>
    <updated>2020-07-27T16:15:39Z</updated>
    <published>2020-07-27T16:15:39Z</published>
    <title>CARAM: A Content-Aware Hybrid PCM/DRAM Main Memory System Framework</title>
    <summary>  The emergence of Phase-Change Memory (PCM) provides opportunities for
directly connecting persistent memory to main memory bus. While PCM achieves
high read throughput and low standby power, the critical concerns are its poor
write performance and limited durability, especially when compared to DRAM. A
naturally inspired design is the hybrid memory architecture that fuses DRAM and
PCM, so as to exploit the positive aspects of both types of memory.
Unfortunately, existing solutions are seriously challenged by the limited main
memory size, which is the primary bottleneck of in-memory computing. In this
paper, we introduce a novel Content Aware hybrid PCM/DRAM main memory system
framework - CARAM, which exploits deduplication to improve line sharing with
high memory efficiency. CARAM effectively reduces write traffic to hybrid
memory by removing unnecessary duplicate line writes. It also substantially
extends available free memory space by coalescing redundant lines in hybrid
memory, thereby further improving the wear-leveling efficiency of PCM. To
obtain high data access performance, we also design a set of acceleration
techniques to minimize the overhead caused by extra computation costs. Our
experiment results show that CARAM effectively reduces 15%~42% of memory usage
and improves I/O bandwidth by 13%~116%, while saving 31%~38% energy
consumption, compared to the state-of-the-art of hybrid systems.
</summary>
    <author>
      <name>Yinjin Fu</name>
    </author>
    <link href="http://arxiv.org/abs/2007.13661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.13661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.13828v2</id>
    <updated>2020-07-30T07:55:13Z</updated>
    <published>2020-07-27T19:44:51Z</published>
    <title>GRIP: A Graph Neural Network Accelerator Architecture</title>
    <summary>  We present GRIP, a graph neural network accelerator architecture designed for
low-latency inference. AcceleratingGNNs is challenging because they combine two
distinct types of computation: arithmetic-intensive vertex-centric operations
and memory-intensive edge-centric operations. GRIP splits GNN inference into a
fixed set of edge- and vertex-centric execution phases that can be implemented
in hardware. We then specialize each unit for the unique computational
structure found in each phase.For vertex-centric phases, GRIP uses a high
performance matrix multiply engine coupled with a dedicated memory subsystem
for weights to improve reuse. For edge-centric phases, GRIP use multiple
parallel prefetch and reduction engines to alleviate the irregularity in memory
accesses. Finally, GRIP supports severalGNN optimizations, including a novel
optimization called vertex-tiling which increases the reuse of weight data.We
evaluate GRIP by performing synthesis and place and route for a 28nm
implementation capable of executing inference for several widely-used GNN
models (GCN, GraphSAGE, G-GCN, and GIN). Across several benchmark graphs, it
reduces 99th percentile latency by a geometric mean of 17x and 23x compared to
a CPU and GPU baseline, respectively, while drawing only 5W.
</summary>
    <author>
      <name>Kevin Kiningham</name>
    </author>
    <author>
      <name>Christopher Re</name>
    </author>
    <author>
      <name>Philip Levis</name>
    </author>
    <link href="http://arxiv.org/abs/2007.13828v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.13828v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.14897v1</id>
    <updated>2020-07-29T15:13:47Z</updated>
    <published>2020-07-29T15:13:47Z</published>
    <title>Transaction-level Model Simulator for Communication-Limited Accelerators</title>
    <summary>  Rapid design space exploration in early design stage is critical to
algorithm-architecture co-design for accelerators. In this work, a pre-RTL
cycle-accurate accelerator simulator based on SystemC transaction-level
modeling (TLM), AccTLMSim, is proposed for convolutional neural network (CNN)
accelerators. The accelerator simulator keeps track of each bus transaction
between accelerator and DRAM, taking into account the communication bandwidth.
The simulation results are validated against the implementation results on the
Xilinx Zynq. Using the proposed simulator, it is shown that the communication
bandwidth is severely affected by DRAM latency and bus protocol overhead. In
addition, the loop tiling is optimized to maximize the performance under the
constraint of on-chip SRAM size. Furthermore, a new performance estimation
model is proposed to speed up the design space exploration. Thanks to the
proposed simulator and performance estimation model, it is possible to explore
a design space of millions of architectural options within a few tens of
minutes.
</summary>
    <author>
      <name>Sunwoo Kim</name>
    </author>
    <author>
      <name>Jooho Wang</name>
    </author>
    <author>
      <name>Youngho Seo</name>
    </author>
    <author>
      <name>Sanghun Lee</name>
    </author>
    <author>
      <name>Yeji Park</name>
    </author>
    <author>
      <name>Sungkyung Park</name>
    </author>
    <author>
      <name>Chester Sungchung Park</name>
    </author>
    <link href="http://arxiv.org/abs/2007.14897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.14897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.02791v1</id>
    <updated>2020-12-07T17:44:58Z</updated>
    <published>2020-12-07T17:44:58Z</published>
    <title>A Unified Model for Gate Level Propagation Analysis</title>
    <summary>  Classic hardware verification techniques (e.g., X-propagation and
fault-propagation) and more recent hardware security verification techniques
based on information flow tracking (IFT) aim to understand how information
passes, affects, and otherwise modifies a circuit. These techniques all have
separate usage scenarios, but when dissected into their core functionality,
they relate in a fundamental manner. In this paper, we develop a common
framework for gate level propagation analysis. We use our model to generate
synthesizable propagation logic to use in standard EDA tools. To justify our
model, we prove that Precise Hardware IFT is equivalent to gate level
X-propagation and imprecise fault propagation. We also show that the difference
between Precise Hardware IFT and fault propagation is not significant for
74X-series and '85 ISCAS benchmarks with more than 313 gates and the difference
between imprecise hardware IFT and Precise Hardware IFT is almost always
significant regardless of size.
</summary>
    <author>
      <name>Jeremy Blackstone</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of California, San Diego, USA</arxiv:affiliation>
    </author>
    <author>
      <name>Wei Hu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Northwestern Polytechnical University, China</arxiv:affiliation>
    </author>
    <author>
      <name>Alric Althoff</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of California, San Diego, USA</arxiv:affiliation>
    </author>
    <author>
      <name>Armaiti Ardeshiricham</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of California, San Diego, USA</arxiv:affiliation>
    </author>
    <author>
      <name>Lu Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Northwestern Polytechnical University, China</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Kastner</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of California, San Diego, USA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2012.02791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.02791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.03672v1</id>
    <updated>2020-11-17T16:20:44Z</updated>
    <published>2020-11-17T16:20:44Z</published>
    <title>FPGA deep learning acceleration based on convolutional neural network</title>
    <summary>  In view of the large amount of calculation and long calculation time of
convolutional neural network (CNN), this paper proposes a convolutional neural
network hardware accelerator based on field programmable logic gate array
(FPGA). First, through in-depth analysis of the forward operation principle of
the convolutional layer and exploration of the parallelism of the convolutional
layer operation, a hardware architecture of input channel parallelism, output
channel parallelism and convolution window deep pipeline is designed. Then in
the above architecture, a fully parallel multiplication-addition tree module is
designed to accelerate the convolution operation and an efficient window buffer
module to implement the pipeline operation of the convolution window. The final
experimental results show that the energy efficiency ratio of the accelerator
proposed in this article reaches 32.73 GOPS/W, which is 34% higher than the
existing solution, and the performance reaches 317.86 GOPS.
</summary>
    <author>
      <name>Xiong Jun</name>
    </author>
    <link href="http://arxiv.org/abs/2012.03672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.03672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.05181v2</id>
    <updated>2021-01-19T19:51:52Z</updated>
    <published>2020-12-09T17:17:50Z</published>
    <title>Virtual-Link: A Scalable Multi-Producer, Multi-Consumer Message Queue
  Architecture for Cross-Core Communication</title>
    <summary>  Cross-core communication is increasingly a bottleneck as the number of
processing elements increase per system-on-chip. Typical hardware solutions to
cross-core communication are often inflexible; while software solutions are
flexible, they have performance scaling limitations. A key problem, as we will
show, is that of shared state in software-based message queue mechanisms. This
paper proposes Virtual-Link (VL), a novel light-weight communication mechanism
with hardware support to facilitate M:N lock-free data movement. VL reduces the
amount of coherent shared state, which is a bottleneck for many approaches, to
zero. VL provides further latency benefit by keeping data on the fast path
(i.e., within the on-chip interconnect). VL enables directed cache-injection
(stashing) between PEs on the coherence bus, reducing the latency for
core-to-core communication. VL is particularly effective for fine-grain tasks
on streaming data. Evaluation on a full system simulator with 7 benchmarks
shows that VL achieves a 2.09x speedup over state-of-the-art software-based
communication mechanisms, while reducing memory traffic by 61%.
</summary>
    <author>
      <name>Qinzhe Wu</name>
    </author>
    <author>
      <name>Jonathan Beard</name>
    </author>
    <author>
      <name>Ashen Ekanayake</name>
    </author>
    <author>
      <name>Andreas Gerstlauer</name>
    </author>
    <author>
      <name>Lizy K. John</name>
    </author>
    <link href="http://arxiv.org/abs/2012.05181v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.05181v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.08320v1</id>
    <updated>2020-12-15T14:28:52Z</updated>
    <published>2020-12-15T14:28:52Z</published>
    <title>A Comparative Study between HLS and HDL on SoC for Image Processing
  Applications</title>
    <summary>  The increasing complexity in today's systems and the limited market times
demand new development tools for FPGA. Currently, in addition to traditional
hardware description languages (HDLs), there are high-level synthesis (HLS)
tools that increase the abstraction level in system development. Despite the
greater simplicity of design and testing, HLS has some drawbacks in describing
harware. This paper presents a comparative study between HLS and HDL for FPGA,
using a Sobel filter as a case study in the image processing field. The results
show that the HDL implementation is slightly better than the HLS version
considering resource usage and response time. However, the programming effort
required in the HDL solution is significantly larger than in the HLS
counterpart.
</summary>
    <author>
      <name>Roberto Millon</name>
    </author>
    <author>
      <name>Emmanuel Frati</name>
    </author>
    <author>
      <name>Enzo Rucci</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.37537/rev.elektron.4.2.117.2020</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.37537/rev.elektron.4.2.117.2020" rel="related"/>
    <link href="http://arxiv.org/abs/2012.08320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.08320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.10597v1</id>
    <updated>2020-12-19T04:55:41Z</updated>
    <published>2020-12-19T04:55:41Z</published>
    <title>MAVIREC: ML-Aided Vectored IR-DropEstimation and Classification</title>
    <summary>  Vectored IR drop analysis is a critical step in chip signoff that checks the
power integrity of an on-chip power delivery network. Due to the prohibitive
runtimes of dynamic IR drop analysis, the large number of test patterns must be
whittled down to a small subset of worst-case IR vectors. Unlike the
traditional slow heuristic method that select a few vectors with incomplete
coverage, MAVIREC uses machine learning techniques -- 3D convolutions and
regression-like layers -- for accurately recommending a larger subset of test
patterns that exercise worst-case scenarios. In under 30 minutes, MAVIREC
profiles 100K-cycle vectors and provides better coverage than a
state-of-the-art industrial flow. Further, MAVIREC's IR drop predictor shows
10x speedup with under 4mV RMSE relative to an industrial flow.
</summary>
    <author>
      <name>Vidya A. Chhabria</name>
    </author>
    <author>
      <name>Yanqing Zhang</name>
    </author>
    <author>
      <name>Haoxing Ren</name>
    </author>
    <author>
      <name>Ben Keller</name>
    </author>
    <author>
      <name>Brucek Khailany</name>
    </author>
    <author>
      <name>Sachin S. Sapatnekar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages paper. This has been reviewed at Design Automation and Test
  Conference 2021 and has been accepted as a four page paper. This is a longer
  version of that</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.10597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.10597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.12381v1</id>
    <updated>2020-12-22T22:01:15Z</updated>
    <published>2020-12-22T22:01:15Z</published>
    <title>Intelligent Architectures for Intelligent Computing Systems</title>
    <summary>  Computing is bottlenecked by data. Large amounts of application data
overwhelm storage capability, communication capability, and computation
capability of the modern machines we design today. We argue that an intelligent
architecture should be designed to handle data well. We show that handling data
well requires designing architectures based on three key principles: 1)
data-centric, 2) data-driven, 3) dataaware. We give several examples for how to
exploit each of these principles to design a much more efficient and high
performance computing system. We discuss how to enable adoption of such
fundamentally more intelligent architectures, which we believe are key to
efficiency, performance, and sustainability. We conclude with some guiding
principles for future computing architecture and system designs. This
accompanying short paper provides a summary of the associated invited talk at
DATE 2021 and points the reader to further work that may be beneficial to
examine.
</summary>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear as an invited talk and accompanying summary paper at DATE
  2021 conference. arXiv admin note: substantial text overlap with
  arXiv:2008.06112</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.12381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.12381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.12563v3</id>
    <updated>2021-02-18T10:32:17Z</updated>
    <published>2020-12-23T09:49:51Z</published>
    <title>Architecture, Dataflow and Physical Design Implications of 3D-ICs for
  DNN-Accelerators</title>
    <summary>  The everlasting demand for higher computing power for deep neural networks
(DNNs) drives the development of parallel computing architectures. 3D
integration, in which chips are integrated and connected vertically, can
further increase performance because it introduces another level of spatial
parallelism. Therefore, we analyze dataflows, performance, area, power and
temperature of such 3D-DNN-accelerators. Monolithic and TSV-based stacked
3D-ICs are compared against 2D-ICs. We identify workload properties and
architectural parameters for efficient 3D-ICs and achieve up to 9.14x speedup
of 3D vs. 2D. We discuss area-performance trade-offs. We demonstrate
applicability as the 3D-IC draws similar power as 2D-ICs and is not thermal
limited.
</summary>
    <author>
      <name>Jan Moritz Joseph</name>
    </author>
    <author>
      <name>Ananda Samajdar</name>
    </author>
    <author>
      <name>Lingjun Zhu</name>
    </author>
    <author>
      <name>Rainer Leupers</name>
    </author>
    <author>
      <name>Sung-Kyu Lim</name>
    </author>
    <author>
      <name>Thilo Pionteck</name>
    </author>
    <author>
      <name>Tushar Krishna</name>
    </author>
    <link href="http://arxiv.org/abs/2012.12563v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.12563v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.00055v1</id>
    <updated>2020-12-31T20:13:10Z</updated>
    <published>2020-12-31T20:13:10Z</published>
    <title>Data Criticality in Multi-Threaded Applications: An Insight for
  Many-Core Systems</title>
    <summary>  Multi-threaded applications are capable of exploiting the full potential of
many-core systems. However, Network-on-Chip (NoC) based inter-core
communication in many-core systems is responsible for 60-75% of the miss
latency experienced by multi-threaded applications. Delay in the arrival of
critical data at the requesting core severely hampers performance. This brief
presents some interesting insights about how critical data is requested from
the memory by multi-threaded applications. Then it investigates the cause of
delay in NoC and how it affects the performance. Finally, this brief shows how
NoC-aware memory access optimisations can significantly improve performance.
Our experimental evaluation considers early restart memory access optimisation
and demonstrates that by exploiting NoC resources, critical data can be
prioritised to reduce miss penalty by 10-12% and improve system performance by
7-11%.
</summary>
    <author>
      <name>Abhijit Das</name>
    </author>
    <author>
      <name>John Jose</name>
    </author>
    <author>
      <name>Prabhat Mishra</name>
    </author>
    <link href="http://arxiv.org/abs/2101.00055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.01173v1</id>
    <updated>2021-01-01T20:49:34Z</updated>
    <published>2021-01-01T20:49:34Z</published>
    <title>Design of a Dynamic Parameter-Controlled Chaotic-PRNG in a 65nm CMOS
  process</title>
    <summary>  In this paper, we present the design of a new chaotic map circuit with a 65nm
CMOS process. This chaotic map circuit uses a dynamic parameter-control
topology and generates a wide chaotic range. We propose two designs of dynamic
parameter-controlled chaotic map (DPCCM)-based pseudo-random number generators
(PRNG). The randomness of the generated sequence is verified using three
different statistical tests, namely, NIST SP 800-22 test, FIPS PUB 140-2 test,
and Diehard test. Our first design offers a throughput of 200 MS/s with an
on-chip area of 0.024mm2 and a power consumption of 2.33mW. The throughput of
our second design is 300 MS/s with an area consumption of 0.132mm2 and power
consumption of 2.14mW. The wider chaotic range and lower-overhead, offered by
our designs, can be highly suitable for various applications such as, logic
obfuscation, chaos-based cryptography, re-configurable random number
generation,and hard-ware security for resource-constrained edge devices like
IoT.
</summary>
    <author>
      <name>Partha Sarathi Paul</name>
    </author>
    <author>
      <name>Maisha Sadia</name>
    </author>
    <author>
      <name>Md Sakib Hasan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted and presented in '14TH IEEE DALLAS CIRCUITS AND SYSTEMS
  CONFERENCE'. The name is appeared in the schedule of the conference:
  https://engineering.utdallas.edu/DCAS/schedule.html</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.01173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.01173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.01416v1</id>
    <updated>2021-01-05T08:54:17Z</updated>
    <published>2021-01-05T08:54:17Z</published>
    <title>An Investigation on Inherent Robustness of Posit Data Representation</title>
    <summary>  As the dimensions and operating voltages of computer electronics shrink to
cope with consumers' demand for higher performance and lower power consumption,
circuit sensitivity to soft errors increases dramatically. Recently, a new
data-type is proposed in the literature called posit data type. Posit
arithmetic has absolute advantages such as higher numerical accuracy, speed,
and simpler hardware design than IEEE 754-2008 technical standard-compliant
arithmetic. In this paper, we propose a comparative robustness study between
32-bit posit and 32-bit IEEE 754-2008 compliant representations. At first, we
propose a theoretical analysis for IEEE 754 compliant numbers and posit numbers
for single bit flip and double bit flips. Then, we conduct exhaustive fault
injection experiments that show a considerable inherent resilience in posit
format compared to classical IEEE 754 compliant representation. To show a
relevant use-case of fault-tolerant applications, we perform experiments on a
set of machine-learning applications. In more than 95% of the exhaustive fault
injection exploration, posit representation is less impacted by faults than the
IEEE 754 compliant floating-point representation. Moreover, in 100% of the
tested machine-learning applications, the accuracy of posit-implemented systems
is higher than the classical floating-point-based ones.
</summary>
    <author>
      <name>Ihsen Alouani</name>
    </author>
    <author>
      <name>Anouar Ben Khalifa</name>
    </author>
    <author>
      <name>Farhad Merchant</name>
    </author>
    <author>
      <name>Rainer Leupers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in VLSID 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.01416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.01416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.02419v1</id>
    <updated>2021-01-07T07:50:01Z</updated>
    <published>2021-01-07T07:50:01Z</published>
    <title>A Low Power In-Memory Multiplication andAccumulation Array with Modified
  Radix-4 Inputand Canonical Signed Digit Weights</title>
    <summary>  A mass of data transfer between the processing and storage units has been the
leading bottleneck in modern Von-Neuman computing systems, especially when used
for Artificial Intelligence (AI) tasks. Computing-in-Memory (CIM) has shown
great potential to reduce both latency and power consumption. However, the
conventional analog CIM schemes are suffering from reliability issues, which
may significantly degenerate the accuracy of the computation. Recently, CIM
schemes with digitized input data and weights have been proposed for high
reliable computing. However, the properties of the digital memory and input
data are not fully utilized. This paper presents a novel low power CIM scheme
to further reduce the power consumption by using a Modified Radix-4 (M-RD4)
booth algorithm at the input and a Modified Canonical Signed Digit (M-CSD) for
the network weights. The simulation results show that M-Rd4 and M-CSD reduce
the ratio of $1\times1$ by 78.5\% on LeNet and 80.2\% on AlexNet, and improve
the computing efficiency by 41.6\% in average. The computing-power rate at the
fixed-point 8-bit is 60.68 TOPS/s/W.
</summary>
    <author>
      <name>Rui Xiao</name>
    </author>
    <author>
      <name>Kejie Huang</name>
    </author>
    <author>
      <name>Yewei Zhang</name>
    </author>
    <author>
      <name>Haibin Shen</name>
    </author>
    <link href="http://arxiv.org/abs/2101.02419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.02419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.02667v1</id>
    <updated>2021-01-07T18:23:48Z</updated>
    <published>2021-01-07T18:23:48Z</published>
    <title>BRDS: An FPGA-based LSTM Accelerator with Row-Balanced Dual-Ratio
  Sparsification</title>
    <summary>  In this paper, first, a hardware-friendly pruning algorithm for reducing
energy consumption and improving the speed of Long Short-Term Memory (LSTM)
neural network accelerators is presented. Next, an FPGA-based platform for
efficient execution of the pruned networks based on the proposed algorithm is
introduced. By considering the sensitivity of two weight matrices of the LSTM
models in pruning, different sparsity ratios (i.e., dual-ratio sparsity) are
applied to these weight matrices. To reduce memory accesses, a row-wise
sparsity pattern is adopted. The proposed hardware architecture makes use of
computation overlapping and pipelining to achieve low-power and high-speed. The
effectiveness of the proposed pruning algorithm and accelerator is assessed
under some benchmarks for natural language processing, binary sentiment
classification, and speech recognition. Results show that, e.g., compared to a
recently published work in this field, the proposed accelerator could provide
up to 272% higher effective GOPS/W and the perplexity error is reduced by up to
1.4% for the PTB dataset.
</summary>
    <author>
      <name>Seyed Abolfazl Ghasemzadeh</name>
    </author>
    <author>
      <name>Erfan Bank Tavakoli</name>
    </author>
    <author>
      <name>Mehdi Kamal</name>
    </author>
    <author>
      <name>Ali Afzali-Kusha</name>
    </author>
    <author>
      <name>Massoud Pedram</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 9 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.02667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.02667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.05591v1</id>
    <updated>2021-01-14T13:52:34Z</updated>
    <published>2021-01-14T13:52:34Z</published>
    <title>ANDROMEDA: An FPGA Based RISC-V MPSoC Exploration Framework</title>
    <summary>  With the growing demands of consumer electronic products, the computational
requirements are increasing exponentially. Due to the applications'
computational needs, the computer architects are trying to pack as many cores
as possible on a single die for accelerated execution of the application
program codes. In a multiprocessor system-on-chip (MPSoC), striking a balance
among the number of cores, memory subsystems, and network-on-chip parameters is
essential to attain the desired performance. In this paper, we present
ANDROMEDA, a RISC-V based framework that allows us to explore the different
configurations of an MPSoC and observe the performance penalties and gains. We
emulate the various configurations of MPSoC on the Synopsys HAPS-80D Dual FPGA
platform. Using STREAM, matrix multiply, and N-body simulations as benchmarks,
we demonstrate our framework's efficacy in quickly identifying the right
parameters for efficient execution of these benchmarks.
</summary>
    <author>
      <name>Farhad Merchant</name>
    </author>
    <author>
      <name>Dominik Sisejkovic</name>
    </author>
    <author>
      <name>Lennart M. Reimann</name>
    </author>
    <author>
      <name>Kirthihan Yasotharan</name>
    </author>
    <author>
      <name>Thomas Grass</name>
    </author>
    <author>
      <name>Rainer Leupers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in VLSI Design 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.05591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.05591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.06665v1</id>
    <updated>2021-01-17T13:19:10Z</updated>
    <published>2021-01-17T13:19:10Z</published>
    <title>Brightening the Optical Flow through Posit Arithmetic</title>
    <summary>  As new technologies are invented, their commercial viability needs to be
carefully examined along with their technical merits and demerits. The posit
data format, proposed as a drop-in replacement for IEEE 754 float format, is
one such invention that requires extensive theoretical and experimental study
to identify products that can benefit from the advantages of posits for
specific market segments. In this paper, we present an extensive empirical
study of posit-based arithmetic vis-\`a-vis IEEE 754 compliant arithmetic for
the optical flow estimation method called Lucas-Kanade (LuKa). First, we use
SoftPosit and SoftFloat format emulators to perform an empirical error analysis
of the LuKa method. Our study shows that the average error in LuKa with
SoftPosit is an order of magnitude lower than LuKa with SoftFloat. We then
present the integration of the hardware implementation of a posit adder and
multiplier in a RISC-V open-source platform. We make several recommendations,
along with the analysis of LuKa in the RISC-V context, for future generation
platforms incorporating posit arithmetic units.
</summary>
    <author>
      <name>Vinay Saxena</name>
    </author>
    <author>
      <name>Ankitha Reddy</name>
    </author>
    <author>
      <name>Jonathan Neudorfer</name>
    </author>
    <author>
      <name>John Gustafson</name>
    </author>
    <author>
      <name>Sangeeth Nambiar</name>
    </author>
    <author>
      <name>Rainer Leupers</name>
    </author>
    <author>
      <name>Farhad Merchant</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ISQED 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.06665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.06665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.08715v1</id>
    <updated>2021-01-21T16:48:28Z</updated>
    <published>2021-01-21T16:48:28Z</published>
    <title>Cain: Automatic Code Generation for Simultaneous Convolutional Kernels
  on Focal-plane Sensor-processors</title>
    <summary>  Focal-plane Sensor-processors (FPSPs) are a camera technology that enable low
power, high frame rate computation, making them suitable for edge computation.
Unfortunately, these devices' limited instruction sets and registers make
developing complex algorithms difficult. In this work, we present Cain - a
compiler that targets SCAMP-5, a general-purpose FPSP - which generates code
from multiple convolutional kernels. As an example, given the convolutional
kernels for an MNIST digit recognition neural network, Cain produces code that
is half as long, when compared to the other available compilers for SCAMP-5.
</summary>
    <author>
      <name>Edward Stow</name>
    </author>
    <author>
      <name>Riku Murai</name>
    </author>
    <author>
      <name>Sajad Saeedi</name>
    </author>
    <author>
      <name>Paul H. J. Kelly</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures, Accepted at LCPC 2020 to be published by
  Springer</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.08715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.08715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.4; I.4.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.08744v3</id>
    <updated>2021-09-01T14:53:54Z</updated>
    <published>2021-01-14T21:38:57Z</published>
    <title>Enabling Large Neural Networks on Tiny Microcontrollers with Swapping</title>
    <summary>  Running neural networks (NNs) on microcontroller units (MCUs) is becoming
increasingly important, but is very difficult due to the tiny SRAM size of MCU.
Prior work proposes many algorithm-level techniques to reduce NN memory
footprints, but all at the cost of sacrificing accuracy and generality, which
disqualifies MCUs for many important use cases. We investigate a system
solution for MCUs to execute NNs out of core: dynamically swapping NN data
chunks between an MCU's tiny SRAM and its large, low-cost external flash.
Out-of-core NNs on MCUs raise multiple concerns: execution slowdown, storage
wear out, energy consumption, and data security. We present a study showing
that none is a showstopper; the key benefit -- MCUs being able to run large NNs
with full accuracy and generality -- triumphs the overheads. Our findings
suggest that MCUs can play a much greater role in edge intelligence.
</summary>
    <author>
      <name>Hongyu Miao</name>
    </author>
    <author>
      <name>Felix Xiaozhu Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2101.08744v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.08744v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.08884v1</id>
    <updated>2021-01-21T23:16:22Z</updated>
    <published>2021-01-21T23:16:22Z</published>
    <title>Direct Spatial Implementation of Sparse Matrix Multipliers for Reservoir
  Computing</title>
    <summary>  Reservoir computing systems rely on the recurrent multiplication of a very
large, sparse, fixed matrix. We argue that direct spatial implementation of
these fixed matrices minimizes the work performed in the computation, and
allows for significant reduction in latency and power through constant
propagation and logic minimization. Bit-serial arithmetic enables massive
static matrices to be implemented. We present the structure of our bit-serial
matrix multiplier, and evaluate using canonical signed digit representation to
further reduce logic utilization. We have implemented these matrices on a large
FPGA and provide a cost model that is simple and extensible. These FPGA
implementations, on average, reduce latency by 50x up to 86x versus GPU
libraries. Comparing against a recent sparse DNN accelerator, we measure a 4.1x
to 47x reduction in latency depending on matrix dimension and sparsity.
Throughput of the FPGA solution is also competitive for a wide range of matrix
dimensions and batch sizes. Finally, we discuss ways these techniques could be
deployed in ASICs, making them applicable for dynamic sparse matrix
computations.
</summary>
    <author>
      <name>Matthew Denton</name>
    </author>
    <author>
      <name>Herman Schmit</name>
    </author>
    <link href="http://arxiv.org/abs/2101.08884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.08884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.09968v1</id>
    <updated>2021-01-25T09:17:20Z</updated>
    <published>2021-01-25T09:17:20Z</published>
    <title>Freezer: A Specialized NVM Backup Controller for Intermittently-Powered
  Systems</title>
    <summary>  The explosion of IoT and wearable devices determined a rising attention
towards energy harvesting as source for powering these systems. In this
context, many applications cannot afford the presence of a battery because of
size, weight and cost issues. Therefore, due to the intermittent nature of
ambient energy sources, these systems must be able to save and restore their
state, in order to guarantee progress across power interruptions. In this work,
we propose a specialized backup/restore controller that dynamically tracks the
memory accesses during the execution of the program. The controller then
commits the changes to a snapshot in a Non-Volatile Memory (NVM) when a power
failure is detected. Our approach does not require complex hybrid memories and
can be implemented with standard components. % and integrated in any MCU with
Results on a set of benchmarks show an average $8\times$ reduction in backup
size. Thanks to our dedicated controller, the backup time is further reduced by
more than $100\times$, with an area and power overhead of only 0.4\% and 0.8\%,
respectively, w.r.t. a low-end IoT node.
</summary>
    <author>
      <name>Davide Pala</name>
    </author>
    <author>
      <name>Ivan Miro-Panades</name>
    </author>
    <author>
      <name>Olivier Sentieys</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCAD.2020.3025063</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCAD.2020.3025063" rel="related"/>
    <link href="http://arxiv.org/abs/2101.09968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.09968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.12351v1</id>
    <updated>2021-01-29T01:43:40Z</updated>
    <published>2021-01-29T01:43:40Z</published>
    <title>DNN-Life: An Energy-Efficient Aging Mitigation Framework for Improving
  the Lifetime of On-Chip Weight Memories in Deep Neural Network Hardware
  Architectures</title>
    <summary>  Negative Biased Temperature Instability (NBTI)-induced aging is one of the
critical reliability threats in nano-scale devices. This paper makes the first
attempt to study the NBTI aging in the on-chip weight memories of deep neural
network (DNN) hardware accelerators, subjected to complex DNN workloads. We
propose DNN-Life, a specialized aging analysis and mitigation framework for
DNNs, which jointly exploits hardware- and software-level knowledge to improve
the lifetime of a DNN weight memory with reduced energy overhead. At the
software-level, we analyze the effects of different DNN quantization methods on
the distribution of the bits of weight values. Based on the insights gained
from this analysis, we propose a micro-architecture that employs low-cost
memory-write (and read) transducers to achieve an optimal duty-cycle at run
time in the weight memory cells, thereby balancing their aging. As a result,
our DNN-Life framework enables efficient aging mitigation of weight memory of
the given DNN hardware at minimal energy overhead during the inference process.
</summary>
    <author>
      <name>Muhammad Abdullah Hanif</name>
    </author>
    <author>
      <name>Muhammad Shafique</name>
    </author>
    <link href="http://arxiv.org/abs/2101.12351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.12351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.00007v3</id>
    <updated>2021-05-21T20:31:04Z</updated>
    <published>2021-02-26T18:12:41Z</published>
    <title>An Architecture for Memory Centric Active Storage (MCAS)</title>
    <summary>  The advent of CPU-attached persistent memory technology, such as Intel's
Optane Persistent Memory Modules (PMM), has brought with it new opportunities
for storage. In 2018, IBM Research Almaden began investigating and developing a
new enterprise-grade storage solution directly aimed at this emerging
technology. MCAS (Memory Centric Active Storage) defines an evolved
network-attached key-value store that offers both near-data compute and the
ability to layer enterprise-grade data management services on shared persistent
memory. As a converged memory-storage tier, MCAS moves towards eliminating the
traditional separation of compute and storage, and thereby unifying the data
space. This paper provides an in-depth review of the MCAS architecture and
implementation, as well as general performance results.
</summary>
    <author>
      <name>Daniel Waddington</name>
    </author>
    <author>
      <name>Clem Dickey</name>
    </author>
    <author>
      <name>Moshik Hershcovitch</name>
    </author>
    <author>
      <name>Sangeetha Seshadri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revision 1.2</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.00007v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.00007v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.00216v1</id>
    <updated>2021-02-27T13:41:16Z</updated>
    <published>2021-02-27T13:41:16Z</published>
    <title>ProbLP: A framework for low-precision probabilistic inference</title>
    <summary>  Bayesian reasoning is a powerful mechanism for probabilistic inference in
smart edge-devices. During such inferences, a low-precision arithmetic
representation can enable improved energy efficiency. However, its impact on
inference accuracy is not yet understood. Furthermore, general-purpose hardware
does not natively support low-precision representation. To address this, we
propose ProbLP, a framework that automates the analysis and design of
low-precision probabilistic inference hardware. It automatically chooses an
appropriate energy-efficient representation based on worst-case error-bounds
and hardware energy-models. It generates custom hardware for the resulting
inference network exploiting parallelism, pipelining and low-precision
operation. The framework is validated on several embedded-sensing benchmarks.
</summary>
    <author>
      <name>Nimish Shah</name>
    </author>
    <author>
      <name>Laura I. Galindez Olascoaga</name>
    </author>
    <author>
      <name>Wannes Meert</name>
    </author>
    <author>
      <name>Marian Verhelst</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3316781.3317885</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3316781.3317885" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 56th Annual Design Automation Conference (DAC)
  2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2103.00216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.00216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.00266v1</id>
    <updated>2021-02-27T16:57:20Z</updated>
    <published>2021-02-27T16:57:20Z</published>
    <title>Acceleration of probabilistic reasoning through custom processor
  architecture</title>
    <summary>  Probabilistic reasoning is an essential tool for robust decision-making
systems because of its ability to explicitly handle real-world uncertainty,
constraints and causal relations. Consequently, researchers are developing
hybrid models by combining Deep Learning with probabilistic reasoning for
safety-critical applications like self-driving vehicles, autonomous drones,
etc. However, probabilistic reasoning kernels do not execute efficiently on
CPUs or GPUs. This paper, therefore, proposes a custom programmable processor
to accelerate sum-product networks, an important probabilistic reasoning
execution kernel. The processor has an optimized datapath architecture and
memory hierarchy optimized for sum-product networks execution. Experimental
results show that the processor, while requiring fewer computational and memory
units, achieves a 12x throughput benefit over the Nvidia Jetson TX2 embedded
GPU platform.
</summary>
    <author>
      <name>Nimish Shah</name>
    </author>
    <author>
      <name>Laura I. Galindez Olascoaga</name>
    </author>
    <author>
      <name>Wannes Meert</name>
    </author>
    <author>
      <name>Marian Verhelst</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23919/DATE48585.2020.9116326</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23919/DATE48585.2020.9116326" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)
  2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2103.00266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.00266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.01195v1</id>
    <updated>2021-03-01T18:42:31Z</updated>
    <published>2021-03-01T18:42:31Z</published>
    <title>Run-time Performance Monitoring of Heterogenous Hw/Sw Platforms Using
  PAPI</title>
    <summary>  In the era of Cyber Physical Systems, designers need to offer support for
run-time adaptivity considering different constraints, including the internal
status of the system. This work presents a run-time monitoring approach, based
on the Performance Application Programming Interface, that offers a unified
interface to transparently access both the standard Performance Monitoring
Counters (PMCs) in the CPUs and the custom ones integrated into hardware
accelerators. Automatic tools offer to Sw programmers the support to design and
implement Coarse-Grain Virtual Reconfigurable Circuits, instrumented with
custom PMCs. This approach has been validated on a heterogeneous application
for image/video processing with an overhead of 6% of the execution time.
</summary>
    <author>
      <name>Tiziana Fanni</name>
    </author>
    <author>
      <name>Daniel Madronal</name>
    </author>
    <author>
      <name>Claudio Rubattu</name>
    </author>
    <author>
      <name>Carlo Sau</name>
    </author>
    <author>
      <name>Francesca Palumbo</name>
    </author>
    <author>
      <name>Eduardo Juarez</name>
    </author>
    <author>
      <name>Maxime Pelcat</name>
    </author>
    <author>
      <name>Cesar Sanz</name>
    </author>
    <author>
      <name>Luigi Raffo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Sixth International Workshop on FPGAs for Software Programmers (FSP
  Workshop) 2019 ISBN: 978-3-8007-5045-0</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.01195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.01195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.02800v1</id>
    <updated>2021-03-04T02:49:16Z</updated>
    <published>2021-03-04T02:49:16Z</published>
    <title>Hardware Acceleration of Fully Quantized BERT for Efficient Natural
  Language Processing</title>
    <summary>  BERT is the most recent Transformer-based model that achieves
state-of-the-art performance in various NLP tasks. In this paper, we
investigate the hardware acceleration of BERT on FPGA for edge computing. To
tackle the issue of huge computational complexity and memory footprint, we
propose to fully quantize the BERT (FQ-BERT), including weights, activations,
softmax, layer normalization, and all the intermediate results. Experiments
demonstrate that the FQ-BERT can achieve 7.94x compression for weights with
negligible performance loss. We then propose an accelerator tailored for the
FQ-BERT and evaluate on Xilinx ZCU102 and ZCU111 FPGA. It can achieve a
performance-per-watt of 3.18 fps/W, which is 28.91x and 12.72x over Intel(R)
Core(TM) i7-8700 CPU and NVIDIA K80 GPU, respectively.
</summary>
    <author>
      <name>Zejian Liu</name>
    </author>
    <author>
      <name>Gang Li</name>
    </author>
    <author>
      <name>Jian Cheng</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Design, Automation &amp; Test in Europe (DATE) 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2103.02800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.02800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.03953v1</id>
    <updated>2021-03-05T21:47:48Z</updated>
    <published>2021-03-05T21:47:48Z</published>
    <title>ODIN: A Bit-Parallel Stochastic Arithmetic Based Accelerator for In-Situ
  Neural Network Processing in Phase Change RAM</title>
    <summary>  Due to the very rapidly growing use of Artificial Neural Networks (ANNs) in
real-world applications related to machine learning and Artificial Intelligence
(AI), several hardware accelerator de-signs for ANNs have been proposed
recently. In this paper, we present a novel processing-in-memory (PIM) engine
called ODIN that employs hybrid binary-stochastic bit-parallel arithmetic
in-side phase change RAM (PCRAM) to enable a low-overhead in-situ acceleration
of all essential ANN functions such as multiply-accumulate (MAC), nonlinear
activation, and pooling. We mapped four ANN benchmark applications on ODIN to
compare its performance with a conventional processor-centric design and a
crossbar-based in-situ ANN accelerator from prior work. The results of our
analysis for the considered ANN topologies indicate that our ODIN accelerator
can be at least 5.8x faster and 23.2x more energy-efficient, and up to 90.8x
faster and 1554x more energy-efficient, compared to the crossbar-based in-situ
ANN accelerator from prior work.
</summary>
    <author>
      <name>Supreeth Mysore Shivanandamurthy</name>
    </author>
    <author>
      <name>Ishan. G. Thakkar</name>
    </author>
    <author>
      <name>Sayed Ahmad Salehi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 Figures, 4 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.03953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.03953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.04808v1</id>
    <updated>2021-03-08T15:09:40Z</updated>
    <published>2021-03-08T15:09:40Z</published>
    <title>Scaling up HBM Efficiency of Top-K SpMV for Approximate Embedding
  Similarity on FPGAs</title>
    <summary>  Top-K SpMV is a key component of similarity-search on sparse embeddings. This
sparse workload does not perform well on general-purpose NUMA systems that
employ traditional caching strategies. Instead, modern FPGA accelerator cards
have a few tricks up their sleeve. We introduce a Top-K SpMV FPGA design that
leverages reduced precision and a novel packet-wise CSR matrix compression,
enabling custom data layouts and delivering bandwidth efficiency often
unreachable even in architectures with higher peak bandwidth. With HBM-based
boards, we are 100x faster than a multi-threaded CPU implementation and 2x
faster than a GPU with 20% higher bandwidth, with 14.2x higher
power-efficiency.
</summary>
    <author>
      <name>Alberto Parravicini</name>
    </author>
    <author>
      <name>Luca Giuseppe Cellamare</name>
    </author>
    <author>
      <name>Marco Siracusa</name>
    </author>
    <author>
      <name>Marco Domenico Santambrogio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings of the 58th Design Automation Conference
  (DAC)</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.04808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.04808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.04812v1</id>
    <updated>2021-03-08T15:13:02Z</updated>
    <published>2021-03-08T15:13:02Z</published>
    <title>Reliability-Aware Quantization for Anti-Aging NPUs</title>
    <summary>  Transistor aging is one of the major concerns that challenges designers in
advanced technologies. It profoundly degrades the reliability of circuits
during its lifetime as it slows down transistors resulting in errors due to
timing violations unless large guardbands are included, which leads to
considerable performance losses. When it comes to Neural Processing Units
(NPUs), where increasing the inference speed is the primary goal, such
performance losses cannot be tolerated. In this work, we are the first to
propose a reliability-aware quantization to eliminate aging effects in NPUs
while completely removing guardbands. Our technique delivers a graceful
inference accuracy degradation over time while compensating for the
aging-induced delay increase of the NPU. Our evaluation, over ten
state-of-the-art neural network architectures trained on the ImageNet dataset,
demonstrates that for an entire lifetime of 10 years, the average accuracy loss
is merely 3%. In the meantime, our technique achieves 23% higher performance
due to the elimination of the aging guardband.
</summary>
    <author>
      <name>Sami Salamin</name>
    </author>
    <author>
      <name>Georgios Zervakis</name>
    </author>
    <author>
      <name>Ourania Spantidi</name>
    </author>
    <author>
      <name>Iraklis Anagnostopoulos</name>
    </author>
    <author>
      <name>Jörg Henkel</name>
    </author>
    <author>
      <name>Hussam Amrouch</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23919/DATE51398.2021.9474094</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23919/DATE51398.2021.9474094" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at the 24th Design Automation and Test in
  Europe Conference (DATE) 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.04812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.04812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.05106v1</id>
    <updated>2021-03-02T09:40:55Z</updated>
    <published>2021-03-02T09:40:55Z</published>
    <title>Representing Gate-Level SET Faults by Multiple SEU Faults at RTL</title>
    <summary>  The advanced complex electronic systems increasingly demand safer and more
secure hardware parts. Correspondingly, fault injection became a major
verification milestone for both safety- and security-critical applications.
However, fault injection campaigns for gate-level designs suffer from huge
execution times. Therefore, designers need to apply early design evaluation
techniques to reduce the execution time of fault injection campaigns. In this
work, we propose a method to represent gate-level Single-Event Transient (SET)
faults by multiple Single-Event Upset (SEU) faults at the Register-Transfer
Level. Introduced approach is to identify true and false logic paths for each
SET in the flip-flops fan-in logic cones to obtain more accurate sets of
flip-flops for multiple SEUs injections at RTL. Experimental results
demonstrate the feasibility of the proposed method to successfully reduce the
fault space and also its advantage with respect to state of the art. It was
shown that the approach is able to reduce the fault space, and therefore the
fault-injection effort, by up to tens to hundreds of times.
</summary>
    <author>
      <name>Ahmet Cagri Bagbaba</name>
    </author>
    <author>
      <name>Maksim Jenihhin</name>
    </author>
    <author>
      <name>Raimund Ubar</name>
    </author>
    <author>
      <name>Christian Sauer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IOLTS50870.2020.9159715</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IOLTS50870.2020.9159715" rel="related"/>
    <link href="http://arxiv.org/abs/2103.05106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.05106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.06656v1</id>
    <updated>2021-03-11T13:29:19Z</updated>
    <published>2021-03-11T13:29:19Z</published>
    <title>Exploring the Mysteries of System-Level Test</title>
    <summary>  System-level test, or SLT, is an increasingly important process step in
today's integrated circuit testing flows. Broadly speaking, SLT aims at
executing functional workloads in operational modes. In this paper, we
consolidate available knowledge about what SLT is precisely and why it is used
despite its considerable costs and complexities. We discuss the types or
failures covered by SLT, and outline approaches to quality assessment, test
generation and root-cause diagnosis in the context of SLT. Observing that the
theoretical understanding for all these questions has not yet reached the level
of maturity of the more conventional structural and functional test methods, we
outline new and promising directions for methodical developments leveraging on
recent findings from software engineering.
</summary>
    <author>
      <name>Ilia Polian</name>
    </author>
    <author>
      <name>Jens Anders</name>
    </author>
    <author>
      <name>Steffen Becker</name>
    </author>
    <author>
      <name>Paolo Bernardi</name>
    </author>
    <author>
      <name>Krishnendu Chakrabarty</name>
    </author>
    <author>
      <name>Nourhan ElHamawy</name>
    </author>
    <author>
      <name>Matthias Sauer</name>
    </author>
    <author>
      <name>Adit Singh</name>
    </author>
    <author>
      <name>Matteo Sonza Reorda</name>
    </author>
    <author>
      <name>Stefan Wagner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ATS49688.2020.9301557</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ATS49688.2020.9301557" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2020 IEEE 29th Asian Test Symposium (ATS)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2103.06656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.06656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.08392v2</id>
    <updated>2022-08-15T15:32:31Z</updated>
    <published>2021-03-15T13:59:36Z</published>
    <title>The SpiNNaker 2 Processing Element Architecture for Hybrid Digital
  Neuromorphic Computing</title>
    <summary>  This paper introduces the processing element architecture of the second
generation SpiNNaker chip, implemented in 22nm FDSOI. On circuit level, the
chip features adaptive body biasing for near-threshold operation, and dynamic
voltage-and-frequency scaling driven by spiking activity. On system level,
processing is centered around an ARM M4 core, similar to the processor-centric
architecture of the first generation SpiNNaker. To speed operation of subtasks,
we have added accelerators for numerical operations of both spiking (SNN) and
rate based (deep) neural networks (DNN). PEs communicate via a dedicated,
custom-designed network-on-chip. We present three benchmarks showing operation
of the whole processor element on SNN, DNN and hybrid SNN/DNN networks.
</summary>
    <author>
      <name>Sebastian Höppner</name>
    </author>
    <author>
      <name>Yexin Yan</name>
    </author>
    <author>
      <name>Andreas Dixius</name>
    </author>
    <author>
      <name>Stefan Scholze</name>
    </author>
    <author>
      <name>Johannes Partzsch</name>
    </author>
    <author>
      <name>Marco Stolba</name>
    </author>
    <author>
      <name>Florian Kelber</name>
    </author>
    <author>
      <name>Bernhard Vogginger</name>
    </author>
    <author>
      <name>Felix Neumärker</name>
    </author>
    <author>
      <name>Georg Ellguth</name>
    </author>
    <author>
      <name>Stephan Hartmann</name>
    </author>
    <author>
      <name>Stefan Schiefer</name>
    </author>
    <author>
      <name>Thomas Hocker</name>
    </author>
    <author>
      <name>Dennis Walter</name>
    </author>
    <author>
      <name>Genting Liu</name>
    </author>
    <author>
      <name>Jim Garside</name>
    </author>
    <author>
      <name>Steve Furber</name>
    </author>
    <author>
      <name>Christian Mayr</name>
    </author>
    <link href="http://arxiv.org/abs/2103.08392v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.08392v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.08910v1</id>
    <updated>2021-03-16T08:34:39Z</updated>
    <published>2021-03-16T08:34:39Z</published>
    <title>An ASIC Implementation and Evaluation of a Profiled Low-Energy
  Instruction Set Architecture Extension</title>
    <summary>  This paper presents an extension to an existing instruction set architecture,
which gains considerable reduction in power consumption. The reduction in power
consumption is achieved through coding of the most commonly executed
instructions in a short format done by the compiler based on a profile of
previous executions. This leads to fewer accesses to the instruction cache and
that more instructions can fit in the cache. As a secondary effect, this turned
out to be very beneficial in terms of power. Another major advantage, which is
the main concern of this paper is the reduction in the number of instruction
fetch cycles which will also contribute significantly towards reduction in
power consumption. The work involves implementing the new processor
architecture in ASIC and estimation of power-consumption compared to the normal
architecture.
</summary>
    <author>
      <name>Bobby Sleeba</name>
    </author>
    <author>
      <name>Mikael Collin</name>
    </author>
    <author>
      <name>Mats Brorsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.08910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.08910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.09301v1</id>
    <updated>2021-03-16T20:04:09Z</updated>
    <published>2021-03-16T20:04:09Z</published>
    <title>Softermax: Hardware/Software Co-Design of an Efficient Softmax for
  Transformers</title>
    <summary>  Transformers have transformed the field of natural language processing. This
performance is largely attributed to the use of stacked self-attention layers,
each of which consists of matrix multiplies as well as softmax operations. As a
result, unlike other neural networks, the softmax operation accounts for a
significant fraction of the total run-time of Transformers. To address this, we
propose Softermax, a hardware-friendly softmax design. Softermax consists of
base replacement, low-precision softmax computations, and an online
normalization calculation. We show Softermax results in 2.35x the energy
efficiency at 0.90x the size of a comparable baseline, with negligible impact
on network accuracy.
</summary>
    <author>
      <name>Jacob R. Stevens</name>
    </author>
    <author>
      <name>Rangharajan Venkatesan</name>
    </author>
    <author>
      <name>Steve Dai</name>
    </author>
    <author>
      <name>Brucek Khailany</name>
    </author>
    <author>
      <name>Anand Raghunathan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings of the 58th Design Automation Conference
  (DAC '21)</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.09301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.09301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.09791v2</id>
    <updated>2021-07-13T11:14:03Z</updated>
    <published>2021-03-17T17:24:46Z</published>
    <title>An Overflow/Underflow-Free Fixed-Point Bit-Width Optimization Method for
  OS-ELM Digital Circuit</title>
    <summary>  Currently there has been increasing demand for real-time training on
resource-limited IoT devices such as smart sensors, which realizes standalone
online adaptation for streaming data without data transfers to remote servers.
OS-ELM (Online Sequential Extreme Learning Machine) has been one of promising
neural-network-based online algorithms for on-chip learning because it can
perform online training at low computational cost and is easy to implement as a
digital circuit. Existing OS-ELM digital circuits employ fixed-point data
format and the bit-widths are often manually tuned, however, this may cause
overflow or underflow which can lead to unexpected behavior of the circuit. For
on-chip learning systems, an overflow/underflow-free design has a great impact
since online training is continuously performed and the intervals of
intermediate variables will dynamically change as time goes by. In this paper,
we propose an overflow/underflow-free bit-width optimization method for
fixed-point digital circuits of OS-ELM. Experimental results show that our
method realizes overflow/underflow-free OS-ELM digital circuits with 1.0x -
1.5x more area cost compared to the baseline simulation method where overflow
or underflow can happen.
</summary>
    <author>
      <name>Mineto Tsukada</name>
    </author>
    <author>
      <name>Hiroki Matsutani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1587/transfun.2021VLP0017</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1587/transfun.2021VLP0017" rel="related"/>
    <link href="http://arxiv.org/abs/2103.09791v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.09791v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.10040v1</id>
    <updated>2021-03-18T06:23:03Z</updated>
    <published>2021-03-18T06:23:03Z</published>
    <title>Solving Large Top-K Graph Eigenproblems with a Memory and
  Compute-optimized FPGA Design</title>
    <summary>  Large-scale eigenvalue computations on sparse matrices are a key component of
graph analytics techniques based on spectral methods. In such applications, an
exhaustive computation of all eigenvalues and eigenvectors is impractical and
unnecessary, as spectral methods can retrieve the relevant properties of
enormous graphs using just the eigenvectors associated with the Top-K largest
eigenvalues.
  In this work, we propose a hardware-optimized algorithm to approximate a
solution to the Top-K eigenproblem on sparse matrices representing large graph
topologies. We prototype our algorithm through a custom FPGA hardware design
that exploits HBM, Systolic Architectures, and mixed-precision arithmetic. We
achieve a speedup of 6.22x compared to the highly optimized ARPACK library
running on an 80-thread CPU, while keeping high accuracy and 49x better power
efficiency.
</summary>
    <author>
      <name>Francesco Sgherzi</name>
    </author>
    <author>
      <name>Alberto Parravicini</name>
    </author>
    <author>
      <name>Marco Siracusa</name>
    </author>
    <author>
      <name>Marco Domenico Santambrogio</name>
    </author>
    <link href="http://arxiv.org/abs/2103.10040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.10040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.10515v1</id>
    <updated>2021-03-18T20:49:28Z</updated>
    <published>2021-03-18T20:49:28Z</published>
    <title>Characterizing the Communication Requirements of GNN Accelerators: A
  Model-Based Approach</title>
    <summary>  Relational data present in real world graph representations demands for tools
capable to study it accurately. In this regard Graph Neural Network (GNN) is a
powerful tool, wherein various models for it have also been developed over the
past decade. Recently, there has been a significant push towards creating
accelerators that speed up the inference and training process of GNNs. These
accelerators, however, do not delve into the impact of their dataflows on the
overall data movement and, hence, on the communication requirements. In this
paper, we formulate analytical models that capture the amount of data movement
in the most recent GNN accelerator frameworks. Specifically, the proposed
models capture the dataflows and hardware setup of these accelerator designs
and expose their scalability characteristics for a set of hardware, GNN model
and input graph parameters. Additionally, the proposed approach provides means
for the comparative analysis of the vastly different GNN accelerators.
</summary>
    <author>
      <name>Robert Guirado</name>
    </author>
    <author>
      <name>Akshay Jain</name>
    </author>
    <author>
      <name>Sergi Abadal</name>
    </author>
    <author>
      <name>Eduard Alarcón</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISCAS 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.10515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.10515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.10836v1</id>
    <updated>2021-03-19T14:36:01Z</updated>
    <published>2021-03-19T14:36:01Z</published>
    <title>GNNerator: A Hardware/Software Framework for Accelerating Graph Neural
  Networks</title>
    <summary>  Graph Neural Networks (GNNs) use a fully-connected layer to extract features
from the nodes of a graph and aggregate these features using message passing
between nodes, combining two distinct computational patterns: dense, regular
computations and sparse, irregular computations.
  To address this challenge, we propose GNNerator, an accelerator with
heterogeneous compute engines optimized for these two patterns.
  Further, GNNerator implements feature-blocking, a novel GNN dataflow that
beneficially trades off irregular memory accesses during aggregation for
regular memory accesses during feature extraction. We show GNNerator achieves
speedups of 5.7-37x over an NVIDIA RTX 2080-Ti, and 2.3x-3.8x over HyGCN, a
state-of-the-art GNN accelerator.
</summary>
    <author>
      <name>Jacob R. Stevens</name>
    </author>
    <author>
      <name>Dipankar Das</name>
    </author>
    <author>
      <name>Sasikanth Avancha</name>
    </author>
    <author>
      <name>Bharat Kaul</name>
    </author>
    <author>
      <name>Anand Raghunathan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings of the 58th Design Automation Conference
  (DAC '21)</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.10836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.10836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.12166v2</id>
    <updated>2021-03-30T01:12:02Z</updated>
    <published>2021-03-22T20:24:45Z</published>
    <title>Special Session: Reliability Analysis for ML/AI Hardware</title>
    <summary>  Artificial intelligence (AI) and Machine Learning (ML) are becoming pervasive
in today's applications, such as autonomous vehicles, healthcare, aerospace,
cybersecurity, and many critical applications. Ensuring the reliability and
robustness of the underlying AI/ML hardware becomes our paramount importance.
In this paper, we explore and evaluate the reliability of different AI/ML
hardware. The first section outlines the reliability issues in a commercial
systolic array-based ML accelerator in the presence of faults engendering from
device-level non-idealities in the DRAM. Next, we quantified the impact of
circuit-level faults in the MSB and LSB logic cones of the Multiply and
Accumulate (MAC) block of the AI accelerator on the AI/ML accuracy. Finally, we
present two key reliability issues -- circuit aging and endurance in emerging
neuromorphic hardware platforms and present our system-level approach to
mitigate them.
</summary>
    <author>
      <name>Shamik Kundu</name>
    </author>
    <author>
      <name>Kanad Basu</name>
    </author>
    <author>
      <name>Mehdi Sadi</name>
    </author>
    <author>
      <name>Twisha Titirsha</name>
    </author>
    <author>
      <name>Shihao Song</name>
    </author>
    <author>
      <name>Anup Das</name>
    </author>
    <author>
      <name>Ujjwal Guin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at VLSI Test Symposium</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.12166v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.12166v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.13060v1</id>
    <updated>2021-03-24T10:16:45Z</updated>
    <published>2021-03-24T10:16:45Z</published>
    <title>De-specializing an HLS library for Deep Neural Networks: improvements
  upon hls4ml</title>
    <summary>  Custom hardware accelerators for Deep Neural Networks are increasingly
popular: in fact, the flexibility and performance offered by FPGAs are
well-suited to the computational effort and low latency constraints required by
many image recognition and natural language processing tasks. The gap between
high-level Machine Learning frameworks (e.g., Tensorflow, Pytorch) and
low-level hardware design in Verilog/VHDL creates a barrier to widespread
adoption of FPGAs, which can be overcome with the help of High-Level Synthesis.
hls4ml is a framework that translates Deep Neural Networks into annotated C++
code for High-Level Synthesis, offering a complete and user-friendly design
process that has been enthusiastically adopted in physics research. We analyze
the strengths and weaknesses of hls4ml, drafting a plan to enhance its core
library of components in order to allow more advanced optimizations, target a
wider selection of FPGAs, and support larger Neural Network models.
</summary>
    <author>
      <name>Serena Curzel</name>
    </author>
    <author>
      <name>Nicolò Ghielmetti</name>
    </author>
    <author>
      <name>Michele Fiorito</name>
    </author>
    <author>
      <name>Fabrizio Ferrandi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.13060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.13060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.14951v2</id>
    <updated>2021-08-16T16:24:21Z</updated>
    <published>2021-03-27T17:44:29Z</published>
    <title>A First Look at RISC-V Virtualization from an Embedded Systems
  Perspective</title>
    <summary>  This article describes the first public implementation and evaluation of the
latest version of the RISC-V hypervisor extension (H-extension v0.6.1)
specification in a Rocket chip core. To perform a meaningful evaluation for
modern multi-core embedded and mixedcriticality systems, we have ported Bao, an
open-source static partitioning hypervisor, to RISC-V. We have also extended
the RISC-V platformlevel interrupt controller (PLIC) to enable direct guest
interrupt injection with low and deterministic latency and we have enhanced the
timer infrastructure to avoid trap and emulation overheads. Experiments were
carried out in FireSim, a cycle-accurate, FPGA-accelerated simulator, and the
system was also successfully deployed and tested in a Zynq UltraScale+ MPSoC
ZCU104. Our hardware implementation was opensourced and is currently in use by
the RISC-V community towards the ratification of the H-extension specification.
</summary>
    <author>
      <name>Bruno Sá</name>
    </author>
    <author>
      <name>José Martins</name>
    </author>
    <author>
      <name>Sandro Pinto</name>
    </author>
    <link href="http://arxiv.org/abs/2103.14951v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.14951v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.15756v1</id>
    <updated>2021-02-19T06:16:42Z</updated>
    <published>2021-02-19T06:16:42Z</published>
    <title>GnetDet: Object Detection Optimized on a 224mW CNN Accelerator Chip at
  the Speed of 106FPS</title>
    <summary>  Object detection is widely used on embedded devices. With the wide
availability of CNN (Convolutional Neural Networks) accelerator chips, the
object detection applications are expected to run with low power consumption,
and high inference speed. In addition, the CPU load is expected to be as low as
possible for a CNN accelerator chip working as a co-processor with a host CPU.
In this paper, we optimize the object detection model on the CNN accelerator
chip by minimizing the CPU load. The resulting model is called GnetDet. The
experimental result shows that the GnetDet model running on a 224mW chip
achieves the speed of 106FPS with excellent accuracy.
</summary>
    <author>
      <name>Baohua Sun</name>
    </author>
    <author>
      <name>Tao Zhang</name>
    </author>
    <author>
      <name>Jiapeng Su</name>
    </author>
    <author>
      <name>Hao Sha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, and 1 table. arXiv admin note: text overlap with
  arXiv:2101.10444</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.15756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.15969v1</id>
    <updated>2021-03-29T21:52:58Z</updated>
    <published>2021-03-29T21:52:58Z</published>
    <title>eWake: A Novel Architecture for Semi-Active Wake-Up Radios Attaining
  Ultra-High Sensitivity at Extremely-Low Consumption</title>
    <summary>  In this work we propose a new scheme for semi-passive Wake-Up Receiver
circuits that exhibits remarkable sensitivity beyond -70 dBm, while
state-of-the-art receivers illustrate sensitivity of up to -55 dBm. The
receiver employs the typical principle of an envelope detector that harvests RF
energy from its antenna, while it employs a nano-power operation amplifier to
intensify the obtained signal prior to the final decoding that is realized with
the aid of a comparator circuit. It operates at the 868 MHz ISM band using OOK
signals propagated through LoRa transceivers, while also supporting addressing
capabilities in order to awake only the specified network's nodes. The power
expenditure of the developed receiver is as low as 580 nA, remaining at the
same power consumption levels as the state-of-the-art implementations.
</summary>
    <author>
      <name>Giannis Kazdaridis</name>
    </author>
    <author>
      <name>Nikos Sidiropoulos</name>
    </author>
    <author>
      <name>Ioannis Zografopoulos</name>
    </author>
    <author>
      <name>Thanasis Korakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures, 2 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.15969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.00192v1</id>
    <updated>2021-04-01T01:42:16Z</updated>
    <published>2021-04-01T01:42:16Z</published>
    <title>An Energy-Efficient Quad-Camera Visual System for Autonomous Machines on
  FPGA Platform</title>
    <summary>  In our past few years' of commercial deployment experiences, we identify
localization as a critical task in autonomous machine applications, and a great
acceleration target. In this paper, based on the observation that the visual
frontend is a major performance and energy consumption bottleneck, we present
our design and implementation of an energy-efficient hardware architecture for
ORB (Oriented-Fast and Rotated- BRIEF) based localization system on FPGAs. To
support our multi-sensor autonomous machine localization system, we present
hardware synchronization, frame-multiplexing, and parallelization techniques,
which are integrated in our design. Compared to Nvidia TX1 and Intel i7, our
FPGA-based implementation achieves 5.6x and 3.4x speedup, as well as 3.0x and
34.6x power reduction, respectively.
</summary>
    <author>
      <name>Zishen Wan</name>
    </author>
    <author>
      <name>Yuyang Zhang</name>
    </author>
    <author>
      <name>Arijit Raychowdhury</name>
    </author>
    <author>
      <name>Bo Yu</name>
    </author>
    <author>
      <name>Yanjun Zhang</name>
    </author>
    <author>
      <name>Shaoshan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEEE International Conference on Artificial Intelligence
  Circuits and Systems (AICAS), June 6-9, 2021, Virtual</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.00192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.00192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.02199v1</id>
    <updated>2021-04-06T00:34:06Z</updated>
    <published>2021-04-06T00:34:06Z</published>
    <title>Designing Efficient and High-performance AI Accelerators with Customized
  STT-MRAM</title>
    <summary>  In this paper, we demonstrate the design of efficient and high-performance
AI/Deep Learning accelerators with customized STT-MRAM and a reconfigurable
core. Based on model-driven detailed design space exploration, we present the
design methodology of an innovative scratchpad-assisted on-chip STT-MRAM based
buffer system for high-performance accelerators. Using analytically derived
expression of memory occupancy time of AI model weights and activation maps,
the volatility of STT-MRAM is adjusted with process and temperature variation
aware scaling of thermal stability factor to optimize the retention time,
energy, read/write latency, and area of STT-MRAM. From the analysis of modern
AI workloads and accelerator implementation in 14nm technology, we verify the
efficacy of our designed AI accelerator with STT-MRAM STT-AI. Compared to an
SRAM-based implementation, the STT-AI accelerator achieves 75% area and 3%
power savings at iso-accuracy. Furthermore, with a relaxed bit error rate and
negligible AI accuracy trade-off, the designed STT-AI Ultra accelerator
achieves 75.4%, and 3.5% savings in area and power, respectively over regular
SRAM-based accelerators.
</summary>
    <author>
      <name>Kaniz Mishty</name>
    </author>
    <author>
      <name>Mehdi Sadi</name>
    </author>
    <link href="http://arxiv.org/abs/2104.02199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.02199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.02676v1</id>
    <updated>2021-04-06T17:07:45Z</updated>
    <published>2021-04-06T17:07:45Z</published>
    <title>Building Beyond HLS: Graph Analysis and Others</title>
    <summary>  High-Level Synthesis has introduced reconfigurable logic to a new world --
that of software development. The newest wave of HLS tools has been successful,
and the future looks bright. But is HLS the end-all-be-all to FPGA
acceleration? Is it enough to allow non-experts to program FPGAs successfully,
even when dealing with troublesome data structures and complex control flows --
such as those often encountered in graph algorithms? We take a look at the
panorama of adoption of HLS by the software community, focusing on graph
analysis in particular in order to generalise to \textit{FPGA-unfriendly}
problems. We argue for the existence of shortcomings in current HLS development
flows which hinder adoption, and present our perspective on the path forward,
including how these issues may be remedied via higher-level tooling.
</summary>
    <author>
      <name>Pedro Filipe Silva</name>
    </author>
    <author>
      <name>João Bispo</name>
    </author>
    <author>
      <name>Nuno Paulino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 table. Accepted at LATTE '21, an ASPLOS workshop. Slightly
  differs from accepted version: includes some corrections and phrasing changes</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.02676v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.02676v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.03420v2</id>
    <updated>2021-04-19T21:58:12Z</updated>
    <published>2021-04-07T22:27:39Z</published>
    <title>On-FPGA Training with Ultra Memory Reduction: A Low-Precision Tensor
  Method</title>
    <summary>  Various hardware accelerators have been developed for energy-efficient and
real-time inference of neural networks on edge devices. However, most training
is done on high-performance GPUs or servers, and the huge memory and computing
costs prevent training neural networks on edge devices. This paper proposes a
novel tensor-based training framework, which offers orders-of-magnitude memory
reduction in the training process. We propose a novel rank-adaptive tensorized
neural network model, and design a hardware-friendly low-precision algorithm to
train this model. We present an FPGA accelerator to demonstrate the benefits of
this training method on edge devices. Our preliminary FPGA implementation
achieves $59\times$ speedup and $123\times$ energy reduction compared to
embedded CPU, and $292\times$ memory reduction over a standard full-size
training.
</summary>
    <author>
      <name>Kaiqi Zhang</name>
    </author>
    <author>
      <name>Cole Hawkins</name>
    </author>
    <author>
      <name>Xiyuan Zhang</name>
    </author>
    <author>
      <name>Cong Hao</name>
    </author>
    <author>
      <name>Zheng Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2104.03420v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.03420v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.03814v3</id>
    <updated>2022-11-02T23:41:07Z</updated>
    <published>2021-04-08T14:49:15Z</published>
    <title>Algorithmic Obfuscation for LDPC Decoders</title>
    <summary>  In order to protect intellectual property against untrusted foundry, many
logic-locking schemes have been developed. The main idea of logic locking is to
insert a key-controlled block into a circuit to make the circuit function
incorrectly without right keys. However, in the case that the algorithm
implemented by the circuit is naturally fault-tolerant or self-correcting,
existing logic-locking schemes do not affect the system performance much even
if wrong keys are used. One example is low-density parity-check (LDPC)
error-correcting decoder, which has broad applications in digital
communications and storage. This paper proposes two algorithmic-level
obfuscation methods for LDPC decoders. By modifying the decoding process and
locking the stopping criterion, our new designs substantially degrade the
decoder throughput and/or error-correcting performance when the wrong key is
used. Besides, our designs are also resistant to the SAT, AppSAT and removal
attacks. For an example LDPC decoder, our proposed methods reduce the
throughput to less than 1/3 and/or increase the decoder error rate by at least
two orders of magnitude with only 0.33% area overhead.
</summary>
    <author>
      <name>Jingbo Zhou</name>
    </author>
    <author>
      <name>Xinmiao Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCAD.2022.3175051</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCAD.2022.3175051" rel="related"/>
    <link href="http://arxiv.org/abs/2104.03814v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.03814v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04763v1</id>
    <updated>2021-04-10T13:36:19Z</updated>
    <published>2021-04-10T13:36:19Z</published>
    <title>Fixed-Posit: A Floating-Point Representation for Error-Resilient
  Applications</title>
    <summary>  Today, almost all computer systems use IEEE-754 floating point to represent
real numbers. Recently, posit was proposed as an alternative to IEEE-754
floating point as it has better accuracy and a larger dynamic range. The
configurable nature of posit, with varying number of regime and exponent bits,
has acted as a deterrent to its adoption. To overcome this shortcoming, we
propose fixed-posit representation where the number of regime and exponent bits
are fixed, and present the design of a fixed-posit multiplier. We evaluate the
fixed-posit multiplier on error-resilient applications of AxBench and OpenBLAS
benchmarks as well as neural networks. The proposed fixed-posit multiplier has
47%, 38.5%, 22% savings for power, area and delay respectively when compared to
posit multipliers and up to 70%, 66%, 26% savings in power, area and delay
respectively when compared to 32-bit IEEE-754 multiplier. These savings are
accompanied with minimal output quality loss (1.2% average relative error)
across OpenBLAS and AxBench workloads. Further, for neural networks like
ResNet-18 on ImageNet we observe a negligible accuracy loss (0.12%) on using
the fixed-posit multiplier.
</summary>
    <author>
      <name>Varun Gohil</name>
    </author>
    <author>
      <name>Sumit Walia</name>
    </author>
    <author>
      <name>Joycee Mekie</name>
    </author>
    <author>
      <name>Manu Awasthi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a pre-print for the paper version that has been accepted to
  TCAS-II, Express Briefs</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.04763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; C.0; B.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.07735v1</id>
    <updated>2021-04-15T19:30:08Z</updated>
    <published>2021-04-15T19:30:08Z</published>
    <title>Performance Analysis and Optimization Opportunities for NVIDIA
  Automotive GPUs</title>
    <summary>  Advanced Driver Assistance Systems (ADAS) and Autonomous Driving (AD) bring
unprecedented performance requirements for automotive systems. Graphic
Processing Unit (GPU) based platforms have been deployed with the aim of
meeting these requirements, being NVIDIA Jetson TX2 and its high-performance
successor, NVIDIA AGX Xavier, relevant representatives. However, to what extent
high-performance GPU configurations are appropriate for ADAS and AD workloads
remains as an open question.
  This paper analyzes this concern and provides valuable insights on this
question by modeling two recent automotive NVIDIA GPU-based platforms, namely
TX2 and AGX Xavier. In particular, our work assesses their microarchitectural
parameters against relevant benchmarks, identifying GPU setups delivering
increased performance within a similar cost envelope, or decreasing hardware
costs while preserving original performance levels. Overall, our analysis
identifies opportunities for the optimization of automotive GPUs to further
increase system efficiency.
</summary>
    <author>
      <name>Hamid Tabani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Barcelona Supercomputing Center</arxiv:affiliation>
    </author>
    <author>
      <name>Fabio Mazzocchetti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Barcelona Supercomputing Center</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universitat Politecnica de Catalunya</arxiv:affiliation>
    </author>
    <author>
      <name>Pedro Benedicte</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Barcelona Supercomputing Center</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universitat Politecnica de Catalunya</arxiv:affiliation>
    </author>
    <author>
      <name>Jaume Abella</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Barcelona Supercomputing Center</arxiv:affiliation>
    </author>
    <author>
      <name>Francisco J. Cazorla</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Barcelona Supercomputing Center</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jpdc.2021.02.008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jpdc.2021.02.008" rel="related"/>
    <link href="http://arxiv.org/abs/2104.07735v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.07735v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.07776v1</id>
    <updated>2021-03-31T14:53:53Z</updated>
    <published>2021-03-31T14:53:53Z</published>
    <title>Demystifying Memory Access Patterns of FPGA-Based Graph Processing
  Accelerators</title>
    <summary>  Recent advances in reprogrammable hardware (e.g., FPGAs) and memory
technology (e.g., DDR4, HBM) promise to solve performance problems inherent to
graph processing like irregular memory access patterns on traditional hardware
(e.g., CPU). While several of these graph accelerators were proposed in recent
years, it remains difficult to assess their performance and compare them on
common graph workloads and accelerator platforms, due to few open source
implementations and excessive implementation effort.
  In this work, we build on a simulation environment for graph processing
accelerators, to make several existing accelerator approaches comparable. This
allows us to study relevant performance dimensions such as partitioning schemes
and memory technology, among others. The evaluation yields insights into the
strengths and weaknesses of current graph processing accelerators along these
dimensions, and features a novel in-depth comparison.
</summary>
    <author>
      <name>Jonas Dann</name>
    </author>
    <author>
      <name>Daniel Ritter</name>
    </author>
    <author>
      <name>Holger Fröning</name>
    </author>
    <link href="http://arxiv.org/abs/2104.07776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.07776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.08333v1</id>
    <updated>2021-04-06T01:23:16Z</updated>
    <published>2021-04-06T01:23:16Z</published>
    <title>A survey on Dependable Digital Systems using FPGAs: Current Methods and
  Challenges</title>
    <summary>  Fault tolerance is increasingly being use to design Dependable Digital
Systems (DDS), which refers to the capability of a system to keep performing
its intended functions in existence of faults. DDS are typically used in
Safety-critical system (SCS) such as medical (I&amp;C) devices, Nuclear power
Plants (I&amp;C) devices and Aerospace (I&amp;C) systems, the failure in these systems
can cause harm to environment, death, injury to people. Different fault
tolerance techniques were developed to overcome these issues and that has led
to increase the reliability and dependability of applications on Field
Programmable Gate Arrays (FPGAs). In this paper, multiple related works are
present dealing with different types of faults and fault tolerance methods in
FPGA based systems. Furthermore, a comparison between the evaluation metrics of
previous works of Fault Tolerant (FT) techniques like hardware redundancy
overhead, time delay, reliability, and performance are also present.
</summary>
    <author>
      <name>Farah Natiq Kassab bashi</name>
    </author>
    <author>
      <name>Shawkat S Khairullah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advances in Computer and Electronics
  Engineering, Vol. 5, No. 12, pp. 1-8, December 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.08333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.08335v1</id>
    <updated>2021-04-14T01:06:49Z</updated>
    <published>2021-04-14T01:06:49Z</published>
    <title>Demystifying BERT: Implications for Accelerator Design</title>
    <summary>  Transfer learning in natural language processing (NLP), as realized using
models like BERT (Bi-directional Encoder Representation from Transformer), has
significantly improved language representation with models that can tackle
challenging language problems. Consequently, these applications are driving the
requirements of future systems. Thus, we focus on BERT, one of the most popular
NLP transfer learning algorithms, to identify how its algorithmic behavior can
guide future accelerator design. To this end, we carefully profile BERT
training and identify key algorithmic behaviors which are worthy of attention
in accelerator design.
  We observe that while computations which manifest as matrix multiplication
dominate BERT's overall runtime, as in many convolutional neural networks,
memory-intensive computations also feature prominently. We characterize these
computations, which have received little attention so far. Further, we also
identify heterogeneity in compute-intensive BERT computations and discuss
software and possible hardware mechanisms to further optimize these
computations. Finally, we discuss implications of these behaviors as networks
get larger and use distributed training environments, and how techniques such
as micro-batching and mixed-precision training scale. Overall, our analysis
identifies holistic solutions to optimize systems for BERT-like models.
</summary>
    <author>
      <name>Suchita Pati</name>
    </author>
    <author>
      <name>Shaizeen Aga</name>
    </author>
    <author>
      <name>Nuwan Jayasena</name>
    </author>
    <author>
      <name>Matthew D. Sinclair</name>
    </author>
    <link href="http://arxiv.org/abs/2104.08335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3; C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.09502v4</id>
    <updated>2021-11-29T17:22:15Z</updated>
    <published>2021-04-18T00:04:49Z</published>
    <title>CodeAPeel: An Integrated and Layered Learning Technology For Computer
  Architecture Courses</title>
    <summary>  This paper introduces a versatile, multi-layered technology to help support
teaching and learning core computer architecture concepts. This technology,
called CodeAPeel is already implemented in one particular form to describe
instruction processing in compiler, assembly, and machine layers of a generic
instruction set architecture by a comprehensive simulation of its
fetch-decode-execute cycle as well as animation of the behavior of its CPU
registers, RAM, VRAM, STACK memories, various control registers, and graphics
screen. Unlike most educational CPU simulators that simulate a real processor
such as MIPS or RISC-V, CodeAPeel is designed and implemented as a generic RISC
instruction set architecture simulator with both scalar and vector instructions
to provide a dual-mode processor simulator as described by Flynn's
classification of SISD and SIMD processors. Vectorization of operations is
built into the instruction repertoire of CodeAPeel, making it straightforward
to simulate such processors with powerful vector instructions.
</summary>
    <author>
      <name>A. Yavuz Oruc</name>
    </author>
    <author>
      <name>A. Atmaca</name>
    </author>
    <author>
      <name>Y. Nevzat Sengun</name>
    </author>
    <author>
      <name>A. Semi Yenimol</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Minor revision and some typos are fixed</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.09502v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.09502v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.09768v1</id>
    <updated>2021-04-20T05:41:01Z</updated>
    <published>2021-04-20T05:41:01Z</published>
    <title>SME: A High Productivity FPGA Tool for Software Programmers</title>
    <summary>  For several decades, the CPU has been the standard model to use in the
majority of computing. While the CPU does excel in some areas, heterogeneous
computing, such as reconfigurable hardware, is showing increasing potential in
areas like parallelization, performance, and power usage. This is especially
prominent in problems favoring deep pipelining or tight latency requirements.
However, due to the nature of these problems, they can be hard to program, at
least for software developers. Synchronous Message Exchange (SME) is a runtime
environment that allows development, testing and verification of hardware
designs for FPGA devices in C#, with access to modern debugging and code
features. The goal is to create a framework for software developers to easily
implement systems for FPGA devices without having to obtain heavy hardware
programming knowledge. This article presents a short introduction to the SME
model as well as new updates to SME. Lastly, a selection of student projects
and examples will be presented in order to show how it is possible to create
quite complex structures in SME, even by students with no hardware experience.
</summary>
    <author>
      <name>Carl-Johannes Johnsen</name>
    </author>
    <author>
      <name>Alberte Thegler</name>
    </author>
    <author>
      <name>Kenneth Skovhede</name>
    </author>
    <author>
      <name>Brian Vinter</name>
    </author>
    <link href="http://arxiv.org/abs/2104.09768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.09768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.10941v1</id>
    <updated>2021-04-22T09:13:57Z</updated>
    <published>2021-04-22T09:13:57Z</published>
    <title>Enabling Cross-Layer Reliability and Functional Safety Assessment
  Through ML-Based Compact Models</title>
    <summary>  Typical design flows are hierarchical and rely on assembling many individual
technology elements from standard cells to complete boards. Providers use
compact models to provide simplified views of their products to their users.
Designers group simpler elements in more complex structures and have to manage
the corresponding propagation of reliability and functional safety information
through the hierarchy of the system, accompanied by the obvious problems of IP
confidentiality, possibility of reverse engineering and so on. This paper
proposes a machine-learning-based approach to integrate the many individual
models of a subsystem's elements in a single compact model that can be re-used
and assembled further up in the hierarchy. The compact models provide
consistency, accuracy and confidentiality, allowing technology, IP, component,
sub-system or system providers to accompany their offering with high-quality
reliability and functional safety compact models that can be safely and
accurately consumed by their users.
</summary>
    <author>
      <name>Dan Alexandrescu</name>
    </author>
    <author>
      <name>Aneesh Balakrishnan</name>
    </author>
    <author>
      <name>Thomas Lange</name>
    </author>
    <author>
      <name>Maximilien Glorieux</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IOLTS50870.2020.9159750</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IOLTS50870.2020.9159750" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages (paper) + 1 page copyright statement, Number of figures: 3,
  Conference: 2020 IEEE 26th International Symposium on On-Line Testing and
  Robust System Design (IOLTS)</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.10941v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.10941v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.11678v1</id>
    <updated>2021-04-23T16:14:40Z</updated>
    <published>2021-04-23T16:14:40Z</published>
    <title>A Case for Fine-grain Coherence Specialization in Heterogeneous Systems</title>
    <summary>  Hardware specialization is becoming a key enabler of energyefficient
performance. Future systems will be increasingly heterogeneous, integrating
multiple specialized and programmable accelerators, each with different memory
demands. Traditionally, communication between accelerators has been
inefficient, typically orchestrated through explicit DMA transfers between
different address spaces. More recently, industry has proposed unified coherent
memory which enables implicit data movement and more data reuse, but often
these interfaces limit the coherence flexibility available to heterogeneous
systems. This paper demonstrates the benefits of fine-grained coherence
specialization for heterogeneous systems. We propose an architecture that
enables low-complexity independent specialization of each individual coherence
request in heterogeneous workloads by building upon a simple and flexible
baseline coherence interface, Spandex. We then describe how to optimize
individual memory requests to improve cache reuse and performance-critical
memory latency in emerging heterogeneous workloads. Collectively, our
techniques enable significant gains, reducing execution time by up to 61% or
network traffic by up to 99% while adding minimal complexity to the Spandex
protocol.
</summary>
    <author>
      <name>Johnathan Alsop</name>
    </author>
    <author>
      <name>Weon Taek Na</name>
    </author>
    <author>
      <name>Matthew D. Sinclair</name>
    </author>
    <author>
      <name>Samuel Grayson</name>
    </author>
    <author>
      <name>Sarita V. Adve</name>
    </author>
    <link href="http://arxiv.org/abs/2104.11678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.11678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12760v2</id>
    <updated>2021-09-23T01:10:59Z</updated>
    <published>2021-04-26T17:52:23Z</published>
    <title>Capstan: A Vector RDA for Sparsity</title>
    <summary>  This paper proposes Capstan: a scalable, parallel-patterns-based,
reconfigurable dataflow accelerator (RDA) for sparse and dense tensor
applications. Instead of designing for one application, we start with common
sparse data formats, each of which supports multiple applications. Using a
declarative programming model, Capstan supports application-independent sparse
iteration and memory primitives that can be mapped to vectorized,
high-performance hardware. We optimize random-access sparse memories with
configurable out-of-order execution to increase SRAM random-access throughput
from 32% to 80%.
  For a variety of sparse applications, Capstan with DDR4 memory is 18x faster
than a multi-core CPU baseline, while Capstan with HBM2 memory is 16x faster
than an Nvidia V100 GPU. For sparse applications that can be mapped to
Plasticine, a recent dense RDA, Capstan is 7.6x to 365x faster and only 16%
larger.
</summary>
    <author>
      <name>Alexander Rucker</name>
    </author>
    <author>
      <name>Matthew Vilim</name>
    </author>
    <author>
      <name>Tian Zhao</name>
    </author>
    <author>
      <name>Yaqi Zhang</name>
    </author>
    <author>
      <name>Raghu Prabhakar</name>
    </author>
    <author>
      <name>Kunle Olukotun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3466752.3480047</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3466752.3480047" rel="related"/>
    <link href="http://arxiv.org/abs/2104.12760v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12760v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.13997v3</id>
    <updated>2022-01-26T22:53:14Z</updated>
    <published>2021-04-28T19:57:55Z</published>
    <title>MAGMA: An Optimization Framework for Mapping Multiple DNNs on Multiple
  Accelerator Cores</title>
    <summary>  As Deep Learning continues to drive a variety of applications in edge and
cloud data centers, there is a growing trend towards building large
accelerators with several sub-accelerator cores/chiplets. This work looks at
the problem of supporting multi-tenancy on such accelerators. In particular, we
focus on the problem of mapping jobs from several DNNs simultaneously on an
accelerator. Given the extremely large search space, we formulate the search as
an optimization problem and develop an optimization framework called M3E. In
addition, we develop a specialized optimization algorithm called MAGMA with
custom operators to enable structured sample-efficient exploration. We
quantitatively compare MAGMA with several state-of-the-art methods, black-box
optimization, and reinforcement learning methods across different accelerator
settings (large/small accelerators) and different sub-accelerator
configurations (homogeneous/heterogeneous), and observe MAGMA can consistently
find better mappings.
</summary>
    <author>
      <name>Sheng-Chun Kao</name>
    </author>
    <author>
      <name>Tushar Krishna</name>
    </author>
    <link href="http://arxiv.org/abs/2104.13997v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.13997v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.04151v1</id>
    <updated>2021-05-10T07:14:43Z</updated>
    <published>2021-05-10T07:14:43Z</published>
    <title>Skew-Oblivious Data Routing for Data-Intensive Applications on FPGAs
  with HLS</title>
    <summary>  FPGAs have become emerging computing infrastructures for accelerating
applications in datacenters. Meanwhile, high-level synthesis (HLS) tools have
been proposed to ease the programming of FPGAs. Even with HLS, irregular
data-intensive applications require explicit optimizations, among which
multiple processing elements (PEs) with each owning a private BRAM-based buffer
are usually adopted to process multiple data per cycle. Data routing, which
dynamically dispatches multiple data to designated PEs, avoids data replication
in buffers compared to statically assigning data to PEs, hence saving BRAM
usage. However, the workload imbalance among PEs vastly diminishes performance
when processing skew datasets. In this paper, we propose a skew-oblivious data
routing architecture that allocates secondary PEs and schedules them to share
the workload of the overloaded PEs at run-time. In addition, we integrate the
proposed architecture into a framework called Ditto to minimize the development
efforts for applications that require skew handling. We evaluate Ditto on five
commonly used applications: histogram building, data partitioning, pagerank,
heavy hitter detection and hyperloglog. The results demonstrate that the
generated implementations are robust to skew datasets and outperform the
stateof-the-art designs in both throughput and BRAM usage efficiency.
</summary>
    <author>
      <name>Xinyu Chen</name>
    </author>
    <author>
      <name>Hongshi Tan</name>
    </author>
    <author>
      <name>Yao Chen</name>
    </author>
    <author>
      <name>Bingsheng He</name>
    </author>
    <author>
      <name>Weng-Fai Wong</name>
    </author>
    <author>
      <name>Deming Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2105.04151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.04151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.04212v1</id>
    <updated>2021-05-10T09:13:07Z</updated>
    <published>2021-05-10T09:13:07Z</published>
    <title>Efficient Error-Correcting-Code Mechanism for High-Throughput Memristive
  Processing-in-Memory</title>
    <summary>  Inefficient data transfer between computation and memory inspired emerging
processing-in-memory (PIM) technologies. Many PIM solutions enable storage and
processing using memristors in a crossbar-array structure, with techniques such
as memristor-aided logic (MAGIC) used for computation. This approach provides
highly-paralleled logic computation with minimal data movement. However,
memristors are vulnerable to soft errors and standard error-correcting-code
(ECC) techniques are difficult to implement without moving data outside the
memory. We propose a novel technique for efficient ECC implementation along
diagonals to support reliable computation inside the memory without explicitly
reading the data. Our evaluation demonstrates an improvement of over eight
orders of magnitude in reliability (mean time to failure) for an increase of
about 26% in computation latency.
</summary>
    <author>
      <name>Orian Leitersdorf</name>
    </author>
    <author>
      <name>Ben Perach</name>
    </author>
    <author>
      <name>Ronny Ronen</name>
    </author>
    <author>
      <name>Shahar Kvatinsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to 58th Design Automation Conference (DAC) 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.04212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.04212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.05588v2</id>
    <updated>2021-05-25T12:11:32Z</updated>
    <published>2021-05-12T11:06:37Z</published>
    <title>On the Approximation of Accuracy-configurable Sequential Multipliers via
  Segmented Carry Chains</title>
    <summary>  In this paper, we present a multiplier based on a sequence of approximated
accumulations. According to a given splitting point of the carry chains, the
technique herein introduced allows varying the quality of the accumulations
and, consequently, the overall product. Our approximate multiplier trades-off
accuracy for a reduced latency (with respect to an accurate sequential
multiplier) and exploits the inherent area savings of sequential over
combinatorial approaches. We implemented multiple versions with different
bit-width and accuracy configurations, targeting an FPGA and a 45nm ASIC to
estimate resources, power consumption, and latency. We also present two error
analyses of the proposed design based on closed-form analysis and simulations.
</summary>
    <author>
      <name>Jorge Echavarria</name>
    </author>
    <author>
      <name>Stefan Wildermann</name>
    </author>
    <author>
      <name>Oliver Keszocze</name>
    </author>
    <author>
      <name>Faramarz Khosravi</name>
    </author>
    <author>
      <name>Andreas Becher</name>
    </author>
    <author>
      <name>Jürgen Teich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.05588v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.05588v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.05821v3</id>
    <updated>2022-04-05T18:40:42Z</updated>
    <published>2021-05-12T17:31:52Z</published>
    <title>SimNet: Accurate and High-Performance Computer Architecture Simulation
  using Deep Learning</title>
    <summary>  While discrete-event simulators are essential tools for architecture
research, design, and development, their practicality is limited by an
extremely long time-to-solution for realistic applications under investigation.
This work describes a concerted effort, where machine learning (ML) is used to
accelerate discrete-event simulation. First, an ML-based instruction latency
prediction framework that accounts for both static instruction properties and
dynamic processor states is constructed. Then, a GPU-accelerated parallel
simulator is implemented based on the proposed instruction latency predictor,
and its simulation accuracy and throughput are validated and evaluated against
a state-of-the-art simulator. Leveraging modern GPUs, the ML-based simulator
outperforms traditional simulators significantly.
</summary>
    <author>
      <name>Lingda Li</name>
    </author>
    <author>
      <name>Santosh Pandey</name>
    </author>
    <author>
      <name>Thomas Flynn</name>
    </author>
    <author>
      <name>Hang Liu</name>
    </author>
    <author>
      <name>Noel Wheeler</name>
    </author>
    <author>
      <name>Adolfy Hoisie</name>
    </author>
    <link href="http://arxiv.org/abs/2105.05821v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.05821v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.06594v1</id>
    <updated>2021-05-13T23:32:59Z</updated>
    <published>2021-05-13T23:32:59Z</published>
    <title>Combining Emulation and Simulation to Evaluate a Near Memory Key/Value
  Lookup Accelerator</title>
    <summary>  Processing large numbers of key/value lookups is an integral part of modern
server databases and other "Big Data" applications. Prior work has shown that
hash table based key/value lookups can benefit significantly from using a
dedicated hardware lookup accelerator placed near memory. However, previous
evaluations of this design on the Logic in Memory Emulator (LiME) were limited
by the capabilities of the hardware on which it was emulated, which only
supports a single CPU core and a single near-memory lookup engine. We extend
the emulation results by incorporating simulation to evaluate this design in
additional scenarios. By incorporating an HMC simulation model, we design
optimizations that better mitigate the effects of the HMC closed page policy
and that better utilize the HMC's parallelism, improving predicted performance
by an order of magnitude. Additionally, we use simulation to evaluate the
scaling performance of multiple near-memory lookup accelerators. Our work
employs an open source emulator LiME, open source simulatation infrastructure
SST, and the open source HMC-Sim simulator.
</summary>
    <author>
      <name>Joshua Landgraf</name>
    </author>
    <author>
      <name>Scott Lloyd</name>
    </author>
    <author>
      <name>Maya Gokhale</name>
    </author>
    <link href="http://arxiv.org/abs/2105.06594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.06594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.07784v1</id>
    <updated>2021-05-17T12:50:17Z</updated>
    <published>2021-05-17T12:50:17Z</published>
    <title>Multi-output, multi-level, multi-gate design using non-linear
  programming</title>
    <summary>  Using logic gates is the traditional way of designing logic circuits.
However, most of the minimization algorithms concern a limited set of gates
(complete sets), like sum of products, exclusive-or sum of products, NAND
gates, NOR gates e.t.c.. In this paper, a method is proposed for minimizing
multi-output Boolean functions using any kind of two-input gates although it
can easily be extended to multi-input gates. The method is based on non-linear
mixed integer programming. The experimental results show that the method gives
the same or better results compared to other methods available in the
literature. However, other methods do not ensure that they produce the minimal
solution, while the main advantages of the proposed method are that it does
guarantee minimality and it can also handle Boolean functions for incompletely
specified functions. The method is general enough and can easily be extended to
more complicated design modules than just basic gates.
</summary>
    <author>
      <name>A. C. Dimopoulos</name>
    </author>
    <author>
      <name>C. Pavlatos</name>
    </author>
    <author>
      <name>G. Papakonstantinou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figues</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.07784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.07784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.08784v1</id>
    <updated>2021-05-18T19:08:16Z</updated>
    <published>2021-05-18T19:08:16Z</published>
    <title>A New, Computationally Efficient "Blech Criterion" for Immortality in
  General Interconnects</title>
    <summary>  Traditional methodologies for analyzing electromigration (EM) in VLSI
circuits first filter immortal wires using Blech's criterion, and then perform
detailed EM analysis on the remaining wires. However, Blech's criterion was
designed for two-terminal wires and does not extend to general structures. This
paper demonstrates a first-principles-based solution technique for determining
the steady-state stress at all the nodes of a general interconnect structure,
and develops an immortality test whose complexity is linear in the number of
edges of an interconnect structure. The proposed model is applied to a variety
of structures. The method is shown to match well with results from numerical
solvers, to be scalable to large structures.
</summary>
    <author>
      <name>Mohammad Abdullah Al Shohel</name>
    </author>
    <author>
      <name>Vidya A. Chhabria</name>
    </author>
    <author>
      <name>Sachin S. Sapatnekar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in the Proceedings of the ACM/IEEE Design
  Automation Conference, 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.08784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.08784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.09163v3</id>
    <updated>2021-11-30T21:18:15Z</updated>
    <published>2021-05-12T06:20:44Z</published>
    <title>High-Performance FPGA-based Accelerator for Bayesian Neural Networks</title>
    <summary>  Neural networks (NNs) have demonstrated their potential in a wide range of
applications such as image recognition, decision making or recommendation
systems. However, standard NNs are unable to capture their model uncertainty
which is crucial for many safety-critical applications including healthcare and
autonomous vehicles. In comparison, Bayesian neural networks (BNNs) are able to
express uncertainty in their prediction via a mathematical grounding.
Nevertheless, BNNs have not been as widely used in industrial practice, mainly
because of their expensive computational cost and limited hardware performance.
This work proposes a novel FPGA-based hardware architecture to accelerate BNNs
inferred through Monte Carlo Dropout. Compared with other state-of-the-art BNN
accelerators, the proposed accelerator can achieve up to 4 times higher energy
efficiency and 9 times better compute efficiency. Considering partial Bayesian
inference, an automatic framework is proposed, which explores the trade-off
between hardware and algorithmic performance. Extensive experiments are
conducted to demonstrate that our proposed framework can effectively find the
optimal points in the design space.
</summary>
    <author>
      <name>Hongxiang Fan</name>
    </author>
    <author>
      <name>Martin Ferianc</name>
    </author>
    <author>
      <name>Miguel Rodrigues</name>
    </author>
    <author>
      <name>Hongyu Zhou</name>
    </author>
    <author>
      <name>Xinyu Niu</name>
    </author>
    <author>
      <name>Wayne Luk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Design Automation Conference (DAC) 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.09163v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.09163v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.09282v1</id>
    <updated>2021-04-14T05:14:52Z</updated>
    <published>2021-04-14T05:14:52Z</published>
    <title>Learning Pareto-Frontier Resource Management Policies for Heterogeneous
  SoCs: An Information-Theoretic Approach</title>
    <summary>  Mobile system-on-chips (SoCs) are growing in their complexity and
heterogeneity (e.g., Arm's Big-Little architecture) to meet the needs of
emerging applications, including games and artificial intelligence. This makes
it very challenging to optimally manage the resources (e.g., controlling the
number and frequency of different types of cores) at runtime to meet the
desired trade-offs among multiple objectives such as performance and energy.
This paper proposes a novel information-theoretic framework referred to as
PaRMIS to create Pareto-optimal resource management policies for given target
applications and design objectives. PaRMIS specifies parametric policies to
manage resources and learns statistical models from candidate policy evaluation
data in the form of target design objective values. The key idea is to select a
candidate policy for evaluation in each iteration guided by statistical models
that maximize the information gain about the true Pareto front. Experiments on
a commercial heterogeneous SoC show that PaRMIS achieves better Pareto fronts
and is easily usable to optimize complex objectives (e.g., performance per
Watt) when compared to prior methods.
</summary>
    <author>
      <name>Aryan Deshwal</name>
    </author>
    <author>
      <name>Syrine Belakaria</name>
    </author>
    <author>
      <name>Ganapati Bhat</name>
    </author>
    <author>
      <name>Janardhan Rao Doppa</name>
    </author>
    <author>
      <name>Partha Pratim Pande</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in proceedings DAC</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.09282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.09282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.09564v1</id>
    <updated>2021-05-20T07:36:16Z</updated>
    <published>2021-05-20T07:36:16Z</published>
    <title>Dual-side Sparse Tensor Core</title>
    <summary>  Leveraging sparsity in deep neural network (DNN) models is promising for
accelerating model inference. Yet existing GPUs can only leverage the sparsity
from weights but not activations, which are dynamic, unpredictable, and hence
challenging to exploit. In this work, we propose a novel architecture to
efficiently harness the dual-side sparsity (i.e., weight and activation
sparsity). We take a systematic approach to understand the (dis)advantages of
previous sparsity-related architectures and propose a novel, unexplored
paradigm that combines outer-product computation primitive and bitmap-based
encoding format. We demonstrate the feasibility of our design with minimal
changes to the existing production-scale inner-product-based Tensor Core. We
propose a set of novel ISA extensions and co-design the matrix-matrix
multiplication and convolution algorithms, which are the two dominant
computation patterns in today's DNN models, to exploit our new dual-side sparse
Tensor Core. Our evaluation shows that our design can fully unleash the
dual-side DNN sparsity and improve the performance by up to one order of
magnitude with \hl{small} hardware overhead.
</summary>
    <author>
      <name>Yang Wang</name>
    </author>
    <author>
      <name>Chen Zhang</name>
    </author>
    <author>
      <name>Zhiqiang Xie</name>
    </author>
    <author>
      <name>Cong Guo</name>
    </author>
    <author>
      <name>Yunxin Liu</name>
    </author>
    <author>
      <name>Jingwen Leng</name>
    </author>
    <link href="http://arxiv.org/abs/2105.09564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.09564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.09696v1</id>
    <updated>2021-05-20T12:17:49Z</updated>
    <published>2021-05-20T12:17:49Z</published>
    <title>A Terabit Hybrid FPGA-ASIC Platform for Switch Virtualization</title>
    <summary>  The roll-out of technologies like 5G and the need for multi-terabit bandwidth
in backbone networks requires networking companies to make significant
investments to keep up with growing service demands. For lower capital
expenditure and faster time-to-market, companies can resort to
anything-as-a-service providers to lease virtual resources. Nevertheless,
existing virtualization technologies are still lagging behind next-generation
networks' requirements. This paper breaks the terabit barrier by introducing a
hybrid FPGA-ASIC architecture to virtualize programmable forwarding planes. In
contrast to existing solutions, our architecture involves an ASIC that
multiplexes network flows between programmable virtual switches running in an
FPGA capable of full and partial reconfiguration, enabling virtual switch
hot-swapping. Our evaluation shows the feasibility of a switch virtualization
architecture capable of achieving a combined throughput of 3.2 Tbps by having
up to 26 virtual switch instances in parallel with low resource occupation
overhead.
</summary>
    <author>
      <name>Mateus Saquetti</name>
    </author>
    <author>
      <name>Raphael M. Brum</name>
    </author>
    <author>
      <name>Bruno Zatt</name>
    </author>
    <author>
      <name>Samuel Pagliarini</name>
    </author>
    <author>
      <name>Weverton Cordeiro</name>
    </author>
    <author>
      <name>Jose R. Azambuja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISVLSI</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.09696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.09696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.12781v1</id>
    <updated>2021-05-26T18:36:01Z</updated>
    <published>2021-05-26T18:36:01Z</published>
    <title>ATRIA: A Bit-Parallel Stochastic Arithmetic Based Accelerator for
  In-DRAM CNN Processing</title>
    <summary>  With the rapidly growing use of Convolutional Neural Networks (CNNs) in
real-world applications related to machine learning and Artificial Intelligence
(AI), several hardware accelerator designs for CNN inference and training have
been proposed recently. In this paper, we present ATRIA, a novel bit-pArallel
sTochastic aRithmetic based In-DRAM Accelerator for energy-efficient and
high-speed inference of CNNs. ATRIA employs light-weight modifications in DRAM
cell arrays to implement bit-parallel stochastic arithmetic based acceleration
of multiply-accumulate (MAC) operations inside DRAM. ATRIA significantly
improves the latency, throughput, and efficiency of processing CNN inferences
by performing 16 MAC operations in only five consecutive memory operation
cycles. We mapped the inference tasks of four benchmark CNNs on ATRIA to
compare its performance with five state-of-the-art in-DRAM CNN accelerators
from prior work. The results of our analysis show that ATRIA exhibits only 3.5%
drop in CNN inference accuracy and still achieves improvements of up to 3.2x in
frames-per-second (FPS) and up to 10x in efficiency (FPS/W/mm2), compared to
the best-performing in-DRAM accelerator from prior work.
</summary>
    <author>
      <name>Supreeth Mysore Shivanandamurthy</name>
    </author>
    <author>
      <name>Ishan. G. Thakkar</name>
    </author>
    <author>
      <name>Sayed Ahmad Salehi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint accepted in ISVLSI 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.12781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.12781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.13262v2</id>
    <updated>2021-06-02T21:51:41Z</updated>
    <published>2021-05-27T15:59:54Z</published>
    <title>A Microarchitecture Implementation Framework for Online Learning with
  Temporal Neural Networks</title>
    <summary>  Temporal Neural Networks (TNNs) are spiking neural networks that use time as
a resource to represent and process information, similar to the mammalian
neocortex. In contrast to compute-intensive deep neural networks that employ
separate training and inference phases, TNNs are capable of extremely efficient
online incremental/continual learning and are excellent candidates for building
edge-native sensory processing units. This work proposes a microarchitecture
framework for implementing TNNs using standard CMOS. Gate-level implementations
of three key building blocks are presented: 1) multi-synapse neurons, 2)
multi-neuron columns, and 3) unsupervised and supervised online learning
algorithms based on Spike Timing Dependent Plasticity (STDP). The proposed
microarchitecture is embodied in a set of characteristic scaling equations for
assessing the gate count, area, delay and power for any TNN design.
Post-synthesis results (in 45nm CMOS) for the proposed designs are presented,
and their online incremental learning capability is demonstrated.
</summary>
    <author>
      <name>Harideep Nair</name>
    </author>
    <author>
      <name>John Paul Shen</name>
    </author>
    <author>
      <name>James E. Smith</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISVLSI51109.2021.00056</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISVLSI51109.2021.00056" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in ISVLSI 2021. arXiv admin note: substantial text
  overlap with arXiv:2009.00457</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2021 IEEE Computer Society Annual Symposium on VLSI (ISVLSI),
  2021, pp. 266-271</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2105.13262v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.13262v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.13434v1</id>
    <updated>2021-05-27T20:19:39Z</updated>
    <published>2021-05-27T20:19:39Z</published>
    <title>FuSeConv: Fully Separable Convolutions for Fast Inference on Systolic
  Arrays</title>
    <summary>  Both efficient neural networks and hardware accelerators are being explored
to speed up DNN inference on edge devices. For example, MobileNet uses
depthwise separable convolution to achieve much lower latency, while systolic
arrays provide much higher performance per watt. Interestingly however, the
combination of these two ideas is inefficient: The computational patterns of
depth-wise separable convolution are not systolic and lack data reuse to
saturate the systolic array's constrained dataflow. In this paper, we propose
FuSeConv (Fully-Separable Convolution) as a drop-in replacement for depth-wise
separable convolution. FuSeConv generalizes the decomposition of convolutions
fully to separable 1D convolutions along spatial and depth dimensions. The
resultant computation is systolic and efficiently utilizes the systolic array
with a slightly modified dataflow. With FuSeConv, we achieve a significant
speed-up of 3x-7x with the MobileNet family of networks on a systolic array of
size 64x64, with comparable accuracy on the ImageNet dataset. The high speed-up
motivates exploration of hardware-aware Neural Operator Search (NOS) in
complement to ongoing efforts on Neural Architecture Search (NAS).
</summary>
    <author>
      <name>Surya Selvam</name>
    </author>
    <author>
      <name>Vinod Ganesan</name>
    </author>
    <author>
      <name>Pratyush Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Proceedings of the Design, Automation &amp; Test in
  Europe (DATE), 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.13434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.13434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.13904v1</id>
    <updated>2021-05-24T23:01:36Z</updated>
    <published>2021-05-24T23:01:36Z</published>
    <title>An In-Memory Analog Computing Co-Processor for Energy-Efficient CNN
  Inference on Mobile Devices</title>
    <summary>  In this paper, we develop an in-memory analog computing (IMAC) architecture
realizing both synaptic behavior and activation functions within non-volatile
memory arrays. Spin-orbit torque magnetoresistive random-access memory
(SOT-MRAM) devices are leveraged to realize sigmoidal neurons as well as
binarized synapses. First, it is shown the proposed IMAC architecture can be
utilized to realize a multilayer perceptron (MLP) classifier achieving orders
of magnitude performance improvement compared to previous mixed-signal and
digital implementations. Next, a heterogeneous mixed-signal and mixed-precision
CPU-IMAC architecture is proposed for convolutional neural networks (CNNs)
inference on mobile processors, in which IMAC is designed as a co-processor to
realize fully-connected (FC) layers whereas convolution layers are executed in
CPU. Architecture-level analytical models are developed to evaluate the
performance and energy consumption of the CPU-IMAC architecture. Simulation
results exhibit 6.5% and 10% energy savings for CPU-IMAC based realizations of
LeNet and VGG CNN models, for MNIST and CIFAR-10 pattern recognition tasks,
respectively.
</summary>
    <author>
      <name>Mohammed Elbtity</name>
    </author>
    <author>
      <name>Abhishek Singh</name>
    </author>
    <author>
      <name>Brendan Reidy</name>
    </author>
    <author>
      <name>Xiaochen Guo</name>
    </author>
    <author>
      <name>Ramtin Zand</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISVLSI51109.2021.00043</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISVLSI51109.2021.00043" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:2012.02695, arXiv:2006.01238</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.13904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.13904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.14401v1</id>
    <updated>2021-05-30T01:12:12Z</updated>
    <published>2021-05-30T01:12:12Z</published>
    <title>A Tapered Floating Point Extension for the Redundant Signed Radix 2
  System Using the Canonical Recoding</title>
    <summary>  A tapered floating point encoding is proposed which uses the redundant signed
radix 2 system and is based on the canonical recoding. By making use of ternary
technology, the encoding has a dynamic range exceeding that of the
recently-proposed Posit number system and the IEEE 754-1985 Standard for
Floating Point Arithmetic (IEEE-754-1985), and precision equal to or better
than that of the IEEE-754-1985 system and the recently proposed Posit system
when equal input sizes are compared. In addition, the encoding is capable of
supporting several proposed extensions, including extensions to integers,
boolean values, complex numbers, higher number systems, low-dimensional
vectors, and system artifacts such as machine instructions. A detailed analytic
comparison is provided between the proposed encoding, the IEEE-754-1985 system,
and the recently proposed Posit number system.
</summary>
    <author>
      <name>Lucius T. Schoenbaum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.14401v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.14401v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.07456v1</id>
    <updated>2021-06-14T14:31:59Z</updated>
    <published>2021-06-14T14:31:59Z</published>
    <title>Extending the RISC-V ISA for exploring advanced reconfigurable SIMD
  instructions</title>
    <summary>  This paper presents a novel, non-standard set of vector instruction types for
exploring custom SIMD instructions in a softcore. The new types allow
simultaneous access to a relatively high number of operands, reducing the
instruction count where applicable. Additionally, a high-performance
open-source RISC-V (RV32 IM) softcore is introduced, optimised for exploring
custom SIMD instructions and streaming performance. By providing instruction
templates for instruction development in HDL/Verilog, efficient FPGA-based
instructions can be developed with few low-level lines of code. In order to
improve custom SIMD instruction performance, the softcore's cache hierarchy is
optimised for bandwidth, such as with very wide blocks for the last-level
cache. The approach is demonstrated on example memory-intensive applications on
an FPGA. Although the exploration is based on the softcore, the goal is to
provide a means to experiment with advanced SIMD instructions which could be
loaded in future CPUs that feature reconfigurable regions as custom
instructions. Finally, we provide some insights on the challenges and
effectiveness of such future micro-architectures.
</summary>
    <author>
      <name>Philippos Papaphilippou</name>
    </author>
    <author>
      <name>Paul H. J. Kelly</name>
    </author>
    <author>
      <name>Wayne Luk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the Fifth Workshop on Computer Architecture Research with
  RISC-V (CARRV 2021), co-located with ISCA 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.07456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.07456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.07894v1</id>
    <updated>2021-06-15T06:08:37Z</updated>
    <published>2021-06-15T06:08:37Z</published>
    <title>S2Engine: A Novel Systolic Architecture for Sparse Convolutional Neural
  Networks</title>
    <summary>  Convolutional neural networks (CNNs) have achieved great success in
performing cognitive tasks. However, execution of CNNs requires a large amount
of computing resources and generates heavy memory traffic, which imposes a
severe challenge on computing system design. Through optimizing parallel
executions and data reuse in convolution, systolic architecture demonstrates
great advantages in accelerating CNN computations. However, regular internal
data transmission path in traditional systolic architecture prevents the
systolic architecture from completely leveraging the benefits introduced by
neural network sparsity. Deployment of fine-grained sparsity on the existing
systolic architectures is greatly hindered by the incurred computational
overheads. In this work, we propose S2Engine $-$ a novel systolic architecture
that can fully exploit the sparsity in CNNs with maximized data reuse. S2Engine
transmits compressed data internally and allows each processing element to
dynamically select an aligned data from the compressed dataflow in convolution.
Compared to the naive systolic array, S2Engine achieves about $3.2\times$ and
about $3.0\times$ improvements on speed and energy efficiency, respectively.
</summary>
    <author>
      <name>Jianlei Yang</name>
    </author>
    <author>
      <name>Wenzhi Fu</name>
    </author>
    <author>
      <name>Xingzhou Cheng</name>
    </author>
    <author>
      <name>Xucheng Ye</name>
    </author>
    <author>
      <name>Pengcheng Dai</name>
    </author>
    <author>
      <name>Weisheng Zhao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TC.2021.3087946</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TC.2021.3087946" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 17 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Computers, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.07894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.07894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.08402v1</id>
    <updated>2021-06-15T19:59:26Z</updated>
    <published>2021-06-15T19:59:26Z</published>
    <title>Exploring the Feasibility of Using 3D XPoint as an In-Memory Computing
  Accelerator</title>
    <summary>  This paper describes how 3D XPoint memory arrays can be used as in-memory
computing accelerators. We first show that thresholded matrix-vector
multiplication (TMVM), the fundamental computational kernel in many
applications including machine learning, can be implemented within a 3D XPoint
array without requiring data to leave the array for processing. Using the
implementation of TMVM, we then discuss the implementation of a binary neural
inference engine. We discuss the application of the core concept to address
issues such as system scalability, where we connect multiple 3D XPoint arrays,
and power integrity, where we analyze the parasitic effects of metal lines on
noise margins. To assure power integrity within the 3D XPoint array during this
implementation, we carefully analyze the parasitic effects of metal lines on
the accuracy of the implementations. We quantify the impact of parasitics on
limiting the size and configuration of a 3D XPoint array, and estimate the
maximum acceptable size of a 3D XPoint subarray.
</summary>
    <author>
      <name>Masoud Zabihi</name>
    </author>
    <author>
      <name>Salonik Resch</name>
    </author>
    <author>
      <name>Husrev Cılasun</name>
    </author>
    <author>
      <name>Zamshed I. Chowdhury</name>
    </author>
    <author>
      <name>Zhengyang Zhao</name>
    </author>
    <author>
      <name>Ulya R. Karpuzcu</name>
    </author>
    <author>
      <name>Jian-Ping Wang</name>
    </author>
    <author>
      <name>Sachin S. Sapatnekar</name>
    </author>
    <link href="http://arxiv.org/abs/2106.08402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.08402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.12810v1</id>
    <updated>2021-06-24T07:53:56Z</updated>
    <published>2021-06-24T07:53:56Z</published>
    <title>A Construction Kit for Efficient Low Power Neural Network Accelerator
  Designs</title>
    <summary>  Implementing embedded neural network processing at the edge requires
efficient hardware acceleration that couples high computational performance
with low power consumption. Driven by the rapid evolution of network
architectures and their algorithmic features, accelerator designs are
constantly updated and improved. To evaluate and compare hardware design
choices, designers can refer to a myriad of accelerator implementations in the
literature. Surveys provide an overview of these works but are often limited to
system-level and benchmark-specific performance metrics, making it difficult to
quantitatively compare the individual effect of each utilized optimization
technique. This complicates the evaluation of optimizations for new accelerator
designs, slowing-down the research progress. This work provides a survey of
neural network accelerator optimization approaches that have been used in
recent works and reports their individual effects on edge processing
performance. It presents the list of optimizations and their quantitative
effects as a construction kit, allowing to assess the design choices for each
building block separately. Reported optimizations range from up to 10'000x
memory savings to 33x energy reductions, providing chip designers an overview
of design choices for implementing efficient low power neural network
accelerators.
</summary>
    <author>
      <name>Petar Jokic</name>
    </author>
    <author>
      <name>Erfan Azarkhish</name>
    </author>
    <author>
      <name>Andrea Bonetti</name>
    </author>
    <author>
      <name>Marc Pons</name>
    </author>
    <author>
      <name>Stephane Emery</name>
    </author>
    <author>
      <name>Luca Benini</name>
    </author>
    <link href="http://arxiv.org/abs/2106.12810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.12810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.15284v1</id>
    <updated>2021-06-29T11:55:05Z</updated>
    <published>2021-06-29T11:55:05Z</published>
    <title>NMPO: Near-Memory Computing Profiling and Offloading</title>
    <summary>  Real-world applications are now processing big-data sets, often bottlenecked
by the data movement between the compute units and the main memory. Near-memory
computing (NMC), a modern data-centric computational paradigm, can alleviate
these bottlenecks, thereby improving the performance of applications. The lack
of NMC system availability makes simulators the primary evaluation tool for
performance estimation. However, simulators are usually time-consuming, and
methods that can reduce this overhead would accelerate the early-stage design
process of NMC systems. This work proposes Near-Memory computing Profiling and
Offloading (NMPO), a high-level framework capable of predicting NMC offloading
suitability employing an ensemble machine learning model. NMPO predicts NMC
suitability with an accuracy of 85.6% and, compared to prior works, can reduce
the prediction time by using hardware-dependent applications features by up to
3 order of magnitude.
</summary>
    <author>
      <name>Stefano Corda</name>
    </author>
    <author>
      <name>Madhurya Kumaraswamy</name>
    </author>
    <author>
      <name>Ahsan Javed Awan</name>
    </author>
    <author>
      <name>Roel Jordans</name>
    </author>
    <author>
      <name>Akash Kumar</name>
    </author>
    <author>
      <name>Henk Corporaal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Euromicro Conference on Digital System Design 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.15284v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.15284v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.00778v2</id>
    <updated>2021-08-10T12:12:04Z</updated>
    <published>2021-08-02T10:50:36Z</published>
    <title>Analysing digital in-memory computing for advanced finFET node</title>
    <summary>  Digital In-memory computing improves energy efficiency and throughput of a
data-intensive process, which incur memory thrashing and, resulting multiple
same memory accesses in a von Neumann architecture. Digital in-memory computing
involves accessing multiple SRAM cells simultaneously, which may result in a
bit flip when not timed critically. Therefore we discuss the transient voltage
characteristics of the bitlines during an SRAM compute. To improve the
packaging density and also avoid MOSFET down-scaling issues, we use a 7-nm
predictive PDK which uses a finFET node. The finFET process has discrete fins
and a lower Voltage supply, which makes the design of in-memory compute SRAM
difficult. In this paper, we design a 6T SRAM cell in 7-nm finFET node and
compare its SNMs with a UMC 28nm node implementation. Further, we design and
simulate the rest of the SRAM peripherals, and in-memory computation for an
advanced finFET node.
</summary>
    <author>
      <name>Veerendra S Devaraddi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">International Institute of Information Technology Bangalore</arxiv:affiliation>
    </author>
    <author>
      <name>Joycee M. Mekie</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Indian Institute of Technology Gandhinagar</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Withdrawn due to miscommunication between authors during submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.00778v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.00778v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.01565v1</id>
    <updated>2021-08-03T15:22:36Z</updated>
    <published>2021-08-03T15:22:36Z</published>
    <title>Hardware-aware Design of Multiplierless Second-Order IIR Filters with
  Minimum Adders</title>
    <summary>  In this work, we optimally solve the problem of multiplierless design of
second-order Infinite Impulse Response filters with minimum number of adders.
Given a frequency specification, we design a stable direct form filter with
hardware-aware fixed-point coefficients that yielding minimal number of adders
when replacing all the multiplications by bit shifts and additions. The
coefficient design, quantization and implementation, typically conducted
independently, are now gathered into one global optimization problem, modeled
through integer linear programming and efficiently solved using generic
solvers. We guarantee the frequency-domain specifications and stability, which
together with optimal number of adders will significantly simplify design-space
exploration for filter designers. The optimal filters are implemented within
the FloPoCo IP core generator and synthesized for Field Programmable Gate
Arrays. With respect to state-of-the-art three-step filter design methods, our
one-step design approach achieves, on average, 42% reduction in the number of
lookup tables and 21% improvement in delay.
</summary>
    <author>
      <name>Rémi Garcia</name>
    </author>
    <author>
      <name>Anastasia Volkova</name>
    </author>
    <author>
      <name>Martin Kumm</name>
    </author>
    <author>
      <name>Alexandre Goldsztejn</name>
    </author>
    <author>
      <name>Jonas Kühle</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2022.3161158</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2022.3161158" rel="related"/>
    <link href="http://arxiv.org/abs/2108.01565v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.01565v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.02073v1</id>
    <updated>2021-08-04T14:13:02Z</updated>
    <published>2021-08-04T14:13:02Z</published>
    <title>Efficient Hardware Realizations of Feedforward Artificial Neural
  Networks</title>
    <summary>  This article presents design techniques proposed for efficient hardware
implementation of feedforward artificial neural networks (ANNs) under parallel
and time-multiplexed architectures. To reduce their design complexity, after
the weights of ANN are determined in a training phase, we introduce a technique
to find the minimum quantization value used to convert the floating-point
weight values to integers. For each design architecture, we also propose an
algorithm that tunes the integer weights to reduce the hardware complexity
avoiding a loss in the hardware accuracy. Furthermore, the multiplications of
constant weights by input variables are implemented under the shift-adds
architecture using the fewest number of addition/subtraction operations found
by prominent previously proposed algorithms. Finally, we introduce a
computer-aided design (CAD) tool, called SIMURG, that can describe an ANN
design in hardware automatically based on the ANN structure and the solutions
of proposed design techniques and algorithms. Experimental results indicate
that the tuning techniques can significantly reduce the ANN hardware complexity
under a design architecture and the multiplierless design of ANN can lead to a
significant reduction in area and energy consumption, increasing the latency
slightly.
</summary>
    <author>
      <name>Mohammadreza Esmali Nojehdeh</name>
    </author>
    <author>
      <name>Sajjad Parvin</name>
    </author>
    <author>
      <name>Mustafa Altun</name>
    </author>
    <link href="http://arxiv.org/abs/2108.02073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.02073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.03978v1</id>
    <updated>2021-08-09T12:27:31Z</updated>
    <published>2021-08-09T12:27:31Z</published>
    <title>VeRLPy: Python Library for Verification of Digital Designs with
  Reinforcement Learning</title>
    <summary>  Digital hardware is verified by comparing its behavior against a reference
model on a range of randomly generated input signals. The random generation of
the inputs hopes to achieve sufficient coverage of the different parts of the
design. However, such coverage is often difficult to achieve, amounting to
large verification efforts and delays. An alternative is to use Reinforcement
Learning (RL) to generate the inputs by learning to prioritize those inputs
which can more efficiently explore the design under test. In this work, we
present VeRLPy an open-source library to allow RL-driven verification with
limited additional engineering overhead. This contributes to two broad
movements within the EDA community of (a) moving to open-source toolchains and
(b) reducing barriers for development with Python support. We also demonstrate
the use of VeRLPy for a few designs and establish its value over randomly
generated input signals.
</summary>
    <author>
      <name>Aebel Joe Shibu</name>
    </author>
    <author>
      <name>Sadhana S</name>
    </author>
    <author>
      <name>Shilpa N</name>
    </author>
    <author>
      <name>Pratyush Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to The first international conference on AI-ML Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.03978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.03978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.07448v1</id>
    <updated>2021-08-17T05:03:30Z</updated>
    <published>2021-08-17T05:03:30Z</published>
    <title>Testable Designs of Toffoli Fredkin Reversible Circuits</title>
    <summary>  Loss of every bit in traditional logic circuits involves dissipation of power
in the form of heat that evolve to the environment. Reversible logic is one of
the alternatives that have capabilities to mitigate this dissipation by
preventing the loss of bits. It also have the potential to broaden the horizon
of futuristic reckon with its applications to quantum computation. Application
of testing strategies to the logic circuits is a necessity that guarantees
their true functioning where the researchers are at par with solutions for the
upcoming challenges and agreements for reversible logic circuits. Novel methods
of designing Toffoli, Fredkin and mixed Toffoli-Fredkin gates based reversible
circuits for testability are put fourth in this article. The proposed designs
are independent of the implementation techniques and can be brought into real
hardware devices after obtaining a stable fabrication environment. The
experimentation for the proposed models are performed on RCViewer and RevKit
tools to verify the functionality and computation of cost metrics. Fault
simulations are carried out using C++ and Java to calculate fault coverage in
respective methodologies. The results confirmed that all the presented work
outperforms existing state-of-art approaches.
</summary>
    <author>
      <name>Hari Mohan Gaur</name>
    </author>
    <author>
      <name>Ashutosh Kumar Singh</name>
    </author>
    <author>
      <name>Umesh Ghanekar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 Figures, 6 sections and for conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.07448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.07448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.08497v1</id>
    <updated>2021-08-19T04:47:54Z</updated>
    <published>2021-08-19T04:47:54Z</published>
    <title>Monarch: A Durable Polymorphic Memory For Data Intensive Applications</title>
    <summary>  3D die stacking has often been proposed to build large-scale DRAM-based
caches. Unfortunately, the power and performance overheads of DRAM limit the
efficiency of high-bandwidth memories. Also, DRAM is facing serious scalability
challenges that make alternative technologies more appealing. This paper
examines Monarch, a resistive 3D stacked memory based on a novel reconfigurable
crosspoint array called XAM. The XAM array is capable of switching between
random access and content-addressable modes, which enables Monarch (i) to
better utilize the in-package bandwidth and (ii) to satisfy both the random
access memory and associative search requirements of various applications.
Moreover, the Monarch controller ensures a given target lifetime for the
resistive stack. Our simulation results on a set of parallel memory-intensive
applications indicate that Monarch outperforms an ideal DRAM caching by 1.21x
on average. For in-memory hash table and string matching workloads, Monarch
improves performance up to 12x over the conventional high bandwidth memories.
</summary>
    <author>
      <name>Ananth Krishna Prasad</name>
    </author>
    <author>
      <name>Mahdi Nazm Bojnordi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE TC</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.08497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.08497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.3; E.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.09601v1</id>
    <updated>2021-08-21T23:53:12Z</updated>
    <published>2021-08-21T23:53:12Z</published>
    <title>Programmable FPGA-based Memory Controller</title>
    <summary>  Even with generational improvements in DRAM technology, memory access latency
still remains the major bottleneck for application accelerators, primarily due
to limitations in memory interface IPs which cannot fully account for
variations in target applications, the algorithms used, and accelerator
architectures. Since developing memory controllers for different applications
is time-consuming, this paper introduces a modular and programmable memory
controller that can be configured for different target applications on
available hardware resources. The proposed memory controller efficiently
supports cache-line accesses along with bulk memory transfers. The user can
configure the controller depending on the available logic resources on the
FPGA, memory access pattern, and external memory specifications. The modular
design supports various memory access optimization techniques including,
request scheduling, internal caching, and direct memory access. These
techniques contribute to reducing the overall latency while maintaining high
sustained bandwidth. We implement the system on a state-of-the-art FPGA and
evaluate its performance using two widely studied domains: graph analytics and
deep learning workloads. We show improved overall memory access time up to 58%
on CNN and GCN workloads compared with commercial memory controller IPs.
</summary>
    <author>
      <name>Sasindu Wijeratne</name>
    </author>
    <author>
      <name>Sanket Pattnaik</name>
    </author>
    <author>
      <name>Zhiyu Chen</name>
    </author>
    <author>
      <name>Rajgopal Kannan</name>
    </author>
    <author>
      <name>Viktor Prasanna</name>
    </author>
    <link href="http://arxiv.org/abs/2108.09601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.09601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.10570v2</id>
    <updated>2025-02-18T02:00:09Z</updated>
    <published>2021-08-24T08:15:20Z</published>
    <title>METRO: A Software-Hardware Co-Design of Interconnections for Spatial DNN
  Accelerators</title>
    <summary>  Tiled spatial architectures have proved to be an effective solution to build
large-scale DNN accelerators. In particular, interconnections between tiles are
critical for high performance in these tile-based architectures. In this work,
we identify the inefficiency of the widely used traditional on-chip networks
and the opportunity of software-hardware co-design. We propose METRO with the
basic idea of decoupling the traffic scheduling policies from hardware fabrics
and moving them to the software level. METRO contains two modules working in
synergy: METRO software scheduling framework to coordinate the traffics and
METRO hardware facilities to deliver the data based on software configurations.
We evaluate the co-design using different flit sizes for synthetic study,
illustrating its effectiveness under various hardware resource constraints, in
addition to a wide range of DNN models selected from real-world workloads. The
results show that METRO achieves 56.3% communication speedup on average and up
to 73.6% overall processing time reduction compared with traditional on-chip
network designs.
</summary>
    <author>
      <name>Zhao Wang</name>
    </author>
    <author>
      <name>Jingchen Zhu</name>
    </author>
    <author>
      <name>Zhe Zhou</name>
    </author>
    <author>
      <name>Guangyu Sun</name>
    </author>
    <link href="http://arxiv.org/abs/2108.10570v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.10570v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.11521v2</id>
    <updated>2022-09-03T21:42:47Z</updated>
    <published>2021-08-26T00:04:42Z</published>
    <title>Efficient On-Chip Communication for Parallel Graph-Analytics on Spatial
  Architectures</title>
    <summary>  Large-scale graph processing has drawn great attention in recent years. Most
of the modern-day datacenter workloads can be represented in the form of Graph
Processing such as MapReduce etc. Consequently, a lot of designs for
Domain-Specific Accelerators have been proposed for Graph Processing. Spatial
Architectures have been promising in the execution of Graph Processing, where
the graph is partitioned into several nodes and each node works in parallel. We
conduct experiments to analyze the on-chip movement of data in graph processing
on a Spatial Architecture. Based on the observations, we identify a data
movement bottleneck, in the execution of such highly parallel processing
accelerators. To mitigate the bottleneck we propose a novel power-law aware
Graph Partitioning and Data Mapping scheme to reduce the communication latency
by minimizing the hop counts on a scalable network-on-chip. The experimental
results on popular graph algorithms show that our implementation makes the
execution 2-5x faster and 2.7-4x energy-efficient by reducing the data movement
time in comparison to a baseline implementation.
</summary>
    <author>
      <name>Khushal Sethi</name>
    </author>
    <link href="http://arxiv.org/abs/2108.11521v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.11521v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.12292v1</id>
    <updated>2021-08-27T13:58:10Z</updated>
    <published>2021-08-27T13:58:10Z</published>
    <title>Terabit-per-Second Multicore Polar Code Successive Cancellation Decoders</title>
    <summary>  This work presents a high throughput and energy efficient multicore (MC)
successive cancellation (SC) decoder architecture for polar codes. SC is a
low-complexity decoding algorithm with a set of sequential operations. The
sequential processing nature of SC limits parallelism but promotes not only
pipelining but also multiple copies of SC decoder with an optimized pipeline
depth to achieve Tb/s throughput. The MCSC decoder architecture consists of
multiple SC decoders with lower frequency and pipeline depth to process
multiple codewords in parallel to achieve lower power consumption. The pipeline
depth of MCSC is optimized separately for each multicore configuration using
register reduction/balancing (R-RB) method. This enables an efficient
implementation for the 1-core, 2-core 4-core and 8-core candidate MCSC
decoders. To reduce the complexity of the implementation, an adaptive
log-likelihood ratio (LLR) quantization scheme is used for internal LLRs within
the range of 1-5 bits. The post-placement-routing results at 28nm High-k Metal
Gate (HKMG) ASIC technology show that 4-core MCSC decoder achieves 1 Tb/s
throughput on 3.92 mm$^2$ area with 1.55 pJ/bit energy efficiency.
</summary>
    <author>
      <name>Altuğ Süral</name>
    </author>
    <author>
      <name>Ertuğrul Kolağasıoğlu</name>
    </author>
    <link href="http://arxiv.org/abs/2108.12292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.12292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.12897v1</id>
    <updated>2021-08-29T19:33:07Z</updated>
    <published>2021-08-29T19:33:07Z</published>
    <title>ACTreS: Analog Clock Tree Synthesis</title>
    <summary>  This paper describes a graph-theoretic formalism and a flow that, to a great
extent, automate the design of clock trees in Sampled-Data Analog Circuits
(SDACs). The current practice for clock tree design of SDACs is a manual
process, which is time-consuming and error-prone. Clock tree design in digital
domain, however, is fully automated and is carried out by Clock Tree Synthesis
(CTS) software. In spite of critical differences, SDAC clock tree design
problem has fundamental similarities with its digital counterpart. We exploited
these similarities and built a design flow and tool set, which uses commercial
digital CTS software as an intermediate step. We will explain our flow using a
0.18 micron 10-bit 60 MHz 2-stage pipelined differential-input flash
analog-to-digital converter as a test circuit.
</summary>
    <author>
      <name>Bilgiday Yuce</name>
    </author>
    <author>
      <name>H. Fatih Ugurdag</name>
    </author>
    <author>
      <name>Iskender Agi</name>
    </author>
    <author>
      <name>Gokhan Guner</name>
    </author>
    <author>
      <name>Vahap Baris Esen</name>
    </author>
    <author>
      <name>Seyrani Korkmaz</name>
    </author>
    <author>
      <name>I. Faik Baskaya</name>
    </author>
    <author>
      <name>Günhan Dündar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 24 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.12897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.12897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.13378v2</id>
    <updated>2021-09-20T14:45:06Z</updated>
    <published>2021-08-30T17:01:33Z</published>
    <title>MultPIM: Fast Stateful Multiplication for Processing-in-Memory</title>
    <summary>  Processing-in-memory (PIM) seeks to eliminate computation/memory data
transfer using devices that support both storage and logic. Stateful logic
techniques such as IMPLY, MAGIC and FELIX can perform logic gates within
memristive crossbar arrays with massive parallelism. Multiplication via
stateful logic is an active field of research due to the wide implications.
Recently, RIME has become the state-of-the-art algorithm for stateful
single-row multiplication by using memristive partitions, reducing the latency
of the previous state-of-the-art by 5.1x. In this paper, we begin by proposing
novel partition-based computation techniques for broadcasting and shifting
data. Then, we design an in-memory multiplication algorithm based on the
carry-save add-shift (CSAS) technique. Finally, we develop a novel stateful
full-adder that significantly improves the state-of-the-art (FELIX) design.
These contributions constitute MultPIM, a multiplier that reduces
state-of-the-art time complexity from quadratic to linear-log. For 32-bit
numbers, MultPIM improves latency by an additional 4.2x over RIME, while even
slightly reducing area overhead. Furthermore, we optimize MultPIM for
full-precision matrix-vector multiplication and improve latency by 25.5x over
FloatPIM matrix-vector multiplication.
</summary>
    <author>
      <name>Orian Leitersdorf</name>
    </author>
    <author>
      <name>Ronny Ronen</name>
    </author>
    <author>
      <name>Shahar Kvatinsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE Transactions On Circuits And Systems-II (TCAS-II)</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.13378v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.13378v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.00665v1</id>
    <updated>2021-09-02T01:37:16Z</updated>
    <published>2021-09-02T01:37:16Z</published>
    <title>Agon: A Scalable Competitive Scheduler for Large Heterogeneous Systems</title>
    <summary>  This work proposes a competitive scheduling approach, designed to scale to
large heterogeneous multicore systems. This scheduler overcomes the challenges
of (1) the high computation overhead of near-optimal schedulers, and (2) the
error introduced by inaccurate performance predictions. This paper presents
Agon, a neural network-based classifier that selects from a range of
schedulers, from simple to very accurate, and learns which scheduler provides
the right balance of accuracy and overhead for each scheduling interval. Agon
also employs a de-noising frontend allowing the individual schedulers to be
tolerant towards noise in performance predictions, producing better overall
schedules. By avoiding expensive scheduling overheads, Agon improves average
system performance by 6\% on average, approaching the performance of an
oracular scheduler (99.1% of oracle performance).
</summary>
    <author>
      <name>Andreas Prodromou</name>
    </author>
    <author>
      <name>Ashish Venkat</name>
    </author>
    <author>
      <name>Dean M. Tullsen</name>
    </author>
    <link href="http://arxiv.org/abs/2109.00665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.00665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.01404v1</id>
    <updated>2021-09-03T09:50:46Z</updated>
    <published>2021-09-03T09:50:46Z</published>
    <title>End-to-end 100-TOPS/W Inference With Analog In-Memory Computing: Are We
  There Yet?</title>
    <summary>  In-Memory Acceleration (IMA) promises major efficiency improvements in deep
neural network (DNN) inference, but challenges remain in the integration of IMA
within a digital system. We propose a heterogeneous architecture coupling 8
RISC-V cores with an IMA in a shared-memory cluster, analyzing the benefits and
trade-offs of in-memory computing on the realistic use case of a MobileNetV2
bottleneck layer. We explore several IMA integration strategies, analyzing
performance, area, and energy efficiency. We show that while pointwise layers
achieve significant speed-ups over software implementation, on depthwise layer
the inability to efficiently map parameters on the accelerator leads to a
significant trade-off between throughput and area. We propose a hybrid solution
where pointwise convolutions are executed on IMA while depthwise on the cluster
cores, achieving a speed-up of 3x over SW execution while saving 50% of area
when compared to an all-in IMA solution with similar performance.
</summary>
    <author>
      <name>Gianmarco Ottavi</name>
    </author>
    <author>
      <name>Geethan Karunaratne</name>
    </author>
    <author>
      <name>Francesco Conti</name>
    </author>
    <author>
      <name>Irem Boybat</name>
    </author>
    <author>
      <name>Luca Benini</name>
    </author>
    <author>
      <name>Davide Rossi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/AICAS51828.2021.9458409</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/AICAS51828.2021.9458409" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages,6 figures, conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2021 IEEE 3rd International Conference on Artificial Intelligence
  Circuits and Systems (AICAS)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2109.01404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.01404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.03024v1</id>
    <updated>2021-08-01T03:40:14Z</updated>
    <published>2021-08-01T03:40:14Z</published>
    <title>Versa: A Dataflow-Centric Multiprocessor with 36 Systolic ARM Cortex-M4F
  Cores and a Reconfigurable Crossbar-Memory Hierarchy in 28nm</title>
    <summary>  We present Versa, an energy-efficient processor with 36 systolic ARM
Cortex-M4F cores and a runtime-reconfigurable memory hierarchy. Versa exploits
algorithm-specific characteristics in order to optimize bandwidth, access
latency, and data reuse. Measured on a set of kernels with diverse data access,
control, and synchronization characteristics, reconfiguration between different
Versa modes yields median energy-efficiency improvements of 11.6x and 37.2x
over mobile CPU and GPU baselines, respectively.
</summary>
    <author>
      <name>Sung Kim</name>
    </author>
    <author>
      <name>Morteza Fayazi</name>
    </author>
    <author>
      <name>Alhad Daftardar</name>
    </author>
    <author>
      <name>Kuan-Yu Chen</name>
    </author>
    <author>
      <name>Jielun Tan</name>
    </author>
    <author>
      <name>Subhankar Pal</name>
    </author>
    <author>
      <name>Tutu Ajayi</name>
    </author>
    <author>
      <name>Yan Xiong</name>
    </author>
    <author>
      <name>Trevor Mudge</name>
    </author>
    <author>
      <name>Chaitali Chakrabarti</name>
    </author>
    <author>
      <name>David Blaauw</name>
    </author>
    <author>
      <name>Ronald Dreslinski</name>
    </author>
    <author>
      <name>Hun-Seok Kim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23919/VLSICircuits52068.2021.9492391</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23919/VLSICircuits52068.2021.9492391" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at the Symposium on VLSI Circuits, 2021. Paper C9-4</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.03024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.03024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.03041v3</id>
    <updated>2021-10-07T07:51:09Z</updated>
    <published>2021-08-23T13:20:25Z</published>
    <title>Only Six Passive Circuit Elements Are Existent</title>
    <summary>  We found that a second-order ideal memristor degenerates into a negative
nonlinear resistor. This phenomenon is quite similar to what are observed in
chemistry: a chemical element with a higher atomic number is unstable and may
decay radioactively into another chemical element with a lower atomic number.
After extending the above local activity (verified both graphically and
analytically) to other higher-order circuit elements, we concluded that all
higher-order passive memory circuit elements do not exist in nature and that
the periodic table of the two-terminal passive circuit elements can be
dramatically reduced to a six-pointed star comprising only six passive
elements. Such a bounded table may mark the end of the hunt for missing
higher-order passive circuit elements predicted nearly 40 years ago.
</summary>
    <author>
      <name>Frank Z. Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.03041v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.03041v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.04614v1</id>
    <updated>2021-09-10T01:54:22Z</updated>
    <published>2021-09-10T01:54:22Z</published>
    <title>A Fast-and-Effective Early-Stage Multi-level Cache Optimization Method
  Based on Reuse-Distance Analysis</title>
    <summary>  In this paper, we propose a practical and effective approach allowing
designers to optimize multi-level cache size at the early system design phase.
Our key contribution is to generalize the reuse distance analysis method and
develop an effective and practical cache design optimization approach. We adopt
a simple scanning search method to locate optimal cache solutions in terms of
cache size, power consumption, or average data access delay. The proposed
approach is particularly useful for early-phase system designers and is
verified to be 150 to 250 times faster than the traditional simulation-based
approach. In addition, we also introduce a simplified analytical model and
provide designers insights about how cache design parameters may affect the
expected results. As a result, designers can make an adequate decision in the
early system design phase.
</summary>
    <author>
      <name>Cheng-Lin Tsai</name>
    </author>
    <author>
      <name>Ren-Song Tsay</name>
    </author>
    <link href="http://arxiv.org/abs/2109.04614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.04614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.05430v1</id>
    <updated>2021-09-12T04:26:31Z</updated>
    <published>2021-09-12T04:26:31Z</published>
    <title>Ohm-GPU: Integrating New Optical Network and Heterogeneous Memory into
  GPU Multi-Processors</title>
    <summary>  Traditional graphics processing units (GPUs) suffer from the low memory
capacity and demand for high memory bandwidth. To address these challenges, we
propose Ohm-GPU, a new optical network based heterogeneous memory design for
GPUs. Specifically, Ohm-GPU can expand the memory capacity by combing a set of
high-density 3D XPoint and DRAM modules as heterogeneous memory. To prevent
memory channels from throttling throughput of GPU memory system, Ohm-GPU
replaces the electrical lanes in the traditional memory channel with a
high-performance optical network. However, the hybrid memory can introduce
frequent data migrations between DRAM and 3D XPoint, which can unfortunately
occupy the memory channel and increase the optical network traffic. To prevent
the intensive data migrations from blocking normal memory services, Ohm-GPU
revises the existing memory controller and designs a new optical network
infrastructure, which enables the memory channel to serve the data migrations
and memory requests, in parallel. Our evaluation results reveal that Ohm-GPU
can improve the performance by 181% and 27%, compared to a DRAM-based GPU
memory system and the baseline optical network based heterogeneous memory
system, respectively.
</summary>
    <author>
      <name>Jie Zhang</name>
    </author>
    <author>
      <name>Myoungsoo Jung</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3466752.3480107</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3466752.3480107" rel="related"/>
    <link href="http://arxiv.org/abs/2109.05430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.05430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.07047v2</id>
    <updated>2022-06-23T10:08:26Z</updated>
    <published>2021-09-15T01:58:12Z</published>
    <title>Dataflow Accelerator Architecture for Autonomous Machine Computing</title>
    <summary>  Commercial autonomous machines is a thriving sector, one that is likely the
next ubiquitous computing platform, after Personal Computers (PC), cloud
computing, and mobile computing. Nevertheless, a suitable computing substrate
for autonomous machines is missing, and many companies are forced to develop ad
hoc computing solutions that are neither principled nor extensible. By
analyzing the demands of autonomous machine computing, this article proposes
Dataflow Accelerator Architecture (DAA), a modern instantiation of the classic
dataflow principle, that matches the characteristics of autonomous machine
software.
</summary>
    <author>
      <name>Shaoshan Liu</name>
    </author>
    <author>
      <name>Yuhao Zhu</name>
    </author>
    <author>
      <name>Bo Yu</name>
    </author>
    <author>
      <name>Jean-Luc Gaudiot</name>
    </author>
    <author>
      <name>Guang R. Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Please note that this may be a special case in that Professor Gao
  sadly passed away on September 12th, just as we had put the finishing touches
  on this submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.07047v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.07047v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.08874v1</id>
    <updated>2021-09-18T08:19:29Z</updated>
    <published>2021-09-18T08:19:29Z</published>
    <title>Reconfigurable Low-latency Memory System for Sparse Matricized Tensor
  Times Khatri-Rao Product on FPGA</title>
    <summary>  Tensor decomposition has become an essential tool in many applications in
various domains, including machine learning. Sparse Matricized Tensor Times
Khatri-Rao Product (MTTKRP) is one of the most computationally expensive
kernels in tensor computations. Despite having significant computational
parallelism, MTTKRP is a challenging kernel to optimize due to its irregular
memory access characteristics. This paper focuses on a multi-faceted memory
system, which explores the spatial and temporal locality of the data structures
of MTTKRP. Further, users can reconfigure our design depending on the behavior
of the compute units used in the FPGA accelerator. Our system efficiently
accesses all the MTTKRP data structures while reducing the total memory access
time, using a distributed cache and Direct Memory Access (DMA) subsystem.
Moreover, our work improves the memory access time by 3.5x compared with
commercial memory controller IPs. Also, our system shows 2x and 1.26x speedups
compared with cache-only and DMA-only memory systems, respectively.
</summary>
    <author>
      <name>Sasindu Wijeratne</name>
    </author>
    <author>
      <name>Rajgopal Kannan</name>
    </author>
    <author>
      <name>Viktor Prasanna</name>
    </author>
    <link href="http://arxiv.org/abs/2109.08874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.08874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.08957v1</id>
    <updated>2021-09-18T15:57:47Z</updated>
    <published>2021-09-18T15:57:47Z</published>
    <title>AI Accelerator Survey and Trends</title>
    <summary>  Over the past several years, new machine learning accelerators were being
announced and released every month for a variety of applications from speech
recognition, video object detection, assisted driving, and many data center
applications. This paper updates the survey of AI accelerators and processors
from past two years. This paper collects and summarizes the current commercial
accelerators that have been publicly announced with peak performance and power
consumption numbers. The performance and power values are plotted on a scatter
graph, and a number of dimensions and observations from the trends on this plot
are again discussed and analyzed. This year, we also compile a list of
benchmarking performance results and compute the computational efficiency with
respect to peak performance.
</summary>
    <author>
      <name>Albert Reuther</name>
    </author>
    <author>
      <name>Peter Michaleas</name>
    </author>
    <author>
      <name>Michael Jones</name>
    </author>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Siddharth Samsi</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HPEC49654.2021.9622867</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HPEC49654.2021.9622867" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures, IEEE High Performance Extreme Computing
  Conference 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.08957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.08957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.4; C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.09687v1</id>
    <updated>2021-09-20T16:58:21Z</updated>
    <published>2021-09-20T16:58:21Z</published>
    <title>Making Memristive Processing-in-Memory Reliable</title>
    <summary>  Processing-in-memory (PIM) solutions vastly accelerate systems by reducing
data transfer between computation and memory. Memristors possess a unique
property that enables storage and logic within the same device, which is
exploited in the memristive Memory Processing Unit (mMPU). The mMPU expands
fundamental stateful logic techniques, such as IMPLY, MAGIC and FELIX, to
high-throughput parallel logic and arithmetic operations within the memory.
Unfortunately, memristive processing-in-memory is highly vulnerable to soft
errors and this massive parallelism is not compatible with traditional
reliability techniques, such as error-correcting-code (ECC). In this paper, we
discuss reliability techniques that efficiently support the mMPU by utilizing
the same principles as the mMPU computation. We detail ECC techniques that are
based on the unique properties of the mMPU to efficiently utilize the massive
parallelism. Furthermore, we present novel solutions for efficiently
implementing triple modular redundancy (TMR). The short-term and long-term
reliability of large-scale applications, such as neural-network acceleration,
are evaluated. The analysis clearly demonstrates the importance of
high-throughput reliability mechanisms for memristive processing-in-memory.
</summary>
    <author>
      <name>Orian Leitersdorf</name>
    </author>
    <author>
      <name>Ronny Ronen</name>
    </author>
    <author>
      <name>Shahar Kvatinsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to 28th International Conference on Electronics Circuits and
  Systems (ICECS) 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.09687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.09687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.11069v1</id>
    <updated>2021-09-22T22:52:56Z</updated>
    <published>2021-09-22T22:52:56Z</published>
    <title>DAS: Dynamic Adaptive Scheduling for Energy-Efficient Heterogeneous SoCs</title>
    <summary>  Domain-specific systems-on-chip (DSSoCs) aim at bridging the gap between
application-specific integrated circuits (ASICs) and general-purpose
processors. Traditional operating system (OS) schedulers can undermine the
potential of DSSoCs since their execution times can be orders of magnitude
larger than the execution time of the task itself. To address this problem, we
propose a dynamic adaptive scheduling (DAS) framework that combines the
benefits of a fast (low-overhead) scheduler and a slow (sophisticated,
high-performance but high-overhead) scheduler. Experiments with five real-world
streaming applications show that DAS consistently outperforms both the fast and
slow schedulers. For 40 different workloads, DAS achieves on average 1.29x
speedup and 45% lower EDP compared to the sophisticated scheduler at low data
rates and 1.28x speedup and 37% lower EDP than the fast scheduler when the
workload complexity increases.
</summary>
    <author>
      <name>A. Alper Goksoy</name>
    </author>
    <author>
      <name>Anish Krishnakumar</name>
    </author>
    <author>
      <name>Md Sahil Hassan</name>
    </author>
    <author>
      <name>Allen J. Farcas</name>
    </author>
    <author>
      <name>Ali Akoglu</name>
    </author>
    <author>
      <name>Radu Marculescu</name>
    </author>
    <author>
      <name>Umit Y. Ogras</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LES.2021.3110426</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LES.2021.3110426" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 tables, 3 figures, 1 algorithm, Accepted for publication
  in IEEE Embedded Systems Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.11069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.11069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.01752v1</id>
    <updated>2021-10-05T00:01:31Z</updated>
    <published>2021-10-05T00:01:31Z</published>
    <title>RASA: Efficient Register-Aware Systolic Array Matrix Engine for CPU</title>
    <summary>  As AI-based applications become pervasive, CPU vendors are starting to
incorporate matrix engines within the datapath to boost efficiency. Systolic
arrays have been the premier architectural choice as matrix engines in offload
accelerators. However, we demonstrate that incorporating them inside CPUs can
introduce under-utilization and stalls due to limited register storage to
amortize the fill and drain times of the array. To address this, we propose
RASA, Register-Aware Systolic Array. We develop techniques to divide an
execution stage into several sub-stages and overlap instructions to hide
overheads and run them concurrently. RASA-based designs improve performance
significantly with negligible area and power overhead.
</summary>
    <author>
      <name>Geonhwa Jeong</name>
    </author>
    <author>
      <name>Eric Qin</name>
    </author>
    <author>
      <name>Ananda Samajdar</name>
    </author>
    <author>
      <name>Christopher J. Hughes</name>
    </author>
    <author>
      <name>Sreenivas Subramoney</name>
    </author>
    <author>
      <name>Hyesoon Kim</name>
    </author>
    <author>
      <name>Tushar Krishna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted to DAC 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.01752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.01752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.08613v1</id>
    <updated>2021-10-16T16:52:18Z</updated>
    <published>2021-10-16T16:52:18Z</published>
    <title>Enabling Large-Reach TLBs for High-Throughput Processors by Exploiting
  Memory Subregion Contiguity</title>
    <summary>  Accelerators, like GPUs, have become a trend to deliver future performance
desire, and sharing the same virtual memory space between CPUs and GPUs is
increasingly adopted to simplify programming. However, address translation,
which is the key factor of virtual memory, is becoming the bottleneck of
performance for GPUs. In GPUs, a single TLB miss can stall hundreds of threads
due to the SIMT execute model, degrading performance dramatically. Through real
system analysis, we observe that the OS shows an advanced contiguity (e.g.,
hundreds of contiguous pages), and more large memory regions with advanced
contiguity tend to be allocated with the increase of working sets. Leveraging
the observation, we propose MESC to improve the translation efficiency for
GPUs. The key idea of MESC is to divide each large page frame (2MB size) in
virtual memory space into memory subregions with fixed size (i.e., 64 4KB
pages), and store the contiguity information of subregions and large page
frames in L2PTEs. With MESC, address translations of up to 512 pages can be
coalesced into single TLB entry, without the needs of changing memory
allocation policy (i.e., demand paging) and the support of large pages. In the
experimental results, MESC achieves 77.2% performance improvement and 76.4%
reduction in dynamic translation energy for translation-sensitive workloads.
</summary>
    <author>
      <name>Chao Yu</name>
    </author>
    <author>
      <name>Yuebin Bai</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2110.08613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.08613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.08685v1</id>
    <updated>2021-10-17T00:25:21Z</updated>
    <published>2021-10-17T00:25:21Z</published>
    <title>A Learning-based Approach Towards Automated Tuning of SSD Configurations</title>
    <summary>  Thanks to the mature manufacturing techniques, solid-state drives (SSDs) are
highly customizable for applications today, which brings opportunities to
further improve their storage performance and resource utilization. However,
the SSD efficiency is usually determined by many hardware parameters, making it
hard for developers to manually tune them and determine the optimal SSD
configurations.
  In this paper, we present an automated learning-based framework, named
LearnedSSD, that utilizes both supervised and unsupervised machine learning
(ML) techniques to drive the tuning of hardware configurations for SSDs.
LearnedSSD automatically extracts the unique access patterns of a new workload
using its block I/O traces, maps the workload to previously workloads for
utilizing the learned experiences, and recommends an optimal SSD configuration
based on the validated storage performance. LearnedSSD accelerates the
development of new SSD devices by automating the hard-ware parameter
configurations and reducing the manual efforts. We develop LearnedSSD with
simple yet effective learning algorithms that can run efficiently on multi-core
CPUs. Given a target storage workload, our evaluation shows that LearnedSSD can
always deliver an optimal SSD configuration for the target workload, and this
configuration will not hurt the performance of non-target workloads.
</summary>
    <author>
      <name>Daixuan Li</name>
    </author>
    <author>
      <name>Jian Huang</name>
    </author>
    <link href="http://arxiv.org/abs/2110.08685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.08685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.10857v1</id>
    <updated>2021-10-21T02:39:54Z</updated>
    <published>2021-10-21T02:39:54Z</published>
    <title>Vortex: Extending the RISC-V ISA for GPGPU and 3D-GraphicsResearch</title>
    <summary>  The importance of open-source hardware and software has been increasing.
However, despite GPUs being one of the more popular accelerators across various
applications, there is very little open-source GPU infrastructure in the public
domain. We argue that one of the reasons for the lack of open-source
infrastructure for GPUs is rooted in the complexity of their ISA and software
stacks.In this work, we first propose an ISA extension to RISC-V that supports
GPGPUs and graphics. The main goal of the ISA extension proposal is to minimize
the ISA changes so that the corresponding changes to the open-source ecosystem
are also minimal, which makes for a sustainable development ecosystem. To
demonstrate the feasibility of the minimally extended RISC-V ISA, we
implemented the complete software and hardware stacks of Vortex on FPGA. Vortex
is a PCIe-based soft GPU that supports OpenCL and OpenGL.Vortex can be used in
a variety of applications, including machine learning, graph analytics, and
graphics rendering. Vortex can scale up to 32 cores on an Altera Stratix 10
FPGA, delivering a peak performance of 25.6 GFlops at 200 Mhz.
</summary>
    <author>
      <name>Blaise Tine</name>
    </author>
    <author>
      <name>Fares Elsabbagh</name>
    </author>
    <author>
      <name>Krishna Yalamarthy</name>
    </author>
    <author>
      <name>Hyesoon Kim</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">MICRO'21: 54th Annual IEEE/ACM International Symposium on
  Microarchitecture (MICRO '21), October 18--22, 2021, Virtual Event, Greece</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.10857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.10857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.11489v2</id>
    <updated>2021-11-08T22:47:25Z</updated>
    <published>2021-10-21T21:29:06Z</published>
    <title>Supporting Massive DLRM Inference Through Software Defined Memory</title>
    <summary>  Deep Learning Recommendation Models (DLRM) are widespread, account for a
considerable data center footprint, and grow by more than 1.5x per year. With
model size soon to be in terabytes range, leveraging Storage ClassMemory (SCM)
for inference enables lower power consumption and cost. This paper evaluates
the major challenges in extending the memory hierarchy to SCM for DLRM, and
presents different techniques to improve performance through a Software Defined
Memory. We show how underlying technologies such as Nand Flash and 3DXP
differentiate, and relate to real world scenarios, enabling from 5% to 29%
power savings.
</summary>
    <author>
      <name>Ehsan K. Ardestani</name>
    </author>
    <author>
      <name>Changkyu Kim</name>
    </author>
    <author>
      <name>Seung Jae Lee</name>
    </author>
    <author>
      <name>Luoshang Pan</name>
    </author>
    <author>
      <name>Valmiki Rampersad</name>
    </author>
    <author>
      <name>Jens Axboe</name>
    </author>
    <author>
      <name>Banit Agrawal</name>
    </author>
    <author>
      <name>Fuxun Yu</name>
    </author>
    <author>
      <name>Ansha Yu</name>
    </author>
    <author>
      <name>Trung Le</name>
    </author>
    <author>
      <name>Hector Yuen</name>
    </author>
    <author>
      <name>Shishir Juluri</name>
    </author>
    <author>
      <name>Akshat Nanda</name>
    </author>
    <author>
      <name>Manoj Wodekar</name>
    </author>
    <author>
      <name>Dheevatsa Mudigere</name>
    </author>
    <author>
      <name>Krishnakumar Nair</name>
    </author>
    <author>
      <name>Maxim Naumov</name>
    </author>
    <author>
      <name>Chris Peterson</name>
    </author>
    <author>
      <name>Mikhail Smelyanskiy</name>
    </author>
    <author>
      <name>Vijay Rao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.11489v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.11489v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.11504v1</id>
    <updated>2021-09-23T12:26:46Z</updated>
    <published>2021-09-23T12:26:46Z</published>
    <title>Maximum Power Point Tracking Circuit for an Energy Harvester in 130 nm
  CMOS Technology</title>
    <summary>  This paper presents design of a Maximum Power Point Tracking (MPPT) circuit
and its functionality for tuning the maximum power transfer from an energy
harvester (EH) unit. Simple and practical Perturb and Observe algorithm is
investigated and implemented. We describe the circuit functionality and the
improvements that have been introduced to the original algorithm. The proposed
MPPT design is divided into three main blocks. The output signal is being
generated by the PWM or PFM block. The tracking speed has been enhanced by
implementing a variable step size in the Tracking Block. Finally, the overall
power consumption of the MPPT circuit itself is controlled by the Power
Management Block, which manages delivering the clock signal to the rest of the
circuit. The RTL code of the proposed MPPT has been described in Verilog, then
has been synthesized and placed-and-routed in a general purpose 130nm CMOS
technology.
</summary>
    <author>
      <name>Adam Hudec</name>
    </author>
    <author>
      <name>Lukas Nagy</name>
    </author>
    <author>
      <name>Martin Kovac</name>
    </author>
    <author>
      <name>Viera Stopjakova</name>
    </author>
    <link href="http://arxiv.org/abs/2110.11504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.11504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.11520v1</id>
    <updated>2021-10-21T23:30:48Z</updated>
    <published>2021-10-21T23:30:48Z</published>
    <title>Power Saving Evaluation with Automatic Offloading</title>
    <summary>  Heterogeneous hardware other than small-core CPU such as GPU, FPGA, or
many-core CPU is increasingly being used. However, heterogeneous hardware usage
presents high technical skill barriers such as familiarity with CUDA. To
overcome this challenge, I previously proposed environment-adaptive software
that enables automatic conversion, automatic configuration, and
high-performance and low-power operation of once-written code, in accordance
with the hardware to be placed. I also previously verified performance
improvement of automatic GPU and FPGA offloading. In this paper, I verify
low-power operation with environment adaptation by evaluating power utilization
after automatic offloading. I compare Watt*seconds of existing applications
after automatic offloading with the case of CPU-only processing.
</summary>
    <author>
      <name>Yoji Yamato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures, The 8th IIAE International Conference on
  Intelligent Systems and Image Processing 2021 (ICISIP 2021), Sep. 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.11520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.11520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.12794v1</id>
    <updated>2021-10-25T10:47:05Z</updated>
    <published>2021-10-25T10:47:05Z</published>
    <title>Mixed precision in Graphics Processing Unit</title>
    <summary>  Modern graphics computing units (GPUs) are designed and optimized to perform
highly parallel numerical calculations. This parallelism has enabled (and
promises) significant advantages, both in terms of energy performance and
calculation. In this document, we take stock of the different applications of
mixed precision. We recall the standards currently used in the overwhelming
majority of systems in terms of numerical computation. We show that the mixed
precision which decreases the precision at the input of an operation does not
necessarily decrease the precision of its output. We show that this previous
principle allows its transposition into one of the branches that most needs
computing power: machine learning. The use of fixed point numbers and
half-precision are two very effective ways to increase the learning ability of
complex neural networks. Mixed precision still requires the use of suitable
hardware, failing which the calculation time could on the contrary be
lengthened. The NVIDIA Tensor Core that is found among others in their Tesla
V100 range, is an example of implementation at the hardware level of mixed
precision. On the other hand, by abandoning the traditional von Neumann model,
mixed precision can also be transposed to a lower level of abstraction, using
phase change memories.
</summary>
    <author>
      <name>Quentin Gallouédec</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">M.S dissertation</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.12794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.12794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.01369v1</id>
    <updated>2021-11-02T04:51:27Z</updated>
    <published>2021-11-02T04:51:27Z</published>
    <title>Wafer-level Variation Modeling for Multi-site RF IC Testing via
  Hierarchical Gaussian Process</title>
    <summary>  Wafer-level performance prediction has been attracting attention to reduce
measurement costs without compromising test quality in production tests.
Although several efficient methods have been proposed, the site-to-site
variation, which is often observed in multi-site testing for radio frequency
circuits, has not yet been sufficiently addressed. In this paper, we propose a
wafer-level performance prediction method for multi-site testing that can
consider the site-to-site variation. The proposed method is based on the
Gaussian process, which is widely used for wafer-level spatial correlation
modeling, improving the prediction accuracy by extending hierarchical modeling
to exploit the test site information provided by test engineers. In addition,
we propose an active test-site sampling method to maximize measurement cost
reduction. Through experiments using industrial production test data, we
demonstrate that the proposed method can reduce the estimation error to 1/19 of
that obtained using a conventional method. Moreover, we demonstrate that the
proposed sampling method can reduce the number of the measurements by 97% while
achieving sufficient estimation accuracy.
</summary>
    <author>
      <name>Michihiro Shintani</name>
    </author>
    <author>
      <name>Riaz-Ul-Haque Mian</name>
    </author>
    <author>
      <name>Tomoki Nakamura</name>
    </author>
    <author>
      <name>Masuo Kajiyama</name>
    </author>
    <author>
      <name>Makoto Eiki</name>
    </author>
    <author>
      <name>Michiko Inoue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.01369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.01369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.01948v1</id>
    <updated>2021-10-29T12:29:25Z</updated>
    <published>2021-10-29T12:29:25Z</published>
    <title>Design and implementation of an out-of-order execution engine of
  floating-point arithmetic operations</title>
    <summary>  In this thesis, work is undertaken towards the design in hardware description
languages and implementation in FPGA of an out-of-order execution engine of
floating-point arithmetic operations for the Lagarto II core. A first proposal
covers the design of a low power consumption issue queue for out-of-order
processors, register bank, bypass network, and the functional units for
addition/subtraction, multiplication, division/reciprocal, and Fused Multiply
Accumulate (FMAC) confirming with the IEEE-754 standard. The design supports
double-precision format and denormalized numbers; A second proposal is based on
a pair of FMAC as functional units which can perform almost all Floating-point
operations, this design is more beneficial in area, performance, and energy
efficiency compared with the first version.
</summary>
    <author>
      <name>Cristóbal Ramírez Lazo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Polytechnic University of Catalonia</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Master Thesis Link: https://upcommons.upc.edu/handle/2117/82655</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.01948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.01948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.07584v1</id>
    <updated>2021-11-15T08:00:03Z</updated>
    <published>2021-11-15T08:00:03Z</published>
    <title>Design and Evaluation Frameworks for Advanced RISC-based Ternary
  Processor</title>
    <summary>  In this paper, we introduce the design and verification frameworks for
developing a fully-functional emerging ternary processor. Based on the existing
compiling environments for binary processors, for the given ternary
instructions, the software-level framework provides an efficient way to convert
the given programs to the ternary assembly codes. We also present a
hardware-level framework to rapidly evaluate the performance of a ternary
processor implemented in arbitrary design technology. As a case study, the
fully-functional 9-trit advanced RISC-based ternary (ART-9) core is newly
developed by using the proposed frameworks. Utilizing 24 custom ternary
instructions, the 5-stage ART-9 prototype architecture is successfully verified
by a number of test programs including dhrystone benchmark in a ternary domain,
achieving the processing efficiency of 57.8 DMIPS/W and 3.06 x 10^6 DMIPS/W in
the FPGA-level ternary-logic emulations and the emerging CNTFET ternary gates,
respectively.
</summary>
    <author>
      <name>Dongyun Kam</name>
    </author>
    <author>
      <name>Jung Gyu Min</name>
    </author>
    <author>
      <name>Jongho Yoon</name>
    </author>
    <author>
      <name>Sunmean Kim</name>
    </author>
    <author>
      <name>Seokhyeong Kang</name>
    </author>
    <author>
      <name>Youngjoo Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to DATE 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.07584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.07584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.08848v2</id>
    <updated>2021-11-22T04:26:18Z</updated>
    <published>2021-11-17T00:36:08Z</published>
    <title>Enabling Automated FPGA Accelerator Optimization Using Graph Neural
  Networks</title>
    <summary>  High-level synthesis (HLS) has freed the computer architects from developing
their designs in a very low-level language and needing to exactly specify how
the data should be transferred in register-level. With the help of HLS, the
hardware designers must describe only a high-level behavioral flow of the
design. Despite this, it still can take weeks to develop a high-performance
architecture mainly because there are many design choices at a higher level
that requires more time to explore. It also takes several minutes to hours to
get feedback from the HLS tool on the quality of each design candidate. In this
paper, we propose to solve this problem by modeling the HLS tool with a graph
neural network (GNN) that is trained to be used for a wide range of
applications. The experimental results demonstrate that by employing the
GNN-based model, we are able to estimate the quality of design in milliseconds
with high accuracy which can help us search through the solution space very
quickly.
</summary>
    <author>
      <name>Atefeh Sohrabizadeh</name>
    </author>
    <author>
      <name>Yunsheng Bai</name>
    </author>
    <author>
      <name>Yizhou Sun</name>
    </author>
    <author>
      <name>Jason Cong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.08848v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.08848v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.09747v1</id>
    <updated>2021-11-18T15:20:36Z</updated>
    <published>2021-11-18T15:20:36Z</published>
    <title>Hamming Distance Tolerant Content-Addressable Memory (HD-CAM) for
  Approximate Matching Applications</title>
    <summary>  We propose a novel Hamming distance tolerant content-addressable memory
(HD-CAM) for energy-efficient in memory approximate matching applications.
HD-CAM implements approximate search using matchline charge redistribution
rather than its rise or fall time, frequently employed in state of-the-art
solutions. HD-CAM was designed in a 65 nm 1.2 V CMOS technology and evaluated
through extensive Monte Carlo simulations. Our analysis shows that HD-CAM
supports robust operation under significant process variations and changes in
the design parameters, enabling a wide range of mismatch threshold (tolerable
Hamming distance) levels and pattern lengths. HD-CAM was functionally evaluated
for virus DNA classification, which makes HD-CAM suitable for hardware
acceleration of genomic surveillance of viral outbreaks such as Covid-19
pandemics.
</summary>
    <author>
      <name>Esteban Garzón</name>
    </author>
    <author>
      <name>Roman Golman</name>
    </author>
    <author>
      <name>Zuher Jahshan</name>
    </author>
    <author>
      <name>Robert Hanhan</name>
    </author>
    <author>
      <name>Natan Vinshtok-Melnik</name>
    </author>
    <author>
      <name>Marco Lanuzza</name>
    </author>
    <author>
      <name>Adam Teman</name>
    </author>
    <author>
      <name>Leonid Yavits</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2022.3158305</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2022.3158305" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Access. 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2111.09747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.09747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.11744v2</id>
    <updated>2021-12-11T04:10:04Z</updated>
    <published>2021-11-23T09:32:54Z</published>
    <title>A Customized NoC Architecture to Enable Highly Localized
  Computing-On-the-Move DNN Dataflow</title>
    <summary>  The ever-increasing computation complexity of fastgrowing Deep Neural
Networks (DNNs) has requested new computing paradigms to overcome the memory
wall in conventional Von Neumann computing architectures. The emerging
Computing-In-Memory (CIM) architecture has been a promising candidate to
accelerate neural network computing. However, data movement between CIM arrays
may still dominate the total power consumption in conventional designs. This
paper proposes a flexible CIM processor architecture named Domino and
"Computing-On-the-Move" (COM) dataflow, to enable stream computing and local
data access to significantly reduce data movement energy. Meanwhile, Domino
employs customized distributed instruction scheduling within Network-on-Chip
(NoC) to implement inter-memory computing and attain mapping flexibility. The
evaluation with prevailing DNN models shows that Domino achieves
1.77-to-2.37$\times$ power efficiency over several state-of-the-art CIM
accelerators and improves the throughput by 1.28-to-13.16$\times$.
</summary>
    <author>
      <name>Kaining Zhou</name>
    </author>
    <author>
      <name>Yangshuo He</name>
    </author>
    <author>
      <name>Rui Xiao</name>
    </author>
    <author>
      <name>Jiayi Liu</name>
    </author>
    <author>
      <name>Kejie Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2107.09500</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.11744v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.11744v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.14252v1</id>
    <updated>2021-11-28T22:32:58Z</updated>
    <published>2021-11-28T22:32:58Z</published>
    <title>Search for Optimal Systolic Arrays: A Comprehensive Automated
  Exploration Framework and Lessons Learned</title>
    <summary>  Systolic arrays have been widely used for accelerating HPC and deep learning
applications. There is a plethora of previous works on the performance tuning
of systolic arrays, but usually based on a number of oversimplified assumptions
(e.g., only considering divisors for loop tiling, pruning based on off-chip
data communication) to reduce the design space.
  In this paper, we present a comprehensive design space exploration tool named
Odyssey for systolic array optimization. Odyssey does not rely on artificial
assumptions to limit the design space, and yet it is highly efficient and
scalable with a hybrid optimization technique. For example, for a
1024x1024x1024 matrix multiplication, it finds designs that reach 90% of the
optimal performance in 5 seconds with a single CPU thread. Moreover, using
Odyssey, we unveil and quantify the suboptimality introduced by multiple
commonly used oversimplifications in prior studies for systolic array design
space exploration. For example, Odyssey results show that limiting to divisors
for loop tiling leads to a 39% performance loss, and pruning based on off-chip
data movement results in a 45% performance loss. We applied Odyssey to explore
the architecture trade-offs for matrix multiplication and convolutional neural
network, providing inspiration into possible optimizations for these two
applications.
</summary>
    <author>
      <name>Jie Wang</name>
    </author>
    <author>
      <name>Jason Cong</name>
    </author>
    <link href="http://arxiv.org/abs/2111.14252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.14252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.14535v1</id>
    <updated>2021-11-29T13:56:52Z</updated>
    <published>2021-11-29T13:56:52Z</published>
    <title>Enabling Reusable Physical Design Flows with Modular Flow Generators</title>
    <summary>  Achieving high code reuse in physical design flows is challenging but
increasingly necessary to build complex systems. Unfortunately, existing
approaches based on parameterized Tcl generators support very limited reuse and
struggle to preserve reusable code as designers customize flows for specific
designs and technologies. We present a vision and framework based on modular
flow generators that encapsulates coarse-grain and fine-grain reusable code in
modular nodes and assembles them into complete flows. The key feature is a flow
consistency and instrumentation layer embedded in Python, which supports
mechanisms for rapid and early feedback on inconsistent composition. The
approach gradually types the Tcl language and allows both automatic and
user-annotated static assertion checks. We evaluate the design flows of
successive generations of silicon prototypes designed in TSMC16, TSMC28,
TSMC40, SKY130, and IBM180 technologies, showing how our approach can enable
significant code reuse in future flows.
</summary>
    <author>
      <name>Alex Carsello</name>
    </author>
    <author>
      <name>James Thomas</name>
    </author>
    <author>
      <name>Ankita Nayak</name>
    </author>
    <author>
      <name>Po-Han Chen</name>
    </author>
    <author>
      <name>Mark Horowitz</name>
    </author>
    <author>
      <name>Priyanka Raina</name>
    </author>
    <author>
      <name>Christopher Torng</name>
    </author>
    <link href="http://arxiv.org/abs/2111.14535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.14535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.15296v2</id>
    <updated>2021-12-14T13:23:52Z</updated>
    <published>2021-11-30T11:19:39Z</published>
    <title>BrainScaleS Large Scale Spike Communication using Extoll</title>
    <summary>  The BrainScaleS Neuromorphic Computing System is currently connected to a
compute cluster via Gigabit-Ethernet network technology. This is convenient for
the currently used experiment mode, where neuronal networks cover at most one
wafer module. When modelling networks of larger size, as for example a full
sized cortical microcircuit model, one has to think about connecting neurons
across wafer modules to larger networks. This can be done, using the Extoll
networking technology, which provides high bandwidth and low latencies, as well
as a low overhead packet protocol format.
</summary>
    <author>
      <name>Tobias Thommes</name>
    </author>
    <author>
      <name>Niels Buwen</name>
    </author>
    <author>
      <name>Andreas Grübl</name>
    </author>
    <author>
      <name>Eric Müller</name>
    </author>
    <author>
      <name>Ulrich Brüning</name>
    </author>
    <author>
      <name>Johannes Schemmel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, submitted to the Neuro Inspired Computational
  Elements 2020 (NICE'2020) conference, accepted and presented as a poster in
  March 2021; 1st replacement: add acknowledgement of DFG (German Research
  Foundation)</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.15296v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.15296v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.15502v1</id>
    <updated>2021-11-30T15:38:35Z</updated>
    <published>2021-11-30T15:38:35Z</published>
    <title>Popcorns-Pro: A Cooperative Network-Server Approach for Data Center
  Energy Optimization</title>
    <summary>  Data centers have become a popular computing platform for various
applications, and they account for nearly 2% of total US energy consumption.
Therefore, it has become important to optimize data center power, and reduce
their energy footprint. Most existing work optimizes power in servers and
networks independently and does not address them together in a holistic fashion
that has the potential to achieve greater power savings. In this article, we
present PopcornsPro, a cooperative server network framework for energy
optimization. We present a comprehensive power model for heterogeneous data
center switches along with low power mode designs in combination with the
server power model. We design job scheduling algorithms that place tasks onto
servers in a power-aware manner, such that servers and network switches can
take effective advantage of low power state and available network link
capacities. Our experimental results show that we are able to achieve
significantly higher savings up to 80% compared to the previously well-known
server and network power optimization policies.
</summary>
    <author>
      <name>Sai Santosh Dayapule</name>
    </author>
    <author>
      <name>Kathy Nguyen</name>
    </author>
    <author>
      <name>Gregory Kahl</name>
    </author>
    <author>
      <name>Suresh Subramaniam</name>
    </author>
    <author>
      <name>Guru Venkataramani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presubmission</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.15502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.15502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.00387v1</id>
    <updated>2021-12-01T10:12:35Z</updated>
    <published>2021-12-01T10:12:35Z</published>
    <title>How Parallel Circuit Execution Can Be Useful for NISQ Computing?</title>
    <summary>  Quantum computing is performed on Noisy Intermediate-Scale Quantum (NISQ)
hardware in the short term. Only small circuits can be executed reliably on a
quantum machine due to the unavoidable noisy quantum operations on NISQ
devices, leading to the under-utilization of hardware resources. With the
growing demand to access quantum hardware, how to utilize it more efficiently
while maintaining output fidelity is becoming a timely issue. A parallel
circuit execution technique has been proposed to address this problem by
executing multiple programs on hardware simultaneously. It can improve the
hardware throughput and reduce the overall runtime. However, accumulative
noises such as crosstalk can decrease the output fidelity in parallel workload
execution. In this paper, we first give an in-depth overview of stateof-the-art
parallel circuit execution methods. Second, we propose a Quantum
Crosstalk-aware Parallel workload execution method (QuCP) without the overhead
of crosstalk characterization. Third, we investigate the trade-off between
hardware throughput and fidelity loss to explore the hardware limitation with
parallel circuit execution. Finally, we apply parallel circuit execution to VQE
and zero-noise extrapolation error mitigation method to showcase its various
applications on advancing NISQ computing.
</summary>
    <author>
      <name>Siyuan Niu</name>
    </author>
    <author>
      <name>Aida Todri-Sanial</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2022 Design, Automation &amp; Test in Europe Conference &amp; Exhibition
  (DATE), Mar 2022, ANTWERP, Belgium</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.00387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.00387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.02204v2</id>
    <updated>2023-01-10T21:20:38Z</updated>
    <published>2021-12-04T00:45:40Z</published>
    <title>Violet: Architecturally Exposed Orchestration, Movement, and Placement
  for Generalized Deep Learning</title>
    <summary>  Deep learning and hardware for it has garnered immense academic and industry
interest in the past 5 years, with many novel proposals. However, the
state-of-art remains NVIDIA's TensorCore-based systems that provide top-of-line
performance and coverage across a wide-spectrum of deep learning applications.
In this paper, we first identify four key problems any new DL solution must
solve: 1) Data orchestration, 2) Data movement, 3) Work placement and blending
these to achieve 4) Coverage across different types of DL applications. With
this as a guide, we propose Violet, a novel architecture with roots in
multicore SIMD which balances the responsibilities for these four problems
between the architecture, microarchitecture and software stack. Compared to the
NVIDIA A100 GPU, we find Violet achieves geo-mean 2.4X/10.6X and 2.1X/9.5X
performance/efficiency for inference and training across the MLPerf benchmark
suite. We present detailed operator-level analysis of the MLPerf benchmark
suite, extracting out key behaviors - with implications for architecture
research beyond this paper, that underpin the speedup and efficiency. Overall,
this paper motivates the importance of balance, that the break down of
responsibilities must be thought through carefully in order to compete with
incumbent architecture designs.
</summary>
    <author>
      <name>Michael Davies</name>
    </author>
    <author>
      <name>Karthikeyan Sankaralingam</name>
    </author>
    <link href="http://arxiv.org/abs/2112.02204v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.02204v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.02263v1</id>
    <updated>2021-12-04T06:48:32Z</updated>
    <published>2021-12-04T06:48:32Z</published>
    <title>On the Implementation of Fixed-point Exponential Function for Machine
  Learning and Signal Processing Accelerators</title>
    <summary>  The natural exponential function is widely used in modeling many engineering
and scientific systems. It is also an integral part of many neural network
activation function such as sigmoid, tanh, ELU, RBF etc. Dedicated hardware
accelerator and processors are designed for faster execution of such
applications. Such accelerators can immensely benefit from an optimal
implementation of exponential function. This can be achieved for most
applications with the knowledge that the exponential function for a negative
domain is more widely used than the positive domain. This paper presents an
optimized implementation of exponential function for variable precision fixed
point negative input. The implementation presented here significantly reduces
the number of multipliers and adders. This is further optimized using mixed
world-length implementation for the series expansion. The reduction in area and
power consumption is more than 30% and 50% respectively over previous
equivalent method.
</summary>
    <author>
      <name>Mahesh Chandra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MDAT.2021.3133373</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MDAT.2021.3133373" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">-</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Design and Test, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2112.02263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.02263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.04136v1</id>
    <updated>2021-12-08T06:22:30Z</updated>
    <published>2021-12-08T06:22:30Z</published>
    <title>SeaPlace: Process Variation Aware Placement for Reliable Combinational
  Circuits against SETs and METs</title>
    <summary>  Nowadays nanoscale combinational circuits are facing significant reliability
challenges including soft errors and process variations. This paper presents
novel process variation-aware placement strategies that include two algorithms
to increase the reliability of combinational circuits against both Single Event
Transients (SETs) and Multiple Event Transients (METs). The first proposed
algorithm is a global placement method (called SeaPlace-G) that places the
cells for hardening the circuit against SETs by solving a quadratic
formulation. Afterwards, a detailed placement algorithm (named SeaPlace-D) is
proposed to increase the circuit reliability against METs by solving a linear
programming optimization problem. Experimental results show that SeaPlace-G and
SeaPlace-D averagely achieve 41.78% and 32.04% soft error reliability
improvement against SET and MET, respectively. Moreover, when SeaPlace-D is
followed by SeaPlace-G, MET reduction can be improved by up to 53.3%.
</summary>
    <author>
      <name>Kiarash Saremi</name>
    </author>
    <author>
      <name>Hossein Pedram</name>
    </author>
    <author>
      <name>Behnam Ghavami</name>
    </author>
    <author>
      <name>Mohsen Raji</name>
    </author>
    <author>
      <name>Zhenman Fang</name>
    </author>
    <author>
      <name>Lesley Shannon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.04136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.04136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.07019v3</id>
    <updated>2023-01-24T09:19:17Z</updated>
    <published>2021-12-13T21:14:35Z</published>
    <title>Synapse Compression for Event-Based Convolutional-Neural-Network
  Accelerators</title>
    <summary>  Manufacturing-viable neuromorphic chips require novel computer architectures
to achieve the massively parallel and efficient information processing the
brain supports so effortlessly. Emerging event-based architectures are making
this dream a reality. However, the large memory requirements for synaptic
connectivity are a showstopper for the execution of modern convolutional neural
networks (CNNs) on massively parallel, event-based (spiking) architectures.
This work overcomes this roadblock by contributing a lightweight hardware
scheme to compress the synaptic memory requirements by several thousand times,
enabling the execution of complex CNNs on a single chip of small form factor. A
silicon implementation in a 12-nm technology shows that the technique increases
the system's implementation cost by only 2%, despite achieving a total
memory-footprint reduction of up to 374x compared to the best previously
published technique.
</summary>
    <author>
      <name>Lennart Bamberg</name>
    </author>
    <author>
      <name>Arash Pourtaherian</name>
    </author>
    <author>
      <name>Luc Waeijen</name>
    </author>
    <author>
      <name>Anupam Chahar</name>
    </author>
    <author>
      <name>Orlando Moreira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint accepted by the IEEE Transactions on Parallel and
  Distributed Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.07019v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.07019v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.07789v1</id>
    <updated>2021-12-14T23:28:57Z</updated>
    <published>2021-12-14T23:28:57Z</published>
    <title>FLOWER: A comprehensive dataflow compiler for high-level synthesis</title>
    <summary>  FPGAs have found their way into data centers as accelerator cards, making
reconfigurable computing more accessible for high-performance applications. At
the same time, new high-level synthesis compilers like Xilinx Vitis and runtime
libraries such as XRT attract software programmers into the reconfigurable
domain. While software programmers are familiar with task-level and
data-parallel programming, FPGAs often require different types of parallelism.
For example, data-driven parallelism is mandatory to obtain satisfactory
hardware designs for pipelined dataflow architectures. However, software
programmers are often not acquainted with dataflow architectures - resulting in
poor hardware designs.
  In this work we present FLOWER, a comprehensive compiler infrastructure that
provides automatic canonical transformations for high-level synthesis from a
domain-specific library. This allows programmers to focus on algorithm
implementations rather than low-level optimizations for dataflow architectures.
We show that FLOWER allows to synthesize efficient implementations for
high-performance streaming applications targeting System-on-Chip and FPGA
accelerator cards, in the context of image processing and computer vision.
</summary>
    <author>
      <name>Puya Amiri</name>
    </author>
    <author>
      <name>Arsène Pérard-Gayot</name>
    </author>
    <author>
      <name>Richard Membarth</name>
    </author>
    <author>
      <name>Philipp Slusallek</name>
    </author>
    <author>
      <name>Roland Leißa</name>
    </author>
    <author>
      <name>Sebastian Hack</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICFPT52863.2021.9609930</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICFPT52863.2021.9609930" rel="related"/>
    <link href="http://arxiv.org/abs/2112.07789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.07789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.09320v1</id>
    <updated>2021-12-17T05:00:32Z</updated>
    <published>2021-12-17T05:00:32Z</published>
    <title>Gate-Level Static Approximate Adders</title>
    <summary>  This work compares and analyzes static approximate adders which are suitable
for FPGA and ASIC type implementations. We consider many static approximate
adders and evaluate their performance with respect to a digital image
processing application using standard figures of merit such as peak signal to
noise ratio and structural similarity index metric. We provide the error
metrics of approximate adders, and the design metrics of accurate and
approximate adders corresponding to FPGA and ASIC type implementations. For the
FPGA implementation, we considered a Xilinx Artix-7 FPGA, and for an ASIC type
implementation, we considered a 32-28 nm CMOS standard digital cell library.
While the inferences from this work could serve as a useful reference to
determine an optimum static approximate adder for a practical application, in
particular, we found approximate adders HOAANED, HERLOA and M-HERLOA to be
preferable.
</summary>
    <author>
      <name>P Balasubramanian</name>
    </author>
    <author>
      <name>R Nayar</name>
    </author>
    <author>
      <name>D L Maskell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.09320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.09320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.10486v1</id>
    <updated>2021-12-20T12:42:13Z</updated>
    <published>2021-12-20T12:42:13Z</published>
    <title>Dijkstra-Through-Time: Ahead of time hardware scheduling method for
  deterministic workloads</title>
    <summary>  Most of the previous works on data flow optimizations for Machine Learning
hardware accelerators try to find algorithmic re-factorization such as
loop-reordering and loop-tiling. However, the analysis and information they
provide are still at very high level and one must further map them onto
instructions that hardware can understand. This paper presents
"Dijkstra-Through-Time" (DTT), an ahead of time compute and memory
scheduling-mapping algorithm for deterministic workloads. It provides a simple
implementation and supports accelerators with complex NoC configurations, at
the expense of a long compilation process. This initial paper illustrates a
proof of concept implementation to merge scheduling and data cache coherence
mechanisms to get more optimized data flows.
</summary>
    <author>
      <name>Vincent Tableau Roche</name>
    </author>
    <author>
      <name>Purushotham Murugappa Velayuthan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper contains 7 pages and 10 figures. It is the result of the
  work performed during an internship at Nokia Bell Labs (Antwerp) in 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.10486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.10486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.12630v1</id>
    <updated>2021-12-23T15:15:47Z</updated>
    <published>2021-12-23T15:15:47Z</published>
    <title>A Survey of Near-Data Processing Architectures for Neural Networks</title>
    <summary>  Data-intensive workloads and applications, such as machine learning (ML), are
fundamentally limited by traditional computing systems based on the von-Neumann
architecture. As data movement operations and energy consumption become key
bottlenecks in the design of computing systems, the interest in unconventional
approaches such as Near-Data Processing (NDP), machine learning, and especially
neural network (NN)-based accelerators has grown significantly. Emerging memory
technologies, such as ReRAM and 3D-stacked, are promising for efficiently
architecting NDP-based accelerators for NN due to their capabilities to work as
both: High-density/low-energy storage and in/near-memory computation/search
engine. In this paper, we present a survey of techniques for designing NDP
architectures for NN. By classifying the techniques based on the memory
technology employed, we underscore their similarities and differences. Finally,
we discuss open challenges and future perspectives that need to be explored in
order to improve and extend the adoption of NDP architectures for future
computing platforms. This paper will be valuable for computer architects, chip
designers and researchers in the area of machine learning.
</summary>
    <author>
      <name>Mehdi Hassanpour</name>
    </author>
    <author>
      <name>Marc Riera</name>
    </author>
    <author>
      <name>Antonio González</name>
    </author>
    <link href="http://arxiv.org/abs/2112.12630v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.12630v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.13150v1</id>
    <updated>2021-12-24T22:34:51Z</updated>
    <published>2021-12-24T22:34:51Z</published>
    <title>Fast 2D Convolutions and Cross-Correlations Using Scalable Architectures</title>
    <summary>  The manuscript describes fast and scalable architectures and associated
algorithms for computing convolutions and cross-correlations. The basic idea is
to map 2D convolutions and cross-correlations to a collection of 1D
convolutions and cross-correlations in the transform domain. This is
accomplished through the use of the Discrete Periodic Radon Transform (DPRT)
for general kernels and the use of SVD-LU decompositions for low-rank kernels.
The approach uses scalable architectures that can be fitted into modern FPGA
and Zynq-SOC devices. Based on different types of available resources, for
$P\times P$ blocks, 2D convolutions and cross-correlations can be computed in
just $O(P)$ clock cycles up to $O(P^2)$ clock cycles. Thus, there is a
trade-off between performance and required numbers and types of resources. We
provide implementations of the proposed architectures using modern programmable
devices (Virtex-7 and Zynq-SOC). Based on the amounts and types of required
resources, we show that the proposed approaches significantly outperform
current methods.
</summary>
    <author>
      <name>Cesar Carranza</name>
    </author>
    <author>
      <name>Daniel Llamocca</name>
    </author>
    <author>
      <name>Marios Pattichis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2017.2678799</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2017.2678799" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper develops the fastest known methods for computing 2D
  convolutions in hardware</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Image Processing 26.5 (2017): 2230-2245</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2112.13150v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.13150v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.13157v1</id>
    <updated>2021-12-24T23:11:27Z</updated>
    <published>2021-12-24T23:11:27Z</published>
    <title>A Parallel SystemC Virtual Platform for Neuromorphic Architectures</title>
    <summary>  With the increasing interest in neuromorphic computing, designers of embedded
systems face the challenge of efficiently simulating such platforms to enable
architecture design exploration early in the development cycle. Executing
artificial neural network applications on neuromorphic systems which are being
simulated on virtual platforms (VPs) is an extremely demanding computational
task. Nevertheless, it is a vital benchmarking task for comparing different
possible architectures. Therefore, exploiting the multicore capabilities of the
VP's host system is essential to achieve faster simulations. Hence, this paper
presents a parallel SystemC based VP for RISC-V multicore platforms integrating
multiple computing-in-memory neuromorphic accelerators. In this paper,
different VP segmentation architectures are explored for the integration of
neuromorphic accelerators and are shown their corresponding speedup simulations
compared to conventional sequential SystemC execution.
</summary>
    <author>
      <name>Melvin Galicia</name>
    </author>
    <author>
      <name>Farhad Merchant</name>
    </author>
    <author>
      <name>Rainer Leupers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at 23rd International Symposium on Quality Electronic Design
  (ISQED'22)</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.13157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.13157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.14911v1</id>
    <updated>2021-12-30T03:44:27Z</updated>
    <published>2021-12-30T03:44:27Z</published>
    <title>A Survey of Deep Learning Techniques for Dynamic Branch Prediction</title>
    <summary>  Branch prediction is an architectural feature that speeds up the execution of
branch instruction on pipeline processors and reduces the cost of branching.
Recent advancements of Deep Learning (DL) in the post Moore's Law era is
accelerating areas of automated chip design, low-power computer architectures,
and much more. Traditional computer architecture design and algorithms could
benefit from dynamic predictors based on deep learning algorithms which learns
from experience by optimizing its parameters on large number of data. In this
survey paper, we focus on traditional branch prediction algorithms, analyzes
its limitations, and presents a literature survey of how deep learning
techniques can be applied to create dynamic branch predictors capable of
predicting conditional branch instructions. Prior surveys in this field focus
on dynamic branch prediction techniques based on neural network perceptrons. We
plan to improve the survey based on latest research in DL and advanced Machine
Learning (ML) based branch predictors.
</summary>
    <author>
      <name>Rinu Joseph</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Survey paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.14911v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.14911v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.14952v1</id>
    <updated>2021-12-30T07:36:11Z</updated>
    <published>2021-12-30T07:36:11Z</published>
    <title>A Survey of fault mitigation techniques for multi-core architectures</title>
    <summary>  Fault tolerance in multi-core architecture has attracted attention of
research community for the past 20 years. Rapid improvements in the CMOS
technology resulted in exponential growth of transistor density. It resulted in
increased challenges for designing resilient multi-core architecture at the
same pace. The article presents a survey of fault tolerant methods like fault
detection, recovery, re-configurability and repair techniques for multi-core
architectures. Salvaging at micro-architectural and architectural level are
also discussed. Gamut of fault tolerant approaches discussed in this article
have tangible improvements on the reliability of the multi-core architectures.
Every concept in the seminal articles is examined with respect to relevant
metrics like performance cost, area overhead, fault coverage, level of
protection, detection latency and Mean Time To Failure. The existing literature
is critically examined. New research directions in the form of new fault
tolerant design alternatives for both homogeneous and heterogeneous multi-core
architectures are presented. Brief on an analytical approach for fault
tolerating model is suggested for Intel and AMD based modern homogeneous
multi-core architecture are presented to enhance the understanding of the
readers about the architecture with respect to performance degradation, memory
access time and execution time.
</summary>
    <author>
      <name>Shashikiran Venkatesha</name>
    </author>
    <author>
      <name>Ranjani Parthasarathi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.14952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.14952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.15176v1</id>
    <updated>2021-12-30T18:57:35Z</updated>
    <published>2021-12-30T18:57:35Z</published>
    <title>Comparing different solutions for testing resistive defects in low-power
  SRAMs</title>
    <summary>  Low-power SRAM architectures are especially sensitive to many types of
defects that may occur during manufacturing. Among these, resistive defects can
appear. This paper analyzes some types of such defects that may impair the
device functionalities in subtle ways, depending on the defect characteristics,
and that may not be directly or easily detectable by traditional test methods,
such as March algorithms. We analyze different methods to test such defects and
discuss them in terms of complexity and test time.
</summary>
    <author>
      <name>Nunzio Mirabella</name>
    </author>
    <author>
      <name>Michelangelo Grosso</name>
    </author>
    <author>
      <name>Giovanna Franchino</name>
    </author>
    <author>
      <name>Salvatore Rinaudo</name>
    </author>
    <author>
      <name>Ioannis Deretzis</name>
    </author>
    <author>
      <name>Antonino La Magna</name>
    </author>
    <author>
      <name>Matteo Sonza Reorda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted and presented in The 22nd IEEE Latin-American Test
  Symposium (LATS 2021) October 27 - 29, 2021, Brazil. It is going to be
  published in the IEEExplorer. 6 pages, 7 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.15176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.15176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.15443v1</id>
    <updated>2021-12-28T17:04:06Z</updated>
    <published>2021-12-28T17:04:06Z</published>
    <title>FPGA Based Accelerator for Neural Networks Computation with Flexible
  Pipelining</title>
    <summary>  FPGA is appropriate for fix-point neural networks computing due to high power
efficiency and configurability. However, its design must be intensively refined
to achieve high performance using limited hardware resources. We present an
FPGA-based neural networks accelerator and its optimization framework, which
can achieve optimal efficiency for various CNN models and FPGA resources.
Targeting high throughput, we adopt layer-wise pipeline architecture for higher
DSP utilization. To get the optimal performance, a flexible algorithm to
allocate balanced hardware resources to each layer is also proposed, supported
by activation buffer design. Through our well-balanced implementation of four
CNN models on ZC706, the DSP utilization and efficiency are over 90%. For VGG16
on ZC706, the proposed accelerator achieves the performance of 2.58x, 1.53x and
1.35x better than the referenced non-pipeline architecture [1], pipeline
architecture [2] and [3], respectively.
</summary>
    <author>
      <name>Qingyang Yi</name>
    </author>
    <author>
      <name>Heming Sun</name>
    </author>
    <author>
      <name>Masahiro Fujita</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.15443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.15443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.00158v4</id>
    <updated>2022-12-01T03:11:27Z</updated>
    <published>2022-03-01T00:26:31Z</published>
    <title>GROW: A Row-Stationary Sparse-Dense GEMM Accelerator for
  Memory-Efficient Graph Convolutional Neural Networks</title>
    <summary>  Graph convolutional neural networks (GCNs) have emerged as a key technology
in various application domains where the input data is relational. A unique
property of GCNs is that its two primary execution stages, aggregation and
combination, exhibit drastically different dataflows. Consequently, prior GCN
accelerators tackle this research space by casting the aggregation and
combination stages as a series of sparse-dense matrix multiplication. However,
prior work frequently suffers from inefficient data movements, leaving
significant performance left on the table. We present GROW, a GCN accelerator
based on Gustavson's algorithm to architect a row-wise product based
sparse-dense GEMM accelerator. GROW co-designs the software/hardware that
strikes a balance in locality and parallelism for GCNs, achieving significant
energy-efficiency improvements vs. state-of-the-art GCN accelerators.
</summary>
    <author>
      <name>Ranggi Hwang</name>
    </author>
    <author>
      <name>Minhoo Kang</name>
    </author>
    <author>
      <name>Jiwon Lee</name>
    </author>
    <author>
      <name>Dongyun Kam</name>
    </author>
    <author>
      <name>Youngjoo Lee</name>
    </author>
    <author>
      <name>Minsoo Rhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at the 29th IEEE International Symposium on
  High-Performance Computer Architecture (HPCA), 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.00158v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.00158v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.02500v1</id>
    <updated>2022-03-04T18:56:13Z</updated>
    <published>2022-03-04T18:56:13Z</published>
    <title>Efficient Analog CAM Design</title>
    <summary>  Content Addressable Memories (CAMs) are considered a key-enabler for
in-memory computing (IMC). IMC shows order of magnitude improvement in energy
efficiency and throughput compared to traditional computing techniques.
Recently, analog CAMs (aCAMs) were proposed as a means to improve storage
density and energy efficiency. In this work, we propose two new aCAM cells to
improve data encoding and robustness as compared to existing aCAM cells. We
propose a methodology to choose the margin and interval width for data
encoding. In addition, we perform a comprehensive comparison against prior work
in terms of the number of intervals, noise sensitivity, dynamic range, energy,
latency, area, and probability of failure.
</summary>
    <author>
      <name>Jinane Bazzi</name>
    </author>
    <author>
      <name>Jana Sweidan</name>
    </author>
    <author>
      <name>Mohammed E. Fouda</name>
    </author>
    <author>
      <name>Rouwaida Kanj</name>
    </author>
    <author>
      <name>Ahmed M. Eltawil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a revised manuscript that is under consideration for
  publication at IEEE TCAS-I</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.02500v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.02500v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.04058v4</id>
    <updated>2022-09-11T20:10:42Z</updated>
    <published>2022-03-08T12:55:24Z</published>
    <title>A Fast Hardware Pseudorandom Number Generator Based on xoroshiro128</title>
    <summary>  The Graphcore Intelligence Processing Unit contains an original pseudorandom
number generator (PRNG) called xoroshiro128aox, based on the F2-linear
generator xoroshiro128. It is designed to be cheap to implement in hardware and
provide high-quality statistical randomness. In this paper, we present a
rigorous assessment of the generator's quality using standard statistical test
suites and compare the results with the fast contemporary PRNGs xoroshiro128+,
pcg64 and philox4x32-10. We show that xoroshiro128aox mitigates the known
weakness in the lower order bits of xoroshiro128+ with a new 'AOX' output
function by passing the BigCrush and PractRand suites, but we note that the
function has some minor non uniformities. We focus our testing with specific
tests for linear artefacts to highlight the weaknesses of both xoroshiro128
PRNGs, but conclude that they are hard to detect, and xoroshiro128aox otherwise
provides a good trade off between statistical quality and hardware
implementation cost.
</summary>
    <author>
      <name>James Hanlon</name>
    </author>
    <author>
      <name>Stephen Felix</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in IEEE Transactions on Computers</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.04058v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.04058v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.04649v1</id>
    <updated>2022-03-09T11:28:31Z</updated>
    <published>2022-03-09T11:28:31Z</published>
    <title>ArithsGen: Arithmetic Circuit Generator for Hardware Accelerators</title>
    <summary>  Generators of arithmetic circuits can automatically deliver various
implementations of arithmetic circuits that show different tradeoffs between
the key circuit parameters (delay, area, power consumption). However, existing
(freely-)available generators are limited if more complex circuits with a
hierarchical structure and additional architecture optimization are requested.
Furthermore, they support only a few output formats. In order to overcome the
above-mentioned limitations, we developed a new generator of arithmetic
circuits called ArithsGen. ArithsGen can generate specific architectures of
signed and unsigned adders and multipliers using basic building elements such
as wires and gates. Compared to existing generators, the user can, for example,
specify the type of adders used in multipliers. The tool supports various
outputs formats (Verilog, BLIF, C/C++, or integer netlists). ArithsGen was
evaluated in the synthesis and optimization of generic customizable accurate
and approximate adders and multipliers. Furthermore, we used the circuits
generated by ArithsGen as seeds for a tool developed to automatically create
approximate implementations of arithmetic circuits. We show that different
initial circuits (generated by ArithsGen) significantly impact the properties
of these approximate implementations. The tool is available online at
https://github.com/ehw-fit/ariths-gen.
</summary>
    <author>
      <name>Jan Klhufek</name>
    </author>
    <author>
      <name>Vojtech Mrazek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DDECS54261.2022.9770152</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DDECS54261.2022.9770152" rel="related"/>
    <link href="http://arxiv.org/abs/2203.04649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.04649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.07516v1</id>
    <updated>2022-03-14T21:52:04Z</updated>
    <published>2022-03-14T21:52:04Z</published>
    <title>Skydiver: A Spiking Neural Network Accelerator Exploiting
  Spatio-Temporal Workload Balance</title>
    <summary>  Spiking Neural Networks (SNNs) are developed as a promising alternative to
Artificial Neural networks (ANNs) due to their more realistic brain-inspired
computing models. SNNs have sparse neuron firing over time, i.e.,
spatio-temporal sparsity; thus, they are useful to enable energy-efficient
hardware inference. However, exploiting spatio-temporal sparsity of SNNs in
hardware leads to unpredictable and unbalanced workloads, degrading the energy
efficiency. In this work, we propose an FPGA-based convolutional SNN
accelerator called Skydiver that exploits spatio-temporal workload balance. We
propose the Approximate Proportional Relation Construction (APRC) method that
can predict the relative workload channel-wisely and a Channel-Balanced
Workload Schedule (CBWS) method to increase the hardware workload balance ratio
to over 90%. Skydiver was implemented on a Xilinx XC7Z045 FPGA and verified on
image segmentation and MNIST classification tasks. Results show improved
throughput by 1.4X and 1.2X for the two tasks. Skydiver achieved 22.6 KFPS
throughput, and 42.4 uJ/Image prediction energy on the classification task with
98.5% accuracy.
</summary>
    <author>
      <name>Qinyu Chen</name>
    </author>
    <author>
      <name>Chang Gao</name>
    </author>
    <author>
      <name>Xinyuan Fang</name>
    </author>
    <author>
      <name>Haitao Luan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCAD.2022.3158834</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCAD.2022.3158834" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to be published in the IEEE Transactions on Computer-Aided
  Design of Integrated Circuits and Systems, 2022</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Computer-Aided Design of Integrated Circuits
  and Systems, 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2203.07516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.07516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.08011v1</id>
    <updated>2022-03-15T15:47:59Z</updated>
    <published>2022-03-15T15:47:59Z</published>
    <title>Approximate Decision Trees For Machine Learning Classification on Tiny
  Printed Circuits</title>
    <summary>  Although Printed Electronics (PE) cannot compete with silicon-based systems
in conventional evaluation metrics, e.g., integration density, area and
performance, PE offers attractive properties such as on-demand ultra-low-cost
fabrication, flexibility and non-toxicity. As a result, it targets application
domains that are untouchable by lithography-based silicon electronics and thus
have not yet seen much proliferation of computing. However, despite the
attractive characteristics of PE, the large feature sizes in PE prohibit the
realization of complex printed circuits, such as Machine Learning (ML)
classifiers. In this work, we exploit the hardware-friendly nature of Decision
Trees for machine learning classification and leverage the hardware-efficiency
of the approximate design in order to generate approximate ML classifiers that
are suitable for tiny, ultra-resource constrained, and battery-powered printed
applications.
</summary>
    <author>
      <name>Konstantinos Balaskas</name>
    </author>
    <author>
      <name>Georgios Zervakis</name>
    </author>
    <author>
      <name>Kostas Siozios</name>
    </author>
    <author>
      <name>Mehdi B. Tahoori</name>
    </author>
    <author>
      <name>Joerg Henkel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the 23rd International Symposium on Quality Electronic
  Design (ISQED'22), April 6-7, 2022 (Virtual Conference)</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.08011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.08011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.09665v1</id>
    <updated>2022-03-18T00:21:32Z</updated>
    <published>2022-03-18T00:21:32Z</published>
    <title>A Cost-Efficient Look-Up Table Based Binary Coded Decimal Adder Design</title>
    <summary>  The BCD (Binary Coded Decimal) being the more accurate and human-readable
representation with ease of conversion, is prevailing in the computing and
electronic communication.In this paper, a tree-structured parallel BCD addition
algorithm is proposed with the reduced time complexity. BCD adder is more
effective with a LUT (Look-Up Table)-based design, due to FPGA (Field
Programmable Gate Array) technology's enumerable benefits and applications. A
size-minimal and depth-minimal LUT-based BCD adder circuit construction is the
main contribution of this paper.
</summary>
    <author>
      <name>Zarrin Tasnim Sworna</name>
    </author>
    <author>
      <name>Mubin Ul Haque</name>
    </author>
    <author>
      <name>Hafiz Md. Hasan Babu</name>
    </author>
    <author>
      <name>Lafifa Jamal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Sworna, Z.T., Haque, M.U., Babu, H.M.H. and Jamal, L., 2017. A
  cost-efficient look-up table based binary coded decimal adder design. In IEEE
  FTC (pp. 874-882)</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.09665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.09665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.10359v3</id>
    <updated>2022-08-21T21:00:41Z</updated>
    <published>2022-03-19T17:21:01Z</published>
    <title>FPGA-extended General Purpose Computer Architecture</title>
    <summary>  This paper introduces a computer architecture, where part of the instruction
set architecture (ISA) is implemented on small highly-integrated
field-programmable gate arrays (FPGAs). Small FPGAs inside a general-purpose
processor (CPU) can be used effectively to implement custom or standardised
instructions. Our proposed architecture directly address related challenges for
high-end CPUs, where such highly-integrated FPGAs would have the highest
impact, such as on main memory bandwidth. This also enables
software-transparent context-switching. The simulation-based evaluation of a
dynamically reconfigurable core shows promising results approaching the
performance of an equivalent core with all enabled instructions. Finally, the
feasibility of adopting the proposed architecture in today's CPUs is studied
through the prototyping of fast-reconfigurable FPGAs and studying the miss
behaviour of opcodes.
</summary>
    <author>
      <name>Philippos Papaphilippou</name>
    </author>
    <author>
      <name>Myrtle Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the 18th International Symposium on Applied
  Reconfigurable Computing (ARC) 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.10359v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.10359v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.11540v1</id>
    <updated>2022-03-22T08:46:11Z</updated>
    <published>2022-03-22T08:46:11Z</published>
    <title>Scale-out Systolic Arrays</title>
    <summary>  Multi-pod systolic arrays are emerging as the architecture of choice in DNN
inference accelerators. Despite their potential, designing multi-pod systolic
arrays to maximize effective throughput/Watt (i.e., throughput/Watt adjusted
when accounting for array utilization) poses a unique set of challenges. In
this work, we study three key pillars in multi-pod systolic array designs,
namely array granularity, interconnect, and tiling. We identify optimal array
granularity across workloads and show that state-of-the-art commercial
accelerators use suboptimal array sizes for single-tenancy workloads. We, then
evaluate the bandwidth/latency trade-offs in interconnects and show that
Butterfly networks offer a scalable topology for accelerators with a large
number of pods. Finally, we introduce a novel data tiling scheme with custom
partition size to maximize utilization in optimally sized pods. We propose
Scale-out Systolic Arrays, a multi-pod inference accelerator for both single-
and multi-tenancy based on these three pillars. We show that SOSA exhibits
scaling of up to 600 TeraOps/s in effective throughput for state-of-the-art DNN
inference workloads, and outperforms state-of-the-art multi-pod accelerators by
a factor of 1.5x.
</summary>
    <author>
      <name>Ahmet Caner Yüzügüler</name>
    </author>
    <author>
      <name>Canberk Sönmez</name>
    </author>
    <author>
      <name>Mario Drumond</name>
    </author>
    <author>
      <name>Yunho Oh</name>
    </author>
    <author>
      <name>Babak Falsafi</name>
    </author>
    <author>
      <name>Pascal Frossard</name>
    </author>
    <link href="http://arxiv.org/abs/2203.11540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.11540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.12268v4</id>
    <updated>2024-04-09T10:56:35Z</updated>
    <published>2022-03-23T08:30:30Z</published>
    <title>Chiplet Actuary: A Quantitative Cost Model and Multi-Chiplet
  Architecture Exploration</title>
    <summary>  Multi-chip integration is widely recognized as the extension of Moore's Law.
Cost-saving is a frequently mentioned advantage, but previous works rarely
present quantitative demonstrations on the cost superiority of multi-chip
integration over monolithic SoC. In this paper, we build a quantitative cost
model and put forward an analytical method for multi-chip systems based on
three typical multi-chip integration technologies to analyze the cost benefits
from yield improvement, chiplet and package reuse, and heterogeneity. We
re-examine the actual cost of multi-chip systems from various perspectives and
show how to reduce the total cost of the VLSI system through appropriate
multi-chiplet architecture.
</summary>
    <author>
      <name>Yinxiao Feng</name>
    </author>
    <author>
      <name>Kaisheng Ma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3489517.3530428</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3489517.3530428" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by and presented at DAC 2022</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 59th ACM/IEEE Design Automation Conference,
  2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2203.12268v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.12268v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.13678v1</id>
    <updated>2022-03-21T13:53:22Z</updated>
    <published>2022-03-21T13:53:22Z</published>
    <title>LQoCo: Learning to Optimize Cache Capacity Overloading in Storage
  Systems</title>
    <summary>  Cache plays an important role to maintain high and stable performance (i.e.
high throughput, low tail latency and throughput jitter) in storage systems.
Existing rule-based cache management methods, coupled with engineers' manual
configurations, cannot meet ever-growing requirements of both time-varying
workloads and complex storage systems, leading to frequent cache overloading.
In this paper, we for the first time propose a light-weight learning-based
cache bandwidth control technique, called \LQoCo which can adaptively control
the cache bandwidth so as to effectively prevent cache overloading in storage
systems. Extensive experiments with various workloads on real systems show that
LQoCo, with its strong adaptability and fast learning ability, can adapt to
various workloads to effectively control cache bandwidth, thereby significantly
improving the storage performance (e.g. increasing the throughput by 10\%-20\%
and reducing the throughput jitter and tail latency by 2X-6X and 1.5X-4X,
respectively, compared with two representative rule-based methods).
</summary>
    <author>
      <name>Ji Zhang</name>
    </author>
    <author>
      <name>Xijun Li</name>
    </author>
    <author>
      <name>Xiyao Zhou</name>
    </author>
    <author>
      <name>Mingxuan Yuan</name>
    </author>
    <author>
      <name>Zhuo Cheng</name>
    </author>
    <author>
      <name>Keji Huang</name>
    </author>
    <author>
      <name>Yifan Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted by DAC 2022. Xijun is the correspoonding
  author</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.13678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.13678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.14882v1</id>
    <updated>2022-03-28T16:39:40Z</updated>
    <published>2022-03-28T16:39:40Z</published>
    <title>Vector In Memory Architecture for simple and high efficiency computing</title>
    <summary>  Data movement is one of the main challenges of contemporary system
architectures. Near-Data Processing (NDP) mitigates this issue by moving
computation closer to the memory, avoiding excessive data movement. Our
proposal, Vector-In-Memory Architecture(VIMA), executes large vector
instructions near 3D-stacked memories using vector functional units and uses a
small data cache to enable short-term data reuse. It provides an easy
programming interface and guarantees precise exceptions. When executing
stream-behaved applications using a single core, VIMA offers a speedup of up to
26x over a CPU system baseline with vector operations in a single-core
processor while spending 93% less energy.
</summary>
    <author>
      <name>Marco Antonio Zanata Alves</name>
    </author>
    <author>
      <name>Sairo Santos</name>
    </author>
    <author>
      <name>Aline S. Cordeiro</name>
    </author>
    <author>
      <name>Francis B. Moreira</name>
    </author>
    <author>
      <name>Paulo C. Santos</name>
    </author>
    <author>
      <name>Luigi Carro</name>
    </author>
    <link href="http://arxiv.org/abs/2203.14882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.14882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.15671v3</id>
    <updated>2022-06-22T13:38:54Z</updated>
    <published>2022-03-29T15:31:40Z</published>
    <title>100 Gb/s High Throughput Serial Protocol (HTSP) for Data Acquisition
  Systems with Interleaved Streaming</title>
    <summary>  Demands on Field-Programmable Gate Array (FPGA) data transport have been
increasing over the years as frame sizes and refresh rates increase. As the
bandwidths requirements increase the ability to implement data transport
protocol layers using "soft" programmable logic becomes harder and start to
require harden IP blocks implementation. To reduce the number of physical links
and interconnects, it is common for data acquisition systems to require
interleaving of streams on the same link (e.g. streaming data and streaming
register access). This paper presents a way to leverage existing FPGA harden IP
blocks to achieve a robust, low latency 100 Gb/s point-to-point link with
minimal programmable logic overhead geared towards the needs of data
acquisition systems with interleaved streaming requirements.
</summary>
    <author>
      <name>L. Ruckman</name>
    </author>
    <author>
      <name>D. Doering</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1748-0221/17/07/P07026</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1748-0221/17/07/P07026" rel="related"/>
    <link href="http://arxiv.org/abs/2203.15671v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.15671v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.01942v1</id>
    <updated>2022-04-05T02:31:18Z</updated>
    <published>2022-04-05T02:31:18Z</published>
    <title>Fault-Tolerant Deep Learning: A Hierarchical Perspective</title>
    <summary>  With the rapid advancements of deep learning in the past decade, it can be
foreseen that deep learning will be continuously deployed in more and more
safety-critical applications such as autonomous driving and robotics. In this
context, reliability turns out to be critical to the deployment of deep
learning in these applications and gradually becomes a first-class citizen
among the major design metrics like performance and energy efficiency.
Nevertheless, the back-box deep learning models combined with the diverse
underlying hardware faults make resilient deep learning extremely challenging.
In this special session, we conduct a comprehensive survey of fault-tolerant
deep learning design approaches with a hierarchical perspective and investigate
these approaches from model layer, architecture layer, circuit layer, and cross
layer respectively.
</summary>
    <author>
      <name>Cheng Liu</name>
    </author>
    <author>
      <name>Zhen Gao</name>
    </author>
    <author>
      <name>Siting Liu</name>
    </author>
    <author>
      <name>Xuefei Ning</name>
    </author>
    <author>
      <name>Huawei Li</name>
    </author>
    <author>
      <name>Xiaowei Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Special session submitted to VTS'22</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.01942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.01942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.2.3; B.8.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.02159v1</id>
    <updated>2022-04-05T12:36:07Z</updated>
    <published>2022-04-05T12:36:07Z</published>
    <title>Systematic Unsupervised Recycled Field-Programmable Gate Array Detection</title>
    <summary>  With the expansion of the semiconductor supply chain, the use of recycled
field-programmable gate arrays (FPGAs) has become a serious concern. Several
methods for detecting recycled FPGAs by analyzing the ring oscillator (RO)
frequencies have been proposed; however, most assume the known fresh FPGAs
(KFFs) as the training data in machine-learning-based classification. In this
study, we propose a novel recycled FPGA detection method based on an
unsupervised anomaly detection scheme when there are few or no KFFs available.
As the RO frequencies in the neighboring logic blocks on an FPGA are similar
because of systematic process variation, our method compares the RO frequencies
and does not require KFFs. The proposed method efficiently identifies recycled
FPGAs through outlier detection using direct density ratio estimation.
Experiments using Xilinx Artix-7 FPGAs demonstrate that the proposed method
successfully distinguishes recycled FPGAs from 35 fresh FPGAs. In contrast, a
conventional recycled FPGA detection method results in certain
misclassification.
</summary>
    <author>
      <name>Yuya Isaka</name>
    </author>
    <author>
      <name>Michihiro Shintani</name>
    </author>
    <author>
      <name>Foisal Ahmed</name>
    </author>
    <author>
      <name>Michiko Inoue</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TDMR.2022.3164788</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TDMR.2022.3164788" rel="related"/>
    <link href="http://arxiv.org/abs/2204.02159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.02159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.03290v1</id>
    <updated>2022-04-07T08:41:02Z</updated>
    <published>2022-04-07T08:41:02Z</published>
    <title>Memory Performance of AMD EPYC Rome and Intel Cascade Lake SP Server
  Processors</title>
    <summary>  Modern processors, in particular within the server segment, integrate more
cores with each generation. This increases their complexity in general, and
that of the memory hierarchy in particular. Software executed on such
processors can suffer from performance degradation when data is distributed
disadvantageously over the available resources. To optimize data placement and
access patterns, an in-depth analysis of the processor design and its
implications for performance is necessary. This paper describes and
experimentally evaluates the memory hierarchy of AMD EPYC Rome and Intel Xeon
Cascade Lake SP server processors in detail. Their distinct microarchitectures
cause different performance patterns for memory latencies, in particular for
remote cache accesses. Our findings illustrate the complex NUMA properties and
how data placement and cache coherence states impact access latencies to local
and remote locations. This paper also compares theoretical and effective
bandwidths for accessing data at the different memory levels and main memory
bandwidth saturation at reduced core counts. The presented insight is a
foundation for modeling performance of the given microarchitectures, which
enables practical performance engineering of complex applications. Moreover,
security research on side-channel attacks can also leverage the presented
findings.
</summary>
    <author>
      <name>Markus Velten</name>
    </author>
    <author>
      <name>Robert Schöne</name>
    </author>
    <author>
      <name>Thomas Ilsche</name>
    </author>
    <author>
      <name>Daniel Hackenberg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3489525.3511689</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3489525.3511689" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2022 ACM/SPEC International Conference on
  Performance Engineering (ICPE '22) (2022) 165-175</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2204.03290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.03290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.04160v1</id>
    <updated>2022-04-08T16:09:55Z</updated>
    <published>2022-04-08T16:09:55Z</published>
    <title>Leverage the Average: Averaged Sampling in Pre-Silicon Side-Channel
  Leakage Assessment</title>
    <summary>  Pre-silicon side-channel leakage assessment is a useful tool to identify
hardware vulnerabilities at design time, but it requires many high-resolution
power traces and increases the power simulation cost of the design. By
downsampling and averaging these high-resolution traces, we show that the power
simulation cost can be considerably reduced without significant loss of
side-channel leakage assessment quality. We introduce a theoretical basis for
our claims. Our results demonstrate up to 6.5-fold power-simulation speed
improvement on a gate-level side-channel leakage assessment of a RISC-V SoC.
Furthermore, we clarify the conditions under which the averaged sampling
technique can be successfully used.
</summary>
    <author>
      <name>Pantea Kiaei</name>
    </author>
    <author>
      <name>Zhenyuan Liu</name>
    </author>
    <author>
      <name>Patrick Schaumont</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3526241.3530337</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3526241.3530337" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Great Lakes Symposium on VLSI 2022 (GLSVLSI
  '22), June 6--8, 2022, Irvine, CA, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2204.04160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.04160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.05009v3</id>
    <updated>2022-06-02T07:16:13Z</updated>
    <published>2022-04-11T11:15:36Z</published>
    <title>VWR2A: A Very-Wide-Register Reconfigurable-Array Architecture for
  Low-Power Embedded Devices</title>
    <summary>  Edge-computing requires high-performance energy-efficient embedded systems.
Fixed-function or custom accelerators, such as FFT or FIR filter engines, are
very efficient at implementing a particular functionality for a given set of
constraints. However, they are inflexible when facing application-wide
optimizations or functionality upgrades. Conversely, programmable cores offer
higher flexibility, but often with a penalty in area, performance, and, above
all, energy consumption. In this paper, we propose VWR2A, an architecture that
integrates high computational density and low power memory structures (i.e.,
very-wide registers and scratchpad memories). VWR2A narrows the energy gap with
similar or better performance on FFT kernels with respect to an FFT
accelerator. Moreover, VWR2A flexibility allows to accelerate multiple kernels,
resulting in significant energy savings at the application level.
</summary>
    <author>
      <name>Benoît Walter Denkinger</name>
    </author>
    <author>
      <name>Miguel Peón-Quirós</name>
    </author>
    <author>
      <name>Mario Konijnenburg</name>
    </author>
    <author>
      <name>David Atienza</name>
    </author>
    <author>
      <name>Francky Catthoor</name>
    </author>
    <link href="http://arxiv.org/abs/2204.05009v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.05009v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.06114v1</id>
    <updated>2022-04-12T23:16:46Z</updated>
    <published>2022-04-12T23:16:46Z</published>
    <title>DT2CAM: A Decision Tree to Content Addressable Memory Framework</title>
    <summary>  Decision trees are considered one of the most powerful tools for data
classification. Accelerating the decision tree search is crucial for
on-the-edge applications that have limited power and latency budget. In this
paper, we propose a Content Addressable Memory (CAM) Compiler for Decision Tree
(DT) inference acceleration. We propose a novel "adaptive-precision" scheme
that results in a compact implementation and enables an efficient bijective
mapping to Ternary Content Addressable Memories while maintaining high
inference accuracies. In addition, a Resistive-CAM (ReCAM) functional
synthesizer is developed for mapping the decision tree to the ReCAM and
performing functional simulations for energy, latency, and accuracy
evaluations. We study the decision tree accuracy under hardware non-idealities
including device defects, manufacturing variability, and input encoding noise.
We test our framework on various DT datasets including \textit{Give Me Some
Credit}, \textit{Titanic}, and \textit{COVID-19}. Our results reveal up to
{42.4\%} energy savings and up to 17.8x better energy-delay-area product
compared to the state-of-art hardware accelerators, and up to 333 million
decisions per sec for the pipelined implementation.
</summary>
    <author>
      <name>Mariam Rakka</name>
    </author>
    <author>
      <name>Mohammed E. Fouda</name>
    </author>
    <author>
      <name>Rouwaida Kanj</name>
    </author>
    <author>
      <name>Fadi Kurdahi</name>
    </author>
    <link href="http://arxiv.org/abs/2204.06114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.06114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.07387v1</id>
    <updated>2022-04-15T08:53:59Z</updated>
    <published>2022-04-15T08:53:59Z</published>
    <title>AID: Accuracy Improvement of Analog Discharge-Based in-SRAM
  Multiplication Accelerator</title>
    <summary>  This paper presents a novel circuit (AID) to improve the accuracy of an
energy-efficient in-memory multiplier using a standard 6T-SRAM. The
state-of-the-art discharge-based in-SRAM multiplication accelerators suffer
from a non-linear behavior in their bit-line (BL, BLB) due to the quadratic
nature of the access transistor that leads to a poor signal-to-noise ratio
(SNR). In order to achieve linearity in the BLB voltage, we propose a novel
root function technique on the access transistor's gate that results in
accuracy improvement of on average 10.77 dB SNR compared to state-of-the-art
discharge-based topologies. Our analytical methods and a circuit simulation in
a 65 nm CMOS technology verify that the proposed technique consumes 0.523 pJ
per computation (multiplication, accumulation, and preset) from a power supply
of 1V, which is 51.18% lower compared to other state-of-the-art techniques. We
have performed an extensive Monte Carlo based simulation for a 4x4
multiplication operation, and our novel technique presents less than 0.086
standard deviations for the worst-case incorrect output scenario.
</summary>
    <author>
      <name>Saeed Seyedfaraji</name>
    </author>
    <author>
      <name>Baset Mesgari</name>
    </author>
    <author>
      <name>Semeen Rehman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23919/DATE54114.2022.9774748</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23919/DATE54114.2022.9774748" rel="related"/>
    <link href="http://arxiv.org/abs/2204.07387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.07387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.09514v1</id>
    <updated>2022-04-18T17:29:46Z</updated>
    <published>2022-04-18T17:29:46Z</published>
    <title>Special Session: Towards an Agile Design Methodology for Efficient,
  Reliable, and Secure ML Systems</title>
    <summary>  The real-world use cases of Machine Learning (ML) have exploded over the past
few years. However, the current computing infrastructure is insufficient to
support all real-world applications and scenarios. Apart from high efficiency
requirements, modern ML systems are expected to be highly reliable against
hardware failures as well as secure against adversarial and IP stealing
attacks. Privacy concerns are also becoming a first-order issue. This article
summarizes the main challenges in agile development of efficient, reliable and
secure ML systems, and then presents an outline of an agile design methodology
to generate efficient, reliable and secure ML systems based on user-defined
constraints and objectives.
</summary>
    <author>
      <name>Shail Dave</name>
    </author>
    <author>
      <name>Alberto Marchisio</name>
    </author>
    <author>
      <name>Muhammad Abdullah Hanif</name>
    </author>
    <author>
      <name>Amira Guesmi</name>
    </author>
    <author>
      <name>Aviral Shrivastava</name>
    </author>
    <author>
      <name>Ihsen Alouani</name>
    </author>
    <author>
      <name>Muhammad Shafique</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VTS52500.2021.9794253</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VTS52500.2021.9794253" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears at 40th IEEE VLSI Test Symposium (VTS 2022), 14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.09514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.09514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.09515v1</id>
    <updated>2022-04-11T05:45:43Z</updated>
    <published>2022-04-11T05:45:43Z</published>
    <title>Multiplier with Reduced Activities and Minimized Interconnect for Inner
  Product Arrays</title>
    <summary>  We present a pipelined multiplier with reduced activities and minimized
interconnect based on online digit-serial arithmetic. The working precision has
been truncated such that $p&lt;n$ bits are used to compute $n$ bits product,
resulting in significant savings in area and power. The digit slices follow
variable precision according to input, increasing upto $p$ and then decreases
according to the error profile. Pipelining has been done to achieve high
throughput and low latency which is desirable for compute intensive inner
products. Synthesis results of the proposed designs have been presented and
compared with the non-pipelined online multiplier, pipelined online multiplier
with full working precision and conventional serial-parallel and array
multipliers. For $8, 16, 24$ and $32$ bit precision, the proposed low power
pipelined design show upto $38\%$ and $44\%$ reduction in power and area
respectively compared to the pipelined online multiplier without working
precision truncation.
</summary>
    <author>
      <name>Muhammad Usman</name>
    </author>
    <author>
      <name>Jeong-A Lee</name>
    </author>
    <author>
      <name>Milos D. Ercegovac</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IEEECONF53345.2021.9723215</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IEEECONF53345.2021.9723215" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2021 55th Asilomar Conference on Signals, Systems, and Computers</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2204.09515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.09515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.09797v1</id>
    <updated>2022-04-20T21:56:50Z</updated>
    <published>2022-04-20T21:56:50Z</published>
    <title>Multiply-and-Fire (MNF): An Event-driven Sparse Neural Network
  Accelerator</title>
    <summary>  Machine learning, particularly deep neural network inference, has become a
vital workload for many computing systems, from data centers and HPC systems to
edge-based computing. As advances in sparsity have helped improve the
efficiency of AI acceleration, there is a continued need for improved system
efficiency for both high-performance and system-level acceleration.
  This work takes a unique look at sparsity with an event (or
activation-driven) approach to ANN acceleration that aims to minimize useless
work, improve utilization, and increase performance and energy efficiency. Our
analytical and experimental results show that this event-driven solution
presents a new direction to enable highly efficient AI inference for both CNN
and MLP workloads.
  This work demonstrates state-of-the-art energy efficiency and performance
centring on activation-based sparsity and a highly-parallel dataflow method
that improves the overall functional unit utilization (at 30 fps). This work
enhances energy efficiency over a state-of-the-art solution by 1.46$\times$.
Taken together, this methodology presents a novel, new direction to achieve
high-efficiency, high-performance designs for next-generation AI acceleration
platforms.
</summary>
    <author>
      <name>Miao Yu</name>
    </author>
    <author>
      <name>Tingting Xiang</name>
    </author>
    <author>
      <name>Venkata Pavan Kumar Miriyala</name>
    </author>
    <author>
      <name>Trevor E. Carlson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures and 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.09797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.09797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.09989v1</id>
    <updated>2022-04-21T09:32:51Z</updated>
    <published>2022-04-21T09:32:51Z</published>
    <title>NAND-SPIN-Based Processing-in-MRAM Architecture for Convolutional Neural
  Network Acceleration</title>
    <summary>  The performance and efficiency of running large-scale datasets on traditional
computing systems exhibit critical bottlenecks due to the existing "power wall"
and "memory wall" problems. To resolve those problems, processing-in-memory
(PIM) architectures are developed to bring computation logic in or near memory
to alleviate the bandwidth limitations during data transmission. NAND-like
spintronics memory (NAND-SPIN) is one kind of promising magnetoresistive
random-access memory (MRAM) with low write energy and high integration density,
and it can be employed to perform efficient in-memory computation operations.
In this work, we propose a NAND-SPIN-based PIM architecture for efficient
convolutional neural network (CNN) acceleration. A straightforward data mapping
scheme is exploited to improve the parallelism while reducing data movements.
Benefiting from the excellent characteristics of NAND-SPIN and in-memory
processing architecture, experimental results show that the proposed approach
can achieve $\sim$2.6$\times$ speedup and $\sim$1.4$\times$ improvement in
energy efficiency over state-of-the-art PIM solutions.
</summary>
    <author>
      <name>Yinglin Zhao</name>
    </author>
    <author>
      <name>Jianlei Yang</name>
    </author>
    <author>
      <name>Bing Li</name>
    </author>
    <author>
      <name>Xingzhou Cheng</name>
    </author>
    <author>
      <name>Xucheng Ye</name>
    </author>
    <author>
      <name>Xueyan Wang</name>
    </author>
    <author>
      <name>Xiaotao Jia</name>
    </author>
    <author>
      <name>Zhaohao Wang</name>
    </author>
    <author>
      <name>Youguang Zhang</name>
    </author>
    <author>
      <name>Weisheng Zhao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11432-021-3472-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11432-021-3472-9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, accepted by SCIENCE CHINA Information Sciences (SCIS) 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.09989v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.09989v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.10466v2</id>
    <updated>2025-02-25T13:32:52Z</updated>
    <published>2022-04-22T02:30:04Z</published>
    <title>AgilePkgC: An Agile System Idle State Architecture for Energy
  Proportional Datacenter Servers</title>
    <summary>  This paper presents the design of AgilePkgC (APC): a new C-state architecture
that improves the energy proportionality of servers that operate at low
utilization while running microservices of user-facing applications. APC
targets the reduction of power when all cores are idle in a shallow C-state,
ready to transition back to service. In particular, APC targets the power of
the resources shared by the cores (e.g., LLC, network-on-chip, IOs, DRAM) which
remain active while no core is active to use them. APC realizes its objective
by using low-overhead hardware to facilitate sub-microsecond entry/exit latency
to a new package C-state and judiciously selecting intermediate power modes for
the different shared resources that offer fast transition and, yet, substantial
power savings. Our experimental evaluation supports that APC holds the
potential to reduce server power by up to 41% with a worst-case performance
degradation of less than 0.1% for several representative workloads. Our results
clearly support the research and development and eventual adoption of new deep
and fast package C-states, like APC, for future server CPUs targeting
datacenters running microservices.
</summary>
    <author>
      <name>Georgia Antoniou</name>
    </author>
    <author>
      <name>Haris Volos</name>
    </author>
    <author>
      <name>Davide B. Bartolini</name>
    </author>
    <author>
      <name>Tom Rollet</name>
    </author>
    <author>
      <name>Yiannakis Sazeides</name>
    </author>
    <author>
      <name>Jawad Haj Yahya</name>
    </author>
    <link href="http://arxiv.org/abs/2204.10466v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.10466v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.10687v2</id>
    <updated>2022-04-29T16:54:45Z</updated>
    <published>2022-04-22T13:05:02Z</published>
    <title>SNE: an Energy-Proportional Digital Accelerator for Sparse Event-Based
  Convolutions</title>
    <summary>  Event-based sensors are drawing increasing attention due to their high
temporal resolution, low power consumption, and low bandwidth. To efficiently
extract semantically meaningful information from sparse data streams produced
by such sensors, we present a 4.5TOP/s/W digital accelerator capable of
performing 4-bits-quantized event-based convolutional neural networks (eCNN).
Compared to standard convolutional engines, our accelerator performs a number
of operations proportional to the number of events contained into the input
data stream, ultimately achieving a high energy-to-information processing
proportionality. On the IBM-DVS-Gesture dataset, we report 80uJ/inf to
261uJ/inf, respectively, when the input activity is 1.2% and 4.9%. Our
accelerator consumes 0.221pJ/SOP, to the best of our knowledge it is the lowest
energy/OP reported on a digital neuromorphic engine.
</summary>
    <author>
      <name>Alfio Di Mauro</name>
    </author>
    <author>
      <name>Arpan Suravi Prasad</name>
    </author>
    <author>
      <name>Zhikai Huang</name>
    </author>
    <author>
      <name>Matteo Spallanzani</name>
    </author>
    <author>
      <name>Francesco Conti</name>
    </author>
    <author>
      <name>Luca Benini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at DATE22</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.10687v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.10687v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.10707v1</id>
    <updated>2022-04-22T13:52:20Z</updated>
    <published>2022-04-22T13:52:20Z</published>
    <title>Crescent: Taming Memory Irregularities for Accelerating Deep Point Cloud
  Analytics</title>
    <summary>  3D perception in point clouds is transforming the perception ability of
future intelligent machines. Point cloud algorithms, however, are plagued by
irregular memory accesses, leading to massive inefficiencies in the memory
sub-system, which bottlenecks the overall efficiency. This paper proposes
Crescent, an algorithm-hardware co-design system that tames the irregularities
in deep point cloud analytics while achieving high accuracy. To that end, we
introduce two approximation techniques, approximate neighbor search and
selectively bank conflict elision, that "regularize" the DRAM and SRAM memory
accesses. Doing so, however, necessarily introduces accuracy loss, which we
mitigate by a new network training procedure that integrates approximation into
the network training process. In essence, our training procedure trains models
that are conditioned upon a specific approximate setting and, thus, retain a
high accuracy. Experiments show that Crescent doubles the performance and
halves the energy consumption compared to an optimized baseline accelerator
with &lt; 1% accuracy loss. The code of our paper is available at:
https://github.com/horizon-research/crescent.
</summary>
    <author>
      <name>Yu Feng</name>
    </author>
    <author>
      <name>Gunnar Hammonds</name>
    </author>
    <author>
      <name>Yiming Gan</name>
    </author>
    <author>
      <name>Yuhao Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.10707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.10707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.11192v1</id>
    <updated>2022-04-24T05:11:35Z</updated>
    <published>2022-04-24T05:11:35Z</published>
    <title>RedMulE: A Compact FP16 Matrix-Multiplication Accelerator for Adaptive
  Deep Learning on RISC-V-Based Ultra-Low-Power SoCs</title>
    <summary>  The fast proliferation of extreme-edge applications using Deep Learning (DL)
based algorithms required dedicated hardware to satisfy extreme-edge
applications' latency, throughput, and precision requirements. While inference
is achievable in practical cases, online finetuning and adaptation of general
DL models are still highly challenging. One of the key stumbling stones is the
need for parallel floating-point operations, which are considered unaffordable
on sub-100 mW extreme-edge SoCs. We tackle this problem with RedMulE
(Reduced-precision matrix Multiplication Engine), a parametric low-power
hardware accelerator for FP16 matrix multiplications - the main kernel of DL
training and inference - conceived for tight integration within a cluster of
tiny RISC-V cores based on the PULP (Parallel Ultra-Low-Power) architecture. In
22 nm technology, a 32-FMA RedMulE instance occupies just 0.07 mm^2 (14% of an
8-core RISC-V cluster) and achieves up to 666 MHz maximum operating frequency,
for a throughput of 31.6 MAC/cycle (98.8% utilization). We reach a
cluster-level power consumption of 43.5 mW and a full-cluster energy efficiency
of 688 16-bit GFLOPS/W. Overall, RedMulE features up to 4.65x higher energy
efficiency and 22x speedup over SW execution on 8 RISC-V cores.
</summary>
    <author>
      <name>Yvan Tortorella</name>
    </author>
    <author>
      <name>Luca Bertaccini</name>
    </author>
    <author>
      <name>Davide Rossi</name>
    </author>
    <author>
      <name>Luca Benini</name>
    </author>
    <author>
      <name>Francesco Conti</name>
    </author>
    <link href="http://arxiv.org/abs/2204.11192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.11192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1014v1</id>
    <updated>2011-05-05T08:41:43Z</updated>
    <published>2011-05-05T08:41:43Z</published>
    <title>Improving Network-on-Chip-based turbo decoder architectures</title>
    <summary>  In this work novel results concerning Network-on-Chip-based turbo decoder
architectures are presented. Stemming from previous publications, this work
concentrates first on improving the throughput by exploiting adaptive-bandwidth
reduction techniques. This technique shows in the best case an improvement of
more than 60 Mb/s. Moreover, it is known that double-binary turbo decoders
require higher area than binary ones. This characteristic has the negative
effect of increasing the data width of the network nodes. Thus, the second
contribution of this work is to reduce the network complexity to support
doublebinary codes, by exploiting bit-level and pseudo-floating-point
representation of the extrinsic information. These two techniques allow for an
area reduction of up to more than the 40% with a performance degradation of
about 0.2 dB.
</summary>
    <author>
      <name>Maurizio Martina</name>
    </author>
    <author>
      <name>Guido Masera</name>
    </author>
    <link href="http://arxiv.org/abs/1105.1014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.2624v1</id>
    <updated>2011-05-13T07:17:16Z</updated>
    <published>2011-05-13T07:17:16Z</published>
    <title>A Flexible LDPC code decoder with a Network on Chip as underlying
  interconnect architecture</title>
    <summary>  LDPC (Low Density Parity Check) codes are among the most powerful and widely
adopted modern error correcting codes. The iterative decoding algorithms
required for these codes involve high computational complexity and high
processing throughput is achieved by allocating a sufficient number of
processing elements (PEs). Supporting multiple heterogeneous LDPC codes on a
parallel decoder poses serious problems in the design of the interconnect
structure for such PEs. The aim of this work is to explore the feasibility of
NoC (Network on Chip) based decoders, where full flexibility in terms of
supported LDPC codes is obtained resorting to an NoC to connect PEs. NoC based
LDPC decoders have been previously considered unfeasible because of the cost
overhead associated to packet management and routing. On the contrary, the
designed NoC adopts a low complexity routing, which introduces a very limited
cost overhead with respect to architectures dedicated to specific classes of
codes. Moreover the paper proposes an efficient configuration technique, which
allows for fast on--the--fly switching among different codes. The decoder
architecture is scalable and VLSI synthesis results are presented for several
cases of study, including the whole set of WiMAX LDPC codes, WiFi codes and
DVB-S2 standard.
</summary>
    <author>
      <name>Carlo Condo</name>
    </author>
    <author>
      <name>Guido Masera</name>
    </author>
    <link href="http://arxiv.org/abs/1105.2624v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.2624v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.1158v3</id>
    <updated>2015-11-14T15:02:29Z</updated>
    <published>2012-10-03T15:57:49Z</published>
    <title>Emulating a large memory with a collection of small ones</title>
    <summary>  Sequential computation is well understood but does not scale well with
current technology. Within the next decade, systems will contain large numbers
of processors with potentially thousands of processors per chip. Despite this,
many computational problems exhibit little or no parallelism and many existing
formulations are sequential. It is therefore essential that highly-parallel
architectures can support sequential computation by emulating large memories
with collections of smaller ones, thus supporting efficient execution of
sequential programs or sequential components of parallel programs.
  This paper demonstrates that a realistic parallel architecture with scalable
low-latency communications can execute large-memory sequential programs with a
factor of only 2 to 3 slowdown, when compared to a conventional sequential
architecture. This overhead seems an acceptable price to pay to be able to
switch between executing highly-parallel programs and sequential programs with
large memory requirements. Efficient emulation of large memories could
therefore facilitate a transition from sequential machines by allowing existing
programs to be compiled directly to a highly-parallel architecture and then for
their performance to be improved by exploiting parallelism in memory accesses
and computation.
</summary>
    <author>
      <name>James Hanlon</name>
    </author>
    <link href="http://arxiv.org/abs/1210.1158v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.1158v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.0089v1</id>
    <updated>2013-06-01T09:04:40Z</updated>
    <published>2013-06-01T09:04:40Z</published>
    <title>A Novel Reconfigurable Architecture of a DSP Processor for Efficient
  Mapping of DSP Functions using Field Programmable DSP Arrays</title>
    <summary>  Development of modern integrated circuit technologies makes it feasible to
develop cheaper, faster and smaller special purpose signal processing function
circuits. Digital Signal processing functions are generally implemented either
on ASICs with inflexibility, or on FPGAs with bottlenecks of relatively smaller
utilization factor or lower speed compared to ASIC. Field Programmable DSP
Array (FPDA) is the proposed DSP dedicated device, redolent to FPGA, but with
basic fixed common modules (CMs) (like adders, subtractors, multipliers,
scaling units, shifters) instead of CLBs. This paper introduces the development
of reconfigurable system architecture with a focus on FPDA that integrates
different DSP functions like DFT, FFT, DCT, FIR, IIR, and DWT etc. The
switching between DSP functions is occurred by reconfiguring the
interconnection between CMs. Validation of the proposed architecture has been
achieved on Virtex5 FPGA. The architecture provides sufficient amount of
flexibility, parallelism and scalability.
</summary>
    <author>
      <name>Amitabha Sinha</name>
    </author>
    <author>
      <name>Mitrava Sarkar</name>
    </author>
    <author>
      <name>Soumojit Acharyya</name>
    </author>
    <author>
      <name>Suranjan Chakraborty</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2490302.2490304</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2490302.2490304" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 12 Figures, ACM SIGARCH Computer Architecture News. arXiv
  admin note: substantial text overlap with arXiv:1305.3251</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SIGARCH Computer Architecture News, Volume 41 Issue 2, May
  2013, Pages 1-8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1306.0089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.0089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.1889v1</id>
    <updated>2013-06-08T07:21:22Z</updated>
    <published>2013-06-08T07:21:22Z</published>
    <title>An Improved Structure Of Reversible Adder And Subtractor</title>
    <summary>  In today's world everyday a new technology which is faster, smaller and more
complex than its predecessor is being developed. The increased number of
transistors packed onto a chip of a conventional system results in increased
power consumption that is why Reversible logic has drawn attention of
Researchers due to its less heat dissipating characteristics. Reversible logic
can be imposed over applications such as quantum computing, optical computing,
quantum dot cellular automata, low power VLSI circuits, DNA computing. This
paper presents the reversible combinational circuit of adder, subtractor and
parity preserving subtractor. The suggested circuit in this paper are designed
using Feynman, Double Feynman and MUX gates which are better than the existing
one in literature in terms of Quantum cost, Garbage output and Total logical
calculations.
</summary>
    <author>
      <name>Aakash Gupta</name>
    </author>
    <author>
      <name>Pradeep Singla</name>
    </author>
    <author>
      <name>Jitendra Gupta</name>
    </author>
    <author>
      <name>Nitin Maheshwari</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Electronics and Computer Science
  Engineering, Vol 2, No. 2, pp712-718, June 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1306.1889v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.1889v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.3302v1</id>
    <updated>2013-06-14T06:20:44Z</updated>
    <published>2013-06-14T06:20:44Z</published>
    <title>The Effect of Communication and Synchronization on Amdahl Law in
  Multicore Systems</title>
    <summary>  This work analyses the effects of sequential-to-parallel synchronization and
inter-core communication on multicore performance, speedup and scaling. A
modification of Amdahl law is formulated, to reflect the finding that parallel
speedup is lower than originally predicted, due to these effects. In
applications with high inter-core communication requirements, the workload
should be executed on a small number of cores, and applications of high
sequential-to-parallel synchronization requirements may better be executed by
the sequential core, even when f, the Amdahl fraction of parallelization, is
very close to 1. To improve the scalability and performance speedup of a
multicore, it is as important to address the synchronization and connectivity
intensities of parallel algorithms as their parallelization factor.
</summary>
    <author>
      <name>Leonid Yavits</name>
    </author>
    <author>
      <name>Amir Morad</name>
    </author>
    <author>
      <name>Ran Ginosar</name>
    </author>
    <link href="http://arxiv.org/abs/1306.3302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.3302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.02986v3</id>
    <updated>2015-05-11T17:44:13Z</updated>
    <published>2015-03-10T16:54:39Z</published>
    <title>Strategies for High-Throughput FPGA-based QC-LDPC Decoder Architecture</title>
    <summary>  We propose without loss of generality strategies to achieve a high-throughput
FPGA-based architecture for a QC-LDPC code based on a circulant-1 identity
matrix construction. We present a novel representation of the parity-check
matrix (PCM) providing a multi-fold throughput gain. Splitting of the node
processing algorithm enables us to achieve pipelining of blocks and hence
layers. By partitioning the PCM into not only layers but superlayers we derive
an upper bound on the pipelining depth for the compact representation. To
validate the architecture, a decoder for the IEEE 802.11n (2012) QC-LDPC is
implemented on the Xilinx Kintex-7 FPGA with the help of the FPGA IP compiler
[2] available in the NI LabVIEW Communication System Design Suite (CSDS) which
offers an automated and systematic compilation flow where an optimized hardware
implementation from the LDPC algorithm was generated in approximately 3
minutes, achieving an overall throughput of 608Mb/s (at 260MHz). As per our
knowledge this is the fastest implementation of the IEEE 802.11n QC-LDPC
decoder using an algorithmic compiler.
</summary>
    <author>
      <name>Swapnil Mhaske</name>
    </author>
    <author>
      <name>Hojin Kee</name>
    </author>
    <author>
      <name>Tai Ly</name>
    </author>
    <author>
      <name>Ahsan Aziz</name>
    </author>
    <author>
      <name>Predrag Spasojevic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.02986v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.02986v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.03166v1</id>
    <updated>2015-03-08T18:23:32Z</updated>
    <published>2015-03-08T18:23:32Z</published>
    <title>Design of High Performance MIPS Cryptography Processor Based on T-DES
  Algorithm</title>
    <summary>  The paper describes the design of high performance MIPS Cryptography
processor based on triple data encryption standard. The organization of
pipeline stages in such a way that pipeline can be clocked at high frequency.
Encryption and Decryption blocks of triple data encryption standard (T-DES)
crypto system and dependency among themselves are explained in detail with the
help of block diagram. In order to increase the processor functionality and
performance, especially for security applications we include three new 32-bit
instructions LKLW, LKUW and CRYPT. The design has been synthesized at 40nm
process technology targeting using Xilinx Virtex-6 device. The overall MIPS
Crypto processor works at 209MHz.
</summary>
    <author>
      <name>Kirat Pal Singh</name>
    </author>
    <author>
      <name>Shivani Parmar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Engineering Research &amp; Technology. arXiv
  admin note: substantial text overlap with arXiv:1306.1916, arXiv:1503.02304</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.03166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.03166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.03169v1</id>
    <updated>2015-03-10T09:38:51Z</updated>
    <published>2015-03-10T09:38:51Z</published>
    <title>Dynamic Partitioning of Physical Memory Among Virtual Machines,
  ASMI:Architectural Support for Memory Isolation</title>
    <summary>  Cloud computing relies on secure and efficient virtualization. Software level
security solutions compromise the performance of virtual machines (VMs), as a
large amount of computational power would be utilized for running the security
modules. Moreover, software solutions are only as secure as the level that they
work on. For example a security module on a hypervisor cannot provide security
in the presence of an infected hypervisor. It is a challenge for virtualization
technology architects to enhance the security of VMs without degrading their
performance. Currently available server machines are not fully equipped to
support a secure VM environment without compromising on performance. A few
hardware modifications have been introduced by manufactures like Intel and AMD
to provide a secure VM environment with low performance degradation. In this
paper we propose a novel memory architecture model named \textit{ Architectural
Support for Memory Isolation(ASMI)}, that can achieve a true isolated physical
memory region to each VM without degrading performance. Along with true memory
isolation, ASMI is designed to provide lower memory access times, better
utilization of available memory, support for DMA isolation and support for
platform independence for users of VMs.
</summary>
    <author>
      <name>Jithin R</name>
    </author>
    <author>
      <name>Priya Chandran</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2851613.2851870</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2851613.2851870" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Rejected this short paper by the VEE 2015 Conference conducted by ACM
  due to the lack of implementation details</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.03169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.03169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.04628v1</id>
    <updated>2015-03-16T12:45:04Z</updated>
    <published>2015-03-16T12:45:04Z</published>
    <title>Logic BIST: State-of-the-Art and Open Problems</title>
    <summary>  Many believe that in-field hardware faults are too rare in practice to
justify the need for Logic Built-In Self-Test (LBIST) in a design. Until now,
LBIST was primarily used in safety-critical applications. However, this may
change soon. First, even if costly methods like burn-in are applied, it is no
longer possible to get rid of all latent defects in devices at leading-edge
technology. Second, demands for high reliability spread to consumer electronics
as smartphones replace our wallets and IDs. However, today many ASIC vendors
are reluctant to use LBIST. In this paper, we describe the needs for successful
deployment of LBIST in the industrial practice and discuss how these needs can
be addressed. Our work is hoped to attract a wider attention to this important
research topic.
</summary>
    <author>
      <name>Nan Li</name>
    </author>
    <author>
      <name>Gunnar Carlsson</name>
    </author>
    <author>
      <name>Elena Dubrova</name>
    </author>
    <author>
      <name>Kim Petersen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.04628v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.04628v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.05694v2</id>
    <updated>2015-06-05T07:28:13Z</updated>
    <published>2015-03-19T10:21:42Z</published>
    <title>Improving GPU Performance Through Resource Sharing</title>
    <summary>  Graphics Processing Units (GPUs) consisting of Streaming Multiprocessors
(SMs) achieve high throughput by running a large number of threads and context
switching among them to hide execution latencies. The number of thread blocks,
and hence the number of threads that can be launched on an SM, depends on the
resource usage--e.g. number of registers, amount of shared memory--of the
thread blocks. Since the allocation of threads to an SM is at the thread block
granularity, some of the resources may not be used up completely and hence will
be wasted.
  We propose an approach that shares the resources of SM to utilize the wasted
resources by launching more thread blocks. We show the effectiveness of our
approach for two resources: register sharing, and scratchpad (shared memory)
sharing. We further propose optimizations to hide long execution latencies,
thus reducing the number of stall cycles. We implemented our approach in
GPGPU-Sim simulator and experimentally validated it on several applications
from 4 different benchmark suites: GPGPU-Sim, Rodinia, CUDA-SDK, and Parboil.
We observed that with register sharing, applications show maximum improvement
of 24%, and average improvement of 11%. With scratchpad sharing, we observed a
maximum improvement of 30% and an average improvement of 12.5%.
</summary>
    <author>
      <name>Vishwesh Jatala</name>
    </author>
    <author>
      <name>Jayvant Anantpur</name>
    </author>
    <author>
      <name>Amey Karkare</name>
    </author>
    <link href="http://arxiv.org/abs/1503.05694v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.05694v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.00425v3</id>
    <updated>2017-11-06T23:00:30Z</updated>
    <published>2017-11-01T16:34:48Z</published>
    <title>Non Uniform On Chip Power Delivery Network Synthesis Methodology</title>
    <summary>  In this paper, we proposed a non-uniform power delivery network (PDN)
synthesis methodology. It first constructs initial PDN using uniform approach.
Then preliminary power integrity analysis is performed to derive IR-safe
candidate window. Congestion map is obtained based global route congestion
estimation. A self-adaptive non-uniform PDN synthesis is then performed to
globally and locally optimize PDN over selected regions. The PDN synthesis is
congestion-driven and IR- guarded. Experimental results show significant timing
important in trade-off small PDN length reduction with no EM/IR impact. We
further explored potential power savings using our non-uniform PDN synthesis
methodology.
</summary>
    <author>
      <name>Patrick Benediktsson</name>
    </author>
    <author>
      <name>Jon A. Flandrin</name>
    </author>
    <author>
      <name>Chen Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.00425v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.00425v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03229v1</id>
    <updated>2017-11-09T01:55:44Z</updated>
    <published>2017-11-09T01:55:44Z</published>
    <title>A Dwarf-based Scalable Big Data Benchmarking Methodology</title>
    <summary>  Different from the traditional benchmarking methodology that creates a new
benchmark or proxy for every possible workload, this paper presents a scalable
big data benchmarking methodology. Among a wide variety of big data analytics
workloads, we identify eight big data dwarfs, each of which captures the common
requirements of each class of unit of computation while being reasonably
divorced from individual implementations. We implement the eight dwarfs on
different software stacks, e.g., OpenMP, MPI, Hadoop as the dwarf components.
For the purpose of architecture simulation, we construct and tune big data
proxy benchmarks using the directed acyclic graph (DAG)-like combinations of
the dwarf components with different weights to mimic the benchmarks in
BigDataBench. Our proxy benchmarks preserve the micro-architecture, memory, and
I/O characteristics, and they shorten the simulation time by 100s times while
maintain the average micro-architectural data accuracy above 90 percentage on
both X86 64 and ARMv8 processors. We will open-source the big data dwarf
components and proxy benchmarks soon.
</summary>
    <author>
      <name>Wanling Gao</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <author>
      <name>Chunjie Luo</name>
    </author>
    <author>
      <name>Daoyi Zheng</name>
    </author>
    <author>
      <name>Zhen Jia</name>
    </author>
    <author>
      <name>Biwei Xie</name>
    </author>
    <author>
      <name>Chen Zheng</name>
    </author>
    <author>
      <name>Qiang Yang</name>
    </author>
    <author>
      <name>Haibin Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.03229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06672v1</id>
    <updated>2017-11-17T18:57:24Z</updated>
    <published>2017-11-17T18:57:24Z</published>
    <title>Decanting the Contribution of Instruction Types and Loop Structures in
  the Reuse of Traces</title>
    <summary>  Reuse has been proposed as a microarchitecture-level mechanism to reduce the
amount of executed instructions, collapsing dependencies and freeing resources
for other instructions. Previous works have used reuse domains such as memory
accesses, integer or not floating point, based on the reusability rate.
However, these works have not studied the specific contribution of reusing
different subsets of instructions for performance. In this work, we analysed
the sensitivity of trace reuse to instruction subsets, comparing their
efficiency to their complementary subsets. We also studied the amount of reuse
that can be extracted from loops. Our experiments show that disabling trace
reuse outside loops does not harm performance but reduces in 12% the number of
accesses to the reuse table. Our experiments with reuse subsets show that most
of the speedup can be retained even when not reusing all types of instructions
previously found in the reuse domain.
</summary>
    <author>
      <name>Andrey M. Coppieters</name>
    </author>
    <author>
      <name>Sheila de Oliveira</name>
    </author>
    <author>
      <name>Felipe M. G. França</name>
    </author>
    <author>
      <name>Maurício L. Pilla</name>
    </author>
    <author>
      <name>Amarildo T. da Costa</name>
    </author>
    <link href="http://arxiv.org/abs/1711.06672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.06790v1</id>
    <updated>2017-11-18T01:43:14Z</updated>
    <published>2017-11-18T01:43:14Z</published>
    <title>Mitigating Read-disturbance Errors in STT-RAM Caches by Using Data
  Compression</title>
    <summary>  Due to its high density and close-to-SRAM read latency, spin transfer torque
RAM (STT-RAM) is considered one of the most-promising emerging memory
technologies for designing large last level caches (LLCs). However, in deep
sub-micron region, STT-RAM shows read-disturbance error (RDE) whereby a read
operation may modify the stored data value and this presents a severe threat to
performance and reliability of STT-RAM caches. In this paper, we present a
technique, named SHIELD, to mitigate RDE in STT-RAM LLCs. SHIELD uses data
compression to reduce number of read operations from STT-RAM blocks to avoid
RDE and also to reduce the number of bits written to cache during both write
and restore operations. Experimental results have shown that SHIELD provides
significant improvement in performance and energy efficiency. SHIELD consumes
smaller energy than two previous RDE-mitigation techniques, namely high-current
restore required read (HCRR, also called restore-after-read) and low-current
long latency read (LCLL) and even an ideal RDE-free STT-RAM cache.
</summary>
    <author>
      <name>Sparsh Mittal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Book Chapter</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.06790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.08458v1</id>
    <updated>2017-11-22T03:53:34Z</updated>
    <published>2017-11-22T03:53:34Z</published>
    <title>BILBO-friendly Hybrid BIST Architecture with Asymmetric Polynomial
  Reseeding</title>
    <summary>  By advances in technology, integrated circuits have come to include more
functionality and more complexity in a single chip. Although methods of testing
have improved, but the increase in complexity of circuits, keeps testing a
challenging problem. Two important challenges in testing of digital circuits
are test time and accessing the circuit under test (CUT) for testing. These
challenges become even more important in complex system on chip (SoC) zone.
This paper presents a multistage test strategy to be implemented on a BIST
architecture for reducing test time of a simple core as solution for more
global application of SoC testing strategy. This strategy implements its test
pattern generation and output response analyzer in a BILBO architecture. The
proposed method benefits from an irregular polynomial BILBO (IP-BILBO)
structure to improve its test results. Experimental results on ISCAS-89
benchmark circuits show an average of 35% improvement in test time in
proportion to pervious work.
</summary>
    <author>
      <name>Elaheh Sadredini</name>
    </author>
    <author>
      <name>Mohammadreza Najafi</name>
    </author>
    <author>
      <name>Mahmood Fathy</name>
    </author>
    <author>
      <name>Zaialabedin Navabi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Architecture and Digital Systems (CADS), 2012 16th CSI
  International Symposium on. IEEE, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.08458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.08458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00585v1</id>
    <updated>2018-05-02T00:24:36Z</updated>
    <published>2018-05-02T00:24:36Z</published>
    <title>Dynamically Improving Branch Prediction Accuracy Between Contexts</title>
    <summary>  Branch prediction is a standard feature in most processors, significantly
improving the run time of programs by allowing a processor to predict the
direction of a branch before it has been evaluated. Current branch prediction
methods can achieve excellent prediction accuracy through global tables,
various hashing methods, and even machine learning techniques such as SVMs or
neural networks. Such designs, however, may lose effectiveness when attempting
to predict across context switches in the operating system. Such a scenario may
lead to destructive interference between contexts, therefore reducing overall
predictor accuracy. To solve this problem, we propose a novel scheme for
deciding whether a context switch produces destructive or constructive
interference. First, we present evidence that shows that destructive
interference can have a significant negative impact on prediction accuracy.
Second, we present an extensible framework that keeps track of context switches
and prediction accuracy to improve overall accuracy. Experimental results show
that this framework effectively reduces the effect of destructive interference
on branch prediction.
</summary>
    <author>
      <name>Adam Auten</name>
    </author>
    <author>
      <name>Tanishq Dubey</name>
    </author>
    <author>
      <name>Rohan Mathur</name>
    </author>
    <link href="http://arxiv.org/abs/1805.00585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02807v1</id>
    <updated>2018-05-08T02:36:04Z</updated>
    <published>2018-05-08T02:36:04Z</published>
    <title>FlashAbacus: A Self-Governing Flash-Based Accelerator for Low-Power
  Systems</title>
    <summary>  Energy efficiency and computing flexibility are some of the primary design
constraints of heterogeneous computing. In this paper, we present FlashAbacus,
a data-processing accelerator that self-governs heterogeneous kernel executions
and data storage accesses by integrating many flash modules in lightweight
multiprocessors. The proposed accelerator can simultaneously process data from
different applications with diverse types of operational functions, and it
allows multiple kernels to directly access flash without the assistance of a
host-level file system or an I/O runtime library. We prototype FlashAbacus on a
multicore-based PCIe platform that connects to FPGA-based flash controllers
with a 20 nm node process. The evaluation results show that FlashAbacus can
improve the bandwidth of data processing by 127%, while reducing energy
consumption by 78.4%, as compared to a conventional method of heterogeneous
computing. \blfootnote{This paper is accepted by and will be published at 2018
EuroSys. This document is presented to ensure timely dissemination of scholarly
and technical work.
</summary>
    <author>
      <name>Jie Zhang</name>
    </author>
    <author>
      <name>Myoungsoo Jung</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3190508.3190544</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3190508.3190544" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is published at the 13th edition of EuroSys</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.02807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02921v1</id>
    <updated>2018-05-08T09:39:29Z</updated>
    <published>2018-05-08T09:39:29Z</published>
    <title>Hierarchical Temporal Memory using Memristor Networks: A Survey</title>
    <summary>  This paper presents a survey of the currently available hardware designs for
implementation of the human cortex inspired algorithm, Hierarchical Temporal
Memory (HTM). In this review, we focus on the state of the art advances of
memristive HTM implementation and related HTM applications. With the advent of
edge computing, HTM can be a potential algorithm to implement on-chip near
sensor data processing. The comparison of analog memristive circuit
implementations with the digital and mixed-signal solutions are provided. The
advantages of memristive HTM over digital implementations against performance
metrics such as processing speed, reduced on-chip area and power dissipation
are discussed. The limitations and open problems concerning the memristive HTM,
such as the design scalability, sneak currents, leakage, parasitic effects,
lack of the analog learning circuits implementations and unreliability of the
memristive devices integrated with CMOS circuits are also discussed.
</summary>
    <author>
      <name>Olga Krestinskaya</name>
    </author>
    <author>
      <name>Irina Dolzhikova</name>
    </author>
    <author>
      <name>Alex Pappachen James</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Emerging Topics in Computational
  Intelligence, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.02921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03502v1</id>
    <updated>2018-05-07T18:54:58Z</updated>
    <published>2018-05-07T18:54:58Z</published>
    <title>RowClone: Accelerating Data Movement and Initialization Using DRAM</title>
    <summary>  In existing systems, to perform any bulk data movement operation (copy or
initialization), the data has to first be read into the on-chip processor, all
the way into the L1 cache, and the result of the operation must be written back
to main memory. This is despite the fact that these operations do not involve
any actual computation. RowClone exploits the organization and operation of
commodity DRAM to perform these operations completely inside DRAM using two
mechanisms. The first mechanism, Fast Parallel Mode, copies data between two
rows inside the same DRAM subarray by issuing back-to-back activate commands to
the source and the destination row. The second mechanism, Pipelined Serial
Mode, transfers cache lines between two banks using the shared internal bus.
RowClone significantly reduces the raw latency and energy consumption of bulk
data copy and initialization. This reduction directly translates to improvement
in performance and energy efficiency of systems running copy or
initialization-intensive workloads
</summary>
    <author>
      <name>Vivek Seshadri</name>
    </author>
    <author>
      <name>Yoongu Kim</name>
    </author>
    <author>
      <name>Chris Fallin</name>
    </author>
    <author>
      <name>Donghyuk Lee</name>
    </author>
    <author>
      <name>Rachata Ausavarungnirun</name>
    </author>
    <author>
      <name>Gennady Pekhimenko</name>
    </author>
    <author>
      <name>Yixin Luo</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <author>
      <name>Phillip B. Gibbons</name>
    </author>
    <author>
      <name>Michael A. Kozuch</name>
    </author>
    <author>
      <name>Todd C. Mowry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1605.06483</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.03502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03648v1</id>
    <updated>2018-05-09T17:26:28Z</updated>
    <published>2018-05-09T17:26:28Z</published>
    <title>Parallel Programming for FPGAs</title>
    <summary>  This book focuses on the use of algorithmic high-level synthesis (HLS) to
build application-specific FPGA systems. Our goal is to give the reader an
appreciation of the process of creating an optimized hardware design using HLS.
Although the details are, of necessity, different from parallel programming for
multicore processors or GPUs, many of the fundamental concepts are similar. For
example, designers must understand memory hierarchy and bandwidth, spatial and
temporal locality of reference, parallelism, and tradeoffs between computation
and storage. This book is a practical guide for anyone interested in building
FPGA systems. In a university environment, it is appropriate for advanced
undergraduate and graduate courses. At the same time, it is also useful for
practicing system designers and embedded programmers. The book assumes the
reader has a working knowledge of C/C++ and includes a significant amount of
sample code. In addition, we assume familiarity with basic computer
architecture concepts (pipelining, speedup, Amdahl's Law, etc.). A knowledge of
the RTL-based FPGA design flow is helpful, although not required.
</summary>
    <author>
      <name>Ryan Kastner</name>
    </author>
    <author>
      <name>Janarbek Matai</name>
    </author>
    <author>
      <name>Stephen Neuendorffer</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03718v1</id>
    <updated>2018-05-09T20:16:37Z</updated>
    <published>2018-05-09T20:16:37Z</published>
    <title>Neural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks</title>
    <summary>  This paper presents the Neural Cache architecture, which re-purposes cache
structures to transform them into massively parallel compute units capable of
running inferences for Deep Neural Networks. Techniques to do in-situ
arithmetic in SRAM arrays, create efficient data mapping and reducing data
movement are proposed. The Neural Cache architecture is capable of fully
executing convolutional, fully connected, and pooling layers in-cache. The
proposed architecture also supports quantization in-cache. Our experimental
results show that the proposed architecture can improve inference latency by
18.3x over state-of-art multi-core CPU (Xeon E5), 7.7x over server class GPU
(Titan Xp), for Inception v3 model. Neural Cache improves inference throughput
by 12.4x over CPU (2.2x over GPU), while reducing power consumption by 50% over
CPU (53% over GPU).
</summary>
    <author>
      <name>Charles Eckert</name>
    </author>
    <author>
      <name>Xiaowei Wang</name>
    </author>
    <author>
      <name>Jingcheng Wang</name>
    </author>
    <author>
      <name>Arun Subramaniyan</name>
    </author>
    <author>
      <name>Ravi Iyer</name>
    </author>
    <author>
      <name>Dennis Sylvester</name>
    </author>
    <author>
      <name>David Blaauw</name>
    </author>
    <author>
      <name>Reetuparna Das</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the 45th ACM/IEEE International Symposium on Computer
  Architecture (ISCA 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.03718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03969v1</id>
    <updated>2018-05-08T18:08:23Z</updated>
    <published>2018-05-08T18:08:23Z</published>
    <title>Exploiting Row-Level Temporal Locality in DRAM to Reduce the Memory
  Access Latency</title>
    <summary>  This paper summarizes the idea of ChargeCache, which was published in HPCA
2016 [51], and examines the work's significance and future potential. DRAM
latency continues to be a critical bottleneck for system performance. In this
work, we develop a low-cost mechanism, called ChargeCache, that enables faster
access to recently-accessed rows in DRAM, with no modifications to DRAM chips.
Our mechanism is based on the key observation that a recently-accessed row has
more charge and thus the following access to the same row can be performed
faster. To exploit this observation, we propose to track the addresses of
recently-accessed rows in a table in the memory controller. If a later DRAM
request hits in that table, the memory controller uses lower timing parameters,
leading to reduced DRAM latency. Row addresses are removed from the table after
a specified duration to ensure rows that have leaked too much charge are not
accessed with lower latency. We evaluate ChargeCache on a wide variety of
workloads and show that it provides significant performance and energy benefits
for both single-core and multi-core systems.
</summary>
    <author>
      <name>Hasan Hassan</name>
    </author>
    <author>
      <name>Gennady Pekhimenko</name>
    </author>
    <author>
      <name>Nandita Vijaykumar</name>
    </author>
    <author>
      <name>Vivek Seshadri</name>
    </author>
    <author>
      <name>Donghyuk Lee</name>
    </author>
    <author>
      <name>Oguz Ergin</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1609.07234</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.03969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06050v1</id>
    <updated>2018-05-15T22:18:10Z</updated>
    <published>2018-05-15T22:18:10Z</published>
    <title>BLASYS: Approximate Logic Synthesis Using Boolean Matrix Factorization</title>
    <summary>  Approximate computing is an emerging paradigm where design accuracy can be
traded off for benefits in design metrics such as design area, power
consumption or circuit complexity. In this work, we present a novel paradigm to
synthesize approximate circuits using Boolean matrix factorization (BMF). In
our methodology the truth table of a sub-circuit of the design is approximated
using BMF to a controllable approximation degree, and the results of the
factorization are used to synthesize a less complex subcircuit. To scale our
technique to large circuits, we devise a circuit decomposition method and a
subcircuit design-space exploration technique to identify the best order for
subcircuit approximations. Our method leads to a smooth trade-off between
accuracy and full circuit complexity as measured by design area and power
consumption. Using an industrial strength design flow, we extensively evaluate
our methodology on a number of testcases, where we demonstrate that the
proposed methodology can achieve up to 63% in power savings, while introducing
an average relative error of 5%. We also compare our work to previous works in
Boolean circuit synthesis and demonstrate significant improvements in design
metrics for same accuracy targets.
</summary>
    <author>
      <name>Soheil Hashemi</name>
    </author>
    <author>
      <name>Hokchhay Tann</name>
    </author>
    <author>
      <name>Sherief Reda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3195970.3196001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3195970.3196001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To Appear in DAC'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06407v2</id>
    <updated>2018-05-29T17:14:33Z</updated>
    <published>2018-05-16T16:27:42Z</published>
    <title>Recent Advances in Overcoming Bottlenecks in Memory Systems and Managing
  Memory Resources in GPU Systems</title>
    <summary>  This article features extended summaries and retrospectives of some of the
recent research done by our research group, SAFARI, on (1) various critical
problems in memory systems and (2) how memory system bottlenecks affect
graphics processing unit (GPU) systems. As more applications share a single
system, operations from each application can contend with each other at various
shared components. Such contention can slow down each application or thread of
execution. The compound effect of contention, high memory latency and access
overheads, as well as inefficient management of resources, greatly degrades
performance, quality-of-service, and energy efficiency. The ten works featured
in this issue study several aspects of (1) inter-application interference in
multicore systems, heterogeneous systems, and GPUs; (2) the growing overheads
and expenses associated with growing memory densities and latencies; and (3)
performance, programmability, and portability issues in modern GPUs, especially
those related to memory system resources.
</summary>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <author>
      <name>Saugata Ghose</name>
    </author>
    <author>
      <name>Rachata Ausavarungnirun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1805.09127</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06407v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06407v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07409v1</id>
    <updated>2018-05-06T07:28:38Z</updated>
    <published>2018-05-06T07:28:38Z</published>
    <title>LECTOR Based Clock Gating for Low Power Multi-Stage Flip Flop
  Applications</title>
    <summary>  Power dissipation in integrated circuits is one of the major concerns to the
research community, at the verge when more number of transistors are integrated
on a single chip. The substantial source of power dissipation in sequential
elements of the integrated circuit is due to the fast switching of high
frequency clock signals. These signals do not carry any information and are
mainly intended to synchronize the operation of sequential components. This
unnecessary switching of Clock, during the HOLD phase of either logic 1 or
logic 0, may be eliminated using a technique, called Clock Gating. In this
paper, we have incorporated a recent clock gating style called LECTOR based
clock gating LB CG to drive multi stage architecture and simulated its
performance using 90nm CMOS Predictive Technology Model PTM with a power supply
of 1.1V at 18GHz clock frequency. A substantial savings in terms of average
power in comparison to its non gated correspondent have been observed.
</summary>
    <author>
      <name>Pritam Bhattacharjee</name>
    </author>
    <author>
      <name>Bipasha Nath</name>
    </author>
    <author>
      <name>Alak Majumder</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICEIC 2017 International Conference on Electronics, Information,
  and Communication, 2017.1, 106-109 (4 pages)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.07409v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07409v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09127v3</id>
    <updated>2018-05-30T05:43:41Z</updated>
    <published>2018-05-17T16:49:36Z</published>
    <title>Recent Advances in DRAM and Flash Memory Architectures</title>
    <summary>  This article features extended summaries and retrospectives of some of the
recent research done by our group, SAFARI, on (1) understanding,
characterizing, and modeling various critical properties of modern DRAM and
NAND flash memory, the dominant memory and storage technologies, respectively;
and (2) several new mechanisms we have proposed based on our observations from
these analyses, characterization, and modeling, to tackle various key
challenges in memory and storage scaling. In order to understand the sources of
various bottlenecks of the dominant memory and storage technologies, these
works perform rigorous studies of device-level and application-level behavior,
using a combination of detailed simulation and experimental characterization of
real memory and storage devices.
</summary>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <author>
      <name>Saugata Ghose</name>
    </author>
    <author>
      <name>Rachata Ausavarungnirun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1805.06407</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09127v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09127v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09612v3</id>
    <updated>2019-03-17T10:28:12Z</updated>
    <published>2018-05-24T11:27:59Z</published>
    <title>PRINS: Resistive CAM Processing in Storage</title>
    <summary>  Near-data in-storage processing research has been gaining momentum in recent
years. Typical processing-in-storage architecture places a single or several
processing cores inside the storage and allows data processing without
transferring it to the host CPU. Since this approach replicates von Neumann
architecture inside storage, it is exposed to the problems faced by von Neumann
architecture, especially the bandwidth wall. We present PRINS, a novel in-data
processing-in-storage architecture based on Resistive Content Addressable
Memory (RCAM). PRINS functions simultaneously as a storage and a massively
parallel associative processor. PRINS alleviates the bandwidth wall faced by
conventional processing-in-storage architectures by keeping the computing
inside the storage arrays, thus implementing in-data, rather than near-data,
processing. We show that PRINS may outperform a reference computer architecture
with a bandwidth-limited external storage. The performance of PRINS Euclidean
distance, dot product and histogram implementation exceeds the attainable
performance of a reference architecture by up to four orders of magnitude,
depending on the dataset size. The performance of PRINS SpMV may exceed the
attainable performance of such reference architecture by more than two orders
of magnitude.
</summary>
    <author>
      <name>Leonid Yavits</name>
    </author>
    <author>
      <name>Roman Kaplan</name>
    </author>
    <author>
      <name>Ran Ginosar</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09612v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09612v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.10431v3</id>
    <updated>2018-06-21T03:38:40Z</updated>
    <published>2018-05-26T05:48:53Z</published>
    <title>Time-Shared Execution of Realtime Computer Vision Pipelines by Dynamic
  Partial Reconfiguration</title>
    <summary>  This paper presents an FPGA runtime framework that demonstrates the
feasibility of using dynamic partial reconfiguration (DPR) for time-sharing an
FPGA by multiple realtime computer vision pipelines. The presented time-sharing
runtime framework manages an FPGA fabric that can be round-robin time-shared by
different pipelines at the time scale of individual frames. In this new
use-case, the challenge is to achieve useful performance despite high
reconfiguration time. The paper describes the basic runtime support as well as
four optimizations necessary to achieve realtime performance given the
limitations of DPR on today's FPGAs. The paper provides a characterization of a
working runtime framework prototype on a Xilinx ZC706 development board. The
paper also reports the performance of realtime computer vision pipelines when
time-shared.
</summary>
    <author>
      <name>Marie Nguyen</name>
    </author>
    <author>
      <name>James C. Hoe</name>
    </author>
    <link href="http://arxiv.org/abs/1805.10431v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.10431v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11344v1</id>
    <updated>2018-05-29T10:16:34Z</updated>
    <published>2018-05-29T10:16:34Z</published>
    <title>LaKe: An Energy Efficient, Low Latency, Accelerated Key-Value Store</title>
    <summary>  Key-value store is a popular type of cloud computing applications. The
performance of key-value store applications have been shown to be very
sensitive to load within the data center, and in particular to latency. As load
within data center increases, it is becoming hard to maintain key-value store
applications' performance, without exceeding both the processing capacity of
hosts and the power budgets of racks. In this paper, we present LaKe: a low
latency, power efficient key-value store design for cloud applications. LaKe is
a modular design, combining multiple cores and cache layering, both in hardware
and software. LaKe achieves full line rate throughput, while maintaining a
latency of 1.1us and better power efficiency than existing hardware based
memcached designs. Using the modularity of our design, we study trade-offs in
the use of on-chip memory, SRAM and DRAM in accelerated designs and provide
insights for future architectures.
</summary>
    <author>
      <name>Yuta Tokusashi</name>
    </author>
    <author>
      <name>Hiroki Matsutani</name>
    </author>
    <author>
      <name>Noa Zilberman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.11344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.02887v1</id>
    <updated>2018-09-08T22:21:14Z</updated>
    <published>2018-09-08T22:21:14Z</published>
    <title>Accelerating Viterbi Algorithm using Custom Instruction Approach</title>
    <summary>  In recent years, the decoding algorithms in communication networks are
becoming increasingly complex aiming to achieve high reliability in correctly
decoding received messages. These decoding algorithms involve computationally
complex operations requiring high performance computing hardware, which are
generally expensive. A cost-effective solution is to enhance the Instruction
Set Architecture (ISA) of the processors by creating new custom instructions
for the computational parts of the decoding algorithms. In this paper, we
propose to utilize the custom instruction approach to efficiently implement the
widely used Viterbi decoding algorithm by adding the assembly language
instructions to the ISA of DLX, PicoJava II and NIOS II processors, which
represent RISC, stack and FPGA-based soft-core processor architectures,
respectively. By using the custom instruction approach, the execution time of
the Viterbi algorithm is significantly improved by approximately 3 times for
DLX and PicoJava II, and by 2 times for NIOS II.
</summary>
    <author>
      <name>Waqar Ahmad</name>
    </author>
    <author>
      <name>Imran Hafeez Abbassi</name>
    </author>
    <author>
      <name>Usman Sanwal</name>
    </author>
    <author>
      <name>Hasan Mahmood</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MESA.2018.8449144</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MESA.2018.8449144" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages, 4 Figures, 2018 14th IEEE/ASME International Conference on
  Mechatronic and Embedded Systems and Applications (MESA)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.02887v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.02887v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.06016v2</id>
    <updated>2018-10-01T05:00:47Z</updated>
    <published>2018-09-17T04:40:52Z</published>
    <title>The Impact of On-chip Communication on Memory Technologies for
  Neuromorphic Systems</title>
    <summary>  Emergent nanoscale non-volatile memory technologies with high integration
density offer a promising solution to overcome the scalability limitations of
CMOS-based neural networks architectures, by efficiently exhibiting the key
principle of neural computation. Despite the potential improvements in
computational costs, designing high-performance on-chip communication networks
that support flexible, large-fanout connectivity remains as daunting task. In
this paper, we elaborate on the communication requirements of large-scale
neuromorphic designs, and point out the differences with the conventional
network-on-chip architectures. We present existing approaches for on-chip
neuromorphic routing networks, and discuss how new memory and integration
technologies may help to alleviate the communication issues in constructing
next-generation intelligent computing machines.
</summary>
    <author>
      <name>Saber Moradi</name>
    </author>
    <author>
      <name>Rajit Manohar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1361-6463/aae641</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1361-6463/aae641" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 6 figures, Journal of Physics D: Applied Physics 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.06016v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.06016v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.08358v1</id>
    <updated>2018-09-22T00:57:09Z</updated>
    <published>2018-09-22T00:57:09Z</published>
    <title>In-memory multiplication engine with SOT-MRAM based stochastic computing</title>
    <summary>  Processing-in-memory (PIM) turns out to be a promising solution to
breakthrough the memory wall and the power wall. While prior PIM designs yield
successful implementation of bitwise Boolean logic operations locally in
memory, it is difficult to accomplish the multiplication (MUL) instruction in a
fast and efficient manner. In this paper, we propose a new stochastic computing
(SC) design to perform MUL with in-memory operations. Instead of using the
stochastic number generators (SNGs), we harness the inherent stochasticity in
the memory write behavior of the magnetic random access memory (MRAM). Each
memory bit serves as an SC engine, performs MUL on operands in the form of
write voltage pulses, and stores the MUL outcome in-situ. The proposed design
provides up to 4x improvement in performance compared with conversational SC
approaches, and achieves 18x speedup over implementing MUL with only in-memory
bitwise Boolean logic operations.
</summary>
    <author>
      <name>Xin Ma</name>
    </author>
    <author>
      <name>Liang Chang</name>
    </author>
    <author>
      <name>Shuangchen Li</name>
    </author>
    <author>
      <name>Lei Deng</name>
    </author>
    <author>
      <name>Yufei Ding</name>
    </author>
    <author>
      <name>Yuan Xie</name>
    </author>
    <link href="http://arxiv.org/abs/1809.08358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.08358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.11156v1</id>
    <updated>2018-09-28T17:40:05Z</updated>
    <published>2018-09-28T17:40:05Z</published>
    <title>Improving Reliability, Security, and Efficiency of Reconfigurable
  Hardware Systems</title>
    <summary>  In this treatise, my research on methods to improve efficiency, reliability,
and security of reconfigurable hardware systems, i.e., FPGAs, through partial
dynamic reconfiguration is outlined. The efficiency of reconfigurable systems
can be improved by loading optimized data paths on-the-fly on an FPGA fabric.
This technique was applied to the acceleration of SQL queries for large
database applications as well as for image and signal processing applications.
The focus was not only on performance improvements and resource efficiency, but
also the energy efficiency has been significantly improved. In the area of
reliability, countermeasures against radiation-induced faults and aging effects
for long mission times were investigated and applied to SRAM-FPGA-based
satellite systems. Finally, to increase the security of cryptographic
FPGA-based implementations against physical attacks, i.e., side-channel and
fault injection analysis as well as reverse engineering, it is proposed to
transform static circuit structures into dynamic ones by applying dynamic
partial reconfiguration.
</summary>
    <author>
      <name>Daniel Ziener</name>
    </author>
    <link href="http://arxiv.org/abs/1809.11156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.11156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.04514v3</id>
    <updated>2018-12-13T16:05:14Z</updated>
    <published>2018-12-11T16:13:35Z</published>
    <title>R3-DLA (Reduce, Reuse, Recycle): A More Efficient Approach to Decoupled
  Look-Ahead Architectures</title>
    <summary>  Modern societies have developed insatiable demands for more computation
capabilities. Exploiting implicit parallelism to provide automatic performance
improvement remains a central goal in engineering future general-purpose
computing systems. One approach is to use a separate thread context to perform
continuous look-ahead to improve the data and instruction supply to the main
pipeline. Such a decoupled look-ahead (DLA) architecture can be quite effective
in accelerating a broad range of applications in a relatively straightforward
implementation. It also has broad design flexibility as the look-ahead agent
need not be concerned with correctness constraints. In this paper, we explore a
number of optimizations that make the look-ahead agent more efficient and yet
extract more utility from it. With these optimizations, a DLA architecture can
achieve an average speedup of 1.4 over a state-of-the-art microarchitecture for
a broad set of benchmark suites, making it a powerful tool to enhance
single-thread performance.
</summary>
    <author>
      <name>Sushant Kondguli</name>
    </author>
    <author>
      <name>Michael Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 16 Figures, Scheduled to appear in 25th IEEE International
  Symposium on High-Performance Computer Architecture 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.04514v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.04514v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.07012v2</id>
    <updated>2018-12-22T02:10:37Z</updated>
    <published>2018-12-17T19:30:57Z</published>
    <title>Rapid Cycle-Accurate Simulator for High-Level Synthesis</title>
    <summary>  A large semantic gap between the high-level synthesis (HLS) design and the
low-level (on-board or RTL) simulation environment often creates a barrier for
those who are not FPGA experts. Moreover, such low-level simulation takes a
long time to complete. Software-based HLS simulators can help bridge this gap
and accelerate the simulation process; however, we found that the current FPGA
HLS commercial software simulators sometimes produce incorrect results. In
order to solve this correctness issue while maintaining the high speed of a
software-based simulator, this paper proposes a new HLS simulation flow named
FLASH. The main idea behind the proposed flow is to extract the scheduling
information from the HLS tool and automatically construct an equivalent
cycle-accurate simulation model while preserving C semantics. Experimental
results show that FLASH runs three orders of magnitude faster than the RTL
simulation.
</summary>
    <author>
      <name>Yuze Chi</name>
    </author>
    <author>
      <name>Young-kyu Choi</name>
    </author>
    <author>
      <name>Jason Cong</name>
    </author>
    <author>
      <name>Jie Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is an extended version of a paper that has been accepted
  for FPGA'19</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.07012v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.07012v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.08918v1</id>
    <updated>2018-12-21T02:41:56Z</updated>
    <published>2018-12-21T02:41:56Z</published>
    <title>Computational RAM to Accelerate String Matching at Scale</title>
    <summary>  Traditional Von Neumann computing is falling apart in the era of exploding
data volumes as the overhead of data transfer becomes forbidding. Instead, it
is more energy-efficient to fuse compute capability with memory where the data
reside. This is particularly critical for pattern matching, a key computational
step in large-scale data analytics, which involves repetitive search over very
large databases residing in memory. Emerging spintronic technologies show
remarkable versatility for the tight integration of logic and memory. In this
paper, we introduce CRAM-PM, a novel high-density, reconfigurable spintronic
in-memory compute substrate for pattern matching.
</summary>
    <author>
      <name>Zamshed I. Chowdhury</name>
    </author>
    <author>
      <name>S. Karen Khatamifard</name>
    </author>
    <author>
      <name>Zhengyang Zhao</name>
    </author>
    <author>
      <name>Masoud Zabihi</name>
    </author>
    <author>
      <name>Salonik Resch</name>
    </author>
    <author>
      <name>Meisam Razaviyayn</name>
    </author>
    <author>
      <name>Jian-Ping Wang</name>
    </author>
    <author>
      <name>Sachin Sapatnekar</name>
    </author>
    <author>
      <name>Ulya R. Karpuzcu</name>
    </author>
    <link href="http://arxiv.org/abs/1812.08918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.08918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.00327v1</id>
    <updated>2019-06-02T02:26:30Z</updated>
    <published>2019-06-02T02:26:30Z</published>
    <title>Sparse Matrix to Matrix Multiplication: A Representation and
  Architecture for Acceleration (long version)</title>
    <summary>  Accelerators for sparse matrix multiplication are important components in
emerging systems. In this paper, we study the main challenges of accelerating
Sparse Matrix Multiplication (SpMM). For the situations that data is not stored
in the desired order (row/column order), we propose a compact high performance
sparse format, which allows for random access to a dataset with low memory
access overhead. We show that using this format results in a 14-49 times
speedup for SpMM. Next, we propose a high performance systolic architecture for
SpMM, which uses a mesh of comparators to locate the useful (non-zero)
computation. This design maximizes data reuse by sharing the input data among a
row/column of the mesh. We also show that, with similar memory access
assumptions, the proposed architecture results in a 9-30 times speedup in
comparison with the state of the art.
</summary>
    <author>
      <name>Pareesa Ameneh Golnari</name>
    </author>
    <author>
      <name>Sharad Malik</name>
    </author>
    <link href="http://arxiv.org/abs/1906.00327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.00877v1</id>
    <updated>2019-06-03T15:34:25Z</updated>
    <published>2019-06-03T15:34:25Z</published>
    <title>Pangloss: a novel Markov chain prefetcher</title>
    <summary>  We present Pangloss, an efficient high-performance data prefetcher that
approximates a Markov chain on delta transitions. With a limited information
scope and space/logic complexity, it is able to reconstruct a variety of both
simple and complex access patterns. This is achieved by a highly-efficient
representation of the Markov chain to provide accurate values for transition
probabilities. In addition, we have added a mechanism to reconstruct delta
transitions originally obfuscated by the out-of-order execution or page
transitions, such as when streaming data from multiple sources. Our
single-level (L2) prefetcher achieves a geometric speedup of 1.7% and 3.2% over
selected state-of-the-art baselines (KPCP and BOP). When combined with an
equivalent for the L1 cache (L1 &amp; L2), the speedups rise to 6.8% and 8.4%, and
40.4% over non-prefetch. In the multi-core evaluation, there seems to be a
considerable performance improvement as well.
</summary>
    <author>
      <name>Philippos Papaphilippou</name>
    </author>
    <author>
      <name>Paul H. J. Kelly</name>
    </author>
    <author>
      <name>Wayne Luk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in The Third Data Prefetching Championship (DPC3), held in
  conjunction with ISCA 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.00877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.06603v1</id>
    <updated>2019-06-15T18:46:40Z</updated>
    <published>2019-06-15T18:46:40Z</published>
    <title>An Overview of In-memory Processing with Emerging Non-volatile Memory
  for Data-intensive Applications</title>
    <summary>  The conventional von Neumann architecture has been revealed as a major
performance and energy bottleneck for rising data-intensive applications. %,
due to the intensive data movements. The decade-old idea of leveraging
in-memory processing to eliminate substantial data movements has returned and
led extensive research activities. The effectiveness of in-memory processing
heavily relies on memory scalability, which cannot be satisfied by traditional
memory technologies. Emerging non-volatile memories (eNVMs) that pose appealing
qualities such as excellent scaling and low energy consumption, on the other
hand, have been heavily investigated and explored for realizing in-memory
processing architecture. In this paper, we summarize the recent research
progress in eNVM-based in-memory processing from various aspects, including the
adopted memory technologies, locations of the in-memory processing in the
system, supported arithmetics, as well as applied applications.
</summary>
    <author>
      <name>Bing Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Helen</arxiv:affiliation>
    </author>
    <author>
      <name>Bonan Yan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Helen</arxiv:affiliation>
    </author>
    <author>
      <name> Hai</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Helen</arxiv:affiliation>
    </author>
    <author>
      <name> Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3299874.3319452</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3299874.3319452" rel="related"/>
    <link href="http://arxiv.org/abs/1906.06603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.06603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.10666v1</id>
    <updated>2019-06-25T17:04:39Z</updated>
    <published>2019-06-25T17:04:39Z</published>
    <title>Automatic Conversion from Flip-flop to 3-phase Latch-based Designs</title>
    <summary>  Latch-based designs have many benefits over their flip-flop based
counterparts but have limited use partially because most RTL specifications are
flop-centric and automatic conversion of FF to latch-based designs is
challenging. Conventional conversion algorithms target master-slave latch-based
designs with two non-overlapping clocks. This paper presents a novel automated
design flow that converts flip-flop to 3-phase latch-based designs. The
resulting circuits have the same performance as the master-slave based designs
but require significantly less latches. Our experimental results demonstrate
the potential for savings in the number of latches (21.3%), area (5.8%), and
power (16.3%) on a variety of ISCAS, CEP, and CPU benchmark circuits, compared
to the master-slave conversions.
</summary>
    <author>
      <name>Huimei Cheng</name>
    </author>
    <author>
      <name>Yichen Gu</name>
    </author>
    <author>
      <name>Peter A. Beerel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.10666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.12221v1</id>
    <updated>2019-09-26T16:20:19Z</updated>
    <published>2019-09-26T16:20:19Z</published>
    <title>Storage Class Memory: Principles, Problems, and Possibilities</title>
    <summary>  Storage Class Memory (SCM) is a class of memory technology which has recently
become viable for use. Their namearises from the fact that they exhibit
non-volatility of data, similar to secondary storage while also having
latencies comparable toprimary memory and byte-addressibility. In this area,
Phase Change Memory (PCM), Spin-Transfer-Torque Random Access Memory(STT-RAM),
and Resistive RAM (ReRAM) have emerged as the major contenders for commercial
and industrial use. In this paper, wedescribe how these memory types function,
while highlighting the problems of endurance and performance that these memory
typesface. We also discuss the future possibilities of Multi-Level Cells
(MLCs), as well as how SCM can be used to construct accelerators.
</summary>
    <author>
      <name>Aditya K Kamath</name>
    </author>
    <author>
      <name>Leslie Monis</name>
    </author>
    <author>
      <name>A Tarun Karthik</name>
    </author>
    <author>
      <name>Basavaraj Talawar</name>
    </author>
    <link href="http://arxiv.org/abs/1909.12221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.12221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.12373v1</id>
    <updated>2019-09-26T20:23:46Z</updated>
    <published>2019-09-26T20:23:46Z</published>
    <title>A Survey of Machine Learning Applied to Computer Architecture Design</title>
    <summary>  Machine learning has enabled significant benefits in diverse fields, but,
with a few exceptions, has had limited impact on computer architecture. Recent
work, however, has explored broader applicability for design, optimization, and
simulation. Notably, machine learning based strategies often surpass prior
state-of-the-art analytical, heuristic, and human-expert approaches. This paper
reviews machine learning applied system-wide to simulation and run-time
optimization, and in many individual components, including memory systems,
branch predictors, networks-on-chip, and GPUs. The paper further analyzes
current practice to highlight useful design strategies and identify areas for
future work, based on optimized implementation strategies, opportune extensions
to existing work, and ambitious long term possibilities. Taken together, these
strategies and techniques present a promising future for increasingly automated
architectural design.
</summary>
    <author>
      <name>Drew D. Penney</name>
    </author>
    <author>
      <name>Lizhong Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1909.12373v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.12373v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.13318v2</id>
    <updated>2020-12-06T21:48:39Z</updated>
    <published>2019-09-29T16:47:40Z</published>
    <title>Run-time reconfigurable multi-precision floating point multiplier design
  for high speed, low-power applications</title>
    <summary>  Floating point multiplication is one of the crucial operations in many
application domains such as image processing, signal processing etc. But every
application requires different working features. Some need high precision, some
need low power consumption, low latency etc. But IEEE-754 format is not really
flexible for these specifications and also design is complex. Optimal run-time
reconfigurable hardware implementations may need the use of custom
floating-point formats that do not necessarily follow IEEE specified sizes. In
this paper, we present a run-time-reconfigurable floating point multiplier
implemented on FPGA with custom floating point format for different
applications. This floating point multiplier can have 6 modes of operations
depending on the accuracy or application requirement. With the use of optimal
design with custom IPs (Intellectual Properties), a better implementation is
done by truncating the inputs before multiplication. And a combination of
Karatsuba algorithm and Urdhva-Tiryagbhyam algorithm (Vedic Mathematics) is
used to implement unsigned binary multiplier. This further increases the
efficiency of the multiplier.
</summary>
    <author>
      <name>S. Arish</name>
    </author>
    <author>
      <name>R. K. Sharma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SPIN.2015.7095315</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SPIN.2015.7095315" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2015 2nd International Conference on Signal Processing and
  Integrated Networks (SPIN)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1909.13318v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.13318v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.13807v2</id>
    <updated>2019-10-03T14:47:25Z</updated>
    <published>2019-09-30T16:03:28Z</published>
    <title>System-level optimization of Network-on-Chips for heterogeneous 3D
  System-on-Chips</title>
    <summary>  For a system-level design of Networks-on-Chip for 3D heterogeneous
System-on-Chip (SoC), the locations of components, routers and vertical links
are determined from an application model and technology parameters. In
conventional methods, the two inputs are accounted for separately; here, we
define an integrated problem that considers both application model and
technology parameters. We show that this problem does not allow for exact
solution in reasonable time, as common for many design problems. Therefore, we
contribute a heuristic by proposing design steps, which are based on separation
of intralayer and interlayer communication. The advantage is that this new
problem can be solved with well-known methods. We use 3D Vision SoC case
studies to quantify the advantages and the practical usability of the proposed
optimization approach. We achieve up to 18.8% reduced white space and up to
12.4% better network performance in comparison to conventional approaches.
</summary>
    <author>
      <name>Jan Moritz Joseph</name>
    </author>
    <author>
      <name>Dominik Ermel</name>
    </author>
    <author>
      <name>Lennart Bamberg</name>
    </author>
    <author>
      <name>Alberto García-Ortiz</name>
    </author>
    <author>
      <name>Thilo Pionteck</name>
    </author>
    <link href="http://arxiv.org/abs/1909.13807v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.13807v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00856v1</id>
    <updated>2020-01-03T15:02:11Z</updated>
    <published>2020-01-03T15:02:11Z</published>
    <title>TrappeD: DRAM Trojan Designs for Information Leakage and Fault Injection
  Attacks</title>
    <summary>  In this paper, we investigate the advanced circuit features such as wordline-
(WL) underdrive (prevents retention failure) and overdrive (assists write)
employed in the peripherals of Dynamic RAM (DRAM) memories from a security
perspective. In an ideal environment, these features ensure fast and reliable
read and write operations. However, an adversary can re-purpose them by
inserting Trojans to deliver malicious payloads such as fault injections,
Denial-of-Service (DoS), and information leakage attacks when activated by the
adversary. Simulation results indicate that wordline voltage can be increased
to cause retention failure and thereby launch a DoS attack in DRAM memory.
Furthermore, two wordlines or bitlines can be shorted to leak information or
inject faults by exploiting the DRAM's refresh operation. We demonstrate an
information leakage system exploit by implementing TrappeD on RocketChip SoC.
</summary>
    <author>
      <name>Karthikeyan Nagarajan</name>
    </author>
    <author>
      <name>Asmit De</name>
    </author>
    <author>
      <name>Mohammad Nasim Imtiaz Khan</name>
    </author>
    <author>
      <name>Swaroop Ghosh</name>
    </author>
    <link href="http://arxiv.org/abs/2001.00856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.01501v4</id>
    <updated>2020-06-29T18:06:23Z</updated>
    <published>2020-01-06T11:40:06Z</published>
    <title>Stochastic Rounding: Algorithms and Hardware Accelerator</title>
    <summary>  Algorithms and a hardware accelerator for performing stochastic rounding (SR)
are presented. The main goal is to augment the ARM M4F based multi-core
processor SpiNNaker2 with a more flexible rounding functionality than is
available in the ARM processor itself. The motivation of adding such an
accelerator in hardware is based on our previous results showing improvements
in numerical accuracy of ODE solvers in fixed-point arithmetic with SR,
compared to standard round to nearest or bit truncation rounding modes.
Furthermore, performing SR purely in software can be expensive, due to
requirement of a pseudorandom number generator (PRNG), multiple masking and
shifting instructions, and an addition operation. Also, saturation of the
rounded values is included, since rounding is usually followed by saturation,
which is especially important in fixed-point arithmetic due to a narrow dynamic
range of representable values. The main intended use of the accelerator is to
round fixed-point multiplier outputs, which are returned unrounded by the ARM
processor in a wider fixed-point format than the arguments.
</summary>
    <author>
      <name>Mantas Mikaitis</name>
    </author>
    <link href="http://arxiv.org/abs/2001.01501v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01501v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.04504v2</id>
    <updated>2020-05-26T15:11:59Z</updated>
    <published>2020-01-13T19:19:50Z</published>
    <title>CHIPKIT: An agile, reusable open-source framework for rapid test chip
  development</title>
    <summary>  The current trend for domain-specific architectures (DSAs) has led to renewed
interest in research test chips to demonstrate new specialized hardware.
Tape-outs also offer huge pedagogical value garnered from real hands-on
exposure to the whole system stack. However, successful tape-outs demand
hard-earned experience, and the design process is time consuming and fraught
with challenges. Therefore, custom chips have remained the preserve of a small
number of research groups, typically focused on circuit design research. This
paper describes the CHIPKIT framework. We describe a reusable SoC subsystem
which provides basic IO, an on-chip programmable host, memory and peripherals.
This subsystem can be readily extended with new IP blocks to generate custom
test chips. We also present an agile RTL development flow, including a code
generation tool calledVGEN. Finally, we outline best practices for full-chip
validation across the entire design cycle.
</summary>
    <author>
      <name>Paul Whatmough</name>
    </author>
    <author>
      <name>Marco Donato</name>
    </author>
    <author>
      <name>Glenn Ko</name>
    </author>
    <author>
      <name>Sae-Kyu Lee</name>
    </author>
    <author>
      <name>David Brooks</name>
    </author>
    <author>
      <name>Gu-Yeon Wei</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MM.2020.2995809</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MM.2020.2995809" rel="related"/>
    <link href="http://arxiv.org/abs/2001.04504v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.04504v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09599v1</id>
    <updated>2020-01-27T06:26:21Z</updated>
    <published>2020-01-27T06:26:21Z</published>
    <title>Achieving Multi-Port Memory Performance on Single-Port Memory with
  Coding Techniques</title>
    <summary>  Many performance critical systems today must rely on performance
enhancements, such as multi-port memories, to keep up with the increasing
demand of memory-access capacity. However, the large area footprints and
complexity of existing multi-port memory designs limit their applicability.
This paper explores a coding theoretic framework to address this problem. In
particular, this paper introduces a framework to encode data across multiple
single-port memory banks in order to {\em algorithmically} realize the
functionality of multi-port memory.
  This paper proposes three code designs with significantly less storage
overhead compared to the existing replication based emulations of multi-port
memories. To further improve performance, we also demonstrate a memory
controller design that utilizes redundancy across coded memory banks to more
efficiently schedule read and write requests sent across multiple cores.
Furthermore, guided by DRAM traces, the paper explores {\em dynamic coding}
techniques to improve the efficiency of the coding based memory design. We then
show significant performance improvements in critical word read and write
latency in the proposed coded-memory design when compared to a traditional
uncoded-memory design.
</summary>
    <author>
      <name>Hardik Jain</name>
    </author>
    <author>
      <name>Matthew Edwards</name>
    </author>
    <author>
      <name>Ethan Elenberg</name>
    </author>
    <author>
      <name>Ankit Singh Rawat</name>
    </author>
    <author>
      <name>Sriram Vishwanath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 20 figures, ICICT 2020 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.09599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09982v1</id>
    <updated>2020-01-24T15:17:55Z</updated>
    <published>2020-01-24T15:17:55Z</published>
    <title>Accelerating Transient Fault Injection Campaigns by using Dynamic HDL
  Slicing</title>
    <summary>  Along with the complexity of electronic systems for safety-critical
applications, the cost of safety mechanisms evaluation by fault injection
simulation is rapidly going up. To reduce these efforts, we propose a fault
injection methodology where Hardware Description Language (HDL) code slicing is
exploited to accelerate transient fault injection campaigns by pruning fault
lists and reducing the number of the injections. In particular, the dynamic HDL
slicing technique provides for a critical fault list and allows avoiding
injections at non-critical time-steps. Experimental results on an industrial
core show that the proposed methodology can successfully reduce the number of
injections by up to 10 percent and speed-up the fault injection campaigns.
</summary>
    <author>
      <name>Ahmet Cagri Bagbaba</name>
    </author>
    <author>
      <name>Maksim Jenihhin</name>
    </author>
    <author>
      <name>Jaan Raik</name>
    </author>
    <author>
      <name>Christian Sauer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/NORCHIP.2019.8906932</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/NORCHIP.2019.8906932" rel="related"/>
    <link href="http://arxiv.org/abs/2001.09982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.10715v1</id>
    <updated>2020-01-29T07:49:39Z</updated>
    <published>2020-01-29T07:49:39Z</published>
    <title>qBSA: Logic Design of a 32-bit Block-Skewed RSFQ Arithmetic Logic Unit</title>
    <summary>  Single flux quantum (SFQ) circuits are an attractive beyond-CMOS technology
because they promise two orders of magnitude lower power at clock frequencies
exceeding 25 GHz.However, every SFQ gate is clocked creating very deep
gate-level pipelines that are difficult to keep full, particularly for
sequences that include data-dependent operations. This paper proposes to
increase the throughput of SFQ pipelines by re-designing the datapath to accept
and operate on least-significant bits (LSBs) clock cycles earlier than more
significant bits. This skewed datapath approach reduces the latency of the LSB
side which can be feedback earlier for use in subsequent data-dependent
operations increasing their throughput. In particular,we propose to group the
bits into 4-bit blocks that are operatedon concurrently and create block-skewed
datapath units for 32-bit operation. This skewed approach allows a subsequent
data-dependent operation to start evaluating as soon as the first 4-bit block
completes. Using this general approach, we developa block-skewed
MIPS-compatible 32-bit ALU. Our gate-level Verilog design improves the
throughput of 32-bit data dependent operations by 2x and 1.5x compared to
previously proposed 4-bit bit-slice and 32-bit Ladner-Fischer ALUs
respectively.
</summary>
    <author>
      <name>Souvik Kundu</name>
    </author>
    <author>
      <name>Gourav datta</name>
    </author>
    <author>
      <name>Peter A. Beerel</name>
    </author>
    <author>
      <name>Massoud Pedram</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.10715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.10715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.00062v1</id>
    <updated>2020-07-31T20:07:15Z</updated>
    <published>2020-07-31T20:07:15Z</published>
    <title>Partial Reconfiguration for Design Optimization</title>
    <summary>  FPGA designers have traditionally shared a similar design methodology with
ASIC designers. Most notably, at design time, FPGA designers commit to a fixed
allocation of logic resources to modules in a design. At runtime, some of the
occupied resources could be left idle or under-utilized due to hard-to-avoid
sources of inefficiencies (e.g., operation dependencies). With partial
reconfiguration (PR), FPGA resources can be re-allocated over time. Therefore,
using PR, a designer can attempt to reduce idleness and under-utilization with
better area-time scheduling.
  In this paper, we explain when, how, and why PR-style designs can improve
over the performance-area Pareto front of ASIC-style designs (without PR). We
first introduce the concept of area-time volume to explain why PR-style designs
can improve upon ASIC-style designs. We identify resource under-utilization as
an opportunity that can be exploited by PR-style designs. We then present a
first-order analytical model to help a designer decide if a PR-style design can
be beneficial. When it is the case, the model points to the most suitable PR
execution strategy and provides an estimate of the improvement. The model is
validated in three case studies.
</summary>
    <author>
      <name>Marie Nguyen</name>
    </author>
    <author>
      <name>Nathan Serafin</name>
    </author>
    <author>
      <name>James C. Hoe</name>
    </author>
    <link href="http://arxiv.org/abs/2008.00062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.00062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.00176v1</id>
    <updated>2020-08-01T05:43:38Z</updated>
    <published>2020-08-01T05:43:38Z</published>
    <title>Custom Tailored Suite of Random Forests for Prefetcher Adaptation</title>
    <summary>  To close the gap between memory and processors, and in turn improve
performance, there has been an abundance of work in the area of
data/instruction prefetcher designs. Prefetchers are deployed in each level of
the memory hierarchy, but typically, each prefetcher gets designed without
comprehensively accounting for other prefetchers in the system. As a result,
these individual prefetcher designs do not always complement each other, and
that leads to low average performance gains and/or many negative outliers. In
this work, we propose SuitAP (Suite of random forests for Adaptation of
Prefetcher system configuration), which is a hardware prefetcher adapter that
uses a suite of random forests to determine at runtime which prefetcher should
be ON at each memory level, such that they complement each other. Compared to a
design with no prefetchers, using SuitAP we improve IPC by 46% on average
across traces generated from SPEC2017 suite with 12KB overhead. Moreover, we
also reduce negative outliers using SuitAP.
</summary>
    <author>
      <name>Furkan Eris</name>
    </author>
    <author>
      <name>Sadullah Canakci</name>
    </author>
    <author>
      <name>Cansu Demirkiran</name>
    </author>
    <author>
      <name>Ajay Joshi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.00176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.00176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.00329v1</id>
    <updated>2020-08-01T20:00:19Z</updated>
    <published>2020-08-01T20:00:19Z</published>
    <title>CuttleSys: Data-Driven Resource Management forInteractive Applications
  on Reconfigurable Multicores</title>
    <summary>  Multi-tenancy for latency-critical applications leads to re-source
interference and unpredictable performance. Core reconfiguration opens up more
opportunities for colocation,as it allows the hardware to adjust to the dynamic
performance and power needs of a specific mix of co-scheduled applications.
However, reconfigurability also introduces challenges, as even for a small
number of reconfigurable cores, exploring the design space becomes more time-
and resource-demanding.
  We present CuttleSys, a runtime for reconfigurable multi-cores that leverages
scalable and lightweight data mining to quickly identify suitable core and
cache configurations for a set of co-scheduled applications. The runtime
combines collaborative filtering to infer the behavior of each job on every
core and cache configuration, with Dynamically Dimensioned Search to
efficiently explore the configuration space. We evaluate CuttleSys on
multicores with tens of reconfigurable cores and show up to 2.46x and 1.55x
performance improvements compared to core-level gating and oracle-like
asymmetric multicores respectively, under stringent power constraints.
</summary>
    <author>
      <name>Neeraj Kulkarni</name>
    </author>
    <author>
      <name>Gonzalo Gonzalez-Pumariega</name>
    </author>
    <author>
      <name>Amulya Khurana</name>
    </author>
    <author>
      <name>Christine Shoemaker</name>
    </author>
    <author>
      <name>Christina Delimitrou</name>
    </author>
    <author>
      <name>David Albonesi</name>
    </author>
    <link href="http://arxiv.org/abs/2008.00329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.00329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.00806v2</id>
    <updated>2020-11-16T10:08:58Z</updated>
    <published>2020-07-21T09:28:40Z</published>
    <title>DAMO: Deep Agile Mask Optimization for Full Chip Scale</title>
    <summary>  Continuous scaling of the VLSI system leaves a great challenge on
manufacturing and optical proximity correction (OPC) is widely applied in
conventional design flow for manufacturability optimization. Traditional
techniques conducted OPC by leveraging a lithography model and suffered from
prohibitive computational overhead, and mostly focused on optimizing a single
clip without addressing how to tackle the full chip. In this paper, we present
DAMO, a high performance and scalable deep learning-enabled OPC system for full
chip scale. It is an end-to-end mask optimization paradigm which contains a
Deep Lithography Simulator (DLS) for lithography modeling and a Deep Mask
Generator (DMG) for mask pattern generation. Moreover, a novel layout splitting
algorithm customized for DAMO is proposed to handle the full chip OPC problem.
Extensive experiments show that DAMO outperforms the state-of-the-art OPC
solutions in both academia and industrial commercial toolkit.
</summary>
    <author>
      <name>Guojin Chen</name>
    </author>
    <author>
      <name>Wanli Chen</name>
    </author>
    <author>
      <name>Yuzhe Ma</name>
    </author>
    <author>
      <name>Haoyu Yang</name>
    </author>
    <author>
      <name>Bei Yu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3400302.3415705</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3400302.3415705" rel="related"/>
    <link href="http://arxiv.org/abs/2008.00806v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.00806v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.00961v2</id>
    <updated>2020-09-22T08:37:43Z</updated>
    <published>2020-07-30T21:27:31Z</published>
    <title>Accelerating Genome Analysis: A Primer on an Ongoing Journey</title>
    <summary>  Genome analysis fundamentally starts with a process known as read mapping,
where sequenced fragments of an organism's genome are compared against a
reference genome. Read mapping is currently a major bottleneck in the entire
genome analysis pipeline, because state-of-the-art genome sequencing
technologies are able to sequence a genome much faster than the computational
techniques employed to analyze the genome. We describe the ongoing journey in
significantly improving the performance of read mapping. We explain
state-of-the-art algorithmic methods and hardware-based acceleration
approaches. Algorithmic approaches exploit the structure of the genome as well
as the structure of the underlying hardware. Hardware-based acceleration
approaches exploit specialized microarchitectures or various execution
paradigms (e.g., processing inside or near memory). We conclude with the
challenges of adopting these hardware-accelerated read mappers.
</summary>
    <author>
      <name>Mohammed Alser</name>
    </author>
    <author>
      <name>Zülal Bingöl</name>
    </author>
    <author>
      <name>Damla Senol Cali</name>
    </author>
    <author>
      <name>Jeremie Kim</name>
    </author>
    <author>
      <name>Saugata Ghose</name>
    </author>
    <author>
      <name>Can Alkan</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MM.2020.3013728</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MM.2020.3013728" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an extended and updated version of a paper published in IEEE
  Micro, vol. 40, no. 5, pp. 65-75, 1 Sept.-Oct. 2020,
  https://doi.org/10.1109/MM.2020.3013728</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Micro, Volume: 40, Issue: 5, Sept.-Oct. 1 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2008.00961v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.00961v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.01869v1</id>
    <updated>2020-08-04T22:37:04Z</updated>
    <published>2020-08-04T22:37:04Z</published>
    <title>Optimum Reconfiguration of Routing Interconnection Network in APSoC
  Fabrics</title>
    <summary>  This paper presents an automated algorithm for optimum configuration of
routing interconnection network in Xilinx Zynq-7000 All programmable
system-on-chip (APSoC) fabrics. A method to configure circuits with optimum
routing resources is presented along with their performance parameters with and
without the proposed algorithm. The proposed algorithm enables full control
over routing resources for using different interconnection types in order to
create routing-based circuit-under-test. The algorithm proposes the routing
techniques through the 2-D array of switch matrices inside the interconnection
network and automatically identifies the involved programmable interconnection
points associated with a node. An experimental setup is proposed to measure the
performance parameters such as slack time and power with and without the
applied algorithm on the APSoC routing resources. The proposed setup requires
no external equipment such as manufactured equipments or external instruments
for performance measurement.
</summary>
    <author>
      <name>Mostafa Darvishi</name>
    </author>
    <link href="http://arxiv.org/abs/2008.01869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.01869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.02078v1</id>
    <updated>2020-07-27T04:57:08Z</updated>
    <published>2020-07-27T04:57:08Z</published>
    <title>A Novel Method for Scalable VLSI Implementation of Hyperbolic Tangent
  Function</title>
    <summary>  Hyperbolic tangent and Sigmoid functions are used as non-linear activation
units in the artificial and deep neural networks. Since, these networks are
computationally expensive, customized accelerators are designed for achieving
the required performance at lower cost and power. The activation function and
MAC units are the key building blocks of these neural networks. A low
complexity and accurate hardware implementation of the activation function is
required to meet the performance and area targets of such neural network
accelerators. Moreover, a scalable implementation is required as the recent
studies show that the DNNs may use different precision in different layers.
This paper presents a novel method based on trigonometric expansion properties
of the hyperbolic function for hardware implementation which can be easily
tuned for different accuracy and precision requirements.
</summary>
    <author>
      <name>Mahesh Chandra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MDAT.2021.3063308</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MDAT.2021.3063308" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2007.11976 and
  arXiv:2007.13516</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Design and Test; 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2008.02078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.02078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.02300v2</id>
    <updated>2020-08-08T04:52:31Z</updated>
    <published>2020-08-05T18:10:28Z</published>
    <title>MGPU-TSM: A Multi-GPU System with Truly Shared Memory</title>
    <summary>  The sizes of GPU applications are rapidly growing. They are exhausting the
compute and memory resources of a single GPU, and are demanding the move to
multiple GPUs. However, the performance of these applications scales
sub-linearly with GPU count because of the overhead of data movement across
multiple GPUs. Moreover, a lack of hardware support for coherency exacerbates
the problem because a programmer must either replicate the data across GPUs or
fetch the remote data using high-overhead off-chip links. To address these
problems, we propose a multi-GPU system with truly shared memory (MGPU-TSM),
where the main memory is physically shared across all the GPUs. We eliminate
remote accesses and avoid data replication using an MGPU-TSM system, which
simplifies the memory hierarchy. Our preliminary analysis shows that MGPU-TSM
with 4 GPUs performs, on average, 3.9x? better than the current best performing
multi-GPU configuration for standard application benchmarks.
</summary>
    <author>
      <name>Saiful A. Mojumder</name>
    </author>
    <author>
      <name>Yifan Sun</name>
    </author>
    <author>
      <name>Leila Delshadtehrani</name>
    </author>
    <author>
      <name>Yenai Ma</name>
    </author>
    <author>
      <name>Trinayan Baruah</name>
    </author>
    <author>
      <name>José L. Abellán</name>
    </author>
    <author>
      <name>John Kim</name>
    </author>
    <author>
      <name>David Kaeli</name>
    </author>
    <author>
      <name>Ajay Joshi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.02300v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.02300v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.03124v1</id>
    <updated>2020-07-10T17:56:02Z</updated>
    <published>2020-07-10T17:56:02Z</published>
    <title>Design Space Exploration of Power Delivery For Advanced Packaging
  Technologies</title>
    <summary>  In this paper, a design space exploration of power delivery networks is
performed for multi-chip 2.5-D and 3-D IC technologies. The focus of the paper
is the effective placement of the voltage regulator modules (VRMs) for power
supply noise (PSN) suppression. Multiple on-package VRM configurations have
been analyzed and compared. Additionally, 3D IC chip-on-VRM and
backside-of-the-package VRM configurations are studied. From the PSN
perspective, the 3D IC chip-on-VRM case suppresses the PSN the most even with
high current density hotspots. The paper also studies the impact of different
parameters such as VRM-chip distance on the package, on-chip decoupling
capacitor density, etc. on the PSN.
</summary>
    <author>
      <name>Md Obaidul Hossen</name>
    </author>
    <author>
      <name>Yang Zhang</name>
    </author>
    <author>
      <name>Hesam Fathi Moghadam</name>
    </author>
    <author>
      <name>Yue Zhang</name>
    </author>
    <author>
      <name>Michael Dayringer</name>
    </author>
    <author>
      <name>Muhannad S Bakir</name>
    </author>
    <link href="http://arxiv.org/abs/2008.03124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.03124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.03378v1</id>
    <updated>2020-08-03T12:09:13Z</updated>
    <published>2020-08-03T12:09:13Z</published>
    <title>Bit Parallel 6T SRAM In-memory Computing with Reconfigurable
  Bit-Precision</title>
    <summary>  This paper presents 6T SRAM cell-based bit-parallel in-memory computing (IMC)
architecture to support various computations with reconfigurable bit-precision.
In the proposed technique, bit-line computation is performed with a short WL
followed by BL boosting circuits, which can reduce BL computing delays. By
performing carry-propagation between each near-memory circuit, bit-parallel
complex computations are also enabled by iterating operations with low latency.
In addition, reconfigurable bit-precision is also supported based on
carry-propagation size. Our 128KB in/near memory computing architecture has
been implemented using a 28nm CMOS process, and it can achieve 2.25GHz clock
frequency at 1.0V with 5.2% of area overhead. The proposed architecture also
achieves 0.68, 8.09 TOPS/W for the parallel addition and multiplication,
respectively. In addition, the proposed work also supports a wide range of
supply voltage, from 0.6V to 1.1V.
</summary>
    <author>
      <name>Kyeongho Lee</name>
    </author>
    <author>
      <name>Jinho Jeong</name>
    </author>
    <author>
      <name>Sungsoo Cheon</name>
    </author>
    <author>
      <name>Woong Choi</name>
    </author>
    <author>
      <name>Jongsun Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2020 Design Automation Conference</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2008.03378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.03378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.06502v2</id>
    <updated>2020-11-20T14:06:25Z</updated>
    <published>2020-08-14T08:54:00Z</published>
    <title>Manticore: A 4096-core RISC-V Chiplet Architecture for Ultra-efficient
  Floating-point Computing</title>
    <summary>  Data-parallel problems demand ever growing floating-point (FP) operations per
second under tight area- and energy-efficiency constraints. In this work, we
present Manticore, a general-purpose, ultra-efficient chiplet-based
architecture for data-parallel FP workloads. We have manufactured a prototype
of the chiplet's computational core in Globalfoundries 22FDX process and
demonstrate more than 5x improvement in energy efficiency on FP intensive
workloads compared to CPUs and GPUs. The compute capability at high energy and
area efficiency is provided by Snitch clusters containing eight small integer
cores, each controlling a large FPU. The core supports two custom ISA
extensions: The SSR extension elides explicit load and store instructions by
encoding them as register reads and writes. The FREP extension decouples the
integer core from the FPU allowing floating-point instructions to be issued
independently. These two extensions allow the single-issue core to minimize its
instruction fetch bandwidth and saturate the instruction bandwidth of the FPU,
achieving FPU utilization above 90%, with more than 40% of core area dedicated
to the FPU.
</summary>
    <author>
      <name>Florian Zaruba</name>
    </author>
    <author>
      <name>Fabian Schuiki</name>
    </author>
    <author>
      <name>Luca Benini</name>
    </author>
    <link href="http://arxiv.org/abs/2008.06502v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.06502v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.09954v1</id>
    <updated>2020-08-23T04:04:01Z</updated>
    <published>2020-08-23T04:04:01Z</published>
    <title>Ptolemy: Architecture Support for Robust Deep Learning</title>
    <summary>  Deep learning is vulnerable to adversarial attacks, where carefully-crafted
input perturbations could mislead a well-trained Deep Neural Network to produce
incorrect results. Today's countermeasures to adversarial attacks either do not
have capability to detect adversarial samples at inference time, or introduce
prohibitively high overhead to be practical at inference time.
  We propose Ptolemy, an algorithm-architecture co-designed system that detects
adversarial attacks at inference time with low overhead and high accuracy.We
exploit the synergies between DNN inference and imperative program execution:
an input to a DNN uniquely activates a set of neurons that contribute
significantly to the inference output, analogous to the sequence of basic
blocks exercised by an input in a conventional program. Critically, we observe
that adversarial samples tend to activate distinctive paths from those of
benign inputs. Leveraging this insight, we propose an adversarial sample
detection framework, which uses canary paths generated from offline profiling
to detect adversarial samples at runtime. The Ptolemy compiler along with the
co-designed hardware enable efficient execution by exploiting the unique
algorithmic characteristics. Extensive evaluations show that Ptolemy achieves
higher or similar adversarial example detection accuracy than today's
mechanisms with a much lower runtime (as low as 2%) overhead.
</summary>
    <author>
      <name>Yiming Gan</name>
    </author>
    <author>
      <name>Yuxian Qiu</name>
    </author>
    <author>
      <name>Jingwen Leng</name>
    </author>
    <author>
      <name>Minyi Guo</name>
    </author>
    <author>
      <name>Yuhao Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2008.09954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.09954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.10169v1</id>
    <updated>2020-08-24T03:07:23Z</updated>
    <published>2020-08-24T03:07:23Z</published>
    <title>Tearing Down the Memory Wall</title>
    <summary>  We present a vision for the Erudite architecture that redefines the compute
and memory abstractions such that memory bandwidth and capacity become
first-class citizens along with compute throughput. In this architecture, we
envision coupling a high-density, massively parallel memory technology like
Flash with programmable near-data accelerators, like the streaming
multiprocessors in modern GPUs. Each accelerator has a local pool of
storage-class memory that it can access at high throughput by initiating very
large numbers of overlapping requests that help to tolerate long access
latency. The accelerators can also communicate with each other and remote
memory through a high-throughput low-latency interconnect. As a result, systems
based on the Erudite architecture scale compute and memory bandwidth at the
same rate, tearing down the notorious memory wall that has plagued computer
architecture for generations. In this paper, we present the motivation,
rationale, design, benefit, and research challenges for Erudite.
</summary>
    <author>
      <name>Zaid Qureshi</name>
    </author>
    <author>
      <name>Vikram Sharma Mailthody</name>
    </author>
    <author>
      <name>Seung Won Min</name>
    </author>
    <author>
      <name>I-Hsin Chung</name>
    </author>
    <author>
      <name>Jinjun Xiong</name>
    </author>
    <author>
      <name>Wen-mei Hwu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SRC Techcon 2020 paper. Discusses vision of GPU-Centric architecture,
  Erudite</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.10169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.10169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.10604v1</id>
    <updated>2020-08-24T15:30:21Z</updated>
    <published>2020-08-24T15:30:21Z</published>
    <title>Evaluation of hybrid run-time power models for the ARM big.LITTLE
  architecture</title>
    <summary>  Heterogeneous processors, formed by binary compatible CPU cores with
different microarchitectures, enable energy reductions by better matching
processing capabilities and software application requirements. This new
hardware platform requires novel techniques to manage power and energy to fully
utilize its capabilities, particularly regarding the mapping of workloads to
appropriate cores. In this paper we validate relevant published work related to
power modelling for heterogeneous systems and propose a new approach for
developing run-time power models that uses a hybrid set of physical predictors,
performance events and CPU state information. We demonstrate the accuracy of
this approach compared with the state-of-the-art and its applicability to
energy aware scheduling. Our results are obtained on a commercially available
platform built around the Samsung Exynos 5 Octa SoC, which features the ARM
big.LITTLE heterogeneous architecture.
</summary>
    <author>
      <name>Kris Nikov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Bristol, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Jose L. Nunez-Yanez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Bristol, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Matthew Horsnell</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ARM Ltd., UK</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/EUC.2015.32</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/EUC.2015.32" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 fugures. 2015 IEEE 13th International Conference on
  Embedded and Ubiquitous Computing</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2015 IEEE 13th International Conference on Embedded and Ubiquitous
  Computing, Porto, 2015, pp. 205-210</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2008.10604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.10604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.10682v1</id>
    <updated>2020-08-24T20:03:10Z</updated>
    <published>2020-08-24T20:03:10Z</published>
    <title>ALIGN: A System for Automating Analog Layout</title>
    <summary>  ALIGN ("Analog Layout, Intelligently Generated from Netlists") is an
open-source automatic layout generation flow for analog circuits. ALIGN
translates an input SPICE netlist to an output GDSII layout, specific to a
given technology, as specified by a set of design rules. The flow first
automatically detects hierarchies in the circuit netlist and translates layout
synthesis to a problem of hierarchical block assembly. At the lowest level,
parameterized cells are generated using an abstraction of the design rules;
these blocks are then assembled under geometric and electrical constraints to
build the circuit layout. ALIGN has been applied to generate layouts for a
diverse set of analog circuit families: low frequency analog blocks, wireline
circuits, wireless circuits, and power delivery circuits.
</summary>
    <author>
      <name>Tonmoy Dhar</name>
    </author>
    <author>
      <name>Kishor Kunal</name>
    </author>
    <author>
      <name>Yaguang Li</name>
    </author>
    <author>
      <name>Meghna Madhusudan</name>
    </author>
    <author>
      <name>Jitesh Poojary</name>
    </author>
    <author>
      <name>Arvind K. Sharma</name>
    </author>
    <author>
      <name>Wenbin Xu</name>
    </author>
    <author>
      <name>Steven M. Burns</name>
    </author>
    <author>
      <name>Ramesh Harjani</name>
    </author>
    <author>
      <name>Jiang Hu</name>
    </author>
    <author>
      <name>Desmond A. Kirkpatrick</name>
    </author>
    <author>
      <name>Parijat Mukherjee</name>
    </author>
    <author>
      <name>Sachin S. Sapatnekar</name>
    </author>
    <author>
      <name>Soner Yaldiz</name>
    </author>
    <link href="http://arxiv.org/abs/2008.10682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.10682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.10802v1</id>
    <updated>2020-08-25T03:58:24Z</updated>
    <published>2020-08-25T03:58:24Z</published>
    <title>Optically Connected Memory for Disaggregated Data Centers</title>
    <summary>  Recent advances in integrated photonics enable the implementation of
reconfigurable, high-bandwidth, and low energy-per-bit interconnects in
next-generation data centers. We propose and evaluate an Optically Connected
Memory (OCM) architecture that disaggregates the main memory from the
computation nodes in data centers. OCM is based on micro-ring resonators
(MRRs), and it does not require any modification to the DRAM memory modules. We
calculate energy consumption from real photonic devices and integrate them into
a system simulator to evaluate performance. Our results show that (1) OCM is
capable of interconnecting four DDR4 memory channels to a computing node using
two fibers with 1.07 pJ energy-per-bit consumption and (2) OCM performs up to
5.5x faster than a disaggregated memory with 40G PCIe NIC connectors to
computing nodes.
</summary>
    <author>
      <name>Jorge Gonzalez</name>
    </author>
    <author>
      <name>Alexander Gazman</name>
    </author>
    <author>
      <name>Maarten Hattink</name>
    </author>
    <author>
      <name>Mauricio G. Palma</name>
    </author>
    <author>
      <name>Meisam Bahadori</name>
    </author>
    <author>
      <name>Ruth Rubio-Noriega</name>
    </author>
    <author>
      <name>Lois Orosa</name>
    </author>
    <author>
      <name>Madeleine Glick</name>
    </author>
    <author>
      <name>Onur Mutlu</name>
    </author>
    <author>
      <name>Keren Bergman</name>
    </author>
    <author>
      <name>Rodolfo Azevedo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work is to appear at SBAC-PAD 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.10802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.10802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.11367v1</id>
    <updated>2020-08-26T03:55:22Z</updated>
    <published>2020-08-26T03:55:22Z</published>
    <title>Mitigating the Latency-Area Tradeoffs for DRAM Design with
  Coarse-Grained Monolithic 3D (M3D) Integration</title>
    <summary>  Over the years, the DRAM latency has not scaled proportionally with its
density due to the cost-centric mindset of the DRAM industry. Prior work has
shown that this shortcoming can be overcome by reducing the critical length of
DRAM access path. However, doing so decreases DRAM area-efficiency,
exacerbating the latency-area tradeoffs for DRAM design. In this paper, we show
that reorganizing DRAM cell-arrays using the emerging monolithic 3D (M3D)
integration technology can mitigate these fundamental latency-area tradeoffs.
Based on our evaluation results for PARSEC benchmarks, our designed M3D DRAM
cell-array organizations can yield up to 9.56% less latency, up to 4.96% less
power consumption, and up to 21.21% less energy-delay product (EDP), with up to
14% less DRAM die area, com-pared to the conventional 2D DDR4 DRAM.
</summary>
    <author>
      <name>Chao-Hsuan Huang</name>
    </author>
    <author>
      <name>Ishan G Thakkar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in ICCD 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.11367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.11591v1</id>
    <updated>2020-08-26T14:47:30Z</updated>
    <published>2020-08-26T14:47:30Z</published>
    <title>An Approximate Carry Estimating Simultaneous Adder with Rectification</title>
    <summary>  Approximate computing has in recent times found significant applications
towards lowering power, area, and time requirements for arithmetic operations.
Several works done in recent years have furthered approximate computing along
these directions. In this work, we propose a new approximate adder that employs
a carry prediction method. This allows parallel propagation of the carry
allowing faster calculations. In addition to the basic adder design, we also
propose a rectification logic which would enable higher accuracy for larger
computations. Experimental results show that our adder produces results 91.2%
faster than the conventional ripple-carry adder. In terms of accuracy, the
addition of rectification logic to the basic design produces results that are
more accurate than state-of-the-art adders like SARA and BCSA by 74%.
</summary>
    <author>
      <name>Rajat Bhattacharjya</name>
    </author>
    <author>
      <name>Vishesh Mishra</name>
    </author>
    <author>
      <name>Saurabh Singh</name>
    </author>
    <author>
      <name>Kaustav Goswami</name>
    </author>
    <author>
      <name>Dip Sankar Banerjee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3386263.3406928</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3386263.3406928" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at the 30th ACM Great Lakes Symposium on VLSI</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.11591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.11669v1</id>
    <updated>2020-08-26T16:50:37Z</updated>
    <published>2020-08-26T16:50:37Z</published>
    <title>An 8-bit In Resistive Memory Computing Core with Regulated Passive
  Neuron and Bit Line Weight Mapping</title>
    <summary>  The rapid development of Artificial Intelligence (AI) and Internet of Things
(IoT) increases the requirement for edge computing with low power and
relatively high processing speed devices. The Computing-In-Memory(CIM) schemes
based on emerging resistive Non-Volatile Memory(NVM) show great potential in
reducing the power consumption for AI computing. However, the device
inconsistency of the non-volatile memory may significantly degenerate the
performance of the neural network. In this paper, we propose a low power
Resistive RAM (RRAM) based CIM core to not only achieve high computing
efficiency but also greatly enhance the robustness by bit line regulator and
bit line weight mapping algorithm. The simulation results show that the power
consumption of our proposed 8-bit CIM core is only 3.61mW (256*256). The SFDR
and SNDR of the CIM core achieve 59.13 dB and 46.13 dB, respectively. The
proposed bit line weight mapping scheme improves the top-1 accuracy by 2.46%
and 3.47% for AlexNet and VGG16 on ImageNet Large Scale Visual Recognition
Competition 2012 (ILSVRC 2012) in 8-bit mode, respectively.
</summary>
    <author>
      <name>Yewei Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Student Member, IEEE</arxiv:affiliation>
    </author>
    <author>
      <name>Kejie Huang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Senior Member, IEEE</arxiv:affiliation>
    </author>
    <author>
      <name>Rui Xiao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Student Member, IEEE</arxiv:affiliation>
    </author>
    <author>
      <name>Haibin Shen</name>
    </author>
    <link href="http://arxiv.org/abs/2008.11669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.13430v1</id>
    <updated>2020-08-31T08:41:51Z</updated>
    <published>2020-08-31T08:41:51Z</published>
    <title>Architectural Analysis of FPGA Technology Impact</title>
    <summary>  The use of high-level languages for designing hardware is gaining popularity
since they increase design productivity by providing higher abstractions.
However, one drawback of such abstraction level has been the difficulty of
relating the low-level implementation problems back to the original high-level
design, which is paramount for architectural optimization. In this work
(developed between April 2013 and April 2014), we propose a methodology to
analyze the effects of technology over the architecture, and to generate
architectural-level area, delay and power metrics. Such feedback allows the
designer to quickly gauge the impact of architectural decisions on the quality
of generated hardware and opens the door to automatic architectural analysis.
We demonstrate the use of our technique on three FPGA platforms using two
designs: a Reed-Solomon error correction decoder and a 32-bit pipelined
processor implementation.
</summary>
    <author>
      <name>Oriol Arcas-Abella</name>
    </author>
    <author>
      <name>Abhinav Agarwal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.13430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.13430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.6.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.13664v2</id>
    <updated>2021-04-02T15:48:17Z</updated>
    <published>2020-08-31T15:03:16Z</published>
    <title>Machine Learning Clustering Techniques for Selective Mitigation of
  Critical Design Features</title>
    <summary>  Selective mitigation or selective hardening is an effective technique to
obtain a good trade-off between the improvements in the overall reliability of
a circuit and the hardware overhead induced by the hardening techniques.
Selective mitigation relies on preferentially protecting circuit instances
according to their susceptibility and criticality. However, ranking circuit
parts in terms of vulnerability usually requires computationally intensive
fault-injection simulation campaigns. This paper presents a new methodology
which uses machine learning clustering techniques to group flip-flops with
similar expected contributions to the overall functional failure rate, based on
the analysis of a compact set of features combining attributes from static
elements and dynamic elements. Fault simulation campaigns can then be executed
on a per-group basis, significantly reducing the time and cost of the
evaluation. The effectiveness of grouping similar sensitive flip-flops by
machine learning clustering algorithms is evaluated on a practical
example.Different clustering algorithms are applied and the results are
compared to an ideal selective mitigation obtained by exhaustive
fault-injection simulation.
</summary>
    <author>
      <name>Thomas Lange</name>
    </author>
    <author>
      <name>Aneesh Balakrishnan</name>
    </author>
    <author>
      <name>Maximilien Glorieux</name>
    </author>
    <author>
      <name>Dan Alexandrescu</name>
    </author>
    <author>
      <name>Luca Sterpone</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IOLTS50870.2020.9159751</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IOLTS50870.2020.9159751" rel="related"/>
    <link href="http://arxiv.org/abs/2008.13664v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.13664v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.00202v1</id>
    <updated>2020-09-01T03:22:14Z</updated>
    <published>2020-09-01T03:22:14Z</published>
    <title>Helper Without Threads: Customized Prefetching for Delinquent Irregular
  Loads</title>
    <summary>  The growing memory footprints of cloud and big data applications mean that
data center CPUs can spend significant time waiting for memory. An attractive
approach to improving performance in such centralized compute settings is to
employ prefetchers that are customized per application, where gains can be
easily scaled across thousands of machines. Helper thread prefetching is such a
technique but has yet to achieve wide adoption since it requires spare thread
contexts or special hardware/firmware support. In this paper, we propose an
inline software prefetching technique that overcomes these restrictions by
inserting the helper code into the main thread itself. Our approach is
complementary to and does not interfere with existing hardware prefetchers
since we target only delinquent irregular load instructions (those with no
constant or striding address patterns). For each chosen load instruction, we
generate and insert a customized software prefetcher extracted from and
mimicking the application's dataflow, all without access to the application
source code. For a set of irregular workloads that are memory-bound, we
demonstrate up to 2X single-thread performance improvement on recent high-end
hardware (Intel Skylake) and up to 83% speedup over a helper thread
implementation on the same hardware, due to the absence of thread spawning
overhead.
</summary>
    <author>
      <name>Karthik Sankaranarayanan</name>
    </author>
    <author>
      <name>Chit-Kwan Lin</name>
    </author>
    <author>
      <name>Gautham Chinya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.00202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.00202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.00223v1</id>
    <updated>2020-09-01T04:48:33Z</updated>
    <published>2020-09-01T04:48:33Z</published>
    <title>RISC micrprocessor verification</title>
    <summary>  Today's microprocessors have grown significantly in complexity and
functionality. Most of today's processors provide at least three levels of
memory hierarchy, are heavily pipelined, and support some sort of cache
coherency protocol. These features are extremely complex and sophisticated, and
present their own set of unique verification challenges. Verification is
clearly not a point tool, but is part of a process that starts from initial
product conception and is to some degrees complete when the product goes to
market. Functional verification is necessary to verify the functionality at RTL
level. Complex micro-processors like ARM are high performance, low cost and low
power 32-bit RISC processors. In our paper complex microprocessor is ARM cortex
M3, developed for the embedded applications having low interrupt latency, low
gate count, 3- stage pipelining, branch prediction, THUMB and THUMB-2
instruction set. Functional verification is used to verify that the circuit
full fills each abstract assertion under the implementation mapping. we explore
several aspects of processor design, including caches, pipeline depth, ALUs,
and bypass logic.The verification was done concurrently with the design
implementation of the processor.
</summary>
    <author>
      <name>Mitul S Nagar</name>
    </author>
    <author>
      <name>Haresh A Suthar</name>
    </author>
    <author>
      <name>Chintan Panchal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 7 images ans a table. ISBN No: 978-81-906220-3 -5</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International conference on Knowledge analysis ans research in
  engineering technology and science 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2009.00223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.00223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.00457v1</id>
    <updated>2020-08-27T20:36:34Z</updated>
    <published>2020-08-27T20:36:34Z</published>
    <title>Direct CMOS Implementation of Neuromorphic Temporal Neural Networks for
  Sensory Processing</title>
    <summary>  Temporal Neural Networks (TNNs) use time as a resource to represent and
process information, mimicking the behavior of the mammalian neocortex. This
work focuses on implementing TNNs using off-the-shelf digital CMOS technology.
A microarchitecture framework is introduced with a hierarchy of building blocks
including: multi-neuron columns, multi-column layers, and multi-layer TNNs. We
present the direct CMOS gate-level implementation of the multi-neuron column
model as the key building block for TNNs. Post-synthesis results are obtained
using Synopsys tools and the 45 nm CMOS standard cell library. The TNN
microarchitecture framework is embodied in a set of characteristic equations
for assessing the total gate count, die area, compute time, and power
consumption for any TNN design. We develop a multi-layer TNN prototype of 32M
gates. In 7 nm CMOS process, it consumes only 1.54 mm^2 die area and 7.26 mW
power and can process 28x28 images at 107M FPS (9.34 ns per image). We evaluate
the prototype's performance and complexity relative to a recent
state-of-the-art TNN model.
</summary>
    <author>
      <name>Harideep Nair</name>
    </author>
    <author>
      <name>John Paul Shen</name>
    </author>
    <author>
      <name>James E. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submission Under Review for an IEEE Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.00457v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.00457v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.00715v1</id>
    <updated>2020-09-01T21:16:51Z</updated>
    <published>2020-09-01T21:16:51Z</published>
    <title>A Survey on Recent Hardware Data Prefetching Approaches with An Emphasis
  on Servers</title>
    <summary>  Data prefetching, i.e., the act of predicting application's future memory
accesses and fetching those that are not in the on-chip caches, is a well-known
and widely-used approach to hide the long latency of memory accesses. The
fruitfulness of data prefetching is evident to both industry and academy:
nowadays, almost every high-performance processor incorporates a few data
prefetchers for capturing various access patterns of applications; besides,
there is a myriad of proposals for data prefetching in the research literature,
where each proposal enhances the efficiency of prefetching in a specific way.
In this survey, we discuss the fundamental concepts in data prefetching and
study state-of-the-art hardware data prefetching approaches. Additional Key
Words and Phrases: Data Prefetching, Scale-Out Workloads, Server Processors,
and Spatio-Temporal Correlation.
</summary>
    <author>
      <name>Mohammad Bakhshalipour</name>
    </author>
    <author>
      <name>Mehran Shakerinava</name>
    </author>
    <author>
      <name>Fatemeh Golshan</name>
    </author>
    <author>
      <name>Ali Ansari</name>
    </author>
    <author>
      <name>Pejman Lotfi-Karman</name>
    </author>
    <author>
      <name>Hamid Sarbazi-Azad</name>
    </author>
    <link href="http://arxiv.org/abs/2009.00715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.00715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.00748v1</id>
    <updated>2020-09-01T23:39:35Z</updated>
    <published>2020-09-01T23:39:35Z</published>
    <title>TensorDash: Exploiting Sparsity to Accelerate Deep Neural Network
  Training and Inference</title>
    <summary>  TensorDash is a hardware level technique for enabling data-parallel MAC units
to take advantage of sparsity in their input operand streams. When used to
compose a hardware accelerator for deep learning, TensorDash can speedup the
training process while also increasing energy efficiency. TensorDash combines a
low-cost, sparse input operand interconnect comprising an 8-input multiplexer
per multiplier input, with an area-efficient hardware scheduler. While the
interconnect allows a very limited set of movements per operand, the scheduler
can effectively extract sparsity when it is present in the activations, weights
or gradients of neural networks. Over a wide set of models covering various
applications, TensorDash accelerates the training process by $1.95{\times}$
while being $1.89\times$ more energy-efficient, $1.6\times$ more energy
efficient when taking on-chip and off-chip memory accesses into account. While
TensorDash works with any datatype, we demonstrate it with both
single-precision floating-point units and bfloat16.
</summary>
    <author>
      <name>Mostafa Mahmoud</name>
    </author>
    <author>
      <name>Isak Edo</name>
    </author>
    <author>
      <name>Ali Hadi Zadeh</name>
    </author>
    <author>
      <name>Omar Mohamed Awad</name>
    </author>
    <author>
      <name>Gennady Pekhimenko</name>
    </author>
    <author>
      <name>Jorge Albericio</name>
    </author>
    <author>
      <name>Andreas Moshovos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MICRO50266.2020.00069</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MICRO50266.2020.00069" rel="related"/>
    <link href="http://arxiv.org/abs/2009.00748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.00748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.00804v2</id>
    <updated>2021-12-24T14:59:46Z</updated>
    <published>2020-09-02T03:36:24Z</published>
    <title>Architectural Implications of Graph Neural Networks</title>
    <summary>  Graph neural networks (GNN) represent an emerging line of deep learning
models that operate on graph structures. It is becoming more and more popular
due to its high accuracy achieved in many graph-related tasks. However, GNN is
not as well understood in the system and architecture community as its
counterparts such as multi-layer perceptrons and convolutional neural networks.
This work tries to introduce the GNN to our community. In contrast to prior
work that only presents characterizations of GCNs, our work covers a large
portion of the varieties for GNN workloads based on a general GNN description
framework. By constructing the models on top of two widely-used libraries, we
characterize the GNN computation at inference stage concerning general-purpose
and application-specific architectures and hope our work can foster more system
and architecture research for GNNs.
</summary>
    <author>
      <name>Zhihui Zhang</name>
    </author>
    <author>
      <name>Jingwen Leng</name>
    </author>
    <author>
      <name>Lingxiao Ma</name>
    </author>
    <author>
      <name>Youshan Miao</name>
    </author>
    <author>
      <name>Chao Li</name>
    </author>
    <author>
      <name>Minyi Guo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LCA.2020.2988991</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LCA.2020.2988991" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, published in IEEE Computer Architecture Letters (CAL) 2020</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">in IEEE Computer Architecture Letters, vol. 19, no. 1, pp. 59-62,
  1 Jan.-June 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2009.00804v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.00804v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.00881v1</id>
    <updated>2020-09-02T08:12:58Z</updated>
    <published>2020-09-02T08:12:58Z</published>
    <title>CONTRA: Area-Constrained Technology Mapping Framework For Memristive
  Memory Processing Unit</title>
    <summary>  Data-intensive applications are poised to benefit directly from
processing-in-memory platforms, such as memristive Memory Processing Units,
which allow leveraging data locality and performing stateful logic operations.
Developing design automation flows for such platforms is a challenging and
highly relevant research problem. In this work, we investigate the problem of
minimizing delay under arbitrary area constraint for MAGIC-based in-memory
computing platforms. We propose an end-to-end area constrained technology
mapping framework, CONTRA. CONTRA uses Look-Up Table(LUT) based mapping of the
input function on the crossbar array to maximize parallel operations and uses a
novel search technique to move data optimally inside the array. CONTRA supports
benchmarks in a variety of formats, along with crossbar dimensions as input to
generate MAGIC instructions. CONTRA scales for large benchmarks, as
demonstrated by our experiments. CONTRA allows mapping benchmarks to smaller
crossbar dimensions than achieved by any other technique before, while allowing
a wide variety of area-delay trade-offs. CONTRA improves the composite metric
of area-delay product by 2.1x to 13.1x compared to seven existing technology
mapping approaches.
</summary>
    <author>
      <name>Debjyoti Bhattacharjee</name>
    </author>
    <author>
      <name>Anupam Chattopadhyay</name>
    </author>
    <author>
      <name>Srijit Dutta</name>
    </author>
    <author>
      <name>Ronny Ronen</name>
    </author>
    <author>
      <name>Shahar Kvatinsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.00881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.00881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
