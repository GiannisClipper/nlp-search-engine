<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acs.CV%26id_list%3D%26start%3D0%26max_results%3D1100" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:cs.CV&amp;id_list=&amp;start=0&amp;max_results=1100</title>
  <id>http://arxiv.org/api/gT1Dyv/d18MWNi0Sb5sFI36XqAg</id>
  <updated>2025-05-27T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">156217</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1100</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/0903.0134v2</id>
    <updated>2010-01-08T10:32:52Z</updated>
    <published>2009-03-01T11:10:27Z</published>
    <title>Recognition of Regular Shapes in Satelite Images</title>
    <summary>  This paper has been withdrawn by the author ali pourmohammad.
</summary>
    <author>
      <name>Ahmad Reza Eskandari</name>
    </author>
    <author>
      <name>Ali Pourmohammad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.0134v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.0134v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.0422v1</id>
    <updated>2010-10-03T16:55:56Z</updated>
    <published>2010-10-03T16:55:56Z</published>
    <title>Convolutional Matching Pursuit and Dictionary Training</title>
    <summary>  Matching pursuit and K-SVD is demonstrated in the translation invariant
setting
</summary>
    <author>
      <name>Arthur Szlam</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <link href="http://arxiv.org/abs/1010.0422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.0422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.7120v1</id>
    <updated>2014-06-27T09:18:44Z</updated>
    <published>2014-06-27T09:18:44Z</published>
    <title>Template Matching based Object Detection Using HOG Feature Pyramid</title>
    <summary>  This article provides a step by step development of designing a Object
Detection scheme using the HOG based Feature Pyramid aligned with the concept
of Template Matching.
</summary>
    <author>
      <name>Anish Acharya</name>
    </author>
    <link href="http://arxiv.org/abs/1406.7120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.7120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01243v1</id>
    <updated>2017-07-05T07:43:00Z</updated>
    <published>2017-07-05T07:43:00Z</published>
    <title>Exploration of object recognition from 3D point cloud</title>
    <summary>  We present our latest experiment results of object recognition from 3D point
cloud data collected through moving car.
</summary>
    <author>
      <name>Lin Duan</name>
    </author>
    <link href="http://arxiv.org/abs/1707.01243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11643v1</id>
    <updated>2018-07-31T02:51:21Z</updated>
    <published>2018-07-31T02:51:21Z</published>
    <title>Brain MRI Image Super Resolution using Phase Stretch Transform and
  Transfer Learning</title>
    <summary>  A hallucination-free and computationally efficient algorithm for enhancing
the resolution of brain MRI images is demonstrated.
</summary>
    <author>
      <name>Sifeng He</name>
    </author>
    <author>
      <name>Bahram Jalali</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.10788v1</id>
    <updated>2019-01-30T12:47:44Z</updated>
    <published>2019-01-30T12:47:44Z</published>
    <title>Blurred Images Lead to Bad Local Minima</title>
    <summary>  Blurred Images Lead to Bad Local Minima
</summary>
    <author>
      <name>Gal Katzhendler</name>
    </author>
    <author>
      <name>Daphna Weinshall</name>
    </author>
    <link href="http://arxiv.org/abs/1901.10788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.13806v1</id>
    <updated>2021-10-25T15:51:35Z</updated>
    <published>2021-10-25T15:51:35Z</published>
    <title>Detecting speaking persons in video</title>
    <summary>  We present a novel method for detecting speaking persons in video, by
extracting facial landmarks with a neural network and analysing these landmarks
statistically over time
</summary>
    <author>
      <name>Hannes Fassold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for MMSP 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.13806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.13806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.05219v1</id>
    <updated>2018-12-13T01:36:36Z</updated>
    <published>2018-12-13T01:36:36Z</published>
    <title>Advances of Scene Text Datasets</title>
    <summary>  This article introduces publicly available datasets in scene text detection
and recognition. The information is as of 2017.
</summary>
    <author>
      <name>Masakazu Iwamura</name>
    </author>
    <link href="http://arxiv.org/abs/1812.05219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.05219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.07245v1</id>
    <updated>2019-09-16T14:44:19Z</updated>
    <published>2019-09-16T14:44:19Z</published>
    <title>BMVC 2019: Workshop on Interpretable and Explainable Machine Vision</title>
    <summary>  Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable
Machine Vision, Cardiff, UK, September 12, 2019.
</summary>
    <author>
      <name>Alun Preece</name>
    </author>
    <link href="http://arxiv.org/abs/1909.07245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.07245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.09016v1</id>
    <updated>2020-10-18T16:16:50Z</updated>
    <published>2020-10-18T16:16:50Z</published>
    <title>Covapixels</title>
    <summary>  We propose and discuss the summarization of superpixel-type image
tiles/patches using mean and covariance information. We refer to the resulting
objects as covapixels.
</summary>
    <author>
      <name>Jeffrey Uhlmann</name>
    </author>
    <link href="http://arxiv.org/abs/2010.09016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.10556v1</id>
    <updated>2022-05-21T10:25:00Z</updated>
    <published>2022-05-21T10:25:00Z</published>
    <title>Cycle-GAN for eye-tracking</title>
    <summary>  This manuscript presents a not typical implementation of the cycle generative
adversarial networks (Cycle-GAN) method for eye-tracking tasks.
</summary>
    <author>
      <name>Ildar Rakhmatulin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.10556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.10556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406047v1</id>
    <updated>2004-06-24T13:14:58Z</updated>
    <published>2004-06-24T13:14:58Z</published>
    <title>Self-organizing neural networks in classification and image recognition</title>
    <summary>  Self-organizing neural networks are used for brick finding in OPERA
experiment. Self-organizing neural networks and wavelet analysis used for
recognition and extraction of car numbers from images.
</summary>
    <author>
      <name>G. A. Ososkov</name>
    </author>
    <author>
      <name>S. G. Dmitrievskiy</name>
    </author>
    <author>
      <name>A. V. Stadnik</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0406047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.2788v2</id>
    <updated>2010-01-08T10:23:17Z</updated>
    <published>2009-02-16T21:13:35Z</published>
    <title>Using SLP Neural Network to Persian Handwritten Digits Recognition</title>
    <summary>  This paper has been withdrawn by the author ali pourmohammad.
</summary>
    <author>
      <name>Ali Pourmohammad</name>
    </author>
    <author>
      <name>Seyed Mohammad Ahadi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/0902.2788v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.2788v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.5120v1</id>
    <updated>2009-06-28T08:10:33Z</updated>
    <published>2009-06-28T08:10:33Z</published>
    <title>Comments on "A new combination of evidence based on compromise" by K.
  Yamada</title>
    <summary>  Comments on ``A new combination of evidence based on compromise'' by K.
Yamada
</summary>
    <author>
      <name>Jean Dezert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ONERA</arxiv:affiliation>
    </author>
    <author>
      <name>Arnaud Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">E3I2</arxiv:affiliation>
    </author>
    <author>
      <name>Florentin Smarandache</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UNM</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Fuzzy Sets and Systems 160, 6 (2009) 853-855</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0906.5120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.5120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4; I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.1369v1</id>
    <updated>2009-08-10T18:33:51Z</updated>
    <published>2009-08-10T18:33:51Z</published>
    <title>Segmentation for radar images based on active contour</title>
    <summary>  We exam various geometric active contour methods for radar image
segmentation. Due to special properties of radar images, we propose our new
model based on modified Chan-Vese functional. Our method is efficient in
separating non-meteorological noises from meteorological images.
</summary>
    <author>
      <name>Meijun Zhu</name>
    </author>
    <author>
      <name>Pengfei Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/0908.1369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.1369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4793v1</id>
    <updated>2010-04-27T13:22:36Z</updated>
    <published>2010-04-27T13:22:36Z</published>
    <title>Logical methods of object recognition on satellite images using spatial
  constraints</title>
    <summary>  A logical approach to object recognition on image is proposed. The main idea
of the approach is to perform the object recognition as a logical inference on
a set of rules describing an object shape.
</summary>
    <author>
      <name>R. K. Fedorov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.4793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.8; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.2085v1</id>
    <updated>2011-07-11T18:44:17Z</updated>
    <published>2011-07-11T18:44:17Z</published>
    <title>Kunchenko's Polynomials for Template Matching</title>
    <summary>  This paper reviews Kunchenko's polynomials using as template matching method
to recognize template in one-dimensional input signal. Kunchenko's polynomials
method is compared with classical methods - cross-correlation and sum of
squared differences according to numerical statistical example.
</summary>
    <author>
      <name>Oleg Chertov</name>
    </author>
    <author>
      <name>Taras Slipets</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.2085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.2085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.0466v1</id>
    <updated>2011-11-02T11:42:37Z</updated>
    <published>2011-11-02T11:42:37Z</published>
    <title>Kernel diff-hash</title>
    <summary>  This paper presents a kernel formulation of the recently introduced diff-hash
algorithm for the construction of similarity-sensitive hash functions. Our
kernel diff-hash algorithm that shows superior performance on the problem of
image feature descriptor matching.
</summary>
    <author>
      <name>Michael M Bronstein</name>
    </author>
    <link href="http://arxiv.org/abs/1111.0466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.0466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.1615v1</id>
    <updated>2012-04-07T09:28:19Z</updated>
    <published>2012-04-07T09:28:19Z</published>
    <title>Discrimination between Arabic and Latin from bilingual documents</title>
    <summary>  2011 International Conference on Communications, Computing and Control
Applications (CCCA)
</summary>
    <author>
      <name>Sofiene Haboubi</name>
    </author>
    <author>
      <name>Samia Maddouri</name>
    </author>
    <author>
      <name>Hamid Amiri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CCCA.2011.6031496</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CCCA.2011.6031496" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.1615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.1615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0819v1</id>
    <updated>2012-12-04T18:39:14Z</updated>
    <published>2012-12-04T18:39:14Z</published>
    <title>A Topological Code for Plane Images</title>
    <summary>  It is proposed a new code for contours of plane images. This code was applied
for optical character recognition of printed and handwritten characters. One
can apply it to recognition of any visual images.
</summary>
    <author>
      <name>Evgeny Shchepin</name>
    </author>
    <link href="http://arxiv.org/abs/1212.0819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.1829v1</id>
    <updated>2013-03-07T21:15:29Z</updated>
    <published>2013-03-07T21:15:29Z</published>
    <title>Watersheds on edge or node weighted graphs "par l'exemple"</title>
    <summary>  Watersheds have been defined both for node and edge weighted graphs. We show
that they are identical: for each edge (resp.\ node) weighted graph exists a
node (resp. edge) weighted graph with the same minima and catchment basin.
</summary>
    <author>
      <name>Fernand Meyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.1829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.1829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7948v2</id>
    <updated>2013-06-02T20:32:05Z</updated>
    <published>2013-04-30T10:41:26Z</published>
    <title>Convolutional Neural Networks learn compact local image descriptors</title>
    <summary>  A standard deep convolutional neural network paired with a suitable loss
function learns compact local image descriptors that perform comparably to
state-of-the art approaches.
</summary>
    <author>
      <name>Christian Osendorfer</name>
    </author>
    <author>
      <name>Justin Bayer</name>
    </author>
    <author>
      <name>Patrick van der Smagt</name>
    </author>
    <link href="http://arxiv.org/abs/1304.7948v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7948v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.5905v1</id>
    <updated>2013-05-25T09:30:49Z</updated>
    <published>2013-05-25T09:30:49Z</published>
    <title>ÖAGM/AAPR 2013 - The 37th Annual Workshop of the Austrian Association
  for Pattern Recognition</title>
    <summary>  In this editorial, the organizers summarize facts and background about the
event.
</summary>
    <author>
      <name>Justus Piater</name>
    </author>
    <author>
      <name>Antonio J. Rodríguez Sánchez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876)</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.5905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.5905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.0261v1</id>
    <updated>2013-09-01T20:35:17Z</updated>
    <published>2013-09-01T20:35:17Z</published>
    <title>Multi-Column Deep Neural Networks for Offline Handwritten Chinese
  Character Classification</title>
    <summary>  Our Multi-Column Deep Neural Networks achieve best known recognition rates on
Chinese characters from the ICDAR 2011 and 2013 offline handwriting
competitions, approaching human performance.
</summary>
    <author>
      <name>Dan Cireşan</name>
    </author>
    <author>
      <name>Jürgen Schmidhuber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, IDSIA tech report</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.0261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.0261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.0365v1</id>
    <updated>2013-10-01T16:06:00Z</updated>
    <published>2013-10-01T16:06:00Z</published>
    <title>The complex-valued encoding for dicision-making based on aliasing data</title>
    <summary>  It is proposed a complex valued channel encoding for multidimensional data.
The basic approach contains overlapping of complex nonlinear mappings. Its
development leads to sparse representation of multi-channel data, increasing
their dimensions and the distance between the images.
</summary>
    <author>
      <name>P. A. Golovinski</name>
    </author>
    <author>
      <name>V. A. Astapenko</name>
    </author>
    <link href="http://arxiv.org/abs/1310.0365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.0365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.5590v1</id>
    <updated>2014-03-21T23:26:47Z</updated>
    <published>2014-03-21T23:26:47Z</published>
    <title>Continuous Optimization for Fields of Experts Denoising Works</title>
    <summary>  Several recent papers use image denoising with a Fields of Experts prior to
benchmark discrete optimization methods. We show that a non-linear least
squares solver significantly outperforms all known discrete methods on this
problem.
</summary>
    <author>
      <name>Petter Strandmark</name>
    </author>
    <author>
      <name>Sameer Agarwal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.5590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.5590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.1446v1</id>
    <updated>2014-11-05T23:14:10Z</updated>
    <published>2014-11-05T23:14:10Z</published>
    <title>Electrocardiography Separation of Mother and Baby</title>
    <summary>  Extraction of Electrocardiography (ECG or EKG) signals of mother and baby is
a challenging task, because one single device is used and it receives a mixture
of multiple heart beats. In this paper, we would like to design a filter to
separate the signals from each other.
</summary>
    <author>
      <name>Wei Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1411.1446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.1446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5796v1</id>
    <updated>2014-12-18T10:32:57Z</updated>
    <published>2014-12-18T10:32:57Z</published>
    <title>Image Enhancement Using a Generalization of Homographic Function</title>
    <summary>  This paper presents a new method of gray level image enhancement, based on
point transforms. In order to define the transform function, it was used a
generalization of the homographic function.
</summary>
    <author>
      <name>Vasile Patrascu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The IEEE International Conference COMMUNICATIONS 2002, pp. 429-434,
  December 5-7, 2002, Bucharest, Romania</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.5796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.01994v1</id>
    <updated>2015-11-06T07:18:56Z</updated>
    <published>2015-11-06T07:18:56Z</published>
    <title>Next Generation Multicuts for Semi-Planar Graphs</title>
    <summary>  We study the problem of multicut segmentation. We introduce modified versions
of the Semi-PlanarCC based on bounding Lagrange multipliers. We apply our work
to natural image segmentation.
</summary>
    <author>
      <name>Julian Yarkony</name>
    </author>
    <link href="http://arxiv.org/abs/1511.01994v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.01994v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.07963v1</id>
    <updated>2015-11-25T06:09:42Z</updated>
    <published>2015-11-25T06:09:42Z</published>
    <title>Calculate distance to object in the area where car, using video analysis</title>
    <summary>  The method of using video cameras installed on the car, to calculate the
distance to the object in its area of movement.
</summary>
    <author>
      <name>Elena Legchekova</name>
    </author>
    <author>
      <name>Oleg Titov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, in Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.07963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.07963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.07021v1</id>
    <updated>2016-01-26T13:41:48Z</updated>
    <published>2016-01-26T13:41:48Z</published>
    <title>Polyhedron Volume-Ratio-based Classification for Image Recognition</title>
    <summary>  In this paper, a novel method, called polyhedron volume ratio classification
(PVRC) is proposed for image recognition
</summary>
    <author>
      <name>Qingxiang Feng</name>
    </author>
    <author>
      <name>Jeng-Shyang Pan</name>
    </author>
    <author>
      <name>Jar-Ferr Yang</name>
    </author>
    <author>
      <name>Yang-Ting Chou</name>
    </author>
    <link href="http://arxiv.org/abs/1601.07021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05518v2</id>
    <updated>2020-11-12T06:45:06Z</updated>
    <published>2016-08-19T07:30:33Z</published>
    <title>On the Existence of a Projective Reconstruction</title>
    <summary>  In this note we study the connection between the existence of a projective
reconstruction and the existence of a fundamental matrix satisfying the
epipolar constraints.
</summary>
    <author>
      <name>Hon-Leung Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05518v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05518v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08380v1</id>
    <updated>2017-01-29T13:42:14Z</updated>
    <published>2017-01-29T13:42:14Z</published>
    <title>The HASYv2 dataset</title>
    <summary>  This paper describes the HASYv2 dataset. HASY is a publicly available, free
of charge dataset of single symbols similar to MNIST. It contains 168233
instances of 369 classes. HASY contains two challenges: A classification
challenge with 10 pre-defined folds for 10-fold cross-validation and a
verification challenge.
</summary>
    <author>
      <name>Martin Thoma</name>
    </author>
    <link href="http://arxiv.org/abs/1701.08380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.01507v1</id>
    <updated>2017-02-06T06:32:14Z</updated>
    <published>2017-02-06T06:32:14Z</published>
    <title>Challenge of Multi-Camera Tracking</title>
    <summary>  Multi-camera tracking is quite different from single camera tracking, and it
faces new technology and system architecture challenges. By analyzing the
corresponding characteristics and disadvantages of the existing algorithms,
problems in multi-camera tracking are summarized and some new directions for
future work are also generalized.
</summary>
    <author>
      <name>Yong Wang</name>
    </author>
    <author>
      <name>Ke Lu</name>
    </author>
    <link href="http://arxiv.org/abs/1702.01507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00523v1</id>
    <updated>2017-03-01T21:41:58Z</updated>
    <published>2017-03-01T21:41:58Z</published>
    <title>ISIC 2017 - Skin Lesion Analysis Towards Melanoma Detection</title>
    <summary>  Our system addresses Part 1, Lesion Segmentation and Part 3, Lesion
Classification of the ISIC 2017 challenge. Both algorithms make use of deep
convolutional networks to achieve the challenge objective.
</summary>
    <author>
      <name>Matt Berseth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISIC2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.00523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05165v2</id>
    <updated>2017-09-28T02:26:32Z</updated>
    <published>2017-03-15T14:18:23Z</published>
    <title>Automatic skin lesion segmentation with fully
  convolutional-deconvolutional networks</title>
    <summary>  This paper summarizes our method and validation results for the ISBI
Challenge 2017 - Skin Lesion Analysis Towards Melanoma Detection - Part I:
Lesion Segmentation
</summary>
    <author>
      <name>Yading Yuan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JBHI.2017.2787487</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JBHI.2017.2787487" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISIC2017 challenge, 4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Journal of Biomedical and Health Informatics, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1703.05165v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05165v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06942v1</id>
    <updated>2017-06-21T14:54:15Z</updated>
    <published>2017-06-21T14:54:15Z</published>
    <title>Graphcut Texture Synthesis for Single-Image Superresolution</title>
    <summary>  Texture synthesis has proven successful at imitating a wide variety of
textures. Adding additional constraints (in the form of a low-resolution
version of the texture to be synthesized) makes it possible to use texture
synthesis methods for texture superresolution.
</summary>
    <author>
      <name>Douglas Summers-Stay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NYU Master's Thesis from 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.06942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.03088v2</id>
    <updated>2017-07-12T04:38:13Z</updated>
    <published>2017-07-11T00:28:23Z</published>
    <title>Online Handwritten Mathematical Expressions Recognition System Using
  Fuzzy Neural Network</title>
    <summary>  The article describes developed information technology for online recognition
of handwritten mathematical expressions that based on proposed approaches to
handwritten symbols recognition and structural analysis.
</summary>
    <author>
      <name>E. Naderan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Russian</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ITHEA, Information Content and Processing, 2014, 1 (3) , 262-268</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.03088v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.03088v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.02890v1</id>
    <updated>2017-12-07T23:35:11Z</updated>
    <published>2017-12-07T23:35:11Z</published>
    <title>Network Analysis for Explanation</title>
    <summary>  Safety critical systems strongly require the quality aspects of artificial
intelligence including explainability. In this paper, we analyzed a trained
network to extract features which mainly contribute the inference. Based on the
analysis, we developed a simple solution to generate explanations of the
inference processes.
</summary>
    <author>
      <name>Hiroshi Kuwajima</name>
    </author>
    <author>
      <name>Masayuki Tanaka</name>
    </author>
    <link href="http://arxiv.org/abs/1712.02890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.02890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.10864v1</id>
    <updated>2018-03-28T21:44:21Z</updated>
    <published>2018-03-28T21:44:21Z</published>
    <title>Human Emotional Facial Expression Recognition</title>
    <summary>  An automatic Facial Expression Recognition (FER) model with Adaboost face
detector, feature selection based on manifold learning and synergetic prototype
based classifier has been proposed. Improved feature selection method and
proposed classifier can achieve favorable effectiveness to performance FER in
reasonable processing time.
</summary>
    <author>
      <name>Chendi Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1803.10864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.10864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00487v1</id>
    <updated>2018-07-02T06:47:47Z</updated>
    <published>2018-07-02T06:47:47Z</published>
    <title>An initial study on estimating area of a leaf using image processing</title>
    <summary>  Calculating leaf area is very important. Computer aided image processing can
make this faster and more accurate. This include scanning the leaf , converting
it to binary image and calculation of number of pixels covered. Later this is
converted to mm2.
</summary>
    <author>
      <name>G. D. Illeperuma</name>
    </author>
    <link href="http://arxiv.org/abs/1807.00487v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00487v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06466v1</id>
    <updated>2018-07-17T14:24:37Z</updated>
    <published>2018-07-17T14:24:37Z</published>
    <title>Automatic Skin Lesion Segmentation Using Deep Fully Convolutional
  Networks</title>
    <summary>  This paper summarizes our method and validation results for the ISIC
Challenge 2018 - Skin Lesion Analysis Towards Melanoma Detection - Task 1:
Lesion Segmentation
</summary>
    <author>
      <name>Hongming Xu</name>
    </author>
    <author>
      <name>Tae Hyun Hwang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, ISIC Challenge 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.06466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.07502v3</id>
    <updated>2020-02-20T01:38:30Z</updated>
    <published>2018-11-19T05:07:50Z</published>
    <title>Fast Efficient Object Detection Using Selective Attention</title>
    <summary>  Retraction due to significant oversight
</summary>
    <author>
      <name>Shivanthan Yohanandan</name>
    </author>
    <author>
      <name>Andy Song</name>
    </author>
    <author>
      <name>Adrian G. Dyer</name>
    </author>
    <author>
      <name>Angela Faragasso</name>
    </author>
    <author>
      <name>Subhrajit Roy</name>
    </author>
    <author>
      <name>Dacheng Tao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Retraction due to significant oversight</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.07502v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.07502v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.12797v1</id>
    <updated>2018-11-30T14:05:37Z</updated>
    <published>2018-11-30T14:05:37Z</published>
    <title>Structure and Motion from Multiframes</title>
    <summary>  The paper gives an overview of the problems and methods of recovery of
structure and motion parameters of rigid bodies from multiframes.
</summary>
    <author>
      <name>Mieczysław A. Kłopotek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 figures, 20 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">M.A. K{\l}opotek: Structure and Motion from Multiframes. Machine
  Graphics and Vision , Vol. 7, nos 1/2, 1998,pp. 383-396</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.12797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.12797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.03068v1</id>
    <updated>2019-01-10T09:13:16Z</updated>
    <published>2019-01-10T09:13:16Z</published>
    <title>New Radon Transform Based Texture Features of Handwritten Document</title>
    <summary>  In this paper, we present some new features describing the handwritten
document as a texture. These features are based on the Radon transform. All
values can be obtained easily and suit for the coarse classification of
documents.
</summary>
    <author>
      <name>Rustam Latypov</name>
    </author>
    <author>
      <name>Evgeni Stolov</name>
    </author>
    <link href="http://arxiv.org/abs/1901.03068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.03068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.05712v1</id>
    <updated>2019-04-11T14:17:52Z</updated>
    <published>2019-04-11T14:17:52Z</published>
    <title>Reconstructing Network Inputs with Additive Perturbation Signatures</title>
    <summary>  In this work, we present preliminary results demonstrating the ability to
recover a significant amount of information about secret model inputs given
only very limited access to model outputs and the ability evaluate the model on
additive perturbations to the input.
</summary>
    <author>
      <name>Nick Moran</name>
    </author>
    <author>
      <name>Chiraag Juvekar</name>
    </author>
    <link href="http://arxiv.org/abs/1904.05712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.06483v1</id>
    <updated>2019-07-15T13:04:31Z</updated>
    <published>2019-07-15T13:04:31Z</published>
    <title>Color Cerberus</title>
    <summary>  Simple convolutional neural network was able to win ISISPA color constancy
competition. Partial reimplementation of (Bianco, 2017) neural architecture
would have shown even better results in this setup.
</summary>
    <author>
      <name>A. ~Savchik</name>
    </author>
    <author>
      <name>E. ~Ershov</name>
    </author>
    <author>
      <name>S. ~Karpenko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISPA.2019.8868425</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISPA.2019.8868425" rel="related"/>
    <link href="http://arxiv.org/abs/1907.06483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.06483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.13734v1</id>
    <updated>2020-05-28T01:53:57Z</updated>
    <published>2020-05-28T01:53:57Z</published>
    <title>Anomaly Detection Based on Deep Learning Using Video for Prevention of
  Industrial Accidents</title>
    <summary>  This paper proposes an anomaly detection method for the prevention of
industrial accidents using machine learning technology.
</summary>
    <author>
      <name>Satoshi Hashimoto</name>
    </author>
    <author>
      <name>Yonghoon Ji</name>
    </author>
    <author>
      <name>Kenichi Kudo</name>
    </author>
    <author>
      <name>Takayuki Takahashi</name>
    </author>
    <author>
      <name>Kazunori Umeda</name>
    </author>
    <link href="http://arxiv.org/abs/2005.13734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.13734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.14386v1</id>
    <updated>2020-05-29T05:03:15Z</updated>
    <published>2020-05-29T05:03:15Z</published>
    <title>Controlling Length in Image Captioning</title>
    <summary>  We develop and evaluate captioning models that allow control of caption
length. Our models can leverage this control to generate captions of different
style and descriptiveness.
</summary>
    <author>
      <name>Ruotian Luo</name>
    </author>
    <author>
      <name>Greg Shakhnarovich</name>
    </author>
    <link href="http://arxiv.org/abs/2005.14386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.14386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.02651v1</id>
    <updated>2021-04-04T04:23:36Z</updated>
    <published>2021-04-04T04:23:36Z</published>
    <title>A Modified Convolutional Network for Auto-encoding based on Pattern
  Theory Growth Function</title>
    <summary>  This brief paper reports the shortcoming of a variant of convolutional neural
network whose components are developed based on the pattern theory framework.
</summary>
    <author>
      <name>Erico Tjoa</name>
    </author>
    <link href="http://arxiv.org/abs/2104.02651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.02651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.08657v1</id>
    <updated>2021-04-17T22:59:15Z</updated>
    <published>2021-04-17T22:59:15Z</published>
    <title>IUPUI Driving Videos and Images in All Weather and Illumination
  Conditions</title>
    <summary>  This document describes an image and video dataset of driving views captured
in all weather and illumination conditions. The data set has been submitted to
CDVL.
</summary>
    <author>
      <name>Jiang Yu Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.08657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.04466v2</id>
    <updated>2021-08-11T02:35:11Z</updated>
    <published>2021-08-10T06:21:42Z</published>
    <title>Method Towards CVPR 2021 SimLocMatch Challenge</title>
    <summary>  This report describes Megvii-3D team's approach towards SimLocMatch Challenge
@ CVPR 2021 Image Matching Workshop.
</summary>
    <author>
      <name>Xiaopeng Bi</name>
    </author>
    <author>
      <name>Ran Yan</name>
    </author>
    <author>
      <name>Zheng Chai</name>
    </author>
    <author>
      <name>Haotian Zhang</name>
    </author>
    <author>
      <name>Xiao Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2108.04466v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.04466v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.07843v2</id>
    <updated>2021-09-18T06:00:56Z</updated>
    <published>2021-09-16T10:11:58Z</published>
    <title>Label Assignment Distillation for Object Detection</title>
    <summary>  This article has been removed by arXiv administrators due to a claim of
copyright infringement
</summary>
    <author>
      <name>Hailun Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article has been removed by arXiv administrators due to a claim
  of copyright infringement. Author list truncated to the submitter</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.07843v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.07843v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.15483v1</id>
    <updated>2021-12-23T07:25:19Z</updated>
    <published>2021-12-23T07:25:19Z</published>
    <title>Cloud Removal from Satellite Images</title>
    <summary>  In this report, we have analyzed available cloud detection technique using
sentinel hub. We have also implemented spatial attention generative adversarial
network and improved quality of generated image compared to previous solution
[7].
</summary>
    <author>
      <name>Rutvik Chauhan</name>
    </author>
    <author>
      <name>Antarpuneet Singh</name>
    </author>
    <author>
      <name>Sujoy Saha</name>
    </author>
    <link href="http://arxiv.org/abs/2112.15483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.15483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.12318v2</id>
    <updated>2022-04-27T08:40:43Z</updated>
    <published>2022-04-26T13:53:40Z</published>
    <title>Evaluating the Quality of a Synthesized Motion with the Fréchet Motion
  Distance</title>
    <summary>  Evaluating the Quality of a Synthesized Motion with the Fr\'echet Motion
Distance
</summary>
    <author>
      <name>Antoine Maiorca</name>
    </author>
    <author>
      <name>Youngwoo Yoon</name>
    </author>
    <author>
      <name>Thierry Dutoit</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.12318v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.12318v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00503v1</id>
    <updated>2018-04-30T03:49:22Z</updated>
    <published>2018-04-30T03:49:22Z</published>
    <title>Machine Learning for Exam Triage</title>
    <summary>  In this project, we extend the state-of-the-art CheXNet (Rajpurkar et al.
[2017]) by making use of the additional non-image features in the dataset. Our
model produced better AUROC scores than the original CheXNet.
</summary>
    <author>
      <name>Xinyu Guan</name>
    </author>
    <author>
      <name>Jessica Lee</name>
    </author>
    <author>
      <name>Peter Wu</name>
    </author>
    <author>
      <name>Yue Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1805.00503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00638v2</id>
    <updated>2019-06-25T05:11:09Z</updated>
    <published>2018-05-02T06:03:47Z</published>
    <title>A Deep Network for Arousal-Valence Emotion Prediction with
  Acoustic-Visual Cues</title>
    <summary>  In this paper, we comprehensively describe the methodology of our submissions
to the One-Minute Gradual-Emotion Behavior Challenge 2018.
</summary>
    <author>
      <name>Songyou Peng</name>
    </author>
    <author>
      <name>Le Zhang</name>
    </author>
    <author>
      <name>Yutong Ban</name>
    </author>
    <author>
      <name>Meng Fang</name>
    </author>
    <author>
      <name>Stefan Winkler</name>
    </author>
    <link href="http://arxiv.org/abs/1805.00638v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00638v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08174v1</id>
    <updated>2018-05-21T16:50:55Z</updated>
    <published>2018-05-21T16:50:55Z</published>
    <title>Reproducibility Report for "Learning To Count Objects In Natural Images
  For Visual Question Answering"</title>
    <summary>  This is the reproducibility report for the paper "Learning To Count Objects
In Natural Images For Visual QuestionAnswering"
</summary>
    <author>
      <name>Shagun Sodhani</name>
    </author>
    <author>
      <name>Vardaan Pahuja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Reproducibility in ML Workshop, ICML'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.08174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.00947v1</id>
    <updated>2020-07-27T15:31:27Z</updated>
    <published>2020-07-27T15:31:27Z</published>
    <title>Pre-training for Video Captioning Challenge 2020 Summary</title>
    <summary>  The Pre-training for Video Captioning Challenge 2020 Summary: results and
challenge participants' technical reports.
</summary>
    <author>
      <name>Yingwei Pan</name>
    </author>
    <author>
      <name>Jun Xu</name>
    </author>
    <author>
      <name>Yehao Li</name>
    </author>
    <author>
      <name>Ting Yao</name>
    </author>
    <author>
      <name>Tao Mei</name>
    </author>
    <link href="http://arxiv.org/abs/2008.00947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.00947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.05388v1</id>
    <updated>2020-09-02T17:14:05Z</updated>
    <published>2020-09-02T17:14:05Z</published>
    <title>Automatic cinematography for 360 video</title>
    <summary>  We describe our method for automatic generation of a visually interesting
camera path (automatic cinematography)from a 360 video. Based on the
information from the scene objects, multiple shot hypotheses for different shot
types are constructed and the best one is rendered.
</summary>
    <author>
      <name>Hannes Fassold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as demo paper for IEEE MMSP 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.05388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.05388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.00975v1</id>
    <updated>2022-01-04T04:44:05Z</updated>
    <published>2022-01-04T04:44:05Z</published>
    <title>StyleM: Stylized Metrics for Image Captioning Built with Contrastive
  N-grams</title>
    <summary>  In this paper, we build two automatic evaluation metrics for evaluating the
association between a machine-generated caption and a ground truth stylized
caption: OnlyStyle and StyleCIDEr.
</summary>
    <author>
      <name>Chengxi Li</name>
    </author>
    <author>
      <name>Brent Harrison</name>
    </author>
    <link href="http://arxiv.org/abs/2201.00975v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.00975v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.09388v1</id>
    <updated>2022-01-23T22:44:45Z</updated>
    <published>2022-01-23T22:44:45Z</published>
    <title>A Survey on Patients Privacy Protection with Stganography and Visual
  Encryption</title>
    <summary>  In this survey, thirty models for steganography and visual encryption methods
have been discussed to provide patients privacy protection.
</summary>
    <author>
      <name>Hussein K. Alzubaidy</name>
    </author>
    <author>
      <name>Dhiah Al-Shammary</name>
    </author>
    <author>
      <name>Mohammed Hamzah Abed</name>
    </author>
    <link href="http://arxiv.org/abs/2201.09388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.09388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.04639v1</id>
    <updated>2022-07-11T05:47:27Z</updated>
    <published>2022-07-11T05:47:27Z</published>
    <title>A Dual-Polarization Information Guided Network for SAR Ship
  Classification</title>
    <summary>  How to fully utilize polarization to enhance synthetic aperture radar (SAR)
ship classification remains an unresolved issue. Thus, we propose a
dual-polarization information guided network (DPIG-Net) to solve it.
</summary>
    <author>
      <name>Tianwen Zhang</name>
    </author>
    <author>
      <name>Xiaoling Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2207.04639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.04639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.07604v1</id>
    <updated>2022-07-15T17:02:55Z</updated>
    <published>2022-07-15T17:02:55Z</published>
    <title>Image and Texture Independent Deep Learning Noise Estimation using
  Multiple Frames</title>
    <summary>  In this study, a novel multiple-frame based image and texture independent
convolutional Neural Network (CNN) noise estimator is introduced. The estimator
works.
</summary>
    <author>
      <name>Hikmet Kirmizitas</name>
    </author>
    <author>
      <name>Nurettin Besli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5755/j02.eie.30586</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5755/j02.eie.30586" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Elektronika Ir Elektrotechnika 2022 28(6) (42-47)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2207.07604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.07604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.04998v1</id>
    <updated>2022-11-09T16:22:34Z</updated>
    <published>2022-11-09T16:22:34Z</published>
    <title>Similarity among the 2D-shapes and the analysis of dissimilarity scores</title>
    <summary>  We present a conceptually simple and intuitive method to calculate and to
measure the dissimilarities among 2D shapes. Several methods to interpret and
to visualize the resulting dissimilarity matrix are presented and compared.
</summary>
    <author>
      <name>Karel Zimmermann</name>
    </author>
    <link href="http://arxiv.org/abs/2211.04998v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.04998v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1; I.4; I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.17361v1</id>
    <updated>2023-03-30T13:19:07Z</updated>
    <published>2023-03-30T13:19:07Z</published>
    <title>Invertible Convolution with Symmetric Paddings</title>
    <summary>  We show that symmetrically padded convolution can be analytically inverted
via DFT. We comprehensively analyze several different symmetric and
anti-symmetric padding modes and show that multiple cases exist where the
inversion can be achieved. The implementation is available at
\url{https://github.com/prclibo/iconv_dft}.
</summary>
    <author>
      <name>Bo Li</name>
    </author>
    <link href="http://arxiv.org/abs/2303.17361v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.17361v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.03340v1</id>
    <updated>2023-08-07T06:47:36Z</updated>
    <published>2023-08-07T06:47:36Z</published>
    <title>A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive
  Learning for Image Deraining</title>
    <summary>  Image deraining is a challenging task that involves restoring degraded images
affected by rain streaks.
</summary>
    <author>
      <name>Cheng Wang</name>
    </author>
    <author>
      <name>Wei Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages,6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.03340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.03340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.01134v1</id>
    <updated>2024-01-02T10:22:06Z</updated>
    <published>2024-01-02T10:22:06Z</published>
    <title>Hybrid Pooling and Convolutional Network for Improving Accuracy and
  Training Convergence Speed in Object Detection</title>
    <summary>  This paper introduces HPC-Net, a high-precision and rapidly convergent object
detection network.
</summary>
    <author>
      <name>Shiwen Zhao</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Junhui Hou</name>
    </author>
    <author>
      <name>Hai Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,5 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.01134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.01134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.18682v1</id>
    <updated>2024-07-26T11:56:23Z</updated>
    <published>2024-07-26T11:56:23Z</published>
    <title>Rapid Object Annotation</title>
    <summary>  In this report we consider the problem of rapidly annotating a video with
bounding boxes for a novel object. We describe a UI and associated workflow
designed to make this process fast for an arbitrary novel target.
</summary>
    <author>
      <name>Misha Denil</name>
    </author>
    <link href="http://arxiv.org/abs/2407.18682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.18682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.18136v1</id>
    <updated>2024-12-24T03:44:26Z</updated>
    <published>2024-12-24T03:44:26Z</published>
    <title>ERVD: An Efficient and Robust ViT-Based Distillation Framework for
  Remote Sensing Image Retrieval</title>
    <summary>  ERVD: An Efficient and Robust ViT-Based Distillation Framework for Remote
Sensing Image Retrieval
</summary>
    <author>
      <name>Le Dong</name>
    </author>
    <author>
      <name>Qixuan Cao</name>
    </author>
    <author>
      <name>Lei Pu</name>
    </author>
    <author>
      <name>Fangfang Wu</name>
    </author>
    <author>
      <name>Weisheng Dong</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Guangming Shi</name>
    </author>
    <link href="http://arxiv.org/abs/2412.18136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.18136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.11424v1</id>
    <updated>2025-05-16T16:38:01Z</updated>
    <published>2025-05-16T16:38:01Z</published>
    <title>Improving Object Detection Performance through YOLOv8: A Comprehensive
  Training and Evaluation Study</title>
    <summary>  This study evaluated the performance of a YOLOv8-based segmentation model for
detecting and segmenting wrinkles in facial images.
</summary>
    <author>
      <name>Rana Poureskandar</name>
    </author>
    <author>
      <name>Shiva Razzagzadeh</name>
    </author>
    <link href="http://arxiv.org/abs/2505.11424v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.11424v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9810003v1</id>
    <updated>1998-10-02T03:34:38Z</updated>
    <published>1998-10-02T03:34:38Z</published>
    <title>A Linear Shift Invariant Multiscale Transform</title>
    <summary>  This paper presents a multiscale decomposition algorithm. Unlike standard
wavelet transforms, the proposed operator is both linear and shift invariant.
The central idea is to obtain shift invariance by averaging the aligned wavelet
transform projections over all circular shifts of the signal. It is shown how
the same transform can be obtained by a linear filter bank.
</summary>
    <author>
      <name>Andreas Siebert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings 1998 International Conference on Image Processing,
  Chicago, 4-7 October 1998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9810003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9810003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9810017v1</id>
    <updated>1998-10-19T20:46:16Z</updated>
    <published>1998-10-19T20:46:16Z</published>
    <title>General Theory of Image Normalization</title>
    <summary>  We give a systematic, abstract formulation of the image normalization method
as applied to a general group of image transformations, and then illustrate the
abstract analysis by applying it to the hierarchy of viewing transformations of
a planar object.
</summary>
    <author>
      <name>Stephen L. Adler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, plain tex, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9810017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9810017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10, I.4.7, I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9908017v1</id>
    <updated>1999-08-26T17:18:49Z</updated>
    <published>1999-08-26T17:18:49Z</published>
    <title>A Differential Invariant for Zooming</title>
    <summary>  This paper presents an invariant under scaling and linear brightness change.
The invariant is based on differentials and therefore is a local feature.
Rotationally invariant 2-d differential Gaussian operators up to third order
are proposed for the implementation of the invariant. The performance is
analyzed by simulating a camera zoom-out.
</summary>
    <author>
      <name>Andreas Siebert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings 1999 International Conference on Image Processing,
  Kobe, 25-28 October 1999</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9908017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9908017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0001024v1</id>
    <updated>2000-01-25T16:09:37Z</updated>
    <published>2000-01-25T16:09:37Z</published>
    <title>A Parallel Algorithm for Dilated Contour Extraction from Bilevel Images</title>
    <summary>  We describe a simple, but efficient algorithm for the generation of dilated
contours from bilevel images. The initial part of the contour extraction is
explained to be a good candidate for parallel computer code generation. The
remainder of the algorithm is of linear nature.
</summary>
    <author>
      <name>B. R. Schlei</name>
    </author>
    <author>
      <name>L. Prasad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, including 3 figures. For additional detail check
  http://www.nis.lanl.gov/~bschlei/labvis/index.html</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0001024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0001024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10, D.1.3, G.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0003079v1</id>
    <updated>2000-03-26T23:18:43Z</updated>
    <published>2000-03-26T23:18:43Z</published>
    <title>Differential Invariants under Gamma Correction</title>
    <summary>  This paper presents invariants under gamma correction and similarity
transformations. The invariants are local features based on differentials which
are implemented using derivatives of the Gaussian. The use of the proposed
invariant representation is shown to yield improved correlation results in a
template matching scenario.
</summary>
    <author>
      <name>Andreas Siebert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 12 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Vision Interface 2000, Montreal, 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0003079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0003079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006001v1</id>
    <updated>2000-05-31T23:37:48Z</updated>
    <published>2000-05-31T23:37:48Z</published>
    <title>Boosting the Differences: A fast Bayesian classifier neural network</title>
    <summary>  A Bayesian classifier that up-weights the differences in the attribute values
is discussed. Using four popular datasets from the UCI repository, some
interesting features of the network are illustrated. The network is suitable
for classification problems.
</summary>
    <author>
      <name>Ninan Sajeeth Philip</name>
    </author>
    <author>
      <name>K. Babu Joseph</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">latex 18pages no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0006001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I1.2;F.1.1;F1.2;C1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006002v1</id>
    <updated>2000-05-31T23:52:31Z</updated>
    <published>2000-05-31T23:52:31Z</published>
    <title>Distorted English Alphabet Identification : An application of Difference
  Boosting Algorithm</title>
    <summary>  The difference-boosting algorithm is used on letters dataset from the UCI
repository to classify distorted raster images of English alphabets. In
contrast to rather complex networks, the difference-boosting is found to
produce comparable or better classification efficiency on this complex problem.
</summary>
    <author>
      <name>Ninan Sajeeth Philip</name>
    </author>
    <author>
      <name>K. Babu Joseph</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">latex 14pages no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0006002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I1.2;F.1.1;F1.2;C1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006040v1</id>
    <updated>2000-06-28T18:34:14Z</updated>
    <published>2000-06-28T18:34:14Z</published>
    <title>Correlation over Decomposed Signals: A Non-Linear Approach to Fast and
  Effective Sequences Comparison</title>
    <summary>  A novel non-linear approach to fast and effective comparison of sequences is
presented, compared to the traditional cross-correlation operator, and
illustrated with respect to DNA sequences.
</summary>
    <author>
      <name>Luciano da Fontoura Costa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0006040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.4; F.2.2; I.5.4; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0301001v1</id>
    <updated>2003-01-01T19:58:03Z</updated>
    <published>2003-01-01T19:58:03Z</published>
    <title>Least squares fitting of circles and lines</title>
    <summary>  We study theoretical and computational aspects of the least squares fit (LSF)
of circles and circular arcs. First we discuss the existence and uniqueness of
LSF and various parametrization schemes. Then we evaluate several popular
circle fitting algorithms and propose a new one that surpasses the existing
methods in reliability. We also discuss and compare direct (algebraic) circle
fits.
</summary>
    <author>
      <name>N. Chernov</name>
    </author>
    <author>
      <name>C. Lesort</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 14 figures, submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0301001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0301001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.8; I.5.1; I.2.10; G.1.2; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0308034v1</id>
    <updated>2003-08-21T10:47:27Z</updated>
    <published>2003-08-21T10:47:27Z</published>
    <title>Fingerprint based bio-starter and bio-access</title>
    <summary>  In the paper will be presented a safety and security system based on
fingerprint technology. The results suggest a new scenario where the new cars
can use a fingerprint sensor integrated in car handle to allow access and in
the dashboard as starter button.
</summary>
    <author>
      <name>G. Iovane</name>
    </author>
    <author>
      <name>P. Giordano</name>
    </author>
    <author>
      <name>C. Iovane</name>
    </author>
    <author>
      <name>F. Rotulo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Proceeding of Automotive 2003, Turin (Italy)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0308034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0308034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10, I.4, I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0308035v1</id>
    <updated>2003-08-21T10:52:53Z</updated>
    <published>2003-08-21T10:52:53Z</published>
    <title>IS (Iris Security)</title>
    <summary>  In the paper will be presented a safety system based on iridology. The
results suggest a new scenario where the security problem in supervised and
unsupervised areas can be treat with the present system and the iris image
recognition.
</summary>
    <author>
      <name>G. Iovane</name>
    </author>
    <author>
      <name>F. S. Tortoriello</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, Proceeding of NIDays 2003 (Sponsored by National
  Instruments), Rome (Italy)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0308035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0308035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5, I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0401018v1</id>
    <updated>2004-01-22T05:53:30Z</updated>
    <published>2004-01-22T05:53:30Z</published>
    <title>Factor Temporal Prognosis of Tick-Borne Encephalitis Foci Functioning on
  the South of Russian Far East</title>
    <summary>  A method of temporal factor prognosis of TE (tick-borne encephalitis)
infection has been developed. The high precision of the prognosis results for a
number of geographical regions of Primorsky Krai has been achieved. The method
can be applied not only to epidemiological research but also to others.
</summary>
    <author>
      <name>E. I. Bolotin</name>
    </author>
    <author>
      <name>G. Sh. Tsitsiashvili</name>
    </author>
    <author>
      <name>I. V. Golycheva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0401018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0401018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0608073v1</id>
    <updated>2006-08-18T08:28:23Z</updated>
    <published>2006-08-18T08:28:23Z</published>
    <title>Parametrical Neural Networks and Some Other Similar Architectures</title>
    <summary>  A review of works on associative neural networks accomplished during last
four years in the Institute of Optical Neural Technologies RAS is given. The
presentation is based on description of parametrical neural networks (PNN). For
today PNN have record recognizing characteristics (storage capacity, noise
immunity and speed of operation). Presentation of basic ideas and principles is
accentuated.
</summary>
    <author>
      <name>Leonid B. Litinskii</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures, accepted for publication in "Optical Memory &amp;
  Neural Networks" (2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0608073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0608073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0609164v1</id>
    <updated>2006-09-29T13:48:35Z</updated>
    <published>2006-09-29T13:48:35Z</published>
    <title>Conditional Expressions for Blind Deconvolution: Multi-point form</title>
    <summary>  We present conditional expression (CE) for finding blurs convolved in given
images. The CE is given in terms of the zero-values of the blurs evaluated at
multi-point. The CE can detect multiple blur all at once. We illustrate the
multiple blur-detection by using a test image.
</summary>
    <author>
      <name>S. Aogaki</name>
    </author>
    <author>
      <name>I. Moritani</name>
    </author>
    <author>
      <name>T. Sugai</name>
    </author>
    <author>
      <name>F. Takeutchi</name>
    </author>
    <author>
      <name>F. M. Toyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0609164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0609164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0609165v1</id>
    <updated>2006-09-29T13:50:12Z</updated>
    <published>2006-09-29T13:50:12Z</published>
    <title>Simple method to eliminate blur based on Lane and Bates algorithm</title>
    <summary>  A simple search method for finding a blur convolved in a given image is
presented. The method can be easily extended to a large blur. The method has
been experimentally tested with a model blurred image.
</summary>
    <author>
      <name>S. Aogaki</name>
    </author>
    <author>
      <name>I. Moritani</name>
    </author>
    <author>
      <name>T. Sugai</name>
    </author>
    <author>
      <name>F. Takeutchi</name>
    </author>
    <author>
      <name>F. M. Toyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0609165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0609165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.0300v1</id>
    <updated>2007-06-03T05:17:38Z</updated>
    <published>2007-06-03T05:17:38Z</published>
    <title>Automatic Detection of Pulmonary Embolism using Computational
  Intelligence</title>
    <summary>  This article describes the implementation of a system designed to
automatically detect the presence of pulmonary embolism in lung scans. These
images are firstly segmented, before alignment and feature extraction using
PCA. The neural network was trained using the Hybrid Monte Carlo method,
resulting in a committee of 250 neural networks and good results are obtained.
</summary>
    <author>
      <name>Simon Scurrell</name>
    </author>
    <author>
      <name>Tshilidzi Marwala</name>
    </author>
    <author>
      <name>David Rubin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0706.0300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.0300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.0131v1</id>
    <updated>2007-12-02T10:02:01Z</updated>
    <published>2007-12-02T10:02:01Z</published>
    <title>Learning Similarity for Character Recognition and 3D Object Recognition</title>
    <summary>  I describe an approach to similarity motivated by Bayesian methods. This
yields a similarity function that is learnable using a standard Bayesian
methods. The relationship of the approach to variable kernel and variable
metric methods is discussed. The approach is related to variable kernel
Experimental results on character recognition and 3D object recognition are
presented..
</summary>
    <author>
      <name>Thomas M. Breuel</name>
    </author>
    <link href="http://arxiv.org/abs/0712.0131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.0131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.2923v1</id>
    <updated>2007-12-18T10:43:23Z</updated>
    <published>2007-12-18T10:43:23Z</published>
    <title>A Class of LULU Operators on Multi-Dimensional Arrays</title>
    <summary>  The LULU operators for sequences are extended to multi-dimensional arrays via
the morphological concept of connection in a way which preserves their
essential properties, e.g. they are separators and form a four element fully
ordered semi-group. The power of the operators is demonstrated by deriving a
total variation preserving discrete pulse decomposition of images.
</summary>
    <author>
      <name>Roumen Anguelov</name>
    </author>
    <author>
      <name>Inger Plaskitt</name>
    </author>
    <link href="http://arxiv.org/abs/0712.2923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.2923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.1258v1</id>
    <updated>2008-02-09T12:22:47Z</updated>
    <published>2008-02-09T12:22:47Z</published>
    <title>Bayesian Nonlinear Principal Component Analysis Using Random Fields</title>
    <summary>  We propose a novel model for nonlinear dimension reduction motivated by the
probabilistic formulation of principal component analysis. Nonlinearity is
achieved by specifying different transformation matrices at different locations
of the latent space and smoothing the transformation using a Markov random
field type prior. The computation is made feasible by the recent advances in
sampling from von Mises-Fisher distributions.
</summary>
    <author>
      <name>Heng Lian</name>
    </author>
    <link href="http://arxiv.org/abs/0802.1258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.1258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.2690v1</id>
    <updated>2008-05-17T17:15:26Z</updated>
    <published>2008-05-17T17:15:26Z</published>
    <title>Increasing Linear Dynamic Range of Commercial Digital Photocamera Used
  in Imaging Systems with Optical Coding</title>
    <summary>  Methods of increasing linear optical dynamic range of commercial photocamera
for optical-digital imaging systems are described. Use of such methods allows
to use commercial photocameras for optical measurements. Experimental results
are reported.
</summary>
    <author>
      <name>M. V. Konnik</name>
    </author>
    <author>
      <name>E. A. Manykin</name>
    </author>
    <author>
      <name>S. N. Starikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">unnecessary figures were removed; typos corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/0805.2690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.2690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.3885v1</id>
    <updated>2008-06-24T13:43:06Z</updated>
    <published>2008-06-24T13:43:06Z</published>
    <title>Conceptualization of seeded region growing by pixels aggregation. Part
  1: the framework</title>
    <summary>  Adams and Bishop have proposed in 1994 a novel region growing algorithm
called seeded region growing by pixels aggregation (SRGPA). This paper
introduces a framework to implement an algorithm using SRGPA. This framework is
built around two concepts: localization and organization of applied action.
This conceptualization gives a quick implementation of algorithms, a direct
translation between the mathematical idea and the numerical implementation, and
an improvement of algorithms efficiency.
</summary>
    <author>
      <name>Vincent Tariel</name>
    </author>
    <link href="http://arxiv.org/abs/0806.3885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.3885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.2227v1</id>
    <updated>2008-08-16T01:34:48Z</updated>
    <published>2008-08-16T01:34:48Z</published>
    <title>Higher Order Moments Generation by Mellin Transform for Compound Models
  of Clutter</title>
    <summary>  The compound models of clutter statistics are found suitable to describe the
nonstationary nature of radar backscattering from high-resolution observations.
In this letter, we show that the properties of Mellin transform can be utilized
to generate higher order moments of simple and compound models of clutter
statistics in a compact manner.
</summary>
    <author>
      <name>C Bhattacharya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.2227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.2227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.3352v1</id>
    <updated>2008-09-19T11:02:39Z</updated>
    <published>2008-09-19T11:02:39Z</published>
    <title>Generalized Prediction Intervals for Arbitrary Distributed
  High-Dimensional Data</title>
    <summary>  This paper generalizes the traditional statistical concept of prediction
intervals for arbitrary probability density functions in high-dimensional
feature spaces by introducing significance level distributions, which provides
interval-independent probabilities for continuous random variables. The
advantage of the transformation of a probability density function into a
significance level distribution is that it enables one-class classification or
outlier detection in a direct manner.
</summary>
    <author>
      <name>Steffen Kuehn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.3352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.3352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.4501v1</id>
    <updated>2008-09-25T20:54:29Z</updated>
    <published>2008-09-25T20:54:29Z</published>
    <title>Audio Classification from Time-Frequency Texture</title>
    <summary>  Time-frequency representations of audio signals often resemble texture
images. This paper derives a simple audio classification algorithm based on
treating sound spectrograms as texture images. The algorithm is inspired by an
earlier visual classification scheme particularly efficient at classifying
textures. While solely based on time-frequency texture features, the algorithm
achieves surprisingly good performance in musical instrument classification
experiments.
</summary>
    <author>
      <name>Guoshen Yu</name>
    </author>
    <author>
      <name>Jean-Jacques Slotine</name>
    </author>
    <link href="http://arxiv.org/abs/0809.4501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.4501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.4699v2</id>
    <updated>2009-03-03T09:49:07Z</updated>
    <published>2008-11-28T12:11:21Z</published>
    <title>Mapping Images with the Coherence Length Diagrams</title>
    <summary>  Statistical pattern recognition methods based on the Coherence Length Diagram
(CLD) have been proposed for medical image analyses, such as quantitative
characterisation of human skin textures, and for polarized light microscopy of
liquid crystal textures. Further investigations are made on image maps
originated from such diagram and some examples related to irregularity of
microstructures are shown.
</summary>
    <author>
      <name>A. Sparavigna</name>
    </author>
    <author>
      <name>R. Marazzato</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Software Engineering and Computing, pp.
  53-57, 2009, Vol. 1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0811.4699v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.4699v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.0340v2</id>
    <updated>2009-10-01T18:44:03Z</updated>
    <published>2008-12-01T18:59:52Z</published>
    <title>A Matlab Implementation of a Flat Norm Motivated Polygonal Edge Matching
  Method using a Decomposition of Boundary into Four 1-Dimensional Currents</title>
    <summary>  We describe and provide code and examples for a polygonal edge matching
method.
</summary>
    <author>
      <name>Simon P. Morgan</name>
    </author>
    <author>
      <name>Wotao Yin</name>
    </author>
    <author>
      <name>Kevin R. Vixie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contains Matlab code and 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0812.0340v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.0340v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.1340v2</id>
    <updated>2009-02-03T16:31:46Z</updated>
    <published>2008-12-07T11:42:41Z</published>
    <title>Obtaining Depth Maps From Color Images By Region Based Stereo Matching
  Algorithms</title>
    <summary>  In the paper, region based stereo matching algorithms are developed for
extraction depth information from two color stereo image pair. A filter
eliminating unreliable disparity estimation was used for increasing reliability
of the disparity map. Obtained results by algorithms were represented and
compared.
</summary>
    <author>
      <name>B. Baykant Alagoz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">New figures were added</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">OncuBilim Algorithm And Systems Labs. Vol.08, Art.No:04,(2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0812.1340v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.1340v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.4073v1</id>
    <updated>2009-02-24T20:34:43Z</updated>
    <published>2009-02-24T20:34:43Z</published>
    <title>Dipole and Quadrupole Moments in Image Processing</title>
    <summary>  This paper proposes an algorithm for image processing, obtained by adapting
to image maps the definitions of two well-known physical quantities. These
quantities are the dipole and quadrupole moments of a charge distribution. We
will see how it is possible to define dipole and quadrupole moments for the
gray-tone maps and apply them in the development of algorithms for edge
detection.
</summary>
    <author>
      <name>Amelia Sparavigna</name>
    </author>
    <link href="http://arxiv.org/abs/0902.4073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.4073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.4663v1</id>
    <updated>2009-02-26T18:42:30Z</updated>
    <published>2009-02-26T18:42:30Z</published>
    <title>Dipole Vectors in Images Processing</title>
    <summary>  Instead of evaluating the gradient field of the brightness map of an image,
we propose the use of dipole vectors. This approach is obtained by adapting to
the image gray-tone distribution the definition of the dipole moment of charge
distributions. We will show how to evaluate the dipoles and obtain a vector
field, which can be a good alternative to the gradient field in pattern
recognition.
</summary>
    <author>
      <name>Amelia Sparavigna</name>
    </author>
    <link href="http://arxiv.org/abs/0902.4663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.4663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1608v1</id>
    <updated>2009-09-09T02:12:22Z</updated>
    <published>2009-09-09T02:12:22Z</published>
    <title>Motion Segmentation by SCC on the Hopkins 155 Database</title>
    <summary>  We apply the Spectral Curvature Clustering (SCC) algorithm to a benchmark
database of 155 motion sequences, and show that it outperforms all other
state-of-the-art methods. The average misclassification rate by SCC is 1.41%
for sequences having two motions and 4.85% for three motions.
</summary>
    <author>
      <name>G. Chen</name>
    </author>
    <author>
      <name>G. Lerman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCVW.2009.5457626</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCVW.2009.5457626" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to 2009 ICCV Workshop on Dynamical Vision</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Vision Workshops (ICCV Workshops), 2009 IEEE 12th
  International Conference on, 2009, pp. 759 - 764</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.1608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.0776v1</id>
    <updated>2010-03-03T10:58:20Z</updated>
    <published>2010-03-03T10:58:20Z</published>
    <title>Properties of the Discrete Pulse Transform for Multi-Dimensional Arrays</title>
    <summary>  This report presents properties of the Discrete Pulse Transform on
multi-dimensional arrays introduced by the authors two or so years ago. The
main result given here in Lemma 2.1 is also formulated in a paper to appear in
IEEE Transactions on Image Processing. However, the proof, being too technical,
was omitted there and hence it appears in full in this publication.
</summary>
    <author>
      <name>Roumen Anguelov</name>
    </author>
    <author>
      <name>Inger Fabris-Rotelli</name>
    </author>
    <link href="http://arxiv.org/abs/1003.0776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.0776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.5249v1</id>
    <updated>2010-03-27T00:17:19Z</updated>
    <published>2010-03-27T00:17:19Z</published>
    <title>Active Testing for Face Detection and Localization</title>
    <summary>  We provide a novel search technique, which uses a hierarchical model and a
mutual information gain heuristic to efficiently prune the search space when
localizing faces in images. We show exponential gains in computation over
traditional sliding window approaches, while keeping similar performance
levels.
</summary>
    <author>
      <name>Raphael Sznitman</name>
    </author>
    <author>
      <name>Bruno Jedynak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPAMI.2010.106</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPAMI.2010.106" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 5 figures, accepted in IEEE Transactions on Pattern
  Analysis and Machine Intelligence (TPAMI), 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.5249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.5249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.2368v1</id>
    <updated>2010-06-11T19:05:05Z</updated>
    <published>2010-06-11T19:05:05Z</published>
    <title>L2-optimal image interpolation and its applications to medical imaging</title>
    <summary>  Digital medical images are always displayed scaled to fit particular view.
Interpolation is responsible for this scaling, and if not done properly, can
significantly degrade diagnostic image quality. However, theoretically-optimal
interpolation algorithms may also be the most time-consuming and impractical.
We propose a new approach, adapted to the needs of digital medical imaging, to
combine high interpolation speed and superior L2-optimal image quality.
</summary>
    <author>
      <name>Oleg Pianykh</name>
    </author>
    <link href="http://arxiv.org/abs/1006.2368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.2368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.1016v1</id>
    <updated>2010-07-06T23:25:39Z</updated>
    <published>2010-07-06T23:25:39Z</published>
    <title>Bilateral filters: what they can and cannot do</title>
    <summary>  Nonlinear bilateral filters (BF) deliver a fine blend of computational
simplicity and blur-free denoising. However, little is known about their
nature, noise-suppressing properties, and optimal choices of filter parameters.
Our study is meant to fill this gap-explaining the underlying mechanism of
bilateral filtering and providing the methodology for optimal filter selection.
Practical application to CT image denoising is discussed to illustrate our
results.
</summary>
    <author>
      <name>Oleg S. Pianykh</name>
    </author>
    <link href="http://arxiv.org/abs/1007.1016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.1016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.5766v1</id>
    <updated>2011-01-30T12:05:12Z</updated>
    <published>2011-01-30T12:05:12Z</published>
    <title>Geometric Models with Co-occurrence Groups</title>
    <summary>  A geometric model of sparse signal representations is introduced for classes
of signals. It is computed by optimizing co-occurrence groups with a maximum
likelihood estimate calculated with a Bernoulli mixture model. Applications to
face image compression and MNIST digit classification illustrate the
applicability of this model.
</summary>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Stéphane Mallat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, ESANN 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.5766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.5766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.0582v1</id>
    <updated>2011-04-04T14:18:51Z</updated>
    <published>2011-04-04T14:18:51Z</published>
    <title>Visual Concept Detection and Real Time Object Detection</title>
    <summary>  Bag-of-words model is implemented and tried on 10-class visual concept
detection problem. The experimental results show that "DURF+ERT+SVM"
outperforms "SIFT+ERT+SVM" both in detection performance and computation
efficiency. Besides, combining DURF and SIFT results in even better detection
performance. Real-time object detection using SIFT and RANSAC is also tried on
simple objects, e.g. drink can, and good result is achieved.
</summary>
    <author>
      <name>Ran Tao</name>
    </author>
    <link href="http://arxiv.org/abs/1104.0582v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.0582v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.2059v1</id>
    <updated>2011-04-11T20:32:54Z</updated>
    <published>2011-04-11T20:32:54Z</published>
    <title>Template-based matching using weight maps</title>
    <summary>  Template matching is one of the most prevalent pattern recognition methods
worldwide. It has found uses in most visual concept detection fields. In this
work, we investigate methods for improving template matching by adjusting the
weights of different regions of the template. We compare several weight maps
and test the methods using the FERET face test set in the context of human eye
detection.
</summary>
    <author>
      <name>Kwie Min Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1104.2059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.2059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.4989v6</id>
    <updated>2011-08-11T09:00:58Z</updated>
    <published>2011-04-26T18:38:01Z</published>
    <title>Preprocessing: A Step in Automating Early Detection of Cervical Cancer</title>
    <summary>  This paper has been withdrawn
</summary>
    <author>
      <name>Abhishek Das</name>
    </author>
    <author>
      <name>Avijit Kar</name>
    </author>
    <author>
      <name>Debasis Bhattacharyya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">wrong conference name mentioned (This paper has been withdrawn)</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.4989v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.4989v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.2807v1</id>
    <updated>2011-07-14T12:51:10Z</updated>
    <published>2011-07-14T12:51:10Z</published>
    <title>Modelling Distributed Shape Priors by Gibbs Random Fields of Second
  Order</title>
    <summary>  We analyse the potential of Gibbs Random Fields for shape prior modelling. We
show that the expressive power of second order GRFs is already sufficient to
express simple shapes and spatial relations between them simultaneously. This
allows to model and recognise complex shapes as spatial compositions of simpler
parts.
</summary>
    <author>
      <name>Boris Flach</name>
    </author>
    <author>
      <name>Dmitrij Schlesinger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Control Systems and Computers, (2) 2011, pp 14-24</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1107.2807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.2807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.1461v1</id>
    <updated>2011-11-07T00:28:37Z</updated>
    <published>2011-11-07T00:28:37Z</published>
    <title>Multimodal diff-hash</title>
    <summary>  Many applications require comparing multimodal data with different structure
and dimensionality that cannot be compared directly. Recently, there has been
increasing interest in methods for learning and efficiently representing such
multimodal similarity. In this paper, we present a simple algorithm for
multimodal similarity-preserving hashing, trying to map multimodal data into
the Hamming space while preserving the intra- and inter-modal similarities. We
show that our method significantly outperforms the state-of-the-art method in
the field.
</summary>
    <author>
      <name>Michael M. Bronstein</name>
    </author>
    <link href="http://arxiv.org/abs/1111.1461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.1461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.6030v2</id>
    <updated>2011-12-06T13:21:25Z</updated>
    <published>2011-11-25T15:46:37Z</published>
    <title>An image processing of a Raphael's portrait of Leonardo</title>
    <summary>  In one of his paintings, the School of Athens, Raphael is depicting Leonardo
da Vinci as the philosopher Plato. Some image processing tools can help us in
comparing this portrait with two Leonardo's portraits, considered as
self-portraits.
</summary>
    <author>
      <name>Amelia Carolina Sparavigna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Image processing. Portrait. Self-portrait. Leonardo da Vinci.
  Raphael. Raffaello Sanzio Images revised using a high-quality image of
  Raphael's Plato</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.6030v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.6030v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.0216v1</id>
    <updated>2012-02-01T17:00:45Z</updated>
    <published>2012-02-01T17:00:45Z</published>
    <title>The watershed concept and its use in segmentation : a brief history</title>
    <summary>  The watershed is one of the most used tools in image segmentation. We present
how its concept is born and developed over time. Its implementation as an
algorithm or a hardwired device evolved together with the technology which
allowed it. We present also how it is used in practice, first together with
markers, and later introduced in a multiscale framework, in order to produce
not a unique partition but a complete hierarchy.
</summary>
    <author>
      <name>Fernand Meyer</name>
    </author>
    <link href="http://arxiv.org/abs/1202.0216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.0216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U10, 05C85" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.1634v1</id>
    <updated>2012-04-07T13:46:24Z</updated>
    <published>2012-04-07T13:46:24Z</published>
    <title>Automatic liver segmentation method in CT images</title>
    <summary>  The aim of this work is to develop a method for automatic segmentation of the
liver based on a priori knowledge of the image, such as location and shape of
the liver.
</summary>
    <author>
      <name>Oussema zayane</name>
    </author>
    <author>
      <name>besma jouini</name>
    </author>
    <author>
      <name>Mohamed Ali Mahjoub</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Canadian Journal on Image Processing &amp; Computer Vision Vol. 2, No.
  8, 1923-1717 December 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.1634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.1634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2994v1</id>
    <updated>2012-04-13T13:48:27Z</updated>
    <published>2012-04-13T13:48:27Z</published>
    <title>Image Restoration with Signal-dependent Camera Noise</title>
    <summary>  This article describes a fast iterative algorithm for image denoising and
deconvolution with signal-dependent observation noise. We use an optimization
strategy based on variable splitting that adapts traditional Gaussian
noise-based restoration algorithms to account for the observed image being
corrupted by mixed Poisson-Gaussian noise and quantization errors.
</summary>
    <author>
      <name>Ayan Chakrabarti</name>
    </author>
    <author>
      <name>Todd Zickler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.2994v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2994v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.3766v1</id>
    <updated>2012-05-16T19:11:51Z</updated>
    <published>2012-05-16T19:11:51Z</published>
    <title>Efficient Topology-Controlled Sampling of Implicit Shapes</title>
    <summary>  Sampling from distributions of implicitly defined shapes enables analysis of
various energy functionals used for image segmentation. Recent work describes a
computationally efficient Metropolis-Hastings method for accomplishing this
task. Here, we extend that framework so that samples are accepted at every
iteration of the sampler, achieving an order of magnitude speed up in
convergence. Additionally, we show how to incorporate topological constraints.
</summary>
    <author>
      <name>Jason Chang</name>
    </author>
    <author>
      <name>John W. Fisher III</name>
    </author>
    <link href="http://arxiv.org/abs/1205.3766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.3766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.7244v1</id>
    <updated>2012-06-29T15:07:26Z</updated>
    <published>2012-06-29T15:07:26Z</published>
    <title>Visual Vocabulary Learning and Its Application to 3D and Mobile Visual
  Search</title>
    <summary>  In this technical report, we review related works and recent trends in visual
vocabulary based web image search, object recognition, mobile visual search,
and 3D object retrieval. Especial focuses would be also given for the recent
trends in supervised/unsupervised vocabulary optimization, compact descriptor
for visual search, as well as in multi-view based 3D object representation.
</summary>
    <author>
      <name>Liujuan Cao</name>
    </author>
    <link href="http://arxiv.org/abs/1207.7244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.7244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.1127v1</id>
    <updated>2012-11-06T07:26:49Z</updated>
    <published>2012-11-06T07:26:49Z</published>
    <title>Visual Transfer Learning: Informal Introduction and Literature Overview</title>
    <summary>  Transfer learning techniques are important to handle small training sets and
to allow for quick generalization even from only a few examples. The following
paper is the introduction as well as the literature overview part of my thesis
related to the topic of transfer learning for visual recognition problems.
</summary>
    <author>
      <name>Erik Rodner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">part of my PhD thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.1127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.1127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.5712v1</id>
    <updated>2012-11-24T23:08:15Z</updated>
    <published>2012-11-24T23:08:15Z</published>
    <title>Detection of elliptical shapes via cross-entropy clustering</title>
    <summary>  The problem of finding elliptical shapes in an image will be considered. We
discuss the solution which uses cross-entropy clustering. The proposed method
allows the search for ellipses with predefined sizes and position in the space.
Moreover, it works well for search of ellipsoids in higher dimensions.
</summary>
    <author>
      <name>Jacek Tabor</name>
    </author>
    <author>
      <name>Krzysztof Misztal</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-38628-2_78</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-38628-2_78" rel="related"/>
    <link href="http://arxiv.org/abs/1211.5712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.4527v1</id>
    <updated>2012-12-18T22:30:23Z</updated>
    <published>2012-12-18T22:30:23Z</published>
    <title>GMM-Based Hidden Markov Random Field for Color Image and 3D Volume
  Segmentation</title>
    <summary>  In this project, we first study the Gaussian-based hidden Markov random field
(HMRF) model and its expectation-maximization (EM) algorithm. Then we
generalize it to Gaussian mixture model-based hidden Markov random field. The
algorithm is implemented in MATLAB. We also apply this algorithm to color image
segmentation problems and 3D volume segmentation problems.
</summary>
    <author>
      <name>Quan Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1212.4527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.4527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.2575v1</id>
    <updated>2013-02-04T22:52:13Z</updated>
    <published>2013-02-04T22:52:13Z</published>
    <title>Coded aperture compressive temporal imaging</title>
    <summary>  We use mechanical translation of a coded aperture for code division multiple
access compression of video. We present experimental results for reconstruction
at 148 frames per coded snapshot.
</summary>
    <author>
      <name>Patrick Llull</name>
    </author>
    <author>
      <name>Xuejun Liao</name>
    </author>
    <author>
      <name>Xin Yuan</name>
    </author>
    <author>
      <name>Jianbo Yang</name>
    </author>
    <author>
      <name>David Kittle</name>
    </author>
    <author>
      <name>Lawrence Carin</name>
    </author>
    <author>
      <name>Guillermo Sapiro</name>
    </author>
    <author>
      <name>David J. Brady</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1364/OE.21.010526</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1364/OE.21.010526" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages (when compiled with Optics Express' TEX template), 15
  figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.2575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.2575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5985v1</id>
    <updated>2013-02-25T03:12:12Z</updated>
    <published>2013-02-25T03:12:12Z</published>
    <title>A Meta-Theory of Boundary Detection Benchmarks</title>
    <summary>  Human labeled datasets, along with their corresponding evaluation algorithms,
play an important role in boundary detection. We here present a psychophysical
experiment that addresses the reliability of such benchmarks. To find better
remedies to evaluate the performance of any boundary detection algorithm, we
propose a computational framework to remove inappropriate human labels and
estimate the intrinsic properties of boundaries.
</summary>
    <author>
      <name>Xiaodi Hou</name>
    </author>
    <author>
      <name>Alan Yuille</name>
    </author>
    <author>
      <name>Christof Koch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS 2012 Workshop on Human Computation for Science and Computational
  Sustainability</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.5985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.2844v1</id>
    <updated>2013-03-12T11:23:47Z</updated>
    <published>2013-03-12T11:23:47Z</published>
    <title>A Stochastic Grammar for Natural Shapes</title>
    <summary>  We consider object detection using a generic model for natural shapes. A
common approach for object recognition involves matching object models directly
to images. Another approach involves building intermediate representations via
a generic grouping processes. We argue that these two processes (model-based
recognition and grouping) may use similar computational mechanisms. By defining
a generic model for shapes we can use model-based techniques to implement a
mid-level vision grouping process.
</summary>
    <author>
      <name>Pedro F. Felzenszwalb</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-1-4471-5195-1_21</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-1-4471-5195-1_21" rel="related"/>
    <link href="http://arxiv.org/abs/1303.2844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.2844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.0421v1</id>
    <updated>2013-04-01T19:14:27Z</updated>
    <published>2013-04-01T19:14:27Z</published>
    <title>Stroke-Based Cursive Character Recognition</title>
    <summary>  Human eye can see and read what is written or displayed either in natural
handwriting or in printed format. The same work in case the machine does is
called handwriting recognition. Handwriting recognition can be broken down into
two categories: off-line and on-line. ...
</summary>
    <author>
      <name>K. C. Santosh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>E. Iwata</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Character Recognition INTECH (Ed.) (2012) 175-192</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.0421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.0421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.1876v3</id>
    <updated>2013-05-28T05:24:25Z</updated>
    <published>2013-04-06T10:36:25Z</published>
    <title>Proceedings of the 37th Annual Workshop of the Austrian Association for
  Pattern Recognition (ÖAGM/AAPR), 2013</title>
    <summary>  This volume represents the proceedings of the 37th Annual Workshop of the
Austrian Association for Pattern Recognition (\"OAGM/AAPR), held May 23-24,
2013, in Innsbruck, Austria.
</summary>
    <author>
      <name>Justus Piater</name>
    </author>
    <author>
      <name>Antonio Rodríguez-Sánchez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contributed papers presented at \"OAGM/AAPR 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.1876v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.1876v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4; I.5; I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.1972v1</id>
    <updated>2013-04-07T09:43:47Z</updated>
    <published>2013-04-07T09:43:47Z</published>
    <title>Facial transformations of ancient portraits: the face of Caesar</title>
    <summary>  Some software solutions used to obtain the facial transformations can help
investigating the artistic metamorphosis of the ancient portraits of the same
person. An analysis with a freely available software of portraitures of Julius
Caesar is proposed, showing his several "morphs". The software helps enhancing
the mood the artist added to a portrait.
</summary>
    <author>
      <name>Amelia Carolina Sparavigna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Image processing, Facial transformation, Morphing, Portraits, Julius
  Caesar, Arles bust, Tusculum bust</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.1972v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.1972v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.2743v1</id>
    <updated>2013-03-27T19:48:54Z</updated>
    <published>2013-03-27T19:48:54Z</published>
    <title>Comparisons of Reasoning Mechanisms for Computer Vision</title>
    <summary>  An evidential reasoning mechanism based on the Dempster-Shafer theory of
evidence is introduced. Its performance in real-world image analysis is
compared with other mechanisms based on the Bayesian formalism and a simple
weight combination method.
</summary>
    <author>
      <name>Ze-Nian Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.2743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.2743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.2749v1</id>
    <updated>2013-03-27T19:49:25Z</updated>
    <published>2013-03-27T19:49:25Z</published>
    <title>Evidential Reasoning in Image Understanding</title>
    <summary>  In this paper, we present some results of evidential reasoning in
understanding multispectral images of remote sensing systems. The
Dempster-Shafer approach of combination of evidences is pursued to yield
contextual classification results, which are compared with previous results of
the Bayesian context free classification, contextual classifications of dynamic
programming and stochastic relaxation approaches.
</summary>
    <author>
      <name>Minchuan Zhang</name>
    </author>
    <author>
      <name>Su-shing Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.2749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.2749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3447v1</id>
    <updated>2013-03-27T19:58:23Z</updated>
    <published>2013-03-27T19:58:23Z</published>
    <title>Developing and Analyzing Boundary Detection Operators Using
  Probabilistic Models</title>
    <summary>  Most feature detectors such as edge detectors or circle finders are
statistical, in the sense that they decide at each point in an image about the
presence of a feature, this paper describes the use of Bayesian feature
detectors.
</summary>
    <author>
      <name>David Sher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.3447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.1303v1</id>
    <updated>2013-07-02T15:25:09Z</updated>
    <published>2013-07-02T15:25:09Z</published>
    <title>Submodularity of a Set Label Disagreement Function</title>
    <summary>  A set label disagreement function is defined over the number of variables
that deviates from the dominant label. The dominant label is the value assumed
by the largest number of variables within a set of binary variables. The
submodularity of a certain family of set label disagreement function is
discussed in this manuscript. Such disagreement function could be utilized as a
cost function in combinatorial optimization approaches for problems defined
over hypergraphs.
</summary>
    <author>
      <name>Toufiq Parag</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Janelia Farm Research Campus-HHMI</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1307.1303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.1303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3759v1</id>
    <updated>2013-07-14T17:37:36Z</updated>
    <published>2013-07-14T17:37:36Z</published>
    <title>A Minimal Six-Point Auto-Calibration Algorithm</title>
    <summary>  A non-iterative auto-calibration algorithm is presented. It deals with a
minimal set of six scene points in three views taken by a camera with fixed but
unknown intrinsic parameters. Calibration is based on the image correspondences
only. The algorithm is implemented and validated on synthetic image data.
</summary>
    <author>
      <name>Evgeniy Martyushev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 23rd International Conference on Computer
  Graphics and Vision, September 16-20, 2013 Vladivostok, Russia</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.3759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.0890v1</id>
    <updated>2013-08-05T05:17:26Z</updated>
    <published>2013-08-05T05:17:26Z</published>
    <title>Head Gesture Recognition using Optical Flow based Classification with
  Reinforcement of GMM based Background Subtraction</title>
    <summary>  This paper describes a technique of real time head gesture recognition
system. The method includes Gaussian mixture model (GMM) accompanied by optical
flow algorithm which provided us the required information regarding head
movement. The proposed model can be implemented in various control system. We
are also presenting the result and implementation of both mentioned method.
</summary>
    <author>
      <name>Parimita Saikia</name>
    </author>
    <author>
      <name>Karen Das</name>
    </author>
    <link href="http://arxiv.org/abs/1308.0890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.0890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.0319v3</id>
    <updated>2013-11-03T08:37:46Z</updated>
    <published>2013-10-01T14:26:29Z</published>
    <title>Second Croatian Computer Vision Workshop (CCVW 2013)</title>
    <summary>  Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013,
http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb,
Croatia. Workshop was organized by the Center of Excellence for Computer Vision
of the University of Zagreb.
</summary>
    <author>
      <name>Sven Lončarić</name>
    </author>
    <author>
      <name>Siniša Šegvić</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Papers presented at the Second Croatian Computer Vision Workshop CCVW
  2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.0319v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.0319v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6500v1</id>
    <updated>2013-11-11T20:32:50Z</updated>
    <published>2013-11-11T20:32:50Z</published>
    <title>Stitched Panoramas from Toy Airborne Video Cameras</title>
    <summary>  Effective panoramic photographs are taken from vantage points that are high.
High vantage points have recently become easier to reach as the cost of
quadrotor helicopters has dropped to nearly disposable levels. Although cameras
carried by such aircraft weigh only a few grams, their low-quality video can be
converted into panoramas of high quality and high resolution. Also, the small
size of these aircraft vastly reduces the risks inherent to flight.
</summary>
    <author>
      <name>Camille Goudeseune</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.6500v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6500v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3; I.4; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.3035v1</id>
    <updated>2013-12-11T04:59:49Z</updated>
    <published>2013-12-11T04:59:49Z</published>
    <title>Heat kernel coupling for multiple graph analysis</title>
    <summary>  In this paper, we introduce heat kernel coupling (HKC) as a method of
constructing multimodal spectral geometry on weighted graphs of different size
without vertex-wise bijective correspondence. We show that Laplacian averaging
can be derived as a limit case of HKC, and demonstrate its applications on
several problems from the manifold learning and pattern recognition domain.
</summary>
    <author>
      <name>Michael M. Bronstein</name>
    </author>
    <author>
      <name>Klaus Glashoff</name>
    </author>
    <link href="http://arxiv.org/abs/1312.3035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.3035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.3724v1</id>
    <updated>2013-12-13T07:54:22Z</updated>
    <published>2013-12-13T07:54:22Z</published>
    <title>ARIANNA: pAth Recognition for Indoor Assisted NavigatioN with Augmented
  perception</title>
    <summary>  ARIANNA stands for pAth Recognition for Indoor Assisted Navigation with
Augmented perception. It is a flexible and low cost navigation system for vi-
sually impaired people. Arianna permits to navigate colored paths painted or
sticked on the floor revealing their directions through vibrational feedback on
commercial smartphones.
</summary>
    <author>
      <name>Pierluigi Gallo</name>
    </author>
    <author>
      <name>Ilenia Tinnirello</name>
    </author>
    <author>
      <name>Laura Giarré</name>
    </author>
    <author>
      <name>Domenico Garlisi</name>
    </author>
    <author>
      <name>Daniele Croce</name>
    </author>
    <author>
      <name>Adriano Fagiolini</name>
    </author>
    <link href="http://arxiv.org/abs/1312.3724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.3724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.2031v1</id>
    <updated>2014-03-09T06:53:13Z</updated>
    <published>2014-03-09T06:53:13Z</published>
    <title>Texture Defect Detection in Gradient Space</title>
    <summary>  In this paper, we propose a machine vision algorithm for automatically
detecting defects in patterned textures with the help of gradient space and its
energy. Experiments on real fabric images with defects show that the proposed
method can be used for automatic detection of fabric defects in textile
industries.
</summary>
    <author>
      <name>V. Asha</name>
    </author>
    <author>
      <name>N. U. Bhajantri</name>
    </author>
    <author>
      <name>P. Nagabhushan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, ICFoCS-2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.2031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.2031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="11K70, 39A14, 39A70, 47A30, 62H30, 68M20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.3964v1</id>
    <updated>2014-03-16T22:03:45Z</updated>
    <published>2014-03-16T22:03:45Z</published>
    <title>Image processing using miniKanren</title>
    <summary>  An integral image is one of the most efficient optimization technique for
image processing. However an integral image is only a special case of delayed
stream or memoization. This research discusses generalizing concept of integral
image optimization technique, and how to generate an integral image optimized
program code automatically from abstracted image processing algorithm. In oder
to abstruct algorithms, we forces to miniKanren.
</summary>
    <author>
      <name>Hirotaka Niitsuma</name>
    </author>
    <link href="http://arxiv.org/abs/1403.3964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.3964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.7748v1</id>
    <updated>2014-04-30T14:54:19Z</updated>
    <published>2014-04-30T14:54:19Z</published>
    <title>A graph-based mathematical morphology reader</title>
    <summary>  This survey paper aims at providing a "literary" anthology of mathematical
morphology on graphs. It describes in the English language many ideas stemming
from a large number of different papers, hence providing a unified view of an
active and diverse field of research.
</summary>
    <author>
      <name>Laurent Najman</name>
    </author>
    <author>
      <name>Jean Cousty</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patrec.2014.05.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patrec.2014.05.007" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pattern Recognition Letters 47 (2014) 3-17</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.7748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.7748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1678v3</id>
    <updated>2015-03-12T12:38:13Z</updated>
    <published>2014-05-07T17:51:52Z</published>
    <title>RPCA-KFE: Key Frame Extraction for Consumer Video based Robust Principal
  Component Analysis</title>
    <summary>  Key frame extraction algorithms consider the problem of selecting a subset of
the most informative frames from a video to summarize its content.
</summary>
    <author>
      <name>Chinh Dang</name>
    </author>
    <author>
      <name>Abdolreza Moghadam</name>
    </author>
    <author>
      <name>Hayder Radha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2015.2445572</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2015.2445572" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.1678v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1678v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.6132v1</id>
    <updated>2014-02-05T00:46:18Z</updated>
    <published>2014-02-05T00:46:18Z</published>
    <title>Comparative analysis of common edge detection techniques in context of
  object extraction</title>
    <summary>  Edges characterize boundaries and are therefore a problem of practical
importance in remote sensing.In this paper a comparative study of various edge
detection techniques and band wise analysis of these algorithms in the context
of object extraction with regard to remote sensing satellite images from the
Indian Remote Sensing Satellite (IRS) sensors LISS 3, LISS 4 and Cartosat1 as
well as Google Earth is presented.
</summary>
    <author>
      <name>S. K. Katiyar</name>
    </author>
    <author>
      <name>P. V. Arun</name>
    </author>
    <link href="http://arxiv.org/abs/1405.6132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.6133v1</id>
    <updated>2014-02-05T14:49:36Z</updated>
    <published>2014-02-05T14:49:36Z</published>
    <title>A review over the applicability of image entropy in analyses of remote
  sensing datasets</title>
    <summary>  Entropy is the measure of uncertainty in any data and is adopted for
maximisation of mutual information in many remote sensing operations. The
availability of wide entropy variations motivated us for an investigation over
the suitability preference of these versions to specific operations.
</summary>
    <author>
      <name>S. K. Katiyar</name>
    </author>
    <author>
      <name>P. V. Arun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1303.6926</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.6133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.3673v2</id>
    <updated>2016-10-21T16:07:15Z</updated>
    <published>2014-07-03T10:55:52Z</published>
    <title>Enhanced EZW Technique for Compression of Image by Setting Detail
  Retaining Pass Number</title>
    <summary>  This submission has been withdrawn by arXiv administrators because it
contains excessive and unattributed reuse of content from other authors.
</summary>
    <author>
      <name>Isha Tyagi</name>
    </author>
    <author>
      <name>Ashish Nautiyal</name>
    </author>
    <author>
      <name>Vishwanath Bijalwan</name>
    </author>
    <author>
      <name>Meenu Balodhi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This submission has been withdrawn by arXiv administrators because it
  contains excessive and unattributed reuse of content from other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.3673v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.3673v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.6492v2</id>
    <updated>2014-08-14T11:28:48Z</updated>
    <published>2014-07-24T08:53:00Z</published>
    <title>Recognition of Handwritten Persian/Arabic Numerals Based on Robust
  Feature Set and K-NN Classifier</title>
    <summary>  This paper has been withdrawn by the author due to a crucial sign error in
equation 2 and some mistake in Table 1 information. please let me for changing
this information and updating this paper.
</summary>
    <author>
      <name>Reza Azad</name>
    </author>
    <author>
      <name>Fatemeh Davami</name>
    </author>
    <author>
      <name>Hamid Reza Shayegh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the main author due to the Table 1
  and equation 2 errors</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.6492v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.6492v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.1986v1</id>
    <updated>2014-08-08T21:24:59Z</updated>
    <published>2014-08-08T21:24:59Z</published>
    <title>Gabor-like Image Filtering using a Neural Microcircuit</title>
    <summary>  In this letter, we present an implementation of a neural microcircuit for
image processing employing Hebbian-adaptive learning. The neuronal circuit
utilizes only excitatory synapses to correlate action potentials, extracting
the uncorrelated ones, which contain significant image information. This
circuit is capable of approximating Gabor-like image filtering and other image
processing functions
</summary>
    <author>
      <name>C. Mayr</name>
    </author>
    <author>
      <name>A. Heittmann</name>
    </author>
    <author>
      <name>R. Schüffny</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNN.2007.891687</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNN.2007.891687" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Neural Networks, vol. 18, pages 955-959, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.1986v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.1986v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.2474v1</id>
    <updated>2014-10-09T14:08:46Z</updated>
    <published>2014-10-09T14:08:46Z</published>
    <title>Genetic Stereo Matching Algorithm with Fuzzy Fitness</title>
    <summary>  This paper presents a genetic stereo matching algorithm with fuzzy evaluation
function. The proposed algorithm presents a new encoding scheme in which a
chromosome is represented by a disparity matrix. Evolution is controlled by a
fuzzy fitness function able to deal with noise and uncertain camera
measurements, and uses classical evolutionary operators. The result of the
algorithm is accurate dense disparity maps obtained in a reasonable
computational time suitable for real-time applications as shown in experimental
results.
</summary>
    <author>
      <name>Haythem Ghazouani</name>
    </author>
    <link href="http://arxiv.org/abs/1410.2474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.2474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.2663v1</id>
    <updated>2014-10-10T02:48:08Z</updated>
    <published>2014-10-10T02:48:08Z</published>
    <title>Challenge IEEE-ISBI/TCB : Application of Covariance matrices and wavelet
  marginals</title>
    <summary>  This short memo aims at explaining our approach for the challenge IEEE-ISBI
on Bone Texture Characterization. In this work, we focus on the use of
covariance matrices and wavelet marginals in an SVM classifier.
</summary>
    <author>
      <name>Florian Yger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 Figues, 2 Tables, Challenge IEEE-ISBI : Bone Texture
  Characterization</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.2663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.2663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.1442v1</id>
    <updated>2014-11-05T22:56:10Z</updated>
    <published>2014-11-05T22:56:10Z</published>
    <title>Optical Character Recognition, Using K-Nearest Neighbors</title>
    <summary>  The problem of optical character recognition, OCR, has been widely discussed
in the literature. Having a hand-written text, the program aims at recognizing
the text. Even though there are several approaches to this issue, it is still
an open problem. In this paper we would like to propose an approach that uses
K-nearest neighbors algorithm, and has the accuracy of more than 90%. The
training and run time is also very short.
</summary>
    <author>
      <name>Wei Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1411.1442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.1442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.6850v1</id>
    <updated>2014-11-25T13:13:47Z</updated>
    <published>2014-11-25T13:13:47Z</published>
    <title>Similarity- based approach for outlier detection</title>
    <summary>  This paper presents a new approach for detecting outliers by introducing the
notion of object's proximity. The main idea is that normal point has similar
characteristics with several neighbors. So the point in not an outlier if it
has a high degree of proximity and its neighbors are several. The performance
of this approach is illustrated through real datasets
</summary>
    <author>
      <name>Amina Dik</name>
    </author>
    <author>
      <name>Khalid Jebari</name>
    </author>
    <author>
      <name>Abdelaziz Bouroumi</name>
    </author>
    <author>
      <name>Aziz Ettouhami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.6850v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6850v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.7855v1</id>
    <updated>2014-11-28T13:18:22Z</updated>
    <published>2014-11-28T13:18:22Z</published>
    <title>V-variable image compression</title>
    <summary>  V-variable fractals, where $V$ is a positive integer, are intuitively
fractals with at most $V$ different "forms" or "shapes" at all levels of
magnification. In this paper we describe how V-variable fractals can be used
for the purpose of image compression.
</summary>
    <author>
      <name>Franklin Mendivil</name>
    </author>
    <author>
      <name>Örjan Stenflo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0218348X15500073</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0218348X15500073" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 22 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Fractals, 23, no 02, (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1411.7855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.7855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="28A80, 68U10, 94A08" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5334v1</id>
    <updated>2014-12-17T10:58:46Z</updated>
    <published>2014-12-17T10:58:46Z</published>
    <title>The Affine Transforms for Image Enhancement in the Context of
  Logarithmic Models</title>
    <summary>  The logarithmic model offers new tools for image processing. An efficient
method for image enhancement is to use an affine transformation with the
logarithmic operations: addition and scalar multiplication. We define some
criteria for automatically determining the parameters of the processing and
this is done via mean and variance computed by logarithmic operations.
</summary>
    <author>
      <name>Vasile Patrascu</name>
    </author>
    <author>
      <name>Vasile Buzuloiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Computer Vision and Graphics, ICCVG2002,
  25-29 September, 2002, Zakopane, Poland</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.5334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5769v1</id>
    <updated>2014-12-18T09:19:50Z</updated>
    <published>2014-12-18T09:19:50Z</published>
    <title>Gray level image enhancement using the Bernstein polynomials</title>
    <summary>  This paper presents a method for enhancing the gray level images. This
presented method takes part from the category of point operations and it is
based on piecewise linear functions. The interpolation nodes of these functions
are calculated using the Bernstein polynomials.
</summary>
    <author>
      <name>Vasile Patrascu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Scientific Bulletin of the Politechnica, University of
  Timisoara,Transactions on Electronics and Communications, Vol. 47 (61), No:
  2,pp.121-126, June 2002</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.5769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6061v1</id>
    <updated>2014-12-15T06:55:28Z</updated>
    <published>2014-12-15T06:55:28Z</published>
    <title>CITlab ARGUS for Arabic Handwriting</title>
    <summary>  In the recent years it turned out that multidimensional recurrent neural
networks (MDRNN) perform very well for offline handwriting recognition tasks
like the OpenHaRT 2013 evaluation DIR. With suitable writing preprocessing and
dictionary lookup, our ARGUS software completed this task with an error rate of
26.27% in its primary setup.
</summary>
    <author>
      <name>Gundram Leifert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <author>
      <name>Roger Labahn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <author>
      <name>Tobias Strauß</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://www.nist.gov/itl/iad/mig/upload/OpenHaRT2013_SysDesc_CITLAB.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.6061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10, 68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6154v1</id>
    <updated>2014-10-06T11:45:07Z</updated>
    <published>2014-10-06T11:45:07Z</published>
    <title>Effective persistent homology of digital images</title>
    <summary>  In this paper, three Computational Topology methods (namely effective
homology, persistent homology and discrete vector fields) are mixed together to
produce algorithms for homological digital image processing. The algorithms
have been implemented as extensions of the Kenzo system and have shown a good
performance when applied on some actual images extracted from a public dataset.
</summary>
    <author>
      <name>Ana Romero</name>
    </author>
    <author>
      <name>Julio Rubio</name>
    </author>
    <author>
      <name>Francis Sergeraert</name>
    </author>
    <link href="http://arxiv.org/abs/1412.6154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.07692v1</id>
    <updated>2015-01-30T08:12:48Z</updated>
    <published>2015-01-30T08:12:48Z</published>
    <title>Blob indentation identification via curvature measurement</title>
    <summary>  This paper presents a novel method for identifying indentations on the
boundary of solid 2D shape. It uses the signed curvature at a set of points
along the boundary to identify indentations and provides one parameter for
tuning the selection mechanism for discriminating indentations from other
boundary irregularities. An efficient implementation is described based on the
Fourier transform for calculating curvature from a sequence of points obtained
from the boundary of a binary blob.
</summary>
    <author>
      <name>Matthew Sottile</name>
    </author>
    <link href="http://arxiv.org/abs/1501.07692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.07692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05565v1</id>
    <updated>2015-02-19T13:35:06Z</updated>
    <published>2015-02-19T13:35:06Z</published>
    <title>Multi-valued Color Representation Based on Frank t-norm Properties</title>
    <summary>  In this paper two knowledge representation models are proposed, FP4 and FP6.
Both combine ideas from fuzzy sets and four-valued and hexa-valued logics. Both
represent imprecise properties whose accomplished degree is unknown or
contradictory for some objects. A possible application in the color analysis
and color image processing is discussed.
</summary>
    <author>
      <name>Vasile Patrascu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12th International Conference Information Processing and Management
  of Uncertainty for Knowledge-Based Systems, IPMU'2008, pp. 1215-1222, June
  22-27, 2008, Malaga, Spain</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.05565v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05565v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.06556v1</id>
    <updated>2015-02-23T19:11:12Z</updated>
    <published>2015-02-23T19:11:12Z</published>
    <title>Shannon, Tsallis and Kaniadakis entropies in bi-level image thresholding</title>
    <summary>  The maximum entropy principle is often used for bi-level or multi-level
thresholding of images. For this purpose, some methods are available based on
Shannon and Tsallis entropies. In this paper, we discuss them and propose a
method based on Kaniadakis entropy.
</summary>
    <author>
      <name>Amelia Carolina Sparavigna</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18483/ijSci.626</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18483/ijSci.626" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: Kaniadakis Entropy, Image Processing, Image Segmentation,
  Image Thresholding, Texture Transitions</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Sciences 4(2), 35-43, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1502.06556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.06556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.01065v1</id>
    <updated>2015-04-30T10:10:16Z</updated>
    <published>2015-04-30T10:10:16Z</published>
    <title>Proceedings of The 39th Annual Workshop of the Austrian Association for
  Pattern Recognition (OAGM), 2015</title>
    <summary>  The 39th annual workshop of the Austrian Association for Pattern Recognition
(OAGM/AAPR) provides a platform for presentation and discussion of research
progress as well as research projects within the OAGM/AAPR community.
</summary>
    <author>
      <name>Sebastian Hegenbart</name>
    </author>
    <author>
      <name>Roland Kwitt</name>
    </author>
    <author>
      <name>Andreas Uhl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Index submitted before individual papers</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.01065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.01065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.02890v2</id>
    <updated>2015-08-25T15:12:37Z</updated>
    <published>2015-05-12T07:30:22Z</published>
    <title>Sparse 3D convolutional neural networks</title>
    <summary>  We have implemented a convolutional neural network designed for processing
sparse three-dimensional input data. The world we live in is three dimensional
so there are a large number of potential applications including 3D object
recognition and analysis of space-time objects. In the quest for efficiency, we
experiment with CNNs on the 2D triangular-lattice and 3D tetrahedral-lattice.
</summary>
    <author>
      <name>Ben Graham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BMVC 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.02890v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.02890v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03795v1</id>
    <updated>2015-05-14T16:43:07Z</updated>
    <published>2015-05-14T16:43:07Z</published>
    <title>Fast and numerically stable circle fit</title>
    <summary>  We develop a new algorithm for fitting circles that does not have drawbacks
commonly found in existing circle fits. Our fit achieves ultimate accuracy (to
machine precision), avoids divergence, and is numerically stable even when
fitting circles get arbitrary large. Lastly, our algorithm takes less than 10
iterations to converge, on average.
</summary>
    <author>
      <name>Houssam Abdul-Rahman</name>
    </author>
    <author>
      <name>Nikolai Chernov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10851-013-0461-4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10851-013-0461-4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Mathematical Imaging and Vision June 2014, Volume 49,
  Issue 2, pp 289-295</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1505.03795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.05240v1</id>
    <updated>2015-05-20T04:09:47Z</updated>
    <published>2015-05-20T04:09:47Z</published>
    <title>Benchmarking KAZE and MCM for Multiclass Classification</title>
    <summary>  In this paper, we propose a novel approach for feature generation by
appropriately fusing KAZE and SIFT features. We then use this feature set along
with Minimal Complexity Machine(MCM) for object classification. We show that
KAZE and SIFT features are complementary. Experimental results indicate that an
elementary integration of these techniques can outperform the state-of-the-art
approaches.
</summary>
    <author>
      <name>Siddharth Srivastava</name>
    </author>
    <author>
      <name>Prerana Mukherjee</name>
    </author>
    <author>
      <name>Brejesh Lall</name>
    </author>
    <link href="http://arxiv.org/abs/1505.05240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.05240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.7; I.5.4; I.4.8; I.4.9; I.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.06769v1</id>
    <updated>2015-05-25T22:18:36Z</updated>
    <published>2015-05-25T22:18:36Z</published>
    <title>VeinPLUS: A Transillumination and Reflection-based Hand Vein Database</title>
    <summary>  This paper gives a short summary of work related to the creation of a
department-hosted hand vein database. After the introducing section, special
properties of the hand vein acquisition are explained, followed by a comparison
table, which shows key differences to existing well-known hand vein databases.
At the end, the ROI extraction process is described and sample images and ROIs
are presented.
</summary>
    <author>
      <name>Alexander Gruschina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at OAGM Workshop, 2015 (arXiv:1505.01065)</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.06769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.06769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06274v1</id>
    <updated>2015-06-20T17:55:49Z</updated>
    <published>2015-06-20T17:55:49Z</published>
    <title>Pose Estimation Based on 3D Models</title>
    <summary>  In this paper, we proposed a pose estimation system based on rendered image
training set, which predicts the pose of objects in real image, with knowledge
of object category and tight bounding box. We developed a patch-based
multi-class classification algorithm, and an iterative approach to improve the
accuracy. We achieved state-of-the-art performance on pose estimation task.
</summary>
    <author>
      <name>Chuiwen Ma</name>
    </author>
    <author>
      <name>Hao Su</name>
    </author>
    <author>
      <name>Liang Shi</name>
    </author>
    <link href="http://arxiv.org/abs/1506.06274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08959v2</id>
    <updated>2015-09-24T09:04:24Z</updated>
    <published>2015-06-30T06:47:50Z</published>
    <title>A Large-Scale Car Dataset for Fine-Grained Categorization and
  Verification</title>
    <summary>  Updated on 24/09/2015: This update provides preliminary experiment results
for fine-grained classification on the surveillance data of CompCars. The
train/test splits are provided in the updated dataset. See details in Section
6.
</summary>
    <author>
      <name>Linjie Yang</name>
    </author>
    <author>
      <name>Ping Luo</name>
    </author>
    <author>
      <name>Chen Change Loy</name>
    </author>
    <author>
      <name>Xiaoou Tang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An extension to our conference paper in CVPR 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.08959v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08959v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05053v1</id>
    <updated>2015-07-17T17:48:49Z</updated>
    <published>2015-07-17T17:48:49Z</published>
    <title>Massively Deep Artificial Neural Networks for Handwritten Digit
  Recognition</title>
    <summary>  Greedy Restrictive Boltzmann Machines yield an fairly low 0.72% error rate on
the famous MNIST database of handwritten digits. All that was required to
achieve this result was a high number of hidden layers consisting of many
neurons, and a graphics card to greatly speed up the rate of learning.
</summary>
    <author>
      <name>Keiron O'Shea</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.05053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05244v1</id>
    <updated>2015-07-19T03:14:56Z</updated>
    <published>2015-07-19T03:14:56Z</published>
    <title>Handwriting Recognition</title>
    <summary>  This paper describes the method to recognize offline handwritten characters.
A robust algorithm for handwriting segmentation is described here with the help
of which individual characters can be segmented from a selected word from a
paragraph of handwritten text image which is given as input.
</summary>
    <author>
      <name>Jayati Ghosh Dastidar</name>
    </author>
    <author>
      <name>Surabhi Sarkar</name>
    </author>
    <author>
      <name>Rick Punyadyuti Sinha</name>
    </author>
    <author>
      <name>Kasturi Basu</name>
    </author>
    <link href="http://arxiv.org/abs/1507.05244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02246v1</id>
    <updated>2015-08-10T13:51:35Z</updated>
    <published>2015-08-10T13:51:35Z</published>
    <title>Feature Learning for Interaction Activity Recognition in RGBD Videos</title>
    <summary>  This paper proposes a human activity recognition method which is based on
features learned from 3D video data without incorporating domain knowledge. The
experiments on data collected by RGBD cameras produce results outperforming
other techniques. Our feature encoding method follows the bag-of-visual-word
model, then we use a SVM classifier to recognise the activities. We do not use
skeleton or tracking information and the same technique is applied on color and
depth data.
</summary>
    <author>
      <name>Ngu Nguyen</name>
    </author>
    <link href="http://arxiv.org/abs/1508.02246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.04232v1</id>
    <updated>2015-09-14T18:30:05Z</updated>
    <published>2015-09-14T18:30:05Z</published>
    <title>gSLICr: SLIC superpixels at over 250Hz</title>
    <summary>  We introduce a parallel GPU implementation of the Simple Linear Iterative
Clustering (SLIC) superpixel segmentation. Using a single graphic card, our
implementation achieves speedups of up to $83\times$ from the standard
sequential implementation. Our implementation is fully compatible with the
standard sequential implementation and the software is now available online and
is open source.
</summary>
    <author>
      <name>Carl Yuheng Ren</name>
    </author>
    <author>
      <name>Victor Adrian Prisacariu</name>
    </author>
    <author>
      <name>Ian D Reid</name>
    </author>
    <link href="http://arxiv.org/abs/1509.04232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.04232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.05054v1</id>
    <updated>2015-09-16T20:23:06Z</updated>
    <published>2015-09-16T20:23:06Z</published>
    <title>Overcomplete Dictionary Learning with Jacobi Atom Updates</title>
    <summary>  Dictionary learning for sparse representations is traditionally approached
with sequential atom updates, in which an optimized atom is used immediately
for the optimization of the next atoms. We propose instead a Jacobi version, in
which groups of atoms are updated independently, in parallel. Extensive
numerical evidence for sparse image representation shows that the parallel
algorithms, especially when all atoms are updated simultaneously, give better
dictionaries than their sequential counterparts.
</summary>
    <author>
      <name>Paul Irofti</name>
    </author>
    <author>
      <name>Bogdan Dumitrescu</name>
    </author>
    <link href="http://arxiv.org/abs/1509.05054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.05054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.03650v3</id>
    <updated>2015-12-07T14:20:01Z</updated>
    <published>2015-11-11T20:54:28Z</published>
    <title>Piecewise Linear Activation Functions For More Efficient Deep Networks</title>
    <summary>  This submission has been withdrawn by arXiv administrators because it is
intentionally incomplete, which is in violation of our policies.
</summary>
    <author>
      <name>Cheng-Yang Fu</name>
    </author>
    <author>
      <name>Alexander C. Berg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Withdrawn by arXiv admins</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.03650v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03650v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06489v2</id>
    <updated>2018-10-23T22:04:42Z</updated>
    <published>2015-11-20T04:56:47Z</published>
    <title>A Simple Hierarchical Pooling Data Structure for Loop Closure</title>
    <summary>  We propose a data structure obtained by hierarchically averaging bag-of-word
descriptors during a sequence of views that achieves average speedups in
large-scale loop closure applications ranging from 4 to 20 times on benchmark
datasets. Although simple, the method works as well as sophisticated
agglomerative schemes at a fraction of the cost with minimal loss of
performance.
</summary>
    <author>
      <name>Xiaohan Fei</name>
    </author>
    <author>
      <name>Konstantine Tsotsos</name>
    </author>
    <author>
      <name>Stefano Soatto</name>
    </author>
    <link href="http://arxiv.org/abs/1511.06489v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06489v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.07347v1</id>
    <updated>2015-11-23T18:15:13Z</updated>
    <published>2015-11-23T18:15:13Z</published>
    <title>Node Specificity in Convolutional Deep Nets Depends on Receptive Field
  Position and Size</title>
    <summary>  In convolutional deep neural networks, receptive field (RF) size increases
with hierarchical depth. When RF size approaches full coverage of the input
image, different RF positions result in RFs with different specificity, as
portions of the RF fall out of the input space. This leads to a departure from
the convolutional concept of positional invariance and opens the possibility
for complex forms of context specificity.
</summary>
    <author>
      <name>Karl Zipser</name>
    </author>
    <link href="http://arxiv.org/abs/1511.07347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.07347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01533v1</id>
    <updated>2015-12-04T20:28:27Z</updated>
    <published>2015-12-04T20:28:27Z</published>
    <title>Motion trails from time-lapse video</title>
    <summary>  From an image sequence captured by a stationary camera, background
subtraction can detect moving foreground objects in the scene. Distinguishing
foreground from background is further improved by various heuristics. Then each
object's motion can be emphasized by duplicating its positions as a motion
trail. These trails clarify the objects' spatial relationships. Also, adding
motion trails to a video before previewing it at high speed reduces the risk of
overlooking transient events.
</summary>
    <author>
      <name>Camille Goudeseune</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.01533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.3; I.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02329v2</id>
    <updated>2015-12-15T06:18:35Z</updated>
    <published>2015-12-08T05:04:57Z</published>
    <title>Computational Models for Multiview Dense Depth Maps of Dynamic Scene</title>
    <summary>  This paper reviews the recent progresses of the depth map generation for
dynamic scene and its corresponding computational models. This paper mainly
covers the homogeneous ambiguity models in depth sensing, resolution models in
depth processing, and consistency models in depth optimization. We also
summarize the future work in the depth map generation.
</summary>
    <author>
      <name>Qifei Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, IEEE COMSOC MMTC E-Letter 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.02329v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02329v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02357v1</id>
    <updated>2015-12-08T07:25:29Z</updated>
    <published>2015-12-08T07:25:29Z</published>
    <title>Towards the Application of Linear Programming Methods For Multi-Camera
  Pose Estimation</title>
    <summary>  We presented a separation based optimization algorithm which, rather than
optimization the entire variables altogether, This would allow us to employ: 1)
a class of nonlinear functions with three variables and 2) a convex quadratic
multivariable polynomial, for minimization of reprojection error. Neglecting
the inversion required to minimize the nonlinear functions, in this paper we
demonstrate how separation allows eradication of matrix inversion.
</summary>
    <author>
      <name>Masoud Aghamohamadian-Sharbaf</name>
    </author>
    <author>
      <name>Ahmadreza Heravi</name>
    </author>
    <author>
      <name>Hamidreza Pourreza</name>
    </author>
    <link href="http://arxiv.org/abs/1512.02357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.06014v2</id>
    <updated>2016-12-04T20:42:48Z</updated>
    <published>2015-12-18T16:17:35Z</published>
    <title>Multiclass Classification of Cervical Cancer Tissues by Hidden Markov
  Model</title>
    <summary>  In this paper, we report a hidden Markov model based multiclass
classification of cervical cancer tissues. This model has been validated
directly over time series generated by the medium refractive index fluctuations
extracted from differential interference contrast images of healthy and
different stages of cancer tissues. The method shows promising results for
multiclass classification with higher accuracy.
</summary>
    <author>
      <name>Sabyasachi Mukhopadhyay</name>
    </author>
    <author>
      <name>Sanket Nandan</name>
    </author>
    <author>
      <name>Indrajit Kurmi</name>
    </author>
    <link href="http://arxiv.org/abs/1512.06014v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.06014v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00396v1</id>
    <updated>2016-01-04T07:28:53Z</updated>
    <published>2016-01-04T07:28:53Z</published>
    <title>Automatic Detection and Decoding of Photogrammetric Coded Targets</title>
    <summary>  Close-range Photogrammetry is widely used in many industries because of the
cost effectiveness and efficiency of the technique. In this research, we
introduce an automated coded target detection method which can be used to
enhance the efficiency of the Photogrammetry.
</summary>
    <author>
      <name>Udaya Wijenayake</name>
    </author>
    <author>
      <name>Sung-In Choi</name>
    </author>
    <author>
      <name>Soon-Yong Park</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ELINFOCOM.2014.6914413</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ELINFOCOM.2014.6914413" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures, Electronics, Information and Communications
  (ICEIC), 2014 International Conference on</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.00396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04568v1</id>
    <updated>2016-01-18T15:22:48Z</updated>
    <published>2016-01-18T15:22:48Z</published>
    <title>Content Aware Neural Style Transfer</title>
    <summary>  This paper presents a content-aware style transfer algorithm for paintings
and photos of similar content using pre-trained neural network, obtaining
better results than the previous work. In addition, the numerical experiments
show that the style pattern and the content information is not completely
separated by neural network.
</summary>
    <author>
      <name>Rujie Yin</name>
    </author>
    <link href="http://arxiv.org/abs/1601.04568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.10; I.5.2; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.08003v1</id>
    <updated>2016-01-29T08:54:22Z</updated>
    <published>2016-01-29T08:54:22Z</published>
    <title>Efficient Robust Mean Value Calculation of 1D Features</title>
    <summary>  A robust mean value is often a good alternative to the standard mean value
when dealing with data containing many outliers. An efficient method for
samples of one-dimensional features and the truncated quadratic error norm is
presented and compared to the method of channel averaging (soft histograms).
</summary>
    <author>
      <name>Erik Jonsson</name>
    </author>
    <author>
      <name>Michael Felsberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the SSBA Symposium 2005, Malm\"o, Sweden</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.08003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.08003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01772v1</id>
    <updated>2016-03-06T00:30:34Z</updated>
    <published>2016-03-06T00:30:34Z</published>
    <title>Fast calculation of correlations in recognition systems</title>
    <summary>  Computationally efficient classification system architecture is proposed. It
utilizes fast tensor-vector multiplication algorithm to apply linear operators
upon input signals . The approach is applicable to wide variety of recognition
system architectures ranging from single stage matched filter bank classifiers
to complex neural networks with unlimited number of hidden layers.
</summary>
    <author>
      <name>Pavel Dourbal</name>
    </author>
    <author>
      <name>Mikhail Pekker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.01772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H30, 65F05, 65F10, 65F30, 65F50, 68T05, 68T10, 94A11, 94A12,&#10;  94A13, 94A15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.1; G.1.0; G.1.3; G.4; H.4.2; I.1.2; I.2.2; I.5.2; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06433v1</id>
    <updated>2016-03-21T14:23:00Z</updated>
    <published>2016-03-21T14:23:00Z</published>
    <title>Illumination-invariant image mosaic calculation based on logarithmic
  search</title>
    <summary>  This technical report describes an improved image mosaicking algorithm. It is
based on Jain's logarithmic search algorithm [Jain 1981] which is coupled to
the method of Kourogi (1999} for matching images in a video sequence.
Logarithmic search has a better invariance against illumination changes than
the original optical-flow-based method of Kourogi.
</summary>
    <author>
      <name>Wolfgang Konen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09037v1</id>
    <updated>2016-03-30T04:40:31Z</updated>
    <published>2016-03-30T04:40:31Z</published>
    <title>Vector Quantization for Machine Vision</title>
    <summary>  This paper shows how to reduce the computational cost for a variety of common
machine vision tasks by operating directly in the compressed domain,
particularly in the context of hardware acceleration. Pyramid Vector
Quantization (PVQ) is the compression technique of choice and its properties
are exploited to simplify Support Vector Machines (SVM), Convolutional Neural
Networks(CNNs), Histogram of Oriented Gradients (HOG) features, interest points
matching and other algorithms.
</summary>
    <author>
      <name>Vincenzo Liguori</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09129v1</id>
    <updated>2016-03-30T11:11:29Z</updated>
    <published>2016-03-30T11:11:29Z</published>
    <title>Exploiting Facial Landmarks for Emotion Recognition in the Wild</title>
    <summary>  In this paper, we describe an entry to the third Emotion Recognition in the
Wild Challenge, EmotiW2015. We detail the associated experiments and show that,
through more accurately locating the facial landmarks, and considering only the
distances between them, we can achieve a surprising level of performance. The
resulting system is not only more accurate than the challenge baseline, but
also much simpler.
</summary>
    <author>
      <name>Matthew Day</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, ICMI 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04926v1</id>
    <updated>2016-04-17T20:48:49Z</updated>
    <published>2016-04-17T20:48:49Z</published>
    <title>Some medical applications of example-based super-resolution</title>
    <summary>  Example-based super-resolution (EBSR) reconstructs a high-resolution image
from a low-resolution image, given a training set of high-resolution images. In
this note I propose some applications of EBSR to medical imaging. A particular
interesting application, which I call "x-ray voxelization", approximates the
result of a CT scan from an x-ray image.
</summary>
    <author>
      <name>Ramin Zabih</name>
    </author>
    <link href="http://arxiv.org/abs/1604.04926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04250v2</id>
    <updated>2016-08-01T13:25:06Z</updated>
    <published>2016-05-13T16:56:10Z</published>
    <title>Color Homography</title>
    <summary>  We show the surprising result that colors across a change in viewing
condition (changing light color, shading and camera) are related by a
homography. Our homography color correction application delivers improved color
fidelity compared with the linear least-square.
</summary>
    <author>
      <name>Graham D. Finlayson</name>
    </author>
    <author>
      <name>Han Gong</name>
    </author>
    <author>
      <name>Robert B. Fisher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by Progress in Colour Studies 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04250v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04250v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04785v1</id>
    <updated>2016-05-16T14:42:34Z</updated>
    <published>2016-05-16T14:42:34Z</published>
    <title>An Alternative Matting Laplacian</title>
    <summary>  Cutting out and object and estimate its transparency mask is a key task in
many applications. We take on the work on closed-form matting by Levin et al.,
that is used at the core of many matting techniques, and propose an alternative
formulation that offers more flexible controls over the matting priors. We also
show that this new approach is efficient at upscaling transparency maps from
coarse estimates.
</summary>
    <author>
      <name>François Pitié</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICIP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03473v1</id>
    <updated>2016-06-10T20:34:39Z</updated>
    <published>2016-06-10T20:34:39Z</published>
    <title>Face Detection with the Faster R-CNN</title>
    <summary>  The Faster R-CNN has recently demonstrated impressive results on various
object detection benchmarks. By training a Faster R-CNN model on the large
scale WIDER face dataset, we report state-of-the-art results on two widely used
face detection benchmarks, FDDB and the recently released IJB-A.
</summary>
    <author>
      <name>Huaizu Jiang</name>
    </author>
    <author>
      <name>Erik Learned-Miller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03473v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03473v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08315v1</id>
    <updated>2016-06-27T15:23:04Z</updated>
    <published>2016-06-27T15:23:04Z</published>
    <title>Depth Estimation from Single Image using Sparse Representations</title>
    <summary>  Monocular depth estimation is an interesting and challenging problem as there
is no analytic mapping known between an intensity image and its depth map.
Recently there has been a lot of data accumulated through depth-sensing
cameras, in parallel to that researchers started to tackle this task using
various learning algorithms. In this paper, a deep sparse coding method is
proposed for monocular depth estimation along with an approach for
deterministic dictionary initialization.
</summary>
    <author>
      <name>Yigit Oktar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.1.5059.0323</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.1.5059.0323" rel="related"/>
    <link href="http://arxiv.org/abs/1606.08315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03617v1</id>
    <updated>2016-08-11T20:53:48Z</updated>
    <published>2016-08-11T20:53:48Z</published>
    <title>Automatic detection of moving objects in video surveillance</title>
    <summary>  This work is in the field of video surveillance including motion detection.
The video surveillance is one of essential techniques for automatic video
analysis to extract crucial information or relevant scenes in video
surveillance systems. The aim of our work is to propose solutions for the
automatic detection of moving objects in real time with a surveillance camera.
The detected objects are objects that have some geometric shape (circle,
ellipse, square, and rectangle).
</summary>
    <author>
      <name>Larbi Guezouli</name>
    </author>
    <author>
      <name>Hanane Belhani</name>
    </author>
    <link href="http://arxiv.org/abs/1608.03617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03832v1</id>
    <updated>2016-08-12T15:55:12Z</updated>
    <published>2016-08-12T15:55:12Z</published>
    <title>On Minimal Accuracy Algorithm Selection in Computer Vision and
  Intelligent Systems</title>
    <summary>  In this paper we discuss certain theoretical properties of algorithm
selection approach to image processing and to intelligent system in general. We
analyze the theoretical limits of algorithm selection with respect to the
algorithm selection accuracy. We show the theoretical formulation of a crisp
bound on the algorithm selector precision guaranteeing to always obtain better
than the best available algorithm result.
</summary>
    <author>
      <name>Martin Lukac</name>
    </author>
    <author>
      <name>Kamila Abdiyeva</name>
    </author>
    <author>
      <name>Michitaka Kameyama</name>
    </author>
    <link href="http://arxiv.org/abs/1608.03832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02271v2</id>
    <updated>2016-09-09T18:37:03Z</updated>
    <published>2016-09-08T04:49:31Z</published>
    <title>Ashwin: Plug-and-Play System for Machine-Human Image Annotation</title>
    <summary>  We present an end-to-end machine-human image annotation system where each
component can be attached in a plug-and-play fashion. These components include
Feature Extraction, Machine Classifier, Task Sampling and Crowd Consensus.
</summary>
    <author>
      <name>Anand Sriraman</name>
    </author>
    <author>
      <name>Mandar Kulkarni</name>
    </author>
    <author>
      <name>Rahul Kumar</name>
    </author>
    <author>
      <name>Kanika Kalra</name>
    </author>
    <author>
      <name>Purushotam Radadia</name>
    </author>
    <author>
      <name>Shirish Karande</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">HCOMP 2016 Demonstrations Track</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.02271v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02271v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03529v1</id>
    <updated>2016-09-12T19:00:24Z</updated>
    <published>2016-09-12T19:00:24Z</published>
    <title>Examining Representational Similarity in ConvNets and the Primate Visual
  Cortex</title>
    <summary>  We compare several ConvNets with different depth and regularization
techniques with multi-unit macaque IT cortex recordings and assess the impact
of the same on representational similarity with the primate visual cortex. We
find that with increasing depth and validation performance, ConvNet features
are closer to cortical IT representations.
</summary>
    <author>
      <name>Abhimanyu Dubey</name>
    </author>
    <author>
      <name> Jayadeva</name>
    </author>
    <author>
      <name>Sumeet Agarwal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, short abstract, Accepted to the Workshop on Biological and
  Artificial Vision, ECCV, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.05001v1</id>
    <updated>2016-09-16T11:20:07Z</updated>
    <published>2016-09-16T11:20:07Z</published>
    <title>Stamp processing with examplar features</title>
    <summary>  Document digitization is becoming increasingly crucial. In this work, we
propose a shape based approach for automatic stamp verification/detection in
document images using an unsupervised feature learning. Given a small set of
training images, our algorithm learns an appropriate shape representation using
an unsupervised clustering. Experimental results demonstrate the effectiveness
of our framework in challenging scenarios.
</summary>
    <author>
      <name>Yash Bhalgat</name>
    </author>
    <author>
      <name>Mandar Kulkarni</name>
    </author>
    <author>
      <name>Shirish Karande</name>
    </author>
    <author>
      <name>Sachin Lodha</name>
    </author>
    <link href="http://arxiv.org/abs/1609.05001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.05001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07597v1</id>
    <updated>2016-09-24T10:32:30Z</updated>
    <published>2016-09-24T10:32:30Z</published>
    <title>DimensionApp : android app to estimate object dimensions</title>
    <summary>  In this project, we develop an android app that uses on computer vision
techniques to estimate an object dimension present in field of view. The app
while having compact size, is accurate upto +/- 5 mm and robust towards touch
inputs. We use single-view metrology to compute accurate measurement. Unlike
previous approaches, our technique does not rely on line detection and can be
generalize to any object shape easily.
</summary>
    <author>
      <name>Suriya Singh</name>
    </author>
    <author>
      <name>Vijay Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Report 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.07597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04575v1</id>
    <updated>2016-04-19T03:16:14Z</updated>
    <published>2016-04-19T03:16:14Z</published>
    <title>Comparing Face Detection and Recognition Techniques</title>
    <summary>  This paper implements and compares different techniques for face detection
and recognition. One is find where the face is located in the images that is
face detection and second is face recognition that is identifying the person.
We study three techniques in this paper: Face detection using self organizing
map (SOM), Face recognition by projection and nearest neighbor and Face
recognition using SVM.
</summary>
    <author>
      <name>Jyothi Korra</name>
    </author>
    <link href="http://arxiv.org/abs/1610.04575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.09520v1</id>
    <updated>2016-10-29T14:54:28Z</updated>
    <published>2016-10-29T14:54:28Z</published>
    <title>Multi-Camera Occlusion and Sudden-Appearance-Change Detection Using
  Hidden Markovian Chains</title>
    <summary>  This paper was originally submitted to Xinova as a response to a Request for
Invention (RFI) on new event monitoring methods. In this paper, a new object
tracking algorithm using multiple cameras for surveillance applications is
proposed. The proposed system can detect sudden-appearance-changes and
occlusions using a hidden Markovian statistical model. The experimental results
confirm that our system detect the sudden-appearance changes and occlusions
reliably.
</summary>
    <author>
      <name>Xudong Ma</name>
    </author>
    <link href="http://arxiv.org/abs/1610.09520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03873v1</id>
    <updated>2016-11-11T21:00:58Z</updated>
    <published>2016-11-11T21:00:58Z</published>
    <title>Effective sparse representation of X-Ray medical images</title>
    <summary>  Effective sparse representation of X-Ray medical images within the context of
data reduction is considered. The proposed framework is shown to render an
enormous reduction in the cardinality of the data set required to represent
this class of images at very good quality. The particularity of the approach is
that it can be implemented at very competitive processing time and low memory
requirements
</summary>
    <author>
      <name>Laura Rebollo-Neira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Routines for implementing the approach are available on
  http://www.nonlinear-approx.info/examples/node06.html</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.03873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.01035v1</id>
    <updated>2017-01-04T14:53:07Z</updated>
    <published>2017-01-04T14:53:07Z</published>
    <title>Path-following based Point Matching using Similarity Transformation</title>
    <summary>  To address the problem of 3D point matching where the poses of two point sets
are unknown, we adapt a recently proposed path following based method to use
similarity transformation instead of the original affine transformation. The
reduced number of transformation parameters leads to more constrained and
desirable matching results. Experimental results demonstrate better robustness
of the proposed method over state-of-the-art methods.
</summary>
    <author>
      <name>Wei Lian</name>
    </author>
    <link href="http://arxiv.org/abs/1701.01035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.01035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.01885v1</id>
    <updated>2017-01-07T21:12:24Z</updated>
    <published>2017-01-07T21:12:24Z</published>
    <title>Group Visual Sentiment Analysis</title>
    <summary>  In this paper, we introduce a framework for classifying images according to
high-level sentiment. We subdivide the task into three primary problems:
emotion classification on faces, human pose estimation, and 3D estimation and
clustering of groups of people. We introduce novel algorithms for matching body
parts to a common individual and clustering people in images based on physical
location and orientation. Our results outperform several baseline approaches.
</summary>
    <author>
      <name>Zeshan Hussain</name>
    </author>
    <author>
      <name>Tariq Patanam</name>
    </author>
    <author>
      <name>Hardie Cate</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.01885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.01885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.07354v1</id>
    <updated>2017-01-25T15:35:09Z</updated>
    <published>2017-01-25T15:35:09Z</published>
    <title>Photographic dataset: playing cards</title>
    <summary>  This is a photographic dataset collected for testing image processing
algorithms. The idea is to have images that can exploit the properties of total
variation, therefore a set of playing cards was distributed on the scene. The
dataset is made available at www.fips.fi/photographic_dataset2.php
</summary>
    <author>
      <name>David Villacis</name>
    </author>
    <author>
      <name>Santeri Kaupinmäki</name>
    </author>
    <author>
      <name>Samuli Siltanen</name>
    </author>
    <author>
      <name>Teemu Helenius</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.07354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.07354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00723v1</id>
    <updated>2017-02-01T18:32:12Z</updated>
    <published>2017-02-01T18:32:12Z</published>
    <title>Handwritten Recognition Using SVM, KNN and Neural Network</title>
    <summary>  Handwritten recognition (HWR) is the ability of a computer to receive and
interpret intelligible handwritten input from source such as paper documents,
photographs, touch-screens and other devices. In this paper we will using three
(3) classification t o re cognize the handwritten which is SVM, KNN and Neural
Network.
</summary>
    <author>
      <name>Norhidayu Abdul Hamid</name>
    </author>
    <author>
      <name>Nilam Nur Amir Sjarif</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages ; 22 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.00723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07006v1</id>
    <updated>2017-02-22T21:03:49Z</updated>
    <published>2017-02-22T21:03:49Z</published>
    <title>Synthesising Dynamic Textures using Convolutional Neural Networks</title>
    <summary>  Here we present a parametric model for dynamic textures. The model is based
on spatiotemporal summary statistics computed from the feature representations
of a Convolutional Neural Network (CNN) trained on object recognition. We
demonstrate how the model can be used to synthesise new samples of dynamic
textures and to predict motion in simple movies.
</summary>
    <author>
      <name>Christina M. Funke</name>
    </author>
    <author>
      <name>Leon A. Gatys</name>
    </author>
    <author>
      <name>Alexander S. Ecker</name>
    </author>
    <author>
      <name>Matthias Bethge</name>
    </author>
    <link href="http://arxiv.org/abs/1702.07006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07963v1</id>
    <updated>2017-02-26T00:56:25Z</updated>
    <published>2017-02-26T00:56:25Z</published>
    <title>Spatially Aware Melanoma Segmentation Using Hybrid Deep Learning
  Techniques</title>
    <summary>  In this paper, we proposed using a hybrid method that utilises deep
convolutional and recurrent neural networks for accurate delineation of skin
lesion of images supplied with ISBI 2017 lesion segmentation challenge. The
proposed method was trained using 1800 images and tested on 150 images from
ISBI 2017 challenge.
</summary>
    <author>
      <name>M. Attia</name>
    </author>
    <author>
      <name>M. Hossny</name>
    </author>
    <author>
      <name>S. Nahavandi</name>
    </author>
    <author>
      <name>A. Yazdabadi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISIC2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.07963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01402v1</id>
    <updated>2017-03-04T06:32:15Z</updated>
    <published>2017-03-04T06:32:15Z</published>
    <title>Skin Lesion Classification Using Deep Multi-scale Convolutional Neural
  Networks</title>
    <summary>  We present a deep learning approach to the ISIC 2017 Skin Lesion
Classification Challenge using a multi-scale convolutional neural network. Our
approach utilizes an Inception-v3 network pre-trained on the ImageNet dataset,
which is fine-tuned for skin lesion classification using two different scales
of input images.
</summary>
    <author>
      <name>Terrance DeVries</name>
    </author>
    <author>
      <name>Dhanesh Ramachandram</name>
    </author>
    <link href="http://arxiv.org/abs/1703.01402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.03108v1</id>
    <updated>2017-03-09T02:35:59Z</updated>
    <published>2017-03-09T02:35:59Z</published>
    <title>Image Classification of Melanoma, Nevus and Seborrheic Keratosis by Deep
  Neural Network Ensemble</title>
    <summary>  This short paper reports the method and the evaluation results of Casio and
Shinshu University joint team for the ISBI Challenge 2017 - Skin Lesion
Analysis Towards Melanoma Detection - Part 3: Lesion Classification hosted by
ISIC. Our online validation score was 0.958 with melanoma classifier AUC 0.924
and seborrheic keratosis classifier AUC 0.993.
</summary>
    <author>
      <name>Kazuhisa Matsunaga</name>
    </author>
    <author>
      <name>Akira Hamada</name>
    </author>
    <author>
      <name>Akane Minagawa</name>
    </author>
    <author>
      <name>Hiroshi Koga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages. 3 figures. ISIC2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.03108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.03108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.03186v1</id>
    <updated>2017-03-09T09:14:40Z</updated>
    <published>2017-03-09T09:14:40Z</published>
    <title>Segmenting Dermoscopic Images</title>
    <summary>  We propose an automatic algorithm, named SDI, for the segmentation of skin
lesions in dermoscopic images, articulated into three main steps: selection of
the image ROI, selection of the segmentation band, and segmentation. We present
extensive experimental results achieved by the SDI algorithm on the lesion
segmentation dataset made available for the ISIC 2017 challenge on Skin Lesion
Analysis Towards Melanoma Detection, highlighting its advantages and
disadvantages.
</summary>
    <author>
      <name>Mario Rosario Guarracino</name>
    </author>
    <author>
      <name>Lucia Maddalena</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.03186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.03186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.03888v1</id>
    <updated>2017-03-11T01:18:14Z</updated>
    <published>2017-03-11T01:18:14Z</published>
    <title>Segmentation of skin lesions based on fuzzy classification of pixels and
  histogram thresholding</title>
    <summary>  This paper proposes an innovative method for segmentation of skin lesions in
dermoscopy images developed by the authors, based on fuzzy classification of
pixels and histogram thresholding.
</summary>
    <author>
      <name>Jose Luis Garcia-Arroyo</name>
    </author>
    <author>
      <name>Begonya Garcia-Zapirain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.03888v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.03888v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08366v1</id>
    <updated>2017-03-24T11:39:26Z</updated>
    <published>2017-03-24T11:39:26Z</published>
    <title>A Hybrid Deep Learning Approach for Texture Analysis</title>
    <summary>  Texture classification is a problem that has various applications such as
remote sensing and forest species recognition. Solutions tend to be custom fit
to the dataset used but fails to generalize. The Convolutional Neural Network
(CNN) in combination with Support Vector Machine (SVM) form a robust selection
between powerful invariant feature extractor and accurate classifier. The
fusion of experts provides stability in classification rates among different
datasets.
</summary>
    <author>
      <name>Hussein Adly</name>
    </author>
    <author>
      <name>Mohamed Moustafa</name>
    </author>
    <link href="http://arxiv.org/abs/1703.08366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02956v1</id>
    <updated>2017-04-10T17:13:00Z</updated>
    <published>2017-04-10T17:13:00Z</published>
    <title>Surface Normals in the Wild</title>
    <summary>  We study the problem of single-image depth estimation for images in the wild.
We collect human annotated surface normals and use them to train a neural
network that directly predicts pixel-wise depth. We propose two novel loss
functions for training with surface normal annotations. Experiments on NYU
Depth and our own dataset demonstrate that our approach can significantly
improve the quality of depth estimation in the wild.
</summary>
    <author>
      <name>Weifeng Chen</name>
    </author>
    <author>
      <name>Donglai Xiang</name>
    </author>
    <author>
      <name>Jia Deng</name>
    </author>
    <link href="http://arxiv.org/abs/1704.02956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03966v1</id>
    <updated>2017-04-13T01:41:30Z</updated>
    <published>2017-04-13T01:41:30Z</published>
    <title>Collaborative Low-Rank Subspace Clustering</title>
    <summary>  In this paper we present Collaborative Low-Rank Subspace Clustering. Given
multiple observations of a phenomenon we learn a unified representation matrix.
This unified matrix incorporates the features from all the observations, thus
increasing the discriminative power compared with learning the representation
matrix on each observation separately. Experimental evaluation shows that our
method outperforms subspace clustering on separate observations and the state
of the art collaborative learning algorithm.
</summary>
    <author>
      <name>Stephen Tierney</name>
    </author>
    <author>
      <name>Yi Guo</name>
    </author>
    <author>
      <name>Junbin Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1704.03966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01148v1</id>
    <updated>2017-05-02T19:21:51Z</updated>
    <published>2017-05-02T19:21:51Z</published>
    <title>Recovery of structure of looped jointed objects from multiframes</title>
    <summary>  A method to recover structural parameters of looped jointed objects from
multiframes is being developed. Each rigid part of the jointed body needs only
to be traced at two (that is at junction) points.
  This method has been linearized for 4-part loops, with recovery from at least
19 frames.
</summary>
    <author>
      <name>Mieczysław Kłopotek</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">a preliminary version for Machine Graphics and Vision, Vol. 3 No.
  4, pp. 645-656, 1995</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1705.01148v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01148v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01809v1</id>
    <updated>2017-05-04T12:20:56Z</updated>
    <published>2017-05-04T12:20:56Z</published>
    <title>Pixel Normalization from Numeric Data as Input to Neural Networks</title>
    <summary>  Text to image transformation for input to neural networks requires
intermediate steps. This paper attempts to present a new approach to pixel
normalization so as to convert textual data into image, suitable as input for
neural networks. This method can be further improved by its Graphics Processing
Unit (GPU) implementation to provide significant speedup in computational time.
</summary>
    <author>
      <name>Parth Sane</name>
    </author>
    <author>
      <name>Ravindra Agrawal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE WiSPNET 2017 conference in Chennai</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04272v1</id>
    <updated>2017-04-29T11:14:16Z</updated>
    <published>2017-04-29T11:14:16Z</published>
    <title>Improved underwater image enhancement algorithms based on partial
  differential equations (PDEs)</title>
    <summary>  The experimental results of improved underwater image enhancement algorithms
based on partial differential equations (PDEs) are presented in this report.
This second work extends the study of previous work and incorporating several
improvements into the revised algorithm. Experiments show the evidence of the
improvements when compared to previously proposed approaches and other
conventional algorithms found in the literature.
</summary>
    <author>
      <name>U. A. Nnolim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00083v1</id>
    <updated>2017-05-31T20:44:55Z</updated>
    <published>2017-05-31T20:44:55Z</published>
    <title>Blood capillaries and vessels segmentation in optical coherence
  tomography angiogram using fuzzy C-means and Curvelet transform</title>
    <summary>  This paper has been removed from arXiv as the submitter did not have
ownership of the data presented in this work.
</summary>
    <author>
      <name>Fariborz Taherkhani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: This paper has been removed from arXiv as the
  submitter did not have ownership of the data presented in this work</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.00083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03497v1</id>
    <updated>2017-06-12T08:04:42Z</updated>
    <published>2017-06-12T08:04:42Z</published>
    <title>A filter based approach for inbetweening</title>
    <summary>  We present a filter based approach for inbetweening. We train a convolutional
neural network to generate intermediate frames. This network aim to generate
smooth animation of line drawings. Our method can process scanned images
directly. Our method does not need to compute correspondence of lines and
topological changes explicitly. We experiment our method with real animation
production data. The results show that our method can generate intermediate
frames partially.
</summary>
    <author>
      <name>Yuichi Yagi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, in Japanese</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.03497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05534v1</id>
    <updated>2017-06-17T13:33:29Z</updated>
    <published>2017-06-17T13:33:29Z</published>
    <title>Rotation Invariance Neural Network</title>
    <summary>  Rotation invariance and translation invariance have great values in image
recognition tasks. In this paper, we bring a new architecture in convolutional
neural network (CNN) named cyclic convolutional layer to achieve rotation
invariance in 2-D symbol recognition. We can also get the position and
orientation of the 2-D symbol by the network to achieve detection purpose for
multiple non-overlap target. Last but not least, this architecture can achieve
one-shot learning in some cases using those invariance.
</summary>
    <author>
      <name>Shiyuan Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.05534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06230v1</id>
    <updated>2017-06-20T00:43:22Z</updated>
    <published>2017-06-20T00:43:22Z</published>
    <title>A Bayesian algorithm for detecting identity matches and fraud in image
  databases</title>
    <summary>  A statistical algorithm for categorizing different types of matches and fraud
in image databases is presented. The approach is based on a generative model of
a graph representing images and connections between pairs of identities,
trained using properties of a matching algorithm between images.
</summary>
    <author>
      <name>Gaurav Thakur</name>
    </author>
    <link href="http://arxiv.org/abs/1706.06230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07757v1</id>
    <updated>2017-04-14T14:54:14Z</updated>
    <published>2017-04-14T14:54:14Z</published>
    <title>Improved Human Emotion Recognition Using Symmetry of Facial Key Points
  with Dihedral Group</title>
    <summary>  This article describes how to deploy dihedral group theory to detect Facial
Key Points (FKP) symmetry to recognize emotions. The method can be applied in
many other areas which those have the same data texture.
</summary>
    <author>
      <name>Mehdi Ghayoumi</name>
    </author>
    <author>
      <name>Arvind Bansal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJASCSE Volume 6 Issue 01 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.07757v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07757v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01159v1</id>
    <updated>2017-07-04T21:34:08Z</updated>
    <published>2017-07-04T21:34:08Z</published>
    <title>UPSET and ANGRI : Breaking High Performance Image Classifiers</title>
    <summary>  In this paper, targeted fooling of high performance image classifiers is
achieved by developing two novel attack methods. The first method generates
universal perturbations for target classes and the second generates image
specific perturbations. Extensive experiments are conducted on MNIST and
CIFAR10 datasets to provide insights about the proposed algorithms and show
their effectiveness.
</summary>
    <author>
      <name>Sayantan Sarkar</name>
    </author>
    <author>
      <name>Ankan Bansal</name>
    </author>
    <author>
      <name>Upal Mahbub</name>
    </author>
    <author>
      <name>Rama Chellappa</name>
    </author>
    <link href="http://arxiv.org/abs/1707.01159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02051v1</id>
    <updated>2017-07-07T06:27:54Z</updated>
    <published>2017-07-07T06:27:54Z</published>
    <title>Image Segmentation Algorithms Overview</title>
    <summary>  The technology of image segmentation is widely used in medical image
processing, face recognition pedestrian detection, etc. The current image
segmentation techniques include region-based segmentation, edge detection
segmentation, segmentation based on clustering, segmentation based on
weakly-supervised learning in CNN, etc. This paper analyzes and summarizes
these algorithms of image segmentation, and compares the advantages and
disadvantages of different algorithms. Finally, we make a prediction of the
development trend of image segmentation with the combination of these
algorithms.
</summary>
    <author>
      <name>Song Yuheng</name>
    </author>
    <author>
      <name>Yan Hao</name>
    </author>
    <link href="http://arxiv.org/abs/1707.02051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06825v1</id>
    <updated>2017-07-21T10:17:33Z</updated>
    <published>2017-07-21T10:17:33Z</published>
    <title>Evaluation of Hashing Methods Performance on Binary Feature Descriptors</title>
    <summary>  In this paper we evaluate performance of data-dependent hashing methods on
binary data. The goal is to find a hashing method that can effectively produce
lower dimensional binary representation of 512-bit FREAK descriptors. A
representative sample of recent unsupervised, semi-supervised and supervised
hashing methods was experimentally evaluated on large datasets of labelled
binary FREAK feature descriptors.
</summary>
    <author>
      <name>Jacek Komorowski</name>
    </author>
    <author>
      <name>Tomasz Trzcinski</name>
    </author>
    <link href="http://arxiv.org/abs/1707.06825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.08722v1</id>
    <updated>2017-07-27T07:00:02Z</updated>
    <published>2017-07-27T07:00:02Z</published>
    <title>Algebraic Relations and Triangulation of Unlabeled Image Points</title>
    <summary>  In multiview geometry when correspondences among multiple views are unknown
the image points can be understood as being unlabeled. This is a common problem
in computer vision. We give a novel approach to handle such a situation by
regarding unlabeled point configurations as points on the Chow variety
$\text{Sym}_m(\mathbb{P}^2)$. For two unlabeled points we design an algorithm
that solves the triangulation problem with unknown correspondences. Further the
unlabeled multiview variety $\text{Sym}_m(V_A)$ is studied.
</summary>
    <author>
      <name>André Wagner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.08722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.08722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09869v1</id>
    <updated>2017-07-23T01:35:55Z</updated>
    <published>2017-07-23T01:35:55Z</published>
    <title>A comment on the paper Prediction of Kidney Function from Biopsy Images
  using Convolutional Neural Networks</title>
    <summary>  This letter presente a comment on the paper Prediction of Kidney Function
from Biopsy Images using Convolutional Neural Networks by Ledbetter et al.
(2017)
</summary>
    <author>
      <name>Washington LC dos-Santos</name>
    </author>
    <author>
      <name>Angelo A Duarte</name>
    </author>
    <author>
      <name>Luiz AR de Freitas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05867v1</id>
    <updated>2017-09-18T11:28:15Z</updated>
    <published>2017-09-18T11:28:15Z</published>
    <title>Combinational neural network using Gabor filters for the classification
  of handwritten digits</title>
    <summary>  A classification algorithm that combines the components of k-nearest
neighbours and multilayer neural networks has been designed and tested. With
this method the computational time required for training the dataset has been
reduced substancially. Gabor filters were used for the feature extraction to
ensure a better performance. This algorithm is tested with MNIST dataset and it
will be integrated as a module in the object recognition software which is
currently under development.
</summary>
    <author>
      <name>N. Joshi</name>
    </author>
    <link href="http://arxiv.org/abs/1709.05867v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05867v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01462v1</id>
    <updated>2017-10-04T05:20:41Z</updated>
    <published>2017-10-04T05:20:41Z</published>
    <title>Secrets in Computing Optical Flow by Convolutional Networks</title>
    <summary>  Convolutional neural networks (CNNs) have been widely used over many areas in
compute vision. Especially in classification. Recently, FlowNet and several
works on opti- cal estimation using CNNs shows the potential ability of CNNs in
doing per-pixel regression. We proposed several CNNs network architectures that
can estimate optical flow, and fully unveiled the intrinsic different between
these structures.
</summary>
    <author>
      <name>Junxuan Li</name>
    </author>
    <link href="http://arxiv.org/abs/1710.01462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08011v1</id>
    <updated>2017-10-22T20:48:49Z</updated>
    <published>2017-10-22T20:48:49Z</published>
    <title>ActivityNet Challenge 2017 Summary</title>
    <summary>  The ActivityNet Large Scale Activity Recognition Challenge 2017 Summary:
results and challenge participants papers.
</summary>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <author>
      <name>Juan Carlos Niebles</name>
    </author>
    <author>
      <name>Cees Snoek</name>
    </author>
    <author>
      <name>Fabian Caba Heilbron</name>
    </author>
    <author>
      <name>Humam Alwassel</name>
    </author>
    <author>
      <name>Ranjay Khrisna</name>
    </author>
    <author>
      <name>Victor Escorcia</name>
    </author>
    <author>
      <name>Kenji Hata</name>
    </author>
    <author>
      <name>Shyamal Buch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">76 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.08011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07286v4</id>
    <updated>2018-11-19T02:16:17Z</updated>
    <published>2017-12-20T01:38:53Z</published>
    <title>LVreID: Person Re-Identification with Long Sequence Videos</title>
    <summary>  This paper mainly establishes a large-scale Long sequence Video database for
person re-IDentification (LVreID).
</summary>
    <author>
      <name>Jianing Li</name>
    </author>
    <author>
      <name>Shiliang Zhang</name>
    </author>
    <author>
      <name>Jingdong Wang</name>
    </author>
    <author>
      <name>Wen Gao</name>
    </author>
    <author>
      <name>Qi Tian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">There is experimental error in secction 5.7</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.07286v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07286v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06104v2</id>
    <updated>2018-05-09T12:18:09Z</updated>
    <published>2018-01-18T15:53:00Z</published>
    <title>Invariants of multidimensional time series based on their
  iterated-integral signature</title>
    <summary>  We introduce a novel class of features for multidimensional time series, that
are invariant with respect to transformations of the ambient space. The general
linear group, the group of rotations and the group of permutations of the axes
are considered. The starting point for their construction is Chen's
iterated-integral signature.
</summary>
    <author>
      <name>Joscha Diehl</name>
    </author>
    <author>
      <name>Jeremy Reizenstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">complete rewrite of Section 3.3</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.06104v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06104v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.RT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06504v2</id>
    <updated>2018-01-24T16:04:15Z</updated>
    <published>2018-01-19T17:41:12Z</published>
    <title>Detecting and counting tiny faces</title>
    <summary>  Finding Tiny Faces (by Hu and Ramanan) proposes a novel approach to find
small objects in an image. Our contribution consists in deeply understanding
the choices of the paper together with applying and extending a similar method
to a real world subject which is the counting of people in a public
demonstration.
</summary>
    <author>
      <name>Alexandre Attia</name>
    </author>
    <author>
      <name>Sharone Dayan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 10 figures, 2 appendix page</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.06504v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06504v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06694v1</id>
    <updated>2018-01-20T16:08:12Z</updated>
    <published>2018-01-20T16:08:12Z</published>
    <title>Determination of Digital Straight Segments Using the Slope</title>
    <summary>  We present a new method for the recognition of digital straight lines based
on the slope. This method combines the Freeman's chain coding scheme and new
discovered properties of the digital slope introduced in this paper. We also
present the efficiency of our method from a testbed.
</summary>
    <author>
      <name>Alejandro Cartas</name>
    </author>
    <author>
      <name>María Elena Algorri</name>
    </author>
    <link href="http://arxiv.org/abs/1801.06694v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06694v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.05779v1</id>
    <updated>2018-01-19T06:30:59Z</updated>
    <published>2018-01-19T06:30:59Z</published>
    <title>A predictor-corrector method for the training of deep neural networks</title>
    <summary>  The training of deep neural nets is expensive. We present a predictor-
corrector method for the training of deep neural nets. It alternates a
predictor pass with a corrector pass using stochastic gradient descent with
backpropagation such that there is no loss in validation accuracy. No special
modifications to SGD with backpropagation is required by this methodology. Our
experiments showed a time improvement of 9% on the CIFAR-10 dataset.
</summary>
    <author>
      <name>Yatin Saraiya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.05779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.05779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.05785v1</id>
    <updated>2018-03-15T14:48:47Z</updated>
    <published>2018-03-15T14:48:47Z</published>
    <title>Aggregated Sparse Attention for Steering Angle Prediction</title>
    <summary>  In this paper, we apply the attention mechanism to autonomous driving for
steering angle prediction. We propose the first model, applying the recently
introduced sparse attention mechanism to visual domain, as well as the
aggregated extension for this model. We show the improvement of the proposed
method, comparing to no attention as well as to different types of attention.
</summary>
    <author>
      <name>Sen He</name>
    </author>
    <author>
      <name>Dmitry Kangin</name>
    </author>
    <author>
      <name>Yang Mi</name>
    </author>
    <author>
      <name>Nicolas Pugeault</name>
    </author>
    <link href="http://arxiv.org/abs/1803.05785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.05785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.07608v1</id>
    <updated>2018-03-20T19:12:05Z</updated>
    <published>2018-03-20T19:12:05Z</published>
    <title>A Survey of Deep Learning Techniques for Mobile Robot Applications</title>
    <summary>  Advancements in deep learning over the years have attracted research into how
deep artificial neural networks can be used in robotic systems. This research
survey will present a summarization of the current research with a specific
focus on the gains and obstacles for deep learning to be applied to mobile
robotics.
</summary>
    <author>
      <name>Jahanzaib Shabbir</name>
    </author>
    <author>
      <name>Tarique Anwer</name>
    </author>
    <link href="http://arxiv.org/abs/1803.07608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02543v1</id>
    <updated>2018-04-07T10:24:04Z</updated>
    <published>2018-04-07T10:24:04Z</published>
    <title>Not quite unreasonable effectiveness of machine learning algorithms</title>
    <summary>  State-of-the-art machine learning algorithms demonstrate close to absolute
performance in selected challenges. We provide arguments that the reason can be
in low variability of the samples and high effectiveness in learning typical
patterns. Due to this fact, standard performance metrics do not reveal model
capacity and new metrics are required for the better understanding of
state-of-the-art.
</summary>
    <author>
      <name>Egor Illarionov</name>
    </author>
    <author>
      <name>Roman Khudorozhkov</name>
    </author>
    <link href="http://arxiv.org/abs/1804.02543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03286v1</id>
    <updated>2018-04-10T04:54:29Z</updated>
    <published>2018-04-10T04:54:29Z</published>
    <title>On the Robustness of the CVPR 2018 White-Box Adversarial Example
  Defenses</title>
    <summary>  Neural networks are known to be vulnerable to adversarial examples. In this
note, we evaluate the two white-box defenses that appeared at CVPR 2018 and
find they are ineffective: when applying existing techniques, we can reduce the
accuracy of the defended models to 0%.
</summary>
    <author>
      <name>Anish Athalye</name>
    </author>
    <author>
      <name>Nicholas Carlini</name>
    </author>
    <link href="http://arxiv.org/abs/1804.03286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05233v1</id>
    <updated>2018-06-13T19:23:51Z</updated>
    <published>2018-06-13T19:23:51Z</published>
    <title>End-to-End Parkinson Disease Diagnosis using Brain MR-Images by 3D-CNN</title>
    <summary>  In this work, we use a deep learning framework for simultaneous
classification and regression of Parkinson disease diagnosis based on MR-Images
and personal information (i.e. age, gender). We intend to facilitate and
increase the confidence in Parkinson disease diagnosis through our deep
learning framework.
</summary>
    <author>
      <name>Soheil Esmaeilzadeh</name>
    </author>
    <author>
      <name>Yao Yang</name>
    </author>
    <author>
      <name>Ehsan Adeli</name>
    </author>
    <link href="http://arxiv.org/abs/1806.05233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00273v1</id>
    <updated>2018-07-01T05:28:27Z</updated>
    <published>2018-07-01T05:28:27Z</published>
    <title>Photorealistic Style Transfer for Videos</title>
    <summary>  Photorealistic style transfer is a technique which transfers colour from one
reference domain to another domain by using deep learning and optimization
techniques. Here, we present a technique which we use to transfer style and
colour from a reference image to a video.
</summary>
    <author>
      <name>Michael Honke</name>
    </author>
    <author>
      <name>Rahul Iyer</name>
    </author>
    <author>
      <name>Dishant Mittal</name>
    </author>
    <link href="http://arxiv.org/abs/1807.00273v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00273v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00686v1</id>
    <updated>2018-06-29T07:49:08Z</updated>
    <published>2018-06-29T07:49:08Z</published>
    <title>YH Technologies at ActivityNet Challenge 2018</title>
    <summary>  This notebook paper presents an overview and comparative analysis of our
systems designed for the following five tasks in ActivityNet Challenge 2018:
temporal action proposals, temporal action localization, dense-captioning
events in videos, trimmed action recognition, and spatio-temporal action
localization.
</summary>
    <author>
      <name>Ting Yao</name>
    </author>
    <author>
      <name>Xue Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Rank 2 in both Temporal Activity Detection Task &amp; Kinetics Task @
  ActivityNet 2018. arXiv admin note: substantial text overlap with
  arXiv:1710.08011 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06644v1</id>
    <updated>2018-07-17T20:03:19Z</updated>
    <published>2018-07-17T20:03:19Z</published>
    <title>A Framework for Moment Invariants</title>
    <summary>  For more than half a century, moments have attracted lot ot interest in the
pattern recognition community.The moments of a distribution (an object) provide
several of its characteristics as center of gravity, orientation, disparity,
volume. Moments can be used to define invariant characteristics to some
transformations that an object can undergo, commonly called moment invariants.
This work provides a simple and systematic formalism to compute geometric
moment invariants in n-dimensional space.
</summary>
    <author>
      <name>Omar Tahri</name>
    </author>
    <link href="http://arxiv.org/abs/1807.06644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08332v1</id>
    <updated>2018-07-22T18:07:50Z</updated>
    <published>2018-07-22T18:07:50Z</published>
    <title>Skin Lesion Analysis Towards Melanoma Detection via End-to-end Deep
  Learning of Convolutional Neural Networks</title>
    <summary>  This article presents the design, experiments and results of our solution
submitted to the 2018 ISIC challenge: Skin Lesion Analysis Towards Melanoma
Detection. We design a pipeline using state-of-the-art Convolutional Neural
Network (CNN) models for a Lesion Boundary Segmentation task and a Lesion
Diagnosis task.
</summary>
    <author>
      <name>Katherine M. Li</name>
    </author>
    <author>
      <name>Evelyn C. Li</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08471v2</id>
    <updated>2018-07-25T09:31:46Z</updated>
    <published>2018-07-23T08:14:36Z</published>
    <title>Deep attention-guided fusion network for lesion segmentation</title>
    <summary>  We participated the Task 1: Lesion Segmentation. The paper describes our
algorithm and the final result of validation set for the ISIC Challenge 2018 -
Skin Lesion Analysis Towards Melanoma Detection.
</summary>
    <author>
      <name>Hengliang Zhu</name>
    </author>
    <author>
      <name>Yangyang Hao</name>
    </author>
    <author>
      <name>Lizhuang Ma</name>
    </author>
    <author>
      <name>Ruixing Li</name>
    </author>
    <author>
      <name>Hua Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08471v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08471v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09083v1</id>
    <updated>2018-07-24T13:17:55Z</updated>
    <published>2018-07-24T13:17:55Z</published>
    <title>ISIC 2017 Skin Lesion Segmentation Using Deep Encoder-Decoder Network</title>
    <summary>  This paper summarizes our method and validation results for part 1 of the
ISBI Challenge 2018. Our algorithm makes use of deep encoder-decoder network
and novel skin lesion data augmentation to segment the challenge objective.
Besides, we also propose an effective testing strategy by applying multi-model
comparison.
</summary>
    <author>
      <name>Ngoc-Quang Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISIC 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09163v1</id>
    <updated>2018-07-24T14:48:57Z</updated>
    <published>2018-07-24T14:48:57Z</published>
    <title>Skin disease identification from dermoscopy images using deep
  convolutional neural network</title>
    <summary>  In this paper, a deep neural network based ensemble method is experimented
for automatic identification of skin disease from dermoscopic images. The
developed algorithm is applied on the task3 of the ISIC 2018 challenge dataset
(Skin Lesion Analysis Towards Melanoma Detection).
</summary>
    <author>
      <name>Anabik Pal</name>
    </author>
    <author>
      <name>Sounak Ray</name>
    </author>
    <author>
      <name>Utpal Garain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Challenge Participation in ISIC 2018: Skin Lesion Analysis Towards
  Melanoma Detection</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09312v1</id>
    <updated>2018-07-24T19:14:29Z</updated>
    <published>2018-07-24T19:14:29Z</published>
    <title>A Simple Probabilistic Model for Uncertainty Estimation</title>
    <summary>  The article focuses on determining the predictive uncertainty of a model on
the example of atrial fibrillation detection problem by a single-lead ECG
signal. To this end, the model predicts parameters of the beta distribution
over class probabilities instead of these probabilities themselves. It was
shown that the described approach allows to detect atypical recordings and
significantly improve the quality of the algorithm on confident predictions.
</summary>
    <author>
      <name>Alexander Kuvaev</name>
    </author>
    <author>
      <name>Roman Khudorozhkov</name>
    </author>
    <link href="http://arxiv.org/abs/1807.09312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02373v2</id>
    <updated>2018-08-14T01:59:29Z</updated>
    <published>2018-08-04T11:57:27Z</published>
    <title>Troy: Give Attention to Saliency and for Saliency</title>
    <summary>  In addition, our work has text overlap with arXiv:1804.06242,
arXiv:1705.00938 by other authors. We want to rewrite this paper for avoiding
this fact.
</summary>
    <author>
      <name>Pingping Zhang</name>
    </author>
    <author>
      <name>Huchuan Lu</name>
    </author>
    <author>
      <name>Chunhua Shen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">All of authors agree to withdrawal this paper because we have noticed
  several important errors</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.02373v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02373v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.00965v1</id>
    <updated>2018-10-01T20:37:07Z</updated>
    <published>2018-10-01T20:37:07Z</published>
    <title>Natural measures of alignment</title>
    <summary>  Natural coordinate system will be proposed. In this coordinate system
alignment procedure of a device and a detector can be easily performed. This
approach is generalization of previous specific formulas in the field of
calibration and provide top level description of the procedure. A basic example
application to linac therapy plan is also provided.
</summary>
    <author>
      <name>R. A. Kycia</name>
    </author>
    <author>
      <name>Z. Tabor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.00965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.00965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.06287v1</id>
    <updated>2018-11-15T10:44:35Z</updated>
    <published>2018-11-15T10:44:35Z</published>
    <title>Sketch based Reduced Memory Hough Transform</title>
    <summary>  This paper proposes using sketch algorithms to represent the votes in Hough
transforms. Replacing the accumulator array with a sketch (Sketch Hough
Transform - SHT) significantly reduces the memory needed to compute a Hough
transform. We also present a new sketch, Count Median Update, which works
better than known sketch methods for replacing the accumulator array in the
Hough Transform.
</summary>
    <author>
      <name>Levi Offen</name>
    </author>
    <author>
      <name>Michael Werman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2018 25th IEEE International Conference on Image Processing (ICIP)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.06287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.06287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.10355v1</id>
    <updated>2018-11-26T13:22:17Z</updated>
    <published>2018-11-26T13:22:17Z</published>
    <title>Unsupervised learning with sparse space-and-time autoencoders</title>
    <summary>  We use spatially-sparse two, three and four dimensional convolutional
autoencoder networks to model sparse structures in 2D space, 3D space, and
3+1=4 dimensional space-time. We evaluate the resulting latent spaces by
testing their usefulness for downstream tasks. Applications are to handwriting
recognition in 2D, segmentation for parts in 3D objects, segmentation for
objects in 3D scenes, and body-part segmentation for 4D wire-frame models
generated from motion capture data.
</summary>
    <author>
      <name>Benjamin Graham</name>
    </author>
    <link href="http://arxiv.org/abs/1811.10355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.10355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.11314v1</id>
    <updated>2018-11-27T23:45:58Z</updated>
    <published>2018-11-27T23:45:58Z</published>
    <title>Skin lesion segmentation using U-Net and good training strategies</title>
    <summary>  In this paper we approach the problem of skin lesion segmentation using a
convolutional neural network based on the U-Net architecture. We present a set
of training strategies that had a significant impact on the performance of this
model. We evaluated this method on the ISIC Challenge 2018 - Skin Lesion
Analysis Towards Melanoma Detection, obtaining threshold Jaccard index of
77.5%.
</summary>
    <author>
      <name>Fred Guth</name>
    </author>
    <author>
      <name>Teofilo E. deCampos</name>
    </author>
    <link href="http://arxiv.org/abs/1811.11314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.11314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T45" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.03706v5</id>
    <updated>2019-05-17T06:39:19Z</updated>
    <published>2019-01-13T12:42:42Z</published>
    <title>Generating Adversarial Perturbation with Root Mean Square Gradient</title>
    <summary>  We focus our attention on the problem of generating adversarial perturbations
based on the gradient in image classification domain
</summary>
    <author>
      <name>Yatie Xiao</name>
    </author>
    <author>
      <name>Chi-Man Pun</name>
    </author>
    <author>
      <name>Jizhe Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The formula in Algorithm 1 lacks important representations</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.03706v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.03706v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.05259v1</id>
    <updated>2019-01-16T12:31:08Z</updated>
    <published>2019-01-16T12:31:08Z</published>
    <title>MRI to CT Translation with GANs</title>
    <summary>  We present a detailed description and reference implementation of
preprocessing steps necessary to prepare the public Retrospective Image
Registration Evaluation (RIRE) dataset for the task of magnetic resonance
imaging (MRI) to X-ray computed tomography (CT) translation. Furthermore we
describe and implement three state of the art convolutional neural network
(CNN) and generative adversarial network (GAN) models where we report
statistics and visual results of two of them.
</summary>
    <author>
      <name>Bodo Kaiser</name>
    </author>
    <author>
      <name>Shadi Albarqouni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.05259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.05531v1</id>
    <updated>2019-01-16T21:27:57Z</updated>
    <published>2019-01-16T21:27:57Z</published>
    <title>Response to "Visual Dialogue without Vision or Dialogue" (Massiceti et
  al., 2018)</title>
    <summary>  In a recent workshop paper, Massiceti et al. presented a baseline model and
subsequent critique of Visual Dialog (Das et al., CVPR 2017) that raises what
we believe to be unfounded concerns about the dataset and evaluation. This
article intends to rebut the critique and clarify potential confusions for
practitioners and future participants in the Visual Dialog challenge.
</summary>
    <author>
      <name>Abhishek Das</name>
    </author>
    <author>
      <name>Devi Parikh</name>
    </author>
    <author>
      <name>Dhruv Batra</name>
    </author>
    <link href="http://arxiv.org/abs/1901.05531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.09156v1</id>
    <updated>2019-01-26T04:22:30Z</updated>
    <published>2019-01-26T04:22:30Z</published>
    <title>Human Pose Estimation using Motion Priors and Ensemble Models</title>
    <summary>  Human pose estimation in images and videos is one of key technologies for
realizing a variety of human activity recognition tasks (e.g., human-computer
interaction, gesture recognition, surveillance, and video summarization). This
paper presents two types of human pose estimation methodologies; 1) 3D human
pose tracking using motion priors and 2) 2D human pose estimation with ensemble
modeling.
</summary>
    <author>
      <name>Norimichi Ukita</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 2017 International Conference on Advanced
  Computer Science and Information Systems (ICACSIS)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1901.09156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.09156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.02831v1</id>
    <updated>2019-02-07T20:22:15Z</updated>
    <published>2019-02-07T20:22:15Z</published>
    <title>Evaluating Crowd Density Estimators via Their Uncertainty Bounds</title>
    <summary>  In this work, we use the Belief Function Theory which extends the
probabilistic framework in order to provide uncertainty bounds to different
categories of crowd density estimators. Our method allows us to compare the
multi-scale performance of the estimators, and also to characterize their
reliability for crowd monitoring applications requiring varying degrees of
prudence.
</summary>
    <author>
      <name>Jennifer Vandoni</name>
    </author>
    <author>
      <name>Emanuel Aldea</name>
    </author>
    <author>
      <name>Sylvie Le Hégarat-Mascle</name>
    </author>
    <link href="http://arxiv.org/abs/1902.02831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.02831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.03091v1</id>
    <updated>2019-02-08T14:24:36Z</updated>
    <published>2019-02-08T14:24:36Z</published>
    <title>FocusNet: An attention-based Fully Convolutional Network for Medical
  Image Segmentation</title>
    <summary>  We propose a novel technique to incorporate attention within convolutional
neural networks using feature maps generated by a separate convolutional
autoencoder. Our attention architecture is well suited for incorporation with
deep convolutional networks. We evaluate our model on benchmark segmentation
datasets in skin cancer segmentation and lung lesion segmentation. Results show
highly competitive performance when compared with U-Net and it's residual
variant.
</summary>
    <author>
      <name>Chaitanya Kaul</name>
    </author>
    <author>
      <name>Suresh Manandhar</name>
    </author>
    <author>
      <name>Nick Pears</name>
    </author>
    <link href="http://arxiv.org/abs/1902.03091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.03601v1</id>
    <updated>2019-02-10T13:51:47Z</updated>
    <published>2019-02-10T13:51:47Z</published>
    <title>Vulnerable road user detection: state-of-the-art and open challenges</title>
    <summary>  Correctly identifying vulnerable road users (VRUs), e.g. cyclists and
pedestrians, remains one of the most challenging environment perception tasks
for autonomous vehicles (AVs). This work surveys the current state-of-the-art
in VRU detection, covering topics such as benchmarks and datasets, object
detection techniques and relevant machine learning algorithms. The article
concludes with a discussion of remaining open challenges and promising future
research directions for this domain.
</summary>
    <author>
      <name>Patrick Mannion</name>
    </author>
    <link href="http://arxiv.org/abs/1902.03601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.05429v1</id>
    <updated>2019-02-13T09:09:52Z</updated>
    <published>2019-02-13T09:09:52Z</published>
    <title>Structured Bayesian Compression for Deep models in mobile enabled
  devices for connected healthcare</title>
    <summary>  Deep Models, typically Deep neural networks, have millions of parameters,
analyze medical data accurately, yet in a time-consuming method. However,
energy cost effectiveness and computational efficiency are important for
prerequisites developing and deploying mobile-enabled devices, the mainstream
trend in connected healthcare.
</summary>
    <author>
      <name>Sijia Chen</name>
    </author>
    <author>
      <name>Bin Song</name>
    </author>
    <author>
      <name>Xiaojiang Du</name>
    </author>
    <author>
      <name>Nadra Guizani</name>
    </author>
    <link href="http://arxiv.org/abs/1902.05429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.05429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.00117v2</id>
    <updated>2019-03-13T14:07:45Z</updated>
    <published>2019-03-01T01:13:30Z</published>
    <title>A Sketch Based 3D Shape Retrieval Approach Based on Efficient Deep
  Point-to-Subspace Metric Learning</title>
    <summary>  A sketch based 3D shape retrieval
</summary>
    <author>
      <name>Yinjie Lei</name>
    </author>
    <author>
      <name>Ziqin Zhou</name>
    </author>
    <author>
      <name>Pingping Zhang</name>
    </author>
    <author>
      <name>Yulan Guo</name>
    </author>
    <author>
      <name>Zijun Ma</name>
    </author>
    <author>
      <name>Lingqiao Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The first author wants to withdraw this paper. He has noticed several
  setting errors in experiment parts</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.00117v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.00117v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.01814v1</id>
    <updated>2019-03-05T13:32:03Z</updated>
    <published>2019-03-05T13:32:03Z</published>
    <title>HexagDLy - Processing hexagonally sampled data with CNNs in PyTorch</title>
    <summary>  HexagDLy is a Python-library extending the PyTorch deep learning framework
with convolution and pooling operations on hexagonal grids. It aims to ease the
access to convolutional neural networks for applications that rely on
hexagonally sampled data as, for example, commonly found in ground-based
astroparticle physics experiments.
</summary>
    <author>
      <name>Constantin Steppa</name>
    </author>
    <author>
      <name>Tim Lukas Holch</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.softx.2019.02.010</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.softx.2019.02.010" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SoftwareX, 9, 193-198, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.01814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.11421v1</id>
    <updated>2019-03-27T13:41:17Z</updated>
    <published>2019-03-27T13:41:17Z</published>
    <title>Social Behavioral Phenotyping of Drosophila with a2D-3D Hybrid CNN
  Framework</title>
    <summary>  Behavioural phenotyping of Drosophila is an important means in biological and
medical research to identify genetic, pathologic or psychologic impact on
animal behaviour.
</summary>
    <author>
      <name>Ziping Jiang</name>
    </author>
    <author>
      <name>Paul L. Chazot</name>
    </author>
    <author>
      <name>M. Emre Celebi</name>
    </author>
    <author>
      <name>Danny Crookes</name>
    </author>
    <author>
      <name>Richard Jiang</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Access 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.11421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.11421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.06577v2</id>
    <updated>2020-05-30T11:18:01Z</updated>
    <published>2019-04-13T17:50:28Z</published>
    <title>Direct Sparse Mapping</title>
    <summary>  Photometric bundle adjustment, PBA, accurately estimates geometry from video.
However, current PBA systems have a temporary map that cannot manage scene
reobservations. We present, DSM, a full monocular visual SLAM based on PBA. Its
persistent map handles reobservations, yielding the most accurate results up to
date on EuRoC for a direct method.
</summary>
    <author>
      <name>Jon Zubizarreta</name>
    </author>
    <author>
      <name>Iker Aguinaga</name>
    </author>
    <author>
      <name>J. M. M. Montiel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TRO.2020.2991614</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TRO.2020.2991614" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in IEEE Transactions on Robotics</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.06577v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.06577v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.04093v1</id>
    <updated>2019-05-10T11:58:10Z</updated>
    <published>2019-05-10T11:58:10Z</published>
    <title>Towards Unsupervised Familiar Scene Recognition in Egocentric Videos</title>
    <summary>  Nowadays, there is an upsurge of interest in using lifelogging devices. Such
devices generate huge amounts of image data; consequently, the need for
automatic methods for analyzing and summarizing these data is drastically
increasing. We present a new method for familiar scene recognition in
egocentric videos, based on background pattern detection through automatically
configurable COSFIRE filters. We present some experiments over egocentric data
acquired with the Narrative Clip.
</summary>
    <author>
      <name>Estefania Talavera</name>
    </author>
    <author>
      <name>Nicolai Petkov</name>
    </author>
    <author>
      <name>Petia Radeva</name>
    </author>
    <link href="http://arxiv.org/abs/1905.04093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.04093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.04740v1</id>
    <updated>2019-05-12T16:38:19Z</updated>
    <published>2019-05-12T16:38:19Z</published>
    <title>Object Detection in Specific Traffic Scenes using YOLOv2</title>
    <summary>  object detection framework plays crucial role in autonomous driving. In this
paper, we introduce the real-time object detection framework called You Only
Look Once (YOLOv1) and the related improvements of YOLOv2. We further explore
the capability of YOLOv2 by implementing its pre-trained model to do the object
detecting tasks in some specific traffic scenes. The four artificially designed
traffic scenes include single-car, single-person, frontperson-rearcar and
frontcar-rearperson.
</summary>
    <author>
      <name>Shouyu Wang</name>
    </author>
    <author>
      <name>Weitao Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1905.04740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.04740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.06540v2</id>
    <updated>2019-05-27T07:11:10Z</updated>
    <published>2019-05-16T05:55:38Z</published>
    <title>Title Redacted</title>
    <summary>  arXiv admin note: This version removed by arXiv administrators as the
submitter did not have the right to agree to the license at the time of
submission
</summary>
    <author>
      <name>Shivang Bharadwaj</name>
    </author>
    <author>
      <name>Bhupendra Niranjan</name>
    </author>
    <author>
      <name>Anant Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn as it is the proprietary property of an
  organization. A revision might or might not be uploaded in the future after
  further internal reviews and revisions. arXiv admin note: Title redacted</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.06540v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.06540v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.13302v1</id>
    <updated>2019-05-26T15:47:28Z</updated>
    <published>2019-05-26T15:47:28Z</published>
    <title>A Survey on Biomedical Image Captioning</title>
    <summary>  Image captioning applied to biomedical images can assist and accelerate the
diagnosis process followed by clinicians. This article is the first survey of
biomedical image captioning, discussing datasets, evaluation measures, and
state of the art methods. Additionally, we suggest two baselines, a weak and a
stronger one; the latter outperforms all current state of the art systems on
one of the datasets.
</summary>
    <author>
      <name>Vasiliki Kougia</name>
    </author>
    <author>
      <name>John Pavlopoulos</name>
    </author>
    <author>
      <name>Ion Androutsopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SiVL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.13302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.13302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.09233v1</id>
    <updated>2019-07-22T11:12:35Z</updated>
    <published>2019-07-22T11:12:35Z</published>
    <title>Adapting Computer Vision Algorithms for Omnidirectional Video</title>
    <summary>  Omnidirectional (360{\deg}) video has got quite popular because it provides a
highly immersive viewing experience. For computer vision algorithms, it poses
several challenges, like the special (equirectangular) projection commonly
employed and the huge image size. In this work, we give a high-level overview
of these challenges and outline strategies how to adapt computer vision
algorithm for the specifics of omnidirectional video.
</summary>
    <author>
      <name>Hannes Fassold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for 27th ACM International Conference on Multimedia (ACMM MM
  2019, Nice, France)</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.09233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.09233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.04386v1</id>
    <updated>2019-08-04T11:31:26Z</updated>
    <published>2019-08-04T11:31:26Z</published>
    <title>Detection of the Group of Traffic Signs with Central Slice Theorem</title>
    <summary>  Our sensor system consists of a combination of Photonic Mixer Device - PMD
and Mono optical cameras. Some traffic signs have stripes at 45{deg}. These
traffic signs cancel different restrictions on the road. We detect this class
of signs with Radon transformation. Here the Radon transformation is calculated
using Central Slice Theorem. We approximate the slice of spectrum by the
Discrete Cosine Transformation (DCT).
</summary>
    <author>
      <name>Koba Natroshvili</name>
    </author>
    <link href="http://arxiv.org/abs/1908.04386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.10585v1</id>
    <updated>2019-08-28T07:41:53Z</updated>
    <published>2019-08-28T07:41:53Z</published>
    <title>Attention-based Fusion for Outfit Recommendation</title>
    <summary>  This paper describes an attention-based fusion method for outfit
recommendation which fuses the information in the product image and description
to capture the most important, fine-grained product features into the item
representation. We experiment with different kinds of attention mechanisms and
demonstrate that the attention-based fusion improves item understanding. We
outperform state-of-the-art outfit recommendation results on three benchmark
datasets.
</summary>
    <author>
      <name>Katrien Laenen</name>
    </author>
    <author>
      <name>Marie-Francine Moens</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.10585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.03903v1</id>
    <updated>2019-10-09T11:22:59Z</updated>
    <published>2019-10-09T11:22:59Z</published>
    <title>MixMatch Domain Adaptaion: Prize-winning solution for both tracks of
  VisDA 2019 challenge</title>
    <summary>  We present a domain adaptation (DA) system that can be used in multi-source
and semi-supervised settings. Using the proposed method we achieved 2nd place
on multi-source track and 3rd place on semi-supervised track of the VisDA 2019
challenge (http://ai.bu.edu/visda-2019/). The source code of the method is
available at https://github.com/filaPro/visda2019.
</summary>
    <author>
      <name>Danila Rukhovich</name>
    </author>
    <author>
      <name>Danil Galeev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at TASK-CV 2019 at ICCV</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.03903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.03903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.11100v1</id>
    <updated>2019-10-18T08:09:23Z</updated>
    <published>2019-10-18T08:09:23Z</published>
    <title>Development of a hand pose recognition system on an embedded computer
  using CNNs</title>
    <summary>  Demand of hand pose recognition systems are growing in the last years in
technologies like human-machine interfaces. This work suggests an approach for
hand pose recognition in embedded computers using hand tracking and CNNs.
Results show a fast time response with an accuracy of 94.50% and low power
consumption.
</summary>
    <author>
      <name>Dennis Núñez Fernández</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LatinX in AI Research at NeurIPS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.11100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.11100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.11534v1</id>
    <updated>2019-10-25T05:28:36Z</updated>
    <published>2019-10-25T05:28:36Z</published>
    <title>Team PFDet's Methods for Open Images Challenge 2019</title>
    <summary>  We present the instance segmentation and the object detection method used by
team PFDet for Open Images Challenge 2019. We tackle a massive dataset size,
huge class imbalance and federated annotations. Using this method, the team
PFDet achieved 3rd and 4th place in the instance segmentation and the object
detection track, respectively.
</summary>
    <author>
      <name>Yusuke Niitani</name>
    </author>
    <author>
      <name>Toru Ogawa</name>
    </author>
    <author>
      <name>Shuji Suzuki</name>
    </author>
    <author>
      <name>Takuya Akiba</name>
    </author>
    <author>
      <name>Tommi Kerola</name>
    </author>
    <author>
      <name>Kohei Ozaki</name>
    </author>
    <author>
      <name>Shotaro Sano</name>
    </author>
    <link href="http://arxiv.org/abs/1910.11534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.11534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.07938v1</id>
    <updated>2019-10-28T03:00:28Z</updated>
    <published>2019-10-28T03:00:28Z</published>
    <title>Towards Good Practices for Multi-Person Pose Estimation</title>
    <summary>  Multi-Person Pose Estimation is an interesting yet challenging task in
computer vision. In this paper, we conduct a series of refinements with the
MSPN and PoseFix Networks, and empirically evaluate their impact on the final
model performance through ablation studies. By taking all the refinements, we
achieve 78.7 on the COCO test-dev dataset and 76.3 on the COCO test-challenge
dataset.
</summary>
    <author>
      <name>Dongdong Yu</name>
    </author>
    <author>
      <name>Kai Su</name>
    </author>
    <author>
      <name>Changhu Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1911.07938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.07938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.07939v1</id>
    <updated>2019-10-28T03:03:48Z</updated>
    <published>2019-10-28T03:03:48Z</published>
    <title>Towards Good Practices for Instance Segmentation</title>
    <summary>  Instance Segmentation is an interesting yet challenging task in computer
vision. In this paper, we conduct a series of refinements with the Hybrid Task
Cascade (HTC) Network, and empirically evaluate their impact on the final model
performance through ablation studies. By taking all the refinements, we achieve
0.47 on the COCO test-dev dataset and 0.47 on the COCO test-challenge dataset.
</summary>
    <author>
      <name>Dongdong Yu</name>
    </author>
    <author>
      <name>Zehuan Yuan</name>
    </author>
    <author>
      <name>Jinlai Liu</name>
    </author>
    <author>
      <name>Kun Yuan</name>
    </author>
    <author>
      <name>Changhu Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1911.07939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.07939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.08169v1</id>
    <updated>2019-11-19T09:25:59Z</updated>
    <published>2019-11-19T09:25:59Z</published>
    <title>Dense Fusion Classmate Network for Land Cover Classification</title>
    <summary>  Recently, FCNs based methods have made great progress in semantic
segmentation. Different with ordinary scenes, satellite image owns specific
characteristics, which elements always extend to large scope and no regular or
clear boundaries. Therefore, effective mid-level structure information
extremely missing, precise pixel-level classification becomes tough issues. In
this paper, a Dense Fusion Classmate Network (DFCNet) is proposed to adopt in
land cover classification.
</summary>
    <author>
      <name>Chao Tian</name>
    </author>
    <author>
      <name>Cong Li</name>
    </author>
    <author>
      <name>Jianping Shi</name>
    </author>
    <link href="http://arxiv.org/abs/1911.08169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.08169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.10636v1</id>
    <updated>2019-11-24T23:03:19Z</updated>
    <published>2019-11-24T23:03:19Z</published>
    <title>Pyramid Vector Quantization and Bit Level Sparsity in Weights for
  Efficient Neural Networks Inference</title>
    <summary>  This paper discusses three basic blocks for the inference of convolutional
neural networks (CNNs). Pyramid Vector Quantization (PVQ) is discussed as an
effective quantizer for CNNs weights resulting in highly sparse and
compressible networks. Properties of PVQ are exploited for the elimination of
multipliers during inference while maintaining high performance. The result is
then extended to any other quantized weights. The Tiny Yolo v3 CNN is used to
compare such basic blocks.
</summary>
    <author>
      <name>Vincenzo Liguori</name>
    </author>
    <link href="http://arxiv.org/abs/1911.10636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.10636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.12249v1</id>
    <updated>2019-11-27T16:14:14Z</updated>
    <published>2019-11-27T16:14:14Z</published>
    <title>Literature Review of Action Recognition in the Wild</title>
    <summary>  The literature review presented below on Action Recognition in the wild is
the in-depth study of Research Papers. Action Recognition problem in the
untrimmed videos is a challenging task and most of the papers have tackled this
problem using hand-crafted features with shallow learning techniques and
sophisticated end-to-end deep learning techniques.
</summary>
    <author>
      <name>Asket Kaur</name>
    </author>
    <author>
      <name>Navya Rao</name>
    </author>
    <author>
      <name>Tanya Joon</name>
    </author>
    <link href="http://arxiv.org/abs/1911.12249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.12249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.12706v1</id>
    <updated>2019-11-28T13:40:06Z</updated>
    <published>2019-11-28T13:40:06Z</published>
    <title>Cameras Viewing Cameras Geometry</title>
    <summary>  A basic problem in computer vision is to understand the structure of a
real-world scene given several images of it. Here we study several theoretical
aspects of the intra multi-view geometry of calibrated cameras when all that
they can reliably recognize is each other. With the proliferation of wearable
cameras, autonomous vehicles and drones, the geometry of these multiple cameras
is a timely and relevant problem to study.
</summary>
    <author>
      <name>Danail Brezov</name>
    </author>
    <author>
      <name>Michael Werman</name>
    </author>
    <link href="http://arxiv.org/abs/1911.12706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.12706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.02202v1</id>
    <updated>2019-12-04T19:00:15Z</updated>
    <published>2019-12-04T19:00:15Z</published>
    <title>MORPHOLO C++ Library for glasses-free multi-view stereo vision and
  streaming of live 3D video</title>
    <summary>  The MORPHOLO C++ extended Library allows to convert a specific stereoscopic
snapshot into a Native multi-view image through morphing algorithms taking into
account display calibration data for specific slanted lenticular 3D monitors.
MORPHOLO can also be implemented for glasses-free live applicatons of 3D video
streaming, and for diverse innovative scientific, engineering and 3D video game
applications -see http://www.morpholo.it
</summary>
    <author>
      <name>Enrique Canessa</name>
    </author>
    <author>
      <name>Livio Tenze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.02202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.02202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.04376v1</id>
    <updated>2019-12-09T21:06:15Z</updated>
    <published>2019-12-09T21:06:15Z</published>
    <title>Modular Multimodal Architecture for Document Classification</title>
    <summary>  Page classification is a crucial component to any document analysis system,
allowing for complex branching control flows for different components of a
given document. Utilizing both the visual and textual content of a page, the
proposed method exceeds the current state-of-the-art performance on the
RVL-CDIP benchmark at 93.03% test accuracy.
</summary>
    <author>
      <name>Tyler Dauphinee</name>
    </author>
    <author>
      <name>Nikunj Patel</name>
    </author>
    <author>
      <name>Mohammad Rashidi</name>
    </author>
    <link href="http://arxiv.org/abs/1912.04376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.04376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.12167v1</id>
    <updated>2019-12-18T19:23:29Z</updated>
    <published>2019-12-18T19:23:29Z</published>
    <title>Design Considerations for Efficient Deep Neural Networks on
  Processing-in-Memory Accelerators</title>
    <summary>  This paper describes various design considerations for deep neural networks
that enable them to operate efficiently and accurately on processing-in-memory
accelerators. We highlight important properties of these accelerators and the
resulting design considerations using experiments conducted on various
state-of-the-art deep neural networks with the large-scale ImageNet dataset.
</summary>
    <author>
      <name>Tien-Ju Yang</name>
    </author>
    <author>
      <name>Vivienne Sze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEDM 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.12167v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.12167v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.05107v3</id>
    <updated>2020-08-16T13:29:19Z</updated>
    <published>2020-02-12T17:32:18Z</published>
    <title>Analysis of Dutch Master Paintings with Convolutional Neural Networks</title>
    <summary>  Trained on the works of an artist under study and visually comparable works
of other artists, convolutional neural networks can identify forgeries and
provide attributions. They can also assign classification probabilities within
a painting, revealing mixed authorship and identifying regions painted by
different hands.
</summary>
    <author>
      <name>Steven J. Frank</name>
    </author>
    <author>
      <name>Andrea M. Frank</name>
    </author>
    <link href="http://arxiv.org/abs/2002.05107v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05107v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.05447v1</id>
    <updated>2020-02-13T11:29:46Z</updated>
    <published>2020-02-13T11:29:46Z</published>
    <title>Emotion Recognition for In-the-wild Videos</title>
    <summary>  This paper is a brief introduction to our submission to the seven basic
expression classification track of Affective Behavior Analysis in-the-wild
Competition held in conjunction with the IEEE International Conference on
Automatic Face and Gesture Recognition (FG) 2020. Our method combines Deep
Residual Network (ResNet) and Bidirectional Long Short-Term Memory Network
(BLSTM), achieving 64.3% accuracy and 43.4% final metric on the validation set.
</summary>
    <author>
      <name>Hanyu Liu</name>
    </author>
    <author>
      <name>Jiabei Zeng</name>
    </author>
    <author>
      <name>Shiguang Shan</name>
    </author>
    <author>
      <name>Xilin Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2002.05447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10363v1</id>
    <updated>2020-02-24T16:48:30Z</updated>
    <published>2020-02-24T16:48:30Z</published>
    <title>Joint Learning of Assignment and Representation for Biometric Group
  Membership</title>
    <summary>  This paper proposes a framework for group membership protocols preventing the
curious but honest server from reconstructing the enrolled biometric signatures
and inferring the identity of querying clients. This framework learns the
embedding parameters, group representations and assignments simultaneously.
Experiments show the trade-off between security/privacy and
verification/identification performances.
</summary>
    <author>
      <name>Marzieh Gheisari</name>
    </author>
    <author>
      <name>Teddy Furon</name>
    </author>
    <author>
      <name>Laurent Amsaleg</name>
    </author>
    <link href="http://arxiv.org/abs/2002.10363v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10363v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.08756v1</id>
    <updated>2020-03-04T15:02:05Z</updated>
    <published>2020-03-04T15:02:05Z</published>
    <title>Deep Neural Network Perception Models and Robust Autonomous Driving
  Systems</title>
    <summary>  This paper analyzes the robustness of deep learning models in autonomous
driving applications and discusses the practical solutions to address that.
</summary>
    <author>
      <name>Mohammad Javad Shafiee</name>
    </author>
    <author>
      <name>Ahmadreza Jeddi</name>
    </author>
    <author>
      <name>Amir Nazemi</name>
    </author>
    <author>
      <name>Paul Fieguth</name>
    </author>
    <author>
      <name>Alexander Wong</name>
    </author>
    <link href="http://arxiv.org/abs/2003.08756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.08756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.09234v1</id>
    <updated>2020-03-11T13:20:42Z</updated>
    <published>2020-03-11T13:20:42Z</published>
    <title>DeepFake Detection: Current Challenges and Next Steps</title>
    <summary>  High quality fake videos and audios generated by AI-algorithms (the deep
fakes) have started to challenge the status of videos and audios as definitive
evidence of events. In this paper, we highlight a few of these challenges and
discuss the research opportunities in this direction.
</summary>
    <author>
      <name>Siwei Lyu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1909.12962</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.09234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.09234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.09971v2</id>
    <updated>2020-05-10T21:59:36Z</updated>
    <published>2020-03-22T19:04:25Z</published>
    <title>A Better Variant of Self-Critical Sequence Training</title>
    <summary>  In this work, we present a simple yet better variant of Self-Critical
Sequence Training. We make a simple change in the choice of baseline function
in REINFORCE algorithm. The new baseline can bring better performance with no
extra cost, compared to the greedy decoding baseline.
</summary>
    <author>
      <name>Ruotian Luo</name>
    </author>
    <link href="http://arxiv.org/abs/2003.09971v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.09971v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.03339v1</id>
    <updated>2020-03-27T23:34:01Z</updated>
    <published>2020-03-27T23:34:01Z</published>
    <title>Automatic Generation of Chinese Handwriting via Fonts Style
  Representation Learning</title>
    <summary>  In this paper, we propose and end-to-end deep Chinese font generation system.
This system can generate new style fonts by interpolation of latent
style-related embeding variables that could achieve smooth transition between
different style. Our method is simpler and more effective than other methods,
which will help to improve the font design efficiency
</summary>
    <author>
      <name>Fenxi Xiao</name>
    </author>
    <author>
      <name>Bo Huang</name>
    </author>
    <author>
      <name>Xia Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2004.03339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.03339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.09430v1</id>
    <updated>2020-04-20T16:36:01Z</updated>
    <published>2020-04-20T16:36:01Z</published>
    <title>Improving correlation method with convolutional neural networks</title>
    <summary>  We present a convolutional neural network for the classification of
correlation responses obtained by correlation filters. The proposed approach
can improve the accuracy of classification, as well as achieve invariance to
the image classes and parameters.
</summary>
    <author>
      <name>Dmitriy Goncharov</name>
    </author>
    <author>
      <name>Rostislav Starikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, 2 tables, 1 formula</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.09430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.09430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.08645v1</id>
    <updated>2020-05-09T12:13:43Z</updated>
    <published>2020-05-09T12:13:43Z</published>
    <title>Multi-Task Learning in Histo-pathology for Widely Generalizable Model</title>
    <summary>  In this work we show preliminary results of deep multi-task learning in the
area of computational pathology. We combine 11 tasks ranging from patch-wise
oral cancer classification, one of the most prevalent cancers in the developing
world, to multi-tissue nuclei instance segmentation and classification.
</summary>
    <author>
      <name>Jevgenij Gamper</name>
    </author>
    <author>
      <name>Navid Alemi Kooohbanani</name>
    </author>
    <author>
      <name>Nasir Rajpoot</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AI4CC ICLR 2020 workshop</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.08645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.08645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.02536v1</id>
    <updated>2020-06-03T21:13:05Z</updated>
    <published>2020-06-03T21:13:05Z</published>
    <title>Phasic dopamine release identification using ensemble of AlexNet</title>
    <summary>  Dopamine (DA) is an organic chemical that influences several parts of
behaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is a
technique used for in vivo phasic dopamine release measurements. The analysis
of such measurements, though, requires notable effort. In this paper, we
present the use of convolutional neural networks (CNNs) for the identification
of phasic dopamine releases.
</summary>
    <author>
      <name>Luca Patarnello</name>
    </author>
    <author>
      <name>Marco Celin</name>
    </author>
    <author>
      <name>Loris Nanni</name>
    </author>
    <link href="http://arxiv.org/abs/2006.02536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.02536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.02692v2</id>
    <updated>2020-06-05T07:51:53Z</updated>
    <published>2020-06-04T08:20:30Z</published>
    <title>Problems of dataset creation for light source estimation</title>
    <summary>  The paper describes our experience collecting a new dataset for the light
source estimation problem in a single image. The analysis of existing color
targets is presented along with various technical and scientific aspects
essential for data collection. The paper also contains an announcement of an
upcoming 2-nd International Illumination Estimation Challenge (IEC 2020).
</summary>
    <author>
      <name>E. I. Ershov</name>
    </author>
    <author>
      <name>A. V. Belokopytov</name>
    </author>
    <author>
      <name>A. V. Savchik</name>
    </author>
    <link href="http://arxiv.org/abs/2006.02692v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.02692v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.05927v1</id>
    <updated>2020-06-10T16:25:28Z</updated>
    <published>2020-06-10T16:25:28Z</published>
    <title>Recent Advances in 3D Object and Hand Pose Estimation</title>
    <summary>  3D object and hand pose estimation have huge potentials for Augmented
Reality, to enable tangible interfaces, natural interfaces, and blurring the
boundaries between the real and virtual worlds. In this chapter, we present the
recent developments for 3D object and hand pose estimation using cameras, and
discuss their abilities and limitations and the possible future development of
the field.
</summary>
    <author>
      <name>Vincent Lepetit</name>
    </author>
    <link href="http://arxiv.org/abs/2006.05927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.05927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.10923v1</id>
    <updated>2020-06-19T01:49:37Z</updated>
    <published>2020-06-19T01:49:37Z</published>
    <title>Hyperparameter Analysis for Image Captioning</title>
    <summary>  In this paper, we perform a thorough sensitivity analysis on state-of-the-art
image captioning approaches using two different architectures: CNN+LSTM and
CNN+Transformer. Experiments were carried out using the Flickr8k dataset. The
biggest takeaway from the experiments is that fine-tuning the CNN encoder
outperforms the baseline and all other experiments carried out for both
architectures.
</summary>
    <author>
      <name>Amish Patel</name>
    </author>
    <author>
      <name>Aravind Varier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 9 figures, and 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.10923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.10923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.14319v1</id>
    <updated>2020-06-25T11:50:35Z</updated>
    <published>2020-06-25T11:50:35Z</published>
    <title>Deep Learning for Cornea Microscopy Blind Deblurring</title>
    <summary>  The goal of this project is to build a deep-learning solution that deblurs
cornea scans, used for medical examination. The spherical shape of the eye
prevents ophtamologist from having completely sharp image. Provided with a
stack of corneas from confocal images, our approach is to build a model that
performs an upscaling of the images using an SR (Super Resolution) Network.
</summary>
    <author>
      <name>Toussain Cardot</name>
    </author>
    <author>
      <name>Pilar Marxer</name>
    </author>
    <author>
      <name>Ivan Snozzi</name>
    </author>
    <link href="http://arxiv.org/abs/2006.14319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.14319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.08170v1</id>
    <updated>2020-07-16T08:10:42Z</updated>
    <published>2020-07-16T08:10:42Z</published>
    <title>VIPriors Object Detection Challenge</title>
    <summary>  This paper is a brief report to our submission to the VIPriors Object
Detection Challenge. Object Detection has attracted many researchers' attention
for its full application, but it is still a challenging task. In this paper, we
study analysis the characteristics of the data, and an effective data
enhancement method is proposed. We carefully choose the model which is more
suitable for training from scratch. We benefit a lot from using softnms and
model fusion skillfully.
</summary>
    <author>
      <name>Zhipeng Luo</name>
    </author>
    <author>
      <name>Lixuan Che</name>
    </author>
    <link href="http://arxiv.org/abs/2007.08170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.08170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.10232v1</id>
    <updated>2020-07-17T09:09:41Z</updated>
    <published>2020-07-17T09:09:41Z</published>
    <title>The Effect of Top-Down Attention in Occluded Object Recognition</title>
    <summary>  This study is concerned with the top-down visual processing benefit in the
task of occluded object recognition. To this end, a psychophysical experiment
is designed and carried out which aimed at investigating the effect of
consistency of contextual information on the recognition of objects which are
partially occluded. The results demonstrate the facilitative impact of
consistent contextual clues on the task of object recognition in presence of
occlusion.
</summary>
    <author>
      <name>Zahra Sadeghi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.10232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.10232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.00350v1</id>
    <updated>2021-01-02T01:51:38Z</updated>
    <published>2021-01-02T01:51:38Z</published>
    <title>Multi-Image Steganography Using Deep Neural Networks</title>
    <summary>  Steganography is the science of hiding a secret message within an ordinary
public message. Over the years, steganography has been used to encode a lower
resolution image into a higher resolution image by simple methods like LSB
manipulation. We aim to utilize deep neural networks for the encoding and
decoding of multiple secret images inside a single cover image of the same
resolution.
</summary>
    <author>
      <name>Abhishek Das</name>
    </author>
    <author>
      <name>Japsimar Singh Wahi</name>
    </author>
    <author>
      <name>Mansi Anand</name>
    </author>
    <author>
      <name>Yugant Rana</name>
    </author>
    <link href="http://arxiv.org/abs/2101.00350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.06022v1</id>
    <updated>2021-01-15T09:14:10Z</updated>
    <published>2021-01-15T09:14:10Z</published>
    <title>Motion-Based Handwriting Recognition</title>
    <summary>  We attempt to overcome the restriction of requiring a writing surface for
handwriting recognition. In this study, we design a prototype of a stylus
equipped with motion sensor, and utilizes gyroscopic and acceleration sensor
reading to perform written letter classification using various deep learning
techniques such as CNN and RNNs. We also explore various data augmentation
techniques and their effects, reaching up to 86% accuracy.
</summary>
    <author>
      <name>Junshen Kevin Chen</name>
    </author>
    <author>
      <name>Wanze Xie</name>
    </author>
    <author>
      <name>Yutong He</name>
    </author>
    <link href="http://arxiv.org/abs/2101.06022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.06022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.06025v1</id>
    <updated>2021-01-15T09:24:04Z</updated>
    <published>2021-01-15T09:24:04Z</published>
    <title>Motion-Based Handwriting Recognition and Word Reconstruction</title>
    <summary>  In this project, we leverage a trained single-letter classifier to predict
the written word from a continuously written word sequence, by designing a word
reconstruction pipeline consisting of a dynamic-programming algorithm and an
auto-correction model. We conduct experiments to optimize models in this
pipeline, then employ domain adaptation to explore using this pipeline on
unseen data distributions.
</summary>
    <author>
      <name>Junshen Kevin Chen</name>
    </author>
    <author>
      <name>Wanze Xie</name>
    </author>
    <author>
      <name>Yutong He</name>
    </author>
    <link href="http://arxiv.org/abs/2101.06025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.06025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.09475v1</id>
    <updated>2021-03-17T07:11:38Z</updated>
    <published>2021-03-17T07:11:38Z</published>
    <title>Virtual Dress Swap Using Landmark Detection</title>
    <summary>  Online shopping has gained popularity recently. This paper addresses one
crucial problem of buying dress online, which has not been solved yet. This
research tries to implement the idea of clothes swapping with the help of
DeepFashion dataset where 6,223 images with eight landmarks each used. Deep
Convolutional Neural Network has been built for Landmark detection.
</summary>
    <author>
      <name>Odar Zeynal</name>
    </author>
    <author>
      <name>Saber Malekzadeh</name>
    </author>
    <link href="http://arxiv.org/abs/2103.09475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.09475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.01622v1</id>
    <updated>2021-04-04T15:02:32Z</updated>
    <published>2021-04-04T15:02:32Z</published>
    <title>OnTarget: An Electronic Archery Scoring</title>
    <summary>  There are several challenges in creating an electronic archery scoring system
using computer vision techniques. Variability of light, reconstruction of the
target from several images, variability of target configuration, and filtering
noise were significant challenges during the creation of this scoring system.
This paper discusses the approach used to determine where an arrow hits a
target, for any possible single or set of targets and provides an algorithm
that balances the difficulty of robust arrow detection while retaining the
required accuracy.
</summary>
    <author>
      <name>Andreea Danielescu</name>
    </author>
    <link href="http://arxiv.org/abs/2104.01622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.01622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.02964v1</id>
    <updated>2021-05-02T00:05:17Z</updated>
    <published>2021-05-02T00:05:17Z</published>
    <title>Object detection for crabs in top-view seabed imagery</title>
    <summary>  This report presents the application of object detection on a database of
underwater images of different species of crabs, as well as aerial images of
sea lions and finally the Pascal VOC dataset. The model is an end-to-end object
detection neural network based on a convolutional network base and a Long
Short-Term Memory detector.
</summary>
    <author>
      <name>Vlad Velici</name>
    </author>
    <author>
      <name>Adam Prügel-Bennett</name>
    </author>
    <link href="http://arxiv.org/abs/2105.02964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.02964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.04093v1</id>
    <updated>2021-05-10T03:48:55Z</updated>
    <published>2021-05-10T03:48:55Z</published>
    <title>Elastic Weight Consolidation (EWC): Nuts and Bolts</title>
    <summary>  In this report, we present a theoretical support of the continual learning
method \textbf{Elastic Weight Consolidation}, introduced in paper titled
`Overcoming catastrophic forgetting in neural networks'. Being one of the most
cited paper in regularized methods for continual learning, this report
disentangles the underlying concept of the proposed objective function. We
assume that the reader is aware of the basic terminologies of continual
learning.
</summary>
    <author>
      <name>Abhishek Aich</name>
    </author>
    <link href="http://arxiv.org/abs/2105.04093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.04093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.08796v1</id>
    <updated>2021-05-18T19:33:17Z</updated>
    <published>2021-05-18T19:33:17Z</published>
    <title>Analyzing the effectiveness of image augmentations for face recognition
  from limited data</title>
    <summary>  This work presents an analysis of the efficiency of image augmentations for
the face recognition problem from limited data. We considered basic
manipulations, generative methods, and their combinations for augmentations.
Our results show that augmentations, in general, can considerably improve the
quality of face recognition systems and the combination of generative and basic
approaches performs better than the other tested techniques.
</summary>
    <author>
      <name>Aleksei Zhuchkov</name>
    </author>
    <link href="http://arxiv.org/abs/2105.08796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.08796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.10063v1</id>
    <updated>2021-05-20T23:11:36Z</updated>
    <published>2021-05-20T23:11:36Z</published>
    <title>Uma implementação do jogo Pedra, Papel e Tesoura utilizando Visao
  Computacional</title>
    <summary>  This paper presents a game, controlled by computer vision, in identification
of hand gestures (hand-tracking). The proposed work is based on image
segmentation and construction of a convex hull with Jarvis Algorithm , and
determination of the pattern based on the extraction of area characteristics in
the convex hull.
</summary>
    <author>
      <name>Ezequiel França dos Santos</name>
    </author>
    <author>
      <name>Gabriel Fontenelle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, in Portuguese</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.10063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.10063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.13264v1</id>
    <updated>2021-05-27T16:02:40Z</updated>
    <published>2021-05-27T16:02:40Z</published>
    <title>How saccadic vision might help with theinterpretability of deep networks</title>
    <summary>  We describe how some problems (interpretability,lack of object-orientedness)
of modern deep networks potentiallycould be solved by adapting a biologically
plausible saccadicmechanism of perception. A sketch of such a saccadic
visionmodel is proposed. Proof of concept experimental results areprovided to
support the proposed approach.
</summary>
    <author>
      <name>Iana Sereda</name>
    </author>
    <author>
      <name>Grigory Osipov</name>
    </author>
    <link href="http://arxiv.org/abs/2105.13264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.13264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.02222v1</id>
    <updated>2021-06-04T02:55:46Z</updated>
    <published>2021-06-04T02:55:46Z</published>
    <title>History Encoding Representation Design for Human Intention Inference</title>
    <summary>  In this extended abstract, we investigate the design of learning
representation for human intention inference. In our designed human intention
prediction task, we propose a history encoding representation that is both
interpretable and effective for prediction. Through extensive experiments, we
show our prediction framework with a history encoding representation design is
successful on the human intention prediction problem.
</summary>
    <author>
      <name>Zhuo Xu</name>
    </author>
    <author>
      <name>Masayoshi Tomizuka</name>
    </author>
    <link href="http://arxiv.org/abs/2106.02222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.02222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.04104v1</id>
    <updated>2021-06-08T05:06:51Z</updated>
    <published>2021-06-08T05:06:51Z</published>
    <title>Design of Low-Artifact Interpolation Kernels by Means of Computer
  Algebra</title>
    <summary>  We present a number of new piecewise-polynomial kernels for image
interpolation. The kernels are constructed by optimizing a measure of
interpolation quality based on the magnitude of anisotropic artifacts. The
kernel design process is performed symbolically using Mathematica computer
algebra system. Experimental evaluation involving 14 image quality assessment
methods demonstrates that our results compare favorably with the existing
linear interpolators.
</summary>
    <author>
      <name>Peter Karpov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.04104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.04104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.08042v1</id>
    <updated>2021-06-15T10:52:07Z</updated>
    <published>2021-06-15T10:52:07Z</published>
    <title>Hotel Recognition via Latent Image Embedding</title>
    <summary>  We approach the problem of hotel recognition with deep metric learning. We
overview the existing approaches and propose a modification to Contrastive loss
called Contrastive-Triplet loss. We construct a robust pipeline for
benchmarking metric learning models and perform experiments on Hotels-50K and
CUB200 datasets. Contrastive-Triplet loss is shown to achieve better retrieval
on Hotels-50k. We open-source our code.
</summary>
    <author>
      <name>Boris Tseytlin</name>
    </author>
    <author>
      <name>Ilya Makarov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IWANN 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.08042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.08042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.08366v1</id>
    <updated>2021-06-15T18:36:54Z</updated>
    <published>2021-06-15T18:36:54Z</published>
    <title>Explaining decision of model from its prediction</title>
    <summary>  This document summarizes different visual explanations methods such as CAM,
Grad-CAM, Localization using Multiple Instance Learning - Saliency-based
methods, Saliency-driven Class-Impressions, Muting pixels in input image -
Adversarial methods and Activation visualization, Convolution filter
visualization - Feature-based methods. We have also shown the results produced
by different methods and a comparison between CAM, GradCAM, and Guided
Backpropagation.
</summary>
    <author>
      <name>Dipesh Tamboli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Literature review</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.08366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.08366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.11467v1</id>
    <updated>2021-06-22T01:02:52Z</updated>
    <published>2021-06-22T01:02:52Z</published>
    <title>Multimodal trajectory forecasting based on discrete heat map</title>
    <summary>  In Argoverse motion forecasting competition, the task is to predict the
probabilistic future trajectory distribution for the interested targets in the
traffic scene. We use vectorized lane map and 2 s targets' history trajectories
as input. Then the model outputs 6 forecasted trajectories with probability for
each target.
</summary>
    <author>
      <name>Jingni Yuan</name>
    </author>
    <author>
      <name>Jianyun Xu</name>
    </author>
    <author>
      <name>Yushi Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2106.11467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.11467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.12016v1</id>
    <updated>2021-06-22T18:42:44Z</updated>
    <published>2021-06-22T18:42:44Z</published>
    <title>On Matrix Factorizations in Subspace Clustering</title>
    <summary>  This article explores subspace clustering algorithms using CUR
decompositions, and examines the effect of various hyperparameters in these
algorithms on clustering performance on two real-world benchmark datasets, the
Hopkins155 motion segmentation dataset and the Yale face dataset. Extensive
experiments are done for a variety of sampling methods and oversampling
parameters for these datasets, and some guidelines for parameter choices are
given for practical applications.
</summary>
    <author>
      <name>Reeshad Arian</name>
    </author>
    <author>
      <name>Keaton Hamm</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages plus 4 pages of tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.12016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.12016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P99, 68T10, 62H30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.15179v1</id>
    <updated>2021-06-29T08:51:23Z</updated>
    <published>2021-06-29T08:51:23Z</published>
    <title>Wrong Colored Vermeer: Color-Symmetric Image Distortion</title>
    <summary>  Color symmetry implies that the colors of geometrical objects are assigned
according to their symmetry properties. It is defined by associating the
elements of the symmetry group with a color permutation. I use this concept for
generative art and apply symmetry-consistent color distortions to images of
paintings by Johannes Vermeer. The color permutations are realized as mappings
of the HSV color space onto itself.
</summary>
    <author>
      <name>Hendrik Richter</name>
    </author>
    <link href="http://arxiv.org/abs/2106.15179v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.15179v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.15306v1</id>
    <updated>2021-06-08T14:57:25Z</updated>
    <published>2021-06-08T14:57:25Z</published>
    <title>Artificial Intelligence in Minimally Invasive Interventional Treatment</title>
    <summary>  Minimally invasive image guided treatment procedures often employ advanced
image processing algorithms. The recent developments of artificial intelligence
algorithms harbor potential to further enhance this domain. In this article we
explore several application areas within the minimally invasive treatment space
and discuss the deployment of artificial intelligence within these areas.
</summary>
    <author>
      <name>Daniel Ruijters</name>
    </author>
    <link href="http://arxiv.org/abs/2106.15306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.15306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1; I.2.10; I.4.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.04453v2</id>
    <updated>2021-08-11T02:36:26Z</updated>
    <published>2021-08-10T05:25:59Z</published>
    <title>Method Towards CVPR 2021 Image Matching Challenge</title>
    <summary>  This report describes Megvii-3D team's approach towards CVPR 2021 Image
Matching Workshop.
</summary>
    <author>
      <name>Xiaopeng Bi</name>
    </author>
    <author>
      <name>Yu Chen</name>
    </author>
    <author>
      <name>Xinyang Liu</name>
    </author>
    <author>
      <name>Dehao Zhang</name>
    </author>
    <author>
      <name>Ran Yan</name>
    </author>
    <author>
      <name>Zheng Chai</name>
    </author>
    <author>
      <name>Haotian Zhang</name>
    </author>
    <author>
      <name>Xiao Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2108.04453v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.04453v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.00816v1</id>
    <updated>2021-09-02T09:47:43Z</updated>
    <published>2021-09-02T09:47:43Z</published>
    <title>Deep Learning-based mitosis detection in breast cancer histologic
  samples</title>
    <summary>  This is the submission for mitosis detection in the context of the MIDOG 2021
challenge. It is based on the two-stage objection model Faster RCNN as well as
DenseNet as a backbone for the neural network architecture. It achieves a
F1-score of 0.6645 on the Preliminary Test Phase Leaderboard.
</summary>
    <author>
      <name>Michel Halmes</name>
    </author>
    <author>
      <name>Hippolyte Heuberger</name>
    </author>
    <author>
      <name>Sylvain Berlemont</name>
    </author>
    <link href="http://arxiv.org/abs/2109.00816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.00816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.13126v1</id>
    <updated>2021-09-27T15:39:27Z</updated>
    <published>2021-09-27T15:39:27Z</published>
    <title>GANiry: Bald-to-Hairy Translation Using CycleGAN</title>
    <summary>  This work presents our computer vision course project called bald
men-to-hairy men translation using CycleGAN. On top of CycleGAN architecture,
we utilize perceptual loss in order to achieve more realistic results. We also
integrate conditional constrains to obtain different stylized and colored hairs
on bald men. We conducted extensive experiments and present qualitative results
in this paper. Our code and models are available at
https://github.com/fidansamet/GANiry.
</summary>
    <author>
      <name>Fidan Samet</name>
    </author>
    <author>
      <name>Oguz Bakir</name>
    </author>
    <link href="http://arxiv.org/abs/2109.13126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.13126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.07201v1</id>
    <updated>2021-10-14T07:54:36Z</updated>
    <published>2021-10-14T07:54:36Z</published>
    <title>Coarse to Fine: Video Retrieval before Moment Localization</title>
    <summary>  The current state-of-the-art methods for video corpus moment retrieval (VCMR)
often use similarity-based feature alignment approach for the sake of
convenience and speed. However, late fusion methods like cosine similarity
alignment are unable to make full use of the information from both query texts
and videos. In this paper, we combine feature alignment with feature fusion to
promote the performance on VCMR.
</summary>
    <author>
      <name>Zijian Gao</name>
    </author>
    <author>
      <name>Huanyu Liu</name>
    </author>
    <author>
      <name>Jingyu Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2110.07201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.07201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.14830v1</id>
    <updated>2021-10-28T00:35:32Z</updated>
    <published>2021-10-28T00:35:32Z</published>
    <title>ODMTCNet: An Interpretable Multi-view Deep Neural Network Architecture
  for Image Feature Representation</title>
    <summary>  This work proposes an interpretable multi-view deep neural network
architecture, namely optimal discriminant multi-view tensor convolutional
network (ODMTCNet), by integrating statistical machine learning (SML)
principles with the deep neural network (DNN) architecture.
</summary>
    <author>
      <name>Lei Gao</name>
    </author>
    <author>
      <name>Zheng Guo</name>
    </author>
    <author>
      <name>Ling Guan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE TPAMI</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.14830v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.14830v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.04839v1</id>
    <updated>2021-11-08T21:23:37Z</updated>
    <published>2021-11-08T21:23:37Z</published>
    <title>Evolving Evocative 2D Views of Generated 3D Objects</title>
    <summary>  We present a method for jointly generating 3D models of objects and 2D
renders at different viewing angles, with the process guided by ImageNet and
CLIP -based models. Our results indicate that it can generate anamorphic
objects, with renders that both evoke the target caption and look visually
appealing.
</summary>
    <author>
      <name>Eric Chu</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2021 Workshop on Machine Learning for Creativity and
  Design</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2111.04839v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.04839v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.05471v1</id>
    <updated>2021-11-10T00:56:45Z</updated>
    <published>2021-11-10T00:56:45Z</published>
    <title>Analysis of PDE-based binarization model for degraded document images</title>
    <summary>  This report presents the results of a PDE-based binarization model for
degraded document images. The model utilizes an edge and binary source term in
its formulation. Results indicate effectiveness for document images with
bleed-through and faded text and stains to a lesser extent.
</summary>
    <author>
      <name>Uche A. Nnolim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.05471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.05471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.06662v1</id>
    <updated>2021-11-12T11:08:15Z</updated>
    <published>2021-11-12T11:08:15Z</published>
    <title>A comprehensive study of clustering a class of 2D shapes</title>
    <summary>  The paper concerns clustering with respect to the shape and size of 2D
contours that are boundaries of cross-sections of 3D objects of revolution. We
propose a number of similarity measures based on combined disparate Procrustes
analysis (PA) and Dynamic Time Warping (DTW) distances. Motivation and the main
application for this study comes from archaeology. The performed computational
experiments refer to the clustering of archaeological pottery.
</summary>
    <author>
      <name>Agnieszka Kaliszewska</name>
    </author>
    <author>
      <name>Monika Syga</name>
    </author>
    <link href="http://arxiv.org/abs/2111.06662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.06662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.09113v1</id>
    <updated>2021-11-15T11:51:36Z</updated>
    <published>2021-11-15T11:51:36Z</published>
    <title>2nd Place Solution to Facebook AI Image Similarity Challenge Matching
  Track</title>
    <summary>  This paper presents the 2nd place solution to the Facebook AI Image
Similarity Challenge : Matching Track on DrivenData. The solution is based on
self-supervised learning, and Vision Transformer(ViT). The main breaktrough
comes from concatenating query and reference image to form as one image and
asking ViT to directly predict from the image if query image used reference
image. The solution scored 0.8291 Micro-average Precision on the private
leaderboard.
</summary>
    <author>
      <name>SeungKee Jeon</name>
    </author>
    <link href="http://arxiv.org/abs/2111.09113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.09113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.02297v1</id>
    <updated>2021-12-04T09:59:01Z</updated>
    <published>2021-12-04T09:59:01Z</published>
    <title>Ablation study of self-supervised learning for image classification</title>
    <summary>  This project focuses on the self-supervised training of convolutional neural
networks (CNNs) and transformer networks for the task of image recognition. A
simple siamese network with different backbones is used in order to maximize
the similarity of two augmented transformed images from the same source image.
In this way, the backbone is able to learn visual information without
supervision. Finally, the method is evaluated on three image recognition
datasets.
</summary>
    <author>
      <name>Ilias Papastratis</name>
    </author>
    <link href="http://arxiv.org/abs/2112.02297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.02297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.05576v1</id>
    <updated>2021-12-10T14:43:24Z</updated>
    <published>2021-12-10T14:43:24Z</published>
    <title>GPU-accelerated image alignment for object detection in industrial
  applications</title>
    <summary>  This research proposes a practical method for detecting featureless objects
by using image alignment approach with a robust similarity measure in
industrial applications. This similarity measure is robust against occlusion,
illumination changes and background clutter. The performance of the proposed
GPU (Graphics Processing Unit) accelerated algorithm is deemed successful in
experiments of comparison between both CPU and GPU implementations
</summary>
    <author>
      <name>Trung-Son Le</name>
    </author>
    <author>
      <name>Chyi-Yeu Lin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ARIS.2017.8297173</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ARIS.2017.8297173" rel="related"/>
    <link href="http://arxiv.org/abs/2112.05576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.05576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.13707v1</id>
    <updated>2021-12-27T14:31:24Z</updated>
    <published>2021-12-27T14:31:24Z</published>
    <title>Visual Place Representation and Recognition from Depth Images</title>
    <summary>  This work proposes a new method for place recognition based on the scene
architecture. From depth video, we compute the 3D model and we derive and
describe geometrically the 2D map from which the scene descriptor is deduced to
constitute the core of the proposed algorithm. The obtained results show the
efficiency and the robustness of the propounded descriptor to scene appearance
changes and light variations.
</summary>
    <author>
      <name>Farah Ibelaiden</name>
    </author>
    <author>
      <name>Slimane Larabi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ijleo.2022.169109</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ijleo.2022.169109" rel="related"/>
    <link href="http://arxiv.org/abs/2112.13707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.13707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.06890v2</id>
    <updated>2022-03-21T15:34:03Z</updated>
    <published>2022-03-14T07:11:20Z</published>
    <title>Attention based Memory video portrait matting</title>
    <summary>  We proposed a novel trimap free video matting method based on the attention
mechanism. By the nature of the problem, most existing approaches use either
multiple computational expansive modules or complex algorithms to exploit
temporal information fully. We designed a temporal aggregation module to
compute the temporal coherence between the current frame and its two previous
frames.
</summary>
    <author>
      <name>Shufeng Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.06890v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.06890v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.13185v1</id>
    <updated>2022-03-24T17:02:43Z</updated>
    <published>2022-03-24T17:02:43Z</published>
    <title>Quantum Motion Segmentation</title>
    <summary>  Motion segmentation is a challenging problem that seeks to identify
independent motions in two or several input images. This paper introduces the
first algorithm for motion segmentation that relies on adiabatic quantum
optimization of the objective function. The proposed method achieves on-par
performance with the state of the art on problem instances which can be mapped
to modern quantum annealers.
</summary>
    <author>
      <name>Federica Arrigoni</name>
    </author>
    <author>
      <name>Willi Menapace</name>
    </author>
    <author>
      <name>Marcel Seelbach Benkner</name>
    </author>
    <author>
      <name>Elisa Ricci</name>
    </author>
    <author>
      <name>Vladislav Golyanik</name>
    </author>
    <link href="http://arxiv.org/abs/2203.13185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.13185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.01081v1</id>
    <updated>2022-04-03T14:28:16Z</updated>
    <published>2022-04-03T14:28:16Z</published>
    <title>Faces: AI Blitz XIII Solutions</title>
    <summary>  AI Blitz XIII Faces challenge hosted on www.aicrowd.com platform consisted of
five problems: Sentiment Classification, Age Prediction, Mask Prediction, Face
Recognition, and Face De-Blurring. Our team GLaDOS took second place. Here we
present our solutions and results. Code implementation:
https://github.com/ndrwmlnk/ai-blitz-xiii
</summary>
    <author>
      <name>Andrew Melnik</name>
    </author>
    <author>
      <name>Eren Akbulut</name>
    </author>
    <author>
      <name>Jannik Sheikh</name>
    </author>
    <author>
      <name>Kira Loos</name>
    </author>
    <author>
      <name>Michael Buettner</name>
    </author>
    <author>
      <name>Tobias Lenze</name>
    </author>
    <link href="http://arxiv.org/abs/2204.01081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.01081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.06120v1</id>
    <updated>2022-04-13T00:11:45Z</updated>
    <published>2022-04-13T00:11:45Z</published>
    <title>Baseline Computation for Attribution Methods Based on Interpolated
  Inputs</title>
    <summary>  We discuss a way to find a well behaved baseline for attribution methods that
work by feeding a neural network with a sequence of interpolated inputs between
two given inputs. Then, we test it with our novel Riemann-Stieltjes Integrated
Gradient-weighted Class Activation Mapping (RSI-Grad-CAM) attribution method.
</summary>
    <author>
      <name>Miguel Lerma</name>
    </author>
    <author>
      <name>Mirtha Lucas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.06120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.06120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.0214v1</id>
    <updated>2007-05-02T07:32:58Z</updated>
    <published>2007-05-02T07:32:58Z</published>
    <title>Riemannian level-set methods for tensor-valued data</title>
    <summary>  We present a novel approach for the derivation of PDE modeling
curvature-driven flows for matrix-valued data. This approach is based on the
Riemannian geometry of the manifold of Symmetric Positive Definite Matrices
Pos(n).
</summary>
    <author>
      <name>Mourad Zerai</name>
    </author>
    <author>
      <name>Maher Moakher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 03 figures, to be published in the proceedings of SSVM
  2007, LNCS Springer</arxiv:comment>
    <link href="http://arxiv.org/abs/0705.0214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.0214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.5653v1</id>
    <updated>2012-10-20T20:37:22Z</updated>
    <published>2012-10-20T20:37:22Z</published>
    <title>Identifications of concealed weapon in a Human Body</title>
    <summary>  The detection of weapons concealed underneath a person cloths is very much
important to the improvement of the security of the public as well as the
safety of public assets like airports, buildings and railway stations etc.
</summary>
    <author>
      <name>Prof. Samir K. Bandyopadhyay</name>
    </author>
    <author>
      <name>Biswajita Datta</name>
    </author>
    <author>
      <name>Sudipta Roy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, International Journal of Scientific &amp; Engineering Research
  (ISSN 2229-5518) 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.5653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.5653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.01557v3</id>
    <updated>2015-04-19T02:42:16Z</updated>
    <published>2015-03-05T07:06:02Z</published>
    <title>Supervised Discrete Hashing</title>
    <summary>  This paper has been withdrawn by the authour.
</summary>
    <author>
      <name>Fumin Shen</name>
    </author>
    <author>
      <name>Chunhua Shen</name>
    </author>
    <author>
      <name>Wei Liu</name>
    </author>
    <author>
      <name>Heng Tao Shen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the authour since the algorithm is
  being used for patent application</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.01557v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.01557v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08416v1</id>
    <updated>2018-05-22T06:11:00Z</updated>
    <published>2018-05-22T06:11:00Z</published>
    <title>Training Convolutional Networks with Web Images</title>
    <summary>  In this thesis we investigate the effect of using web images to build a large
scale database to be used along a deep learning method for a classification
task. We replicate the ImageNet large scale database (ILSVRC-2012) from images
collected from the web using 4 different download strategies varying: the
search engine, the query and the image resolution. As a deep learning method,
we will choose the Convolutional Neural Network that was very successful with
recognition tasks; the AlexNet.
</summary>
    <author>
      <name>Nizar Massouh</name>
    </author>
    <link href="http://arxiv.org/abs/1805.08416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09421v1</id>
    <updated>2018-05-23T20:57:31Z</updated>
    <published>2018-05-23T20:57:31Z</published>
    <title>Use of symmetric kernels for convolutional neural networks</title>
    <summary>  At this work we introduce horizontally symmetric convolutional kernels for
CNNs which make the network output invariant to horizontal flips of the image.
We also study other types of symmetric kernels which lead to vertical flip
invariance, and approximate rotational invariance. We show that usage of such
kernels acts as regularizer, and improves generalization of the convolutional
neural networks at the cost of more complicated training process.
</summary>
    <author>
      <name>Viacheslav Dudar</name>
    </author>
    <author>
      <name>Vladimir Semenov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICDSIAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.05076v1</id>
    <updated>2018-09-13T17:33:18Z</updated>
    <published>2018-09-13T17:33:18Z</published>
    <title>Computer Vision-aided Atom Tracking in STEM Imaging</title>
    <summary>  To address the SMC'17 data challenge -- "Data mining atomically resolved
images for material properties", we first used the classic "blob detection"
algorithms developed in computer vision to identify all atom centers in each
STEM image frame. With the help of nearest neighbor analysis, we then found and
labeled every atom center common to all the STEM frames and tracked their
movements through the given time interval for both Molybdenum or Selenium
atoms.
</summary>
    <author>
      <name>Yawei Hui</name>
    </author>
    <author>
      <name>Yaohua Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1809.05076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.05076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.06079v1</id>
    <updated>2018-09-17T08:59:22Z</updated>
    <published>2018-09-17T08:59:22Z</published>
    <title>An Integral Pose Regression System for the ECCV2018 PoseTrack Challenge</title>
    <summary>  For the ECCV 2018 PoseTrack Challenge, we present a 3D human pose estimation
system based mainly on the integral human pose regression method. We show a
comprehensive ablation study to examine the key performance factors of the
proposed system. Our system obtains 47mm MPJPE on the CHALL_H80K test dataset,
placing second in the ECCV2018 3D human pose estimation challenge. Code will be
released to facilitate future work.
</summary>
    <author>
      <name>Xiao Sun</name>
    </author>
    <author>
      <name>Chuankang Li</name>
    </author>
    <author>
      <name>Stephen Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1809.06079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.06079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.11066v1</id>
    <updated>2018-09-28T14:42:29Z</updated>
    <published>2018-09-28T14:42:29Z</published>
    <title>Camera Pose Estimation from Sequence of Calibrated Images</title>
    <summary>  In this paper a method for camera pose estimation from a sequence of images
is presented. The method assumes camera is calibrated (intrinsic parameters are
known) which allows to decrease a number of required pairs of corresponding
points compared to uncalibrated case. Our algorithm can be used as a first
stage in a structure from motion stereo reconstruction system.
</summary>
    <author>
      <name>Jacek Komorowski</name>
    </author>
    <author>
      <name>Przemyslaw Rokita</name>
    </author>
    <link href="http://arxiv.org/abs/1809.11066v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.11066v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.00877v1</id>
    <updated>2018-11-23T22:36:36Z</updated>
    <published>2018-11-23T22:36:36Z</published>
    <title>Automatic lesion boundary detection in dermoscopy</title>
    <summary>  This manuscript addresses the problem of the automatic lesion boundary
detection in dermoscopy, using deep neural networks. An approach is based on
the adaptation of the U-net convolutional neural network with skip connections
for lesion boundary segmentation task. I hope this paper could serve, to some
extent, as an experiment of using deep convolutional networks in biomedical
segmentation task and as a guideline of the boundary detection benchmark,
inspiring further attempts and researches.
</summary>
    <author>
      <name>Glib Kechyn</name>
    </author>
    <link href="http://arxiv.org/abs/1812.00877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.00877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.02542v1</id>
    <updated>2018-12-06T14:16:56Z</updated>
    <published>2018-12-06T14:16:56Z</published>
    <title>Computer Vision for Autonomous Vehicles</title>
    <summary>  In this work, we try to implement Image Processing techniques in the area of
autonomous vehicles, both indoor and outdoor. The challenges for both are
different and the ways to tackle them vary too. We also showed deep learning
makes things easier and precise. We also made base models for all the problems
we tackle while building an autonomous car for Indian Institute of Space
science and Technology.
</summary>
    <author>
      <name>Rohit Gandikota</name>
    </author>
    <link href="http://arxiv.org/abs/1812.02542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.02542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.10915v1</id>
    <updated>2018-12-28T07:51:08Z</updated>
    <published>2018-12-28T07:51:08Z</published>
    <title>Spatiotemporal Data Fusion for Precipitation Nowcasting</title>
    <summary>  Precipitation nowcasting using neural networks and ground-based radars has
become one of the key components of modern weather prediction services, but it
is limited to the regions covered by ground-based radars. Truly global
precipitation nowcasting requires fusion of radar and satellite observations.
We propose the data fusion pipeline based on computer vision techniques,
including novel inpainting algorithm with soft masking.
</summary>
    <author>
      <name>Vladimir Ivashkin</name>
    </author>
    <author>
      <name>Vadim Lebedev</name>
    </author>
    <link href="http://arxiv.org/abs/1812.10915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.10915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.05893v1</id>
    <updated>2019-06-13T18:59:29Z</updated>
    <published>2019-06-13T18:59:29Z</published>
    <title>IntrinSeqNet: Learning to Estimate the Reflectance from Varying
  Illumination</title>
    <summary>  This article has been removed by arXiv administrators because the submitter
did not have the rights to agree to the license at the time of submission
</summary>
    <author>
      <name>Grégoire Nieto</name>
    </author>
    <author>
      <name>Mohammad Rouhani</name>
    </author>
    <author>
      <name>Philippe Robert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article has been removed by arXiv administrators because the
  submitter did not have the rights to agree to the license at the time of
  submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.05893v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05893v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.07016v1</id>
    <updated>2019-06-14T08:39:52Z</updated>
    <published>2019-06-14T08:39:52Z</published>
    <title>Trimmed Action Recognition, Dense-Captioning Events in Videos, and
  Spatio-temporal Action Localization with Focus on ActivityNet Challenge 2019</title>
    <summary>  This notebook paper presents an overview and comparative analysis of our
systems designed for the following three tasks in ActivityNet Challenge 2019:
trimmed action recognition, dense-captioning events in videos, and
spatio-temporal action localization.
</summary>
    <author>
      <name>Zhaofan Qiu</name>
    </author>
    <author>
      <name>Dong Li</name>
    </author>
    <author>
      <name>Yehao Li</name>
    </author>
    <author>
      <name>Qi Cai</name>
    </author>
    <author>
      <name>Yingwei Pan</name>
    </author>
    <author>
      <name>Ting Yao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1807.00686,
  arXiv:1710.08011</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.07016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.07016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.08866v1</id>
    <updated>2019-09-19T08:54:51Z</updated>
    <published>2019-09-19T08:54:51Z</published>
    <title>Challenging deep image descriptors for retrieval in heterogeneous
  iconographic collections</title>
    <summary>  This article proposes to study the behavior of recent and efficient
state-of-the-art deep-learning based image descriptors for content-based image
retrieval, facing a panel of complex variations appearing in heterogeneous
image datasets, in particular in cultural collections that may involve
multi-source, multi-date and multi-view Permission to make digital
</summary>
    <author>
      <name>Dimitri Gominski</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaSTIG</arxiv:affiliation>
    </author>
    <author>
      <name>Martyna Poreba</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaSTIG</arxiv:affiliation>
    </author>
    <author>
      <name>Valérie Gouet-Brunet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaSTIG</arxiv:affiliation>
    </author>
    <author>
      <name>Liming Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaSTIG</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3347317.3357246</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3347317.3357246" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SUMAC '19, 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.08866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.08866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.05841v1</id>
    <updated>2020-01-13T08:36:10Z</updated>
    <published>2020-01-13T08:36:10Z</published>
    <title>Predicting population neural activity in the Algonauts challenge using
  end-to-end trained Siamese networks and group convolutions</title>
    <summary>  The Algonauts challenge is about predicting the object representations in the
form of Representational Dissimilarity Matrices (RDMS) derived from visual
brain regions. We used a customized deep learning model using the concept of
Siamese networks and group convolutions to predict neural distances
corresponding to a pair of images. Training data was best explained by
distances computed over the last layer.
</summary>
    <author>
      <name>Georgin Jacob</name>
    </author>
    <author>
      <name>Harish Katti</name>
    </author>
    <link href="http://arxiv.org/abs/2001.05841v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05841v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.05865v1</id>
    <updated>2020-01-15T08:20:54Z</updated>
    <published>2020-01-15T08:20:54Z</published>
    <title>Ensemble based discriminative models for Visual Dialog Challenge 2018</title>
    <summary>  This manuscript describes our approach for the Visual Dialog Challenge 2018.
We use an ensemble of three discriminative models with different encoders and
decoders for our final submission. Our best performing model on 'test-std'
split achieves the NDCG score of 55.46 and the MRR value of 63.77, securing
third position in the challenge.
</summary>
    <author>
      <name>Shubham Agarwal</name>
    </author>
    <author>
      <name>Raghav Goyal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Rankings: https://visualdialog.org/challenge/2018#winners</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.05865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.00168v2</id>
    <updated>2020-10-21T11:10:31Z</updated>
    <published>2020-08-01T04:31:11Z</published>
    <title>Land Cover Classification from Remote Sensing Images Based on
  Multi-Scale Fully Convolutional Network</title>
    <summary>  In this paper, a Multi-Scale Fully Convolutional Network (MSFCN) with
multi-scale convolutional kernel is proposed to exploit discriminative
representations from two-dimensional (2D) satellite images.
</summary>
    <author>
      <name>Rui Li</name>
    </author>
    <author>
      <name>Shunyi Zheng</name>
    </author>
    <author>
      <name>Chenxi Duan</name>
    </author>
    <author>
      <name>Ce Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/10095020.2021.2017237</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/10095020.2021.2017237" rel="related"/>
    <link href="http://arxiv.org/abs/2008.00168v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.00168v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.05336v1</id>
    <updated>2020-08-12T14:13:53Z</updated>
    <published>2020-08-12T14:13:53Z</published>
    <title>Image-based Portrait Engraving</title>
    <summary>  This paper describes a simple image-based method that applies engraving
stylisation to portraits using ordered dithering. Face detection is used to
estimate a rough proxy geometry of the head consisting of a cylinder, which is
used to warp the dither matrix, causing the engraving lines to curve around the
face for better stylisation. Finally, an application of the approach to colour
engraving is demonstrated.
</summary>
    <author>
      <name>Paul L. Rosin</name>
    </author>
    <author>
      <name>Yu-Kun Lai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.05336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.05336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.10236v1</id>
    <updated>2020-08-24T07:37:12Z</updated>
    <published>2020-08-24T07:37:12Z</published>
    <title>Strawberry Detection using Mixed Training on Simulated and Real Data</title>
    <summary>  This paper demonstrates how simulated images can be useful for object
detection tasks in the agricultural sector, where labeled data can be scarce
and costly to collect. We consider training on mixed datasets with real and
simulated data for strawberry detection in real images. Our results show that
using the real dataset augmented by the simulated dataset resulted in slightly
higher accuracy.
</summary>
    <author>
      <name>Sunny Goondram</name>
    </author>
    <author>
      <name>Akansel Cosgun</name>
    </author>
    <author>
      <name>Dana Kulic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">DICTA 2020 Short Paper Track</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.10236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.10236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.05132v1</id>
    <updated>2020-08-24T05:45:20Z</updated>
    <published>2020-08-24T05:45:20Z</published>
    <title>1st Place Solution to Google Landmark Retrieval 2020</title>
    <summary>  This paper presents the 1st place solution to the Google Landmark Retrieval
2020 Competition on Kaggle. The solution is based on metric learning to
classify numerous landmark classes, and uses transfer learning with two train
datasets, fine-tuning on bigger images, adjusting loss weight for cleaner
samples, and esemble to enhance the model's performance further. Finally, it
scored 0.38677 mAP@100 on the private leaderboard.
</summary>
    <author>
      <name>SeungKee Jeon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.05132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.05132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.09703v1</id>
    <updated>2020-09-21T09:18:19Z</updated>
    <published>2020-09-21T09:18:19Z</published>
    <title>The High-Quality Wide Multi-Channel Attack (HQ-WMCA) database</title>
    <summary>  The High-Quality Wide Multi-Channel Attack database (HQ-WMCA) database
extends the previous Wide Multi-Channel Attack database(WMCA), with more
channels including color, depth, thermal, infrared (spectra), and short-wave
infrared (spectra), and also a wide variety of attacks.
</summary>
    <author>
      <name>Zohreh Mostaani</name>
    </author>
    <author>
      <name>Anjith George</name>
    </author>
    <author>
      <name>Guillaume Heusch</name>
    </author>
    <author>
      <name>David Geissbuhler</name>
    </author>
    <author>
      <name>Sebastien Marcel</name>
    </author>
    <link href="http://arxiv.org/abs/2009.09703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.10115v1</id>
    <updated>2020-09-21T18:19:23Z</updated>
    <published>2020-09-21T18:19:23Z</published>
    <title>Extreme compression of grayscale images</title>
    <summary>  Given an grayscale digital image, and a positive integer $n$, how well can we
store the image at a compression ratio of $n:1$?
  In this paper we address the above question in extreme cases when $n&gt;&gt;50$
using "$\mathbf{V}$-variable image compression".
</summary>
    <author>
      <name>Franklin Mendivil</name>
    </author>
    <author>
      <name>Örjan Stenflo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cnsns.2020.105546</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cnsns.2020.105546" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.10115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.10115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="28A80, 68U10, 94A08" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.13793v1</id>
    <updated>2020-09-29T05:58:31Z</updated>
    <published>2020-09-29T05:58:31Z</published>
    <title>A comparison of classical and variational autoencoders for anomaly
  detection</title>
    <summary>  This paper analyzes and compares a classical and a variational autoencoder in
the context of anomaly detection. To better understand their architecture and
functioning, describe their properties and compare their performance, it
explores how they address a simple problem: reconstructing a line with a slope.
</summary>
    <author>
      <name>Fabrizio Patuzzo</name>
    </author>
    <link href="http://arxiv.org/abs/2009.13793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.13793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.05131v1</id>
    <updated>2020-10-11T01:20:55Z</updated>
    <published>2020-10-11T01:20:55Z</published>
    <title>Segmenting Epipolar Line</title>
    <summary>  Identifying feature correspondence between two images is a fundamental
procedure in three-dimensional computer vision. Usually the feature search
space is confined by the epipolar line. Using the cheirality constraint, this
paper finds that the feature search space can be restrained to one of two or
three segments of the epipolar line that are defined by the epipole and a
so-called virtual infinity point.
</summary>
    <author>
      <name>Shengjie Li</name>
    </author>
    <author>
      <name>Qi Cai</name>
    </author>
    <author>
      <name>Yuanxin Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.05131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.05131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.06454v2</id>
    <updated>2020-11-05T05:52:08Z</updated>
    <published>2020-09-21T04:19:27Z</published>
    <title>The DongNiao International Birds 10000 Dataset</title>
    <summary>  DongNiao International Birds 10000 (DIB-10K) is a challenging image dataset
which has more than 10 thousand different types of birds. It was created to
enable the study of machine learning and also ornithology research. DIB-10K
does not own the copyright of these images. It only provides thumbnails of
images, in a way similar to ImageNet.
</summary>
    <author>
      <name>Jian Mei</name>
    </author>
    <author>
      <name>Hao Dong</name>
    </author>
    <link href="http://arxiv.org/abs/2010.06454v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.06454v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.11339v1</id>
    <updated>2020-10-21T22:42:19Z</updated>
    <published>2020-10-21T22:42:19Z</published>
    <title>Voronoi Convolutional Neural Networks</title>
    <summary>  In this technical report, we investigate extending convolutional neural
networks to the setting where functions are not sampled in a grid pattern. We
show that by treating the samples as the average of a function within a cell,
we can find a natural equivalent of most layers used in CNN. We also present an
algorithm for running inference for these models exactly using standard convex
geometry algorithms.
</summary>
    <author>
      <name>Soroosh Yazdani</name>
    </author>
    <author>
      <name>Andrea Tagliasacchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.11339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.11339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.15250v1</id>
    <updated>2020-10-28T21:42:16Z</updated>
    <published>2020-10-28T21:42:16Z</published>
    <title>Semantic video segmentation for autonomous driving</title>
    <summary>  We aim to solve semantic video segmentation in autonomous driving, namely
road detection in real time video, using techniques discussed in (Shelhamer et
al., 2016a). While fully convolutional network gives good result, we show that
the speed can be halved while preserving the accuracy. The test dataset being
used is KITTI, which consists of real footage from Germany's streets.
</summary>
    <author>
      <name>Minh Triet Chau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work was done around 2017. Some minor changes were added</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.15250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.15250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.07230v1</id>
    <updated>2020-11-14T06:55:31Z</updated>
    <published>2020-11-14T06:55:31Z</published>
    <title>TDAsweep: A Novel Dimensionality Reduction Method for Image
  Classification Tasks</title>
    <summary>  One of the most celebrated achievements of modern machine learning technology
is automatic classification of images. However, success is typically achieved
only with major computational costs. Here we introduce TDAsweep, a machine
learning tool aimed at improving the efficiency of automatic classification of
images.
</summary>
    <author>
      <name>Yu-Shih Chen</name>
    </author>
    <author>
      <name>Melissa Goh</name>
    </author>
    <author>
      <name>Norm Matloff</name>
    </author>
    <link href="http://arxiv.org/abs/2011.07230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.07230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.10759v1</id>
    <updated>2020-11-21T10:27:21Z</updated>
    <published>2020-11-21T10:27:21Z</published>
    <title>Visual Recognition of Great Ape Behaviours in the Wild</title>
    <summary>  We propose a first great ape-specific visual behaviour recognition system
utilising deep learning that is capable of detecting nine core ape behaviours.
</summary>
    <author>
      <name>Faizaan Sakib</name>
    </author>
    <author>
      <name>Tilo Burghardt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures, to be published in the proceedings of ICPR 2020
  at the Visual observation and analysis of Vertebrate And Insect Behaviour
  (VAIB) workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.10759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.10759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.13698v1</id>
    <updated>2020-11-27T12:25:24Z</updated>
    <published>2020-11-27T12:25:24Z</published>
    <title>Lightweight U-Net for High-Resolution Breast Imaging</title>
    <summary>  We study the fully convolutional neural networks in the context of malignancy
detection for breast cancer screening. We work on a supervised segmentation
task looking for an acceptable compromise between the precision of the network
and the computational complexity.
</summary>
    <author>
      <name>Mickael Tardy</name>
    </author>
    <author>
      <name>Diana Mateus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Proceedings of iTWIST'20, Paper-ID: 30, Nantes, France, December,
  2-4, 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.13698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.13698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.14761v1</id>
    <updated>2020-11-30T13:15:51Z</updated>
    <published>2020-11-30T13:15:51Z</published>
    <title>How Good MVSNets Are at Depth Fusion</title>
    <summary>  We study the effects of the additional input to deep multi-view stereo
methods in the form of low-quality sensor depth. We modify two state-of-the-art
deep multi-view stereo methods for using with the input depth. We show that the
additional input depth may improve the quality of deep multi-view stereo.
</summary>
    <author>
      <name>Oleg Voynov</name>
    </author>
    <author>
      <name>Aleksandr Safin</name>
    </author>
    <author>
      <name>Savva Ignatyev</name>
    </author>
    <author>
      <name>Evgeny Burnaev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures, 1 table. Accepted to ICMV 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.14761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.14761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.06793v1</id>
    <updated>2021-01-25T17:56:15Z</updated>
    <published>2021-01-25T17:56:15Z</published>
    <title>Unanswerable Questions about Images and Texts</title>
    <summary>  Questions about a text or an image that cannot be answered raise distinctive
issues for an AI. This note discusses the problem of unanswerable questions in
VQA (visual question answering), in QA (visual question answering), and in AI
generally.
</summary>
    <author>
      <name>Ernest Davis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/frai.2020.00051</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/frai.2020.00051" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Frontiers in Artificial Intelligence: Language and Computation.
  July 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2102.06793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.06793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.07455v1</id>
    <updated>2021-02-15T11:06:46Z</updated>
    <published>2021-02-15T11:06:46Z</published>
    <title>Video Analytics on IoT devices</title>
    <summary>  Deep Learning (DL) combined with advanced model optimization methods such as
RC-NN and Edge2Train has enabled offline execution of large networks on the IoT
devices. In this paper, we compare the modern Deep Learning (DL) based video
analytics approaches with the standard Computer Vision (CV) based approaches
and finally, discuss the best-suited approach for video analytics on IoT
devices.
</summary>
    <author>
      <name>Sree Premkumar</name>
    </author>
    <author>
      <name>Vimal Premkumar</name>
    </author>
    <author>
      <name>Rakesh Dhakshinamurthy</name>
    </author>
    <link href="http://arxiv.org/abs/2102.07455v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.07455v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.05186v1</id>
    <updated>2021-07-12T04:02:57Z</updated>
    <published>2021-07-12T04:02:57Z</published>
    <title>Early warning of pedestrians and cyclists</title>
    <summary>  State-of-the-art motor vehicles are able to break for pedestrians in an
emergency. We investigate what it would take to issue an early warning to the
driver so he/she has time to react. We have identified that predicting the
intention of a pedestrian reliably by position is a particularly hard
challenge. This paper describes an early pedestrian warning demonstration
system.
</summary>
    <author>
      <name>Joerg Christian Wolf</name>
    </author>
    <link href="http://arxiv.org/abs/2107.05186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.05186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.06780v1</id>
    <updated>2021-07-03T09:41:53Z</updated>
    <published>2021-07-03T09:41:53Z</published>
    <title>Person-MinkUNet: 3D Person Detection with LiDAR Point Cloud</title>
    <summary>  In this preliminary work we attempt to apply submanifold sparse convolution
to the task of 3D person detection. In particular, we present Person-MinkUNet,
a single-stage 3D person detection network based on Minkowski Engine with U-Net
architecture. The network achieves a 76.4% average precision (AP) on the JRDB
3D detection benchmark.
</summary>
    <author>
      <name>Dan Jia</name>
    </author>
    <author>
      <name>Bastian Leibe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted as an extended abstract in JRDB-ACT Workshop at CVPR21</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.06780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.06780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.08122v1</id>
    <updated>2022-01-20T12:01:06Z</updated>
    <published>2022-01-20T12:01:06Z</published>
    <title>A Computational Model for Machine Thinking</title>
    <summary>  A machine thinking model is proposed in this report based on recent advances
of computer vision and the recent results of neuroscience devoted to brain
understanding. We deliver the result of machine thinking in the form of
sentences of natural-language or drawn sketches either informative or
decisional. This result is obtained from a reasoning performed on new acquired
data and memorized data.
</summary>
    <author>
      <name>Slimane Larabi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Internal report, RIIMA Laboratory</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.08122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.08122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.10522v1</id>
    <updated>2022-01-22T08:21:12Z</updated>
    <published>2022-01-22T08:21:12Z</published>
    <title>Blind Image Deblurring: a Review</title>
    <summary>  This is a review on blind image deblurring. First, we formulate the blind
image deblurring problem and explain why it is challenging. Next, we bring some
psychological and cognitive studies on the way our human vision system deblurs.
Then, relying on several previous reviews, we discuss the topic of metrics and
datasets, which is non-trivial to blind deblurring. Finally, we introduce some
typical optimization-based methods and learning-based methods.
</summary>
    <author>
      <name>Zhengrong Xue</name>
    </author>
    <link href="http://arxiv.org/abs/2201.10522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.10522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.07437v3</id>
    <updated>2023-03-19T13:11:59Z</updated>
    <published>2022-02-09T01:24:36Z</published>
    <title>Mathematical Cookbook for Snapshot Compressive Imaging</title>
    <summary>  The author intends to provide you with a beautiful, elegant, user-friendly
cookbook for mathematics in Snapshot Compressive Imaging (SCI). Currently, the
cookbook is composed of introduction, conventional optimization, and deep
equilibrium models. The latest releases are strongly recommended! For any other
questions, suggestions, or comments, feel free to email the author.
</summary>
    <author>
      <name>Yaping Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.07437v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.07437v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.07572v2</id>
    <updated>2024-12-10T03:57:39Z</updated>
    <published>2022-02-15T16:58:49Z</published>
    <title>On Representation Learning with Feedback</title>
    <summary>  This note complements the author's recent paper "Robust representation
learning with feedback for single image deraining" by providing heuristically
theoretical explanations on the mechanism of representation learning with
feedback, namely an essential merit of the works presented in this recent
article. This note facilitates understanding of key points in the mechanism of
representation learning with feedback.
</summary>
    <author>
      <name>Hao Li</name>
    </author>
    <link href="http://arxiv.org/abs/2202.07572v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.07572v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.13837v1</id>
    <updated>2022-02-28T14:48:51Z</updated>
    <published>2022-02-28T14:48:51Z</published>
    <title>Fuse Local and Global Semantics in Representation Learning</title>
    <summary>  We propose Fuse Local and Global Semantics in Representation Learning (FLAGS)
to generate richer representations. FLAGS aims at extract both global and local
semantics from images to benefit various downstream tasks. It shows promising
results under common linear evaluation protocol. We also conduct detection and
segmentation on PASCAL VOC and COCO to show the representations extracted by
FLAGS are transferable.
</summary>
    <author>
      <name>Yuchi Zhao</name>
    </author>
    <author>
      <name>Yuhao Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2202.13837v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.13837v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.00934v1</id>
    <updated>2022-05-02T14:32:59Z</updated>
    <published>2022-05-02T14:32:59Z</published>
    <title>Assessing unconstrained surgical cuttings in VR using CNNs</title>
    <summary>  We present a Convolutional Neural Network (CNN) suitable to assess
unconstrained surgical cuttings, trained on a dataset created with a data
augmentation technique.
</summary>
    <author>
      <name>Ilias Chrysovergis</name>
    </author>
    <author>
      <name>Manos Kamarianakis</name>
    </author>
    <author>
      <name>Mike Kentros</name>
    </author>
    <author>
      <name>Dimitris Angelis</name>
    </author>
    <author>
      <name>Antonis Protopsaltis</name>
    </author>
    <author>
      <name>George Papagiannakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures, Submitted to the Siggraph '22 Poster Session
  (Vancouver, 8-11 Aug 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.00934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.00934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.06873v1</id>
    <updated>2022-05-13T20:12:12Z</updated>
    <published>2022-05-13T20:12:12Z</published>
    <title>Using Augmented Face Images to Improve Facial Recognition Tasks</title>
    <summary>  We present a framework that uses GAN-augmented images to complement certain
specific attributes, usually underrepresented, for machine learning model
training. This allows us to improve inference quality over those attributes for
the facial recognition tasks.
</summary>
    <author>
      <name>Shuo Cheng</name>
    </author>
    <author>
      <name>Guoxian Song</name>
    </author>
    <author>
      <name>Wan-Chun Ma</name>
    </author>
    <author>
      <name>Chao Wang</name>
    </author>
    <author>
      <name>Linjie Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CHI 2022 Workshop: AI-Generated Characters: Putting Deepfakes to Good
  Use</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.06873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.06873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.02002v1</id>
    <updated>2022-06-04T14:55:24Z</updated>
    <published>2022-06-04T14:55:24Z</published>
    <title>CVNets: High Performance Library for Computer Vision</title>
    <summary>  We introduce CVNets, a high-performance open-source library for training deep
neural networks for visual recognition tasks, including classification,
detection, and segmentation. CVNets supports image and video understanding
tools, including data loading, data transformations, novel data sampling
methods, and implementations of several standard networks with similar or
better performance than previous studies.
  Our source code is available at: \url{https://github.com/apple/ml-cvnets}.
</summary>
    <author>
      <name>Sachin Mehta</name>
    </author>
    <author>
      <name>Farzad Abdolhosseini</name>
    </author>
    <author>
      <name>Mohammad Rastegari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.02002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.02002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.03843v1</id>
    <updated>2022-07-08T11:54:37Z</updated>
    <published>2022-07-08T11:54:37Z</published>
    <title>Continuous Methods : Hamiltonian Domain Translation</title>
    <summary>  This paper proposes a novel approach to domain translation. Leveraging
established parallels between generative models and dynamical systems, we
propose a reformulation of the Cycle-GAN architecture. By embedding our model
with a Hamiltonian structure, we obtain a continuous, expressive and most
importantly invertible generative model for domain translation.
</summary>
    <author>
      <name>Emmanuel Menier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LISN, Inria, IRT SystemX</arxiv:affiliation>
    </author>
    <author>
      <name>Michele Alessandro Bucci</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Mouadh Yagoubi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRT SystemX</arxiv:affiliation>
    </author>
    <author>
      <name>Lionel Mathelin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LISN</arxiv:affiliation>
    </author>
    <author>
      <name>Marc Schoenauer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria, LISN</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2207.03843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.03843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.06553v1</id>
    <updated>2022-07-13T23:25:30Z</updated>
    <published>2022-07-13T23:25:30Z</published>
    <title>QML for Argoverse 2 Motion Forecasting Challenge</title>
    <summary>  To safely navigate in various complex traffic scenarios, autonomous driving
systems are generally equipped with a motion forecasting module to provide
vital information for the downstream planning module. For the real-world
onboard applications, both accuracy and latency of a motion forecasting model
are essential. In this report, we present an effective and efficient solution,
which ranks the 3rd place in the Argoverse 2 Motion Forecasting Challenge 2022.
</summary>
    <author>
      <name>Tong Su</name>
    </author>
    <author>
      <name>Xishun Wang</name>
    </author>
    <author>
      <name>Xiaodong Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2207.06553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.06553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.10201v1</id>
    <updated>2022-07-20T21:38:47Z</updated>
    <published>2022-07-20T21:38:47Z</published>
    <title>Hybrid CNN-Transformer Model For Facial Affect Recognition In the ABAW4
  Challenge</title>
    <summary>  This paper describes our submission to the fourth Affective Behavior Analysis
(ABAW) competition. We proposed a hybrid CNN-Transformer model for the
Multi-Task-Learning (MTL) and Learning from Synthetic Data (LSD) task.
Experimental results on validation dataset shows that our method achieves
better performance than baseline model, which verifies that the effectiveness
of proposed network.
</summary>
    <author>
      <name>Lingfeng Wang</name>
    </author>
    <author>
      <name>Haocheng Li</name>
    </author>
    <author>
      <name>Chunyin Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2207.10201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.10201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.11329v1</id>
    <updated>2022-07-22T20:45:05Z</updated>
    <published>2022-07-22T20:45:05Z</published>
    <title>Video Swin Transformers for Egocentric Video Understanding @ Ego4D
  Challenges 2022</title>
    <summary>  We implemented Video Swin Transformer as a base architecture for the tasks of
Point-of-No-Return temporal localization and Object State Change
Classification. Our method achieved competitive performance on both challenges.
</summary>
    <author>
      <name>Maria Escobar</name>
    </author>
    <author>
      <name>Laura Daza</name>
    </author>
    <author>
      <name>Cristina González</name>
    </author>
    <author>
      <name>Jordi Pont-Tuset</name>
    </author>
    <author>
      <name>Pablo Arbeláez</name>
    </author>
    <link href="http://arxiv.org/abs/2207.11329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.11329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.08863v1</id>
    <updated>2022-08-03T01:42:49Z</updated>
    <published>2022-08-03T01:42:49Z</published>
    <title>Compressive Self-localization Using Relative Attribute Embedding</title>
    <summary>  The use of relative attribute (e.g., beautiful, safe, convenient) -based
image embeddings in visual place recognition, as a domain-adaptive compact
image descriptor that is orthogonal to the typical approach of absolute
attribute (e.g., color, shape, texture) -based image embeddings, is explored in
this paper.
</summary>
    <author>
      <name>Ryogo Yamamoto</name>
    </author>
    <author>
      <name>Kanji Tanaka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures, An extended abstract version of a manuscript
  submitted to an international conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.08863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.08863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.10497v2</id>
    <updated>2023-01-26T07:26:02Z</updated>
    <published>2022-09-21T16:53:04Z</published>
    <title>Animating Still Images</title>
    <summary>  We present a method for imparting motion to a still 2D image. Our method uses
deep learning to segment a section of the image denoted as subject, then uses
in-painting to complete the background, and finally adds animation to the
subject by embedding the image in a triangle mesh, while preserving the rest of
the image.
</summary>
    <author>
      <name>Kushagr Batra</name>
    </author>
    <author>
      <name>Mridul Kavidayal</name>
    </author>
    <link href="http://arxiv.org/abs/2209.10497v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.10497v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.12330v1</id>
    <updated>2022-09-25T22:03:39Z</updated>
    <published>2022-09-25T22:03:39Z</published>
    <title>Personalizing Text-to-Image Generation via Aesthetic Gradients</title>
    <summary>  This work proposes aesthetic gradients, a method to personalize a
CLIP-conditioned diffusion model by guiding the generative process towards
custom aesthetics defined by the user from a set of images. The approach is
validated with qualitative and quantitative experiments, using the recent
stable diffusion model and several aesthetically-filtered datasets. Code is
released at https://github.com/vicgalle/stable-diffusion-aesthetic-gradients
</summary>
    <author>
      <name>Victor Gallego</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to NeurIPS 2022 Machine Learning for Creativity and Design
  Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.12330v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.12330v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.09296v1</id>
    <updated>2022-10-14T12:26:13Z</updated>
    <published>2022-10-14T12:26:13Z</published>
    <title>3rd Place Solution for Google Universal Image Embedding</title>
    <summary>  This paper presents the 3rd place solution to the Google Universal Image
Embedding Competition on Kaggle. We use ViT-H/14 from OpenCLIP for the backbone
of ArcFace, and trained in 2 stage. 1st stage is done with freezed backbone,
and 2nd stage is whole model training. We achieve 0.692 mean Precision @5 on
private leaderboard. Code available at
https://github.com/YasumasaNamba/google-universal-image-embedding
</summary>
    <author>
      <name>Nobuaki Aoki</name>
    </author>
    <author>
      <name>Yasumasa Namba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.09296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.09296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.10594v1</id>
    <updated>2022-10-16T06:36:10Z</updated>
    <published>2022-10-16T06:36:10Z</published>
    <title>Motion-Based Weak Supervision for Video Parsing with Application to
  Colonoscopy</title>
    <summary>  We propose a two-stage unsupervised approach for parsing videos into phases.
We use motion cues to divide the video into coarse segments. Noisy segment
labels are then used to weakly supervise an appearance-based classifier. We
show the effectiveness of the method for phase detection in colonoscopy videos.
</summary>
    <author>
      <name>Ori Kelner</name>
    </author>
    <author>
      <name>Or Weinstein</name>
    </author>
    <author>
      <name>Ehud Rivlin</name>
    </author>
    <author>
      <name>Roman Goldenberg</name>
    </author>
    <link href="http://arxiv.org/abs/2210.10594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.10594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.12417v2</id>
    <updated>2022-11-10T09:06:27Z</updated>
    <published>2022-10-22T11:17:30Z</published>
    <title>SLAMs: Semantic Learning based Activation Map for Weakly Supervised
  Semantic Segmentation</title>
    <summary>  Recent mainstream weakly-supervised semantic segmentation (WSSS) approaches
mainly relies on image-level classification learning, which has limited
representation capacity. In this paper, we propose a novel semantic learning
based framework, named SLAMs (Semantic Learning based Activation Map), for
WSSS.
</summary>
    <author>
      <name>Junliang Chen</name>
    </author>
    <author>
      <name>Xiaodong Zhao</name>
    </author>
    <author>
      <name>Minmin Liu</name>
    </author>
    <author>
      <name>Linlin Shen</name>
    </author>
    <link href="http://arxiv.org/abs/2210.12417v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.12417v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.15286v1</id>
    <updated>2022-11-18T16:05:15Z</updated>
    <published>2022-11-18T16:05:15Z</published>
    <title>Masked Autoencoders for Egocentric Video Understanding @ Ego4D Challenge
  2022</title>
    <summary>  In this report, we present our approach and empirical results of applying
masked autoencoders in two egocentric video understanding tasks, namely, Object
State Change Classification and PNR Temporal Localization, of Ego4D Challenge
2022. As team TheSSVL, we ranked 2nd place in both tasks. Our code will be made
available.
</summary>
    <author>
      <name>Jiachen Lei</name>
    </author>
    <author>
      <name>Shuang Ma</name>
    </author>
    <author>
      <name>Zhongjie Ba</name>
    </author>
    <author>
      <name>Sai Vemprala</name>
    </author>
    <author>
      <name>Ashish Kapoor</name>
    </author>
    <author>
      <name>Kui Ren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.15286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.15286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.09027v1</id>
    <updated>2022-12-18T07:36:32Z</updated>
    <published>2022-12-18T07:36:32Z</published>
    <title>2D Pose Estimation based Child Action Recognition</title>
    <summary>  We present a graph convolutional network with 2D pose estimation for the
first time on child action recognition task achieving on par results with an
RGB modality based model on a novel benchmark dataset containing unconstrained
environment based videos.
</summary>
    <author>
      <name>Sanka Mohottala</name>
    </author>
    <author>
      <name>Sandun Abeygunawardana</name>
    </author>
    <author>
      <name>Pradeepa Samarasinghe</name>
    </author>
    <author>
      <name>Dharshana Kasthurirathna</name>
    </author>
    <author>
      <name>Charith Abhayaratne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper Accepted for the IEEE TENCON Conference (2022). 7 pages, 5
  figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.09027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.09027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.13810v1</id>
    <updated>2022-12-28T13:06:30Z</updated>
    <published>2022-12-28T13:06:30Z</published>
    <title>All's well that FID's well? Result quality and metric scores in GAN
  models for lip-sychronization tasks</title>
    <summary>  We test the performance of GAN models for lip-synchronization. For this, we
reimplement LipGAN in Pytorch, train it on the dataset GRID and compare it to
our own variation, L1WGAN-GP, adapted to the LipGAN architecture and also
trained on GRID.
</summary>
    <author>
      <name>Carina Geldhauser</name>
    </author>
    <author>
      <name>Johan Liljegren</name>
    </author>
    <author>
      <name>Pontus Nordqvist</name>
    </author>
    <link href="http://arxiv.org/abs/2212.13810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.13810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.06936v1</id>
    <updated>2022-12-12T21:24:03Z</updated>
    <published>2022-12-12T21:24:03Z</published>
    <title>The use of Octree in point cloud analysis with application to cultural
  heritage</title>
    <summary>  In this article we present the effects of our work on the subject of the
technical approach to the 3D point cloud data analysis through the use of the
Octree method to compress, analyse and compute the initial data.
</summary>
    <author>
      <name>Rafał Bieńkowski</name>
    </author>
    <author>
      <name>Krzysztof E. Rutkowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 12 figures, 7 citations</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.06936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.06936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; E.2; J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.07947v1</id>
    <updated>2023-01-19T08:47:31Z</updated>
    <published>2023-01-19T08:47:31Z</published>
    <title>Point Cloud Data Simulation and Modelling with Aize Workspace</title>
    <summary>  This work takes a look at data models often used in digital twins and
presents preliminary results specifically from surface reconstruction and
semantic segmentation models trained using simulated data. This work is
expected to serve as a ground work for future endeavours in data
contextualisation inside a digital twin.
</summary>
    <author>
      <name>Boris Mocialov</name>
    </author>
    <author>
      <name>Eirik Eythorsson</name>
    </author>
    <author>
      <name>Reza Parseh</name>
    </author>
    <author>
      <name>Hoang Tran</name>
    </author>
    <author>
      <name>Vegard Flovik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract, Northern Lights Deep Learning Conference, 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.07947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.07947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.12185v1</id>
    <updated>2023-02-02T19:19:10Z</updated>
    <published>2023-02-02T19:19:10Z</published>
    <title>Scaling Up Computer Vision Neural Networks Using Fast Fourier Transform</title>
    <summary>  Deep Learning-based Computer Vision field has recently been trying to explore
larger kernels for convolution to effectively scale up Convolutional Neural
Networks. Simultaneously, new paradigm of models such as Vision Transformers
find it difficult to scale up to larger higher resolution images due to their
quadratic complexity in terms of input sequence. In this report, Fast Fourier
Transform is utilised in various ways to provide some solutions to these
issues.
</summary>
    <author>
      <name>Siddharth Agrawal</name>
    </author>
    <link href="http://arxiv.org/abs/2302.12185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.12185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.04208v1</id>
    <updated>2023-03-07T19:58:57Z</updated>
    <published>2023-03-07T19:58:57Z</published>
    <title>EscherNet 101</title>
    <summary>  A deep learning model, EscherNet 101, is constructed to categorize images of
2D periodic patterns into their respective 17 wallpaper groups. Beyond
evaluating EscherNet 101 performance by classification rates, at a micro-level
we investigate the filters learned at different layers in the network, capable
of capturing second-order invariants beyond edge and curvature.
</summary>
    <author>
      <name>Christopher Funk</name>
    </author>
    <author>
      <name>Yanxi Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 page, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.04208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.04208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="20-08 Computational methods for problems pertaining to group theory" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.12727v1</id>
    <updated>2023-03-13T11:31:35Z</updated>
    <published>2023-03-13T11:31:35Z</published>
    <title>A XGBoost Algorithm-based Fatigue Recognition Model Using Face Detection</title>
    <summary>  As fatigue is normally revealed in the eyes and mouth of a person's face,
this paper tried to construct a XGBoost Algorithm-Based fatigue recognition
model using the two indicators, EAR (Eye Aspect Ratio) and MAR(Mouth Aspect
Ratio). With an accuracy rate of 87.37% and sensitivity rate of 89.14%, the
model was proved to be efficient and valid for further applications.
</summary>
    <author>
      <name>Xinrui Chen</name>
    </author>
    <author>
      <name>Bingquan Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages;2 fiqures</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.12727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.12727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.12946v1</id>
    <updated>2023-03-01T22:36:54Z</updated>
    <published>2023-03-01T22:36:54Z</published>
    <title>Underwater Camouflage Object Detection Dataset</title>
    <summary>  We have made a dataset of camouflage object detection mainly for complex
seabed scenes, and named it UnderWater RGB&amp;Sonar,or UW-RS for short. The UW-RS
dataset contains a total of 1972 image data. The dataset mainly consists of two
parts, namely underwater optical data part (UW-R dataset) and underwater sonar
data part (UW-S dataset).
</summary>
    <author>
      <name>Feng Dong</name>
    </author>
    <author>
      <name>Jinchao Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2303.12946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.12946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.02766v1</id>
    <updated>2023-04-05T22:14:53Z</updated>
    <published>2023-04-05T22:14:53Z</published>
    <title>Shape complexity estimation using VAE</title>
    <summary>  In this paper, we compare methods for estimating the complexity of
two-dimensional shapes and introduce a method that exploits reconstruction loss
of Variational Autoencoders with different sizes of latent vectors. Although
complexity of a shape is not a well defined attribute, different aspects of it
can be estimated. We demonstrate that our methods captures some aspects of
shape complexity. Code and training details will be publicly available.
</summary>
    <author>
      <name>Markus Rothgaenger</name>
    </author>
    <author>
      <name>Andrew Melnik</name>
    </author>
    <author>
      <name>Helge Ritter</name>
    </author>
    <link href="http://arxiv.org/abs/2304.02766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.02766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.11923v2</id>
    <updated>2023-09-18T12:30:10Z</updated>
    <published>2023-04-24T09:06:06Z</published>
    <title>Improving Knowledge Distillation via Transferring Learning Ability</title>
    <summary>  Existing knowledge distillation methods generally use a teacher-student
approach, where the student network solely learns from a well-trained teacher.
However, this approach overlooks the inherent differences in learning abilities
between the teacher and student networks, thus causing the capacity-gap
problem. To address this limitation, we propose a novel method called SLKD.
</summary>
    <author>
      <name>Long Liu</name>
    </author>
    <author>
      <name>Tong Li</name>
    </author>
    <author>
      <name>Hui Cheng</name>
    </author>
    <link href="http://arxiv.org/abs/2304.11923v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.11923v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.14485v1</id>
    <updated>2023-03-10T12:58:16Z</updated>
    <published>2023-03-10T12:58:16Z</published>
    <title>Inter-sphere consistency-based method for camera-projector pair
  calibration</title>
    <summary>  We construct constraints from consistency between estimated parameters from
different spheres, termed inter-sphere consistency. It facilitates more
flexible calibration using only two spheres, which has been considered a
challenging and not well addressed ill-posed problem.
</summary>
    <author>
      <name>Zhaoshuai Qi</name>
    </author>
    <author>
      <name>Jingqi Pang</name>
    </author>
    <author>
      <name>Yifeng Hao</name>
    </author>
    <author>
      <name>Yanning Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages,1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.14485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.14485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.00204v1</id>
    <updated>2023-04-29T09:10:32Z</updated>
    <published>2023-04-29T09:10:32Z</published>
    <title>CARLA-BSP: a simulated dataset with pedestrians</title>
    <summary>  We present a sample dataset featuring pedestrians generated using the ARCANE
framework, a new framework for generating datasets in CARLA (0.9.13). We
provide use cases for pedestrian detection, autoencoding, pose estimation, and
pose lifting. We also showcase baseline results. For more information, visit
https://project-arcane.eu/.
</summary>
    <author>
      <name>Maciej Wielgosz</name>
    </author>
    <author>
      <name>Antonio M. López</name>
    </author>
    <author>
      <name>Muhammad Naveed Riaz</name>
    </author>
    <link href="http://arxiv.org/abs/2305.00204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.00204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.17786v1</id>
    <updated>2023-05-28T18:17:31Z</updated>
    <published>2023-05-28T18:17:31Z</published>
    <title>Real-time Object Detection: YOLOv1 Re-Implementation in PyTorch</title>
    <summary>  Real-time object detection is a crucial problem to solve when in comes to
computer vision systems that needs to make appropriate decision based on
detection in a timely manner. I have chosen the YOLO v1 architecture to
implement it using PyTorch framework, with goal to familiarize with entire
object detection pipeline I attempted different techniques to modify the
original architecture to improve the results. Finally, I compare the metrics of
my implementation to the original.
</summary>
    <author>
      <name>Michael Shenoda</name>
    </author>
    <link href="http://arxiv.org/abs/2305.17786v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.17786v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.00360v2</id>
    <updated>2023-07-22T23:32:49Z</updated>
    <published>2023-06-01T05:40:58Z</published>
    <title>How Do ConvNets Understand Image Intensity?</title>
    <summary>  Convolutional Neural Networks (ConvNets) usually rely on edge/shape
information to classify images. Visualization methods developed over the last
decade confirm that ConvNets rely on edge information. We investigate
situations where the ConvNet needs to rely on image intensity in addition to
shape. We show that the ConvNet relies on image intensity information using
visualization.
</summary>
    <author>
      <name>Jackson Kaunismaa</name>
    </author>
    <author>
      <name>Michael Guerzhoy</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR Tiny Papers 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2306.00360v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.00360v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.15445v2</id>
    <updated>2023-07-16T13:49:21Z</updated>
    <published>2023-06-27T13:02:24Z</published>
    <title>UniUD Submission to the EPIC-Kitchens-100 Multi-Instance Retrieval
  Challenge 2023</title>
    <summary>  In this report, we present the technical details of our submission to the
EPIC-Kitchens-100 Multi-Instance Retrieval Challenge 2023. To participate in
the challenge, we ensembled two models trained with two different loss
functions on 25% of the training data. Our submission, visible on the public
leaderboard, obtains an average score of 56.81% nDCG and 42.63% mAP.
</summary>
    <author>
      <name>Alex Falcon</name>
    </author>
    <author>
      <name>Giuseppe Serra</name>
    </author>
    <link href="http://arxiv.org/abs/2306.15445v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.15445v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.00083v1</id>
    <updated>2023-06-30T18:39:57Z</updated>
    <published>2023-06-30T18:39:57Z</published>
    <title>A Parts Based Registration Loss for Detecting Knee Joint Areas</title>
    <summary>  In this paper, a parts based loss is considered for finetune registering knee
joint areas. Here the parts are defined as abstract feature vectors with
location and they are automatically selected from a reference image. For a test
image the detected parts are encouraged to have a similar spatial configuration
than the corresponding parts in the reference image.
</summary>
    <author>
      <name>Juha Tiirola</name>
    </author>
    <link href="http://arxiv.org/abs/2307.00083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.00083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.00411v1</id>
    <updated>2023-07-01T19:04:11Z</updated>
    <published>2023-07-01T19:04:11Z</published>
    <title>Applications of Binary Similarity and Distance Measures</title>
    <summary>  In the recent past, binary similarity measures have been applied in solving
biometric identification problems, including fingerprint, handwritten character
detection, and in iris image recognition. The application of the relevant
measurements has also resulted in more accurate data analysis. This paper
surveys the applicability of binary similarity and distance measures in various
fields.
</summary>
    <author>
      <name>Manoj Muniswamaiah</name>
    </author>
    <author>
      <name>Tilak Agerwala</name>
    </author>
    <author>
      <name>Charles C. Tappert</name>
    </author>
    <link href="http://arxiv.org/abs/2307.00411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.00411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.05601v1</id>
    <updated>2023-07-10T20:28:58Z</updated>
    <published>2023-07-10T20:28:58Z</published>
    <title>Unsupervised Domain Adaptation with Deep Neural-Network</title>
    <summary>  This report contributes to the field of unsupervised domain adaptation by
providing an analysis of existing methods, introducing a new approach, and
demonstrating the potential for improving visual recognition tasks across
different domains. The results of this study open up opportunities for further
study and development of advanced methods in the field of domain adaptation.
</summary>
    <author>
      <name>Artem Bituitskii</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Master's thesis, 34 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.05601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.05601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.13215v1</id>
    <updated>2023-07-25T02:56:20Z</updated>
    <published>2023-07-25T02:56:20Z</published>
    <title>Image Segmentation Keras : Implementation of Segnet, FCN, UNet, PSPNet
  and other models in Keras</title>
    <summary>  Semantic segmentation plays a vital role in computer vision tasks, enabling
precise pixel-level understanding of images. In this paper, we present a
comprehensive library for semantic segmentation, which contains implementations
of popular segmentation models like SegNet, FCN, UNet, and PSPNet. We also
evaluate and compare these models on several datasets, offering researchers and
practitioners a powerful toolset for tackling diverse segmentation challenges.
</summary>
    <author>
      <name>Divam Gupta</name>
    </author>
    <link href="http://arxiv.org/abs/2307.13215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.13215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.15097v1</id>
    <updated>2023-09-26T17:43:58Z</updated>
    <published>2023-09-26T17:43:58Z</published>
    <title>Case Study: Ensemble Decision-Based Annotation of Unconstrained Real
  Estate Images</title>
    <summary>  We describe a proof-of-concept for annotating real estate images using simple
iterative rule-based semi-supervised learning. In this study, we have gained
important insights into the content characteristics and uniqueness of
individual image classes as well as essential requirements for a practical
implementation.
</summary>
    <author>
      <name>Miroslav Despotovic</name>
    </author>
    <author>
      <name>Zedong Zhang</name>
    </author>
    <author>
      <name>Eric Stumpe</name>
    </author>
    <author>
      <name>Matthias Zeppelzauer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.15097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.15097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.10517v1</id>
    <updated>2023-10-16T15:33:58Z</updated>
    <published>2023-10-16T15:33:58Z</published>
    <title>Distribution prediction for image compression: An experimental
  re-compressor for JPEG images</title>
    <summary>  We propose a new scheme to re-compress JPEG images in a lossless way. Using a
JPEG image as an input the algorithm partially decodes the signal to obtain
quantized DCT coefficients and then re-compress them in a more effective way.
</summary>
    <author>
      <name>Maxim Koroteev</name>
    </author>
    <author>
      <name>Yaroslav Borisov</name>
    </author>
    <author>
      <name>Pavel Frolov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.10517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.10517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.07357v1</id>
    <updated>2023-12-12T15:26:06Z</updated>
    <published>2023-12-12T15:26:06Z</published>
    <title>Automatic coral reef fish identification and 3D measurement in the wild</title>
    <summary>  In this paper we present a pipeline using stereo images in order to
automatically identify, track in 3D fish, and measure fish population.
</summary>
    <author>
      <name>Cyril Barrelet</name>
    </author>
    <author>
      <name>Marc Chaumont</name>
    </author>
    <author>
      <name>Gérard Subsol</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is in its draft version and should be improved in order to
  be published. This paper is issued from one Year of Engineering work</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.07357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.07357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.09876v1</id>
    <updated>2023-12-15T15:24:49Z</updated>
    <published>2023-12-15T15:24:49Z</published>
    <title>Automatic Image Colourizer</title>
    <summary>  In this project we have designed and described a model which colourize a
gray-scale image, with no human intervention. We propose a fully automatic
process of colouring and re-colouring faded or gray-scale image with vibrant
and pragmatic colours. We have used Convolutional Neural Network to hallucinate
input images and feed-forwarded by training thousands of images. This approach
results in trailblazing results.
</summary>
    <author>
      <name>Aditya Parikh</name>
    </author>
    <link href="http://arxiv.org/abs/2312.09876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.09876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.09880v1</id>
    <updated>2023-12-15T15:27:41Z</updated>
    <published>2023-12-15T15:27:41Z</published>
    <title>Information Extraction from Unstructured data using Augmented-AI and
  Computer Vision</title>
    <summary>  Process of information extraction (IE) is often used to extract meaningful
information from unstructured and unlabeled data. Conventional methods of data
extraction including application of OCR and passing extraction engine, are
inefficient on large data and have their limitation. In this paper, a peculiar
technique of information extraction is proposed using A2I and computer vision
technologies, which also includes NLP.
</summary>
    <author>
      <name>Aditya Parikh</name>
    </author>
    <link href="http://arxiv.org/abs/2312.09880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.09880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.16987v1</id>
    <updated>2023-12-28T12:15:58Z</updated>
    <published>2023-12-28T12:15:58Z</published>
    <title>Image Quality, Uniformity and Computation Improvement of Compressive
  Light Field Displays with U-Net</title>
    <summary>  We apply the U-Net model for compressive light field synthesis. Compared to
methods based on stacked CNN and iterative algorithms, this method offers
better image quality, uniformity and less computation.
</summary>
    <author>
      <name>Chen Gao</name>
    </author>
    <author>
      <name>Haifeng Li</name>
    </author>
    <author>
      <name>Xu Liu</name>
    </author>
    <author>
      <name>Xiaodi Tan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.16987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.16987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="78-06" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.06167v1</id>
    <updated>2024-01-02T03:49:41Z</updated>
    <published>2024-01-02T03:49:41Z</published>
    <title>Enhancing Multimodal Understanding with CLIP-Based Image-to-Text
  Transformation</title>
    <summary>  The process of transforming input images into corresponding textual
explanations stands as a crucial and complex endeavor within the domains of
computer vision and natural language processing. In this paper, we propose an
innovative ensemble approach that harnesses the capabilities of Contrastive
Language-Image Pretraining models.
</summary>
    <author>
      <name>Chang Che</name>
    </author>
    <author>
      <name>Qunwei Lin</name>
    </author>
    <author>
      <name>Xinyu Zhao</name>
    </author>
    <author>
      <name>Jiaxin Huang</name>
    </author>
    <author>
      <name>Liqiang Yu</name>
    </author>
    <link href="http://arxiv.org/abs/2401.06167v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.06167v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.14675v1</id>
    <updated>2024-01-26T06:40:00Z</updated>
    <published>2024-01-26T06:40:00Z</published>
    <title>Multi-model learning by sequential reading of untrimmed videos for
  action recognition</title>
    <summary>  We propose a new method for learning videos by aggregating multiple models by
sequentially extracting video clips from untrimmed video. The proposed method
reduces the correlation between clips by feeding clips to multiple models in
turn and synchronizes these models through federated learning. Experimental
results show that the proposed method improves the performance compared to the
no synchronization.
</summary>
    <author>
      <name>Kodai Kamiya</name>
    </author>
    <author>
      <name>Toru Tamaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The International Workshop on Frontiers of Computer Vision
  (IW-FCV2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.14675v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.14675v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.04953v1</id>
    <updated>2024-02-07T15:37:17Z</updated>
    <published>2024-02-07T15:37:17Z</published>
    <title>4-Dimensional deformation part model for pose estimation using Kalman
  filter constraints</title>
    <summary>  The main goal of this article is to analyze the effect on pose estimation
accuracy when using a Kalman filter added to 4-dimensional deformation part
model partial solutions. The experiments run with two data sets showing that
this method improves pose estimation accuracy compared with state-of-the-art
methods and that a Kalman filter helps to increase this accuracy.
</summary>
    <author>
      <name>Enrique Martinez-Berti</name>
    </author>
    <author>
      <name>Antonio-Jose Sanchez-Salmeron</name>
    </author>
    <author>
      <name>Carlos Ricolfe-Viala</name>
    </author>
    <link href="http://arxiv.org/abs/2402.04953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.04953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.06212v1</id>
    <updated>2024-02-09T06:44:21Z</updated>
    <published>2024-02-09T06:44:21Z</published>
    <title>Halo Reduction in Display Systems through Smoothed Local Histogram
  Equalization and Human Visual System Modeling</title>
    <summary>  Halo artifacts significantly impact display quality. We propose a method to
reduce halos in Local Histogram Equalization (LHE) algorithms by separately
addressing dark and light variants. This approach results in visually natural
images by exploring the relationship between lateral inhibition and halo
artifacts in the human visual system.
</summary>
    <author>
      <name>Prasoon Ambalathankandy</name>
    </author>
    <author>
      <name>Yafei Ou</name>
    </author>
    <author>
      <name>Masayuki Ikebe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.36463/idw.2023.1488</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.36463/idw.2023.1488" rel="related"/>
    <link href="http://arxiv.org/abs/2402.06212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.06212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.15990v1</id>
    <updated>2024-03-24T02:55:45Z</updated>
    <published>2024-03-24T02:55:45Z</published>
    <title>Mars Spectrometry 2: Gas Chromatography -- Second place solution</title>
    <summary>  The Mars Spectrometry 2: Gas Chromatography challenge was sponsored by NASA
and run on the DrivenData competition platform in 2022. This report describes
the solution which achieved the second-best score on the competition's test
dataset. The solution utilized two-dimensional, image-like representations of
the competition's chromatography data samples. A number of different
Convolutional Neural Network models were trained and ensembled for the final
submission.
</summary>
    <author>
      <name>Dmitry A. Konovalov</name>
    </author>
    <link href="http://arxiv.org/abs/2403.15990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.15990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.00597v1</id>
    <updated>2024-03-31T08:08:05Z</updated>
    <published>2024-03-31T08:08:05Z</published>
    <title>Parameter and Data-Efficient Spectral StyleDCGAN</title>
    <summary>  We present a simple, highly parameter, and data-efficient adversarial network
for unconditional face generation. Our method: Spectral Style-DCGAN or SSD
utilizes only 6.574 million parameters and 4739 dog faces from the Animal Faces
HQ (AFHQ) dataset as training samples while preserving fidelity at low
resolutions up to 64x64. Code available at
https://github.com/Aryan-Garg/StyleDCGAN.
</summary>
    <author>
      <name>Aryan Garg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Notable ICLR Tiny Paper 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.00597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.00597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.10319v1</id>
    <updated>2024-04-16T06:59:26Z</updated>
    <published>2024-04-16T06:59:26Z</published>
    <title>Application of Deep Learning Methods to Processing of Noisy Medical
  Video Data</title>
    <summary>  Cells count become a challenging problem when the cells move in a continuous
stream, and their boundaries are difficult for visual detection. To resolve
this problem we modified the training and decision making processes using
curriculum learning and multi-view predictions techniques, respectively.
</summary>
    <author>
      <name>Danil Afonchikov</name>
    </author>
    <author>
      <name>Elena Kornaeva</name>
    </author>
    <author>
      <name>Irina Makovik</name>
    </author>
    <author>
      <name>Alexey Kornaev</name>
    </author>
    <link href="http://arxiv.org/abs/2404.10319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.10319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.00196v1</id>
    <updated>2024-04-30T20:59:53Z</updated>
    <published>2024-04-30T20:59:53Z</published>
    <title>Synthetic Image Verification in the Era of Generative AI: What Works and
  What Isn't There Yet</title>
    <summary>  In this work we present an overview of approaches for the detection and
attribution of synthetic images and highlight their strengths and weaknesses.
We also point out and discuss hot topics in this field and outline promising
directions for future research.
</summary>
    <author>
      <name>Diangarti Tariang</name>
    </author>
    <author>
      <name>Riccardo Corvi</name>
    </author>
    <author>
      <name>Davide Cozzolino</name>
    </author>
    <author>
      <name>Giovanni Poggi</name>
    </author>
    <author>
      <name>Koki Nagano</name>
    </author>
    <author>
      <name>Luisa Verdoliva</name>
    </author>
    <link href="http://arxiv.org/abs/2405.00196v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.00196v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.05260v1</id>
    <updated>2024-03-18T20:46:35Z</updated>
    <published>2024-03-18T20:46:35Z</published>
    <title>Financial Table Extraction in Image Documents</title>
    <summary>  Table extraction has long been a pervasive problem in financial services.
This is more challenging in the image domain, where content is locked behind
cumbersome pixel format. Luckily, advances in deep learning for image
segmentation, OCR, and sequence modeling provides the necessary heavy lifting
to achieve impressive results. This paper presents an end-to-end pipeline for
identifying, extracting and transcribing tabular content in image documents,
while retaining the original spatial relations with high fidelity.
</summary>
    <author>
      <name>William Watson</name>
    </author>
    <author>
      <name>Bo Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3383455.3422520</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3383455.3422520" rel="related"/>
    <link href="http://arxiv.org/abs/2405.05260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.05260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.11351v1</id>
    <updated>2024-05-18T17:43:50Z</updated>
    <published>2024-05-18T17:43:50Z</published>
    <title>PlantTracing: Tracing Arabidopsis Thaliana Apex with CenterTrack</title>
    <summary>  This work applies an encoder-decoder-based machine learning network to detect
and track the motion and growth of the flowering stem apex of Arabidopsis
Thaliana. Based on the CenterTrack, a machine learning back-end network, we
trained a model based on ten time-lapsed labeled videos and tested against
three videos.
</summary>
    <author>
      <name>Yuanzhe Liu</name>
    </author>
    <author>
      <name>Yixiang Mao</name>
    </author>
    <author>
      <name>Yao Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.11351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.11351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.15932v2</id>
    <updated>2025-02-24T17:10:11Z</updated>
    <published>2024-05-24T20:43:19Z</published>
    <title>Steerable Transformers</title>
    <summary>  In this work we introduce Steerable Transformers, an extension of the Vision
Transformer mechanism that maintains equivariance to the special Euclidean
group $\mathrm{SE}(d)$. We propose an equivariant attention mechanism that
operates on features extracted by steerable convolutions. Operating in Fourier
space, our network utilizes Fourier space non-linearities. Our experiments in
both two and three dimensions show that adding steerable transformer layers to
steerable convolutional networks enhances performance.
</summary>
    <author>
      <name>Soumyabrata Kundu</name>
    </author>
    <author>
      <name>Risi Kondor</name>
    </author>
    <link href="http://arxiv.org/abs/2405.15932v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.15932v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.18587v1</id>
    <updated>2024-06-06T21:02:51Z</updated>
    <published>2024-06-06T21:02:51Z</published>
    <title>Nomic Embed Vision: Expanding the Latent Space</title>
    <summary>  This technical report describes the training of nomic-embed-vision, a highly
performant, open-code, open-weights image embedding model that shares the same
latent space as nomic-embed-text. Together, nomic-embed-vision and
nomic-embed-text form the first unified latent space to achieve high
performance across vision, language, and multimodal tasks.
</summary>
    <author>
      <name>Zach Nussbaum</name>
    </author>
    <author>
      <name>Brandon Duderstadt</name>
    </author>
    <author>
      <name>Andriy Mulyar</name>
    </author>
    <link href="http://arxiv.org/abs/2406.18587v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.18587v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.04265v1</id>
    <updated>2024-07-05T05:38:20Z</updated>
    <published>2024-07-05T05:38:20Z</published>
    <title>Parametric Curve Segment Extraction by Support Regions</title>
    <summary>  We introduce a method to extract curve segments in parametric form from the
image directly using the Laplacian of Gaussian (LoG) filter response. Our
segmentation gives convex and concave curves. To do so, we form curve support
regions by grouping pixels of the thresholded filter response. Then, we model
each support region boundary by Fourier series and extract the corresponding
parametric curve segment.
</summary>
    <author>
      <name>Cem Ünsalan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.04265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.04265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.04592v1</id>
    <updated>2024-07-05T15:40:39Z</updated>
    <published>2024-07-05T15:40:39Z</published>
    <title>Smell and Emotion: Recognising emotions in smell-related artworks</title>
    <summary>  Emotions and smell are underrepresented in digital art history. In this
exploratory work, we show that recognising emotions from smell-related artworks
is technically feasible but has room for improvement. Using style transfer and
hyperparameter optimization we achieve a minor performance boost and open up
the field for future extensions.
</summary>
    <author>
      <name>Vishal Patoliya</name>
    </author>
    <author>
      <name>Mathias Zinnen</name>
    </author>
    <author>
      <name>Andreas Maier</name>
    </author>
    <author>
      <name>Vincent Christlein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.04592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.04592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.05312v1</id>
    <updated>2024-07-07T09:52:04Z</updated>
    <published>2024-07-07T09:52:04Z</published>
    <title>An Improved Method for Personalizing Diffusion Models</title>
    <summary>  Diffusion models have demonstrated impressive image generation capabilities.
Personalized approaches, such as textual inversion and Dreambooth, enhance
model individualization using specific images. These methods enable generating
images of specific objects based on diverse textual contexts. Our proposed
approach aims to retain the model's original knowledge during new information
integration, resulting in superior outcomes while necessitating less training
time compared to Dreambooth and textual inversion.
</summary>
    <author>
      <name>Yan Zeng</name>
    </author>
    <author>
      <name>Masanori Suganuma</name>
    </author>
    <author>
      <name>Takayuki Okatani</name>
    </author>
    <link href="http://arxiv.org/abs/2407.05312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.05312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.18290v1</id>
    <updated>2024-07-11T02:49:13Z</updated>
    <published>2024-07-11T02:49:13Z</published>
    <title>Several questions of visual generation in 2024</title>
    <summary>  This paper does not propose any new algorithms but instead outlines various
problems in the field of visual generation based on the author's personal
understanding. The core of these problems lies in how to decompose visual
signals, with all other issues being closely related to this central problem
and stemming from unsuitable approaches to signal decomposition. This paper
aims to draw researchers' attention to the significance of Visual Signal
Decomposition.
</summary>
    <author>
      <name>Shuyang Gu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.18290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.18290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.07225v1</id>
    <updated>2024-08-01T19:40:55Z</updated>
    <published>2024-08-01T19:40:55Z</published>
    <title>Longitudinal Evaluation of Child Face Recognition and the Impact of
  Underlying Age</title>
    <summary>  The need for reliable identification of children in various emerging
applications has sparked interest in leveraging child face recognition
technology. This study introduces a longitudinal approach to enrollment and
verification accuracy for child face recognition, focusing on the YFA database
collected by Clarkson University CITeR research group over an 8 year period, at
6 month intervals.
</summary>
    <author>
      <name>Surendra Singh</name>
    </author>
    <author>
      <name>Keivan Bahmani</name>
    </author>
    <author>
      <name>Stephanie Schuckers</name>
    </author>
    <link href="http://arxiv.org/abs/2408.07225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.07225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.08529v1</id>
    <updated>2024-08-16T04:57:21Z</updated>
    <published>2024-08-16T04:57:21Z</published>
    <title>Privacy-Preserving Vision Transformer Using Images Encrypted with
  Restricted Random Permutation Matrices</title>
    <summary>  We propose a novel method for privacy-preserving fine-tuning vision
transformers (ViTs) with encrypted images. Conventional methods using encrypted
images degrade model performance compared with that of using plain images due
to the influence of image encryption. In contrast, the proposed encryption
method using restricted random permutation matrices can provide a higher
performance than the conventional ones.
</summary>
    <author>
      <name>Kouki Horio</name>
    </author>
    <author>
      <name>Kiyoshi Nishikawa</name>
    </author>
    <author>
      <name>Hitoshi Kiya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.08529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.08529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.15374v2</id>
    <updated>2024-11-21T23:51:29Z</updated>
    <published>2024-08-27T19:22:06Z</published>
    <title>CycleGAN with Better Cycles</title>
    <summary>  CycleGAN provides a framework to train image-to-image translation with
unpaired datasets using cycle consistency loss [4]. While results are great in
many applications, the pixel level cycle consistency can potentially be
problematic and causes unrealistic images in certain cases. In this project, we
propose three simple modifications to cycle consistency, and show that such an
approach achieves better results with fewer artifacts.
</summary>
    <author>
      <name>Tongzhou Wang</name>
    </author>
    <author>
      <name>Yihan Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.15374v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.15374v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.02448v1</id>
    <updated>2024-09-04T05:06:34Z</updated>
    <published>2024-09-04T05:06:34Z</published>
    <title>Detecting Korean Food Using Image using Hierarchical Model</title>
    <summary>  A solution was made available for Korean Food lovers who have dietary
restrictions to identify the Korean food before consuming. Just by uploading a
clear photo of the dish, people can get to know what they are eating. Image
processing techniques together with machine learning helped to come up with
this solution.
</summary>
    <author>
      <name>Hoang Khanh Lam</name>
    </author>
    <author>
      <name>Kahandakanaththage Maduni Pramuditha Perera</name>
    </author>
    <link href="http://arxiv.org/abs/2409.02448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.02448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.15028v1</id>
    <updated>2024-09-23T13:55:16Z</updated>
    <published>2024-09-23T13:55:16Z</published>
    <title>Region Mixup</title>
    <summary>  This paper introduces a simple extension of mixup (Zhang et al., 2018) data
augmentation to enhance generalization in visual recognition tasks. Unlike the
vanilla mixup method, which blends entire images, our approach focuses on
combining regions from multiple images.
</summary>
    <author>
      <name>Saptarshi Saha</name>
    </author>
    <author>
      <name>Utpal Garain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a Tiny Paper at ICLR 2024</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The Second Tiny Papers Track at ICLR 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2409.15028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.15028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.15997v2</id>
    <updated>2024-09-26T21:56:01Z</updated>
    <published>2024-09-24T11:57:12Z</published>
    <title>Improvements to SDXL in NovelAI Diffusion V3</title>
    <summary>  In this technical report, we document the changes we made to SDXL in the
process of training NovelAI Diffusion V3, our state of the art anime image
generation model.
</summary>
    <author>
      <name>Juan Ossa</name>
    </author>
    <author>
      <name>Eren Doğan</name>
    </author>
    <author>
      <name>Alex Birch</name>
    </author>
    <author>
      <name>F. Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.15997v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.15997v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.05680v1</id>
    <updated>2024-10-08T04:40:16Z</updated>
    <published>2024-10-08T04:40:16Z</published>
    <title>Convolutional neural networks applied to modification of images</title>
    <summary>  The reader will learn how digital images are edited using linear algebra and
calculus. Starting from the concept of filter towards machine learning
techniques such as convolutional neural networks.
</summary>
    <author>
      <name>Carlos I. Aguirre-Velez</name>
    </author>
    <author>
      <name>Jose Antonio Arciniega-Nevarez</name>
    </author>
    <author>
      <name>Eric Dolores-Cuenca</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-93954-0_5-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-93954-0_5-1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In: Sriraman, B. (eds) Handbook of Visual, Experimental and
  Computational Mathematics . Springer, Cham. (2023)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2410.05680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.05680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1; G.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.07125v1</id>
    <updated>2024-09-20T21:41:57Z</updated>
    <published>2024-09-20T21:41:57Z</published>
    <title>A Simplified Positional Cell Type Visualization using Spatially
  Aggregated Clusters</title>
    <summary>  We introduce a novel method for overlaying cell type proportion data onto
tissue images. This approach preserves spatial context while avoiding visual
clutter or excessively obscuring the underlying slide. Our proposed technique
involves clustering the data and aggregating neighboring points of the same
cluster into polygons.
</summary>
    <author>
      <name>Lee Mason</name>
    </author>
    <author>
      <name>Jonas Almeida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">For the Bio+MedVis 2024 redesign challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.07125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.07125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.13871v2</id>
    <updated>2024-11-05T09:07:14Z</updated>
    <published>2024-10-02T12:14:31Z</published>
    <title>Explaining an image classifier with a generative model conditioned by
  uncertainty</title>
    <summary>  We propose to condition a generative model by a given image classifier
uncertainty in order to analyze and explain its behavior. Preliminary
experiments on synthetic data and a corrupted version of MNIST dataset
illustrate the idea.
</summary>
    <author>
      <name>Adrien LeCoz</name>
    </author>
    <author>
      <name>Stéphane Herbin</name>
    </author>
    <author>
      <name>Faouzi Adjed</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Uncertainty meets Explainability | Workshop and Tutorial @
  ECML-PKDD 2023, Sep 2023, Torino, Italy</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2410.13871v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.13871v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.14958v1</id>
    <updated>2024-10-19T03:17:29Z</updated>
    <published>2024-10-19T03:17:29Z</published>
    <title>Neural Radiance Field Image Refinement through End-to-End Sampling Point
  Optimization</title>
    <summary>  Neural Radiance Field (NeRF), capable of synthesizing high-quality novel
viewpoint images, suffers from issues like artifact occurrence due to its fixed
sampling points during rendering. This study proposes a method that optimizes
sampling points to reduce artifacts and produce more detailed images.
</summary>
    <author>
      <name>Kazuhiro Ohta</name>
    </author>
    <author>
      <name>Satoshi Ono</name>
    </author>
    <link href="http://arxiv.org/abs/2410.14958v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.14958v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.18051v1</id>
    <updated>2024-10-23T17:25:26Z</updated>
    <published>2024-10-23T17:25:26Z</published>
    <title>Real time anomalies detection on video</title>
    <summary>  Nowadays, many places use security cameras. Unfortunately, when an incident
occurs, these technologies are used to show past events. So it can be
considered as a deterrence tool than a detection tool. In this article, we will
propose a deep learning approach trying to solve this problematic. This
approach uses convolutional models (CNN) to extract relevant characteristics
linked to the video images, theses characteristics will form times series to be
analyzed by LSTM / GRU models.
</summary>
    <author>
      <name>Fabien Poirier</name>
    </author>
    <link href="http://arxiv.org/abs/2410.18051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.18051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.22777v1</id>
    <updated>2024-10-30T07:46:06Z</updated>
    <published>2024-10-30T07:46:06Z</published>
    <title>Bregman implementation of Meyer's $G-$norm for cartoon + textures
  decomposition</title>
    <summary>  In this paper, we design a very simple algorithm based on Split Bregman
iterations to numerically solve the cartoon + textures decomposition model of
Meyer. This results in a significant gain in speed compared to Chambolle's
nonlinear projectors.
</summary>
    <author>
      <name>Jerome Gilles</name>
    </author>
    <author>
      <name>Stanley Osher</name>
    </author>
    <link href="http://arxiv.org/abs/2410.22777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.22777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.05603v1</id>
    <updated>2024-11-08T14:47:28Z</updated>
    <published>2024-11-08T14:47:28Z</published>
    <title>Efficient Audio-Visual Fusion for Video Classification</title>
    <summary>  We present Attend-Fusion, a novel and efficient approach for audio-visual
fusion in video classification tasks. Our method addresses the challenge of
exploiting both audio and visual modalities while maintaining a compact model
architecture. Through extensive experiments on the YouTube-8M dataset, we
demonstrate that our Attend-Fusion achieves competitive performance with
significantly reduced model complexity compared to larger baseline models.
</summary>
    <author>
      <name>Mahrukh Awan</name>
    </author>
    <author>
      <name>Asmar Nadeem</name>
    </author>
    <author>
      <name>Armin Mustafa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVMP Short Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.05603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.05603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.08878v1</id>
    <updated>2024-11-13T18:55:10Z</updated>
    <published>2024-11-13T18:55:10Z</published>
    <title>A Short Note on Evaluating RepNet for Temporal Repetition Counting in
  Videos</title>
    <summary>  We discuss some consistent issues on how RepNet has been evaluated in various
papers. As a way to mitigate these issues, we report RepNet performance results
on different datasets, and release evaluation code and the RepNet checkpoint to
obtain these results. Code URL:
https://github.com/google-research/google-research/blob/master/repnet/
</summary>
    <author>
      <name>Debidatta Dwibedi</name>
    </author>
    <author>
      <name>Yusuf Aytar</name>
    </author>
    <author>
      <name>Jonathan Tompson</name>
    </author>
    <author>
      <name>Pierre Sermanet</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <link href="http://arxiv.org/abs/2411.08878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.08878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.10705v1</id>
    <updated>2024-11-16T05:17:27Z</updated>
    <published>2024-11-16T05:17:27Z</published>
    <title>Poster: Reliable 3D Reconstruction for Ad-hoc Edge Implementations</title>
    <summary>  Ad-hoc edge deployments to support real-time complex video processing
applications such as, multi-view 3D reconstruction often suffer from
spatio-temporal system disruptions that greatly impact reconstruction quality.
In this poster paper, we present a novel portfolio theory-inspired edge
resource management strategy to ensure reliable multi-view 3D reconstruction by
accounting for possible system disruptions.
</summary>
    <author>
      <name>Md Nurul Absur</name>
    </author>
    <author>
      <name>Swastik Brahma</name>
    </author>
    <author>
      <name>Saptarshi Debroy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 Pages, 2 figures, IEEE SEC 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.10705v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.10705v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.12331v1</id>
    <updated>2024-11-19T08:32:17Z</updated>
    <published>2024-11-19T08:32:17Z</published>
    <title>Accelerating UMAP for Large-Scale Datasets Through Spectral Coarsening</title>
    <summary>  This paper introduces an innovative approach to dramatically accelerate UMAP
using spectral data compression.The proposed method significantly reduces the
size of the dataset, preserving its essential manifold structure through an
advanced spectral compression technique. This allows UMAP to perform much
faster while maintaining the quality of its embeddings. Experiments on
real-world datasets, such as USPS, demonstrate the method's ability to achieve
substantial data reduction without compromising embedding fidelity.
</summary>
    <author>
      <name>Yongyu Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2411.12331v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.12331v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.18314v1</id>
    <updated>2024-11-27T13:10:18Z</updated>
    <published>2024-11-27T13:10:18Z</published>
    <title>Real-time Video Target Tracking Algorithm Utilizing Convolutional Neural
  Networks (CNN)</title>
    <summary>  Thispaperaimstoresearchandimplementa
real-timevideotargettrackingalgorithmbasedon
ConvolutionalNeuralNetworks(CNN),enhancingthe
accuracyandrobustnessoftargettrackingincomplex
scenarios.Addressingthelimitationsoftraditionaltracking
algorithmsinhandlingissuessuchastargetocclusion,morphologicalchanges,andbackgroundinterference,our
approachintegratestargetdetectionandtrackingstrategies.It
continuouslyupdatesthetargetmodelthroughanonline
learningmechanismtoadapttochangesinthetarget's
appearance.Experimentalresultsdemonstratethat,when
dealingwithsituationsinvolvingrapidmotion,partial
occlusion,andcomplexbackgrounds,theproposedalgorithm
exhibitshighertrackingsuccessratesandlowerfailurerates
comparedtoseveralmainstreamtrackingalgorithms.This
studysuccessfullyappliesCNNtoreal-timevideotarget
tracking,improvingtheaccuracyandstabilityofthetracking
algorithmwhilemaintaininghighprocessingspeeds,thus
meetingthedemandsofreal-timeapplications.Thisalgorithm
isexpectedtoprovidenewsolutionsfortargettrackingtasksin
videosurveillanceandintelligenttransportationdomains.
</summary>
    <author>
      <name>Chaoyi Tan</name>
    </author>
    <author>
      <name>Xiangtian Li</name>
    </author>
    <author>
      <name>Xiaobo Wang</name>
    </author>
    <author>
      <name>Zhen Qi</name>
    </author>
    <author>
      <name>Ao Xiang</name>
    </author>
    <link href="http://arxiv.org/abs/2411.18314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.18314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.17517v1</id>
    <updated>2024-12-23T12:30:28Z</updated>
    <published>2024-12-23T12:30:28Z</published>
    <title>Dataset for Real-World Human Action Detection Using FMCW mmWave Radar</title>
    <summary>  Human action detection using privacy-preserving mmWave radar sensors is
studied for its applications in healthcare and home automation. Unlike existing
research, limited to simulations in controlled environments, we present a
real-world mmWave radar dataset with baseline results for human action
detection.
</summary>
    <author>
      <name>Dylan jayabahu</name>
    </author>
    <author>
      <name>Parthipan Siva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in JCVIS (proceedings of 10th Annual Conference on
  Vision and Intelligent Systems)</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.17517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.17517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.00400v1</id>
    <updated>2025-03-01T08:29:31Z</updated>
    <published>2025-03-01T08:29:31Z</published>
    <title>Inteval Analysis for two spherical functions arising from robust
  Perspective-n-Lines problem</title>
    <summary>  This report presents a comprehensive interval analysis of two spherical
functions derived from the robust Perspective-n-Lines (PnL) problem. The study
is motivated by the application of a dimension-reduction technique to achieve
global solutions for the robust PnL problem. We establish rigorous theoretical
results, supported by detailed proofs, and validate our findings through
extensive numerical simulations.
</summary>
    <author>
      <name>Xiang Zheng</name>
    </author>
    <author>
      <name>Haodong Jiang</name>
    </author>
    <author>
      <name>Junfeng Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2503.00400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.00400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06099v1</id>
    <updated>2025-04-08T14:41:42Z</updated>
    <published>2025-04-08T14:41:42Z</published>
    <title>Towards Varroa destructor mite detection using a narrow spectra
  illumination</title>
    <summary>  This paper focuses on the development and modification of a beehive
monitoring device and Varroa destructor detection on the bees with the help of
hyperspectral imagery while utilizing a U-net, semantic segmentation
architecture, and conventional computer vision methods. The main objectives
were to collect a dataset of bees and mites, and propose the computer vision
model which can achieve the detection between bees and mites.
</summary>
    <author>
      <name>Samuel Bielik</name>
    </author>
    <author>
      <name>Simon Bilik</name>
    </author>
    <link href="http://arxiv.org/abs/2504.06099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.17619v1</id>
    <updated>2025-04-24T14:43:55Z</updated>
    <published>2025-04-24T14:43:55Z</published>
    <title>Enhancing CNNs robustness to occlusions with bioinspired filters for
  border completion</title>
    <summary>  We exploit the mathematical modeling of the visual cortex mechanism for
border completion to define custom filters for CNNs. We see a consistent
improvement in performance, particularly in accuracy, when our modified LeNet 5
is tested with occluded MNIST images.
</summary>
    <author>
      <name>Catarina P. Coutinho</name>
    </author>
    <author>
      <name>Aneeqa Merhab</name>
    </author>
    <author>
      <name>Janko Petkovic</name>
    </author>
    <author>
      <name>Ferdinando Zanchetta</name>
    </author>
    <author>
      <name>Rita Fioresi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the 7th International Conference on Geometric Science of
  Information</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.17619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.17619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.03204v2</id>
    <updated>2025-05-07T04:09:12Z</updated>
    <published>2025-05-06T05:38:17Z</published>
    <title>DCS-ST for Classification of Breast Cancer Histopathology Images with
  Limited Annotations</title>
    <summary>  Deep learning methods have shown promise in classifying breast cancer
histopathology images, but their performance often declines with limited
annotated data, a critical challenge in medical imaging due to the high cost
and expertise required for annotations.
</summary>
    <author>
      <name>Liu Suxing</name>
    </author>
    <author>
      <name>Byungwon Min</name>
    </author>
    <link href="http://arxiv.org/abs/2505.03204v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.03204v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.06389v1</id>
    <updated>2025-05-09T19:33:35Z</updated>
    <published>2025-05-09T19:33:35Z</published>
    <title>Deep Learning-Based Robust Optical Guidance for Hypersonic Platforms</title>
    <summary>  Sensor-based guidance is required for long-range platforms. To bypass the
structural limitation of classical registration on reference image framework,
we offer in this paper to encode a stack of images of the scene into a deep
network. Relying on a stack is showed to be relevant on bimodal scene (e.g.
when the scene can or can not be snowy).
</summary>
    <author>
      <name>Adrien Chan-Hon-Tong</name>
    </author>
    <author>
      <name>Aurélien Plyer</name>
    </author>
    <author>
      <name>Baptiste Cadalen</name>
    </author>
    <author>
      <name>Laurent Serre</name>
    </author>
    <link href="http://arxiv.org/abs/2505.06389v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06389v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.17808v1</id>
    <updated>2025-05-23T12:25:13Z</updated>
    <published>2025-05-23T12:25:13Z</published>
    <title>An Attention Infused Deep Learning System with Grad-CAM Visualization
  for Early Screening of Glaucoma</title>
    <summary>  This research work reveals the eye opening wisdom of the hybrid labyrinthine
deep learning models synergy born out of combining a trailblazing convolutional
neural network with a disruptive Vision Transformer, both intertwined together
with a radical Cross Attention module. Here, two high yielding datasets for
artificial intelligence models in detecting glaucoma, namely ACRIMA and
Drishti, are utilized.
</summary>
    <author>
      <name>Ramanathan Swaminathan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages in general IEEE format, 8 figures, 4 tables, pdflatex</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.17808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.17808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0001025v1</id>
    <updated>2000-01-28T14:23:18Z</updated>
    <published>2000-01-28T14:23:18Z</published>
    <title>Computational Geometry Column 38</title>
    <summary>  Recent results on curve reconstruction are described.
</summary>
    <author>
      <name>Joseph O'Rourke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure, 18 refs</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0001025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0001025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3590v1</id>
    <updated>2013-01-16T05:20:01Z</updated>
    <published>2013-01-16T05:20:01Z</published>
    <title>Tree structured sparse coding on cubes</title>
    <summary>  A brief description of tree structured sparse coding on the binary cube.
</summary>
    <author>
      <name>Arthur Szlam</name>
    </author>
    <link href="http://arxiv.org/abs/1301.3590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04311v1</id>
    <updated>2016-07-14T20:44:27Z</updated>
    <published>2016-07-14T20:44:27Z</published>
    <title>Defensive Distillation is Not Robust to Adversarial Examples</title>
    <summary>  We show that defensive distillation is not secure: it is no more resistant to
targeted misclassification attacks than unprotected neural networks.
</summary>
    <author>
      <name>Nicholas Carlini</name>
    </author>
    <author>
      <name>David Wagner</name>
    </author>
    <link href="http://arxiv.org/abs/1607.04311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05549v1</id>
    <updated>2017-01-19T18:43:56Z</updated>
    <published>2017-01-19T18:43:56Z</published>
    <title>Deep Neural Networks - A Brief History</title>
    <summary>  Introduction to deep neural networks and their history.
</summary>
    <author>
      <name>Krzysztof J. Cios</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.15477v1</id>
    <updated>2023-09-27T08:18:04Z</updated>
    <published>2023-09-27T08:18:04Z</published>
    <title>A Tutorial on Uniform B-Spline</title>
    <summary>  This document facilitates understanding of core concepts about uniform
B-spline and its matrix representation.
</summary>
    <author>
      <name>Yi Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2309.15477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.15477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0004012v1</id>
    <updated>2000-04-21T17:32:29Z</updated>
    <published>2000-04-21T17:32:29Z</published>
    <title>Assisted Video Sequences Indexing : Motion Analysis Based on Interest
  Points</title>
    <summary>  This work deals with content-based video indexing. Our viewpoint is
semi-automatic analysis of compressed video. We consider the possible
applications of motion analysis and moving object detection : assisting moving
object indexing, summarising videos, and allowing image and motion queries. We
propose an approach based on interest points. As first results, we test and
compare the stability of different types of interest point detectors in
compressed sequences.
</summary>
    <author>
      <name>Emmanuel Etievent</name>
    </author>
    <author>
      <name>Frank Lebourgeois</name>
    </author>
    <author>
      <name>Jean-Michel Jolion</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">HTML, 8 pages, 6 figures, http://rfv.insa-lyon.fr/~etievent/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Iciap 99, Venezia, 27-29 sept., 1059-1062</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0004012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0004012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.8; I.4.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006047v1</id>
    <updated>2000-06-30T22:17:42Z</updated>
    <published>2000-06-30T22:17:42Z</published>
    <title>Geometric Morphology of Granular Materials</title>
    <summary>  We present a new method to transform the spectral pixel information of a
micrograph into an affine geometric description, which allows us to analyze the
morphology of granular materials. We use spectral and pulse-coupled neural
network based segmentation techniques to generate blobs, and a newly developed
algorithm to extract dilated contours. A constrained Delaunay tesselation of
the contour points results in a triangular mesh. This mesh is the basic
ingredient of the Chodal Axis Transform, which provides a morphological
decomposition of shapes. Such decomposition allows for grain separation and the
efficient computation of the statistical features of granular materials.
</summary>
    <author>
      <name>B. R. Schlei</name>
    </author>
    <author>
      <name>L. Prasad</name>
    </author>
    <author>
      <name>A. N. Skourikhine</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.404821</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.404821" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 9 figures. For more information visit
  http://www.nis.lanl.gov/~bschlei/labvis/index.html</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0006047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10;I.4.6;I.4.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0109116v1</id>
    <updated>2001-09-26T22:14:40Z</updated>
    <published>2001-09-26T22:14:40Z</published>
    <title>Digital Color Imaging</title>
    <summary>  This paper surveys current technology and research in the area of digital
color imaging. In order to establish the background and lay down terminology,
fundamental concepts of color perception and measurement are first presented
us-ing vector-space notation and terminology. Present-day color recording and
reproduction systems are reviewed along with the common mathematical models
used for representing these devices. Algorithms for processing color images for
display and communication are surveyed, and a forecast of research trends is
attempted. An extensive bibliography is provided.
</summary>
    <author>
      <name>Gaurav Sharma</name>
    </author>
    <author>
      <name>H. Joel Trussell</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/83.597268</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/83.597268" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Trans. Image Proc., vol. 6, no. 7, pp. 901-932, Jul. 1997</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0109116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0109116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1;I.4,I.3.3,I.2.10;I.3.7;B.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0201019v1</id>
    <updated>2002-01-22T21:00:35Z</updated>
    <published>2002-01-22T21:00:35Z</published>
    <title>Structure from Motion: Theoretical Foundations of a Novel Approach Using
  Custom Built Invariants</title>
    <summary>  We rephrase the problem of 3D reconstruction from images in terms of
intersections of projections of orbits of custom built Lie groups actions. We
then use an algorithmic method based on moving frames "a la Fels-Olver" to
obtain a fundamental set of invariants of these groups actions. The invariants
are used to define a set of equations to be solved by the points of the 3D
object, providing a new technique for recovering 3D structure from motion.
</summary>
    <author>
      <name>Pierre-Louis Bazin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Brown University</arxiv:affiliation>
    </author>
    <author>
      <name>Mireille Boutin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Brown University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0201019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0201019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.8;I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0303015v1</id>
    <updated>2003-03-18T21:30:36Z</updated>
    <published>2003-03-18T21:30:36Z</published>
    <title>Statistical efficiency of curve fitting algorithms</title>
    <summary>  We study the problem of fitting parametrized curves to noisy data. Under
certain assumptions (known as Cartesian and radial functional models), we
derive asymptotic expressions for the bias and the covariance matrix of the
parameter estimates. We also extend Kanatani's version of the Cramer-Rao lower
bound, which he proved for unbiased estimates only, to more general estimates
that include many popular algorithms (most notably, the orthogonal least
squares and algebraic fits). We then show that the gradient-weighted algebraic
fit is statistically efficient and describe all other statistically efficient
algebraic fits.
</summary>
    <author>
      <name>N. Chernov</name>
    </author>
    <author>
      <name>C. Lesort</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0303015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0303015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.8;I.5.1;I.2.10;G.3;G.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0303024v1</id>
    <updated>2003-03-24T02:36:21Z</updated>
    <published>2003-03-24T02:36:21Z</published>
    <title>Differential Methods in Catadioptric Sensor Design with Applications to
  Panoramic Imaging</title>
    <summary>  We discuss design techniques for catadioptric sensors that realize given
projections. In general, these problems do not have solutions, but approximate
solutions may often be found that are visually acceptable. There are several
methods to approach this problem, but here we focus on what we call the
``vector field approach''. An application is given where a true panoramic
mirror is derived, i.e. a mirror that yields a cylindrical projection to the
viewer without any digital unwarping.
</summary>
    <author>
      <name>R. Andrew Hicks</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0303024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0303024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307046v1</id>
    <updated>2003-07-20T05:18:59Z</updated>
    <published>2003-07-20T05:18:59Z</published>
    <title>A New Analytical Radial Distortion Model for Camera Calibration</title>
    <summary>  Common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
radial distortion model with an easy analytical undistortion formula, which
also belongs to the polynomial approximation category. Experimental results are
presented to show that with this radial distortion model, satisfactory accuracy
is achieved.
</summary>
    <author>
      <name>Lili Ma</name>
    </author>
    <author>
      <name>YangQuan Chen</name>
    </author>
    <author>
      <name>Kevin L. Moore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 Postscript figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0307046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307047v1</id>
    <updated>2003-07-20T05:54:42Z</updated>
    <published>2003-07-20T05:54:42Z</published>
    <title>Rational Radial Distortion Models with Analytical Undistortion Formulae</title>
    <summary>  The common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
class of rational radial distortion models with easy analytical undistortion
formulae. Experimental results are presented to show that with this class of
rational radial distortion models, satisfactory and comparable accuracy is
achieved.
</summary>
    <author>
      <name>Lili Ma</name>
    </author>
    <author>
      <name>YangQuan Chen</name>
    </author>
    <author>
      <name>Kevin L. Moore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0307047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307072v1</id>
    <updated>2003-07-31T19:33:48Z</updated>
    <published>2003-07-31T19:33:48Z</published>
    <title>Camera Calibration: a USU Implementation</title>
    <summary>  The task of camera calibration is to estimate the intrinsic and extrinsic
parameters of a camera model. Though there are some restricted techniques to
infer the 3-D information about the scene from uncalibrated cameras, effective
camera calibration procedures will open up the possibility of using a wide
range of existing algorithms for 3-D reconstruction and recognition.
  The applications of camera calibration include vision-based metrology, robust
visual platooning and visual docking of mobile robots where the depth
information is important.
</summary>
    <author>
      <name>Lili Ma</name>
    </author>
    <author>
      <name>YangQuan Chen</name>
    </author>
    <author>
      <name>Kevin L. Moore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 19 eps figures, source codes are in the codes.m and
  corners.dat</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0307072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0308037v1</id>
    <updated>2003-08-22T10:31:53Z</updated>
    <published>2003-08-22T10:31:53Z</published>
    <title>Distributed and Parallel Net Imaging</title>
    <summary>  A very complex vision system is developed to detect luminosity variations
connected with the discovery of new planets in the Universe. The traditional
imaging system can not manage a so large load. A private net is implemented to
perform an automatic vision and decision architecture. It lets to carry out an
on-line discrimination of interesting events by using two levels of triggers.
This system can even manage many Tbytes of data per day. The architecture
avails itself of a distributed parallel network system based on a maximum of
256 standard workstations with Microsoft Window as OS.
</summary>
    <author>
      <name>G. Iovane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 8 figures, Procedding of NIDays 2003 (sponsored by National
  Instruments), Rome 2003. Winner (2nd classified) of the price "Best
  Application of Measurement and Automation</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0308037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0308037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4,I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0308038v1</id>
    <updated>2003-08-22T18:47:33Z</updated>
    <published>2003-08-22T18:47:33Z</published>
    <title>Image Analysis in Astronomy for very large vision machine</title>
    <summary>  It is developed a very complex system (hardware/software) to detect
luminosity variations connected with the discovery of new planets outside the
Solar System. Traditional imaging approaches are very demanding in terms of
computing time; then, the implementation of an automatic vision and decision
software architecture is presented. It allows to perform an on-line
discrimination of interesting events by using two levels of triggers. A
fundamental challenge was to work with very large CCD camera (even 16k*16k
pixels) in line with very large telescopes. Then, the architecture can use a
distributed parallel network system based on a maximum of 256 standard
workstations.
</summary>
    <author>
      <name>G. Iovane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 9 figures, Proceeding of NIWEEK 2002 (sponsored by National
  Instruments), Austin (Usa), 2002</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0308038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0308038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4,I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0311012v1</id>
    <updated>2003-11-12T19:15:41Z</updated>
    <published>2003-11-12T19:15:41Z</published>
    <title>A rigorous definition of axial lines: ridges on isovist fields</title>
    <summary>  We suggest that 'axial lines' defined by (Hillier and Hanson, 1984) as lines
of uninterrupted movement within urban streetscapes or buildings, appear as
ridges in isovist fields (Benedikt, 1979). These are formed from the maximum
diametric lengths of the individual isovists, sometimes called viewsheds, that
make up these fields (Batty and Rana, 2004). We present an image processing
technique for the identification of lines from ridges, discuss current
strengths and weaknesses of the method, and show how it can be implemented
easily and effectively.
</summary>
    <author>
      <name>Rui Carvalho</name>
    </author>
    <author>
      <name>Michael Batty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0311012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0311012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I2.10; I.4.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402020v1</id>
    <updated>2004-02-11T16:34:16Z</updated>
    <published>2004-02-11T16:34:16Z</published>
    <title>Geometrical Complexity of Classification Problems</title>
    <summary>  Despite encouraging recent progresses in ensemble approaches, classification
methods seem to have reached a plateau in development. Further advances depend
on a better understanding of geometrical and topological characteristics of
point sets in high-dimensional spaces, the preservation of such characteristics
under feature transformations and sampling processes, and their interaction
with geometrical models used in classifiers. We discuss an attempt to measure
such properties from data sets and relate them to classifier accuracies.
</summary>
    <author>
      <name>Tin Kam Ho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 7th Course on Ensemble Methods for Learning
  Machines at the International School on Neural Nets ``E.R. Caianiello''</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404046v1</id>
    <updated>2004-04-22T13:42:48Z</updated>
    <published>2004-04-22T13:42:48Z</published>
    <title>Visualising the structure of architectural open spaces based on shape
  analysis</title>
    <summary>  This paper proposes the application of some well known two-dimensional
geometrical shape descriptors for the visualisation of the structure of
architectural open spaces. The paper demonstrates the use of visibility
measures such as distance to obstacles and amount of visible space to calculate
shape descriptors such as convexity and skeleton of the open space. The aim of
the paper is to indicate a simple, objective and quantifiable approach to
understand the structure of open spaces otherwise impossible due to the complex
construction of built structures.
</summary>
    <author>
      <name>Sanjay Rana</name>
    </author>
    <author>
      <name>Mike Batty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Architectural Computing, 2(1), 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0404046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.5;I.4.8;I.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405093v2</id>
    <updated>2004-10-20T10:07:43Z</updated>
    <published>2004-05-25T11:36:34Z</published>
    <title>Computerized Face Detection and Recognition</title>
    <summary>  This publication presents methods for face detection, analysis and
recognition: fast normalized cross-correlation (fast correlation coefficient)
between multiple templates based face pre-detection method, method for
detection of exact face contour based on snakes and Generalized Gradient Vector
Flow field, method for combining recognition algorithms based on Cumulative
Match Characteristics in order to increase recognition speed and accuracy, and
face recognition method based on Principal Component Analysis of the Wavelet
Packet Decomposition allowing to use PCA - based recognition method with large
number of training images. For all the methods are presented experimental
results and comparisons of speed and accuracy with large face databases.
</summary>
    <author>
      <name>Vytautas Perlibakas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD dissertation summary. 35 pages, 12 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0405093v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405093v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.8; I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405095v1</id>
    <updated>2004-05-25T22:40:42Z</updated>
    <published>2004-05-25T22:40:42Z</published>
    <title>Blind Detection and Compensation of Camera Lens Geometric Distortions</title>
    <summary>  This paper presents a blind detection and compensation technique for camera
lens geometric distortions. The lens distortion introduces higher-order
correlations in the frequency domain and in turn it can be detected using
higher-order spectral analysis tools without assuming any specific calibration
target. The existing blind lens distortion removal method only considered a
single-coefficient radial distortion model. In this paper, two coefficients are
considered to model approximately the geometric distortion. All the models
considered have analytical closed-form inverse formulae.
</summary>
    <author>
      <name>Lili Ma</name>
    </author>
    <author>
      <name>YangQuan Chen</name>
    </author>
    <author>
      <name>Kevin L. Moore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIAM Imaging Science, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0405095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I 4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0503001v1</id>
    <updated>2005-03-01T05:17:33Z</updated>
    <published>2005-03-01T05:17:33Z</published>
    <title>Top-Down Unsupervised Image Segmentation (it sounds like oxymoron, but
  actually it is not)</title>
    <summary>  Pattern recognition is generally assumed as an interaction of two inversely
directed image-processing streams: the bottom-up information details gathering
and localization (segmentation) stream, and the top-down information features
aggregation, association and interpretation (recognition) stream. Inspired by
recent evidence from biological vision research and by the insights of
Kolmogorov Complexity theory, we propose a new, just top-down evolving,
procedure of initial image segmentation. We claim that traditional top-down
cognitive reasoning, which is supposed to guide the segmentation process to its
final result, is not at all a part of the image information content evaluation.
And that initial image segmentation is certainly an unsupervised process. We
present some illustrative examples, which support our claims.
</summary>
    <author>
      <name>Emanuel Diamant</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0503001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0503001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504037v2</id>
    <updated>2006-10-31T14:34:47Z</updated>
    <published>2005-04-11T12:41:19Z</published>
    <title>Bayesian Restoration of Digital Images Employing Markov Chain Monte
  Carlo a Review</title>
    <summary>  A review of Bayesian restoration of digital images based on Monte Carlo
techniques is presented. The topics covered include Likelihood, Prior and
Posterior distributions, Poisson, Binay symmetric channel, and Gaussian channel
models of Likelihood distribution,Ising and Potts spin models of Prior
distribution, restoration of an image through Posterior maximization,
statistical estimation of a true image from Posterior ensembles, Markov Chain
Monte Carlo methods and cluster algorithms.
</summary>
    <author>
      <name>K. P. N. Murthy</name>
    </author>
    <author>
      <name>M. Janani</name>
    </author>
    <author>
      <name>B. Shenbga Priya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages; 16 figures; revised version with several typos removed and
  mistakes in equations corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0504037v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504037v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0604062v1</id>
    <updated>2006-04-14T04:40:29Z</updated>
    <published>2006-04-14T04:40:29Z</published>
    <title>Biologically Inspired Hierarchical Model for Feature Extraction and
  Localization</title>
    <summary>  Feature extraction and matching are among central problems of computer
vision. It is inefficent to search features over all locations and scales.
Neurophysiological evidence shows that to locate objects in a digital image the
human visual system employs visual attention to a specific object while
ignoring others. The brain also has a mechanism to search from coarse to fine.
In this paper, we present a feature extractor and an associated hierarchical
searching model to simulate such processes. With the hierarchical
representation of the object, coarse scanning is done through the matching of
the larger scale and precise localization is conducted through the matching of
the smaller scale. Experimental results justify the proposed model in its
effectiveness and efficiency to localize features.
</summary>
    <author>
      <name>Liang Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0604062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0604062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0606060v1</id>
    <updated>2006-06-13T12:53:45Z</updated>
    <published>2006-06-13T12:53:45Z</published>
    <title>Complex Networks: New Concepts and Tools for Real-Time Imaging and
  Vision</title>
    <summary>  This article discusses how concepts and methods of complex networks can be
applied to real-time imaging and computer vision. After a brief introduction of
complex networks basic concepts, their use as means to represent and
characterize images, as well as for modeling visual saliency, are briefly
described. The possibility to apply complex networks in order to model and
simulate the performance of parallel and distributed computing systems for
performance of visual methods is also proposed.
</summary>
    <author>
      <name>Luciano da Fontoura Costa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0606060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0608115v1</id>
    <updated>2006-08-29T13:24:37Z</updated>
    <published>2006-08-29T13:24:37Z</published>
    <title>Neural Network Clustering Based on Distances Between Objects</title>
    <summary>  We present an algorithm of clustering of many-dimensional objects, where only
the distances between objects are used. Centers of classes are found with the
aid of neuron-like procedure with lateral inhibition. The result of clustering
does not depend on starting conditions. Our algorithm makes it possible to give
an idea about classes that really exist in the empirical data. The results of
computer simulations are presented.
</summary>
    <author>
      <name>Leonid B. Litinskii</name>
    </author>
    <author>
      <name>Dmitry E. Romanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages,4 figures, presentation on ICANN (Athens, Greece, 2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0608115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0608115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0610002v1</id>
    <updated>2006-09-30T08:05:02Z</updated>
    <published>2006-09-30T08:05:02Z</published>
    <title>Conditional Expressions for Blind Deconvolution: Derivative form</title>
    <summary>  We developed novel conditional expressions (CEs) for Lane and Bates' blind
deconvolution. The CEs are given in term of the derivatives of the zero-values
of the z-transform of given images. The CEs make it possible to automatically
detect multiple blur convolved in the given images all at once without
performing any analysis of the zero-sheets of the given images. We illustrate
the multiple blur-detection by the CEs for a model image
</summary>
    <author>
      <name>S. Aogaki</name>
    </author>
    <author>
      <name>I. Moritani</name>
    </author>
    <author>
      <name>T. Sugai</name>
    </author>
    <author>
      <name>F. Takeutchi</name>
    </author>
    <author>
      <name>F. M. Toyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 page, 3 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0610002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0610002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701127v3</id>
    <updated>2007-12-28T15:30:19Z</updated>
    <published>2007-01-20T15:45:03Z</published>
    <title>A novel set of rotationally and translationally invariant features for
  images based on the non-commutative bispectrum</title>
    <summary>  We propose a new set of rotationally and translationally invariant features
for image or pattern recognition and classification. The new features are cubic
polynomials in the pixel intensities and provide a richer representation of the
original image than most existing systems of invariants. Our construction is
based on the generalization of the concept of bispectrum to the
three-dimensional rotation group SO(3), and a projection of the image onto the
sphere.
</summary>
    <author>
      <name>Risi Kondor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The claim that the invariants uniquely determine the original image
  had to be dropped</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0701127v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701127v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.7; I.2.10; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701150v1</id>
    <updated>2007-01-24T15:13:06Z</updated>
    <published>2007-01-24T15:13:06Z</published>
    <title>Contains and Inside relationships within combinatorial Pyramids</title>
    <summary>  Irregular pyramids are made of a stack of successively reduced graphs
embedded in the plane. Such pyramids are used within the segmentation framework
to encode a hierarchy of partitions. The different graph models used within the
irregular pyramid framework encode different types of relationships between
regions. This paper compares different graph models used within the irregular
pyramid framework according to a set of relationships between regions. We also
define a new algorithm based on a pyramid of combinatorial maps which allows to
determine if one region contains the other using only local calculus.
</summary>
    <author>
      <name>Luc Brun</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GREYC</arxiv:affiliation>
    </author>
    <author>
      <name>Walter G. Kropatsch</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRIP</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pattern Recognition 39 (01/04/2006) 515-526</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0701150v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701150v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703088v1</id>
    <updated>2007-03-16T00:18:11Z</updated>
    <published>2007-03-16T00:18:11Z</published>
    <title>Plot 94 in ambiance X-Window</title>
    <summary>  &lt;PLOT &gt; is a collection of routines to draw surfaces, contours and so on. In
this work we are presenting a version, that functions over work stations with
the operative system UNIX, that count with the graphic ambiance X-WINDOW with
the tools XLIB and OSF/MOTIF. This implant was realized for the work stations
DEC 5000-200, DEC IPX, and DEC ALFA of the CINVESTAV (Center of Investigation
and Advanced Studies). Also implanted in SILICON GRAPHICS of the CENAC
(National Center of Calculation of the Polytechnic National Institute
</summary>
    <author>
      <name>Ignacio Vega-Paez</name>
    </author>
    <author>
      <name>Carlos Alberto Hernandez-Hernandez</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings in Information Systems Analysis and Synthesis ISAS
  1995, 5th, International Symposium on Systems Research, Informatics and
  Cybernetics, pp. 135-139, August 16-20, 95, Baden-Baden, Germany</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0703088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.3635v1</id>
    <updated>2007-04-26T22:22:45Z</updated>
    <published>2007-04-26T22:22:45Z</published>
    <title>Rough Sets Computations to Impute Missing Data</title>
    <summary>  Many techniques for handling missing data have been proposed in the
literature. Most of these techniques are overly complex. This paper explores an
imputation technique based on rough set computations. In this paper,
characteristic relations are introduced to describe incompletely specified
decision tables.It is shown that the basic rough set idea of lower and upper
approximations for incompletely specified decision tables may be defined in a
variety of different ways. Empirical results obtained using real data are given
and they provide a valuable and promising insight to the problem of missing
data. Missing data were predicted with an accuracy of up to 99%.
</summary>
    <author>
      <name>Fulufhelo Vincent Nelwamondo</name>
    </author>
    <author>
      <name>Tshilidzi Marwala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0704.3635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.3635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.1926v1</id>
    <updated>2007-06-13T15:15:00Z</updated>
    <published>2007-06-13T15:15:00Z</published>
    <title>Towards understanding and modelling office daily life</title>
    <summary>  Measuring and modeling human behavior is a very complex task. In this paper
we present our initial thoughts on modeling and automatic recognition of some
human activities in an office. We argue that to successfully model human
activities, we need to consider both individual behavior and group dynamics. To
demonstrate these theoretical approaches, we introduce an experimental system
for analyzing everyday activity in our office.
</summary>
    <author>
      <name>Michele Bezzi</name>
    </author>
    <author>
      <name>Robin Groenevelt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, ECHISE 2006 - 2nd International Workshop on Exploiting
  Context Histories in Smart Environments - Infrastructures and Design, 8th
  International Conference of Ubiquitous Computing (Ubicomp 2006), Orange
  County, CA, 17-21 September 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/0706.1926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.1926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.8; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.2432v1</id>
    <updated>2007-08-18T14:36:28Z</updated>
    <published>2007-08-18T14:36:28Z</published>
    <title>A structure from motion inequality</title>
    <summary>  We state an elementary inequality for the structure from motion problem for m
cameras and n points. This structure from motion inequality relates space
dimension, camera parameter dimension, the number of cameras and number points
and global symmetry properties and provides a rigorous criterion for which
reconstruction is not possible with probability 1. Mathematically the
inequality is based on Frobenius theorem which is a geometric incarnation of
the fundamental theorem of linear algebra. The paper also provides a general
mathematical formalism for the structure from motion problem. It includes the
situation the points can move while the camera takes the pictures.
</summary>
    <author>
      <name>Oliver Knill</name>
    </author>
    <author>
      <name>Jose Ramirez-Herran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 22 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0708.2432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.2432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.2438v1</id>
    <updated>2007-08-17T21:36:08Z</updated>
    <published>2007-08-17T21:36:08Z</published>
    <title>On Ullman's theorem in computer vision</title>
    <summary>  Both in the plane and in space, we invert the nonlinear Ullman transformation
for 3 points and 3 orthographic cameras. While Ullman's theorem assures a
unique reconstruction modulo a reflection for 3 cameras and 4 points, we find a
locally unique reconstruction for 3 cameras and 3 points. Explicit
reconstruction formulas allow to decide whether picture data of three cameras
seeing three points can be realized as a point-camera configuration.
</summary>
    <author>
      <name>Oliver Knill</name>
    </author>
    <author>
      <name>Jose Ramirez-Herran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0708.2438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.2438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.2974v1</id>
    <updated>2007-08-22T08:28:02Z</updated>
    <published>2007-08-22T08:28:02Z</published>
    <title>The Fuzzy Vault for fingerprints is Vulnerable to Brute Force Attack</title>
    <summary>  The \textit{fuzzy vault} approach is one of the best studied and well
accepted ideas for binding cryptographic security into biometric
authentication. The vault has been implemented in connection with fingerprint
data by Uludag and Jain. We show that this instance of the vault is vulnerable
to brute force attack. An interceptor of the vault data can recover both secret
and template data using only generally affordable computational resources. Some
possible alternatives are then discussed and it is suggested that cryptographic
security may be preferable to the one - way function approach to biometric
security.
</summary>
    <author>
      <name>Preda Mihailescu</name>
    </author>
    <link href="http://arxiv.org/abs/0708.2974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.2974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.1771v1</id>
    <updated>2007-09-12T08:41:36Z</updated>
    <published>2007-09-12T08:41:36Z</published>
    <title>Variational local structure estimation for image super-resolution</title>
    <summary>  Super-resolution is an important but difficult problem in image/video
processing. If a video sequence or some training set other than the given
low-resolution image is available, this kind of extra information can greatly
aid in the reconstruction of the high-resolution image. The problem is
substantially more difficult with only a single low-resolution image on hand.
The image reconstruction methods designed primarily for denoising is
insufficient for super-resolution problem in the sense that it tends to
oversmooth images with essentially no noise. We propose a new adaptive linear
interpolation method based on variational method and inspired by local linear
embedding (LLE). The experimental result shows that our method avoids the
problem of oversmoothing and preserves image structures well.
</summary>
    <author>
      <name>Heng Lian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0709.1771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.1771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.0243v1</id>
    <updated>2007-10-01T09:18:36Z</updated>
    <published>2007-10-01T09:18:36Z</published>
    <title>High-Order Nonparametric Belief-Propagation for Fast Image Inpainting</title>
    <summary>  In this paper, we use belief-propagation techniques to develop fast
algorithms for image inpainting. Unlike traditional gradient-based approaches,
which may require many iterations to converge, our techniques achieve
competitive results after only a few iterations. On the other hand, while
belief-propagation techniques are often unable to deal with high-order models
due to the explosion in the size of messages, we avoid this problem by
approximating our high-order prior model using a Gaussian mixture. By using
such an approximation, we are able to inpaint images quickly while at the same
time retaining good visual results.
</summary>
    <author>
      <name>Julian John McAuley</name>
    </author>
    <author>
      <name>Tiberio S. Caetano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0710.0243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.0243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.0736v1</id>
    <updated>2007-10-03T08:51:44Z</updated>
    <published>2007-10-03T08:51:44Z</published>
    <title>Colour image segmentation by the vector-valued Allen-Cahn phase-field
  model: a multigrid solution</title>
    <summary>  We propose a new method for the numerical solution of a PDE-driven model for
colour image segmentation and give numerical examples of the results. The
method combines the vector-valued Allen-Cahn phase field equation with initial
data fitting terms. This method is known to be closely related to the
Mumford-Shah problem and the level set segmentation by Chan and Vese. Our
numerical solution is performed using a multigrid splitting of a finite element
space, thereby producing an efficient and robust method for the segmentation of
large images.
</summary>
    <author>
      <name>David A Kay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Oxford University Computational Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>Alessandro Tomasi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Sussex</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2009.2026678</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2009.2026678" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Trans. Im. Proc. 18.10 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.0736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.0736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.6; G.1.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.2037v2</id>
    <updated>2007-10-11T11:32:25Z</updated>
    <published>2007-10-10T15:12:20Z</published>
    <title>An Affinity Propagation Based method for Vector Quantization Codebook
  Design</title>
    <summary>  In this paper, we firstly modify a parameter in affinity propagation (AP) to
improve its convergence ability, and then, we apply it to vector quantization
(VQ) codebook design problem. In order to improve the quality of the resulted
codebook, we combine the improved AP (IAP) with the conventional LBG algorithm
to generate an effective algorithm call IAP-LBG. According to the experimental
results, the proposed method not only enhances the convergence abilities but
also is capable of providing higher-quality codebooks than conventional LBG
method.
</summary>
    <author>
      <name>Wu Jiang</name>
    </author>
    <author>
      <name>Fei Ding</name>
    </author>
    <author>
      <name>Qiao-liang Xiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In this version we make some explaination about the network-support
  similarity</arxiv:comment>
    <link href="http://arxiv.org/abs/0710.2037v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.2037v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.4015v1</id>
    <updated>2007-12-24T17:11:56Z</updated>
    <published>2007-12-24T17:11:56Z</published>
    <title>A Fast Hierarchical Multilevel Image Segmentation Method using Unbiased
  Estimators</title>
    <summary>  This paper proposes a novel method for segmentation of images by hierarchical
multilevel thresholding. The method is global, agglomerative in nature and
disregards pixel locations. It involves the optimization of the ratio of the
unbiased estimators of within class to between class variances. We obtain a
recursive relation at each step for the variances which expedites the process.
The efficacy of the method is shown in a comparison with some well-known
methods.
</summary>
    <author>
      <name>Sreechakra Goparaju</name>
    </author>
    <author>
      <name>Jayadev Acharya</name>
    </author>
    <author>
      <name>Ajoy K. Ray</name>
    </author>
    <author>
      <name>Jaideva C. Goswami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures, submitted to "IEEE Transactions on Pattern
  Analysis and Machine Intelligence"</arxiv:comment>
    <link href="http://arxiv.org/abs/0712.4015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.4015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3285v1</id>
    <updated>2008-02-22T10:48:44Z</updated>
    <published>2008-02-22T10:48:44Z</published>
    <title>Some Aspects of Testing Process for Transport Streams in Digital Video
  Broadcasting</title>
    <summary>  This paper presents some aspects related to the DVB (Digital Video
Broadcasting) investigation. The basic aspects of DVB are presented, with an
emphasis on DVB-T version of standard. The main purpose of this research is to
analyze the way that the transmission of the transport streams is realized in
case of the Terrestrial Digital Video Broadcasting (DVB-T). To accomplish this,
first, Digital Video Broadcasting standard is presented, and then the main
aspects of DVB testing and analysis of the transport streams are investigated.
The paper presents also the results obtained using two programs designed for
DVB analysis: Mosalina and TSA.
</summary>
    <author>
      <name>Radu Arsinte</name>
    </author>
    <author>
      <name>Ciprian Ilioaei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, 3 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Acta Technica Napocensis, Electronics and Telecommunications,
  nr.1/2004 pp.59-74</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.3285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.1046v1</id>
    <updated>2008-04-07T14:47:03Z</updated>
    <published>2008-04-07T14:47:03Z</published>
    <title>Discrete schemes for Gaussian curvature and their convergence</title>
    <summary>  In this paper, several discrete schemes for Gaussian curvature are surveyed.
The convergence property of a modified discrete scheme for the Gaussian
curvature is considered. Furthermore, a new discrete scheme for Gaussian
curvature is resented. We prove that the new scheme converges at the regular
vertex with valence not less than 5. By constructing a counterexample, we also
show that it is impossible for building a discrete scheme for Gaussian
curvature which converges over the regular vertex with valence 4. Finally,
asymptotic errors of several discrete scheme for Gaussian curvature are
compared.
</summary>
    <author>
      <name>Zhiqiang Xu</name>
    </author>
    <author>
      <name>Guoliang Xu</name>
    </author>
    <link href="http://arxiv.org/abs/0804.1046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.1046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.1448v1</id>
    <updated>2008-04-09T10:06:15Z</updated>
    <published>2008-04-09T10:06:15Z</published>
    <title>Fast k Nearest Neighbor Search using GPU</title>
    <summary>  The recent improvements of graphics processing units (GPU) offer to the
computer vision community a powerful processing platform. Indeed, a lot of
highly-parallelizable computer vision problems can be significantly accelerated
using GPU architecture. Among these algorithms, the k nearest neighbor search
(KNN) is a well-known problem linked with many applications such as
classification, estimation of statistical properties, etc. The main drawback of
this task lies in its computation burden, as it grows polynomially with the
data size. In this paper, we show that the use of the NVIDIA CUDA API
accelerates the search for the KNN up to a factor of 120.
</summary>
    <author>
      <name>Vincent Garcia</name>
    </author>
    <author>
      <name>Eric Debreuve</name>
    </author>
    <author>
      <name>Michel Barlaud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2figures, submitted to CVGPU 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.1448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.1448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.2324v1</id>
    <updated>2008-05-15T13:15:08Z</updated>
    <published>2008-05-15T13:15:08Z</published>
    <title>A multilateral filtering method applied to airplane runway image</title>
    <summary>  By considering the features of the airport runway image filtering, an
improved bilateral filtering method was proposed which can remove noise with
edge preserving. Firstly the steerable filtering decomposition is used to
calculate the sub-band parameters of 4 orients, and the texture feature matrix
is then obtained from the sub-band local median energy. The texture similar,
the spatial closer and the color similar functions are used to filter the
image.The effect of the weighting function parameters is qualitatively analyzed
also. In contrast with the standard bilateral filter and the simulation results
for the real airport runway image show that the multilateral filtering is more
effective than the standard bilateral filtering.
</summary>
    <author>
      <name>Zhang Yu</name>
    </author>
    <author>
      <name>Shi Zhong-ke</name>
    </author>
    <author>
      <name>Wang Run-quan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/0805.2324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.2324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.0870v1</id>
    <updated>2008-06-04T22:58:41Z</updated>
    <published>2008-06-04T22:58:41Z</published>
    <title>The Euler-Poincare theory of Metamorphosis</title>
    <summary>  In the pattern matching approach to imaging science, the process of
``metamorphosis'' is template matching with dynamical templates. Here, we
recast the metamorphosis equations of into the Euler-Poincare variational
framework of and show that the metamorphosis equations contain the equations
for a perfect complex fluid \cite{Ho2002}. This result connects the ideas
underlying the process of metamorphosis in image matching to the physical
concept of order parameter in the theory of complex fluids. After developing
the general theory, we reinterpret various examples, including point set, image
and density metamorphosis. We finally discuss the issue of matching measures
with metamorphosis, for which we provide existence theorems for the initial and
boundary value problems.
</summary>
    <author>
      <name>Darryl D. Holm</name>
    </author>
    <author>
      <name>Alain Trouve</name>
    </author>
    <author>
      <name>Laurent Younes</name>
    </author>
    <link href="http://arxiv.org/abs/0806.0870v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.0870v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.1446v1</id>
    <updated>2008-06-08T10:15:04Z</updated>
    <published>2008-06-08T10:15:04Z</published>
    <title>Fast Wavelet-Based Visual Classification</title>
    <summary>  We investigate a biologically motivated approach to fast visual
classification, directly inspired by the recent work of Serre et al.
Specifically, trading-off biological accuracy for computational efficiency, we
explore using wavelet and grouplet-like transforms to parallel the tuning of
visual cortex V1 and V2 cells, alternated with max operations to achieve scale
and translation invariance. A feature selection procedure is applied during
learning to accelerate recognition. We introduce a simple attention-like
feedback mechanism, significantly improving recognition and robustness in
multiple-object scenes. In experiments, the proposed algorithm achieves or
exceeds state-of-the-art success rate on object recognition, texture and
satellite image classification, language identification and sound
classification.
</summary>
    <author>
      <name>Guoshen Yu</name>
    </author>
    <author>
      <name>Jean-Jacques Slotine</name>
    </author>
    <link href="http://arxiv.org/abs/0806.1446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.1446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.2006v2</id>
    <updated>2012-01-06T20:39:14Z</updated>
    <published>2008-06-12T06:42:07Z</published>
    <title>Fusion de classifieurs pour la classification d'images sonar</title>
    <summary>  In this paper, we present some high level information fusion approaches for
numeric and symbolic data. We study the interest of such method particularly
for classifier fusion. A comparative study is made in a context of sea bed
characterization from sonar images. The classi- fication of kind of sediment is
a difficult problem because of the data complexity. We compare high level
information fusion and give the obtained performance.
</summary>
    <author>
      <name>Arnaud Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">E3I2</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Revue Nationale des Technologies de l'Information E, 5 (2005)
  259-268</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0806.2006v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.2006v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.2007v1</id>
    <updated>2008-06-12T06:44:55Z</updated>
    <published>2008-06-12T06:44:55Z</published>
    <title>Experts Fusion and Multilayer Perceptron Based on Belief Learning for
  Sonar Image Classification</title>
    <summary>  The sonar images provide a rapid view of the seabed in order to characterize
it. However, in such as uncertain environment, real seabed is unknown and the
only information we can obtain, is the interpretation of different human
experts, sometimes in conflict. In this paper, we propose to manage this
conflict in order to provide a robust reality for the learning step of
classification algorithms. The classification is conducted by a multilayer
perceptron, taking into account the uncertainty of the reality in the learning
stage. The results of this seabed characterization are presented on real sonar
images.
</summary>
    <author>
      <name>Arnaud Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">E3I2</arxiv:affiliation>
    </author>
    <author>
      <name>Christophe Osswald</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">E3I2</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Information &amp; Communication Technologies:
  from Theory to Applications (ICTTA), Damascus : Syrie (2008)</arxiv:comment>
    <link href="http://arxiv.org/abs/0806.2007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.2007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4; I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.3887v1</id>
    <updated>2008-06-24T13:34:15Z</updated>
    <published>2008-06-24T13:34:15Z</published>
    <title>Conceptualization of seeded region growing by pixels aggregation. Part
  2: how to localize a final partition invariant about the seeded region
  initialisation order</title>
    <summary>  In the previous paper, we have conceptualized the localization and the
organization of seeded region growing by pixels aggregation (SRGPA) but we do
not give the issue when there is a collision between two distinct regions
during the growing process. In this paper, we propose two implementations to
manage two classical growing processes: one without a boundary region region to
divide the other regions and another with. Unfortunately, as noticed by Mehnert
and Jakway (1997), this partition depends on the seeded region initialisation
order (SRIO). We propose a growing process, invariant about SRIO such as the
boundary region is the set of ambiguous pixels.
</summary>
    <author>
      <name>Vincent Tariel</name>
    </author>
    <link href="http://arxiv.org/abs/0806.3887v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.3887v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.3928v1</id>
    <updated>2008-06-24T17:02:47Z</updated>
    <published>2008-06-24T17:02:47Z</published>
    <title>Conceptualization of seeded region growing by pixels aggregation. Part
  3: a wide range of algorithms</title>
    <summary>  In the two previous papers of this serie, we have created a library, called
Population, dedicated to seeded region growing by pixels aggregation and we
have proposed different growing processes to get a partition with or without a
boundary region to divide the other regions or to get a partition invariant
about the seeded region initialisation order. Using this work, we implement
some algorithms belonging to the field of SRGPA using this library and these
growing processes.
</summary>
    <author>
      <name>Vincent Tariel</name>
    </author>
    <link href="http://arxiv.org/abs/0806.3928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.3928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.2928v1</id>
    <updated>2008-07-18T11:23:27Z</updated>
    <published>2008-07-18T11:23:27Z</published>
    <title>Visual Grouping by Neural Oscillators</title>
    <summary>  Distributed synchronization is known to occur at several scales in the brain,
and has been suggested as playing a key functional role in perceptual grouping.
State-of-the-art visual grouping algorithms, however, seem to give
comparatively little attention to neural synchronization analogies. Based on
the framework of concurrent synchronization of dynamic systems, simple networks
of neural oscillators coupled with diffusive connections are proposed to solve
visual grouping problems. Multi-layer algorithms and feedback mechanisms are
also studied. The same algorithm is shown to achieve promising results on
several classical visual grouping problems, including point clustering, contour
integration and image segmentation.
</summary>
    <author>
      <name>Guoshen Yu</name>
    </author>
    <author>
      <name>Jean-Jacques Slotine</name>
    </author>
    <link href="http://arxiv.org/abs/0807.2928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.2928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.4701v1</id>
    <updated>2008-07-29T16:28:44Z</updated>
    <published>2008-07-29T16:28:44Z</published>
    <title>An image processing analysis of skin textures</title>
    <summary>  Colour and coarseness of skin are visually different. When image processing
is involved in the skin analysis, it is important to quantitatively evaluate
such differences using texture features. In this paper, we discuss a texture
analysis and measurements based on a statistical approach to the pattern
recognition. Grain size and anisotropy are evaluated with proper diagrams. The
possibility to determine the presence of pattern defects is also discussed.
</summary>
    <author>
      <name>A. Sparavigna</name>
    </author>
    <author>
      <name>R. Marazzato</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/j.1600-0846.2009.00413.x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/j.1600-0846.2009.00413.x" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Skin Research and Technology, Volume 16 Issue 2, Pages 161 - 167,
  2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0807.4701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.4701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.3690v1</id>
    <updated>2008-09-22T12:08:24Z</updated>
    <published>2008-09-22T12:08:24Z</published>
    <title>Modeling and Control with Local Linearizing Nadaraya Watson Regression</title>
    <summary>  Black box models of technical systems are purely descriptive. They do not
explain why a system works the way it does. Thus, black box models are
insufficient for some problems. But there are numerous applications, for
example, in control engineering, for which a black box model is absolutely
sufficient. In this article, we describe a general stochastic framework with
which such models can be built easily and fully automated by observation.
Furthermore, we give a practical example and show how this framework can be
used to model and control a motorcar powertrain.
</summary>
    <author>
      <name>Steffen Kühn</name>
    </author>
    <author>
      <name>Clemens Gühmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.3690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.3690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.3418v1</id>
    <updated>2008-10-19T18:04:51Z</updated>
    <published>2008-10-19T18:04:51Z</published>
    <title>Detecting the Most Unusual Part of a Digital Image</title>
    <summary>  The purpose of this paper is to introduce an algorithm that can detect the
most unusual part of a digital image. The most unusual part of a given shape is
defined as a part of the image that has the maximal distance to all non
intersecting shapes with the same form.
  The method can be used to scan image databases with no clear model of the
interesting part or large image databases, as for example medical databases.
</summary>
    <author>
      <name>K. Koroutchev</name>
    </author>
    <author>
      <name>E. Korutcheva</name>
    </author>
    <link href="http://arxiv.org/abs/0810.3418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.3418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.0221v2</id>
    <updated>2009-09-14T10:14:32Z</updated>
    <published>2009-02-02T10:41:53Z</published>
    <title>Over-enhancement Reduction in Local Histogram Equalization using its
  Degrees of Freedom</title>
    <summary>  A well-known issue of local (adaptive) histogram equalization (LHE) is
over-enhancement (i.e., generation of spurious details) in homogenous areas of
the image. In this paper, we show that the LHE problem has many solutions due
to the ambiguity in ranking pixels with the same intensity. The LHE solution
space can be searched for the images having the maximum PSNR or structural
similarity (SSIM) with the input image. As compared to the results of the prior
art, these solutions are more similar to the input image while offering the
same local contrast.
  Index Terms: histogram modification or specification, contrast enhancement
</summary>
    <author>
      <name>Alireza Avanaki</name>
    </author>
    <link href="http://arxiv.org/abs/0902.0221v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.0221v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.0538v1</id>
    <updated>2009-03-03T14:08:24Z</updated>
    <published>2009-03-03T14:08:24Z</published>
    <title>Real-time Texture Error Detection</title>
    <summary>  This paper advocates an improved solution for real-time error detection of
texture errors that occurs in the production process in textile industry. The
research is focused on the mono-color products with 3D texture model (Jaquard
fabrics). This is a more difficult task than, for example, 2D multicolor
textures.
</summary>
    <author>
      <name>Dan Laurentiu Lacrama</name>
    </author>
    <author>
      <name>Florin Alexa</name>
    </author>
    <author>
      <name>Adriana Balta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, exposed on 2nd "European conference on Computer Science &amp;
  Applications" - XA2008, Timisoara, Romania</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus, Comp. Sci. Series 6 (2008), 127-134</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0903.0538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.0538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.1448v1</id>
    <updated>2009-03-09T08:06:09Z</updated>
    <published>2009-03-09T08:06:09Z</published>
    <title>The Digital Restoration of Da Vinci's Sketches</title>
    <summary>  A sketch, found in one of Leonardo da Vinci's notebooks and covered by the
written notes of this genius, has been recently restored. The restoration
reveals a possible self-portrait of the artist, drawn when he was young. Here,
we discuss the discovery of this self-portrait and the procedure used for
restoration. Actually, this is a restoration performed on the digital image of
the sketch, a procedure that can easily extended and applied to ancient
documents for studies of art and palaeography.
</summary>
    <author>
      <name>Amelia Sparavigna</name>
    </author>
    <link href="http://arxiv.org/abs/0903.1448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.1448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.3676v1</id>
    <updated>2009-03-23T17:18:08Z</updated>
    <published>2009-03-23T17:18:08Z</published>
    <title>Combinatorial Ricci Curvature and Laplacians for Image Processing</title>
    <summary>  A new Combinatorial Ricci curvature and Laplacian operators for grayscale
images are introduced and tested on 2D synthetic, natural and medical images.
Analogue formulae for voxels are also obtained. These notions are based upon
more general concepts developed by R. Forman. Further applications, in
particular a fitting Ricci flow, are discussed.
</summary>
    <author>
      <name>Emil Saucan</name>
    </author>
    <author>
      <name>Eli Appleboilm</name>
    </author>
    <author>
      <name>Gershon Wolansky</name>
    </author>
    <author>
      <name>Yehoshua Y. Zeevi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures (some of the these may be of lesser quality than
  those in the Technical report version)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of CISP'09, Vol. 2, 992-997, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0903.3676v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.3676v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.5045v1</id>
    <updated>2009-03-30T06:00:15Z</updated>
    <published>2009-03-30T06:00:15Z</published>
    <title>Digital Restoration of Ancient Papyri</title>
    <summary>  Image processing can be used for digital restoration of ancient papyri, that
is, for a restoration performed on their digital images. The digital
manipulation allows reducing the background signals and enhancing the
readability of texts. In the case of very old and damaged documents, this is
fundamental for identification of the patterns of letters. Some examples of
restoration, obtained with an image processing which uses edges detection and
Fourier filtering, are shown. One of them concerns 7Q5 fragment of the Dead Sea
Scrolls.
</summary>
    <author>
      <name>Amelia Sparavigna</name>
    </author>
    <link href="http://arxiv.org/abs/0903.5045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.5045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.0962v1</id>
    <updated>2009-04-06T16:25:08Z</updated>
    <published>2009-04-06T16:25:08Z</published>
    <title>Color Dipole Moments for Edge Detection</title>
    <summary>  Dipole and higher moments are physical quantities used to describe a charge
distribution. In analogy with electromagnetism, it is possible to define the
dipole moments for a gray-scale image, according to the single aspect of a
gray-tone map. In this paper we define the color dipole moments for color
images. For color maps in fact, we have three aspects, the three primary
colors, to consider. Associating three color charges to each pixel, color
dipole moments can be easily defined and used for edge detection.
</summary>
    <author>
      <name>Amelia Sparavigna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0904.0962v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.0962v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.1613v1</id>
    <updated>2009-04-09T22:15:25Z</updated>
    <published>2009-04-09T22:15:25Z</published>
    <title>On the closed-form solution of the rotation matrix arising in computer
  vision problems</title>
    <summary>  We show the closed-form solution to the maximization of trace(A'R), where A
is given and R is unknown rotation matrix. This problem occurs in many computer
vision tasks involving optimal rotation matrix estimation. The solution has
been continuously reinvented in different fields as part of specific problems.
We summarize the historical evolution of the problem and present the general
proof of the solution. We contribute to the proof by considering the degenerate
cases of A and discuss the uniqueness of R.
</summary>
    <author>
      <name>Andriy Myronenko</name>
    </author>
    <author>
      <name>Xubo Song</name>
    </author>
    <link href="http://arxiv.org/abs/0904.1613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.1613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.3944v1</id>
    <updated>2009-04-24T21:19:59Z</updated>
    <published>2009-04-24T21:19:59Z</published>
    <title>Better Global Polynomial Approximation for Image Rectification</title>
    <summary>  When using images to locate objects, there is the problem of correcting for
distortion and misalignment in the images. An elegant way of solving this
problem is to generate an error correcting function that maps points in an
image to their corrected locations. We generate such a function by fitting a
polynomial to a set of sample points. The objective is to identify a polynomial
that passes "sufficiently close" to these points with "good" approximation of
intermediate points. In the past, it has been difficult to achieve good global
polynomial approximation using only sample points. We report on the development
of a global polynomial approximation algorithm for solving this problem. Key
Words: Polynomial approximation, interpolation, image rectification.
</summary>
    <author>
      <name>Christopher O. Ward</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2316/Journal.205.2008.3.205-4669</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2316/Journal.205.2008.3.205-4669" rel="related"/>
    <link href="http://arxiv.org/abs/0904.3944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.3944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.3347v1</id>
    <updated>2009-05-20T16:37:16Z</updated>
    <published>2009-05-20T16:37:16Z</published>
    <title>Information Distance in Multiples</title>
    <summary>  Information distance is a parameter-free similarity measure based on
compression, used in pattern recognition, data mining, phylogeny, clustering,
and classification. The notion of information distance is extended from pairs
to multiples (finite lists). We study maximal overlap, metricity, universality,
minimal overlap, additivity, and normalized information distance in multiples.
We use the theoretical notion of Kolmogorov complexity which for practical
purposes is approximated by the length of the compressed version of the file
involved, using a real-world compression program.
  {\em Index Terms}-- Information distance, multiples, pattern recognition,
data mining, similarity, Kolmogorov complexity
</summary>
    <author>
      <name>Paul M. B. Vitanyi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LateX 14 pages, Submitted to a technical journal</arxiv:comment>
    <link href="http://arxiv.org/abs/0905.3347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.3347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3; E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.3323v1</id>
    <updated>2009-06-17T23:24:38Z</updated>
    <published>2009-06-17T23:24:38Z</published>
    <title>Adaptive Regularization of Ill-Posed Problems: Application to Non-rigid
  Image Registration</title>
    <summary>  We introduce an adaptive regularization approach. In contrast to conventional
Tikhonov regularization, which specifies a fixed regularization operator, we
estimate it simultaneously with parameters. From a Bayesian perspective we
estimate the prior distribution on parameters assuming that it is close to some
given model distribution. We constrain the prior distribution to be a
Gauss-Markov random field (GMRF), which allows us to solve for the prior
distribution analytically and provides a fast optimization algorithm. We apply
our approach to non-rigid image registration to estimate the spatial
transformation between two images. Our evaluation shows that the adaptive
regularization approach significantly outperforms standard variational methods.
</summary>
    <author>
      <name>Andriy Myronenko</name>
    </author>
    <author>
      <name>Xubo Song</name>
    </author>
    <link href="http://arxiv.org/abs/0906.3323v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.3323v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0288v1</id>
    <updated>2009-07-02T04:57:32Z</updated>
    <published>2009-07-02T04:57:32Z</published>
    <title>An Iterative Fingerprint Enhancement Algorithm Based on Accurate
  Determination of Orientation Flow</title>
    <summary>  We describe an algorithm to enhance and binarize a fingerprint image. The
algorithm is based on accurate determination of orientation flow of the ridges
of the fingerprint image by computing variance of the neighborhood pixels
around a pixel in different directions. We show that an iterative algorithm
which captures the mutual interdependence of orientation flow computation,
enhancement and binarization gives very good results on poor quality images.
</summary>
    <author>
      <name>Simant Dube</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures. Ongoing work. To be submitted to appropriate
  conference/journal</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.0288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.5321v2</id>
    <updated>2009-08-04T06:01:53Z</updated>
    <published>2009-07-30T12:23:25Z</published>
    <title>Multiple pattern classification by sparse subspace decomposition</title>
    <summary>  A robust classification method is developed on the basis of sparse subspace
decomposition. This method tries to decompose a mixture of subspaces of
unlabeled data (queries) into class subspaces as few as possible. Each query is
classified into the class whose subspace significantly contributes to the
decomposed subspace. Multiple queries from different classes can be
simultaneously classified into their respective classes. A practical greedy
algorithm of the sparse subspace decomposition is designed for the
classification. The present method achieves high recognition rate and robust
performance exploiting joint sparsity.
</summary>
    <author>
      <name>Tomoya Sakai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCVW.2009.5457702</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCVW.2009.5457702" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, 2nd IEEE International Workshop on Subspace
  Methods, Workshop Proceedings of ICCV 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.5321v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.5321v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.1919v3</id>
    <updated>2009-10-01T06:50:18Z</updated>
    <published>2009-08-13T15:41:44Z</published>
    <title>A dyadic solution of relative pose problems</title>
    <summary>  A hierarchical interval subdivision is shown to lead to a $p$-adic encoding
of image data. This allows in the case of the relative pose problem in computer
vision and photogrammetry to derive equations having 2-adic numbers as
coefficients, and to use Hensel's lifting method to their solution. This method
is applied to the linear and non-linear equations coming from eight, seven or
five point correspondences. An inherent property of the method is its
robustness.
</summary>
    <author>
      <name>Patrick Erik Bradley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages; references added; typos and Thm 2 corrected (not affecting
  the other results)</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.1919v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.1919v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.5656v1</id>
    <updated>2009-09-30T18:50:24Z</updated>
    <published>2009-09-30T18:50:24Z</published>
    <title>Improvements of the 3D images captured with Time-of-Flight cameras</title>
    <summary>  3D Time-of-Flight camera's images are affected by errors due to the diffuse
(indirect) light and to the flare light. The presented method improves the 3D
image reducing the distance's errors to dark surface objects. This is achieved
by placing one or two contrast tags in the scene at different distances from
the ToF camera. The white and black parts of the tags are situated at the same
distance to the camera but the distances measured by the camera are different.
This difference is used to compute a correction vector. The distance to black
surfaces is corrected by subtracting this vector from the captured vector
image.
</summary>
    <author>
      <name>D. Falie</name>
    </author>
    <link href="http://arxiv.org/abs/0909.5656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.5656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.1293v1</id>
    <updated>2009-10-07T15:42:03Z</updated>
    <published>2009-10-07T15:42:03Z</published>
    <title>Introducing New AdaBoost Features for Real-Time Vehicle Detection</title>
    <summary>  This paper shows how to improve the real-time object detection in complex
robotics applications, by exploring new visual features as AdaBoost weak
classifiers. These new features are symmetric Haar filters (enforcing global
horizontal and vertical symmetry) and N-connexity control points. Experimental
evaluation on a car database show that the latter appear to provide the best
results for the vehicle-detection problem.
</summary>
    <author>
      <name>Bogdan Stanciulescu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CAOR</arxiv:affiliation>
    </author>
    <author>
      <name>Amaury Breheret</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CAOR</arxiv:affiliation>
    </author>
    <author>
      <name>Fabien Moutarde</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CAOR</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">COGIS'07 conference on COGnitive systems with Interactive Sensors,
  Stanford, Palo Alto : United States (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0910.1293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.1293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.0490v1</id>
    <updated>2009-11-03T04:33:55Z</updated>
    <published>2009-11-03T04:33:55Z</published>
    <title>Breast Cancer Detection Using Multilevel Thresholding</title>
    <summary>  This paper presents an algorithm which aims to assist the radiologist in
identifying breast cancer at its earlier stages. It combines several image
processing techniques like image negative, thresholding and segmentation
techniques for detection of tumor in mammograms. The algorithm is verified by
using mammograms from Mammographic Image Analysis Society. The results obtained
by applying these techniques are described.
</summary>
    <author>
      <name>Y. Ireaneus Anna Rejani</name>
    </author>
    <author>
      <name>S. Thamarai Selvi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 6, No. 1, pp. 111-115, October 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.0490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.0490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.4874v2</id>
    <updated>2009-12-04T12:58:23Z</updated>
    <published>2009-11-25T15:16:53Z</published>
    <title>Non-photorealistic image processing: an Impressionist rendering</title>
    <summary>  The paper describes an image processing for a non-photorealistic rendering.
The algorithm is based on a random choice of a set of pixels from those ot the
original image and substitution of them with colour spots. An iterative
procedure is applied to cover, at a desired level, the canvas. The resulting
effect mimics the impressionist painting and Pointillism.
</summary>
    <author>
      <name>Amelia Carolina Sparavigna</name>
    </author>
    <author>
      <name>Roberto Marazzato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: Image processing. Non-photorealistic processing.
  Image-based rendering</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.4874v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.4874v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.0927v1</id>
    <updated>2010-01-06T16:05:25Z</updated>
    <published>2010-01-06T16:05:25Z</published>
    <title>Accelerating Competitive Learning Graph Quantization</title>
    <summary>  Vector quantization(VQ) is a lossy data compression technique from signal
processing for which simple competitive learning is one standard method to
quantize patterns from the input space. Extending competitive learning VQ to
the domain of graphs results in competitive learning for quantizing input
graphs. In this contribution, we propose an accelerated version of competitive
learning graph quantization (GQ) without trading computational time against
solution quality. For this, we lift graphs locally to vectors in order to avoid
unnecessary calculations of intractable graph distances. In doing so, the
accelerated version of competitive learning GQ gradually turns locally into a
competitive learning VQ with increasing number of iterations. Empirical results
show a significant speedup by maintaining a comparable solution quality.
</summary>
    <author>
      <name>Brijnesh J. Jain</name>
    </author>
    <author>
      <name>Klaus Obermayer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages; submitted to CVIU</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.0927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.0927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.5352v1</id>
    <updated>2010-01-29T08:29:57Z</updated>
    <published>2010-01-29T08:29:57Z</published>
    <title>Kannada Character Recognition System A Review</title>
    <summary>  Intensive research has been done on optical character recognition ocr and a
large number of articles have been published on this topic during the last few
decades. Many commercial OCR systems are now available in the market, but most
of these systems work for Roman, Chinese, Japanese and Arabic characters. There
are no sufficient number of works on Indian language character recognition
especially Kannada script among 12 major scripts in India. This paper presents
a review of existing work on printed Kannada script and their results. The
characteristics of Kannada script and Kannada Character Recognition System kcr
are discussed in detail. Finally fusion at the classifier level is proposed to
increase the recognition accuracy.
</summary>
    <author>
      <name>K. Indira</name>
    </author>
    <author>
      <name>S. Sethu Selvi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.5352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.5352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.0416v1</id>
    <updated>2010-02-02T08:15:20Z</updated>
    <published>2010-02-02T08:15:20Z</published>
    <title>Fusion of Multiple Matchers using SVM for Offline Signature
  Identification</title>
    <summary>  This paper uses Support Vector Machines (SVM) to fuse multiple classifiers
for an offline signature system. From the signature images, global and local
features are extracted and the signatures are verified with the help of
Gaussian empirical rule, Euclidean and Mahalanobis distance based classifiers.
SVM is used to fuse matching scores of these matchers. Finally, recognition of
query signatures is done by comparing it with all signatures of the database.
The proposed system is tested on a signature database contains 5400 offline
signatures of 600 individuals and the results are found to be promising.
</summary>
    <author>
      <name>Dakshina Ranjan Kisku</name>
    </author>
    <author>
      <name>Phalguni Gupta</name>
    </author>
    <author>
      <name>Jamuna Kanta Sing</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.0416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.0416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.2; I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.3344v1</id>
    <updated>2010-02-17T18:29:09Z</updated>
    <published>2010-02-17T18:29:09Z</published>
    <title>Iterative exact global histogram specification and SSIM gradient ascent:
  a proof of convergence, step size and parameter selection</title>
    <summary>  The SSIM-optimized exact global histogram specification (EGHS) is shown to
converge in the sense that the first order approximation of the result's
quality (i.e., its structural similarity with input) does not decrease in an
iteration, when the step size is small. Each iteration is composed of SSIM
gradient ascent and basic EGHS with the specified target histogram. Selection
of step size and other parameters is also discussed.
</summary>
    <author>
      <name>Alireza Avanaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supplement to published work, on SSIM-optimized exact global
  histogram specification; please see arXiv:0901.0065</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.3344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.3344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.4317v1</id>
    <updated>2010-02-23T12:32:34Z</updated>
    <published>2010-02-23T12:32:34Z</published>
    <title>CLD-shaped Brushstrokes in Non-Photorealistic Rendering</title>
    <summary>  Rendering techniques based on a random grid can be improved by adapting
brushstrokes to the shape of different areas of the original picture. In this
paper, the concept of Coherence Length Diagram is applied to determine the
adaptive brushstrokes, in order to simulate an impressionist painting. Some
examples are provided to instance the proposed algorithm.
</summary>
    <author>
      <name>Amelia Carolina Sparavigna</name>
    </author>
    <author>
      <name>Roberto Marazzato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: Image processing, Non-photorealistic processing,
  Image-based rendering Coherence Length Diagram</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Software Engineering and Computing, 2011,
  Volume 3, Issue 1, Pages 11-15</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.4317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.4317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4021v1</id>
    <updated>2010-03-21T20:21:09Z</updated>
    <published>2010-03-21T20:21:09Z</published>
    <title>System-theoretic approach to image interest point detection</title>
    <summary>  Interest point detection is a common task in various computer vision
applications. Although a big variety of detector are developed so far
computational efficiency of interest point based image analysis remains to be
the problem. Current paper proposes a system-theoretic approach to interest
point detection. Starting from the analysis of interdependency between detector
and descriptor it is shown that given a descriptor it is possible to introduce
to notion of detector redundancy. Furthermore for each detector it is possible
to construct its irredundant and equivalent modification. Modified detector
possesses lower computational complexity and is preferable. It is also shown
that several known approaches to reduce computational complexity of image
registration can be generalized in terms of proposed theory.
</summary>
    <author>
      <name>Vitaly Pimenov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.4021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4053v1</id>
    <updated>2010-03-22T03:39:46Z</updated>
    <published>2010-03-22T03:39:46Z</published>
    <title>A Comprehensive Review of Image Enhancement Techniques</title>
    <summary>  Principle objective of Image enhancement is to process an image so that
result is more suitable than original image for specific application. Digital
image enhancement techniques provide a multitude of choices for improving the
visual quality of images. Appropriate choice of such techniques is greatly
influenced by the imaging modality, task at hand and viewing conditions. This
paper will provide an overview of underlying concepts, along with algorithms
commonly used for image enhancement. The paper focuses on spatial domain
techniques for image enhancement, with particular reference to point processing
methods and histogram processing.
</summary>
    <author>
      <name>Raman Maini</name>
    </author>
    <author>
      <name>Himanshu Aggarwal</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 2, Issue 3, March 2010,
  https://sites.google.com/site/journalofcomputing/</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.5865v1</id>
    <updated>2010-03-30T16:36:36Z</updated>
    <published>2010-03-30T16:36:36Z</published>
    <title>Offline Signature Identification by Fusion of Multiple Classifiers using
  Statistical Learning Theory</title>
    <summary>  This paper uses Support Vector Machines (SVM) to fuse multiple classifiers
for an offline signature system. From the signature images, global and local
features are extracted and the signatures are verified with the help of
Gaussian empirical rule, Euclidean and Mahalanobis distance based classifiers.
SVM is used to fuse matching scores of these matchers. Finally, recognition of
query signatures is done by comparing it with all signatures of the database.
The proposed system is tested on a signature database contains 5400 offline
signatures of 600 individuals and the results are found to be promising.
</summary>
    <author>
      <name>Dakshina Ranjan Kisku</name>
    </author>
    <author>
      <name>Phalguni Gupta</name>
    </author>
    <author>
      <name>Jamuna Kanta Sing</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, IJSIA 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.5865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.5865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.2; I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.5891v1</id>
    <updated>2010-03-30T18:35:37Z</updated>
    <published>2010-03-30T18:35:37Z</published>
    <title>Recognition of Handwritten Roman Script Using Tesseract Open source OCR
  Engine</title>
    <summary>  In the present work, we have used Tesseract 2.01 open source Optical
Character Recognition (OCR) Engine under Apache License 2.0 for recognition of
handwriting samples of lower case Roman script. Handwritten isolated and
free-flow text samples were collected from multiple users. Tesseract is trained
to recognize user-specific handwriting samples of both the categories of
document pages. On a single user model, the system is trained with 1844
isolated handwritten characters and the performance is tested on 1133
characters, taken form the test set. The overall character-level accuracy of
the system is observed as 83.5%. The system fails to segment 5.56% characters
and erroneously classifies 10.94% characters.
</summary>
    <author>
      <name>Sandip Rakshit</name>
    </author>
    <author>
      <name>Subhadip Basu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. National Conference on NAQC (2008) 141-145</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.5891v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.5891v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.1227v1</id>
    <updated>2010-04-08T02:39:49Z</updated>
    <published>2010-04-08T02:39:49Z</published>
    <title>Signature Recognition using Multi Scale Fourier Descriptor And Wavelet
  Transform</title>
    <summary>  This paper present a novel off-line signature recognition method based on
multi scale Fourier Descriptor and wavelet transform . The main steps of
constructing a signature recognition system are discussed and experiments on
real data sets show that the average error rate can reach 1%. Finally we
compare 8 distance measures between feature vectors with respect to the
recognition performance.
  Key words: signature recognition; Fourier Descriptor; Wavelet transform;
personal verification
</summary>
    <author>
      <name>Ismail A. Ismail</name>
    </author>
    <author>
      <name>Mohammed A. Ramadan</name>
    </author>
    <author>
      <name>Talaat S. El danaf</name>
    </author>
    <author>
      <name>Ahmed H. Samak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Publication format, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSIS, Vol. 7 No. 3, March 2010,</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.1227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.1227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.5351v2</id>
    <updated>2010-05-11T19:04:22Z</updated>
    <published>2010-04-29T17:56:47Z</published>
    <title>Isometric Embeddings in Imaging and Vision: Facts and Fiction</title>
    <summary>  We explore the practicability of Nash's Embedding Theorem in vision and
imaging sciences. In particular, we investigate the relevance of a result of
Burago and Zalgaller regarding the existence of isometric embeddings of
polyhedral surfaces in $\mathbb{R}^3$ and we show that their proof does not
extended directly to higher dimensions.
</summary>
    <author>
      <name>Emil Saucan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 1 figure Second version: Corrections made, subsection added</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.5351v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.5351v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="52B70, 57R40, 53C42, 30C65" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.1471v1</id>
    <updated>2010-05-10T08:49:56Z</updated>
    <published>2010-05-10T08:49:56Z</published>
    <title>Classification via Incoherent Subspaces</title>
    <summary>  This article presents a new classification framework that can extract
individual features per class. The scheme is based on a model of incoherent
subspaces, each one associated to one class, and a model on how the elements in
a class are represented in this subspace. After the theoretical analysis an
alternate projection algorithm to find such a collection is developed. The
classification performance and speed of the proposed method is tested on the AR
and YaleB databases and compared to that of Fisher's LDA and a recent approach
based on on $\ell_1$ minimisation. Finally connections of the presented scheme
to already existing work are discussed and possible ways of extensions are
pointed out.
</summary>
    <author>
      <name>Karin Schnass</name>
    </author>
    <author>
      <name>Pierre Vandergheynst</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 2 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.1471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.1471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.2715v1</id>
    <updated>2010-05-16T00:31:19Z</updated>
    <published>2010-05-16T00:31:19Z</published>
    <title>On the Subspace of Image Gradient Orientations</title>
    <summary>  We introduce the notion of Principal Component Analysis (PCA) of image
gradient orientations. As image data is typically noisy, but noise is
substantially different from Gaussian, traditional PCA of pixel intensities
very often fails to estimate reliably the low-dimensional subspace of a given
data population. We show that replacing intensities with gradient orientations
and the $\ell_2$ norm with a cosine-based distance measure offers, to some
extend, a remedy to this problem. Our scheme requires the eigen-decomposition
of a covariance matrix and is as computationally efficient as standard $\ell_2$
PCA. We demonstrate some of its favorable properties on robust subspace
estimation.
</summary>
    <author>
      <name>Georgios Tzimiropoulos</name>
    </author>
    <author>
      <name>Stefanos Zafeiriou</name>
    </author>
    <link href="http://arxiv.org/abs/1005.2715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.2715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.4020v1</id>
    <updated>2010-05-21T17:30:08Z</updated>
    <published>2010-05-21T17:30:08Z</published>
    <title>Image Segmentation by Using Threshold Techniques</title>
    <summary>  This paper attempts to undertake the study of segmentation image techniques
by using five threshold methods as Mean method, P-tile method, Histogram
Dependent Technique (HDT), Edge Maximization Technique (EMT) and visual
Technique and they are compared with one another so as to choose the best
technique for threshold segmentation techniques image. These techniques applied
on three satellite images to choose base guesses for threshold segmentation
image.
</summary>
    <author>
      <name>Salem Saleh Al-amri</name>
    </author>
    <author>
      <name>N. V. Kalyankar</name>
    </author>
    <author>
      <name>Khamitkar S. D.</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://www.journalofcomputing.org</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 2, Issue 5, May 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.4020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.4020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.4034v1</id>
    <updated>2010-05-21T18:03:44Z</updated>
    <published>2010-05-21T18:03:44Z</published>
    <title>Face Synthesis (FASY) System for Generation of a Face Image from Human
  Description</title>
    <summary>  This paper aims at generating a new face based on the human like description
using a new concept. The FASY (FAce SYnthesis) System is a Face Database
Retrieval and new Face generation System that is under development. One of its
main features is the generation of the requested face when it is not found in
the existing database, which allows a continuous growing of the database also.
</summary>
    <author>
      <name>Santanu Halder</name>
    </author>
    <author>
      <name>Debotosh Bhattacharjee</name>
    </author>
    <author>
      <name>Mita Nasipuri</name>
    </author>
    <author>
      <name>Dipak Kumar Basu</name>
    </author>
    <author>
      <name>Mahantapas Kundu</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICIIS 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.4034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.4034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.4175v1</id>
    <updated>2010-06-21T20:59:43Z</updated>
    <published>2010-06-21T20:59:43Z</published>
    <title>Optimization of Weighted Curvature for Image Segmentation</title>
    <summary>  Minimization of boundary curvature is a classic regularization technique for
image segmentation in the presence of noisy image data. Techniques for
minimizing curvature have historically been derived from descent methods which
could be trapped in a local minimum and therefore required a good
initialization. Recently, combinatorial optimization techniques have been
applied to the optimization of curvature which provide a solution that achieves
nearly a global optimum. However, when applied to image segmentation these
methods required a meaningful data term. Unfortunately, for many images,
particularly medical images, it is difficult to find a meaningful data term.
Therefore, we propose to remove the data term completely and instead weight the
curvature locally, while still achieving a global optimum.
</summary>
    <author>
      <name>Noha El-Zehiry</name>
    </author>
    <author>
      <name>Leo Grady</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages , 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.4175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.4175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.4910v5</id>
    <updated>2022-12-05T10:44:47Z</updated>
    <published>2010-06-25T04:51:32Z</published>
    <title>Kalman Filters and Homography: Utilizing the Matrix $A$</title>
    <summary>  Many problems in Computer Vision can be reduced to either working around a
known transform, or given a model for the transform computing the inverse
problem of the transform itself. We will look at two ways of working with the
matrix $A$ and see how transforms are at the root of image processing and
vision problems.
</summary>
    <author>
      <name>Burak Bayramli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Typos fixed</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.4910v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.4910v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.5920v1</id>
    <updated>2010-06-30T16:54:43Z</updated>
    <published>2010-06-30T16:54:43Z</published>
    <title>A Two Stage Classification Approach for Handwritten Devanagari
  Characters</title>
    <summary>  The paper presents a two stage classification approach for handwritten
devanagari characters The first stage is using structural properties like
shirorekha, spine in character and second stage exploits some intersection
features of characters which are fed to a feedforward neural network. Simple
histogram based method does not work for finding shirorekha, vertical bar
(Spine) in handwritten devnagari characters. So we designed a differential
distance based technique to find a near straight line for shirorekha and spine.
This approach has been tested for 50000 samples and we got 89.12% success
</summary>
    <author>
      <name>Sandhya Arora</name>
    </author>
    <author>
      <name>Debotosh Bhattacharjee</name>
    </author>
    <author>
      <name>Mita Nasipuri</name>
    </author>
    <author>
      <name>Latesh Malik</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICCIMA 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.5920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.5920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.5924v1</id>
    <updated>2010-06-30T17:09:39Z</updated>
    <published>2010-06-30T17:09:39Z</published>
    <title>A novel approach for handwritten Devnagari character recognition</title>
    <summary>  In this paper a method for recognition of handwritten devanagari characters
is described. Here, feature vector is constituted by accumulated directional
gradient changes in different segments, number of intersections points for the
character, type of spine present and type of shirorekha present in the
character. One Multi-layer Perceptron with conjugate-gradient training is used
to classify these feature vectors. This method is applied to a database with
1000 sample characters and the recognition rate obtained is 88.12%
</summary>
    <author>
      <name>Sandhya Arora</name>
    </author>
    <author>
      <name>Latesh Malik</name>
    </author>
    <author>
      <name>Debotosh Bhattacharjee</name>
    </author>
    <author>
      <name>Mita Nasipuri</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICSIP 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.5924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.5924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.5942v1</id>
    <updated>2010-06-30T18:01:47Z</updated>
    <published>2010-06-30T18:01:47Z</published>
    <title>FPGA Based Assembling of Facial Components for Human Face Construction</title>
    <summary>  This paper aims at VLSI realization for generation of a new face from textual
description. The FASY (FAce SYnthesis) System is a Face Database Retrieval and
new Face generation System that is under development. One of its main features
is the generation of the requested face when it is not found in the existing
database. The new face generation system works in three steps - searching
phase, assembling phase and tuning phase. In this paper the tuning phase using
hardware description language and its implementation in a Field Programmable
Gate Array (FPGA) device is presented.
</summary>
    <author>
      <name>Santanu Halder</name>
    </author>
    <author>
      <name>Debotosh Bhattacharjee</name>
    </author>
    <author>
      <name>Mita Nasipuri</name>
    </author>
    <author>
      <name>Dipak Kumar Basu</name>
    </author>
    <author>
      <name>Mahantapas Kundu</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJRTE 1(1):541-545(2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.5942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.5942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.0547v1</id>
    <updated>2010-07-04T12:19:16Z</updated>
    <published>2010-07-04T12:19:16Z</published>
    <title>A Fast Decision Technique for Hierarchical Hough Transform for Line
  Detection</title>
    <summary>  Many techniques have been proposed to speedup the performance of classic
Hough Transform. These techniques are primarily based on converting the voting
procedure to a hierarchy based voting method. These methods use approximate
decision-making process. In this paper, we propose a fast decision making
process that enhances the speed and reduces the space requirements.
Experimental results demonstrate that the proposed algorithm is much faster
than a similar Fast Hough Transform.
</summary>
    <author>
      <name>Chandan Singh</name>
    </author>
    <author>
      <name>Nitin Bhatia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, published at IEEE conference on Signal and Image Processing
  - 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/1007.0547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.0547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.0626v1</id>
    <updated>2010-07-05T07:33:45Z</updated>
    <published>2010-07-05T07:33:45Z</published>
    <title>Fusion of Wavelet Coefficients from Visual and Thermal Face Images for
  Human Face Recognition - A Comparative Study</title>
    <summary>  In this paper we present a comparative study on fusion of visual and thermal
images using different wavelet transformations. Here, coefficients of discrete
wavelet transforms from both visual and thermal images are computed separately
and combined. Next, inverse discrete wavelet transformation is taken in order
to obtain fused face image. Both Haar and Daubechies (db2) wavelet transforms
have been used to compare recognition results. For experiments IRIS
Thermal/Visual Face Database was used. Experimental results using Haar and
Daubechies wavelets show that the performance of the approach presented here
achieves maximum success rate of 100% in many cases.
</summary>
    <author>
      <name>M. K. Bhowmik</name>
    </author>
    <author>
      <name>Debotosh Bhattacharjee</name>
    </author>
    <author>
      <name>M. Nasipuri</name>
    </author>
    <author>
      <name>D. K. Basu</name>
    </author>
    <author>
      <name>M. Kundu</name>
    </author>
    <link href="http://arxiv.org/abs/1007.0626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.0626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.3881v3</id>
    <updated>2021-08-18T21:58:15Z</updated>
    <published>2010-07-22T13:22:50Z</published>
    <title>Orthogonal multifilters image processing of astronomical images from
  scanned photographic plates</title>
    <summary>  In this paper orthogonal multifilters for astronomical image processing are
presented. We obtained new orthogonal multifilters based on the orthogonal
wavelet of Haar and Daubechies. Recently, multiwavelets have been introduced as
a more powerful multiscale analysis tool. It adds several degrees of freedom in
multifilter design and makes it possible to have several useful properties such
as symmetry, orthogonality, short support, and a higher number of vanishing
moments simultaneously. Multifilter decomposition of scanned photographic
plates with astronomical images is made.
</summary>
    <author>
      <name>Vasil Kolev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, The ACM proceedings of CompSysTech 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1007.3881v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.3881v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="42Cxx, 65Txx, 68Uxx, 94Axx, 85-XX, 97Mxx, 94Axx," scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2; E.4; G.1; I.4; I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.3346v1</id>
    <updated>2010-08-19T16:38:35Z</updated>
    <published>2010-08-19T16:38:35Z</published>
    <title>A Miniature-Based Image Retrieval System</title>
    <summary>  Due to the rapid development of World Wide Web (WWW) and imaging technology,
more and more images are available in the Internet and stored in databases.
Searching the related images by the querying image is becoming tedious and
difficult. Most of the images on the web are compressed by methods based on
discrete cosine transform (DCT) including Joint Photographic Experts
Group(JPEG) and H.261. This paper presents an efficient content-based image
indexing technique for searching similar images using discrete cosine transform
features. Experimental results demonstrate its superiority with the existing
techniques.
</summary>
    <author>
      <name>Md. Saiful Islam</name>
    </author>
    <author>
      <name>Md. Haider Ali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures, 4 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dhaka University Journal of Science,Vol. 57, No. 2, pp. 187-191,
  July 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1008.3346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.3346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.0854v1</id>
    <updated>2010-09-04T17:44:06Z</updated>
    <published>2010-09-04T17:44:06Z</published>
    <title>Fast Color Space Transformations Using Minimax Approximations</title>
    <summary>  Color space transformations are frequently used in image processing,
graphics, and visualization applications. In many cases, these transformations
are complex nonlinear functions, which prohibits their use in time-critical
applications. In this paper, we present a new approach called Minimax
Approximations for Color-space Transformations (MACT).We demonstrate MACT on
three commonly used color space transformations. Extensive experiments on a
large and diverse image set and comparisons with well-known multidimensional
lookup table interpolation methods show that MACT achieves an excellent balance
among four criteria: ease of implementation, memory usage, accuracy, and
computational speed.
</summary>
    <author>
      <name>M. Emre Celebi</name>
    </author>
    <author>
      <name>Hassan Kingravi</name>
    </author>
    <author>
      <name>Fatih Celiker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1049/iet-ipr.2008.0172</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1049/iet-ipr.2008.0172" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IET Image Processing 4 (2010) 70-80</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.0854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.0854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.2; I.4.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.0957v1</id>
    <updated>2010-09-05T23:49:38Z</updated>
    <published>2010-09-05T23:49:38Z</published>
    <title>Distance Measures for Reduced Ordering Based Vector Filters</title>
    <summary>  Reduced ordering based vector filters have proved successful in removing
long-tailed noise from color images while preserving edges and fine image
details. These filters commonly utilize variants of the Minkowski distance to
order the color vectors with the aim of distinguishing between noisy and
noise-free vectors. In this paper, we review various alternative distance
measures and evaluate their performance on a large and diverse set of images
using several effectiveness and efficiency criteria. The results demonstrate
that there are in fact strong alternatives to the popular Minkowski metrics.
</summary>
    <author>
      <name>M. Emre Celebi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1049/iet-ipr.2009.0056</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1049/iet-ipr.2009.0056" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IET Image Processing 3 (2009) 249-260</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.0957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.0957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.0958v1</id>
    <updated>2010-09-05T23:53:27Z</updated>
    <published>2010-09-05T23:53:27Z</published>
    <title>Real-Time Implementation of Order-Statistics Based Directional Filters</title>
    <summary>  Vector filters based on order-statistics have proved successful in removing
impulsive noise from color images while preserving edges and fine image
details. Among these filters, the ones that involve the cosine distance
function (directional filters) have particularly high computational
requirements, which limits their use in time critical applications. In this
paper, we introduce two methods to speed up these filters. Experiments on a
diverse set of color images show that the proposed methods provide substantial
computational gains without significant loss of accuracy.
</summary>
    <author>
      <name>M. Emre Celebi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1049/iet-ipr:20080080</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1049/iet-ipr:20080080" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IET Image Processing 3 (2009) 1-9</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.0958v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.0958v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.0959v1</id>
    <updated>2010-09-06T00:02:35Z</updated>
    <published>2010-09-06T00:02:35Z</published>
    <title>Cost-Effective Implementation of Order-Statistics Based Vector Filters
  Using Minimax Approximations</title>
    <summary>  Vector operators based on robust order statistics have proved successful in
digital multichannel imaging applications, particularly color image filtering
and enhancement, in dealing with impulsive noise while preserving edges and
fine image details. These operators often have very high computational
requirements which limits their use in time-critical applications. This paper
introduces techniques to speed up vector filters using the minimax
approximation theory. Extensive experiments on a large and diverse set of color
images show that proposed approximations achieve an excellent balance among
ease of implementation, accuracy, and computational speed.
</summary>
    <author>
      <name>M. Emre Celebi</name>
    </author>
    <author>
      <name>Hassan A. Kingravi</name>
    </author>
    <author>
      <name>Rastislav Lukac</name>
    </author>
    <author>
      <name>Fatih Celiker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1364/JOSAA.26.001518</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1364/JOSAA.26.001518" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of the Optical Society of America A 26 (2009) 1518-1524</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.0959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.0959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.0961v1</id>
    <updated>2010-09-06T00:13:25Z</updated>
    <published>2010-09-06T00:13:25Z</published>
    <title>A Fast Switching Filter for Impulsive Noise Removal from Color Images</title>
    <summary>  In this paper, we present a fast switching filter for impulsive noise removal
from color images. The filter exploits the HSL color space, and is based on the
peer group concept, which allows for the fast detection of noise in a
neighborhood without resorting to pairwise distance computations between each
pixel. Experiments on large set of diverse images demonstrate that the proposed
approach is not only extremely fast, but also gives excellent results in
comparison to various state-of-the-art filters.
</summary>
    <author>
      <name>M. Emre Celebi</name>
    </author>
    <author>
      <name>Hassan A. Kingravi</name>
    </author>
    <author>
      <name>Bakhtiyar Uddin</name>
    </author>
    <author>
      <name>Y. Alp Aslandogan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2352/J.ImagingSci.Technol.(2007)51:2(155)</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2352/J.ImagingSci.Technol.(2007)51:2(155)" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Imaging Science and Technology 51 (2007) 155-165</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.0961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.0961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4004v2</id>
    <updated>2011-12-19T02:31:16Z</updated>
    <published>2010-09-21T06:32:52Z</published>
    <title>A family of statistical symmetric divergences based on Jensen's
  inequality</title>
    <summary>  We introduce a novel parametric family of symmetric information-theoretic
distances based on Jensen's inequality for a convex functional generator. In
particular, this family unifies the celebrated Jeffreys divergence with the
Jensen-Shannon divergence when the Shannon entropy generator is chosen. We then
design a generic algorithm to compute the unique centroid defined as the
minimum average divergence. This yields a smooth family of centroids linking
the Jeffreys to the Jensen-Shannon centroid. Finally, we report on our
experimental results.
</summary>
    <author>
      <name>Frank Nielsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.4004v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4004v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.3467v1</id>
    <updated>2010-10-18T02:31:21Z</updated>
    <published>2010-10-18T02:31:21Z</published>
    <title>Fast Inference in Sparse Coding Algorithms with Applications to Object
  Recognition</title>
    <summary>  Adaptive sparse coding methods learn a possibly overcomplete set of basis
functions, such that natural image patches can be reconstructed by linearly
combining a small subset of these bases. The applicability of these methods to
visual object recognition tasks has been limited because of the prohibitive
cost of the optimization algorithms required to compute the sparse
representation. In this work we propose a simple and efficient algorithm to
learn basis functions. After training, this model also provides a fast and
smooth approximator to the optimal representation, achieving even better
accuracy than exact sparse coding algorithms on visual object recognition
tasks.
</summary>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <author>
      <name>Marc'Aurelio Ranzato</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <link href="http://arxiv.org/abs/1010.3467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.3467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.3023v4</id>
    <updated>2013-11-20T16:42:37Z</updated>
    <published>2010-11-12T20:15:25Z</published>
    <title>Classification with Scattering Operators</title>
    <summary>  A scattering vector is a local descriptor including multiscale and
multi-direction co-occurrence information. It is computed with a cascade of
wavelet decompositions and complex modulus. This scattering representation is
locally translation invariant and linearizes deformations. A supervised
classification algorithm is computed with a PCA model selection on scattering
vectors. State of the art results are obtained for handwritten digit
recognition and texture classification.
</summary>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Stéphane Mallat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages. CVPR 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.3023v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.3023v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.4321v1</id>
    <updated>2010-11-18T22:20:45Z</updated>
    <published>2010-11-18T22:20:45Z</published>
    <title>A Fuzzy Clustering Model for Fuzzy Data with Outliers</title>
    <summary>  In this paper a fuzzy clustering model for fuzzy data with outliers is
proposed. The model is based on Wasserstein distance between interval valued
data which is generalized to fuzzy data. In addition, Keller's approach is used
to identify outliers and reduce their influences. We have also defined a
transformation to change our distance to the Euclidean distance. With the help
of this approach, the problem of fuzzy clustering of fuzzy data is reduced to
fuzzy clustering of crisp data. In order to show the performance of the
proposed clustering algorithm, two simulation experiments are discussed.
</summary>
    <author>
      <name>M. H. Fazel Zarandi</name>
    </author>
    <author>
      <name>Zahra S. Razaee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, Journal paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.4321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.4321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.2491v1</id>
    <updated>2010-12-11T21:48:51Z</updated>
    <published>2010-12-11T21:48:51Z</published>
    <title>Affine Invariant, Model-Based Object Recognition Using Robust Metrics
  and Bayesian Statistics</title>
    <summary>  We revisit the problem of model-based object recognition for intensity images
and attempt to address some of the shortcomings of existing Bayesian methods,
such as unsuitable priors and the treatment of residuals with a non-robust
error norm. We do so by using a refor- mulation of the Huber metric and
carefully chosen prior distributions. Our proposed method is invariant to
2-dimensional affine transforma- tions and, because it is relatively easy to
train and use, it is suited for general object matching problems.
</summary>
    <author>
      <name>Vasileios Zografos</name>
    </author>
    <author>
      <name>Bernard Buxton</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/11559573_51</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/11559573_51" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Image Analysis and Recognition Lecture Notes in Computer Science,
  2005, Volume 3656/2005, 407-414</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1012.2491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.2491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.3802v1</id>
    <updated>2010-12-17T03:27:54Z</updated>
    <published>2010-12-17T03:27:54Z</published>
    <title>Detecting Image Forgeries using Geometric Cues</title>
    <summary>  This chapter presents a framework for detecting fake regions by using various
methods including watermarking technique and blind approaches. In particular,
we describe current categories on blind approaches which can be divided into
five: pixel-based techniques, format-based techniques, camera-based techniques,
physically-based techniques and geometric-based techniques. Then we take a
second look on the geometric-based techniques and further categorize them in
detail. In the following section, the state-of-the-art methods involved in the
geometric technique are elaborated.
</summary>
    <author>
      <name>Lin Wu</name>
    </author>
    <author>
      <name>Yang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.3802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.3802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.3951v1</id>
    <updated>2010-12-17T18:23:35Z</updated>
    <published>2010-12-17T18:23:35Z</published>
    <title>Diffusion-geometric maximally stable component detection in deformable
  shapes</title>
    <summary>  Maximally stable component detection is a very popular method for feature
analysis in images, mainly due to its low computation cost and high
repeatability. With the recent advance of feature-based methods in geometric
shape analysis, there is significant interest in finding analogous approaches
in the 3D world. In this paper, we formulate a diffusion-geometric framework
for stable component detection in non-rigid 3D shapes, which can be used for
geometric feature detection and description. A quantitative evaluation of our
method on the SHREC'10 feature detection benchmark shows its potential as a
source of high-quality features.
</summary>
    <author>
      <name>Roee Litman</name>
    </author>
    <author>
      <name>Alex M. Bronstein</name>
    </author>
    <author>
      <name>Michael M. Bronstein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cag.2011.03.011</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cag.2011.03.011" rel="related"/>
    <link href="http://arxiv.org/abs/1012.3951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.3951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.7; I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.5933v1</id>
    <updated>2010-12-29T13:11:41Z</updated>
    <published>2010-12-29T13:11:41Z</published>
    <title>Affine-invariant diffusion geometry for the analysis of deformable 3D
  shapes</title>
    <summary>  We introduce an (equi-)affine invariant diffusion geometry by which surfaces
that go through squeeze and shear transformations can still be properly
analyzed. The definition of an affine invariant metric enables us to construct
an invariant Laplacian from which local and global geometric structures are
extracted. Applications of the proposed framework demonstrate its power in
generalizing and enriching the existing set of tools for shape analysis.
</summary>
    <author>
      <name>Dan Raviv</name>
    </author>
    <author>
      <name>Alexander M. Bronstein</name>
    </author>
    <author>
      <name>Michael M. Bronstein</name>
    </author>
    <author>
      <name>Ron Kimmel</name>
    </author>
    <author>
      <name>Nir Sochen</name>
    </author>
    <link href="http://arxiv.org/abs/1012.5933v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.5933v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.2243v1</id>
    <updated>2010-12-12T20:49:00Z</updated>
    <published>2010-12-12T20:49:00Z</published>
    <title>Illustrating Color Evolution and Color Blindness by the Decoding Model
  of Color Vision</title>
    <summary>  A symmetrical model of color vision, the decoding model as a new version of
zone model, was introduced. The model adopts new continuous-valued logic and
works in a way very similar to the way a 3-8 decoder in a numerical circuit
works. By the decoding model, Young and Helmholtz's tri-pigment theory and
Hering's opponent theory are unified more naturally; opponent process, color
evolution, and color blindness are illustrated more concisely. According to the
decoding model, we can obtain a transform from RGB system to HSV system, which
is formally identical to the popular transform for computer graphics provided
by Smith (1978). Advantages, problems, and physiological tests of the decoding
model are also discussed.
</summary>
    <author>
      <name>Chenguang Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.2243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.2243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.2312v1</id>
    <updated>2011-01-12T10:16:11Z</updated>
    <published>2011-01-12T10:16:11Z</published>
    <title>Automatic segmentation of HeLa cell images</title>
    <summary>  In this work, the possibilities for segmentation of cells from their
background and each other in digital image were tested, combined and improoved.
Lot of images with young, adult and mixture cells were able to prove the
quality of described algorithms. Proper segmentation is one of the main task of
image analysis and steps order differ from work to work, depending on input
images. Reply for biologicaly given question was looking for in this work,
including filtration, details emphasizing, segmentation and sphericity
computing. Order of algorithms and way to searching for them was also
described. Some questions and ideas for further work were mentioned in the
conclusion part.
</summary>
    <author>
      <name>Jan Urban</name>
    </author>
    <link href="http://arxiv.org/abs/1101.2312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.2312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.4301v1</id>
    <updated>2011-01-22T16:41:20Z</updated>
    <published>2011-01-22T16:41:20Z</published>
    <title>Diffusion framework for geometric and photometric data fusion in
  non-rigid shape analysis</title>
    <summary>  In this paper, we explore the use of the diffusion geometry framework for the
fusion of geometric and photometric information in local and global shape
descriptors. Our construction is based on the definition of a diffusion process
on the shape manifold embedded into a high-dimensional space where the
embedding coordinates represent the photometric information. Experimental
results show that such data fusion is useful in coping with different
challenges of shape analysis where pure geometric and pure photometric methods
fail.
</summary>
    <author>
      <name>Artiom Kovnatsky</name>
    </author>
    <author>
      <name>Michael M. Bronstein</name>
    </author>
    <author>
      <name>Alexander M. Bronstein</name>
    </author>
    <author>
      <name>Ron Kimmel</name>
    </author>
    <link href="http://arxiv.org/abs/1101.4301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.4301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.1587v2</id>
    <updated>2011-03-10T18:46:23Z</updated>
    <published>2011-03-08T17:50:56Z</published>
    <title>All Roads Lead To Rome</title>
    <summary>  This short article presents a class of projection-based solution algorithms
to the problem considered in the pioneering work on compressed sensing -
perfect reconstruction of a phantom image from 22 radial lines in the frequency
domain. Under the framework of projection-based image reconstruction, we will
show experimentally that several old and new tools of nonlinear filtering
(including Perona-Malik diffusion, nonlinear diffusion, Translation-Invariant
thresholding and SA-DCT thresholding) all lead to perfect reconstruction of the
phantom image.
</summary>
    <author>
      <name>Xin Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, submitted</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE SPM'2011 as a Column Paper for DSP Tips&amp;Tricks</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.1587v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.1587v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.3440v1</id>
    <updated>2011-03-17T15:52:15Z</updated>
    <published>2011-03-17T15:52:15Z</published>
    <title>Off-Line Handwritten Signature Identification Using Rotated Complex
  Wavelet Filters</title>
    <summary>  In this paper, a new method for handwritten signature identification based on
rotated complex wavelet filters is proposed. We have proposed to use the
rotated complex wavelet filters (RCWF) and dual tree complex wavelet
transform(DTCWT) together to derive signature feature extraction, which
captures information in twelve different directions. In identification phase,
Canberra distance measure is used. The proposed method is compared with
discrete wavelet transform (DWT). From experimental results it is found that
signature identification rate of proposed method is superior over DWT
</summary>
    <author>
      <name>M. S. Shirdhonkar</name>
    </author>
    <author>
      <name>Manesh Kokare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 1, January 2011 ISSN (Online): 1694-0814</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.3440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.3440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.0579v1</id>
    <updated>2011-04-04T14:14:47Z</updated>
    <published>2011-04-04T14:14:47Z</published>
    <title>Image Retrieval Method Using Top-surf Descriptor</title>
    <summary>  This report presents the results and details of a content-based image
retrieval project using the Top-surf descriptor. The experimental results are
preliminary, however, it shows the capability of deducing objects from parts of
the objects or from the objects that are similar. This paper uses a dataset
consisting of 1200 images of which 800 images are equally divided into 8
categories, namely airplane, beach, motorbike, forest, elephants, horses, bus
and building, while the other 400 images are randomly picked from the Internet.
The best results achieved are from building category.
</summary>
    <author>
      <name>Ye Ji</name>
    </author>
    <link href="http://arxiv.org/abs/1104.0579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.0579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.1472v1</id>
    <updated>2011-04-08T03:15:43Z</updated>
    <published>2011-04-08T03:15:43Z</published>
    <title>Gaussian Affine Feature Detector</title>
    <summary>  A new method is proposed to get image features' geometric information. Using
Gaussian as an input signal, a theoretical optimal solution to calculate
feature's affine shape is proposed. Based on analytic result of a feature
model, the method is different from conventional iterative approaches. From the
model, feature's parameters such as position, orientation, background
luminance, contrast, area and aspect ratio can be extracted. Tested with
synthesized and benchmark data, the method achieves or outperforms existing
approaches in term of accuracy, speed and stability. The method can detect
small, long or thin objects precisely, and works well under general conditions,
such as for low contrast, blurred or noisy images.
</summary>
    <author>
      <name>Xiaopeng Xu</name>
    </author>
    <author>
      <name>Xiaochun Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A paper about two dimension image signal detection, including
  position, length, width, height, orentation</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.1472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.1472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.1485v1</id>
    <updated>2011-04-08T05:18:15Z</updated>
    <published>2011-04-08T05:18:15Z</published>
    <title>Fuzzy Rules and Evidence Theory for Satellite Image Analysis</title>
    <summary>  Design of a fuzzy rule based classifier is proposed. The performance of the
classifier for multispectral satellite image classification is improved using
Dempster- Shafer theory of evidence that exploits information of the
neighboring pixels. The classifiers are tested rigorously with two known images
and their performance are found to be better than the results available in the
literature. We also demonstrate the improvement of performance while using D-S
theory along with fuzzy rule based classifiers over the basic fuzzy rule based
classifiers for all the test cases.
</summary>
    <author>
      <name>Arijit Laha</name>
    </author>
    <author>
      <name>J. Das</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, International Conference on Advances in Pattern Recognition
  2003 (ICAPR03)</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.1485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.1485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.2171v1</id>
    <updated>2011-04-12T11:20:06Z</updated>
    <published>2011-04-12T11:20:06Z</published>
    <title>From a Modified Ambrosio-Tortorelli to a Randomized Part Hierarchy Tree</title>
    <summary>  We demonstrate the possibility of coding parts, features that are higher
level than boundaries, using a modified AT field after augmenting the
interaction term of the AT energy with a non-local term and weakening the
separation into boundary/not-boundary phases. The iteratively extracted parts
using the level curves with double point singularities are organized as a
proper binary tree. Inconsistencies due to non-generic configurations for level
curves as well as due to visual changes such as occlusion are successfully
handled once the tree is endowed with a probabilistic structure. The work is a
step in establishing the AT function as a bridge between low and high level
visual processing.
</summary>
    <author>
      <name>Sibel Tari</name>
    </author>
    <author>
      <name>Murat Genctav</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Scale Space and Variational Methods 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.2171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.2171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.3742v1</id>
    <updated>2011-04-19T13:36:15Z</updated>
    <published>2011-04-19T13:36:15Z</published>
    <title>Hue Histograms to Spatiotemporal Local Features for Action Recognition</title>
    <summary>  Despite the recent developments in spatiotemporal local features for action
recognition in video sequences, local color information has so far been
ignored. However, color has been proved an important element to the success of
automated recognition of objects and scenes. In this paper we extend the
space-time interest point descriptor STIP to take into account the color
information on the features' neighborhood. We compare the performance of our
color-aware version of STIP (which we have called HueSTIP) with the original
one.
</summary>
    <author>
      <name>Fillipe Souza</name>
    </author>
    <author>
      <name>Eduardo Valle</name>
    </author>
    <author>
      <name>Guillermo Chávez</name>
    </author>
    <author>
      <name>Arnaldo Araújo</name>
    </author>
    <link href="http://arxiv.org/abs/1104.3742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.3742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.4295v1</id>
    <updated>2011-04-21T15:45:52Z</updated>
    <published>2011-04-21T15:45:52Z</published>
    <title>Improving digital signal interpolation: L2-optimal kernels with
  kernel-invariant interpolation speed</title>
    <summary>  Interpolation is responsible for digital signal resampling and can
significantly degrade the original signal quality if not done properly. For
many years, optimal interpolation algorithms were sought within constrained
classes of interpolation kernel functions. We derive a new family of
unconstrained L2-optimal interpolation kernels, and compare their properties to
the previously known. Although digital images are used to illustrate this work,
our L2-optimal kernels can be applied to interpolate any digital signals.
</summary>
    <author>
      <name>Oleg S. Pianykh</name>
    </author>
    <link href="http://arxiv.org/abs/1104.4295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.4295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.0107v1</id>
    <updated>2011-06-01T07:22:04Z</updated>
    <published>2011-06-01T07:22:04Z</published>
    <title>Handwritten Character Recognition of South Indian Scripts: A Review</title>
    <summary>  Handwritten character recognition is always a frontier area of research in
the field of pattern recognition and image processing and there is a large
demand for OCR on hand written documents. Even though, sufficient studies have
performed in foreign scripts like Chinese, Japanese and Arabic characters, only
a very few work can be traced for handwritten character recognition of Indian
scripts especially for the South Indian scripts. This paper provides an
overview of offline handwritten character recognition in South Indian Scripts,
namely Malayalam, Tamil, Kannada and Telungu.
</summary>
    <author>
      <name>John Jomy</name>
    </author>
    <author>
      <name>K. V. Pramod</name>
    </author>
    <author>
      <name>Balakrishnan Kannan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented on the "National Conference on Indian Language
  Computing", Kochi, February 19-20, 2011. 6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.0107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.0107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.5341v1</id>
    <updated>2011-06-27T09:47:28Z</updated>
    <published>2011-06-27T09:47:28Z</published>
    <title>Pose Estimation from a Single Depth Image for Arbitrary Kinematic
  Skeletons</title>
    <summary>  We present a method for estimating pose information from a single depth image
given an arbitrary kinematic structure without prior training. For an arbitrary
skeleton and depth image, an evolutionary algorithm is used to find the optimal
kinematic configuration to explain the observed image. Results show that our
approach can correctly estimate poses of 39 and 78 degree-of-freedom models
from a single depth image, even in cases of significant self-occlusion.
</summary>
    <author>
      <name>Daniel L. Ly</name>
    </author>
    <author>
      <name>Ashutosh Saxena</name>
    </author>
    <author>
      <name>Hod Lipson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures, RGB-D workshop in Robotics: Science and Systems
  (RSS 2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.5341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.5341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.5571v1</id>
    <updated>2011-06-28T06:08:38Z</updated>
    <published>2011-06-28T06:08:38Z</published>
    <title>Mobile Augmented Reality Applications</title>
    <summary>  Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications.
</summary>
    <author>
      <name>David Prochazka</name>
    </author>
    <author>
      <name>Michael Stencl</name>
    </author>
    <author>
      <name>Ondrej Popelka</name>
    </author>
    <author>
      <name>Jiri Stastny</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Mendel 2011: 17th International Conference on Soft
  Computing, pp. 469-476, ISBN 978-80-214-4302-0</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1106.5571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.5571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.1058v1</id>
    <updated>2011-07-06T08:43:38Z</updated>
    <published>2011-07-06T08:43:38Z</published>
    <title>Online Vehicle Detection For Estimating Traffic Status</title>
    <summary>  We propose a traffic congestion estimation system based on unsupervised
on-line learning algorithm. The system does not rely on background extraction
or motion detection. It extracts local features inside detection regions of
variable size which are drawn on lanes in advance. The extracted features are
then clustered into two classes using K-means and Gaussian Mixture Models(GMM).
A Bayes classifier is used to detect vehicles according to the previous cluster
information which keeps updated whenever system is running by on-line EM
algorithm. Experimental result shows that our system can be adapted to various
traffic scenes for estimating traffic status.
</summary>
    <author>
      <name>Ranch Y. Q. Lai</name>
    </author>
    <link href="http://arxiv.org/abs/1107.1058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.1058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.1081v1</id>
    <updated>2011-07-06T10:02:42Z</updated>
    <published>2011-07-06T10:02:42Z</published>
    <title>Spatial Features for Multi-Font/Multi-Size Kannada Numerals and Vowels
  Recognition</title>
    <summary>  This paper presents multi-font/multi-size Kannada numerals and vowels
recognition based on spatial features. Directional spatial features viz stroke
density, stroke length and the number of stokes in an image are employed as
potential features to characterize the printed Kannada numerals and vowels.
Based on these features 1100 numerals and 1400 vowels are classified with
Multi-class Support Vector Machines (SVM). The proposed system achieves the
recognition accuracy as 98.45% and 90.64% for numerals and vowels respectively.
</summary>
    <author>
      <name>B. V. Dhandra</name>
    </author>
    <author>
      <name>Mallikarjun Hangarge</name>
    </author>
    <author>
      <name>Gururaj Mukarambi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 Figures, 4 Tables, "International Conference on
  Communication, Computation, Control and Nanotechnology (2010)"</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.1081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.1081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.2336v1</id>
    <updated>2011-07-12T16:21:06Z</updated>
    <published>2011-07-12T16:21:06Z</published>
    <title>A Variation of the Box-Counting Algorithm Applied to Colour Images</title>
    <summary>  The box counting method for fractal dimension estimation had not been applied
to large or colour images thus far due to the processing time required. In this
letter we present a fast, easy to implement and very easily expandable to any
number of dimensions variation, the box merging method. It is applied here in
RGB images which are considered as sets in 5-D space.
</summary>
    <author>
      <name>N. S. Nikolaidis</name>
    </author>
    <author>
      <name>I. N. Nikolaidis</name>
    </author>
    <author>
      <name>C. C. Tsouros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.2336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.2336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="28A78, 28A80" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.2693v1</id>
    <updated>2011-07-13T22:46:58Z</updated>
    <published>2011-07-13T22:46:58Z</published>
    <title>A Fuzzy View on k-Means Based Signal Quantization with Application in
  Iris Segmentation</title>
    <summary>  This paper shows that the k-means quantization of a signal can be interpreted
both as a crisp indicator function and as a fuzzy membership assignment
describing fuzzy clusters and fuzzy boundaries. Combined crisp and fuzzy
indicator functions are defined here as natural generalizations of the ordinary
crisp and fuzzy indicator functions, respectively. An application to iris
segmentation is presented together with a demo program.
</summary>
    <author>
      <name>Nicolaie Popescu-Bodorin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4, pages, 3 figures, 17th Telecommunications Forum TELFOR 2009,
  Belgrade, Serbia</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.2693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.2693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U10, 68T10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.6; I.5.1; I.5.3; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.4958v1</id>
    <updated>2011-07-25T14:20:34Z</updated>
    <published>2011-07-25T14:20:34Z</published>
    <title>Efficient and Accurate Gaussian Image Filtering Using Running Sums</title>
    <summary>  This paper presents a simple and efficient method to convolve an image with a
Gaussian kernel. The computation is performed in a constant number of
operations per pixel using running sums along the image rows and columns. We
investigate the error function used for kernel approximation and its relation
to the properties of the input signal. Based on natural image statistics we
propose a quadratic form kernel error function so that the output image l2
error is minimized. We apply the proposed approach to approximate the Gaussian
kernel by linear combination of constant functions. This results in very
efficient Gaussian filtering method. Our experiments show that the proposed
technique is faster than state of the art methods while preserving a similar
accuracy.
</summary>
    <author>
      <name>Elhanan Elboher</name>
    </author>
    <author>
      <name>Michael Werman</name>
    </author>
    <link href="http://arxiv.org/abs/1107.4958v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.4958v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.5349v1</id>
    <updated>2011-07-26T22:29:35Z</updated>
    <published>2011-07-26T22:29:35Z</published>
    <title>Multi Layer Analysis</title>
    <summary>  This thesis presents a new methodology to analyze one-dimensional signals
trough a new approach called Multi Layer Analysis, for short MLA. It also
provides some new insights on the relationship between one-dimensional signals
processed by MLA and tree kernels, test of randomness and signal processing
techniques. The MLA approach has a wide range of application to the fields of
pattern discovery and matching, computational biology and many other areas of
computer science and signal processing. This thesis includes also some
applications of this approach to real problems in biology and seismology.
</summary>
    <author>
      <name>Luca Pinello</name>
    </author>
    <link href="http://arxiv.org/abs/1107.5349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.5349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.0007v1</id>
    <updated>2011-07-29T20:06:59Z</updated>
    <published>2011-07-29T20:06:59Z</published>
    <title>A Invertible Dimension Reduction of Curves on a Manifold</title>
    <summary>  In this paper, we propose a novel lower dimensional representation of a shape
sequence. The proposed dimension reduction is invertible and computationally
more efficient in comparison to other related works. Theoretically, the
differential geometry tools such as moving frame and parallel transportation
are successfully adapted into the dimension reduction problem of high
dimensional curves. Intuitively, instead of searching for a global flat
subspace for curve embedding, we deployed a sequence of local flat subspaces
adaptive to the geometry of both of the curve and the manifold it lies on. In
practice, the experimental results of the dimension reduction and
reconstruction algorithms well illustrate the advantages of the proposed
theoretical innovation.
</summary>
    <author>
      <name>Sheng Yi</name>
    </author>
    <author>
      <name>Hamid Krim</name>
    </author>
    <author>
      <name>Larry K. Norris</name>
    </author>
    <link href="http://arxiv.org/abs/1108.0007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.0007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.1169v1</id>
    <updated>2011-08-04T19:00:14Z</updated>
    <published>2011-08-04T19:00:14Z</published>
    <title>Learning Representations by Maximizing Compression</title>
    <summary>  We give an algorithm that learns a representation of data through
compression. The algorithm 1) predicts bits sequentially from those previously
seen and 2) has a structure and a number of computations similar to an
autoencoder. The likelihood under the model can be calculated exactly, and
arithmetic coding can be used directly for compression. When training on digits
the algorithm learns filters similar to those of restricted boltzman machines
and denoising autoencoders. Independent samples can be drawn from the model by
a single sweep through the pixels. The algorithm has a good compression
performance when compared to other methods that work under random ordering of
pixels.
</summary>
    <author>
      <name>Karol Gregor</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.1169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.1169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.3525v1</id>
    <updated>2011-08-17T17:06:41Z</updated>
    <published>2011-08-17T17:06:41Z</published>
    <title>Hamiltonian Streamline Guided Feature Extraction with Applications to
  Face Detection</title>
    <summary>  We propose a new feature extraction method based on two dynamical systems
induced by intensity landscape: the negative gradient system and the
Hamiltonian system. We build features based on the Hamiltonian streamlines.
These features contain nice global topological information about the intensity
landscape, and can be used for object detection. We show that for training
images of same size, our feature space is much smaller than that generated by
Haar-like features. The training time is extremely short, and detection speed
and accuracy is similar to Haar-like feature based classifiers.
</summary>
    <author>
      <name>Yingjie Miao</name>
    </author>
    <author>
      <name>Jason J. Corso</name>
    </author>
    <link href="http://arxiv.org/abs/1108.3525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.3525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.4315v1</id>
    <updated>2011-08-22T13:49:57Z</updated>
    <published>2011-08-22T13:49:57Z</published>
    <title>Edge detection based on morphological amoebas</title>
    <summary>  Detecting the edges of objects within images is critical for quality image
processing. We present an edge-detecting technique that uses morphological
amoebas that adjust their shape based on variation in image contours. We
evaluate the method both quantitatively and qualitatively for edge detection of
images, and compare it to classic morphological methods. Our amoeba-based
edge-detection system performed better than the classic edge detectors.
</summary>
    <author>
      <name>Won Yeol Lee</name>
    </author>
    <author>
      <name>Young Woo Kim</name>
    </author>
    <author>
      <name>Se Yun Kim</name>
    </author>
    <author>
      <name>Jae Young Lim</name>
    </author>
    <author>
      <name>Dong Hoon Lim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1179/1743131X11Y.0000000013</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1179/1743131X11Y.0000000013" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in The Imaging Science Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.4315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.4315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.5710v1</id>
    <updated>2011-08-29T19:06:30Z</updated>
    <published>2011-08-29T19:06:30Z</published>
    <title>Generalized Fast Approximate Energy Minimization via Graph Cuts:
  Alpha-Expansion Beta-Shrink Moves</title>
    <summary>  We present alpha-expansion beta-shrink moves, a simple generalization of the
widely-used alpha-beta swap and alpha-expansion algorithms for approximate
energy minimization. We show that in a certain sense, these moves dominate both
alpha-beta-swap and alpha-expansion moves, but unlike previous generalizations
the new moves require no additional assumptions and are still solvable in
polynomial-time. We show promising experimental results with the new moves,
which we believe could be used in any context where alpha-expansions are
currently employed.
</summary>
    <author>
      <name>Mark Schmidt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Paris - Rocquencourt</arxiv:affiliation>
    </author>
    <author>
      <name>Karteek Alahari</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Paris - Rocquencourt</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference on Uncertainty in Artificial Intelligence (2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.5710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.5710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.5720v1</id>
    <updated>2011-08-29T19:28:13Z</updated>
    <published>2011-08-29T19:28:13Z</published>
    <title>Conjugate Variables as a Resource in Signal and Image Processing</title>
    <summary>  In this paper we develop a new technique to model joint distributions of
signals. Our technique is based on quantum mechanical conjugate variables. We
show that the transition probability of quantum states leads to a distance
function on the signals. This distance function obeys the triangle inequality
on all quantum states and becomes a metric on pure quantum states. Treating
signals as conjugate variables allows us to create a new approach to segment
them.
  Keywords: Quantum information, transition probability, Euclidean distance,
Fubini-study metric, Bhattacharyya coefficients, conjugate variable,
signal/sensor fusion, signal and image segmentation.
</summary>
    <author>
      <name>Michael Nölle</name>
    </author>
    <author>
      <name>Martin Suda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 2 tables, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.5720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.5720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.0090v1</id>
    <updated>2011-09-01T04:47:08Z</updated>
    <published>2011-09-01T04:47:08Z</published>
    <title>An Efficient Codebook Initialization Approach for LBG Algorithm</title>
    <summary>  In VQ based image compression technique has three major steps namely (i)
Codebook Design, (ii) VQ Encoding Process and (iii) VQ Decoding Process. The
performance of VQ based image compression technique depends upon the
constructed codebook. A widely used technique for VQ codebook design is the
Linde-Buzo-Gray (LBG) algorithm. However the performance of the standard LBG
algorithm is highly dependent on the choice of the initial codebook. In this
paper, we have proposed a simple and very effective approach for codebook
initialization for LBG algorithm. The simulation results show that the proposed
scheme is computationally efficient and gives expected performance as compared
to the standard LBG algorithm.
</summary>
    <author>
      <name>Arup Kumar Pal</name>
    </author>
    <author>
      <name>Anup Sar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsea.2011.1407</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsea.2011.1407" rel="related"/>
    <link href="http://arxiv.org/abs/1109.0090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.0090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.1068v1</id>
    <updated>2011-09-06T05:34:28Z</updated>
    <published>2011-09-06T05:34:28Z</published>
    <title>An Automatic Clustering Technique for Optimal Clusters</title>
    <summary>  This paper proposes a simple, automatic and efficient clustering algorithm,
namely, Automatic Merging for Optimal Clusters (AMOC) which aims to generate
nearly optimal clusters for the given datasets automatically. The AMOC is an
extension to standard k-means with a two phase iterative procedure combining
certain validation techniques in order to find optimal clusters with automation
of merging of clusters. Experiments on both synthetic and real data have proved
that the proposed algorithm finds nearly optimal clustering structures in terms
of number of clusters, compactness and separation.
</summary>
    <author>
      <name>K. Karteeka Pavan</name>
    </author>
    <author>
      <name>Allam Appa Rao</name>
    </author>
    <author>
      <name>A. V. Dattatreya Rao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsea.2011.1412</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsea.2011.1412" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International journal of Computer Sciene Engineering and
  Applications, Vol., No.4, 2011, pp 133-144</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.1068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.1068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.3126v1</id>
    <updated>2011-09-14T16:24:26Z</updated>
    <published>2011-09-14T16:24:26Z</published>
    <title>A Non-Iterative Solution to the Four-Point Three-Views Pose Problem in
  Case of Collinear Cameras</title>
    <summary>  We give a non-iterative solution to a particular case of the four-point
three-views pose problem when three camera centers are collinear. Using the
well-known Cayley representation of orthogonal matrices, we derive from the
epipolar constraints a system of three polynomial equations in three variables.
The eliminant of that system is a multiple of a 36th degree univariate
polynomial. The true (unique) solution to the problem can be expressed in terms
of one of real roots of that polynomial. Experiments on synthetic data confirm
that our method is robust enough even in case of planar configurations.
</summary>
    <author>
      <name>Evgeniy Martyushev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.3126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.3126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.3850v2</id>
    <updated>2020-01-16T03:19:40Z</updated>
    <published>2011-09-18T07:48:36Z</published>
    <title>Digital (co)homology modules and digital Pontryagin algebras</title>
    <summary>  In the current study, we explore digital homology and cohomology modules, and
investigate their fundamental properties on pointed digital images. We also
examine pointed digital Hopf spaces and base point preserving digital Hopf
functions between the pointed digital Hopf spaces with suitable digital
multiplications, and explore the digital primitive homology and cohomology
classes, the digital Pontryagin algebras and coalgebras on the digital Hopf
spaces as digital images.
</summary>
    <author>
      <name>Dae-Woong Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.3850v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.3850v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.4683v1</id>
    <updated>2011-09-22T00:55:32Z</updated>
    <published>2011-09-22T00:55:32Z</published>
    <title>Detachable Object Detection: Segmentation and Depth Ordering From
  Short-Baseline Video</title>
    <summary>  We describe an approach for segmenting an image into regions that correspond
to surfaces in the scene that are partially surrounded by the medium. It
integrates both appearance and motion statistics into a cost functional, that
is seeded with occluded regions and minimized efficiently by solving a linear
programming problem. Where a short observation time is insufficient to
determine whether the object is detachable, the results of the minimization can
be used to seed a more costly optimization based on a longer sequence of video
data. The result is an entirely unsupervised scheme to detect and segment an
arbitrary and unknown number of objects. We test our scheme to highlight the
potential, as well as limitations, of our approach.
</summary>
    <author>
      <name>Alper Ayvaci</name>
    </author>
    <author>
      <name>Stefano Soatto</name>
    </author>
    <link href="http://arxiv.org/abs/1109.4683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.4683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.4744v1</id>
    <updated>2011-09-22T09:26:23Z</updated>
    <published>2011-09-22T09:26:23Z</published>
    <title>Probabilistic prototype models for attributed graphs</title>
    <summary>  This contribution proposes a new approach towards developing a class of
probabilistic methods for classifying attributed graphs. The key concept is
random attributed graph, which is defined as an attributed graph whose nodes
and edges are annotated by random variables. Every node/edge has two random
processes associated with it- occurence probability and the probability
distribution over the attribute values. These are estimated within the maximum
likelihood framework. The likelihood of a random attributed graph to generate
an outcome graph is used as a feature for classification. The proposed approach
is fast and robust to noise.
</summary>
    <author>
      <name>S. Deepak Srinivasan</name>
    </author>
    <author>
      <name>Klaus Obermayer</name>
    </author>
    <link href="http://arxiv.org/abs/1109.4744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.4744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.0872v1</id>
    <updated>2011-10-04T23:58:55Z</updated>
    <published>2011-10-04T23:58:55Z</published>
    <title>Non-Gaussian Scale Space Filtering with 2 by 2 Matrix of Linear Filters</title>
    <summary>  Construction of a scale space with a convolution filter has been studied
extensively in the past. It has been proven that the only convolution kernel
that satisfies the scale space requirements is a Gaussian type. In this paper,
we consider a matrix of convolution filters introduced in [1] as a building
kernel for a scale space, and shows that we can construct a non-Gaussian scale
space with a $2\times 2$ matrix of filters. The paper derives sufficient
conditions for the matrix of filters for being a scale space kernel, and
present some numerical demonstrations.
</summary>
    <author>
      <name>Toshiro Kubota</name>
    </author>
    <link href="http://arxiv.org/abs/1110.0872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.0872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.3194v1</id>
    <updated>2011-10-14T13:02:36Z</updated>
    <published>2011-10-14T13:02:36Z</published>
    <title>Controlled Total Variation regularization for inverse problems</title>
    <summary>  This paper provides a new algorithm for solving inverse problems, based on
the minimization of the $L^2$ norm and on the control of the Total Variation.
It consists in relaxing the role of the Total Variation in the classical Total
Variation minimization approach, which permits us to get better approximation
to the inverse problems. The numerical results on the deconvolution problem
show that our method outperforms some previous ones.
</summary>
    <author>
      <name>Qiyu Jin</name>
    </author>
    <author>
      <name>Ion Grama</name>
    </author>
    <author>
      <name>Quansheng Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures and 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.3194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.3194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.5404v1</id>
    <updated>2011-10-25T03:54:51Z</updated>
    <published>2011-10-25T03:54:51Z</published>
    <title>Face Recognition Based on SVM and 2DPCA</title>
    <summary>  The paper will present a novel approach for solving face recognition problem.
Our method combines 2D Principal Component Analysis (2DPCA), one of the
prominent methods for extracting feature vectors, and Support Vector Machine
(SVM), the most powerful discriminative method for classification. Experiments
based on proposed method have been conducted on two public data sets FERET and
AT&amp;T; the results show that the proposed method could improve the
classification rates.
</summary>
    <author>
      <name>Thai Hoang Le</name>
    </author>
    <author>
      <name>Len Bui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures, 2 tables, International Journal of Signal
  Processing, Image Processing and Pattern Recognition Vol. 4, No. 3,
  September, 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.5404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.5404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.0885v1</id>
    <updated>2011-11-03T15:46:47Z</updated>
    <published>2011-11-03T15:46:47Z</published>
    <title>Graph Regularized Nonnegative Matrix Factorization for Hyperspectral
  Data Unmixing</title>
    <summary>  Spectral unmixing is an important tool in hyperspectral data analysis for
estimating endmembers and abundance fractions in a mixed pixel. This paper
examines the applicability of a recently developed algorithm called graph
regularized nonnegative matrix factorization (GNMF) for this aim. The proposed
approach exploits the intrinsic geometrical structure of the data besides
considering positivity and full additivity constraints. Simulated data based on
the measured spectral signatures, is used for evaluating the proposed
algorithm. Results in terms of abundance angle distance (AAD) and spectral
angle distance (SAD) show that this method can effectively unmix hyperspectral
data.
</summary>
    <author>
      <name>Roozbeh Rajabi</name>
    </author>
    <author>
      <name>Mahdi Khodadadzadeh</name>
    </author>
    <author>
      <name>Hassan Ghassemian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.0885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.0885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.1014v1</id>
    <updated>2011-11-03T23:50:36Z</updated>
    <published>2011-11-03T23:50:36Z</published>
    <title>Sparsity and Robustness in Face Recognition</title>
    <summary>  This report concerns the use of techniques for sparse signal representation
and sparse error correction for automatic face recognition. Much of the recent
interest in these techniques comes from the paper "Robust Face Recognition via
Sparse Representation" by Wright et al. (2009), which showed how, under certain
technical conditions, one could cast the face recognition problem as one of
seeking a sparse representation of a given input face image in terms of a
"dictionary" of training images and images of individual pixels. In this
report, we have attempted to clarify some frequently encountered questions
about this work and particularly, on the validity of using sparse
representation techniques for face recognition.
</summary>
    <author>
      <name>John Wright</name>
    </author>
    <author>
      <name>Arvind Ganesh</name>
    </author>
    <author>
      <name>Allen Yang</name>
    </author>
    <author>
      <name>Zihan Zhou</name>
    </author>
    <author>
      <name>Yi Ma</name>
    </author>
    <link href="http://arxiv.org/abs/1111.1014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.1014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.1311v1</id>
    <updated>2011-11-05T14:09:05Z</updated>
    <published>2011-11-05T14:09:05Z</published>
    <title>Covariant fractional extension of the modified Laplace-operator used in
  3D-shape recovery</title>
    <summary>  Extending the Liouville-Caputo definition of a fractional derivative to a
nonlocal covariant generalization of arbitrary bound operators acting on
multidimensional Riemannian spaces an appropriate approach for the 3D shape
recovery of aperture afflicted 2D slide sequences is proposed. We demonstrate,
that the step from a local to a nonlocal algorithm yields an order of magnitude
in accuracy and by using the specific fractional approach an additional factor
2 in accuracy of the derived results.
</summary>
    <author>
      <name>Richard Herrmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2478/s13540-012-0024-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2478/s13540-012-0024-1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, draft for proceedings IFAC FDA12 in Nanjing,
  China</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Fract. Calc. Appl. Anal. (2012) Vol. 15 Num. 2, 332--343</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1111.1311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.1311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.2391v1</id>
    <updated>2011-11-10T04:28:08Z</updated>
    <published>2011-11-10T04:28:08Z</published>
    <title>A Novel Approach to Texture classification using statistical feature</title>
    <summary>  Texture is an important spatial feature which plays a vital role in content
based image retrieval. The enormous growth of the internet and the wide use of
digital data have increased the need for both efficient image database creation
and retrieval procedure. This paper describes a new approach for texture
classification by combining statistical texture features of Local Binary
Pattern and Texture spectrum. Since most significant information of a texture
often appears in the high frequency channels, the features are extracted by the
computation of LBP and Texture Spectrum and Legendre Moments. Euclidean
distance is used for similarity measurement. The experimental result shows that
97.77% classification accuracy is obtained by the proposed method.
</summary>
    <author>
      <name>B. Vijayalakshmi</name>
    </author>
    <author>
      <name>V. Subbiah Bharathi</name>
    </author>
    <link href="http://arxiv.org/abs/1111.2391v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.2391v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.3818v1</id>
    <updated>2011-11-16T14:37:07Z</updated>
    <published>2011-11-16T14:37:07Z</published>
    <title>Good Pairs of Adjacency Relations in Arbitrary Dimensions</title>
    <summary>  In this text we show, that the notion of a "good pair" that was introduced in
the paper "Digital Manifolds and the Theorem of Jordan-Brouwer" has actually
known models. We will show, how to choose cubical adjacencies, the
generalizations of the well known 4- and 8-neighborhood to arbitrary
dimensions, in order to find good pairs. Furthermore, we give another proof for
the well known fact that the Khalimsky-topology implies good pairs. The outcome
is consistent with the known theory as presented by T.Y. Kong, A. Rosenfeld,
G.T. Herman and M. Khachan et.al and gives new insights in higher dimensions.
</summary>
    <author>
      <name>Martin Hünniger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.3818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.3818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.4290v1</id>
    <updated>2011-11-18T06:34:07Z</updated>
    <published>2011-11-18T06:34:07Z</published>
    <title>A Single Euler Number Feature for Multi-font Multi-size Kannada Numeral
  Recognition</title>
    <summary>  In this paper a novel approach is proposed based on single Euler number
feature which is free from thinning and size normalization for multi-font and
multi-size Kannada numeral recognition system. A nearest neighbor
classification is used for classification of Kannada numerals by considering
the Euclidian distance. A total 1500 numeral images with different font sizes
between (10..84) are tested for algorithm efficiency and the overall the
classification accuracy is found to be 99.00% .The said method is thinning
free, fast, and showed encouraging results on varying font styles and sizes of
Kannada numerals.
</summary>
    <author>
      <name>B. V. Dhandra</name>
    </author>
    <author>
      <name>R. G. Benne</name>
    </author>
    <author>
      <name>Mallikarjun Hangarge</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure, 5 tables, "Recent Trends in Information
  Technology(RTIT-2009)"</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.4290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.4290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.4654v1</id>
    <updated>2011-11-20T17:41:01Z</updated>
    <published>2011-11-20T17:41:01Z</published>
    <title>A self-portrait of young Leonardo</title>
    <summary>  One of the most famous drawings by Leonardo da Vinci is a self-portrait in
red chalk, where he looks quite old. In fact, there is a sketch in one of his
notebooks, partially covered by written notes, that can be a self-portrait of
the artist when he was young. The use of image processing, to remove the
handwritten text and improve the image, allows a comparison of the two
portraits.
</summary>
    <author>
      <name>Amelia Carolina Sparavigna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Image processing, digital restoration, Leonardo da Vinci</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.4654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.4654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.1417v1</id>
    <updated>2011-11-29T06:24:33Z</updated>
    <published>2011-11-29T06:24:33Z</published>
    <title>Picture Collage with Genetic Algorithm and Stereo vision</title>
    <summary>  In this paper, a salient region extraction method for creating picture
collage based on stereo vision is proposed. Picture collage is a kind of visual
image summary to arrange all input images on a given canvas, allowing overlay,
to maximize visible visual information. The salient regions of each image are
firstly extracted and represented as a depth map. The output picture collage
shows as many visible salient regions (without being overlaid by others) from
all images as possible. A very efficient Genetic algorithm is used here for the
optimization. The experimental results showed the superior performance of the
proposed method.
</summary>
    <author>
      <name>Hesam Ekhtiyar</name>
    </author>
    <author>
      <name>Mahdi Sheida</name>
    </author>
    <author>
      <name>Mahmood Amintoosi</name>
    </author>
    <link href="http://arxiv.org/abs/1201.1417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.1417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.2050v1</id>
    <updated>2012-01-10T13:41:56Z</updated>
    <published>2012-01-10T13:41:56Z</published>
    <title>Adaptive Noise Reduction Scheme for Salt and Pepper</title>
    <summary>  In this paper, a new adaptive noise reduction scheme for images corrupted by
impulse noise is presented. The proposed scheme efficiently identifies and
reduces salt and pepper noise. MAG (Mean Absolute Gradient) is used to identify
pixels which are most likely corrupted by salt and pepper noise that are
candidates for further median based noise reduction processing. Directional
filtering is then applied after noise reduction to achieve a good tradeoff
between detail preservation and noise removal. The proposed scheme can remove
salt and pepper noise with noise density as high as 90% and produce better
result in terms of qualitative and quantitative measures of images.
</summary>
    <author>
      <name>Tina Gebreyohannes</name>
    </author>
    <author>
      <name>Dong-Yoon Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.2050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.2050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.3109v1</id>
    <updated>2012-01-15T17:42:07Z</updated>
    <published>2012-01-15T17:42:07Z</published>
    <title>Automatic system for counting cells with elliptical shape</title>
    <summary>  This paper presents a new method for automatic quantification of ellipse-like
cells in images, an important and challenging problem that has been studied by
the computer vision community. The proposed method can be described by two main
steps. Initially, image segmentation based on the k-means algorithm is
performed to separate different types of cells from the background. Then, a
robust and efficient strategy is performed on the blob contour for touching
cells splitting. Due to the contour processing, the method achieves excellent
results of detection compared to manual detection performed by specialists.
</summary>
    <author>
      <name>Wesley Nunes Gonçalves</name>
    </author>
    <author>
      <name>Odemir Martinez Bruno</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Learning and NonLinear Models, Volume 9, Issue 1, 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.3109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.3109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.3803v1</id>
    <updated>2012-01-16T07:33:56Z</updated>
    <published>2012-01-16T07:33:56Z</published>
    <title>Image Labeling and Segmentation using Hierarchical Conditional Random
  Field Model</title>
    <summary>  The use of hierarchical Conditional Random Field model deal with the problem
of labeling images . At the time of labeling a new image, selection of the
nearest cluster and using the related CRF model to label this image. When one
give input image, one first use the CRF model to get initial pixel labels then
finding the cluster with most similar images. Then at last relabeling the input
image by the CRF model associated with this cluster. This paper presents a
approach to label and segment specific image having correct information.
</summary>
    <author>
      <name>Manoj K. Vairalkar</name>
    </author>
    <author>
      <name>Sonali. Nimbhorkar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">08 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.3803v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.3803v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.0549v1</id>
    <updated>2012-01-29T19:19:33Z</updated>
    <published>2012-01-29T19:19:33Z</published>
    <title>Comparing Background Subtraction Algorithms and Method of Car Counting</title>
    <summary>  In this paper, we compare various image background subtraction algorithms
with the ground truth of cars counted. We have given a sample of thousand
images, which are the snap shots of current traffic as records at various
intersections and highways. We have also counted an approximate number of cars
that are visible in these images. In order to ascertain the accuracy of
algorithms to be used for the processing of million images, we compare them on
many metrics that includes (i) Scalability (ii) Accuracy (iii) Processing time.
</summary>
    <author>
      <name>Gautam S. Thakur</name>
    </author>
    <author>
      <name>Mohsen Ali</name>
    </author>
    <author>
      <name>Pan Hui</name>
    </author>
    <author>
      <name>Ahmed Helmy</name>
    </author>
    <link href="http://arxiv.org/abs/1202.0549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.0549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.1585v1</id>
    <updated>2012-02-08T03:07:39Z</updated>
    <published>2012-02-08T03:07:39Z</published>
    <title>Robust seed selection algorithm for k-means type algorithms</title>
    <summary>  Selection of initial seeds greatly affects the quality of the clusters and in
k-means type algorithms. Most of the seed selection methods result different
results in different independent runs. We propose a single, optimal, outlier
insensitive seed selection algorithm for k-means type algorithms as extension
to k-means++. The experimental results on synthetic, real and on microarray
data sets demonstrated that effectiveness of the new algorithm in producing the
clustering results
</summary>
    <author>
      <name>K. Karteeka Pavan</name>
    </author>
    <author>
      <name>Allam Appa Rao</name>
    </author>
    <author>
      <name>A. V. Dattatreya Rao</name>
    </author>
    <author>
      <name>G. R. Sridhar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsit.2011.3513</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsit.2011.3513" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 5 tables, 9figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Technology (IJCSIT),
  Vol 3, No 5, Oct 2011 pp 147-163</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.1585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.1585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.2528v1</id>
    <updated>2012-02-12T13:40:11Z</updated>
    <published>2012-02-12T13:40:11Z</published>
    <title>Using Covariance Matrices as Feature Descriptors for Vehicle Detection
  from a Fixed Camera</title>
    <summary>  A method is developed to distinguish between cars and trucks present in a
video feed of a highway. The method builds upon previously done work using
covariance matrices as an accurate descriptor for regions. Background
subtraction and other similar proven image processing techniques are used to
identify the regions where the vehicles are most likely to be, and a distance
metric comparing the vehicle inside the region to a fixed library of vehicles
is used to determine the class of vehicle.
</summary>
    <author>
      <name>Kevin Mader</name>
    </author>
    <author>
      <name>Gil Reese</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Written as part of the requirements for the SC/EC520 course in
  Digital Image Processing at Boston University</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.2528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.2528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.3884v1</id>
    <updated>2012-02-17T11:41:28Z</updated>
    <published>2012-02-17T11:41:28Z</published>
    <title>A feature extraction technique based on character geometry for character
  recognition</title>
    <summary>  This paper describes a geometry based technique for feature extraction
applicable to segmentation-based word recognition systems. The proposed system
extracts the geometric features of the character contour. This features are
based on the basic line types that forms the character skeleton. The system
gives a feature vector as its output. The feature vectors so generated from a
training set, were then used to train a pattern recognition engine based on
Neural Networks so that the system can be benchmarked.
</summary>
    <author>
      <name>Dinesh Dileep Gaurav</name>
    </author>
    <author>
      <name>Renu Ramesh</name>
    </author>
    <link href="http://arxiv.org/abs/1202.3884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.3884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.4495v1</id>
    <updated>2012-02-20T23:48:38Z</updated>
    <published>2012-02-20T23:48:38Z</published>
    <title>Stochastic-Based Pattern Recognition Analysis</title>
    <summary>  In this work we review the basic principles of stochastic logic and propose
its application to probabilistic-based pattern-recognition analysis. The
proposed technique is intrinsically a parallel comparison of input data to
various pre-stored categories using Bayesian techniques. We design smart
pulse-based stochastic-logic blocks to provide an efficient pattern recognition
analysis. The proposed rchitecture is applied to a specific navigation problem.
The resulting system is orders of magnitude faster than processor-based
solutions.
</summary>
    <author>
      <name>V. Canals</name>
    </author>
    <author>
      <name>A. Morro</name>
    </author>
    <author>
      <name>J. L. Rosselló</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patrec.2010.07.008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patrec.2010.07.008" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Pattern Recognition Letters in 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.4495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.4495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.0265v1</id>
    <updated>2012-02-29T17:57:12Z</updated>
    <published>2012-02-29T17:57:12Z</published>
    <title>Image Fusion and Re-Modified SPIHT for Fused Image</title>
    <summary>  This paper presents the Discrete Wavelet based fusion techniques for
combining perceptually important image features. SPIHT (Set Partitioning in
Hierarchical Trees) algorithm is an efficient method for lossy and lossless
coding of fused image. This paper presents some modifications on the SPIHT
algorithm. It is based on the idea of insignificant correlation of wavelet
coefficient among the medium and high frequency sub bands. In RE-MSPIHT
algorithm, wavelet coefficients are scaled prior to SPIHT coding based on the
sub band importance, with the goal of minimizing the MSE.
</summary>
    <author>
      <name>S. Chitra</name>
    </author>
    <author>
      <name>J. B. Bhattacharjee</name>
    </author>
    <author>
      <name>B. Thilakavathi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 143-158</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.0265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.0265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.1513v2</id>
    <updated>2012-03-08T10:29:32Z</updated>
    <published>2012-03-05T17:12:42Z</published>
    <title>Invariant Scattering Convolution Networks</title>
    <summary>  A wavelet scattering network computes a translation invariant image
representation, which is stable to deformations and preserves high frequency
information for classification. It cascades wavelet transform convolutions with
non-linear modulus and averaging operators. The first network layer outputs
SIFT-type descriptors whereas the next layers provide complementary invariant
information which improves classification. The mathematical analysis of wavelet
scattering networks explains important properties of deep convolution networks
for classification.
  A scattering representation of stationary processes incorporates higher order
moments and can thus discriminate textures having the same Fourier power
spectrum. State of the art classification results are obtained for handwritten
digits and texture discrimination, using a Gaussian kernel SVM and a generative
PCA classifier.
</summary>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Stéphane Mallat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages double column, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.1513v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.1513v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.1765v1</id>
    <updated>2012-03-08T12:07:27Z</updated>
    <published>2012-03-08T12:07:27Z</published>
    <title>A comparative evaluation of two algorithms of detection of masses on
  mammograms</title>
    <summary>  In this paper, we implement and carry out the comparison of two methods of
computer-aided-detection of masses on mammograms. The two algorithms basically
consist of 3 steps each: segmentation, binarization and noise suppression using
different techniques for each step. A database of 60 images was used to compare
the performance of the two algorithms in terms of general detection efficiency,
conservation of size and shape of detected masses.
</summary>
    <author>
      <name>Guillaume Kom</name>
    </author>
    <author>
      <name>Alain Tiedeu</name>
    </author>
    <author>
      <name>Martin Kom</name>
    </author>
    <author>
      <name>John Ngundam</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/sipij.2012.3102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/sipij.2012.3102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures, 1 table, Vol.3, No.1, February 2012,pp19-27;
  Signal &amp; Image Processing : An International Journal (SIPIJ),2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.1765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.1765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3114v1</id>
    <updated>2012-03-14T15:31:16Z</updated>
    <published>2012-03-14T15:31:16Z</published>
    <title>Integrated three-dimensional reconstruction using reflectance fields</title>
    <summary>  A method to obtain three-dimensional data of real-world objects by
integrating their material properties is presented. The material properties are
defined by capturing the Reflectance Fields of the real-world objects. It is
shown, unlike conventional reconstruction methods, the method is able to use
the reflectance information to recover surface depth for objects having a
non-Lambertian surface reflectance. It is, for recovering 3D data of objects
exhibiting an anisotropic BRDF with an error less than 0.3%.
</summary>
    <author>
      <name>Maria-Luisa Sosas</name>
    </author>
    <author>
      <name>Miguel-Octavio Arias</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures; Published in IJCSI Journal, Volume 9, Issue 1,
  No. 3, January 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.3114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3230v1</id>
    <updated>2012-03-14T22:46:29Z</updated>
    <published>2012-03-14T22:46:29Z</published>
    <title>Reconstruction error in a motion capture system</title>
    <summary>  Marker-based motion capture (MoCap) systems can be composed by several dozens
of cameras with the purpose of reconstructing the trajectories of hundreds of
targets. With a large amount of cameras it becomes interesting to determine the
optimal reconstruction strategy. For such aim it is of fundamental importance
to understand the information provided by different camera measurements and how
they are combined, i.e. how the reconstruction error changes by considering
different cameras. In this work, first, an approximation of the reconstruction
error variance is derived. The results obtained in some simulations suggest
that the proposed strategy allows to obtain a good approximation of the real
error variance with significant reduction of the computational time.
</summary>
    <author>
      <name>Andrea Masiero</name>
    </author>
    <author>
      <name>Angelo Cenedese</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.3230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.1198v1</id>
    <updated>2012-04-05T12:28:11Z</updated>
    <published>2012-04-05T12:28:11Z</published>
    <title>A Complete Workflow for Development of Bangla OCR</title>
    <summary>  Developing a Bangla OCR requires bunch of algorithm and methods. There were
many effort went on for developing a Bangla OCR. But all of them failed to
provide an error free Bangla OCR. Each of them has some lacking. We discussed
about the problem scope of currently existing Bangla OCR's. In this paper, we
present the basic steps required for developing a Bangla OCR and a complete
workflow for development of a Bangla OCR with mentioning all the possible
algorithms required.
</summary>
    <author>
      <name>Farjana Yeasmin Omee</name>
    </author>
    <author>
      <name>Shiam Shabbir Himel</name>
    </author>
    <author>
      <name>Md. Abu Naser Bikas</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications, Volume 21, No.9,
  May 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.1198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.1198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.1611v1</id>
    <updated>2012-04-07T08:17:40Z</updated>
    <published>2012-04-07T08:17:40Z</published>
    <title>Vision-based Human Gender Recognition: A Survey</title>
    <summary>  Gender is an important demographic attribute of people. This paper provides a
survey of human gender recognition in computer vision. A review of approaches
exploiting information from face and whole body (either from a still image or
gait sequence) is presented. We highlight the challenges faced and survey the
representative methods of these approaches. Based on the results, good
performance have been achieved for datasets captured under controlled
environments, but there is still much work that can be done to improve the
robustness of gender recognition under real-life environments.
</summary>
    <author>
      <name>Choon Boon Ng</name>
    </author>
    <author>
      <name>Yong Haur Tay</name>
    </author>
    <author>
      <name>Bok Min Goi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.1611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.1611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.1811v1</id>
    <updated>2012-04-09T06:44:42Z</updated>
    <published>2012-04-09T06:44:42Z</published>
    <title>Skin-color based videos categorization</title>
    <summary>  On dedicated websites, people can upload videos and share it with the rest of
the world. Currently these videos are cat- egorized manually by the help of the
user community. In this paper, we propose a combination of color spaces with
the Bayesian network approach for robust detection of skin color followed by an
automated video categorization. Exper- imental results show that our method can
achieve satisfactory performance for categorizing videos based on skin color.
</summary>
    <author>
      <name>Rehanullah Khan</name>
    </author>
    <author>
      <name>Asad Maqsood</name>
    </author>
    <author>
      <name>Zeeshan Khan</name>
    </author>
    <author>
      <name>Muhammad Ishaq</name>
    </author>
    <author>
      <name>Arsalan Arif</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues (IJCSI), Volume 9,
  Issue 1, No 3, January 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.1811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.1811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2062v1</id>
    <updated>2012-04-10T07:10:06Z</updated>
    <published>2012-04-10T07:10:06Z</published>
    <title>SVD-EBP Algorithm for Iris Pattern Recognition</title>
    <summary>  This paper proposes a neural network approach based on Error Back Propagation
(EBP) for classification of different eye images. To reduce the complexity of
layered neural network the dimensions of input vectors are optimized using
Singular Value Decomposition (SVD). The main of this work is to provide for
best method for feature extraction and classification. The details of this
combined system named as SVD-EBP system, and results thereof are presented in
this paper.
  Keywords- Singular value decomposition(SVD), Error back Propagation(EBP).
</summary>
    <author>
      <name>Babasaheb G. Patil</name>
    </author>
    <author>
      <name>Shaila Subbaraman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Dec2011-volume2.Issue 12 (IJACSA)</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.2062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2741v1</id>
    <updated>2012-04-12T14:47:41Z</updated>
    <published>2012-04-12T14:47:41Z</published>
    <title>Simultaneous Object Detection, Tracking, and Event Recognition</title>
    <summary>  The common internal structure and algorithmic organization of object
detection, detection-based tracking, and event recognition facilitates a
general approach to integrating these three components. This supports
multidirectional information flow between these components allowing object
detection to influence tracking and event recognition and event recognition to
influence tracking and object detection. The performance of the combination can
exceed the performance of the components in isolation. This can be done with
linear asymptotic complexity.
</summary>
    <author>
      <name>Andrei Barbu</name>
    </author>
    <author>
      <name>Aaron Michaux</name>
    </author>
    <author>
      <name>Siddharth Narayanaswamy</name>
    </author>
    <author>
      <name>Jeffrey Mark Siskind</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Cognitive Systems, Vol. 2, pp. 203-220, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.2741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.3618v1</id>
    <updated>2012-04-13T20:16:11Z</updated>
    <published>2012-04-13T20:16:11Z</published>
    <title>Compensating Interpolation Distortion by Using New Optimized Modular
  Method</title>
    <summary>  A modular method was suggested before to recover a band limited signal from
the sample and hold and linearly interpolated (or, in general, an
nth-order-hold) version of the regular samples. In this paper a novel approach
for compensating the distortion of any interpolation based on modular method
has been proposed. In this method the performance of the modular method is
optimized by adding only some simply calculated coefficients. This approach
causes drastic improvement in terms of signal-to-noise ratios with fewer
modules compared to the classical modular method. Simulation results clearly
confirm the improvement of the proposed method and also its superior robustness
against additive noise.
</summary>
    <author>
      <name>Mohammad Tofighi</name>
    </author>
    <author>
      <name>Ali Ayremlou</name>
    </author>
    <author>
      <name>Farokh Marvasti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages. Journal paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.3618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.3618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.3999v1</id>
    <updated>2012-05-17T18:15:45Z</updated>
    <published>2012-05-17T18:15:45Z</published>
    <title>Optimal Weights Mixed Filter for Removing Mixture of Gaussian and
  Impulse Noises</title>
    <summary>  According to the character of Gaussian, we modify the Rank-Ordered Absolute
Differences (ROAD) to Rank-Ordered Absolute Differences of mixture of Gaussian
and impulse noises (ROADG). It will be more effective to detect impulse noise
when the impulse is mixed with Gaussian noise. Combining rightly the ROADG with
Optimal Weights Filter (OWF), we obtain a new method to deal with the mixed
noise, called Optimal Weights Mixed Filter (OWMF). The simulation results show
that the method is effective to remove the mixed noise.
</summary>
    <author>
      <name>Qiyu Jin</name>
    </author>
    <author>
      <name>Ion Grama</name>
    </author>
    <author>
      <name>Quansheng Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures and 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.3999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.3999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.4463v2</id>
    <updated>2012-12-30T00:58:09Z</updated>
    <published>2012-05-20T22:07:27Z</published>
    <title>Pilgrims Face Recognition Dataset -- HUFRD</title>
    <summary>  In this work, we define a new pilgrims face recognition dataset, called HUFRD
dataset. The new developed dataset presents various pilgrims' images taken from
outside the Holy Masjid El-Harram in Makkah during the 2011-2012 Hajj and Umrah
seasons. Such dataset will be used to test our developed facial recognition and
detection algorithms, as well as assess in the missing and found recognition
system \cite{crowdsensing}.
</summary>
    <author>
      <name>Salah A. Aly</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 13 images, 1 table of a new HUFRD work</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.4463v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.4463v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.4831v1</id>
    <updated>2012-05-22T08:00:45Z</updated>
    <published>2012-05-22T08:00:45Z</published>
    <title>Gray Level Co-Occurrence Matrices: Generalisation and Some New Features</title>
    <summary>  Gray Level Co-occurrence Matrices (GLCM) are one of the earliest techniques
used for image texture analysis. In this paper we defined a new feature called
trace extracted from the GLCM and its implications in texture analysis are
discussed in the context of Content Based Image Retrieval (CBIR). The
theoretical extension of GLCM to n-dimensional gray scale images are also
discussed. The results indicate that trace features outperform Haralick
features when applied to CBIR.
</summary>
    <author>
      <name>Bino Sebastian V</name>
    </author>
    <author>
      <name>A. Unnikrishnan</name>
    </author>
    <author>
      <name>Kannan Balakrishnan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.4831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.4831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.0238v1</id>
    <updated>2012-06-01T16:20:41Z</updated>
    <published>2012-06-01T16:20:41Z</published>
    <title>Rapid Feature Extraction for Optical Character Recognition</title>
    <summary>  Feature extraction is one of the fundamental problems of character
recognition. The performance of character recognition system is depends on
proper feature extraction and correct classifier selection. In this article, a
rapid feature extraction method is proposed and named as Celled Projection (CP)
that compute the projection of each section formed through partitioning an
image. The recognition performance of the proposed method is compared with
other widely used feature extraction methods that are intensively studied for
many different scripts in literature. The experiments have been conducted using
Bangla handwritten numerals along with three different well known classifiers
which demonstrate comparable results including 94.12% recognition accuracy
using celled projection.
</summary>
    <author>
      <name>M. Zahid Hossain</name>
    </author>
    <author>
      <name>M. Ashraful Amin</name>
    </author>
    <author>
      <name>Hong Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.0238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.0238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.2; I.7.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.2437v1</id>
    <updated>2012-06-12T04:23:38Z</updated>
    <published>2012-06-12T04:23:38Z</published>
    <title>A Novel Windowing Technique for Efficient Computation of MFCC for
  Speaker Recognition</title>
    <summary>  In this paper, we propose a novel family of windowing technique to compute
Mel Frequency Cepstral Coefficient (MFCC) for automatic speaker recognition
from speech. The proposed method is based on fundamental property of discrete
time Fourier transform (DTFT) related to differentiation in frequency domain.
Classical windowing scheme such as Hamming window is modified to obtain
derivatives of discrete time Fourier transform coefficients. It has been
mathematically shown that the slope and phase of power spectrum are inherently
incorporated in newly computed cepstrum. Speaker recognition systems based on
our proposed family of window functions are shown to attain substantial and
consistent performance improvement over baseline single tapered Hamming window
as well as recently proposed multitaper windowing technique.
</summary>
    <author>
      <name>Md. Sahidullah</name>
    </author>
    <author>
      <name>Goutam Saha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2012.2235067</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2012.2235067" rel="related"/>
    <link href="http://arxiv.org/abs/1206.2437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.2437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.2807v1</id>
    <updated>2012-06-13T13:49:23Z</updated>
    <published>2012-06-13T13:49:23Z</published>
    <title>An efficient hierarchical graph based image segmentation</title>
    <summary>  Hierarchical image segmentation provides region-oriented scalespace, i.e., a
set of image segmentations at different detail levels in which the
segmentations at finer levels are nested with respect to those at coarser
levels. Most image segmentation algorithms, such as region merging algorithms,
rely on a criterion for merging that does not lead to a hierarchy, and for
which the tuning of the parameters can be difficult. In this work, we propose a
hierarchical graph based image segmentation relying on a criterion popularized
by Felzenzwalb and Huttenlocher. We illustrate with both real and synthetic
images, showing efficiency, ease of use, and robustness of our method.
</summary>
    <author>
      <name>Silvio Jamil F. Guimarães</name>
    </author>
    <author>
      <name>Jean Cousty</name>
    </author>
    <author>
      <name>Yukiko Kenmochi</name>
    </author>
    <author>
      <name>Laurent Najman</name>
    </author>
    <link href="http://arxiv.org/abs/1206.2807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.2807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4609v1</id>
    <updated>2012-06-18T14:45:17Z</updated>
    <published>2012-06-18T14:45:17Z</published>
    <title>On multi-view feature learning</title>
    <summary>  Sparse coding is a common approach to learning local features for object
recognition. Recently, there has been an increasing interest in learning
features from spatio-temporal, binocular, or other multi-observation data,
where the goal is to encode the relationship between images rather than the
content of a single image. We provide an analysis of multi-view feature
learning, which shows that hidden variables encode transformations by detecting
rotation angles in the eigenspaces shared among multiple image warps. Our
analysis helps explain recent experimental results showing that
transformation-specific features emerge when training complex cell models on
videos. Our analysis also shows that transformation-invariant features can
emerge as a by-product of learning representations of transformations.
</summary>
    <author>
      <name>Roland Memisevic</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Frankfurt</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4866v1</id>
    <updated>2012-06-21T13:14:59Z</updated>
    <published>2012-06-21T13:14:59Z</published>
    <title>Portraits of Julius Caesar: a proposal for 3D analysis</title>
    <summary>  Here I suggest the use of a 3D scanning and rendering to create some virtual
copies of ancient artifacts to study and compare them. In particular, this
approach could be interesting for some roman marble busts, two of which are
portraits of Julius Caesar, and the third is a realistic portrait of a man
recently found at Arles, France. The comparison of some images indicates that a
three-dimensional visualization is necessary.
</summary>
    <author>
      <name>Amelia Carolina Sparavigna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Key-words: Image processing, 3D Scanner, 3D visualization, Ancient
  Rome, Julius Caesar</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.4866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6437v1</id>
    <updated>2012-06-27T19:59:59Z</updated>
    <published>2012-06-27T19:59:59Z</published>
    <title>Large Scale Variational Bayesian Inference for Structured Scale Mixture
  Models</title>
    <summary>  Natural image statistics exhibit hierarchical dependencies across multiple
scales. Representing such prior knowledge in non-factorial latent tree models
can boost performance of image denoising, inpainting, deconvolution or
reconstruction substantially, beyond standard factorial "sparse" methodology.
We derive a large scale approximate Bayesian inference algorithm for linear
models with non-factorial (latent tree-structured) scale mixture priors.
Experimental results on a range of denoising and inpainting problems
demonstrate substantially improved performance compared to MAP estimation or to
inference with factorial priors.
</summary>
    <author>
      <name>Young Jun Ko</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Ecole Polytechnique Federale de Lausanne</arxiv:affiliation>
    </author>
    <author>
      <name>Matthias Seeger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Ecole Polytechnique Federale de Lausanne</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the 29th International Conference on
  Machine Learning (ICML 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.0805v3</id>
    <updated>2013-03-30T05:25:58Z</updated>
    <published>2012-07-03T14:32:20Z</published>
    <title>Anatomical Structure Segmentation in Liver MRI Images</title>
    <summary>  Segmentation of medical images is a challenging task owing to their
complexity. A standard segmentation problem within Magnetic Resonance Imaging
(MRI) is the task of labeling voxels according to their tissue type. Image
segmentation provides volumetric quantification of liver area and thus helps in
the diagnosis of disorders, such as Hepatitis, Cirrhosis, Jaundice,
Hemochromatosis etc.This work deals with comparison of segmentation by applying
Level Set Method,Fuzzy Level Information C-Means Clustering Algorithm and
Gradient Vector Flow Snake Algorithm.The results are compared using the
parameters such as Number of pixels correctly classified, and percentage of
area segmented.
</summary>
    <author>
      <name>G. Geethu Lakshmi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Withdrawn by author for final modification</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.0805v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.0805v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.1522v1</id>
    <updated>2012-07-06T04:58:52Z</updated>
    <published>2012-07-06T04:58:52Z</published>
    <title>Multimodal similarity-preserving hashing</title>
    <summary>  We introduce an efficient computational framework for hashing data belonging
to multiple modalities into a single representation space where they become
mutually comparable. The proposed approach is based on a novel coupled siamese
neural network architecture and allows unified treatment of intra- and
inter-modality similarity learning. Unlike existing cross-modality similarity
learning approaches, our hashing functions are not limited to binarized linear
projections and can assume arbitrarily complex forms. We show experimentally
that our method significantly outperforms state-of-the-art hashing approaches
on multimedia retrieval tasks.
</summary>
    <author>
      <name>Jonathan Masci</name>
    </author>
    <author>
      <name>Michael M. Bronstein</name>
    </author>
    <author>
      <name>Alexander A. Bronstein</name>
    </author>
    <author>
      <name>Jürgen Schmidhuber</name>
    </author>
    <link href="http://arxiv.org/abs/1207.1522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.1522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.2922v1</id>
    <updated>2012-07-12T11:11:35Z</updated>
    <published>2012-07-12T11:11:35Z</published>
    <title>ROI Segmentation for Feature Extraction from Human Facial Images</title>
    <summary>  Human Computer Interaction (HCI) is the biggest goal of computer vision
researchers. Features form the different facial images are able to provide a
very deep knowledge about the activities performed by the different facial
movements. In this paper we presented a technique for feature extraction from
various regions of interest with the help of Skin color segmentation technique,
Thresholding, knowledge based technique for face recognition.
</summary>
    <author>
      <name> Surbhi</name>
    </author>
    <author>
      <name>Vishal Arora</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures; International Journal of Research in Computer
  Science, pp. 61-64 (2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.2922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.2922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.3510v2</id>
    <updated>2012-12-18T22:15:36Z</updated>
    <published>2012-07-15T14:50:17Z</published>
    <title>HMRF-EM-image: Implementation of the Hidden Markov Random Field Model
  and its Expectation-Maximization Algorithm</title>
    <summary>  In this project, we study the hidden Markov random field (HMRF) model and its
expectation-maximization (EM) algorithm. We implement a MATLAB toolbox named
HMRF-EM-image for 2D image segmentation using the HMRF-EM framework. This
toolbox also implements edge-prior-preserving image segmentation, and can be
easily reconfigured for other problems, such as 3D image segmentation.
</summary>
    <author>
      <name>Quan Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work originally appears as the final project of Prof. Birsen
  Yazici's course Detection and Estimation Theory at RPI</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.3510v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.3510v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.3607v1</id>
    <updated>2012-07-16T09:23:06Z</updated>
    <published>2012-07-16T09:23:06Z</published>
    <title>Fusing image representations for classification using support vector
  machines</title>
    <summary>  In order to improve classification accuracy different image representations
are usually combined. This can be done by using two different fusing schemes.
In feature level fusion schemes, image representations are combined before the
classification process. In classifier fusion, the decisions taken separately
based on individual representations are fused to make a decision. In this paper
the main methods derived for both strategies are evaluated. Our experimental
results show that classifier fusion performs better. Specifically Bayes belief
integration is the best performing strategy for image classification task.
</summary>
    <author>
      <name>Can Demirkesen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">BIT Lab, LJK</arxiv:affiliation>
    </author>
    <author>
      <name>Hocine Cherifi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">BIT Lab, Le2i</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IVCNZ.2009.5378367</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IVCNZ.2009.5378367" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Image and Vision Computing New Zealand, 2009. IVCNZ '09. 24th
  International Conference, Wellington : Nouvelle-Z\'elande (2009)</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.3607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.3607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.5007v1</id>
    <updated>2012-07-20T17:37:27Z</updated>
    <published>2012-07-20T17:37:27Z</published>
    <title>Multisegmentation through wavelets: Comparing the efficacy of Daubechies
  vs Coiflets</title>
    <summary>  In this paper, we carry out a comparative study of the efficacy of wavelets
belonging to Daubechies and Coiflet family in achieving image segmentation
through a fast statistical algorithm.The fact that wavelets belonging to
Daubechies family optimally capture the polynomial trends and those of Coiflet
family satisfy mini-max condition, makes this comparison interesting. In the
context of the present algorithm, it is found that the performance of Coiflet
wavelets is better, as compared to Daubechies wavelet.
</summary>
    <author>
      <name>Madhur Srivastava</name>
    </author>
    <author>
      <name>Yashwant Yashu</name>
    </author>
    <author>
      <name>Satish K. Singh</name>
    </author>
    <author>
      <name>Prasanta K. Panigrahi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings in Signal Processing and Real Time Operating System (
  SPRTOS), March 26 - 27 , 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1207.5007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.5007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.6774v1</id>
    <updated>2012-07-29T13:07:09Z</updated>
    <published>2012-07-29T13:07:09Z</published>
    <title>A Survey Of Activity Recognition And Understanding The Behavior In Video
  Survelliance</title>
    <summary>  This paper presents a review of human activity recognition and behaviour
understanding in video sequence. The key objective of this paper is to provide
a general review on the overall process of a surveillance system used in the
current trend. Visual surveillance system is directed on automatic
identification of events of interest, especially on tracking and classification
of moving objects. The processing step of the video surveillance system
includes the following stages: Surrounding model, object representation, object
tracking, activity recognition and behaviour understanding. It describes
techniques that use to define a general set of activities that are applicable
to a wide range of scenes and environments in video sequence.
</summary>
    <author>
      <name>A. R. Revathi</name>
    </author>
    <author>
      <name>Dhananjay Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.6774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.6774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.0378v1</id>
    <updated>2012-08-02T00:54:02Z</updated>
    <published>2012-08-02T00:54:02Z</published>
    <title>Fast Planar Correlation Clustering for Image Segmentation</title>
    <summary>  We describe a new optimization scheme for finding high-quality correlation
clusterings in planar graphs that uses weighted perfect matching as a
subroutine. Our method provides lower-bounds on the energy of the optimal
correlation clustering that are typically fast to compute and tight in
practice. We demonstrate our algorithm on the problem of image segmentation
where this approach outperforms existing global optimization techniques in
minimizing the objective and is competitive with the state of the art in
producing high-quality segmentations.
</summary>
    <author>
      <name>Julian Yarkony</name>
    </author>
    <author>
      <name>Alexander T. Ihler</name>
    </author>
    <author>
      <name>Charless C. Fowlkes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the extended version of a paper to appear at the 12th
  European Conference on Computer Vision (ECCV 2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.0378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.0378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.3687v1</id>
    <updated>2012-08-17T20:38:56Z</updated>
    <published>2012-08-17T20:38:56Z</published>
    <title>Information-theoretic Dictionary Learning for Image Classification</title>
    <summary>  We present a two-stage approach for learning dictionaries for object
classification tasks based on the principle of information maximization. The
proposed method seeks a dictionary that is compact, discriminative, and
generative. In the first stage, dictionary atoms are selected from an initial
dictionary by maximizing the mutual information measure on dictionary
compactness, discrimination and reconstruction. In the second stage, the
selected dictionary atoms are updated for improved reconstructive and
discriminative power using a simple gradient ascent algorithm on mutual
information. Experiments using real datasets demonstrate the effectiveness of
our approach for image classification tasks.
</summary>
    <author>
      <name>Qiang Qiu</name>
    </author>
    <author>
      <name>Vishal M. Patel</name>
    </author>
    <author>
      <name>Rama Chellappa</name>
    </author>
    <link href="http://arxiv.org/abs/1208.3687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.3687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.5365v1</id>
    <updated>2012-08-27T11:20:49Z</updated>
    <published>2012-08-27T11:20:49Z</published>
    <title>A Missing and Found Recognition System for Hajj and Umrah</title>
    <summary>  This note describes an integrated recognition system for identifying missing
and found objects as well as missing, dead, and found people during Hajj and
Umrah seasons in the two Holy cities of Makkah and Madina in the Kingdom of
Saudi Arabia. It is assumed that the total estimated number of pilgrims will
reach 20 millions during the next decade. The ultimate goal of this system is
to integrate facial recognition and object identification solutions into the
Hajj and Umrah rituals. The missing and found computerized system is part of
the CrowdSensing system for Hajj and Umrah crowd estimation, management and
safety.
</summary>
    <author>
      <name>Salah A. Aly</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">website available via http://www.mfhajj.com</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.5365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.5365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.1558v1</id>
    <updated>2012-09-07T14:52:02Z</updated>
    <published>2012-09-07T14:52:02Z</published>
    <title>A Comparative Study between Moravec and Harris Corner Detection of Noisy
  Images Using Adaptive Wavelet Thresholding Technique</title>
    <summary>  In this paper a comparative study between Moravec and Harris Corner Detection
has been done for obtaining features required to track and recognize objects
within a noisy image. Corner detection of noisy images is a challenging task in
image processing. Natural images often get corrupted by noise during
acquisition and transmission. As Corner detection of these noisy images does
not provide desired results, hence de-noising is required. Adaptive wavelet
thresholding approach is applied for the same.
</summary>
    <author>
      <name>Nilanjan Dey</name>
    </author>
    <author>
      <name>Pradipti Nandi</name>
    </author>
    <author>
      <name>Nilanjana Barman</name>
    </author>
    <author>
      <name>Debolina Das</name>
    </author>
    <author>
      <name>Subhabrata Chakraborty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 13 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Engineering Research and Applications
  (IJERA) Vol. 2, Issue 1, Jan-Feb 2012, pp.599-606</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.1558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.1558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.2295v2</id>
    <updated>2012-09-12T07:31:14Z</updated>
    <published>2012-09-11T12:01:08Z</published>
    <title>Multimodal diffusion geometry by joint diagonalization of Laplacians</title>
    <summary>  We construct an extension of diffusion geometry to multiple modalities
through joint approximate diagonalization of Laplacian matrices. This naturally
extends classical data analysis tools based on spectral geometry, such as
diffusion maps and spectral clustering. We provide several synthetic and real
examples of manifold learning, retrieval, and clustering demonstrating that the
joint diffusion geometry frequently better captures the inherent structure of
multi-modal data. We also show that many previous attempts to construct
multimodal spectral clustering can be seen as particular cases of joint
approximate diagonalization of the Laplacians.
</summary>
    <author>
      <name>Davide Eynard</name>
    </author>
    <author>
      <name>Klaus Glashoff</name>
    </author>
    <author>
      <name>Michael M. Bronstein</name>
    </author>
    <author>
      <name>Alexander M. Bronstein</name>
    </author>
    <link href="http://arxiv.org/abs/1209.2295v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.2295v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.2903v1</id>
    <updated>2012-09-13T14:15:16Z</updated>
    <published>2012-09-13T14:15:16Z</published>
    <title>A Novel Approach of Harris Corner Detection of Noisy Images using
  Adaptive Wavelet Thresholding Technique</title>
    <summary>  In this paper we propose a method of corner detection for obtaining features
which is required to track and recognize objects within a noisy image. Corner
detection of noisy images is a challenging task in image processing. Natural
images often get corrupted by noise during acquisition and transmission. Though
Corner detection of these noisy images does not provide desired results, hence
de-noising is required. Adaptive wavelet thresholding approach is applied for
the same.
</summary>
    <author>
      <name>Nilanjan Dey</name>
    </author>
    <author>
      <name>Pradipti Nandi</name>
    </author>
    <author>
      <name>Nilanjana Barman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 10 figures. arXiv admin note: substantial text overlap with
  arXiv:1209.1558</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science &amp; Technology(IJCST) Vol.
  2, ISSUE 4, OCT. - DEC. 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.2903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.2903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4420v1</id>
    <updated>2012-09-20T04:20:40Z</updated>
    <published>2012-09-20T04:20:40Z</published>
    <title>An Efficient Color Face Verification Based on 2-Directional
  2-Dimensional Feature Extraction</title>
    <summary>  A novel and uniform framework for face verification is presented in this
paper. First of all, a 2-directional 2-dimensional feature extraction method is
adopted to extract client-specific template - 2D discrimant projection matrix.
Then the face skin color information is utilized as an additive feature to
enhance decision making strategy that makes use of not only 2D grey feature but
also 2D skin color feature. A fusion decision of both is applied to experiment
the performance on the XM2VTS database according to Lausanne protocol.
Experimental results show that the framework achieves high verification
accuracy and verification speed.
</summary>
    <author>
      <name>Lan-Ting LI</name>
    </author>
    <link href="http://arxiv.org/abs/1209.4420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.5039v1</id>
    <updated>2012-09-23T07:52:01Z</updated>
    <published>2012-09-23T07:52:01Z</published>
    <title>Creation of Digital Test Form for Prepress Department</title>
    <summary>  The main problem in colour management in prepress department is lack of
availability of literature on colour management and knowledge gap between
prepress department and press department. So a digital test from has been
created by Adobe Photoshop to analyse the ICC profile and to create a new
profile and this analysed data is used to study about various grey scale of RGB
and CMYK images. That helps in conversion of image from RGB to CMYK in prepress
department.
</summary>
    <author>
      <name>Jaswinder Singh Dilawari</name>
    </author>
    <author>
      <name>Ravinder Khanna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages,4 Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">(IJCSIS) International Journal of Computer Science and Information
  Security, Vol. 10, No. 9, September 2012 (IJCSIS) International Journal of
  Computer Science and Information Security, Vol. 10, No. 9, September 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.5039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.5039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.5041v1</id>
    <updated>2012-09-23T09:15:20Z</updated>
    <published>2012-09-23T09:15:20Z</published>
    <title>An Implementation of Computer Graphics as Prepress Image Enhancement
  Process</title>
    <summary>  The production of a printed product involves three stages: prepress, the
printing process (press) itself, and finishing (post press). There are various
types of equipments (printers, scanners) and various qualities image are
present in the market. These give different color rendering each time during
reproduction. So, a color key tool has been developed keeping Color Management
Scheme (CMS) in mind so that during reproduction no color rendering takes place
irrespective of use of any device and resolution level has also been improved.
</summary>
    <author>
      <name>Jaswinder Singh Dilawari</name>
    </author>
    <author>
      <name>Ravinder Khanna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 Pages,8 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.5041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.5041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.5417v1</id>
    <updated>2012-09-24T20:47:27Z</updated>
    <published>2012-09-24T20:47:27Z</published>
    <title>Model based neuro-fuzzy ASR on Texas processor</title>
    <summary>  In this paper an algorithm for recognizing speech has been proposed. The
recognized speech is used to execute related commands which use the MFCC and
two kind of classifiers, first one uses MLP and second one uses fuzzy inference
system as a classifier. The experimental results demonstrate the high gain and
efficiency of the proposed algorithm. We have implemented this system based on
graphical design and tested on a fix point digital signal processor (DSP) of
600 MHz, with reference DM6437-EVM of Texas instrument.
</summary>
    <author>
      <name>Hesam Ekhtiyar</name>
    </author>
    <author>
      <name>Mehdi Sheida</name>
    </author>
    <author>
      <name>Somaye Sobati Moghadam</name>
    </author>
    <link href="http://arxiv.org/abs/1209.5417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.5417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.5756v1</id>
    <updated>2012-09-25T20:11:23Z</updated>
    <published>2012-09-25T20:11:23Z</published>
    <title>Environmental Sounds Spectrogram Classification using Log-Gabor Filters
  and Multiclass Support Vector Machines</title>
    <summary>  This paper presents novel approaches for efficient feature extraction using
environmental sound magnitude spectrogram. We propose approach based on the
visual domain. This approach included three methods. The first method is based
on extraction for each spectrogram a single log-Gabor filter followed by mutual
information procedure. In the second method, the spectrogram is passed by the
same steps of the first method but with an averaged bank of 12 log-Gabor
filter. The third method consists of spectrogram segmentation into three
patches, and after that for each spectrogram patch we applied the second
method. The classification results prove that the second method is the most
efficient in our environmental sound classification system.
</summary>
    <author>
      <name>Sameh Souli</name>
    </author>
    <author>
      <name>Zied Lachiri</name>
    </author>
    <link href="http://arxiv.org/abs/1209.5756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.5756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.6037v1</id>
    <updated>2012-09-26T19:25:56Z</updated>
    <published>2012-09-26T19:25:56Z</published>
    <title>Reproduction of Images by Gamut Mapping and Creation of New Test Charts
  in Prepress Process</title>
    <summary>  With the advent of digital images the problem of keeping picture
visualization uniformity arises because each printing or scanning device has
its own color chart. So, universal color profiles are made by ICC to bring
uniformity in various types of devices. Keeping that color profile in mind
various new color charts are created and calibrated with the help of standard
IT8 test charts available in the market. The main objective to color
reproduction is to produce the identical picture at device output. For that
principles for gamut mapping has been designed
</summary>
    <author>
      <name>Jaswinder Singh Dilawari</name>
    </author>
    <author>
      <name>Ravinder Khanna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages,10 Figures; International Journal of Scientific and
  Engineering Research,Volume 3, Issue 10, October 2012 Edition</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.6037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.6037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.0191v1</id>
    <updated>2012-10-25T23:21:46Z</updated>
    <published>2012-10-25T23:21:46Z</published>
    <title>Performance Evaluation of Random Set Based Pedestrian Tracking
  Algorithms</title>
    <summary>  The paper evaluates the error performance of three random finite set based
multi-object trackers in the context of pedestrian video tracking. The
evaluation is carried out using a publicly available video dataset of 4500
frames (town centre street) for which the ground truth is available. The input
to all pedestrian tracking algorithms is an identical set of head and body
detections, obtained using the Histogram of Oriented Gradients (HOG) detector.
The tracking error is measured using the recently proposed OSPA metric for
tracks, adopted as the only known mathematically rigorous metric for measuring
the distance between two sets of tracks. A comparative analysis is presented
under various conditions.
</summary>
    <author>
      <name>Branko Ristic</name>
    </author>
    <author>
      <name>Jamie Sherrah</name>
    </author>
    <author>
      <name>Ángel F. García-Fernández</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.0191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.0191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.1482v4</id>
    <updated>2013-01-16T03:05:42Z</updated>
    <published>2012-11-07T08:19:04Z</published>
    <title>Gender Recognition in Walk Gait through 3D Motion by Quadratic Bezier
  Curve and Statistical Techniques</title>
    <summary>  Motion capture is the process of recording the movement of objects or people.
It is used in military, entertainment, sports, and medical applications, and
for validation of computer vision[2] and robotics. In filmmaking and video game
development, it refers to recording actions of human actors, and using that
information to animate digital character models in 2D or 3D computer animation.
When it includes face and fingers or captures subtle
</summary>
    <author>
      <name>Sajid Ali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">wrongly uploaded</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.1482v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.1482v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.1800v1</id>
    <updated>2012-11-08T09:24:21Z</updated>
    <published>2012-11-08T09:24:21Z</published>
    <title>A Comparative study of Arabic handwritten characters invariant feature</title>
    <summary>  This paper is practically interested in the unchangeable feature of Arabic
handwritten character. It presents results of comparative study achieved on
certain features extraction techniques of handwritten character, based on Hough
transform, Fourier transform, Wavelet transform and Gabor Filter. Obtained
results show that Hough Transform and Gabor filter are insensible to the
rotation and translation, Fourier Transform is sensible to the rotation but
insensible to the translation, in contrast to Hough Transform and Gabor filter,
Wavelets Transform is sensitive to the rotation as well as to the translation.
</summary>
    <author>
      <name>Hamdi Hassen</name>
    </author>
    <author>
      <name>Maher khemakhem</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 2, No. 12, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.1800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.1800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.2116v1</id>
    <updated>2012-11-09T12:59:11Z</updated>
    <published>2012-11-09T12:59:11Z</published>
    <title>Localisation of Numerical Date Field in an Indian Handwritten Document</title>
    <summary>  This paper describes a method to localise all those areas which may
constitute the date field in an Indian handwritten document. Spatial patterns
of the date field are studied from various handwritten documents and an
algorithm is developed through statistical analysis to identify those sets of
connected components which may constitute the date. Common date patterns
followed in India are considered to classify the date formats in different
classes. Reported results demonstrate promising performance of the proposed
approach
</summary>
    <author>
      <name>S Arunkumar</name>
    </author>
    <author>
      <name>Pallab Kumar Sahu</name>
    </author>
    <author>
      <name>Sudeep Gorai</name>
    </author>
    <author>
      <name>Kalyan Ghosh</name>
    </author>
    <link href="http://arxiv.org/abs/1211.2116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.2116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.4307v1</id>
    <updated>2012-11-19T05:44:24Z</updated>
    <published>2012-11-19T05:44:24Z</published>
    <title>Efficient Superimposition Recovering Algorithm</title>
    <summary>  In this article, we address the issue of recovering latent transparent layers
from superimposition images. Here, we assume we have the estimated
transformations and extracted gradients of latent layers. To rapidly recover
high-quality image layers, we propose an Efficient Superimposition Recovering
Algorithm (ESRA) by extending the framework of accelerated gradient method. In
addition, a key building block (in each iteration) in our proposed method is
the proximal operator calculating. Here we propose to employ a dual approach
and present our Parallel Algorithm with Constrained Total Variation (PACTV)
method. Our recovering method not only reconstructs high-quality layers without
color-bias problem, but also theoretically guarantees good convergence
performance.
</summary>
    <author>
      <name>Han Li</name>
    </author>
    <author>
      <name>Kun Gai</name>
    </author>
    <author>
      <name>Pinghua Gong</name>
    </author>
    <author>
      <name>Changshui Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1211.4307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.4307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.4385v1</id>
    <updated>2012-11-19T12:21:49Z</updated>
    <published>2012-11-19T12:21:49Z</published>
    <title>Artificial Neural Network Based Optical Character Recognition</title>
    <summary>  Optical Character Recognition deals in recognition and classification of
characters from an image. For the recognition to be accurate, certain
topological and geometrical properties are calculated, based on which a
character is classified and recognized. Also, the Human psychology perceives
characters by its overall shape and features such as strokes, curves,
protrusions, enclosures etc. These properties, also called Features are
extracted from the image by means of spatial pixel-based calculation. A
collection of such features, called Vectors, help in defining a character
uniquely, by means of an Artificial Neural Network that uses these Feature
Vectors.
</summary>
    <author>
      <name>Vivek Shrivastava</name>
    </author>
    <author>
      <name>Navdeep Sharma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/sipij.2012.3506</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/sipij.2012.3506" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Signal &amp; Image Processing : An International Journal (SIPIJ) Vol.3,
  No.5, October 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.4385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.4385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.4503v1</id>
    <updated>2012-11-19T17:13:26Z</updated>
    <published>2012-11-19T17:13:26Z</published>
    <title>An Effective Fingerprint Classification and Search Method</title>
    <summary>  This paper presents an effective fingerprint classification method designed
based on a hierarchical agglomerative clustering technique. The performance of
the technique was evaluated in terms of several real-life datasets and a
significant improvement in reducing the misclassification error has been
noticed. This paper also presents a query based faster fingerprint search
method over the clustered fingerprint databases. The retrieval accuracy of the
search method has been found effective in light of several real-life databases.
</summary>
    <author>
      <name>Monowar H. Bhuyan</name>
    </author>
    <author>
      <name>D. K. Bhattacharyya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures, 6 tables, referred journal publication</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Network Security,
  Vol. 9, No.11, pp. 39-48, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.4503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.4503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U35" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.4860v1</id>
    <updated>2012-11-20T20:54:30Z</updated>
    <published>2012-11-20T20:54:30Z</published>
    <title>Domain Adaptations for Computer Vision Applications</title>
    <summary>  A basic assumption of statistical learning theory is that train and test data
are drawn from the same underlying distribution. Unfortunately, this assumption
doesn't hold in many applications. Instead, ample labeled data might exist in a
particular `source' domain while inference is needed in another, `target'
domain. Domain adaptation methods leverage labeled data from both domains to
improve classification on unseen data in the target domain. In this work we
survey domain transfer learning methods for various application domains with
focus on recent work in Computer Vision.
</summary>
    <author>
      <name>Oscar Beijbom</name>
    </author>
    <link href="http://arxiv.org/abs/1211.4860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.4860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.5556v1</id>
    <updated>2012-11-23T17:13:07Z</updated>
    <published>2012-11-23T17:13:07Z</published>
    <title>Improving Perceptual Color Difference using Basic Color Terms</title>
    <summary>  We suggest a new color distance based on two observations. First, perceptual
color differences were designed to be used to compare very similar colors. They
do not capture human perception for medium and large color differences well.
Thresholding was proposed to solve the problem for large color differences,
i.e. two totally different colors are always the same distance apart. We show
that thresholding alone cannot improve medium color differences. We suggest to
alleviate this problem using basic color terms. Second, when a color distance
is used for edge detection, many small distances around the just noticeable
difference may account for false edges. We suggest to reduce the effect of
small distances.
</summary>
    <author>
      <name>Ofir Pele</name>
    </author>
    <author>
      <name>Michael Werman</name>
    </author>
    <link href="http://arxiv.org/abs/1211.5556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0030v1</id>
    <updated>2012-11-30T22:35:19Z</updated>
    <published>2012-11-30T22:35:19Z</published>
    <title>Viewpoint Invariant Object Detector</title>
    <summary>  Object Detection is the task of identifying the existence of an object class
instance and locating it within an image. Difficulties in handling high
intra-class variations constitute major obstacles to achieving high performance
on standard benchmark datasets (scale, viewpoint, lighting conditions and
orientation variations provide good examples). Suggested model aims at
providing more robustness to detecting objects suffering severe distortion due
to &lt; 60{\deg} viewpoint changes. In addition, several model computational
bottlenecks have been resolved leading to a significant increase in the model
performance (speed and space) without compromising the resulting accuracy.
Finally, we produced two illustrative applications showing the potential of the
object detection technology being deployed in real life applications; namely
content-based image search and content-based video search.
</summary>
    <author>
      <name>Osama Khalil</name>
    </author>
    <author>
      <name>Andrew Habib</name>
    </author>
    <link href="http://arxiv.org/abs/1212.0030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0134v1</id>
    <updated>2012-12-01T16:59:07Z</updated>
    <published>2012-12-01T16:59:07Z</published>
    <title>Fingertip Detection: A Fast Method with Natural Hand</title>
    <summary>  Many vision based applications have used fingertips to track or manipulate
gestures in their applications. Gesture identification is a natural way to pass
the signals to the machine, as the human express its feelings most of the time
with hand expressions. Here a novel time efficient algorithm has been described
for fingertip detection. This method is invariant to hand direction and in
preprocessing it cuts only hand part from the full image, hence further
computation would be much faster than processing full image. Binary silhouette
of the input image is generated using HSV color space based skin filter and
hand cropping done based on intensity histogram of the hand image
</summary>
    <author>
      <name>J. L. Raheja</name>
    </author>
    <author>
      <name>Karen Das</name>
    </author>
    <author>
      <name>Ankit Chaudhary</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Embedded Systems and Computer
  Engineering, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1212.0134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0142v2</id>
    <updated>2013-04-02T18:05:46Z</updated>
    <published>2012-12-01T18:13:03Z</published>
    <title>Pedestrian Detection with Unsupervised Multi-Stage Feature Learning</title>
    <summary>  Pedestrian detection is a problem of considerable practical interest. Adding
to the list of successful applications of deep learning methods to vision, we
report state-of-the-art and competitive results on all major pedestrian
datasets with a convolutional network model. The model uses a few new twists,
such as multi-stage features, connections that skip layers to integrate global
shape information with local distinctive motif information, and an unsupervised
method based on convolutional sparse coding to pre-train the filters at each
stage.
</summary>
    <author>
      <name>Pierre Sermanet</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <author>
      <name>Soumith Chintala</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.0142v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0142v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0402v1</id>
    <updated>2012-12-03T14:45:31Z</updated>
    <published>2012-12-03T14:45:31Z</published>
    <title>UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild</title>
    <summary>  We introduce UCF101 which is currently the largest dataset of human actions.
It consists of 101 action classes, over 13k clips and 27 hours of video data.
The database consists of realistic user uploaded videos containing camera
motion and cluttered background. Additionally, we provide baseline action
recognition results on this new dataset using standard bag of words approach
with overall performance of 44.5%. To the best of our knowledge, UCF101 is
currently the most challenging dataset of actions due to its large number of
classes, large number of clips and also unconstrained nature of such clips.
</summary>
    <author>
      <name>Khurram Soomro</name>
    </author>
    <author>
      <name>Amir Roshan Zamir</name>
    </author>
    <author>
      <name>Mubarak Shah</name>
    </author>
    <link href="http://arxiv.org/abs/1212.0402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0888v1</id>
    <updated>2012-12-04T21:59:35Z</updated>
    <published>2012-12-04T21:59:35Z</published>
    <title>Unmixing of Hyperspectral Data Using Robust Statistics-based NMF</title>
    <summary>  Mixed pixels are presented in hyperspectral images due to low spatial
resolution of hyperspectral sensors. Spectral unmixing decomposes mixed pixels
spectra into endmembers spectra and abundance fractions. In this paper using of
robust statistics-based nonnegative matrix factorization (RNMF) for spectral
unmixing of hyperspectral data is investigated. RNMF uses a robust cost
function and iterative updating procedure, so is not sensitive to outliers.
This method has been applied to simulated data using USGS spectral library,
AVIRIS and ROSIS datasets. Unmixing results are compared to traditional NMF
method based on SAD and AAD measures. Results demonstrate that this method can
be used efficiently for hyperspectral unmixing purposes.
</summary>
    <author>
      <name>Roozbeh Rajabi</name>
    </author>
    <author>
      <name>Hassan Ghassemian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.0888v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0888v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.2546v1</id>
    <updated>2012-12-11T17:29:04Z</updated>
    <published>2012-12-11T17:29:04Z</published>
    <title>A Learning Framework for Morphological Operators using Counter-Harmonic
  Mean</title>
    <summary>  We present a novel framework for learning morphological operators using
counter-harmonic mean. It combines concepts from morphology and convolutional
neural networks. A thorough experimental validation analyzes basic
morphological operators dilation and erosion, opening and closing, as well as
the much more complex top-hat transform, for which we report a real-world
application from the steel industry. Using online learning and stochastic
gradient descent, our system learns both the structuring element and the
composition of operators. It scales well to large datasets and online settings.
</summary>
    <author>
      <name>Jonathan Masci</name>
    </author>
    <author>
      <name>Jesús Angulo</name>
    </author>
    <author>
      <name>Jürgen Schmidhuber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ISMM'13</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.2546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.2546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.3034v1</id>
    <updated>2012-12-13T01:55:15Z</updated>
    <published>2012-12-13T01:55:15Z</published>
    <title>Multi-target tracking algorithms in 3D</title>
    <summary>  Ladars provide a unique capability for identification of objects and motions
in scenes with fixed 3D field of view (FOV). This paper describes algorithms
for multi-target tracking in 3D scenes including the preprocessing
(mathematical morphology and Parzen windows), labeling of connected components,
sorting of targets by selectable attributes (size, length of track, velocity),
and handling of target states (acquired, coasting, re-acquired and tracked) in
order to assemble the target trajectories. This paper is derived from working
algorithms coded in Matlab, which were tested and reviewed by others, and does
not speculate about usage of general formulas or frameworks.
</summary>
    <author>
      <name>Rastislav Telgarsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, conference proceedings</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scientific Issues, MATHEMATICA IV, Ruzomberok 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1212.3034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.3034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D18, 68W05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.4; I.2.10; I.4.7; I.4.8; I.4.9; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.4608v1</id>
    <updated>2012-12-19T09:40:09Z</updated>
    <published>2012-12-19T09:40:09Z</published>
    <title>Perceptually Motivated Shape Context Which Uses Shape Interiors</title>
    <summary>  In this paper, we identify some of the limitations of current-day shape
matching techniques. We provide examples of how contour-based shape matching
techniques cannot provide a good match for certain visually similar shapes. To
overcome this limitation, we propose a perceptually motivated variant of the
well-known shape context descriptor. We identify that the interior properties
of the shape play an important role in object recognition and develop a
descriptor that captures these interior properties. We show that our method can
easily be augmented with any other shape matching algorithm. We also show from
our experiments that the use of our descriptor can significantly improve the
retrieval rates.
</summary>
    <author>
      <name>Vittal Premachandran</name>
    </author>
    <author>
      <name>Ramakrishna Kakarala</name>
    </author>
    <link href="http://arxiv.org/abs/1212.4608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.4608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.5877v2</id>
    <updated>2013-03-29T02:49:46Z</updated>
    <published>2012-12-24T08:58:02Z</published>
    <title>Blinking Molecule Tracking</title>
    <summary>  We discuss a method for tracking individual molecules which globally
optimizes the likelihood of the connections between molecule positions fast and
with high reliability even for high spot densities and blinking molecules. Our
method works with cost functions which can be freely chosen to combine costs
for distances between spots in space and time and which can account for the
reliability of positioning a molecule. To this end, we describe a top-down
polyhedral approach to the problem of tracking many individual molecules. This
immediately yields an effective implementation using standard linear
programming solvers. Our method can be applied to 2D and 3D tracking.
</summary>
    <author>
      <name>Andreas Karrenbauer</name>
    </author>
    <author>
      <name>Dominik Wöll</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12th International Symposium on Experimental Algorithms 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.5877v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.5877v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0435v1</id>
    <updated>2013-01-03T12:16:25Z</updated>
    <published>2013-01-03T12:16:25Z</published>
    <title>Investigating the performance of Correspondence Algorithms in Vision
  based Driver-assistance in Indoor Environment</title>
    <summary>  This paper presents the experimental comparison of fourteen stereo matching
algorithms in variant illumination conditions. Different adaptations of global
and local stereo matching techniques are chosen for evaluation The variant
strength and weakness of the chosen correspondence algorithms are explored by
employing the methodology of the prediction error strategy. The algorithms are
gauged on the basis of their performance on real world data set taken in
various indoor lighting conditions and at different times of the day
</summary>
    <author>
      <name>F. Mahmood</name>
    </author>
    <author>
      <name>Syed. M. B. Haider</name>
    </author>
    <author>
      <name>F. Kunwar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/9718-3663</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/9718-3663" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 9 figures,Published with International Journal of Computer
  Applications (IJCA)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCA 60(9):6-12, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.0435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0612v1</id>
    <updated>2012-12-12T15:59:10Z</updated>
    <published>2012-12-12T15:59:10Z</published>
    <title>Adaptive Foreground and Shadow Detection inImage Sequences</title>
    <summary>  This paper presents a novel method of foreground segmentation that
distinguishes moving objects from their moving cast shadows in monocular image
sequences. The models of background, edge information, and shadow are set up
and adaptively updated. A Bayesian belief network is proposed to describe the
relationships among the segmentation label, background, intensity, and edge
information. The notion of Markov random field is used to encourage the spatial
connectivity of the segmented regions. The solution is obtained by maximizing
the posterior possibility density of the segmentation field.
</summary>
    <author>
      <name>Yang Wang</name>
    </author>
    <author>
      <name>Tele Tan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.0612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.1671v1</id>
    <updated>2013-01-08T20:56:17Z</updated>
    <published>2013-01-08T20:56:17Z</published>
    <title>Causal graph-based video segmentation</title>
    <summary>  Numerous approaches in image processing and computer vision are making use of
super-pixels as a pre-processing step. Among the different methods producing
such over-segmentation of an image, the graph-based approach of Felzenszwalb
and Huttenlocher is broadly employed. One of its interesting properties is that
the regions are computed in a greedy manner in quasi-linear time. The algorithm
may be trivially extended to video segmentation by considering a video as a 3D
volume, however, this can not be the case for causal segmentation, when
subsequent frames are unknown. We propose an efficient video segmentation
approach that computes temporally consistent pixels in a causal manner, filling
the need for causal and real time applications.
</summary>
    <author>
      <name>Camille Couprie</name>
    </author>
    <author>
      <name>Clément Farabet</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.1671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.1671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.2351v1</id>
    <updated>2013-01-10T22:57:01Z</updated>
    <published>2013-01-10T22:57:01Z</published>
    <title>Application of Hopfield Network to Saccades</title>
    <summary>  Human eye movement mechanisms (saccades) are very useful for scene analysis,
including object representation and pattern recognition. In this letter, a
Hopfield neural network to emulate saccades is proposed. The network uses an
energy function that includes location and identification tasks. Computer
simulation shows that the network performs those tasks cooperatively. The
result suggests that the network is applicable to shift-invariant pattern
recognition.
</summary>
    <author>
      <name>Teruyoshi Washizawa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/72.286896</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/72.286896" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on NEURAL NETWORKS, vol.4, no.6, pp-995-997,
  NOVEMBER 1993</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.2351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.2351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.2820v3</id>
    <updated>2013-03-13T22:48:38Z</updated>
    <published>2013-01-13T20:49:30Z</published>
    <title>Clustering Learning for Robotic Vision</title>
    <summary>  We present the clustering learning technique applied to multi-layer
feedforward deep neural networks. We show that this unsupervised learning
technique can compute network filters with only a few minutes and a much
reduced set of parameters. The goal of this paper is to promote the technique
for general-purpose robotic vision systems. We report its use in static image
datasets and object tracking datasets. We show that networks trained with
clustering learning can outperform large networks trained for many hours on
complex datasets.
</summary>
    <author>
      <name>Eugenio Culurciello</name>
    </author>
    <author>
      <name>Jordan Bates</name>
    </author>
    <author>
      <name>Aysegul Dundar</name>
    </author>
    <author>
      <name>Jose Carrasco</name>
    </author>
    <author>
      <name>Clement Farabet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code for this paper is available here:
  https://github.com/culurciello/CL_paper1_code</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.2820v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.2820v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3385v2</id>
    <updated>2013-01-16T14:56:44Z</updated>
    <published>2013-01-15T15:34:07Z</published>
    <title>Recurrent Online Clustering as a Spatio-Temporal Feature Extractor in
  DeSTIN</title>
    <summary>  This paper presents a basic enhancement to the DeSTIN deep learning
architecture by replacing the explicitly calculated transition tables that are
used to capture temporal features with a simpler, more scalable mechanism. This
mechanism uses feedback of state information to cluster over a space comprised
of both the spatial input and the current state. The resulting architecture
achieves state-of-the-art results on the MNIST classification benchmark.
</summary>
    <author>
      <name>Steven R. Young</name>
    </author>
    <author>
      <name>Itamar Arel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, Submitted to ICLR 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3385v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3385v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3457v2</id>
    <updated>2013-04-10T18:32:07Z</updated>
    <published>2013-01-15T19:18:52Z</published>
    <title>A Geometric Descriptor for Cell-Division Detection</title>
    <summary>  We describe a method for cell-division detection based on a geometric-driven
descriptor that can be represented as a 5-layers processing network, based
mainly on wavelet filtering and a test for mirror symmetry between pairs of
pixels. After the centroids of the descriptors are computed for a sequence of
frames, the two-steps piecewise constant function that best fits the sequence
of centroids determines the frame where the division occurs.
</summary>
    <author>
      <name>Marcelo Cicconet</name>
    </author>
    <author>
      <name>Italo Lima</name>
    </author>
    <author>
      <name>Davi Geiger</name>
    </author>
    <author>
      <name>Kris Gunsalus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author since the review process
  for the conference to which it was applied ended</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3457v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3457v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3572v2</id>
    <updated>2013-03-14T18:18:17Z</updated>
    <published>2013-01-16T03:31:30Z</published>
    <title>Indoor Semantic Segmentation using depth information</title>
    <summary>  This work addresses multi-class segmentation of indoor scenes with RGB-D
inputs. While this area of research has gained much attention recently, most
works still rely on hand-crafted features. In contrast, we apply a multiscale
convolutional network to learn features directly from the images and the depth
information. We obtain state-of-the-art on the NYU-v2 depth dataset with an
accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos
sequences that could be processed in real-time using appropriate hardware such
as an FPGA.
</summary>
    <author>
      <name>Camille Couprie</name>
    </author>
    <author>
      <name>Clément Farabet</name>
    </author>
    <author>
      <name>Laurent Najman</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3572v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3572v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.3755v1</id>
    <updated>2013-01-16T17:05:57Z</updated>
    <published>2013-01-16T17:05:57Z</published>
    <title>Gradient Driven Learning for Pooling in Visual Pipeline Feature
  Extraction Models</title>
    <summary>  Hyper-parameter selection remains a daunting task when building a pattern
recognition architecture which performs well, particularly in recently
constructed visual pipeline models for feature extraction. We re-formulate
pooling in an existing pipeline as a function of adjustable pooling map weight
parameters and propose the use of supervised error signals from gradient
descent to tune the established maps within the model. This technique allows us
to learn what would otherwise be a design choice within the model and
specialize the maps to aggregate areas of invariance for the task presented.
Preliminary results show moderate potential gains in classification accuracy
and highlight areas of importance within the intermediate feature
representation space.
</summary>
    <author>
      <name>Derek Rose</name>
    </author>
    <author>
      <name>Itamar Arel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, submitted to ICLR2013 workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.3755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.3755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.0077v1</id>
    <updated>2013-02-01T05:08:58Z</updated>
    <published>2013-02-01T05:08:58Z</published>
    <title>Sparse MRI for motion correction</title>
    <summary>  MR image sparsity/compressibility has been widely exploited for imaging
acceleration with the development of compressed sensing. A sparsity-based
approach to rigid-body motion correction is presented for the first time in
this paper. A motion is sought after such that the compensated MR image is
maximally sparse/compressible among the infinite candidates. Iterative
algorithms are proposed that jointly estimate the motion and the image content.
The proposed method has a lot of merits, such as no need of additional data and
loose requirement for the sampling sequence. Promising results are presented to
demonstrate its performance.
</summary>
    <author>
      <name>Zai Yang</name>
    </author>
    <author>
      <name>Cishen Zhang</name>
    </author>
    <author>
      <name>Lihua Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings of ISBI 2013. 4 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.0077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.0077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.1690v1</id>
    <updated>2013-02-07T10:17:07Z</updated>
    <published>2013-02-07T10:17:07Z</published>
    <title>A Fast Learning Algorithm for Image Segmentation with Max-Pooling
  Convolutional Networks</title>
    <summary>  We present a fast algorithm for training MaxPooling Convolutional Networks to
segment images. This type of network yields record-breaking performance in a
variety of tasks, but is normally trained on a computationally expensive
patch-by-patch basis. Our new method processes each training image in a single
pass, which is vastly more efficient.
  We validate the approach in different scenarios and report a 1500-fold
speed-up. In an application to automated steel defect detection and
segmentation, we obtain excellent performance with short training times.
</summary>
    <author>
      <name>Jonathan Masci</name>
    </author>
    <author>
      <name>Alessandro Giusti</name>
    </author>
    <author>
      <name>Dan Cireşan</name>
    </author>
    <author>
      <name>Gabriel Fricout</name>
    </author>
    <author>
      <name>Jürgen Schmidhuber</name>
    </author>
    <link href="http://arxiv.org/abs/1302.1690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.1690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.1700v1</id>
    <updated>2013-02-07T10:33:47Z</updated>
    <published>2013-02-07T10:33:47Z</published>
    <title>Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks</title>
    <summary>  Deep Neural Networks now excel at image classification, detection and
segmentation. When used to scan images by means of a sliding window, however,
their high computational complexity can bring even the most powerful hardware
to its knees. We show how dynamic programming can speedup the process by orders
of magnitude, even when max-pooling layers are present.
</summary>
    <author>
      <name>Alessandro Giusti</name>
    </author>
    <author>
      <name>Dan C. Cireşan</name>
    </author>
    <author>
      <name>Jonathan Masci</name>
    </author>
    <author>
      <name>Luca M. Gambardella</name>
    </author>
    <author>
      <name>Jürgen Schmidhuber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures, 3 tables, 21 references, submitted to ICIP 2013</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Image Processing (ICIP) 2013,
  Melbourne</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.1700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.1700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5056v1</id>
    <updated>2013-01-15T18:47:11Z</updated>
    <published>2013-01-15T18:47:11Z</published>
    <title>Pooling-Invariant Image Feature Learning</title>
    <summary>  Unsupervised dictionary learning has been a key component in state-of-the-art
computer vision recognition architectures. While highly effective methods exist
for patch-based dictionary learning, these methods may learn redundant features
after the pooling stage in a given early vision architecture. In this paper, we
offer a novel dictionary learning scheme to efficiently take into account the
invariance of learned features after the spatial pooling stage. The algorithm
is built on simple clustering, and thus enjoys efficiency and scalability. We
discuss the underlying mechanism that justifies the use of clustering
algorithms, and empirically show that the algorithm finds better dictionaries
than patch-based methods with the same dictionary size.
</summary>
    <author>
      <name>Yangqing Jia</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <link href="http://arxiv.org/abs/1302.5056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5894v1</id>
    <updated>2013-02-24T10:49:39Z</updated>
    <published>2013-02-24T10:49:39Z</published>
    <title>Four Side Distance: A New Fourier Shape Signature</title>
    <summary>  Shape is one of the main features in content based image retrieval (CBIR).
This paper proposes a new shape signature. In this technique, features of each
shape are extracted based on four sides of the rectangle that covers the shape.
The proposed technique is Fourier based and it is invariant to translation,
scaling and rotation. The retrieval performance between some commonly used
Fourier based signatures and the proposed four sides distance (FSD) signature
has been tested using MPEG-7 database. Experimental results are shown that the
FSD signature has better performance compared with those signatures.
</summary>
    <author>
      <name>Sonya Eini</name>
    </author>
    <author>
      <name>Abdolah Chalechale</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 7 figures, International Journal of Advanced Studies in
  Computers, Science and Engineering</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.5894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5957v1</id>
    <updated>2013-02-24T21:38:20Z</updated>
    <published>2013-02-24T21:38:20Z</published>
    <title>Shape Characterization via Boundary Distortion</title>
    <summary>  In this paper, we derive new shape descriptors based on a directional
characterization. The main idea is to study the behavior of the shape
neighborhood under family of transformations. We obtain a description invariant
with respect to rotation, reflection, translation and scaling. A well-defined
metric is then proposed on the associated feature space. We show the continuity
of this metric. Some results on shape retrieval are provided on two databases
to show the accuracy of the proposed shape metric.
</summary>
    <author>
      <name>Xavier Descombes</name>
    </author>
    <author>
      <name>Serguei Komech</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.5957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.7082v1</id>
    <updated>2013-02-28T04:50:31Z</updated>
    <published>2013-02-28T04:50:31Z</published>
    <title>K Means Segmentation of Alzheimers Disease in PET scan datasets: An
  implementation</title>
    <summary>  The Positron Emission Tomography (PET) scan image requires expertise in the
segmentation where clustering algorithm plays an important role in the
automation process. The algorithm optimization is concluded based on the
performance, quality and number of clusters extracted. This paper is proposed
to study the commonly used K Means clustering algorithm and to discuss a brief
list of toolboxes for reproducing and extending works presented in medical
image analysis. This work is compiled using AForge .NET framework in windows
environment and MATrix LABoratory (MATLAB 7.0.1)
</summary>
    <author>
      <name>A. Meena</name>
    </author>
    <author>
      <name>K. Raja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Joint Conference on Advances in Signal Processing and
  Information Technology, SPIT2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNICST, ISSN:1867 To 8211 pp. 158 To 162, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.7082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.7082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.0634v1</id>
    <updated>2013-03-04T08:06:07Z</updated>
    <published>2013-03-04T08:06:07Z</published>
    <title>Indian Sign Language Recognition Using Eigen Value Weighted Euclidean
  Distance Based Classification Technique</title>
    <summary>  Sign Language Recognition is one of the most growing fields of research
today. Many new techniques have been developed recently in these fields. Here
in this paper, we have proposed a system using Eigen value weighted Euclidean
distance as a classification technique for recognition of various Sign
Languages of India. The system comprises of four parts: Skin Filtering, Hand
Cropping, Feature Extraction and Classification. Twenty four signs were
considered in this paper, each having ten samples, thus a total of two hundred
forty images was considered for which recognition rate obtained was 97 percent.
</summary>
    <author>
      <name>Joyeeta Singha</name>
    </author>
    <author>
      <name>Karen Das</name>
    </author>
    <link href="http://arxiv.org/abs/1303.0634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.0634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.0635v1</id>
    <updated>2013-03-04T08:09:22Z</updated>
    <published>2013-03-04T08:09:22Z</published>
    <title>Recognition of Facial Expression Using Eigenvector Based Distributed
  Features and Euclidean Distance Based Decision Making Technique</title>
    <summary>  In this paper, an Eigenvector based system has been presented to recognize
facial expressions from digital facial images. In the approach, firstly the
images were acquired and cropping of five significant portions from the image
was performed to extract and store the Eigenvectors specific to the
expressions. The Eigenvectors for the test images were also computed, and
finally the input facial image was recognized when similarity was obtained by
calculating the minimum Euclidean distance between the test image and the
different expressions.
</summary>
    <author>
      <name>Jeemoni Kalita</name>
    </author>
    <author>
      <name>Karen Das</name>
    </author>
    <link href="http://arxiv.org/abs/1303.0635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.0635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.4840v1</id>
    <updated>2013-03-20T04:59:08Z</updated>
    <published>2013-03-20T04:59:08Z</published>
    <title>Asynchronous Cellular Operations on Gray Images Extracting Topographic
  Shape Features and Their Relations</title>
    <summary>  A variety of operations of cellular automata on gray images is presented. All
operations are of a wave-front nature finishing in a stable state. They are
used to extract shape descripting gray objects robust to a variety of pattern
distortions. Topographic terms are used: "lakes", "dales", "dales of dales". It
is shown how mutual object relations like "above" can be presented in terms of
gray image analysis and how it can be used for character classification and for
gray pattern decomposition. Algorithms can be realized with a parallel
asynchronous architecture. Keywords: Pattern Recognition, Mathematical
Morphology, Cellular Automata, Wave-front Algorithms, Gray Image Analysis,
Topographical Shape Descriptors, Asynchronous Parallel Processors, Holes,
Cavities, Concavities, Graphs.
</summary>
    <author>
      <name>Igor Polkovnikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 37 figures, 10 function classes</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.4840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.4840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.4866v1</id>
    <updated>2013-03-20T08:15:07Z</updated>
    <published>2013-03-20T08:15:07Z</published>
    <title>A Robust Rapid Approach to Image Segmentation with Optimal Thresholding
  and Watershed Transform</title>
    <summary>  This paper describes a novel method for partitioning image into meaningful
segments. The proposed method employs watershed transform, a well-known image
segmentation technique. Along with that, it uses various auxiliary schemes such
as Binary Gradient Masking, dilation which segment the image in proper way. The
algorithm proposed in this paper considers all these methods in effective way
and takes little time. It is organized in such a manner so that it operates on
input image adaptively. Its robustness and efficiency makes it more convenient
and suitable for all types of images.
</summary>
    <author>
      <name>Ankit R. Chadha</name>
    </author>
    <author>
      <name>Neha S. Satam</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/10949-5908</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/10949-5908" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1303.4866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.4866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.6926v1</id>
    <updated>2013-03-27T18:57:12Z</updated>
    <published>2013-03-27T18:57:12Z</published>
    <title>A Comparative Analysis on the Applicability of Entropy in remote sensing</title>
    <summary>  Entropy is the measure of uncertainty in any data and is adopted for
maximisation of mutual information in many remote sensing operations. The
availability of wide entropy variations motivated us for an investigation over
the suitability preference of these versions to specific operations.
Methodologies were implemented in Matlab and were enhanced with entropy
variations. Evaluation of various implementations was based on different
statistical parameters with reference to the study area The popular available
versions like Tsalli's, Shanon's, and Renyi's entropies were analysed in
context of various remote sensing operations namely thresholding, clustering
and registration.
</summary>
    <author>
      <name>Dr. S. K. Katiyar</name>
    </author>
    <author>
      <name>Arun P. V.</name>
    </author>
    <link href="http://arxiv.org/abs/1303.6926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.6926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.0019v1</id>
    <updated>2013-03-29T20:32:04Z</updated>
    <published>2013-03-29T20:32:04Z</published>
    <title>Age group and gender recognition from human facial images</title>
    <summary>  This work presents an automatic human gender and age group recognition system
based on human facial images. It makes an extensive experiment with row pixel
intensity valued features and Discrete Cosine Transform (DCT) coefficient
features with Principal Component Analysis and k-Nearest Neighbor
classification to identify the best recognition approach. The final results
show approaches using DCT coefficient outperform their counter parts resulting
in a 99% correct gender recognition rate and 68% correct age group recognition
rate (considering four distinct age groups) in unseen test images. Detailed
experimental settings and obtained results are clearly presented and explained
in this report.
</summary>
    <author>
      <name>Tizita Nesibu Shewaye</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, October, 2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ethiopian Society of Electrical Engineers 6th Scientific
  Conference on Electrical Engineering (CEE-2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.0019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.0019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.1022v1</id>
    <updated>2013-04-03T17:34:20Z</updated>
    <published>2013-04-03T17:34:20Z</published>
    <title>A software for aging faces applied to ancient marble busts</title>
    <summary>  The study and development of software able to show the effect of aging of
faces is one of the tasks of face recognition technologies. Some software
solutions are used for investigations, some others to show the effects of drugs
on healthy appearance, however some other applications can be proposed for the
analysis of visual arts. Here we use a freely available software, which is
providing interesting results, for the comparison of ancient marble busts. An
analysis of Augustus busts is proposed.
</summary>
    <author>
      <name>Amelia Carolina Sparavigna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Image processing. Aging faces. Freely available software. Ancient
  marble busts. Augustus</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.1022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.1022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.1568v1</id>
    <updated>2013-04-04T22:07:27Z</updated>
    <published>2013-04-04T22:07:27Z</published>
    <title>Multiscale Fractal Descriptors Applied to Texture Classification</title>
    <summary>  This work proposes the combination of multiscale transform with fractal
descriptors employed in the classification of gray-level texture images. We
apply the space-scale transform (derivative + Gaussian filter) over the
Bouligand-Minkowski fractal descriptors, followed by a threshold over the
filter response, aiming at attenuating noise effects caused by the final part
of this response. The method is tested in the classification of a well-known
data set (Brodatz) and compared with other classical texture descriptor
techniques. The results demonstrate the advantage of the proposed approach,
achieving a higher success rate with a reduced amount of descriptors.
</summary>
    <author>
      <name>João Batista Florindo</name>
    </author>
    <author>
      <name>Odemir Martinez Bruno</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-6596/410/1/012022</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-6596/410/1/012022" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Physics: Conference Series, 410, 012022, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.1568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.1568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.1930v1</id>
    <updated>2013-04-06T19:24:40Z</updated>
    <published>2013-04-06T19:24:40Z</published>
    <title>Client-Driven Content Extraction Associated with Table</title>
    <summary>  The goal of the project is to extract content within table in document images
based on learnt patterns. Real-world users i.e., clients first provide a set of
key fields within the table which they think are important. These are first
used to represent the graph where nodes are labelled with semantics including
other features and edges are attributed with relations. Attributed relational
graph (ARG) is then employed to mine similar graphs from a document image. Each
mined graph will represent an item within the table, and hence a set of such
graphs will compose a table. We have validated the concept by using a
real-world industrial problem.
</summary>
    <author>
      <name>K. C. Santosh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Abdel Belaïd</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Vision Applications (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.1930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.1930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.5212v1</id>
    <updated>2013-04-18T18:41:47Z</updated>
    <published>2013-04-18T18:41:47Z</published>
    <title>Object Tracking in Videos: Approaches and Issues</title>
    <summary>  Mobile object tracking has an important role in the computer vision
applications. In this paper, we use a tracked target-based taxonomy to present
the object tracking algorithms. The tracked targets are divided into three
categories: points of interest, appearance and silhouette of mobile objects.
Advantages and limitations of the tracking approaches are also analyzed to find
the future directions in the object tracking domain.
</summary>
    <author>
      <name>Duc Phu Chau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>François Bremond</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>Monique Thonnat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The International Workshop "Rencontres UNS-UD" (RUNSUD) (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.5212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.5212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.6933v2</id>
    <updated>2013-04-26T08:35:18Z</updated>
    <published>2013-04-25T15:14:42Z</published>
    <title>Digit Recognition in Handwritten Weather Records</title>
    <summary>  This paper addresses the automatic recognition of handwritten temperature
values in weather records. The localization of table cells is based on line
detection using projection profiles. Further, a stroke-preserving line removal
method which is based on gradient images is proposed. The presented digit
recognition utilizes features which are extracted using a set of filters and a
Support Vector Machine classifier. It was evaluated on the MNIST and the USPS
dataset and our own database with about 17,000 RGB digit images. An accuracy of
99.36% per digit is achieved for the entire system using a set of 84 weather
records.
</summary>
    <author>
      <name>Manuel Keglevic</name>
    </author>
    <author>
      <name>Robert Sablatnig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876), 8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.6933v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.6933v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.6990v1</id>
    <updated>2013-04-25T19:44:26Z</updated>
    <published>2013-04-25T19:44:26Z</published>
    <title>Euclidean Upgrade from a Minimal Number of Segments</title>
    <summary>  In this paper, we propose an algebraic approach to upgrade a projective
reconstruction to a Euclidean one, and aim at computing the rectifying
homography from a minimal number of 9 segments of known length. Constraints are
derived from these segments which yield a set of polynomial equations that we
solve by means of Gr\"obner bases. We explain how a solver for such a system of
equations can be constructed from simplified template data. Moreover, we
present experiments that demonstrate that the given problem can be solved in
this way.
</summary>
    <author>
      <name>Tanja Schilling</name>
    </author>
    <author>
      <name>Tomas Pajdla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876)</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.6990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.6990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7153v1</id>
    <updated>2013-04-26T13:10:22Z</updated>
    <published>2013-04-26T13:10:22Z</published>
    <title>A Convex Approach for Image Hallucination</title>
    <summary>  In this paper we propose a global convex approach for image hallucination.
Altering the idea of classical multi image super resolution (SU) systems to
single image SU, we incorporate aligned images to hallucinate the output. Our
work is based on the paper of Tappen et al. where they use a non-convex model
for image hallucination. In comparison we formulate a convex primal
optimization problem and derive a fast converging primal-dual algorithm with a
global optimal solution. We use a database with face images to incorporate
high-frequency details to the high-resolution output. We show that we can
achieve state-of-the-art results by using a convex approach.
</summary>
    <author>
      <name>Peter Innerhofer</name>
    </author>
    <author>
      <name>Thomas Pock</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to \"OAGM-AAPR 2013, 8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.7153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7184v1</id>
    <updated>2013-04-26T14:33:52Z</updated>
    <published>2013-04-26T14:33:52Z</published>
    <title>Reading Ancient Coin Legends: Object Recognition vs. OCR</title>
    <summary>  Standard OCR is a well-researched topic of computer vision and can be
considered solved for machine-printed text. However, when applied to
unconstrained images, the recognition rates drop drastically. Therefore, the
employment of object recognition-based techniques has become state of the art
in scene text recognition applications. This paper presents a scene text
recognition method tailored to ancient coin legends and compares the results
achieved in character and word recognition experiments to a standard OCR
engine. The conducted experiments show that the proposed method outperforms the
standard OCR engine on a set of 180 cropped coin legend words.
</summary>
    <author>
      <name>Albert Kavelar</name>
    </author>
    <author>
      <name>Sebastian Zambanini</name>
    </author>
    <author>
      <name>Martin Kampel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876)</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.7184v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7184v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7399v1</id>
    <updated>2013-04-27T19:24:30Z</updated>
    <published>2013-04-27T19:24:30Z</published>
    <title>Bingham Procrustean Alignment for Object Detection in Clutter</title>
    <summary>  A new system for object detection in cluttered RGB-D images is presented. Our
main contribution is a new method called Bingham Procrustean Alignment (BPA) to
align models with the scene. BPA uses point correspondences between oriented
features to derive a probability distribution over possible model poses. The
orientation component of this distribution, conditioned on the position, is
shown to be a Bingham distribution. This result also applies to the classic
problem of least-squares alignment of point sets, when point features are
orientation-less, and gives a principled, probabilistic way to measure pose
uncertainty in the rigid alignment problem. Our detection system leverages BPA
to achieve more reliable object detections in clutter.
</summary>
    <author>
      <name>Jared Glover</name>
    </author>
    <author>
      <name>Sanja Popovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IROS 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.7399v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7399v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.0020v1</id>
    <updated>2013-04-30T20:28:37Z</updated>
    <published>2013-04-30T20:28:37Z</published>
    <title>Image Compression By Embedding Five Modulus Method Into JPEG</title>
    <summary>  The standard JPEG format is almost the optimum format in image compression.
The compression ratio in JPEG sometimes reaches 30:1. The compression ratio of
JPEG could be increased by embedding the Five Modulus Method (FMM) into the
JPEG algorithm. The novel algorithm gives twice the time as the standard JPEG
algorithm or more. The novel algorithm was called FJPEG (Five-JPEG). The
quality of the reconstructed image after compression is approximately
approaches the JPEG. Standard test images have been used to support and
implement the suggested idea in this paper and the error metrics have been
computed and compared with JPEG.
</summary>
    <author>
      <name>Firas A. Jassim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/sipij.2013.4203</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/sipij.2013.4203" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 tables, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.4, No.2, April 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1305.0020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.0020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.3939v1</id>
    <updated>2013-05-16T21:25:54Z</updated>
    <published>2013-05-16T21:25:54Z</published>
    <title>Analysis Of Interest Points Of Curvelet Coefficients Contributions Of
  Microscopic Images And Improvement Of Edges</title>
    <summary>  This paper focuses on improved edge model based on Curvelet coefficients
analysis. Curvelet transform is a powerful tool for multiresolution
representation of object with anisotropic edge. Curvelet coefficients
contributions have been analyzed using Scale Invariant Feature Transform
(SIFT), commonly used to study local structure in images. The permutation of
Curvelet coefficients from original image and edges image obtained from
gradient operator is used to improve original edges. Experimental results show
that this method brings out details on edges when the decomposition scale
increases.
</summary>
    <author>
      <name>A. Djimeli</name>
    </author>
    <author>
      <name>D. Tchiotsop</name>
    </author>
    <author>
      <name>R. Tchinda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/sipij.2013.4201</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/sipij.2013.4201" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.4, No.2, April 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1305.3939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.3939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.4298v1</id>
    <updated>2013-05-18T20:45:40Z</updated>
    <published>2013-05-18T20:45:40Z</published>
    <title>Blockwise SURE Shrinkage for Non-Local Means</title>
    <summary>  In this letter, we investigate the shrinkage problem for the non-local means
(NLM) image denoising. In particular, we derive the closed-form of the optimal
blockwise shrinkage for NLM that minimizes the Stein's unbiased risk estimator
(SURE). We also propose a constant complexity algorithm allowing fast blockwise
shrinkage. Simulation results show that the proposed blockwise shrinkage method
improves NLM performance in attaining higher peak signal noise ratio (PSNR) and
structural similarity index (SSIM), and makes NLM more robust against parameter
changes. Similar ideas can be applicable to other patchwise image denoising
techniques.
</summary>
    <author>
      <name>Yue Wu</name>
    </author>
    <author>
      <name>Brian Tracey</name>
    </author>
    <author>
      <name>Premkumar Natarajan</name>
    </author>
    <author>
      <name>Joseph P. Noonan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.sigpro.2014.01.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.sigpro.2014.01.007" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Signal Processing 103 (2014): 45-59</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1305.4298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.4298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.5160v1</id>
    <updated>2013-05-22T15:00:43Z</updated>
    <published>2013-05-22T15:00:43Z</published>
    <title>A novel automatic thresholding segmentation method with local adaptive
  thresholds</title>
    <summary>  A novel method for segmenting bright objects from dark background for
grayscale image is proposed. The concept of this method can be stated simply
as: to pick out the local-thinnest bands on the grayscale grade-map. It turns
out to be a threshold-based method with local adaptive thresholds, where each
local threshold is determined by requiring the average normal-direction
gradient on the object boundary to be local minimal. The method is highly
automatic and the segmentation mimics a man's natural expectation even the
object boundaries are fuzzy.
</summary>
    <author>
      <name>Bo Xiao</name>
    </author>
    <author>
      <name>Yuefeng Jing</name>
    </author>
    <author>
      <name>Yonghong Guan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.5160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.5160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.6883v1</id>
    <updated>2013-05-29T17:58:04Z</updated>
    <published>2013-05-29T17:58:04Z</published>
    <title>Rotation invariants of two dimensional curves based on iterated
  integrals</title>
    <summary>  We introduce a novel class of rotation invariants of two dimensional curves
based on iterated integrals. The invariants we present are in some sense
complete and we describe an algorithm to calculate them, giving explicit
computations up to order six. We present an application to online
(stroke-trajectory based) character recognition. This seems to be the first
time in the literature that the use of iterated integrals of a curve is
proposed for (invariant) feature extraction in machine learning applications.
</summary>
    <author>
      <name>Joscha Diehl</name>
    </author>
    <link href="http://arxiv.org/abs/1305.6883v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.6883v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.0998v3</id>
    <updated>2014-07-01T00:24:20Z</updated>
    <published>2013-07-03T12:59:53Z</published>
    <title>A Unified Framework of Elementary Geometric Transformation
  Representation</title>
    <summary>  As an extension of projective homology, stereohomology is proposed via an
extension of Desargues theorem and the extended Desargues configuration.
Geometric transformations such as reflection, translation, central symmetry,
central projection, parallel projection, shearing, central dilation, scaling,
and so on are all included in stereohomology and represented as
Householder-Chen elementary matrices. Hence all these geometric transformations
are called elementary. This makes it possible to represent these elementary
geometric transformations in homogeneous square matrices independent of a
particular choice of coordinate system.
</summary>
    <author>
      <name>F. Lu</name>
    </author>
    <author>
      <name>Z. Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 11 figures, 1 table, 21 referneces</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.0998v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.0998v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.2440v1</id>
    <updated>2013-07-09T13:14:11Z</updated>
    <published>2013-07-09T13:14:11Z</published>
    <title>Image Fusion Technologies In Commercial Remote Sensing Packages</title>
    <summary>  Several remote sensing software packages are used to the explicit purpose of
analyzing and visualizing remotely sensed data, with the developing of remote
sensing sensor technologies from last ten years. Accord-ing to literature, the
remote sensing is still the lack of software tools for effective information
extraction from remote sensing data. So, this paper provides a state-of-art of
multi-sensor image fusion technologies as well as review on the quality
evaluation of the single image or fused images in the commercial remote sensing
pack-ages. It also introduces program (ALwassaiProcess) developed for image
fusion and classification.
</summary>
    <author>
      <name>Firouz Abdullah Al-Wassai</name>
    </author>
    <author>
      <name>N. V. Kalyankar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: Commercial Processing Systems, Image Fusion, quality
  evaluation</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Global Research in Computer Science, 4 (5), May 2013,
  44-50</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.2440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.2440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3043v2</id>
    <updated>2013-09-13T15:48:23Z</updated>
    <published>2013-07-11T10:07:19Z</published>
    <title>A two-layer Conditional Random Field for the classification of partially
  occluded objects</title>
    <summary>  Conditional Random Fields (CRF) are among the most popular techniques for
image labelling because of their flexibility in modelling dependencies between
the labels and the image features. This paper proposes a novel CRF-framework
for image labeling problems which is capable to classify partially occluded
objects. Our approach is evaluated on aerial near-vertical images as well as on
urban street-view images and compared with another methods.
</summary>
    <author>
      <name>Sergey Kosov</name>
    </author>
    <author>
      <name>Pushmeet Kohli</name>
    </author>
    <author>
      <name>Franz Rottensteiner</name>
    </author>
    <author>
      <name>Christian Heipke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference Submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.3043v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3043v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3271v1</id>
    <updated>2013-07-11T21:01:23Z</updated>
    <published>2013-07-11T21:01:23Z</published>
    <title>Fuzzy Fibers: Uncertainty in dMRI Tractography</title>
    <summary>  Fiber tracking based on diffusion weighted Magnetic Resonance Imaging (dMRI)
allows for noninvasive reconstruction of fiber bundles in the human brain. In
this chapter, we discuss sources of error and uncertainty in this technique,
and review strategies that afford a more reliable interpretation of the
results. This includes methods for computing and rendering probabilistic
tractograms, which estimate precision in the face of measurement noise and
artifacts. However, we also address aspects that have received less attention
so far, such as model selection, partial voluming, and the impact of
parameters, both in preprocessing and in fiber tracking itself. We conclude by
giving impulses for future research.
</summary>
    <author>
      <name>Thomas Schultz</name>
    </author>
    <author>
      <name>Anna Vilanova</name>
    </author>
    <author>
      <name>Ralph Brecheisen</name>
    </author>
    <author>
      <name>Gordon Kindlmann</name>
    </author>
    <link href="http://arxiv.org/abs/1307.3271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.6549v1</id>
    <updated>2013-07-19T10:57:31Z</updated>
    <published>2013-07-19T10:57:31Z</published>
    <title>Making Laplacians commute</title>
    <summary>  In this paper, we construct multimodal spectral geometry by finding a pair of
closest commuting operators (CCO) to a given pair of Laplacians. The CCOs are
jointly diagonalizable and hence have the same eigenbasis. Our construction
naturally extends classical data analysis tools based on spectral geometry,
such as diffusion maps and spectral clustering. We provide several synthetic
and real examples of applications in dimensionality reduction, shape analysis,
and clustering, demonstrating that our method better captures the inherent
structure of multi-modal data.
</summary>
    <author>
      <name>Michael M. Bronstein</name>
    </author>
    <author>
      <name>Klaus Glashoff</name>
    </author>
    <author>
      <name>Terry A. Loring</name>
    </author>
    <link href="http://arxiv.org/abs/1307.6549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.6962v2</id>
    <updated>2013-12-01T20:46:41Z</updated>
    <published>2013-07-26T09:06:44Z</published>
    <title>Reduced egomotion estimation drift using omnidirectional views</title>
    <summary>  Estimation of camera motion from a given image sequence becomes degraded as
the length of the sequence increases. In this letter, this phenomenon is
demonstrated and an approach to increase the estimation accuracy is proposed.
The proposed method uses an omnidirectional camera in addition to the
perspective one and takes advantage of its enlarged view by exploiting the
correspondences between the omnidirectional and perspective images. Simulated
and real image experiments show that the proposed approach improves the
estimation accuracy.
</summary>
    <author>
      <name>Yalin Bastanlar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Another publisher does not want this article to be shared at
  arxiv.org in order to publish it</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronic Letters on Computer Vision and Image Analysis,
  vol.13(3), p.1-12, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.6962v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6962v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.7848v1</id>
    <updated>2013-07-30T07:18:20Z</updated>
    <published>2013-07-30T07:18:20Z</published>
    <title>An Integrated System for 3D Gaze Recovery and Semantic Analysis of Human
  Attention</title>
    <summary>  This work describes a computer vision system that enables pervasive mapping
and monitoring of human attention. The key contribution is that our methodology
enables full 3D recovery of the gaze pointer, human view frustum and associated
human centered measurements directly into an automatically computed 3D model in
real-time. We apply RGB-D SLAM and descriptor matching methodologies for the 3D
modeling, localization and fully automated annotation of ROIs (regions of
interest) within the acquired 3D model. This innovative methodology will open
new avenues for attention studies in real world environments, bringing new
potential into automated processing for human factors technologies.
</summary>
    <author>
      <name>Lucas Paletta</name>
    </author>
    <author>
      <name>Katrin Santner</name>
    </author>
    <author>
      <name>Gerald Fritz</name>
    </author>
    <link href="http://arxiv.org/abs/1307.7848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.7848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.8233v1</id>
    <updated>2013-07-31T06:24:28Z</updated>
    <published>2013-07-31T06:24:28Z</published>
    <title>A Prototyping Environment for Integrated Artificial Attention Systems</title>
    <summary>  Artificial visual attention systems aim to support technical systems in
visual tasks by applying the concepts of selective attention observed in humans
and other animals. Such systems are typically evaluated against ground truth
obtained from human gaze-data or manually annotated test images. When applied
to robotics, the systems are required to be adaptable to the target system.
Here, we describe a flexible environment based on a robotic middleware layer
allowing the development and testing of attention-guided vision systems. In
such a framework, the systems can be tested with input from various sources,
different attention algorithms at the core, and diverse subsequent tasks.
</summary>
    <author>
      <name>Jan Tünnermann</name>
    </author>
    <author>
      <name>Markus Hennig</name>
    </author>
    <author>
      <name>Michael Silbernagel</name>
    </author>
    <author>
      <name>Bärbel Mertsching</name>
    </author>
    <link href="http://arxiv.org/abs/1307.8233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.8233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.8405v1</id>
    <updated>2013-07-31T17:53:10Z</updated>
    <published>2013-07-31T17:53:10Z</published>
    <title>Who and Where: People and Location Co-Clustering</title>
    <summary>  In this paper, we consider the clustering problem on images where each image
contains patches in people and location domains. We exploit the correlation
between people and location domains, and proposed a semi-supervised
co-clustering algorithm to cluster images. Our algorithm updates the
correlation links at the runtime, and produces clustering in both domains
simultaneously. We conduct experiments in a manually collected dataset and a
Flickr dataset. The result shows that the such correlation improves the
clustering performance.
</summary>
    <author>
      <name>Zixuan Wang</name>
    </author>
    <author>
      <name>Jinyun Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2013 IEEE International Conference on Image Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.8405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.8405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.2292v1</id>
    <updated>2013-08-10T08:19:02Z</updated>
    <published>2013-08-10T08:19:02Z</published>
    <title>Fast image segmentation and restoration using parametric curve evolution
  with junctions and topology changes</title>
    <summary>  Curve evolution schemes for image segmentation based on a region based
contour model allowing for junctions, vector-valued images and topology changes
are introduced. Together with an a posteriori denoising in the segmented
homogeneous regions this leads to a fast and efficient method for image
segmentation and restoration. An uneven spread of mesh points is avoided by
using the tangential degrees of freedom. Several numerical simulations on
artificial test problems and on real images illustrate the performance of the
method.
</summary>
    <author>
      <name>Heike Benninghoff</name>
    </author>
    <author>
      <name>Harald Garcke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.2292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.2292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94A08, 68U10, 65K10, 35K55, 49Q10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.4902v1</id>
    <updated>2013-08-22T15:38:15Z</updated>
    <published>2013-08-22T15:38:15Z</published>
    <title>A review on handwritten character and numeral recognition for Roman,
  Arabic, Chinese and Indian scripts</title>
    <summary>  There are a lot of intensive researches on handwritten character recognition
(HCR) for almost past four decades. The research has been done on some of
popular scripts such as Roman, Arabic, Chinese and Indian. In this paper we
present a review on HCR work on the four popular scripts. We have summarized
most of the published paper from 2005 to recent and also analyzed the various
methods in creating a robust HCR system. We also added some future direction of
research on HCR.
</summary>
    <author>
      <name>Aini Najwa Azmi</name>
    </author>
    <author>
      <name>Dewi Nasien</name>
    </author>
    <author>
      <name>Siti Mariyam Shamsuddin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of advanced studies in Computers, Science &amp;
  Engineering (IJASCSE), Volume 2, Issue 4, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1308.4902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.4902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.5315v1</id>
    <updated>2013-08-24T11:07:05Z</updated>
    <published>2013-08-24T11:07:05Z</published>
    <title>Edge-detection applied to moving sand dunes on Mars</title>
    <summary>  Here we discuss the application of an edge detection filter, the Sobel filter
of GIMP, to the recently discovered motion of some sand dunes on Mars. The
filter allows a good comparison of an image HiRISE of 2007 and an image of 1999
recorded by the Mars Global Surveyor of the dunes in the Nili Patera caldera,
measuring therefore the motion of the dunes on a longer period of time than
that previously investigated.
</summary>
    <author>
      <name>Amelia Carolina Sparavigna</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.18483/ijSci.251</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.18483/ijSci.251" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: Edge detection, Sobel filter, GIMP, Image processing,
  Google Mars, Dune motion, Mars Reconnaissance Orbiter, Mars Global Surveyor;
  Ref.14 available at
  http://www.scribd.com/doc/162390676/Moving-Sand-Dunes-on-Mars</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Sciences, 2013, 2(8):102-104</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1308.5315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.5315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.5876v1</id>
    <updated>2013-08-27T13:57:16Z</updated>
    <published>2013-08-27T13:57:16Z</published>
    <title>Hierarchized block wise image approximation by greedy pursuit strategies</title>
    <summary>  An approach for effective implementation of greedy selection methodologies,
to approximate an image partitioned into blocks, is proposed. The method is
specially designed for approximating partitions on a transformed image. It
evolves by selecting, at each iteration step, i) the elements for approximating
each of the blocks partitioning the image and ii) the hierarchized sequence in
which the blocks are approximated to reach the required global condition on
sparsity.
</summary>
    <author>
      <name>Laura Rebollo-Neira</name>
    </author>
    <author>
      <name>Ryszard Maciol</name>
    </author>
    <author>
      <name>Shabnam Bibi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2013.2283510</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2013.2283510" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages. An example and the computing routines for implementing the
  approach are available on
  http://www.nonlinear-approx.info/examples/node0.html</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.5876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.5876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U10, 94A08" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.0123v2</id>
    <updated>2013-10-02T01:53:49Z</updated>
    <published>2013-08-31T14:29:52Z</published>
    <title>A Robust Alternating Direction Method for Constrained Hybrid Variational
  Deblurring Model</title>
    <summary>  In this work, a new constrained hybrid variational deblurring model is
developed by combining the non-convex first- and second-order total variation
regularizers. Moreover, a box constraint is imposed on the proposed model to
guarantee high deblurring performance. The developed constrained hybrid
variational model could achieve a good balance between preserving image details
and alleviating ringing artifacts. In what follows, we present the
corresponding numerical solution by employing an iteratively reweighted
algorithm based on alternating direction method of multipliers. The
experimental results demonstrate the superior performance of the proposed
method in terms of quantitative and qualitative image quality assessments.
</summary>
    <author>
      <name>Ryan Wen Liu</name>
    </author>
    <author>
      <name>Tian Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.0123v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.0123v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65K10, 68U10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.4; G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.3418v1</id>
    <updated>2013-09-13T10:12:22Z</updated>
    <published>2013-09-13T10:12:22Z</published>
    <title>A Novel Approach in detecting pose orientation of a 3D face required for
  face</title>
    <summary>  In this paper we present a novel approach that takes as input a 3D image and
gives as output its pose i.e. it tells whether the face is oriented with
respect the X, Y or Z axes with angles of rotation up to 40 degree. All the
experiments have been performed on the FRAV3D Database. After applying the
proposed algorithm to the 3D facial surface we have obtained i.e. on 848 3D
face images our method detected the pose correctly for 566 face images,thus
giving an approximately 67 % of correct pose detection.
</summary>
    <author>
      <name>Parama Bagchi</name>
    </author>
    <author>
      <name>Debotosh Bhattacharjee</name>
    </author>
    <author>
      <name>Mita Nasipuri</name>
    </author>
    <author>
      <name>Dipak Kumar Basu</name>
    </author>
    <link href="http://arxiv.org/abs/1309.3418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.3418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.3425v1</id>
    <updated>2013-09-13T11:28:29Z</updated>
    <published>2013-09-13T11:28:29Z</published>
    <title>A method for nose-tip based 3D face registration using maximum intensity
  algorithm</title>
    <summary>  In this paper we present a novel technique of registering 3D images across
pose. In this context, we have taken into account the images which are aligned
across X, Y and Z axes. We have first determined the angle across which the
image is rotated with respect to X, Y and Z axes and then translation is
performed on the images. After testing the proposed method on 472 images from
the FRAV3D database, the method correctly registers 358 images thus giving a
performance rate of 75.84%.
</summary>
    <author>
      <name>Parama Bagchi</name>
    </author>
    <author>
      <name>Debotosh Bhattacharjee</name>
    </author>
    <author>
      <name>Mita Nasipuri</name>
    </author>
    <author>
      <name>Dipak kr. Basu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.3425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.3425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.0310v1</id>
    <updated>2013-10-01T14:15:48Z</updated>
    <published>2013-10-01T14:15:48Z</published>
    <title>A Novel Georeferenced Dataset for Stereo Visual Odometry</title>
    <summary>  In this work, we present a novel dataset for assessing the accuracy of stereo
visual odometry. The dataset has been acquired by a small-baseline stereo rig
mounted on the top of a moving car. The groundtruth is supplied by a consumer
grade GPS device without IMU. Synchronization and alignment between GPS
readings and stereo frames are recovered after the acquisition. We show that
the attained groundtruth accuracy allows to draw useful conclusions in
practice. The presented experiments address influence of camera calibration,
baseline distance and zero-disparity features to the achieved reconstruction
performance.
</summary>
    <author>
      <name>Ivan Krešo</name>
    </author>
    <author>
      <name>Marko Ševrović</name>
    </author>
    <author>
      <name>Siniša Šegvić</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Part of the Proceedings of the Croatian Computer Vision Workshop,
  CCVW 2013, Year 1</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.0310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.0310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.2050v1</id>
    <updated>2013-10-08T09:04:34Z</updated>
    <published>2013-10-08T09:04:34Z</published>
    <title>A State Of the Art Report on Research in Multiple RGB-D sensor Setups</title>
    <summary>  That the Microsoft Kinect, an RGB-D sensor, transformed the gaming and end
consumer sector has been anticipated by the developers. That it also impacted
in rigorous computer vision research has probably been a surprise to the whole
community. Shortly before the commercial deployment of its successor, Kinect
One, the research literature fills with resumees and state-of-the art papers to
summarize the development over the past 3 years. This particular report
describes significant research projects which have built on sensoring setups
that include two or more RGB-D sensors in one scene.
</summary>
    <author>
      <name>Kai Berger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.2050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.2050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.2053v1</id>
    <updated>2013-10-08T09:16:56Z</updated>
    <published>2013-10-08T09:16:56Z</published>
    <title>The role of RGB-D benchmark datasets: an overview</title>
    <summary>  The advent of the Microsoft Kinect three years ago stimulated not only the
computer vision community for new algorithms and setups to tackle well-known
problems in the community but also sparked the launch of several new benchmark
datasets to which future algorithms can be compared 019 to. This review of the
literature and industry developments concludes that the current RGB-D benchmark
datasets can be useful to determine the accuracy of a variety of applications
of a single or multiple RGB-D sensors.
</summary>
    <author>
      <name>Kai Berger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.2053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.2053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.3452v1</id>
    <updated>2013-10-13T06:58:57Z</updated>
    <published>2013-10-13T06:58:57Z</published>
    <title>Dense Scattering Layer Removal</title>
    <summary>  We propose a new model, together with advanced optimization, to separate a
thick scattering media layer from a single natural image. It is able to handle
challenging underwater scenes and images taken in fog and sandstorm, both of
which are with significantly reduced visibility. Our method addresses the
critical issue -- this is, originally unnoticeable impurities will be greatly
magnified after removing the scattering media layer -- with transmission-aware
optimization. We introduce non-local structure-aware regularization to properly
constrain transmission estimation without introducing the halo artifacts. A
selective-neighbor criterion is presented to convert the unconventional
constrained optimization problem to an unconstrained one where the latter can
be efficiently solved.
</summary>
    <author>
      <name>Qiong Yan</name>
    </author>
    <author>
      <name>Li Xu</name>
    </author>
    <author>
      <name>Jiaya Jia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures, Siggraph Asia 2013 Technial Briefs</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.3452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.3452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.3717v1</id>
    <updated>2013-10-14T15:19:45Z</updated>
    <published>2013-10-14T15:19:45Z</published>
    <title>Misfire Detection in IC Engine using Kstar Algorithm</title>
    <summary>  Misfire in an IC Engine continues to be a problem leading to reduced fuel
efficiency, increased power loss and emissions containing heavy concentration
of hydrocarbons. Misfiring creates a unique vibration pattern attributed to a
particular cylinder. Useful features can be extracted from these patterns and
can be analyzed to detect misfire. Statistical features from these vibration
signals were extracted. Out of these, useful features were identified using the
J48 decision tree algorithm and selected features were used for classification
using the Kstar algorithm. In this paper performance analysis of Kstar
algorithm is presented.
</summary>
    <author>
      <name>Anish Bahri</name>
    </author>
    <author>
      <name>V Sugumaran</name>
    </author>
    <author>
      <name>S Babu Devasenapati</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages, 8 Figures, 4 Tables. International Journal of Research in
  Mechanical Engineering, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.3717v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.3717v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.6376v1</id>
    <updated>2013-10-23T20:10:36Z</updated>
    <published>2013-10-23T20:10:36Z</published>
    <title>Can Facial Uniqueness be Inferred from Impostor Scores?</title>
    <summary>  In Biometrics, facial uniqueness is commonly inferred from impostor
similarity scores. In this paper, we show that such uniqueness measures are
highly unstable in the presence of image quality variations like pose, noise
and blur. We also experimentally demonstrate the instability of a recently
introduced impostor-based uniqueness measure of [Klare and Jain 2013] when
subject to poor quality facial images.
</summary>
    <author>
      <name>Abhishek Dutta</name>
    </author>
    <author>
      <name>Raymond Veldhuis</name>
    </author>
    <author>
      <name>Luuk Spreeuwers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A 6 page paper presented in the Biometric Technologies in Forensic
  Science (BTFS) 2013 Conference, Oct 14-15 2013, Nijmegen, Netherlands. Full
  proceeding is available at http://www.ru.nl/clst/btfs/btfs-2013/</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.6376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.6376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.6808v1</id>
    <updated>2013-10-25T02:37:44Z</updated>
    <published>2013-10-25T02:37:44Z</published>
    <title>Gender Classification Using Gradient Direction Pattern</title>
    <summary>  A novel methodology for gender classification is presented in this paper. It
extracts feature from local region of a face using gray color intensity
difference. The facial area is divided into sub-regions and GDP histogram
extracted from those regions are concatenated into a single vector to represent
the face. The classification accuracy obtained by using support vector machine
has outperformed all traditional feature descriptors for gender classification.
It is evaluated on the images collected from FERET database and obtained very
high accuracy.
</summary>
    <author>
      <name>Mohammad shahidul Islam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 5 figures, 3 tables, SCI journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Sci.Int(Lahore),25(4),797-799,2013 ISSN 1013-5316; CODEN: SINTE 8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1310.6808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.6808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.7114v1</id>
    <updated>2013-10-26T14:21:06Z</updated>
    <published>2013-10-26T14:21:06Z</published>
    <title>Efficient Information Theoretic Clustering on Discrete Lattices</title>
    <summary>  We consider the problem of clustering data that reside on discrete, low
dimensional lattices. Canonical examples for this setting are found in image
segmentation and key point extraction. Our solution is based on a recent
approach to information theoretic clustering where clusters result from an
iterative procedure that minimizes a divergence measure. We replace costly
processing steps in the original algorithm by means of convolutions. These
allow for highly efficient implementations and thus significantly reduce
runtime. This paper therefore bridges a gap between machine learning and signal
processing.
</summary>
    <author>
      <name>Christian Bauckhage</name>
    </author>
    <author>
      <name>Kristian Kersting</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been presented at the workshop LWA 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.7114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.7114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.2102v1</id>
    <updated>2013-11-08T22:49:07Z</updated>
    <published>2013-11-08T22:49:07Z</published>
    <title>An Experimental Comparison of Trust Region and Level Sets</title>
    <summary>  High-order (non-linear) functionals have become very popular in segmentation,
stereo and other computer vision problems. Level sets is a well established
general gradient descent framework, which is directly applicable to
optimization of such functionals and widely used in practice. Recently, another
general optimization approach based on trust region methodology was proposed
for regional non-linear functionals. Our goal is a comprehensive experimental
comparison of these two frameworks in regard to practical efficiency,
robustness to parameters, and optimality. We experiment on a wide range of
problems with non-linear constraints on segment volume, appearance and shape.
</summary>
    <author>
      <name>Lena Gorelick</name>
    </author>
    <author>
      <name>Ismail BenAyed</name>
    </author>
    <author>
      <name>Frank R. Schmidt</name>
    </author>
    <author>
      <name>Yuri Boykov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.2102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.2102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6049v1</id>
    <updated>2013-11-23T20:52:05Z</updated>
    <published>2013-11-23T20:52:05Z</published>
    <title>Skin Texture Recognition Using Neural Networks</title>
    <summary>  Skin recognition is used in many applications ranging from algorithms for
face detection, hand gesture analysis, and to objectionable image filtering. In
this work a skin recognition system was developed and tested. While many skin
segmentation algorithms relay on skin color, our work relies on both skin color
and texture features (features derives from the GLCM) to give a better and more
efficient recognition accuracy of skin textures. We used feed forward neural
networks to classify input textures images to be skin or non skin textures. The
system gave very encouraging results during the neural network generalization
face.
</summary>
    <author>
      <name>Nidhal K. El Abbadi</name>
    </author>
    <author>
      <name>Nazar Dahir</name>
    </author>
    <author>
      <name>Zaid Abd Alkareem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures, conference ACIT 2008, Tunisia</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.6049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6758v1</id>
    <updated>2013-11-24T16:59:19Z</updated>
    <published>2013-11-24T16:59:19Z</published>
    <title>Detection of Partially Visible Objects</title>
    <summary>  An "elephant in the room" for most current object detection and localization
methods is the lack of explicit modelling of partial visibility due to
occlusion by other objects or truncation by the image boundary. Based on a
sliding window approach, we propose a detection method which explicitly models
partial visibility by treating it as a latent variable. A novel non-maximum
suppression scheme is proposed which takes into account the inferred partial
visibility of objects while providing a globally optimal solution. The method
gives more detailed scene interpretations than conventional detectors in that
we are able to identify the visible parts of an object. We report improved
average precision on the PASCAL VOC 2010 dataset compared to a baseline
detector.
</summary>
    <author>
      <name>Patrick Ott</name>
    </author>
    <author>
      <name>Mark Everingham</name>
    </author>
    <author>
      <name>Jiri Matas</name>
    </author>
    <link href="http://arxiv.org/abs/1311.6758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6799v2</id>
    <updated>2013-12-10T16:40:09Z</updated>
    <published>2013-10-30T10:37:28Z</published>
    <title>Wavelet and Fast Fourier Transform based analysis of Solar Image</title>
    <summary>  Both of Wavelet and Fast Fourier Transform are strong signal processing tools
in the field of Data Analysis. In this paper fast fourier transform (FFT) and
Wavelet Transform are employed to observe some important features of Solar
image (December, 2004). We have tried to find out the periodicity and coherence
of different sections of the solar image. We plotted the distribution of energy
in solar surface by analyzing the solar image with scalograms and
3D-coefficient plots.
</summary>
    <author>
      <name>Sabyasachi Mukhopadhyay</name>
    </author>
    <author>
      <name>Debadatta Dash</name>
    </author>
    <author>
      <name>Swapnil Barmase</name>
    </author>
    <author>
      <name>Prasanta K Panigrahi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to some modifications
  are required for this current paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.6799v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6799v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6932v1</id>
    <updated>2013-11-27T11:06:05Z</updated>
    <published>2013-11-27T11:06:05Z</published>
    <title>A novel framework for image forgery localization</title>
    <summary>  Image forgery localization is a very active and open research field for the
difficulty to handle the large variety of manipulations a malicious user can
perform by means of more and more sophisticated image editing tools. Here, we
propose a localization framework based on the fusion of three very different
tools, based, respectively, on sensor noise, patch-matching, and machine
learning. The binary masks provided by these tools are finally fused based on
some suitable reliability indexes. According to preliminary experiments on the
training set, the proposed framework provides often a very good localization
accuracy and sometimes valuable clues for visual scrutiny.
</summary>
    <author>
      <name>Davide Cozzolino</name>
    </author>
    <author>
      <name>Diego Gragnaniello</name>
    </author>
    <author>
      <name>Luisa Verdoliva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.6932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6934v1</id>
    <updated>2013-11-27T11:17:55Z</updated>
    <published>2013-11-27T11:17:55Z</published>
    <title>Image forgery detection based on the fusion of machine learning and
  block-matching methods</title>
    <summary>  Dense local descriptors and machine learning have been used with success in
several applications, like classification of textures, steganalysis, and
forgery detection. We develop a new image forgery detector building upon some
descriptors recently proposed in the steganalysis field suitably merging some
of such descriptors, and optimizing a SVM classifier on the available training
set. Despite the very good performance, very small forgeries are hardly ever
detected because they contribute very little to the descriptors. Therefore we
also develop a simple, but extremely specific, copy-move detector based on
region matching and fuse decisions so as to reduce the missing detection rate.
Overall results appear to be extremely encouraging.
</summary>
    <author>
      <name>Davide Cozzolino</name>
    </author>
    <author>
      <name>Diego Gragnaniello</name>
    </author>
    <author>
      <name>Luisa Verdoliva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.6934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.7251v1</id>
    <updated>2013-11-28T09:44:45Z</updated>
    <published>2013-11-28T09:44:45Z</published>
    <title>Spatially-Adaptive Reconstruction in Computed Tomography using Neural
  Networks</title>
    <summary>  We propose a supervised machine learning approach for boosting existing
signal and image recovery methods and demonstrate its efficacy on example of
image reconstruction in computed tomography. Our technique is based on a local
nonlinear fusion of several image estimates, all obtained by applying a chosen
reconstruction algorithm with different values of its control parameters.
Usually such output images have different bias/variance trade-off. The fusion
of the images is performed by feed-forward neural network trained on a set of
known examples. Numerical experiments show an improvement in reconstruction
quality relatively to existing direct and iterative reconstruction methods.
</summary>
    <author>
      <name>Joseph Shtok</name>
    </author>
    <author>
      <name>Michael Zibulevsky</name>
    </author>
    <author>
      <name>Michael Elad</name>
    </author>
    <link href="http://arxiv.org/abs/1311.7251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.7251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.3787v2</id>
    <updated>2015-02-14T17:32:32Z</updated>
    <published>2013-12-13T12:25:09Z</published>
    <title>Analysis and Understanding of Various Models for Efficient
  Representation and Accurate Recognition of Human Faces</title>
    <summary>  In this paper we have tried to compare the various face recognition models
against their classical problems. We look at the methods followed by these
approaches and evaluate to what extent they are able to solve the problems. All
methods proposed have some drawbacks under certain conditions. To overcome
these drawbacks we propose a multi-model approach
</summary>
    <author>
      <name>Dharini S.</name>
    </author>
    <author>
      <name>Guru Prasad M.</name>
    </author>
    <author>
      <name>Hari haran. V.</name>
    </author>
    <author>
      <name>Kiran Tej J. L.</name>
    </author>
    <author>
      <name>Kunal Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of National Conference on "Emerging Trends in IT" -
  eit10, March 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.3787v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.3787v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4659v3</id>
    <updated>2014-08-20T17:42:45Z</updated>
    <published>2013-12-17T06:36:10Z</published>
    <title>DeepPose: Human Pose Estimation via Deep Neural Networks</title>
    <summary>  We propose a method for human pose estimation based on Deep Neural Networks
(DNNs). The pose estimation is formulated as a DNN-based regression problem
towards body joints. We present a cascade of such DNN regressors which results
in high precision pose estimates. The approach has the advantage of reasoning
about pose in a holistic fashion and has a simple but yet powerful formulation
which capitalizes on recent advances in Deep Learning. We present a detailed
empirical analysis with state-of-art or better performance on four academic
benchmarks of diverse real-world images.
</summary>
    <author>
      <name>Alexander Toshev</name>
    </author>
    <author>
      <name>Christian Szegedy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CVPR.2014.214</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CVPR.2014.214" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Conference on Computer Vision and Pattern Recognition, 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.4659v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4659v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4746v1</id>
    <updated>2013-12-17T12:35:19Z</updated>
    <published>2013-12-17T12:35:19Z</published>
    <title>Co-Sparse Textural Similarity for Image Segmentation</title>
    <summary>  We propose an algorithm for segmenting natural images based on texture and
color information, which leverages the co-sparse analysis model for image
segmentation within a convex multilabel optimization framework. As a key
ingredient of this method, we introduce a novel textural similarity measure,
which builds upon the co-sparse representation of image patches. We propose a
Bayesian approach to merge textural similarity with information about color and
location. Combined with recently developed convex multilabel optimization
methods this leads to an efficient algorithm for both supervised and
unsupervised segmentation, which is easily parallelized on graphics hardware.
The approach provides competitive results in unsupervised segmentation and
outperforms state-of-the-art interactive segmentation methods on the Graz
Benchmark.
</summary>
    <author>
      <name>Claudia Nieuwenhuis</name>
    </author>
    <author>
      <name>Daniel Cremers</name>
    </author>
    <author>
      <name>Simon Hawe</name>
    </author>
    <author>
      <name>Martin Kleinsteuber</name>
    </author>
    <link href="http://arxiv.org/abs/1312.4746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5402v1</id>
    <updated>2013-12-19T04:23:23Z</updated>
    <published>2013-12-19T04:23:23Z</published>
    <title>Some Improvements on Deep Convolutional Neural Network Based Image
  Classification</title>
    <summary>  We investigate multiple techniques to improve upon the current state of the
art deep convolutional neural network based image classification pipeline. The
techiques include adding more image transformations to training data, adding
more transformations to generate additional predictions at test time and using
complementary models applied to higher resolution images. This paper summarizes
our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our
system achieved a top 5 classification error rate of 13.55% using no external
data which is over a 20% relative improvement on the previous year's winner.
</summary>
    <author>
      <name>Andrew G. Howard</name>
    </author>
    <link href="http://arxiv.org/abs/1312.5402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5568v1</id>
    <updated>2013-12-19T14:41:29Z</updated>
    <published>2013-12-19T14:41:29Z</published>
    <title>An Adaptive Dictionary Learning Approach for Modeling Dynamical Textures</title>
    <summary>  Video representation is an important and challenging task in the computer
vision community. In this paper, we assume that image frames of a moving scene
can be modeled as a Linear Dynamical System. We propose a sparse coding
framework, named adaptive video dictionary learning (AVDL), to model a video
adaptively. The developed framework is able to capture the dynamics of a moving
scene by exploring both sparse properties and the temporal correlations of
consecutive video frames. The proposed method is compared with state of the art
video processing methods on several benchmark data sequences, which exhibit
appearance changes and heavy occlusions.
</summary>
    <author>
      <name>Xian Wei</name>
    </author>
    <author>
      <name>Hao Shen</name>
    </author>
    <author>
      <name>Martin Kleinsteuber</name>
    </author>
    <link href="http://arxiv.org/abs/1312.5568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5604v2</id>
    <updated>2014-02-06T12:24:54Z</updated>
    <published>2013-12-19T16:01:41Z</published>
    <title>Learning Transformations for Classification Forests</title>
    <summary>  This work introduces a transformation-based learner model for classification
forests. The weak learner at each split node plays a crucial role in a
classification tree. We propose to optimize the splitting objective by learning
a linear transformation on subspaces using nuclear norm as the optimization
criteria. The learned linear transformation restores a low-rank structure for
data from the same class, and, at the same time, maximizes the separation
between different classes, thereby improving the performance of the split
function. Theoretical and experimental results support the proposed framework.
</summary>
    <author>
      <name>Qiang Qiu</name>
    </author>
    <author>
      <name>Guillermo Sapiro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1309.2074</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5604v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5604v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5940v3</id>
    <updated>2014-03-10T18:44:50Z</updated>
    <published>2013-12-20T13:48:20Z</published>
    <title>Generic Deep Networks with Wavelet Scattering</title>
    <summary>  We introduce a two-layer wavelet scattering network, for object
classification. This scattering transform computes a spatial wavelet transform
on the first layer and a new joint wavelet transform along spatial, angular and
scale variables in the second layer. Numerical experiments demonstrate that
this two layer convolution network, which involves no learning and no max
pooling, performs efficiently on complex image data sets such as CalTech, with
structural objects variability and clutter. It opens the possibility to
simplify deep neural network learning by initializing the first layers with
wavelet filters.
</summary>
    <author>
      <name>Edouard Oyallon</name>
    </author>
    <author>
      <name>Stéphane Mallat</name>
    </author>
    <author>
      <name>Laurent Sifre</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop, 3 pages, prepared for ICLR 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5940v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5940v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6370v1</id>
    <updated>2013-12-22T12:08:20Z</updated>
    <published>2013-12-22T12:08:20Z</published>
    <title>An Efficient Edge Detection Technique by Two Dimensional Rectangular
  Cellular Automata</title>
    <summary>  This paper proposes a new pattern of two dimensional cellular automata linear
rules that are used for efficient edge detection of an image. Since cellular
automata is inherently parallel in nature, it has produced desired output
within a unit time interval. We have observed four linear rules among 512 total
linear rules of a rectangular cellular automata in adiabatic or reflexive
boundary condition that produces an optimal result. These four rules are
directly applied once to the images and produced edge detected output. We
compare our results with the existing edge detection algorithms and found that
our results shows better edge detection with an enhancement of edges.
</summary>
    <author>
      <name>Jahangir Mohammed</name>
    </author>
    <author>
      <name>Deepak Ranjan Nayak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.6370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6410v1</id>
    <updated>2013-12-22T18:04:54Z</updated>
    <published>2013-12-22T18:04:54Z</published>
    <title>A Survey on Eye-Gaze Tracking Techniques</title>
    <summary>  Study of eye-movement is being employed in Human Computer Interaction (HCI)
research. Eye - gaze tracking is one of the most challenging problems in the
area of computer vision. The goal of this paper is to present a review of
latest research in this continued growth of remote eye-gaze tracking. This
overview includes the basic definitions and terminologies, recent advances in
the field and finally the need of future development in the field.
</summary>
    <author>
      <name>H. R. Chennamma</name>
    </author>
    <author>
      <name>Xiaohui Yuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, Journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Indian Journal of Computer Science and Engineering, ISSN :
  0976-5166, Vol. 4, No. 5, Oct-Nov 2013, pp. 388-393</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1312.6410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6885v1</id>
    <updated>2013-12-24T20:38:18Z</updated>
    <published>2013-12-24T20:38:18Z</published>
    <title>Deep learning for class-generic object detection</title>
    <summary>  We investigate the use of deep neural networks for the novel task of class
generic object detection. We show that neural networks originally designed for
image recognition can be trained to detect objects within images, regardless of
their class, including objects for which no bounding box labels have been
provided. In addition, we show that bounding box labels yield a 1% performance
increase on the ImageNet recognition challenge.
</summary>
    <author>
      <name>Brody Huval</name>
    </author>
    <author>
      <name>Adam Coates</name>
    </author>
    <author>
      <name>Andrew Ng</name>
    </author>
    <link href="http://arxiv.org/abs/1312.6885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.1190v1</id>
    <updated>2014-01-06T20:25:26Z</updated>
    <published>2014-01-06T20:25:26Z</published>
    <title>Bangla Text Recognition from Video Sequence: A New Focus</title>
    <summary>  Extraction and recognition of Bangla text from video frame images is
challenging due to complex color background, low-resolution etc. In this paper,
we propose an algorithm for extraction and recognition of Bangla text form such
video frames with complex background. Here, a two-step approach has been
proposed. First, the text line is segmented into words using information based
on line contours. First order gradient value of the text blocks are used to
find the word gap. Next, a local binarization technique is applied on each word
and text line is reconstructed using those words. Secondly, this binarized text
block is sent to OCR for recognition purpose.
</summary>
    <author>
      <name>Souvik Bhowmick</name>
    </author>
    <author>
      <name>Purnendu Banerjee</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">NATIONAL CONFERENCE ON COMPUTING AND SYSTEMS (NaCCS), pp.
  62-67,2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.1190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.1190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.1742v1</id>
    <updated>2014-01-08T16:22:09Z</updated>
    <published>2014-01-08T16:22:09Z</published>
    <title>Content Based Image Indexing and Retrieval</title>
    <summary>  In this paper, we present the efficient content based image retrieval systems
which employ the color, texture and shape information of images to facilitate
the retrieval process. For efficient feature extraction, we extract the color,
texture and shape feature of images automatically using edge detection which is
widely used in signal processing and image compression. For facilitated the
speedy retrieval we are implements the antipole-tree algorithm for indexing the
images.
</summary>
    <author>
      <name>Avinash N Bhute</name>
    </author>
    <author>
      <name>B. B. Meshram</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJGIP 2013 Vol 3 issue 4</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.1742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.1742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.2058v1</id>
    <updated>2014-01-09T16:26:54Z</updated>
    <published>2014-01-09T16:26:54Z</published>
    <title>Gesture recognition based mouse events</title>
    <summary>  This paper presents the maneuver of mouse pointer and performs various mouse
operations such as left click, right click, double click, drag etc using
gestures recognition technique. Recognizing gestures is a complex task which
involves many aspects such as motion modeling, motion analysis, pattern
recognition and machine learning. Keeping all the essential factors in mind a
system has been created which recognizes the movement of fingers and various
patterns formed by them. Color caps have been used for fingers to distinguish
it from the background color such as skin color. Thus recognizing the gestures
various mouse events have been performed. The application has been created on
MATLAB environment with operating system as windows 7.
</summary>
    <author>
      <name>Rachit Puri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, IJCSIT</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.2058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.2058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.2686v1</id>
    <updated>2014-01-13T00:19:34Z</updated>
    <published>2014-01-13T00:19:34Z</published>
    <title>A parameterless scale-space approach to find meaningful modes in
  histograms - Application to image and spectrum segmentation</title>
    <summary>  In this paper, we present an algorithm to automatically detect meaningful
modes in a histogram. The proposed method is based on the behavior of local
minima in a scale-space representation. We show that the detection of such
meaningful modes is equivalent in a two classes clustering problem on the
length of minima scale-space curves. The algorithm is easy to implement, fast,
and does not require any parameters. We present several results on histogram
and spectrum segmentation, grayscale image segmentation and color image
reduction.
</summary>
    <author>
      <name>Jérôme Gilles</name>
    </author>
    <author>
      <name>Kathryn Heal</name>
    </author>
    <link href="http://arxiv.org/abs/1401.2686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.2686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.4648v1</id>
    <updated>2014-01-19T09:52:19Z</updated>
    <published>2014-01-19T09:52:19Z</published>
    <title>Visual Tracking using Particle Swarm Optimization</title>
    <summary>  The problem of robust extraction of visual odometry from a sequence of images
obtained by an eye in hand camera configuration is addressed. A novel approach
toward solving planar template based tracking is proposed which performs a
non-linear image alignment for successful retrieval of camera transformations.
In order to obtain global optimum a bio-metaheuristic is used for optimization
of similarity among the planar regions. The proposed method is validated on
image sequences with real as well as synthetic transformations and found to be
resilient to intensity variations. A comparative analysis of the various
similarity measures as well as various state-of-art methods reveal that the
algorithm succeeds in tracking the planar regions robustly and has good
potential to be used in real applications.
</summary>
    <author>
      <name>Rafid Siddiqui</name>
    </author>
    <author>
      <name>Siamak Khatibi</name>
    </author>
    <link href="http://arxiv.org/abs/1401.4648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.4648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.5245v1</id>
    <updated>2014-01-21T09:57:23Z</updated>
    <published>2014-01-21T09:57:23Z</published>
    <title>Edge detection of binary images using the method of masks</title>
    <summary>  In this work the method of masks, creating and using of inverted image masks,
together with binary operation of image data are used in edge detection of
binary images, monochrome images, which yields about 300 times faster than
ordinary methods. The method is divided into three stages: Mask construction,
Fundamental edge detection, and Edge Construction Comparison with an ordinary
method and a fuzzy based method is carried out.
</summary>
    <author>
      <name>Ayman M Bahaa-Eldeen</name>
    </author>
    <author>
      <name>Abdel-Moneim A. Wahdan</name>
    </author>
    <author>
      <name>Hani M. K. Mahdi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ain Shams University, Faculty of Engineering Scientific Bulletin,
  Volume 35, Issue 3, pp 349-355, (2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.5245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.5245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.5891v1</id>
    <updated>2014-01-23T08:33:44Z</updated>
    <published>2014-01-23T08:33:44Z</published>
    <title>Hierarchical pixel clustering for image segmentation</title>
    <summary>  In the paper a piecewise constant image approximations of sequential number
of pixel clusters or segments are treated. A majorizing of optimal
approximation sequence by hierarchical sequence of image approximations is
studied. Transition from pixel clustering to image segmentation by reducing of
segment numbers in clusters is provided. Algorithms are proved by elementary
formulas.
</summary>
    <author>
      <name>M. Kharinov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, 4 formulas, submitted to the 12 International
  Conference on Pattern Recognition and Information Processing May 28-30, 2014,
  Minsk, Belarus</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of the 12th International Conference on Pattern Recognition
  and Information Processing (PRIP'2014), May 28-30, 2014, Minsk, Belarus,
  pp.103-107</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.5891v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.5891v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.6126v1</id>
    <updated>2013-12-19T08:17:24Z</updated>
    <published>2013-12-19T08:17:24Z</published>
    <title>Delegating Custom Object Detection Tasks to a Universal Classification
  System</title>
    <summary>  In this paper, a concept of multipurpose object detection system, recently
introduced in our previous work, is clarified. The business aspect of this
method is transformation of a classifier into an object detector/locator via an
image grid. This is a universal framework for locating objects of interest
through classification. The framework standardizes and simplifies
implementation of custom systems by doing only a custom analysis of the
classification results on the image grid.
</summary>
    <author>
      <name>Andrew Gleibman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, 6 refs. arXiv admin note: substantial text
  overlap with arXiv:1310.7170</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.6126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.6126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10; I.4.7; I.4.8; I.4.9; I.5; I.5.2; I.5.4; I.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.7486v1</id>
    <updated>2014-01-29T12:27:39Z</updated>
    <published>2014-01-29T12:27:39Z</published>
    <title>Use HMM and KNN for classifying corneal data</title>
    <summary>  These days to gain classification system with high accuracy that can classify
complicated pattern are so useful in medicine and industry. In this article a
process for getting the best classifier for Lasik data is suggested. However at
first it's been tried to find the best line and curve by this classifier in
order to gain classifier fitting, and in the end by using the Markov method a
classifier for topographies is gained.
</summary>
    <author>
      <name>Payam Porkar Rezaeiye</name>
    </author>
    <author>
      <name>mehrnoosh bazrafkan</name>
    </author>
    <author>
      <name>ali akbar movassagh</name>
    </author>
    <author>
      <name>Mojtaba Sedigh Fazli</name>
    </author>
    <author>
      <name>Gholam hossein bazyari</name>
    </author>
    <link href="http://arxiv.org/abs/1401.7486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.7486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.0936v2</id>
    <updated>2014-03-27T07:12:28Z</updated>
    <published>2014-02-05T05:31:59Z</published>
    <title>An Optimization Method For Slice Interpolation Of Medical Images</title>
    <summary>  Slice interpolation is a fast growing field in medical image processing.
Intensity-based interpolation and object-based interpolation are two major
groups of methods in the literature. In this paper, we describe an
object-oriented, optimization method based on a modified version of
curvature-based image registration, in which a displacement field is computed
for the missing slice between two known slices and used to interpolate the
intensities of the missing slice. The proposed approach is evaluated
quantitatively by using the Mean Squared Difference (MSD) as a metric. The
produced results also show visual improvement in preserving sharp edges in
images.
</summary>
    <author>
      <name>Ahmadreza Baghaie</name>
    </author>
    <author>
      <name>Zeyun Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.0936v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.0936v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.1151v1</id>
    <updated>2014-02-05T20:18:26Z</updated>
    <published>2014-02-05T20:18:26Z</published>
    <title>Image Acquisition in an Underwater Vision System with NIR and VIS
  Illumination</title>
    <summary>  The paper describes the image acquisition system able to capture images in
two separated bands of light, used to underwater autonomous navigation. The
channels are: the visible light spectrum and near infrared spectrum. The
characteristics of natural, underwater environment were also described together
with the process of the underwater image creation. The results of an experiment
with comparison of selected images acquired in these channels are discussed.
</summary>
    <author>
      <name>Wojciech Biegański</name>
    </author>
    <author>
      <name>Andrzej Kasiński</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Science &amp; Information Technology, Volume 4, Number 1,
  2014, pp. 215-224</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.1151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.1331v1</id>
    <updated>2014-02-06T11:58:42Z</updated>
    <published>2014-02-06T11:58:42Z</published>
    <title>An Estimation Method of Measuring Image Quality for Compressed Images of
  Human Face</title>
    <summary>  Nowadays digital image compression and decompression techniques are very much
important. So our aim is to calculate the quality of face and other regions of
the compressed image with respect to the original image. Image segmentation is
typically used to locate objects and boundaries (lines, curves etc.)in images.
After segmentation the image is changed into something which is more meaningful
to analyze. Using Universal Image Quality Index(Q),Structural Similarity
Index(SSIM) and Gradient-based Structural Similarity Index(G-SSIM) it can be
shown that face region is less compressed than any other region of the image.
</summary>
    <author>
      <name>Abhishek Bhattacharya</name>
    </author>
    <author>
      <name>Tanusree Chatterjee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V7P144</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V7P144" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.1331v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1331v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.1359v1</id>
    <updated>2014-02-06T14:09:25Z</updated>
    <published>2014-02-06T14:09:25Z</published>
    <title>Real-time Pedestrian Surveillance with Top View Cumulative Grids</title>
    <summary>  This manuscript presents an efficient approach to map pedestrian surveillance
footage to an aerial view for global assessment of features. The analysis of
the footages relies on low level computer vision and enable real-time
surveillance. While we neglect object tracking, we introduce cumulative grids
on top view scene flow visualization to highlight situations of interest in the
footage. Our approach is tested on multiview footage both from RGB cameras and,
for the first time in the field, on RGB-D-sensors.
</summary>
    <author>
      <name>Kai Berger</name>
    </author>
    <author>
      <name>Jeyarajan Thiyagalingam</name>
    </author>
    <link href="http://arxiv.org/abs/1402.1359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2013v1</id>
    <updated>2014-02-10T01:22:35Z</updated>
    <published>2014-02-10T01:22:35Z</published>
    <title>Foreground segmentation based on multi-resolution and matting</title>
    <summary>  We propose a foreground segmentation algorithm that does foreground
extraction under different scales and refines the result by matting. First, the
input image is filtered and resampled to 5 different resolutions. Then each of
them is segmented by adaptive figure-ground classification and the best
segmentation is automatically selected by an evaluation score that maximizes
the difference between foreground and background. This segmentation is
upsampled to the original size, and a corresponding trimap is built.
Closed-form matting is employed to label the boundary region, and the result is
refined by a final figure-ground classification. Experiments show the success
of our method in treating challenging images with cluttered background and
adapting to loose initial bounding-box.
</summary>
    <author>
      <name>Xintong Yu</name>
    </author>
    <author>
      <name>Xiaohan Liu</name>
    </author>
    <author>
      <name>Yisong Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages. 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.2013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2188v1</id>
    <updated>2014-02-10T15:41:48Z</updated>
    <published>2014-02-10T15:41:48Z</published>
    <title>Handwritten Character Recognition In Malayalam Scripts- A Review</title>
    <summary>  Handwritten character recognition is one of the most challenging and ongoing
areas of research in the field of pattern recognition. HCR research is matured
for foreign languages like Chinese and Japanese but the problem is much more
complex for Indian languages. The problem becomes even more complicated for
South Indian languages due to its large character set and the presence of
vowels modifiers and compound characters. This paper provides an overview of
important contributions and advances in offline as well as online handwritten
character recognition of Malayalam scripts.
</summary>
    <author>
      <name>Anitha Mary M. O. Chacko</name>
    </author>
    <author>
      <name>P. M Dhanya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages,4 figures,2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol. 5, No. 1, January 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.2188v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2188v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.2426v1</id>
    <updated>2014-02-11T10:26:31Z</updated>
    <published>2014-02-11T10:26:31Z</published>
    <title>Imaging with Rays: Microscopy, Medical Imaging, and Computer Vision</title>
    <summary>  In this paper we broadly consider techniques which utilize projections on
rays for data collection, with particular emphasis on optical techniques. We
formulate a variety of imaging techniques as either special cases or extensions
of tomographic reconstruction. We then consider how the techniques must be
extended to describe objects containing occlusion, as with a self-occluding
opaque object. We formulate the reconstruction problem as a regularized
nonlinear optimization problem to simultaneously solve for object brightness
and attenuation, where the attenuation can become infinite. We demonstrate
various simulated examples for imaging opaque objects, including sparse point
sources, a conventional multiview reconstruction technique, and a
super-resolving technique which exploits occlusion to resolve an image.
</summary>
    <author>
      <name>Keith Dillon</name>
    </author>
    <author>
      <name>Yeshaiahu Fainman</name>
    </author>
    <link href="http://arxiv.org/abs/1402.2426v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.2426v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.3869v2</id>
    <updated>2014-05-16T03:24:09Z</updated>
    <published>2014-02-17T02:13:30Z</published>
    <title>FTVd is beyond Fast Total Variation regularized Deconvolution</title>
    <summary>  In this paper, we revisit the "FTVd" algorithm for Fast Total Variation
Regularized Deconvolution, which has been widely used in the past few years.
Both its original version implemented in the MATLAB software FTVd 3.0 and its
related variant implemented in the latter version FTVd 4.0 are considered
\cite{Wang08FTVdsoftware}. We propose that the intermediate results during the
iterations are the solutions of a series of combined Tikhonov and total
variation regularized image deconvolution models and therefore some of them
often have even better image quality than the final solution, which is
corresponding to the pure total variation regularized model.
</summary>
    <author>
      <name>Yilun Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1402.3869v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.3869v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; G.4; I.4.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.6416v1</id>
    <updated>2014-02-26T05:37:41Z</updated>
    <published>2014-02-26T05:37:41Z</published>
    <title>Deconstruction of compound objects from image sets</title>
    <summary>  We propose a method to recover the structure of a compound object from
multiple silhouettes. Structure is expressed as a collection of 3D primitives
chosen from a pre-defined library, each with an associated pose. This has
several advantages over a volume or mesh representation both for estimation and
the utility of the recovered model. The main challenge in recovering such a
model is the combinatorial number of possible arrangements of parts. We address
this issue by exploiting the sparse nature of the problem, and show that our
method scales to objects constructed from large libraries of parts.
</summary>
    <author>
      <name>Anton van den Hengel</name>
    </author>
    <author>
      <name>John Bastian</name>
    </author>
    <author>
      <name>Anthony Dick</name>
    </author>
    <author>
      <name>Lachlan Fleming</name>
    </author>
    <link href="http://arxiv.org/abs/1402.6416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.6416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.0087v1</id>
    <updated>2014-03-01T14:08:22Z</updated>
    <published>2014-03-01T14:08:22Z</published>
    <title>Temporal Image Fusion</title>
    <summary>  This paper introduces temporal image fusion. The proposed technique builds
upon previous research in exposure fusion and expands it to deal with the
limited Temporal Dynamic Range of existing sensors and camera technologies. In
particular, temporal image fusion enables the rendering of long-exposure
effects on full frame-rate video, as well as the generation of arbitrarily long
exposures from a sequence of images of the same scene taken over time. We
explore the problem of temporal under-exposure, and show how it can be
addressed by selectively enhancing dynamic structure. Finally, we show that the
use of temporal image fusion together with content-selective image filters can
produce a range of striking visual effects on a given input sequence.
</summary>
    <author>
      <name>Francisco J. Estrada</name>
    </author>
    <link href="http://arxiv.org/abs/1403.0087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.0087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.0728v1</id>
    <updated>2014-03-04T09:52:13Z</updated>
    <published>2014-03-04T09:52:13Z</published>
    <title>A Novel Method for Vectorization</title>
    <summary>  Vectorization of images is a key concern uniting computer graphics and
computer vision communities. In this paper we are presenting a novel idea for
efficient, customizable vectorization of raster images, based on Catmull Rom
spline fitting. The algorithm maintains a good balance between photo-realism
and photo abstraction, and hence is applicable to applications with artistic
concerns or applications where less information loss is crucial. The resulting
algorithm is fast, parallelizable and can satisfy general soft realtime
requirements. Moreover, the smoothness of the vectorized images aesthetically
outperforms outputs of many polygon-based methods
</summary>
    <author>
      <name>Tolga Birdal</name>
    </author>
    <author>
      <name>Emrah Bala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Prepared in Siggraph format, not published in a conference, 7 pages,
  9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.0728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.0728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.1687v1</id>
    <updated>2014-03-07T08:57:12Z</updated>
    <published>2014-03-07T08:57:12Z</published>
    <title>Rigid-Motion Scattering for Texture Classification</title>
    <summary>  A rigid-motion scattering computes adaptive invariants along translations and
rotations, with a deep convolutional network. Convolutions are calculated on
the rigid-motion group, with wavelets defined on the translation and rotation
variables. It preserves joint rotation and translation information, while
providing global invariants at any desired scale. Texture classification is
studied, through the characterization of stationary processes from a single
realization. State-of-the-art results are obtained on multiple texture data
bases, with important rotation and scaling variabilities.
</summary>
    <author>
      <name>Laurent SIfre</name>
    </author>
    <author>
      <name>Stéphane Mallat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, submitted to International Journal of Computer Vision</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.1687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.1687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.2482v1</id>
    <updated>2014-03-11T06:48:58Z</updated>
    <published>2014-03-11T06:48:58Z</published>
    <title>Removing Mixture of Gaussian and Impulse Noise by Patch-Based Weighted
  Means</title>
    <summary>  We first establish a law of large numbers and a convergence theorem in
distribution to show the rate of convergence of the non-local means filter for
removing Gaussian noise. We then introduce the notion of degree of similarity
to measure the role of similarity for the non-local means filter. Based on the
convergence theorems, we propose a patch-based weighted means filter for
removing impulse noise and its mixture with Gaussian noise by combining the
essential idea of the trilateral filter and that of the non-local means filter.
Our experiments show that our filter is competitive compared to recently
proposed methods.
</summary>
    <author>
      <name>Haijuan Hu</name>
    </author>
    <author>
      <name>Bing Li</name>
    </author>
    <author>
      <name>Quansheng Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.2482v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.2482v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.3021v1</id>
    <updated>2014-03-12T16:44:20Z</updated>
    <published>2014-03-12T16:44:20Z</published>
    <title>Image reconstruction from limited range projections using orthogonal
  moments</title>
    <summary>  A set of orthonormal polynomials is proposed for image reconstruction from
projection data. The relationship between the projection moments and image
moments is discussed in detail, and some interesting properties are
demonstrated. Simulation results are provided to validate the method and to
compare its performance with previous works.
</summary>
    <author>
      <name>Huazhong Shu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRIBS, LIST</arxiv:affiliation>
    </author>
    <author>
      <name>Jian Zhou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRIBS, LTSI</arxiv:affiliation>
    </author>
    <author>
      <name>Guo-Niu Han</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRMA</arxiv:affiliation>
    </author>
    <author>
      <name>Limin M. Luo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRIBS, LIST</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Louis Coatrieux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRIBS, LTSI</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patcog.2006.05.035</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patcog.2006.05.035" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pattern Recognition 40, 2 (2007) 670-680</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1403.3021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.3021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.3683v1</id>
    <updated>2014-03-14T19:45:56Z</updated>
    <published>2014-03-14T19:45:56Z</published>
    <title>Removal and Contraction Operations in $n$D Generalized Maps for
  Efficient Homology Computation</title>
    <summary>  In this paper, we show that contraction operations preserve the homology of
$n$D generalized maps, under some conditions. Removal and contraction
operations are used to propose an efficient algorithm that compute homology
generators of $n$D generalized maps. Its principle consists in simplifying a
generalized map as much as possible by using removal and contraction
operations. We obtain a generalized map having the same homology than the
initial one, while the number of cells decreased significantly.
  Keywords: $n$D Generalized Maps; Cellular Homology; Homology Generators;
Contraction and Removal Operations.
</summary>
    <author>
      <name>Guillaume Damiand</name>
    </author>
    <author>
      <name>Rocio Gonzalez-Diaz</name>
    </author>
    <author>
      <name>Samuel Peltier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Research report</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.3683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.3683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.8067v2</id>
    <updated>2014-04-20T16:31:55Z</updated>
    <published>2014-03-31T16:09:27Z</published>
    <title>Robust Subspace Recovery via Bi-Sparsity Pursuit</title>
    <summary>  Successful applications of sparse models in computer vision and machine
learning imply that in many real-world applications, high dimensional data is
distributed in a union of low dimensional subspaces. Nevertheless, the
underlying structure may be affected by sparse errors and/or outliers. In this
paper, we propose a bi-sparse model as a framework to analyze this problem and
provide a novel algorithm to recover the union of subspaces in presence of
sparse corruptions. We further show the effectiveness of our method by
experiments on both synthetic data and real-world vision data.
</summary>
    <author>
      <name>Xiao Bian</name>
    </author>
    <author>
      <name>Hamid Krim</name>
    </author>
    <link href="http://arxiv.org/abs/1403.8067v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.8067v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.0566v1</id>
    <updated>2014-02-17T09:42:37Z</updated>
    <published>2014-02-17T09:42:37Z</published>
    <title>Weyl group orbit functions in image processing</title>
    <summary>  We deal with the Fourier-like analysis of functions on discrete grids in
two-dimensional simplexes using $C-$ and $E-$ Weyl group orbit functions. For
these cases we present the convolution theorem. We provide an example of
application of image processing using the $C-$ functions and the convolutions
for spatial filtering of the treated image.
</summary>
    <author>
      <name>Goce Chadzitaskos</name>
    </author>
    <author>
      <name>Lenka Háková</name>
    </author>
    <author>
      <name>Ondřej Kajínek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4236/am.2014.53049.</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4236/am.2014.53049." rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Applied Mathematics, Vol. 5 No. 3, 2014, pp. 501-511</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.0566v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.0566v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1292v1</id>
    <updated>2014-03-20T19:47:58Z</updated>
    <published>2014-03-20T19:47:58Z</published>
    <title>Review of Face Detection Systems Based Artificial Neural Networks
  Algorithms</title>
    <summary>  Face detection is one of the most relevant applications of image processing
and biometric systems. Artificial neural networks (ANN) have been used in the
field of image processing and pattern recognition. There is lack of literature
surveys which give overview about the studies and researches related to the
using of ANN in face detection. Therefore, this research includes a general
review of face detection studies and systems which based on different ANN
approaches and algorithms. The strengths and limitations of these literature
studies and systems were included also.
</summary>
    <author>
      <name>Omaima N. A. AL-Allaf</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijma.2013.6101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijma.2013.6101" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 12 figures, 1 table, IJMA Journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The International Journal of Multimedia &amp; Its Applications (IJMA)
  Vol.6, No.1, February 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.1292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1869v1</id>
    <updated>2014-04-07T18:08:56Z</updated>
    <published>2014-04-07T18:08:56Z</published>
    <title>DenseNet: Implementing Efficient ConvNet Descriptor Pyramids</title>
    <summary>  Convolutional Neural Networks (CNNs) can provide accurate object
classification. They can be extended to perform object detection by iterating
over dense or selected proposed object regions. However, the runtime of such
detectors scales as the total number and/or area of regions to examine per
image, and training such detectors may be prohibitively slow. However, for some
CNN classifier topologies, it is possible to share significant work among
overlapping regions to be classified. This paper presents DenseNet, an open
source system that computes dense, multiscale features from the convolutional
layers of a CNN based object classifier. Future work will involve training
efficient object detectors with DenseNet feature descriptors.
</summary>
    <author>
      <name>Forrest Iandola</name>
    </author>
    <author>
      <name>Matt Moskewicz</name>
    </author>
    <author>
      <name>Sergey Karayev</name>
    </author>
    <author>
      <name>Ross Girshick</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Kurt Keutzer</name>
    </author>
    <link href="http://arxiv.org/abs/1404.1869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.3184v1</id>
    <updated>2014-04-11T18:50:34Z</updated>
    <published>2014-04-11T18:50:34Z</published>
    <title>Decreasing Weighted Sorted $\ell_1$ Regularization</title>
    <summary>  We consider a new family of regularizers, termed {\it weighted sorted
$\ell_1$ norms} (WSL1), which generalizes the recently introduced {\it
octagonal shrinkage and clustering algorithm for regression} (OSCAR) and also
contains the $\ell_1$ and $\ell_{\infty}$ norms as particular instances. We
focus on a special case of the WSL1, the {\sl decreasing WSL1} (DWSL1), where
the elements of the argument vector are sorted in non-increasing order and the
weights are also non-increasing. In this paper, after showing that the DWSL1 is
indeed a norm, we derive two key tools for its use as a regularizer: the dual
norm and the Moreau proximity operator.
</summary>
    <author>
      <name>Xiangrong Zeng</name>
    </author>
    <author>
      <name>Mário A. T. Figueiredo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.3184v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.3184v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.3538v2</id>
    <updated>2014-04-30T09:53:00Z</updated>
    <published>2014-04-14T11:01:04Z</published>
    <title>Proceedings of The 38th Annual Workshop of the Austrian Association for
  Pattern Recognition (ÖAGM), 2014</title>
    <summary>  The 38th Annual Workshop of the Austrian Association for Pattern Recognition
(\"OAGM) will be held at IST Austria, on May 22-23, 2014. The workshop provides
a platform for researchers and industry to discuss traditional and new areas of
computer vision. This year the main topic is: Pattern Recognition:
interdisciplinary challenges and opportunities.
</summary>
    <author>
      <name>Vladimir Kolmogorov</name>
    </author>
    <author>
      <name>Christoph Lampert</name>
    </author>
    <author>
      <name>Emilie Morvant</name>
    </author>
    <author>
      <name>Rustem Takhanov</name>
    </author>
    <link href="http://arxiv.org/abs/1404.3538v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.3538v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.3991v1</id>
    <updated>2014-04-15T17:12:40Z</updated>
    <published>2014-04-15T17:12:40Z</published>
    <title>Spiralet Sparse Representation</title>
    <summary>  This is the first report on Working Paper WP-RFM-14-01. The potential and
capability of sparse representations is well-known. However, their
(multivariate variable) vectorial form, which is completely fine in many fields
and disciplines, results in removal and filtering of important "spatial"
relations that are implicitly carried by two-dimensional [or multi-dimensional]
objects, such as images. In this paper, a new approach, called spiralet sparse
representation, is proposed in order to develop an augmented representation and
therefore a modified sparse representation and theory, which is capable to
preserve the data associated to the spatial relations.
</summary>
    <author>
      <name>Reza Farrahi Moghaddam</name>
    </author>
    <author>
      <name>Mohamed Cheriet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, Working Paper Number: WP-RFM-14-01</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.3991v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.3991v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.4316v1</id>
    <updated>2014-04-16T17:23:47Z</updated>
    <published>2014-04-16T17:23:47Z</published>
    <title>Generic Object Detection With Dense Neural Patterns and Regionlets</title>
    <summary>  This paper addresses the challenge of establishing a bridge between deep
convolutional neural networks and conventional object detection frameworks for
accurate and efficient generic object detection. We introduce Dense Neural
Patterns, short for DNPs, which are dense local features derived from
discriminatively trained deep convolutional neural networks. DNPs can be easily
plugged into conventional detection frameworks in the same way as other dense
local features(like HOG or LBP). The effectiveness of the proposed approach is
demonstrated with the Regionlets object detection framework. It achieved 46.1%
mean average precision on the PASCAL VOC 2007 dataset, and 44.1% on the PASCAL
VOC 2010 dataset, which dramatically improves the original Regionlets approach
without DNPs.
</summary>
    <author>
      <name>Will Y. Zou</name>
    </author>
    <author>
      <name>Xiaoyu Wang</name>
    </author>
    <author>
      <name>Miao Sun</name>
    </author>
    <author>
      <name>Yuanqing Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1404.4316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.4316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.4661v1</id>
    <updated>2014-04-17T22:09:16Z</updated>
    <published>2014-04-17T22:09:16Z</published>
    <title>Learning Fine-grained Image Similarity with Deep Ranking</title>
    <summary>  Learning fine-grained image similarity is a challenging task. It needs to
capture between-class and within-class image differences. This paper proposes a
deep ranking model that employs deep learning techniques to learn similarity
metric directly from images.It has higher learning capability than models based
on hand-crafted features. A novel multiscale network structure has been
developed to describe the images effectively. An efficient triplet sampling
algorithm is proposed to learn the model with distributed asynchronized
stochastic gradient. Extensive experiments show that the proposed algorithm
outperforms models based on hand-crafted visual features and deep
classification models.
</summary>
    <author>
      <name>Jiang Wang</name>
    </author>
    <author>
      <name>Yang song</name>
    </author>
    <author>
      <name>Thomas Leung</name>
    </author>
    <author>
      <name>Chuck Rosenberg</name>
    </author>
    <author>
      <name>Jinbin Wang</name>
    </author>
    <author>
      <name>James Philbin</name>
    </author>
    <author>
      <name>Bo Chen</name>
    </author>
    <author>
      <name>Ying Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.4661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.4661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.6071v1</id>
    <updated>2014-04-24T10:09:41Z</updated>
    <published>2014-04-24T10:09:41Z</published>
    <title>Rough Clustering Based Unsupervised Image Change Detection</title>
    <summary>  This paper introduces an unsupervised technique to detect the changed region
of multitemporal images on a same reference plane with the help of rough
clustering. The proposed technique is a soft-computing approach, based on the
concept of rough set with rough clustering and Pawlak's accuracy. It is less
noisy and avoids pre-deterministic knowledge about the distribution of the
changed and unchanged regions. To show the effectiveness, the proposed
technique is compared with some other approaches.
</summary>
    <author>
      <name>Chandranath Adak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. IEEE Conf. #30853, International Conference on Human Computer
  Interactions (ICHCI'13), Chennai, India, 23-24 Aug., 2013. (In Press)</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.6071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.6071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.6075v1</id>
    <updated>2014-04-24T10:24:49Z</updated>
    <published>2014-04-24T10:24:49Z</published>
    <title>Unsupervised Text Extraction from G-Maps</title>
    <summary>  This paper represents an text extraction method from Google maps, GIS
maps/images. Due to an unsupervised approach there is no requirement of any
prior knowledge or training set about the textual and non-textual parts. Fuzzy
CMeans clustering technique is used for image segmentation and Prewitt method
is used to detect the edges. Connected component analysis and gridding
technique enhance the correctness of the results. The proposed method reaches
98.5% accuracy level on the basis of experimental data sets.
</summary>
    <author>
      <name>Chandranath Adak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICHCI-IEEE.2013.6887782</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICHCI-IEEE.2013.6887782" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. IEEE Conf. #30853, International Conference on Human Computer
  Interactions (ICHCI'13), Chennai, India, 23-24 Aug., 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.6075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.6075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.7594v1</id>
    <updated>2014-04-30T05:37:44Z</updated>
    <published>2014-04-30T05:37:44Z</published>
    <title>Selecting a Small Set of Optimal Gestures from an Extensive Lexicon</title>
    <summary>  Finding the best set of gestures to use for a given computer recognition
problem is an essential part of optimizing the recognition performance while
being mindful to those who may articulate the gestures. An objective function,
called the ellipsoidal distance ratio metric (EDRM), for determining the best
gestures from a larger lexicon library is presented, along with a numerical
method for incorporating subjective preferences. In particular, we demonstrate
an efficient algorithm that chooses the best $n$ gestures from a lexicon of $m$
gestures where typically $n \ll m$ using a weighting of both subjective and
objective measures.
</summary>
    <author>
      <name>Jacob Grosek</name>
    </author>
    <author>
      <name>J. Nathan Kutz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/21060-3722</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/21060-3722" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.7594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.7594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.0921v1</id>
    <updated>2014-04-30T06:30:19Z</updated>
    <published>2014-04-30T06:30:19Z</published>
    <title>Gabor Filter and Rough Clustering Based Edge Detection</title>
    <summary>  This paper introduces an efficient edge detection method based on Gabor
filter and rough clustering. The input image is smoothed by Gabor function, and
the concept of rough clustering is used to focus on edge detection with soft
computational approach. Hysteresis thresholding is used to get the actual
output, i.e. edges of the input image. To show the effectiveness, the proposed
technique is compared with some other edge detection methods.
</summary>
    <author>
      <name>Chandranath Adak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICHCI-IEEE.2013.6887768</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICHCI-IEEE.2013.6887768" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. IEEE Conf. #30853, International Conference on Human Computer
  Interactions (ICHCI'13), Chennai, India, 23-24 Aug., 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.0921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.0921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1020v1</id>
    <updated>2014-03-19T07:32:37Z</updated>
    <published>2014-03-19T07:32:37Z</published>
    <title>Study on performance improvement of oil paint image filter algorithm
  using parallel pattern library</title>
    <summary>  This paper gives a detailed study on the performance of oil paint image
filter algorithm with various parameters applied on an image of RGB model. Oil
Paint image processing, being very performance hungry, current research tries
to find improvement using parallel pattern library. With increasing
kernel-size, the processing time of oil paint image filter algorithm increases
exponentially.
</summary>
    <author>
      <name>Siddhartha Mukherjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures, 4 code snippets, 4 tables, 2 graphs, 2 images of
  experimental result, Conference: CCSEA 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.1020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1717v1</id>
    <updated>2014-05-07T19:39:10Z</updated>
    <published>2014-05-07T19:39:10Z</published>
    <title>Entropy Based Cartoon Texture Separation</title>
    <summary>  Separating an image into cartoon and texture components comes useful in image
processing applications, such as image compression, image segmentation, image
inpainting. Yves Meyer's influential cartoon texture decomposition model
involves deriving an energy functional by choosing appropriate spaces and
functionals. Minimizers of the derived energy functional are cartoon and
texture components of an image. In this study, cartoon part of an image is
separated, by reconstructing it from pixels of multi scale Total-Variation
filtered versions of the original image which is sought to be decomposed into
cartoon and texture parts. An information theoretic pixel by pixel selection
criteria is employed to choose the contributing pixels and their scales.
</summary>
    <author>
      <name>Kutlu Emre Yilmaz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.1717v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1717v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1815v1</id>
    <updated>2014-05-08T06:49:41Z</updated>
    <published>2014-05-08T06:49:41Z</published>
    <title>Implementation And Performance Evaluation Of Background Subtraction
  Algorithms</title>
    <summary>  The study evaluates three background subtraction techniques. The techniques
ranges from very basic algorithm to state of the art published techniques
categorized based on speed, memory requirements and accuracy. Such a review can
effectively guide the designer to select the most suitable method for a given
application in a principled way. The algorithms used in the study ranges from
varying levels of accuracy and computational complexity. Few of them can also
deal with real time challenges like rain, snow, hails, swaying branches,
objects overlapping, varying light intensity or slow moving objects.
</summary>
    <author>
      <name>Deepjoy Das</name>
    </author>
    <author>
      <name>Dr. Sarat Saharia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsa.2014.4206</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsa.2014.4206" rel="related"/>
    <link href="http://arxiv.org/abs/1405.1815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1999v1</id>
    <updated>2014-03-21T07:42:11Z</updated>
    <published>2014-03-21T07:42:11Z</published>
    <title>Model-Driven Applications of Fractional Derivatives and Integrals</title>
    <summary>  Fractional order derivatives and integrals (differintegrals) are viewed from
a frequency-domain perspective using the formalism of Riesz, providing a
computational tool as well as a way to interpret the operations in the
frequency domain. Differintegrals provide a logical extension of current
techniques, generalizing the notion of integral and differential operators and
acting as kind of frequency-domain filtering that has many of the advantages of
a nonlocal linear operator. Several important properties of differintegrals are
presented, and sample applications are given to one- and two-dimensional
signals. Computer code to carry out the computations is made available on the
author's website.
</summary>
    <author>
      <name>William A. Sethares</name>
    </author>
    <author>
      <name>Selçuk Ş. Bayın</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.1999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.2539v1</id>
    <updated>2014-05-11T15:13:56Z</updated>
    <published>2014-05-11T15:13:56Z</published>
    <title>A Review of Image Mosaicing Techniques</title>
    <summary>  Image Mosaicing is a method of constructing multiple images of the same scene
into a larger image. The output of the image mosaic will be the union of two
input images. Image-mosaicing algorithms are used to get mosaiced image. Image
Mosaicing processed is basically divided in to 5 phases. Which includes;
Feature point extraction, Image registration, Homography computation, Warping
and Blending if Image. Various corner detection algorithm is being used for
Feature extraction. This corner produces an efficient and informative output
mosaiced image. Image mosaicing is widely used in creating 3D images, medical
imaging, computer vision, data from satellites, and military automatic target
recognition.
</summary>
    <author>
      <name>Dushyant Vaghela</name>
    </author>
    <author>
      <name>Prof. Kapildev Naina</name>
    </author>
    <link href="http://arxiv.org/abs/1405.2539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.2539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.4389v1</id>
    <updated>2014-05-17T12:33:29Z</updated>
    <published>2014-05-17T12:33:29Z</published>
    <title>Efficient Tracking of a Moving Object using Inter-Frame Coding</title>
    <summary>  Video surveillance has long been in use to monitor security sensitive areas
such as banks, department stores, highways, crowded public places and
borders.The advance in computing power, availability of large-capacity storage
devices and high speed network infrastructure paved the way for cheaper,
multi-sensor video surveillance systems.Traditionally, the video outputs are
processed online by human operators and are usually saved to tapes for later
use only after a forensic event.The increase in the number of cameras in
ordinary surveillance systems overloaded both the human operators and the
storage devices with high volumes of data and made it in-feasible to ensure
proper monitoring of sensitive areas for long times.
</summary>
    <author>
      <name>Shraddha Mehta</name>
    </author>
    <author>
      <name>Vaishali Kalariya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.4389v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4389v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.6135v1</id>
    <updated>2014-02-05T16:14:36Z</updated>
    <published>2014-02-05T16:14:36Z</published>
    <title>Cellular Automata based adaptive resampling technique for the processing
  of remotely sensed imagery</title>
    <summary>  Resampling techniques are being widely used at different stages of satellite
image processing. The existing methodologies cannot perfectly recover features
from a completely under sampled image and hence an intelligent adaptive
resampling methodology is required. We address these issues and adopt an error
metric from the available literature to define interpolation quality. We also
propose a new resampling scheme that adapts itself with regard to the pixel and
texture variation in the image. The proposed CNN based hybrid method has been
found to perform better than the existing methods as it adapts itself with
reference to the image features.
</summary>
    <author>
      <name>S. K. Katiyar</name>
    </author>
    <author>
      <name>P. V. Arun</name>
    </author>
    <link href="http://arxiv.org/abs/1405.6135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7032v1</id>
    <updated>2014-05-27T19:54:35Z</updated>
    <published>2014-05-27T19:54:35Z</published>
    <title>An FPGA-based Parallel Architecture for Face Detection using Mixed Color
  Models</title>
    <summary>  In this paper, a reliable method for detecting human faces in color images is
proposed. This system firstly detects skin color in YCgCr and YIQ color space,
then filters binary texture and the result is morphological processed, finally
converts skin tone to the preferred skin color configured by users in YIQ color
space. The real-time adjusting circuit is implemented and some of simulation
results are given out. Experimental results demonstrate that the method has
achieved high rates and low false positives, another advantage is its
simplicity and minor computational costs.
</summary>
    <author>
      <name>Luo Tao</name>
    </author>
    <author>
      <name>Shi zaifeng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.7032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7626v1</id>
    <updated>2014-05-29T17:41:06Z</updated>
    <published>2014-05-29T17:41:06Z</published>
    <title>Classification of Basmati Rice Grain Variety using Image Processing and
  Principal Component Analysis</title>
    <summary>  All important decisions about the variety of rice grain end product are based
on the different features of rice grain.There are various methods available for
classification of basmati rice. This paper proposed a new principal component
analysis based approach for classification of different variety of basmati
rice. The experimental result shows the effectiveness of the proposed
methodology for various samples of different variety of basmati rice.
</summary>
    <author>
      <name>Rubi Kambo</name>
    </author>
    <author>
      <name>Amit Yerpude</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V11P117</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V11P117" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages from page no:80-85, 8 Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJAREEIE, vol. 2, no. 7, pp. 2893-2900, july 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.7626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.0231v1</id>
    <updated>2014-06-02T01:44:50Z</updated>
    <published>2014-06-02T01:44:50Z</published>
    <title>Ambiguous Proximity Distribution</title>
    <summary>  Proximity Distribution Kernel is an effective method for bag-of-featues based
image representation. In this paper, we investigate the soft assignment of
visual words to image features for proximity distribution. Visual word
contribution function is proposed to model ambiguous proximity distributions.
Three ambiguous proximity distributions is developed by three ambiguous
contribution functions. The experiments are conducted on both classification
and retrieval of medical image data sets. The results show that the performance
of the proposed methods, Proximity Distribution Kernel (PDK), is better or
comparable to the state-of-the-art bag-of-features based image representation
methods.
</summary>
    <author>
      <name>Quanquan Wang</name>
    </author>
    <author>
      <name>Yongping Li</name>
    </author>
    <link href="http://arxiv.org/abs/1406.0231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.0231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.0289v1</id>
    <updated>2014-06-02T08:27:27Z</updated>
    <published>2014-06-02T08:27:27Z</published>
    <title>The constitution of visual perceptual units in the functional
  architecture of V1</title>
    <summary>  Scope of this paper is to consider a mean field neural model which takes into
account the functional neurogeometry of the visual cortex modelled as a group
of rotations and translations. The model generalizes well known results of
Bressloff and Cowan which, in absence of input, accounts for hallucination
patterns. The main result of our study consists in showing that in presence of
a visual input, the eigenmodes of the linearized operator which become stable
represent perceptual units present in the image. The result is strictly related
to dimensionality reduction and clustering problems.
</summary>
    <author>
      <name>Alessandro Sarti</name>
    </author>
    <author>
      <name>Giovanna Citti</name>
    </author>
    <link href="http://arxiv.org/abs/1406.0289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.0289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.0588v2</id>
    <updated>2014-06-05T02:23:21Z</updated>
    <published>2014-06-03T06:32:24Z</published>
    <title>Image retrieval with hierarchical matching pursuit</title>
    <summary>  A novel representation of images for image retrieval is introduced in this
paper, by using a new type of feature with remarkable discriminative power.
Despite the multi-scale nature of objects, most existing models perform feature
extraction on a fixed scale, which will inevitably degrade the performance of
the whole system. Motivated by this, we introduce a hierarchical sparse coding
architecture for image retrieval to explore multi-scale cues. Sparse codes
extracted on lower layers are transmitted to higher layers recursively. With
this mechanism, cues from different scales are fused. Experiments on the
Holidays dataset show that the proposed method achieves an excellent retrieval
performance with a small code length.
</summary>
    <author>
      <name>Shasha Bu</name>
    </author>
    <author>
      <name>Yu-Jin Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.0588v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.0588v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.0909v1</id>
    <updated>2014-06-04T00:03:51Z</updated>
    <published>2014-06-04T00:03:51Z</published>
    <title>Improvement Tracking Dynamic Programming using Replication Function for
  Continuous Sign Language Recognition</title>
    <summary>  In this paper we used a Replication Function (R. F.)for improvement tracking
with dynamic programming. The R. F. transforms values of gray level [0 255] to
[0 1]. The resulting images of R. F. are more striking and visible in skin
regions. The R. F. improves Dynamic Programming (D. P.) in overlapping hand and
face. Results show that Tracking Error Rate 11% and Average Tracked Distance 7%
reduced
</summary>
    <author>
      <name>S. Ildarabadi</name>
    </author>
    <author>
      <name>M. Ebrahimi</name>
    </author>
    <author>
      <name>H. R. Pourreza</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22315381/IJETT-V7P254</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22315381/IJETT-V7P254" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 13 figures, Published with "International Journal of
  Engineering Trends and Technology (IJETT)"</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.0909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.0909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2984v2</id>
    <updated>2014-09-17T22:43:45Z</updated>
    <published>2014-06-11T18:16:29Z</published>
    <title>Joint Training of a Convolutional Network and a Graphical Model for
  Human Pose Estimation</title>
    <summary>  This paper proposes a new hybrid architecture that consists of a deep
Convolutional Network and a Markov Random Field. We show how this architecture
is successfully applied to the challenging problem of articulated human pose
estimation in monocular images. The architecture can exploit structural domain
constraints such as geometric relationships between body joint locations. We
show that joint training of these two model paradigms improves performance and
allows us to significantly outperform existing state-of-the-art techniques.
</summary>
    <author>
      <name>Jonathan Tompson</name>
    </author>
    <author>
      <name>Arjun Jain</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <author>
      <name>Christoph Bregler</name>
    </author>
    <link href="http://arxiv.org/abs/1406.2984v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2984v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.3474v1</id>
    <updated>2014-06-13T10:11:18Z</updated>
    <published>2014-06-13T10:11:18Z</published>
    <title>Heterogeneous Multi-task Learning for Human Pose Estimation with Deep
  Convolutional Neural Network</title>
    <summary>  We propose an heterogeneous multi-task learning framework for human pose
estimation from monocular image with deep convolutional neural network. In
particular, we simultaneously learn a pose-joint regressor and a sliding-window
body-part detector in a deep network architecture. We show that including the
body-part detection task helps to regularize the network, directing it to
converge to a good solution. We report competitive and state-of-art results on
several data sets. We also empirically show that the learned neurons in the
middle layer of our network are tuned to localized body parts.
</summary>
    <author>
      <name>Sijin Li</name>
    </author>
    <author>
      <name>Zhi-Qiang Liu</name>
    </author>
    <author>
      <name>Antoni B. Chan</name>
    </author>
    <link href="http://arxiv.org/abs/1406.3474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.5035v1</id>
    <updated>2014-06-19T13:25:47Z</updated>
    <published>2014-06-19T13:25:47Z</published>
    <title>Why are images smooth?</title>
    <summary>  It is a well observed phenomenon that natural images are smooth, in the sense
that nearby pixels tend to have similar values. We describe a mathematical
model of images that makes no assumptions on the nature of the environment that
images depict. It only assumes that images can be taken at different scales
(zoom levels). We provide quantitative bounds on the smoothness of a typical
image in our model, as a function of the number of available scales. These
bounds can serve as a baseline against which to compare the observed smoothness
of natural images.
</summary>
    <author>
      <name>Uriel Feige</name>
    </author>
    <link href="http://arxiv.org/abs/1406.5035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.5035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.5074v1</id>
    <updated>2014-06-19T15:12:49Z</updated>
    <published>2014-06-19T15:12:49Z</published>
    <title>Robust Outlier Detection Technique in Data Mining: A Univariate Approach</title>
    <summary>  Outliers are the points which are different from or inconsistent with the
rest of the data. They can be novel, new, abnormal, unusual or noisy
information. Outliers are sometimes more interesting than the majority of the
data. The main challenges of outlier detection with the increasing complexity,
size and variety of datasets, are how to catch similar outliers as a group, and
how to evaluate the outliers. This paper describes an approach which uses
Univariate outlier detection as a pre-processing step to detect the outlier and
then applies K-means algorithm hence to analyse the effects of the outliers on
the cluster analysis of dataset.
</summary>
    <author>
      <name>Singh Vijendra</name>
    </author>
    <author>
      <name>Pathak Shivani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1402.6859 by other authors
  without attribution</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.5074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.5074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.5212v1</id>
    <updated>2014-06-19T20:56:08Z</updated>
    <published>2014-06-19T20:56:08Z</published>
    <title>R-CNNs for Pose Estimation and Action Detection</title>
    <summary>  We present convolutional neural networks for the tasks of keypoint (pose)
prediction and action classification of people in unconstrained images. Our
approach involves training an R-CNN detector with loss functions depending on
the task being tackled. We evaluate our method on the challenging PASCAL VOC
dataset and compare it to previous leading approaches. Our method gives
state-of-the-art results for keypoint and action prediction. Additionally, we
introduce a new dataset for action detection, the task of simultaneously
localizing people and classifying their actions, and present results using our
approach.
</summary>
    <author>
      <name>Georgia Gkioxari</name>
    </author>
    <author>
      <name>Bharath Hariharan</name>
    </author>
    <author>
      <name>Ross Girshick</name>
    </author>
    <author>
      <name>Jitendra Malik</name>
    </author>
    <link href="http://arxiv.org/abs/1406.5212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.5212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.5807v1</id>
    <updated>2014-06-23T05:00:31Z</updated>
    <published>2014-06-23T05:00:31Z</published>
    <title>A Unified Quantitative Model of Vision and Audition</title>
    <summary>  We have put forwards a unified quantitative framework of vision and audition,
based on existing data and theories. According to this model, the retina is a
feedforward network self-adaptive to inputs in a specific period. After fully
grown, cells become specialized detectors based on statistics of stimulus
history. This model has provided explanations for perception mechanisms of
colour, shape, depth and motion. Moreover, based on this ground we have put
forwards a bold conjecture that single ear can detect sound direction. This is
complementary to existing theories and has provided better explanations for
sound localization.
</summary>
    <author>
      <name>Peilei Liu</name>
    </author>
    <author>
      <name>Ting Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.5807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.5807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.4; I.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.5947v1</id>
    <updated>2014-06-23T15:34:54Z</updated>
    <published>2014-06-23T15:34:54Z</published>
    <title>Committees of deep feedforward networks trained with few data</title>
    <summary>  Deep convolutional neural networks are known to give good results on image
classification tasks. In this paper we present a method to improve the
classification result by combining multiple such networks in a committee. We
adopt the STL-10 dataset which has very few training examples and show that our
method can achieve results that are better than the state of the art. The
networks are trained layer-wise and no backpropagation is used. We also explore
the effects of dataset augmentation by mirroring, rotation, and scaling.
</summary>
    <author>
      <name>Bogdan Miclut</name>
    </author>
    <author>
      <name>Thomas Kaester</name>
    </author>
    <author>
      <name>Thomas Martinetz</name>
    </author>
    <author>
      <name>Erhardt Barth</name>
    </author>
    <link href="http://arxiv.org/abs/1406.5947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.5947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.6201v1</id>
    <updated>2014-06-24T10:57:50Z</updated>
    <published>2014-06-24T10:57:50Z</published>
    <title>Saccadic Eye Movements and the Generalized Pareto Distribution</title>
    <summary>  We describe a statistical analysis of the eye tracker measurements in a
database with 15 observers viewing 1003 images under free-viewing conditions.
In contrast to the common approach of investigating the properties of the
fixation points we analyze the properties of the transition phases between
fixations. We introduce hyperbolic geometry as a tool to measure the step
length between consecutive eye positions. We show that the step lengths,
measured in hyperbolic and euclidean geometry, follow a generalized Pareto
distribution. The results based on the hyperbolic distance are more robust than
those based on euclidean geometry. We show how the structure of the space of
generalized Pareto distributions can be used to characterize and identify
individual observers.
</summary>
    <author>
      <name>Reiner Lenz</name>
    </author>
    <link href="http://arxiv.org/abs/1406.6201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.6507v1</id>
    <updated>2014-06-25T09:35:40Z</updated>
    <published>2014-06-25T09:35:40Z</published>
    <title>Weakly-supervised Discovery of Visual Pattern Configurations</title>
    <summary>  The increasing prominence of weakly labeled data nurtures a growing demand
for object detection methods that can cope with minimal supervision. We propose
an approach that automatically identifies discriminative configurations of
visual patterns that are characteristic of a given object class. We formulate
the problem as a constrained submodular optimization problem and demonstrate
the benefits of the discovered configurations in remedying mislocalizations and
finding informative positive and negative training examples. Together, these
lead to state-of-the-art weakly-supervised detection results on the challenging
PASCAL VOC dataset.
</summary>
    <author>
      <name>Hyun Oh Song</name>
    </author>
    <author>
      <name>Yong Jae Lee</name>
    </author>
    <author>
      <name>Stefanie Jegelka</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <link href="http://arxiv.org/abs/1406.6507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.6962v2</id>
    <updated>2014-07-22T15:11:02Z</updated>
    <published>2014-06-26T18:00:56Z</published>
    <title>How good are detection proposals, really?</title>
    <summary>  Current top performing Pascal VOC object detectors employ detection proposals
to guide the search for objects thereby avoiding exhaustive sliding window
search across images. Despite the popularity of detection proposals, it is
unclear which trade-offs are made when using them during object detection. We
provide an in depth analysis of ten object proposal methods along with four
baselines regarding ground truth annotation recall (on Pascal VOC 2007 and
ImageNet 2013), repeatability, and impact on DPM detector performance. Our
findings show common weaknesses of existing methods, and provide insights to
choose the most adequate method for different settings.
</summary>
    <author>
      <name>Jan Hosang</name>
    </author>
    <author>
      <name>Rodrigo Benenson</name>
    </author>
    <author>
      <name>Bernt Schiele</name>
    </author>
    <link href="http://arxiv.org/abs/1406.6962v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6962v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.7444v1</id>
    <updated>2014-06-28T21:56:31Z</updated>
    <published>2014-06-28T21:56:31Z</published>
    <title>Learning to Deblur</title>
    <summary>  We describe a learning-based approach to blind image deconvolution. It uses a
deep layered architecture, parts of which are borrowed from recent work on
neural network learning, and parts of which incorporate computations that are
specific to image deconvolution. The system is trained end-to-end on a set of
artificially generated training examples, enabling competitive performance in
blind deconvolution, both with respect to quality and runtime.
</summary>
    <author>
      <name>Christian J. Schuler</name>
    </author>
    <author>
      <name>Michael Hirsch</name>
    </author>
    <author>
      <name>Stefan Harmeling</name>
    </author>
    <author>
      <name>Bernhard Schölkopf</name>
    </author>
    <link href="http://arxiv.org/abs/1406.7444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.7444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0221v1</id>
    <updated>2014-07-01T13:06:00Z</updated>
    <published>2014-07-01T13:06:00Z</published>
    <title>Imaging with Kantorovich-Rubinstein discrepancy</title>
    <summary>  We propose the use of the Kantorovich-Rubinstein norm from optimal transport
in imaging problems. In particular, we discuss a variational regularisation
model endowed with a Kantorovich-Rubinstein discrepancy term and total
variation regularization in the context of image denoising and cartoon-texture
decomposition. We point out connections of this approach to several other
recently proposed methods such as total generalized variation and norms
capturing oscillating patterns. We also show that the respective optimization
problem can be turned into a convex-concave saddle point problem with simple
constraints and hence, can be solved by standard tools. Numerical examples
exhibit interesting features and favourable performance for denoising and
cartoon-texture decomposition.
</summary>
    <author>
      <name>Jan Lellmann</name>
    </author>
    <author>
      <name>Dirk A. Lorenz</name>
    </author>
    <author>
      <name>Carola Schönlieb</name>
    </author>
    <author>
      <name>Tuomo Valkonen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/140975528</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/140975528" rel="related"/>
    <link href="http://arxiv.org/abs/1407.0221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.1610v2</id>
    <updated>2014-09-22T17:49:01Z</updated>
    <published>2014-07-07T08:00:57Z</published>
    <title>Analyzing the Performance of Multilayer Neural Networks for Object
  Recognition</title>
    <summary>  In the last two years, convolutional neural networks (CNNs) have achieved an
impressive suite of results on standard recognition datasets and tasks.
CNN-based features seem poised to quickly replace engineered representations,
such as SIFT and HOG. However, compared to SIFT and HOG, we understand much
less about the nature of the features learned by large CNNs. In this paper, we
experimentally probe several aspects of CNN feature learning in an attempt to
help practitioners gain useful, evidence-backed intuitions about how to apply
CNNs to computer vision problems.
</summary>
    <author>
      <name>Pulkit Agrawal</name>
    </author>
    <author>
      <name>Ross Girshick</name>
    </author>
    <author>
      <name>Jitendra Malik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in European Conference on Computer Vision 2014 (ECCV-2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.1610v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.1610v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.2961v1</id>
    <updated>2014-07-10T20:55:25Z</updated>
    <published>2014-07-10T20:55:25Z</published>
    <title>On the Convergence of the Mean Shift Algorithm in the One-Dimensional
  Space</title>
    <summary>  The mean shift algorithm is a non-parametric and iterative technique that has
been used for finding modes of an estimated probability density function. It
has been successfully employed in many applications in specific areas of
machine vision, pattern recognition, and image processing. Although the mean
shift algorithm has been used in many applications, a rigorous proof of its
convergence is still missing in the literature. In this paper we address the
convergence of the mean shift algorithm in the one-dimensional space and prove
that the sequence generated by the mean shift algorithm is a monotone and
convergent sequence.
</summary>
    <author>
      <name>Youness Aliyari Ghassabeh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patrec.2013.05.004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patrec.2013.05.004" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 10 figures, Published in Pattern Recognition Letters</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pattern Recognition Letters, 2013, vol. 34(12)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.2961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.2961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.3969v1</id>
    <updated>2014-07-15T12:56:35Z</updated>
    <published>2014-07-15T12:56:35Z</published>
    <title>An iterative approach to Hough transform without re-voting</title>
    <summary>  Many bone shapes in the human skeleton are characterized by profiles that can
be associated to equations of algebraic curves. Fixing the parameters in the
curve equation, by means of a classical pattern recognition procedure like the
Hough transform technique, it is then possible to associate an equation to a
specific bone profile. However, most skeleton districts are more accurately
described by piecewise defined curves. This paper utilizes an iterative
approach of the Hough transform without re-voting, to provide an efficient
procedure for describing the profile of a bone in the human skeleton as a
collection of different but continuously attached curves.
</summary>
    <author>
      <name>Giorgio Ricca</name>
    </author>
    <author>
      <name>Mauro C. Beltrametti</name>
    </author>
    <author>
      <name>Anna Maria Massone</name>
    </author>
    <link href="http://arxiv.org/abs/1407.3969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.3969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T45, 68U10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.4739v1</id>
    <updated>2014-07-17T17:10:06Z</updated>
    <published>2014-07-17T17:10:06Z</published>
    <title>An landcover fuzzy logic classification by maximumlikelihood</title>
    <summary>  In present days remote sensing is most used application in many sectors. This
remote sensing uses different images like multispectral, hyper spectral or
ultra spectral. The remote sensing image classification is one of the
significant method to classify image. In this state we classify the maximum
likelihood classification with fuzzy logic. In this we experimenting fuzzy
logic like spatial, spectral texture methods in that different sub methods to
be used for image classification.
</summary>
    <author>
      <name>T. Sarath</name>
    </author>
    <author>
      <name>G. Nagalakshmi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages, 3 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.4739v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.4739v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.4867v2</id>
    <updated>2014-08-14T12:10:04Z</updated>
    <published>2014-07-18T01:45:17Z</published>
    <title>Analysis of Gait Pattern to Recognize the Human Activities</title>
    <summary>  Human activity recognition based on the computer vision is the process of
labelling image sequences with action labels. Accurate systems for this problem
are applied in areas such as visual surveillance, human computer interaction
and video retrieval.
</summary>
    <author>
      <name>Jay Prakash Gupta</name>
    </author>
    <author>
      <name>Pushkar Dixit</name>
    </author>
    <author>
      <name>Nishant Singh</name>
    </author>
    <author>
      <name>Vijay Bhaskar Semwal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to a crucial sign
  error in equation 3</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.4867v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.4867v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.6423v1</id>
    <updated>2014-07-24T01:39:33Z</updated>
    <published>2014-07-24T01:39:33Z</published>
    <title>Performance evaluation of wavelet scattering network in image texture
  classification in various color spaces</title>
    <summary>  Texture plays an important role in many image analysis applications. In this
paper, we give a performance evaluation of color texture classification by
performing wavelet scattering network in various color spaces. Experimental
results on the KTH_TIPS_COL database show that opponent RGB based wavelet
scattering network outperforms other color spaces. Therefore, when dealing with
the problem of color texture classification, opponent RGB based wavelet
scattering network is recommended.
</summary>
    <author>
      <name>Jiasong Wu</name>
    </author>
    <author>
      <name>Longyu Jiang</name>
    </author>
    <author>
      <name>Xu Han</name>
    </author>
    <author>
      <name>Lotfi Senhadji</name>
    </author>
    <author>
      <name>Huazhong Shu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.6423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.6423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.7686v1</id>
    <updated>2014-07-29T10:29:28Z</updated>
    <published>2014-07-29T10:29:28Z</published>
    <title>Hyperspectral Imaging and Analysis for Sparse Reconstruction and
  Recognition</title>
    <summary>  This thesis proposes spatio-spectral techniques for hyperspectral image
analysis. Adaptive spatio-spectral support and variable exposure hyperspectral
imaging is demonstrated to improve spectral reflectance recovery from
hyperspectral images. Novel spectral dimensionality reduction techniques have
been proposed from the perspective of spectral only and spatio-spectral
information preservation. It was found that the joint sparse and joint group
sparse hyperspectral image models achieve lower reconstruction error and higher
recognition accuracy using only a small subset of bands. Hyperspectral image
databases have been developed and made publicly available for further research
in compressed hyperspectral imaging, forensic document analysis and spectral
reflectance recovery.
</summary>
    <author>
      <name>Zohaib Khan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD Thesis, School of Computer Science and Software Engineering, The
  University of Western Australia</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.7686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.7686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.1; I.4.5; I.4.7; I.4.10; I.5.4; I.7.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.8121v1</id>
    <updated>2014-07-30T16:45:00Z</updated>
    <published>2014-07-30T16:45:00Z</published>
    <title>Clustering Approach Towards Image Segmentation: An Analytical Study</title>
    <summary>  Image processing is an important research area in computer vision. Image
segmentation plays the vital rule in image processing research. There exist so
many methods for image segmentation. Clustering is an unsupervised study.
Clustering can also be used for image segmentation. In this paper, an in-depth
study is done on different clustering techniques that can be used for image
segmentation with their pros and cons. An experiment for color image
segmentation based on clustering with K-Means algorithm is performed to observe
the accuracy of clustering technique for the segmentation purpose.
</summary>
    <author>
      <name>Dibya Jyoti Bora</name>
    </author>
    <author>
      <name>Anil Kumar Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Research in Computer Applications and
  Robotics, ISSN 2320-7345, Vol.2, Issue.7, Pg.: 115-124 July 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.8121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.8121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.2938v1</id>
    <updated>2014-08-13T08:27:12Z</updated>
    <published>2014-08-13T08:27:12Z</published>
    <title>Learning Multi-Scale Representations for Material Classification</title>
    <summary>  The recent progress in sparse coding and deep learning has made unsupervised
feature learning methods a strong competitor to hand-crafted descriptors. In
computer vision, success stories of learned features have been predominantly
reported for object recognition tasks. In this paper, we investigate if and how
feature learning can be used for material recognition. We propose two
strategies to incorporate scale information into the learning procedure
resulting in a novel multi-scale coding procedure. Our results show that our
learned features for material recognition outperform hand-crafted descriptors
on the FMD and the KTH-TIPS2 material classification benchmarks.
</summary>
    <author>
      <name>Wenbin Li</name>
    </author>
    <author>
      <name>Mario Fritz</name>
    </author>
    <link href="http://arxiv.org/abs/1408.2938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.2938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.3139v1</id>
    <updated>2014-07-10T21:06:02Z</updated>
    <published>2014-07-10T21:06:02Z</published>
    <title>Real-Time Impulse Noise Suppression from Images Using an Efficient
  Weighted-Average Filtering</title>
    <summary>  In this paper, we propose a method for real-time high density impulse noise
suppression from images. In our method, we first apply an impulse detector to
identify the corrupted pixels and then employ an innovative weighted-average
filter to restore them. The filter takes the nearest neighboring interpolated
image as the initial image and computes the weights according to the relative
positions of the corrupted and uncorrupted pixels. Experimental results show
that the proposed method outperforms the best existing methods in both PSNR
measure and visual quality and is quite suitable for real-time applications.
</summary>
    <author>
      <name>Hossein Hosseini</name>
    </author>
    <author>
      <name>Farzad Hessar</name>
    </author>
    <author>
      <name>Farokh Marvasti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2014.2381649</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2014.2381649" rel="related"/>
    <link href="http://arxiv.org/abs/1408.3139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.3139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.3573v1</id>
    <updated>2014-08-15T15:51:29Z</updated>
    <published>2014-08-15T15:51:29Z</published>
    <title>Turkish Presidential Elections TRT Publicity Speech Facial Expression
  Analysis</title>
    <summary>  In this paper, facial expressions of the three Turkish presidential
candidates Demirtas, Erdogan and Ihsanoglu (in alphabetical order) are analyzed
during the publicity speeches featured at TRT (Turkish Radio and Television) on
03.08.2014. FaceReader is used for the analysis where 3D modeling of the face
is achieved using the active appearance models (AAM). Over 500 landmark points
are tracked and analyzed for obtaining the facial expressions during the whole
speech. All source videos and the data are publicly available for research
purposes.
</summary>
    <author>
      <name>H. Emrah Tasli</name>
    </author>
    <author>
      <name>Paul Ivan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.3573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.3573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.3750v1</id>
    <updated>2014-08-16T17:11:44Z</updated>
    <published>2014-08-16T17:11:44Z</published>
    <title>Real-time emotion recognition for gaming using deep convolutional
  network features</title>
    <summary>  The goal of the present study is to explore the application of deep
convolutional network features to emotion recognition. Results indicate that
they perform similarly to other published models at a best recognition rate of
94.4%, and do so with a single still image rather than a video stream. An
implementation of an affective feedback game is also described, where a
classifier using these features tracks the facial expressions of a player in
real-time.
</summary>
    <author>
      <name>Sébastien Ouellet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures, IEEE style</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.3750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.3750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.5552v1</id>
    <updated>2014-08-24T04:46:58Z</updated>
    <published>2014-08-24T04:46:58Z</published>
    <title>Fuzzy and entropy facial recognition</title>
    <summary>  This paper suggests an effective method for facial recognition using fuzzy
theory and Shannon entropy. Combination of fuzzy theory and Shannon entropy
eliminates the complication of other methods. Shannon entropy calculates the
ratio of an element between faces, and fuzzy theory calculates the member ship
of the entropy with 1. More details will be mentioned in Section 3. The
learning performance is better than others as it is very simple, and only need
two data per learning. By using factors that don't usually change during the
life, the method will have a high accuracy.
</summary>
    <author>
      <name>Jaejun Lee</name>
    </author>
    <author>
      <name>Taeseon Yun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.5552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.6915v1</id>
    <updated>2014-08-29T03:34:52Z</updated>
    <published>2014-08-29T03:34:52Z</published>
    <title>Binary matrices of optimal autocorrelations as alignment marks</title>
    <summary>  We define a new class of binary matrices by maximizing the peak-sidelobe
distances in the aperiodic autocorrelations. These matrices can be used as
robust position marks for in-plane spatial alignment. The optimal square
matrices of dimensions up to 7 by 7 and optimal diagonally-symmetric matrices
of 8 by 8 and 9 by 9 were found by exhaustive searches.
</summary>
    <author>
      <name>Scott A. Skirlo</name>
    </author>
    <author>
      <name>Ling Lu</name>
    </author>
    <author>
      <name>Marin Soljačić</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1116/1.4913316</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1116/1.4913316" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures and 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.6915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.6915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.6963v1</id>
    <updated>2014-08-29T09:37:56Z</updated>
    <published>2014-08-29T09:37:56Z</published>
    <title>Comment on "Ensemble Projection for Semi-supervised Image
  Classification"</title>
    <summary>  In a series of papers by Dai and colleagues [1,2], a feature map (or kernel)
was introduced for semi- and unsupervised learning. This feature map is build
from the output of an ensemble of classifiers trained without using the
ground-truth class labels. In this critique, we analyze the latest version of
this series of papers, which is called Ensemble Projections [2]. We show that
the results reported in [2] were not well conducted, and that Ensemble
Projections performs poorly for semi-supervised learning.
</summary>
    <author>
      <name>Xavier Boix</name>
    </author>
    <author>
      <name>Gemma Roig</name>
    </author>
    <author>
      <name>Luc Van Gool</name>
    </author>
    <link href="http://arxiv.org/abs/1408.6963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.6963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0347v1</id>
    <updated>2014-09-01T09:46:52Z</updated>
    <published>2014-09-01T09:46:52Z</published>
    <title>Multi-tensor Completion for Estimating Missing Values in Video Data</title>
    <summary>  Many tensor-based data completion methods aim to solve image and video
in-painting problems. But, all methods were only developed for a single
dataset. In most of real applications, we can usually obtain more than one
dataset to reflect one phenomenon, and all the datasets are mutually related in
some sense. Thus one question raised whether such the relationship can improve
the performance of data completion or not? In the paper, we proposed a novel
and efficient method by exploiting the relationship among datasets for
multi-video data completion. Numerical results show that the proposed method
significantly improve the performance of video in-painting, particularly in the
case of very high missing percentage.
</summary>
    <author>
      <name>Chao Li</name>
    </author>
    <author>
      <name>Lili Guo</name>
    </author>
    <author>
      <name>Andrzej Cichocki</name>
    </author>
    <link href="http://arxiv.org/abs/1409.0347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0908v1</id>
    <updated>2014-09-02T22:34:29Z</updated>
    <published>2014-09-02T22:34:29Z</published>
    <title>Action Recognition in the Frequency Domain</title>
    <summary>  In this paper, we describe a simple strategy for mitigating variability in
temporal data series by shifting focus onto long-term, frequency domain
features that are less susceptible to variability. We apply this method to the
human action recognition task and demonstrate how working in the frequency
domain can yield good recognition features for commonly used optical flow and
articulated pose features, which are highly sensitive to small differences in
motion, viewpoint, dynamic backgrounds, occlusion and other sources of
variability. We show how these frequency-based features can be used in
combination with a simple forest classifier to achieve good and robust results
on the popular KTH Actions dataset.
</summary>
    <author>
      <name>Anh Tran</name>
    </author>
    <author>
      <name>Jinyan Guan</name>
    </author>
    <author>
      <name>Thanima Pilantanakitti</name>
    </author>
    <author>
      <name>Paul Cohen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: Artificial Intelligence, Computer Vision, Action
  Recognition</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.1789v1</id>
    <updated>2014-09-05T13:36:51Z</updated>
    <published>2014-09-05T13:36:51Z</published>
    <title>Identifying Synapses Using Deep and Wide Multiscale Recursive Networks</title>
    <summary>  In this work, we propose a learning framework for identifying synapses using
a deep and wide multi-scale recursive (DAWMR) network, previously considered in
image segmentation applications. We apply this approach on electron microscopy
data from invertebrate fly brain tissue. By learning features directly from the
data, we are able to achieve considerable improvements over existing techniques
that rely on a small set of hand-designed features. We show that this system
can reduce the amount of manual annotation required, in both acquisition of
training data as well as verification of inferred detections.
</summary>
    <author>
      <name>Gary B. Huang</name>
    </author>
    <author>
      <name>Stephen Plaza</name>
    </author>
    <link href="http://arxiv.org/abs/1409.1789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.1789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.2413v1</id>
    <updated>2014-08-25T03:39:41Z</updated>
    <published>2014-08-25T03:39:41Z</published>
    <title>Image processing</title>
    <summary>  Gabor filters can extract multi-orientation and multiscale features from face
images. Researchers have designed different ways to use the magnitude of the
filtered results for face recognition: Gabor Fisher classifier exploited only
the magnitude information of Gabor magnitude pictures (GMPs); Local Gabor
Binary Pattern uses only the gradient information. In this paper, we regard
GMPs as smooth surfaces. By completely describing the shape of GMPs, we get a
face representation method called Gabor Surface Feature (GSF). First, we
compute the magnitude, 1st and 2nd derivatives of GMPs, then binarize them and
transform them into decimal values. Finally we construct joint histograms and
use subspace methods for classification. Experiments on FERET, ORL and FRGC
1.0.4 database show the effectiveness of GSF.
</summary>
    <author>
      <name>Franco Rino</name>
    </author>
    <link href="http://arxiv.org/abs/1409.2413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.2413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.3024v1</id>
    <updated>2014-09-10T11:18:08Z</updated>
    <published>2014-09-10T11:18:08Z</published>
    <title>One-Dimensional Vector based Pattern Matching</title>
    <summary>  Template matching is a basic method in image analysis to extract useful
information from images. In this paper, we suggest a new method for pattern
matching. Our method transform the template image from two dimensional image
into one dimensional vector. Also all sub-windows (same size of template) in
the reference image will transform into one dimensional vectors. The three
similarity measures SAD, SSD, and Euclidean are used to compute the likeness
between template and all sub-windows in the reference image to find the best
match. The experimental results show the superior performance of the proposed
method over the conventional methods on various template of different sizes.
</summary>
    <author>
      <name>Y. M. Fouda</name>
    </author>
    <link href="http://arxiv.org/abs/1409.3024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.3024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.4205v1</id>
    <updated>2014-09-15T10:53:04Z</updated>
    <published>2014-09-15T10:53:04Z</published>
    <title>Speeding-up Graphical Model Optimization via a Coarse-to-fine Cascade of
  Pruning Classifiers</title>
    <summary>  We propose a general and versatile framework that significantly speeds-up
graphical model optimization while maintaining an excellent solution accuracy.
The proposed approach relies on a multi-scale pruning scheme that is able to
progressively reduce the solution space by use of a novel strategy based on a
coarse-to-fine cascade of learnt classifiers. We thoroughly experiment with
classic computer vision related MRF problems, where our framework constantly
yields a significant time speed-up (with respect to the most efficient
inference methods) and obtains a more accurate solution than directly
optimizing the MRF.
</summary>
    <author>
      <name>B. Conejo</name>
    </author>
    <author>
      <name>N. Komodakis</name>
    </author>
    <author>
      <name>S. Leprince</name>
    </author>
    <author>
      <name>J. P. Avouac</name>
    </author>
    <link href="http://arxiv.org/abs/1409.4205v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.4205v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.4326v2</id>
    <updated>2015-10-20T15:08:48Z</updated>
    <published>2014-09-15T16:54:42Z</published>
    <title>Computing the Stereo Matching Cost with a Convolutional Neural Network</title>
    <summary>  We present a method for extracting depth information from a rectified image
pair. We train a convolutional neural network to predict how well two image
patches match and use it to compute the stereo matching cost. The cost is
refined by cross-based cost aggregation and semiglobal matching, followed by a
left-right consistency check to eliminate errors in the occluded regions. Our
stereo method achieves an error rate of 2.61 % on the KITTI stereo dataset and
is currently (August 2014) the top performing method on this dataset.
</summary>
    <author>
      <name>Jure Žbontar</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CVPR.2015.7298767</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CVPR.2015.7298767" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference on Computer Vision and Pattern Recognition (CVPR), June
  2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.4326v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.4326v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.4689v2</id>
    <updated>2015-05-24T09:16:03Z</updated>
    <published>2014-09-16T16:31:07Z</published>
    <title>Compute Less to Get More: Using ORC to Improve Sparse Filtering</title>
    <summary>  Sparse Filtering is a popular feature learning algorithm for image
classification pipelines. In this paper, we connect the performance of Sparse
Filtering with spectral properties of the corresponding feature matrices. This
connection provides new insights into Sparse Filtering; in particular, it
suggests early stopping of Sparse Filtering. We therefore introduce the Optimal
Roundness Criterion (ORC), a novel stopping criterion for Sparse Filtering. We
show that this stopping criterion is related with pre-processing procedures
such as Statistical Whitening and demonstrate that it can make image
classification with Sparse Filtering considerably faster and more accurate.
</summary>
    <author>
      <name>Johannes Lederer</name>
    </author>
    <author>
      <name>Sergio Guadarrama</name>
    </author>
    <link href="http://arxiv.org/abs/1409.4689v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.4689v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.5114v2</id>
    <updated>2014-10-10T13:23:30Z</updated>
    <published>2014-09-17T19:55:34Z</published>
    <title>A Survey on Heterogeneous Face Recognition: Sketch, Infra-red, 3D and
  Low-resolution</title>
    <summary>  Heterogeneous face recognition (HFR) refers to matching face imagery across
different domains. It has received much interest from the research community as
a result of its profound implications in law enforcement. A wide variety of new
invariant features, cross-modality matching models and heterogeneous datasets
being established in recent years. This survey provides a comprehensive review
of established techniques and recent developments in HFR. Moreover, we offer a
detailed account of datasets and benchmarks commonly used for evaluation. We
finish by assessing the state of the field and discussing promising directions
for future research.
</summary>
    <author>
      <name>Shuxin Ouyang</name>
    </author>
    <author>
      <name>Timothy Hospedales</name>
    </author>
    <author>
      <name>Yi-Zhe Song</name>
    </author>
    <author>
      <name>Xueming Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">survey paper(35 pages)</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.5114v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.5114v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1; I.4.9; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.5230v1</id>
    <updated>2014-09-18T09:10:28Z</updated>
    <published>2014-09-18T09:10:28Z</published>
    <title>Deep Regression for Face Alignment</title>
    <summary>  In this paper, we present a deep regression approach for face alignment. The
deep architecture consists of a global layer and multi-stage local layers. We
apply the back-propagation algorithm with the dropout strategy to jointly
optimize the regression parameters. We show that the resulting deep regressor
gradually and evenly approaches the true facial landmarks stage by stage,
avoiding the tendency to yield over-strong early stage regressors while
over-weak later stage regressors. Experimental results show that our approach
achieves the state-of-the-art
</summary>
    <author>
      <name>Baoguang Shi</name>
    </author>
    <author>
      <name>Xiang Bai</name>
    </author>
    <author>
      <name>Wenyu Liu</name>
    </author>
    <author>
      <name>Jingdong Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1409.5230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.5230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.5957v2</id>
    <updated>2015-02-10T08:24:48Z</updated>
    <published>2014-09-21T09:10:24Z</published>
    <title>A Global Approach for Solving Edge-Matching Puzzles</title>
    <summary>  We consider apictorial edge-matching puzzles, in which the goal is to arrange
a collection of puzzle pieces with colored edges so that the colors match along
the edges of adjacent pieces. We devise an algebraic representation for this
problem and provide conditions under which it exactly characterizes a puzzle.
Using the new representation, we recast the combinatorial, discrete problem of
solving puzzles as a global, polynomial system of equations with continuous
variables. We further propose new algorithms for generating approximate
solutions to the continuous problem by solving a sequence of convex
relaxations.
</summary>
    <author>
      <name>Shahar Z. Kovalsky</name>
    </author>
    <author>
      <name>Daniel Glasner</name>
    </author>
    <author>
      <name>Ronen Basri</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIAM J. Imaging Sciences, Vol. 8, Issue 2, 916--938, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1409.5957v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.5957v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.6745v1</id>
    <updated>2014-09-23T20:25:46Z</updated>
    <published>2014-09-23T20:25:46Z</published>
    <title>A Concept Learning Approach to Multisensory Object Perception</title>
    <summary>  This paper presents a computational model of concept learning using Bayesian
inference for a grammatically structured hypothesis space, and test the model
on multisensory (visual and haptics) recognition of 3D objects. The study is
performed on a set of artificially generated 3D objects known as fribbles,
which are complex, multipart objects with categorical structures. The goal of
this work is to develop a working multisensory representational model that
integrates major themes on concepts and concepts learning from the cognitive
science literature. The model combines the representational power of a
probabilistic generative grammar with the inferential power of Bayesian
induction.
</summary>
    <author>
      <name>Ifeoma Nwogu</name>
    </author>
    <author>
      <name>Goker Erdogan</name>
    </author>
    <author>
      <name>Ilker Yildirim</name>
    </author>
    <author>
      <name>Robert Jacobs</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages and 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.6745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.6745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7307v1</id>
    <updated>2014-09-25T15:52:05Z</updated>
    <published>2014-09-25T15:52:05Z</published>
    <title>Image Classification with A Deep Network Model based on Compressive
  Sensing</title>
    <summary>  To simplify the parameter of the deep learning network, a cascaded
compressive sensing model "CSNet" is implemented for image classification.
Firstly, we use cascaded compressive sensing network to learn feature from the
data. Secondly, CSNet generates the feature by binary hashing and block-wise
histograms. Finally, a linear SVM classifier is used to classify these
features. The experiments on the MNIST dataset indicate that higher
classification accuracy can be obtained by this algorithm.
</summary>
    <author>
      <name>Yufei Gan</name>
    </author>
    <author>
      <name>Tong Zhuo</name>
    </author>
    <author>
      <name>Chu He</name>
    </author>
    <link href="http://arxiv.org/abs/1409.7307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7313v1</id>
    <updated>2014-09-25T16:14:18Z</updated>
    <published>2014-09-25T16:14:18Z</published>
    <title>A Deep Graph Embedding Network Model for Face Recognition</title>
    <summary>  In this paper, we propose a new deep learning network "GENet", it combines
the multi-layer network architec- ture and graph embedding framework. Firstly,
we use simplest unsupervised learning PCA/LDA as first layer to generate the
low- level feature. Secondly, many cascaded dimensionality reduction layers
based on graph embedding framework are applied to GENet. Finally, a linear SVM
classifier is used to classify dimension-reduced features. The experiments
indicate that higher classification accuracy can be obtained by this algorithm
on the CMU-PIE, ORL, Extended Yale B dataset.
</summary>
    <author>
      <name>Yufei Gan</name>
    </author>
    <author>
      <name>Teng Yang</name>
    </author>
    <author>
      <name>Chu He</name>
    </author>
    <link href="http://arxiv.org/abs/1409.7313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7556v3</id>
    <updated>2015-05-26T03:14:19Z</updated>
    <published>2014-09-26T12:36:54Z</published>
    <title>Location Recognition Over Large Time Lags</title>
    <summary>  Would it be possible to automatically associate ancient pictures to modern
ones and create fancy cultural heritage city maps? We introduce here the task
of recognizing the location depicted in an old photo given modern annotated
images collected from the Internet. We present an extensive analysis on
different features, looking for the most discriminative and most robust to the
image variability induced by large time lags. Moreover, we show that the
described task benefits from domain adaptation.
</summary>
    <author>
      <name>Basura Fernando</name>
    </author>
    <author>
      <name>Tatiana Tommasi</name>
    </author>
    <author>
      <name>Tinne Tuytelaars</name>
    </author>
    <link href="http://arxiv.org/abs/1409.7556v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7556v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7963v1</id>
    <updated>2014-09-28T21:32:15Z</updated>
    <published>2014-09-28T21:32:15Z</published>
    <title>MoDeep: A Deep Learning Framework Using Motion Features for Human Pose
  Estimation</title>
    <summary>  In this work, we propose a novel and efficient method for articulated human
pose estimation in videos using a convolutional network architecture, which
incorporates both color and motion features. We propose a new human body pose
dataset, FLIC-motion, that extends the FLIC dataset with additional motion
features. We apply our architecture to this dataset and report significantly
better performance than current state-of-the-art pose detection systems.
</summary>
    <author>
      <name>Arjun Jain</name>
    </author>
    <author>
      <name>Jonathan Tompson</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <author>
      <name>Christoph Bregler</name>
    </author>
    <link href="http://arxiv.org/abs/1409.7963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.0243v1</id>
    <updated>2014-10-01T14:41:55Z</updated>
    <published>2014-10-01T14:41:55Z</published>
    <title>Pattern Encoding on the Poincare Sphere</title>
    <summary>  This paper presents a convenient graphical tool for encoding visual patterns
(such as image patches and image atoms) as point constellations in a space
spanned by perceptual features and with a clear geometrical interpretation.
General theory and a practical pattern encoding scheme are presented, inspired
by encoding polarization states of a light wave on the Poincare sphere. This
new pattern encoding scheme can be useful for many applications in image
processing and computer vision. Here, three possible applications are
illustrated, in clustering perceptually similar patterns, visualizing
properties of learned dictionaries of image atoms and generating new
dictionaries of image atoms from spherical codes.
</summary>
    <author>
      <name>Aleksandra Pizurica</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 23 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.0243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.0243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.3080v1</id>
    <updated>2014-10-12T11:43:37Z</updated>
    <published>2014-10-12T11:43:37Z</published>
    <title>Tree-Structure Bayesian Compressive Sensing for Video</title>
    <summary>  A Bayesian compressive sensing framework is developed for video
reconstruction based on the color coded aperture compressive temporal imaging
(CACTI) system. By exploiting the three dimension (3D) tree structure of the
wavelet and Discrete Cosine Transformation (DCT) coefficients, a Bayesian
compressive sensing inversion algorithm is derived to reconstruct (up to 22)
color video frames from a single monochromatic compressive measurement. Both
simulated and real datasets are adopted to verify the performance of the
proposed algorithm.
</summary>
    <author>
      <name>Xin Yuan</name>
    </author>
    <author>
      <name>Patrick Llull</name>
    </author>
    <author>
      <name>David J. Brady</name>
    </author>
    <author>
      <name>Lawrence Carin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.3080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.3080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.3970v1</id>
    <updated>2014-10-15T08:42:25Z</updated>
    <published>2014-10-15T08:42:25Z</published>
    <title>Shape and Color Object Tracking for Real-Time Robotic Navigation</title>
    <summary>  This paper presents a real-time approach for single-colored ball detection
and tracking. The approach consists of two main phases. In a first offline
calibration phase, the intrinsic parameters of the camera and the radial
distortion are estimated, and a classification of colors is learned from a
sample image of colored balls. The second phase consists of four main steps:
(1) color segmentation of the input image into several regions based on the
offline classification, (2) robust estimation of the circle parameters (3)
refinement of the circle parameters, and (4) ball tracking. The experimental
results showed that the approach presents a good compromise between suitability
for real-time navigation and robustness to occlusions, background congestion
and colors interference in the scene.
</summary>
    <author>
      <name>Haythem Ghazouani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.3970v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.3970v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.4017v1</id>
    <updated>2014-10-15T11:24:55Z</updated>
    <published>2014-10-15T11:24:55Z</published>
    <title>Online Tracking of Skin Colour Regions Against a Complex Background</title>
    <summary>  Online tracking of human activity against a complex background is a
challenging task for many applications. In this paper, we have developed a
robust technique for localizing skin colour regions from unconstrained image
frames. A simple and fast segmentation algorithm is used to train a multiplayer
perceptron (MLP) for detection of skin colours. Stepper motors are synchronized
with the MLP to track the movement of the skin colour regions.
</summary>
    <author>
      <name>Subhadip Basu</name>
    </author>
    <author>
      <name>S. Chakraborty</name>
    </author>
    <author>
      <name>K. Mukherjee</name>
    </author>
    <author>
      <name>S. K. Pandit</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/INDICO.2004.1497734</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/INDICO.2004.1497734" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of IEEE INDICON, pp. 184-186, Dec-2004, Kharagpur</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1410.4017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.4017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.4393v2</id>
    <updated>2014-11-11T07:07:30Z</updated>
    <published>2014-10-16T12:25:50Z</published>
    <title>The HAWKwood Database</title>
    <summary>  We present a database consisting of wood pile images, which can be used as a
benchmark to evaluate the performance of wood pile detection and surveying
algorithms. We distinguish six database cate- gories which can be used for
different types of algorithms. Images of real and synthetic scenes are
provided, which consist of 7655 images divided into 354 data sets. Depending on
the category the data sets either include ground truth data or forestry
specific measurements with which algorithms may be compared.
</summary>
    <author>
      <name>Christopher Herbon</name>
    </author>
    <link href="http://arxiv.org/abs/1410.4393v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.4393v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.4441v1</id>
    <updated>2014-10-16T14:17:21Z</updated>
    <published>2014-10-16T14:17:21Z</published>
    <title>Improve CAPTCHA's Security Using Gaussian Blur Filter</title>
    <summary>  Providing security for webservers against unwanted and automated
registrations has become a big concern. To prevent these kinds of false
registrations many websites use CAPTCHAs. Among all kinds of CAPTCHAs OCR-Based
or visual CAPTCHAs are very common. Actually visual CAPTCHA is an image
containing a sequence of characters. So far most of visual CAPTCHAs, in order
to resist against OCR programs, use some common implementations such as
wrapping the characters, random placement and rotations of characters, etc. In
this paper we applied Gaussian Blur filter, which is an image transformation,
to visual CAPTCHAs to reduce their readability by OCR programs. We concluded
that this technique made CAPTCHAs almost unreadable for OCR programs but, their
readability by human users still remained high.
</summary>
    <author>
      <name>Ariyan Zarei</name>
    </author>
    <link href="http://arxiv.org/abs/1410.4441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.4441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.5358v3</id>
    <updated>2015-09-01T09:25:50Z</updated>
    <published>2014-10-20T17:15:50Z</published>
    <title>Remote sensing image classification exploiting multiple kernel learning</title>
    <summary>  We propose a strategy for land use classification which exploits Multiple
Kernel Learning (MKL) to automatically determine a suitable combination of a
set of features without requiring any heuristic knowledge about the
classification task. We present a novel procedure that allows MKL to achieve
good performance in the case of small training sets. Experimental results on
publicly available datasets demonstrate the feasibility of the proposed
approach.
</summary>
    <author>
      <name>Claudio Cusano</name>
    </author>
    <author>
      <name>Paolo Napoletano</name>
    </author>
    <author>
      <name>Raimondo Schettini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LGRS.2015.2476365</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LGRS.2015.2476365" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication on the IEEE Geoscience and Remote Sensing
  letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.5358v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5358v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.5894v1</id>
    <updated>2014-10-22T01:52:53Z</updated>
    <published>2014-10-22T01:52:53Z</published>
    <title>Vehicle Detection and Tracking Techniques: A Concise Review</title>
    <summary>  Vehicle detection and tracking applications play an important role for
civilian and military applications such as in highway traffic surveillance
control, management and urban traffic planning. Vehicle detection process on
road are used for vehicle tracking, counts, average speed of each individual
vehicle, traffic analysis and vehicle categorizing objectives and may be
implemented under different environments changes. In this review, we present a
concise overview of image processing methods and analysis tools which used in
building these previous mentioned applications that involved developing traffic
surveillance systems. More precisely and in contrast with other reviews, we
classified the processing methods under three categories for more clarification
to explain the traffic systems.
</summary>
    <author>
      <name>Raad Ahmed Hadi</name>
    </author>
    <author>
      <name>Ghazali Sulong</name>
    </author>
    <author>
      <name>Loay Edwar George</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/sipij.2013.5101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/sipij.2013.5101" rel="related"/>
    <link href="http://arxiv.org/abs/1410.5894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.6333v3</id>
    <updated>2017-03-24T14:51:09Z</updated>
    <published>2014-10-23T12:04:31Z</published>
    <title>A Regularization Approach to Blind Deblurring and Denoising of QR
  Barcodes</title>
    <summary>  QR bar codes are prototypical images for which part of the image is a priori
known (required patterns). Open source bar code readers, such as ZBar, are
readily available. We exploit both these facts to provide and assess purely
regularization-based methods for blind deblurring of QR bar codes in the
presence of noise.
</summary>
    <author>
      <name>Yves van Gennip</name>
    </author>
    <author>
      <name>Prashant Athavale</name>
    </author>
    <author>
      <name>Jérôme Gilles</name>
    </author>
    <author>
      <name>Rustum Choksi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2015.2432675</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2015.2432675" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 19 figures (with a total of 57 subfigures), 1 table; v3:
  previously missing reference [35] added</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.6333v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6333v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U10, 65K10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.7265v1</id>
    <updated>2014-10-27T14:58:28Z</updated>
    <published>2014-10-27T14:58:28Z</published>
    <title>An Unsupervised Ensemble-based Markov Random Field Approach to
  Microscope Cell Image Segmentation</title>
    <summary>  In this paper, we propose an approach to the unsupervised segmentation of
images using Markov Random Field. The proposed approach is based on the idea of
Bit Plane Slicing. We use the planes as initial labellings for an ensemble of
segmentations. With pixelwise voting, a robust segmentation approach can be
achieved, which we demonstrate on microscope cell images. We tested our
approach on a publicly available database, where it proven to be competitive
with other methods and manual segmentation.
</summary>
    <author>
      <name>Balint Antal</name>
    </author>
    <author>
      <name>Bence Remenyik</name>
    </author>
    <author>
      <name>Andras Hajdu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5220/0004612900940099</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5220/0004612900940099" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceeingds of the 10th International Conference on Signal
  Processing and Multimedia Applications (SIGMAP 2013), Reykjavik, Iceland,
  2013, pp. 94-99</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1410.7265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.7265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.7730v1</id>
    <updated>2014-10-28T18:55:37Z</updated>
    <published>2014-10-28T18:55:37Z</published>
    <title>New similarity index based on entropy and group theory</title>
    <summary>  In this work, we propose a new similarity index for images considering the
entropy function and group theory. This index considers an algebraic group of
images, it is defined by an inner law that provides a novel approach for the
subtraction of images. Through an equivalence relationship in the field of
images, we prove the existence of the quotient group, on which the new
similarity index is defined. We also present the main properties of the new
index, and the immediate application thereof as a stopping criterion of the
"Mean Shift Iterative Algorithm".
</summary>
    <author>
      <name>Yasel Garcés</name>
    </author>
    <author>
      <name>Esley Torres</name>
    </author>
    <author>
      <name>Osvaldo Pereira</name>
    </author>
    <author>
      <name>Roberto Rodríguez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Spanish</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.7730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.7730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.0791v1</id>
    <updated>2014-11-04T05:53:45Z</updated>
    <published>2014-11-04T05:53:45Z</published>
    <title>A Robust Point Sets Matching Method</title>
    <summary>  Point sets matching method is very important in computer vision, feature
extraction, fingerprint matching, motion estimation and so on. This paper
proposes a robust point sets matching method. We present an iterative algorithm
that is robust to noise case. Firstly, we calculate all transformations between
two points. Then similarity matrix are computed to measure the possibility that
two transformation are both true. We iteratively update the matching score
matrix by using the similarity matrix. By using matching algorithm on graph, we
obtain the matching result. Experimental results obtained by our approach show
robustness to outlier and jitter.
</summary>
    <author>
      <name>Xiao Liu</name>
    </author>
    <author>
      <name>Congying Han</name>
    </author>
    <author>
      <name>Tiande Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.0791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.0791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.1171v1</id>
    <updated>2014-11-05T07:27:08Z</updated>
    <published>2014-11-05T07:27:08Z</published>
    <title>Multilinear Principal Component Analysis Network for Tensor Object
  Classification</title>
    <summary>  The recently proposed principal component analysis network (PCANet) has been
proved high performance for visual content classification. In this letter, we
develop a tensorial extension of PCANet, namely, multilinear principal analysis
component network (MPCANet), for tensor object classification. Compared to
PCANet, the proposed MPCANet uses the spatial structure and the relationship
between each dimension of tensor objects much more efficiently. Experiments
were conducted on different visual content datasets including UCF sports action
video sequences database and UCF11 database. The experimental results have
revealed that the proposed MPCANet achieves higher classification accuracy than
PCANet for tensor object classification.
</summary>
    <author>
      <name>Rui Zeng</name>
    </author>
    <author>
      <name>Jiasong Wu</name>
    </author>
    <author>
      <name>Zhuhong Shao</name>
    </author>
    <author>
      <name>Lotfi Senhadji</name>
    </author>
    <author>
      <name>Huazhong Shu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.1171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.1171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.1172v1</id>
    <updated>2014-11-05T07:36:04Z</updated>
    <published>2014-11-05T07:36:04Z</published>
    <title>Tensor object classification via multilinear discriminant analysis
  network</title>
    <summary>  This paper proposes a multilinear discriminant analysis network (MLDANet) for
the recognition of multidimensional objects, known as tensor objects. The
MLDANet is a variation of linear discriminant analysis network (LDANet) and
principal component analysis network (PCANet), both of which are the recently
proposed deep learning algorithms. The MLDANet consists of three parts: 1) The
encoder learned by MLDA from tensor data. 2) Features maps ob-tained from
decoder. 3) The use of binary hashing and histogram for feature pooling. A
learning algorithm for MLDANet is described. Evaluations on UCF11 database
indicate that the proposed MLDANet outperforms the PCANet, LDANet, MPCA + LDA,
and MLDA in terms of classification for tensor objects.
</summary>
    <author>
      <name>Rui Zeng</name>
    </author>
    <author>
      <name>Jiasong Wu</name>
    </author>
    <author>
      <name>Lotfi Senhadji</name>
    </author>
    <author>
      <name>Huazhong Shu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.1172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.1172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.2214v1</id>
    <updated>2014-11-09T09:51:06Z</updated>
    <published>2014-11-09T09:51:06Z</published>
    <title>Abnormal Object Recognition: A Comprehensive Study</title>
    <summary>  When describing images, humans tend not to talk about the obvious, but rather
mention what they find interesting. We argue that abnormalities and deviations
from typicalities are among the most important components that form what is
worth mentioning. In this paper we introduce the abnormality detection as a
recognition problem and show how to model typicalities and, consequently,
meaningful deviations from prototypical properties of categories. Our model can
recognize abnormalities and report the main reasons of any recognized
abnormality. We introduce the abnormality detection dataset and show
interesting results on how to reason about abnormalities.
</summary>
    <author>
      <name>Babak Saleh</name>
    </author>
    <author>
      <name>Ali Farhadi</name>
    </author>
    <author>
      <name>Ahmed Elgammal</name>
    </author>
    <link href="http://arxiv.org/abs/1411.2214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.3041v1</id>
    <updated>2014-11-12T01:34:46Z</updated>
    <published>2014-11-12T01:34:46Z</published>
    <title>Collecting Image Description Datasets using Crowdsourcing</title>
    <summary>  We describe our two new datasets with images described by humans. Both the
datasets were collected using Amazon Mechanical Turk, a crowdsourcing platform.
The two datasets contain significantly more descriptions per image than other
existing datasets. One is based on a popular image description dataset called
the UIUC Pascal Sentence Dataset, whereas the other is based on the Abstract
Scenes dataset con- taining images made from clipart objects. In this paper we
describe our interfaces, analyze some properties of and show example
descriptions from our two datasets.
</summary>
    <author>
      <name>Ramakrishna Vedantam</name>
    </author>
    <author>
      <name>C. Lawrence Zitnick</name>
    </author>
    <author>
      <name>Devi Parikh</name>
    </author>
    <link href="http://arxiv.org/abs/1411.3041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.3169v1</id>
    <updated>2014-11-12T13:21:48Z</updated>
    <published>2014-11-12T13:21:48Z</published>
    <title>On Coarse Graining of Information and Its Application to Pattern
  Recognition</title>
    <summary>  We propose a method based on finite mixture models for classifying a set of
observations into number of different categories. In order to demonstrate the
method, we show how the component densities for the mixture model can be
derived by using the maximum entropy method in conjunction with conservation of
Pythagorean means. Several examples of distributions belonging to the
Pythagorean family are derived. A discussion on estimation of model parameters
and the number of categories is also given.
</summary>
    <author>
      <name>Ali Ghaderi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.4906011</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.4906011" rel="related"/>
    <link href="http://arxiv.org/abs/1411.3169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4304v1</id>
    <updated>2014-11-16T21:25:53Z</updated>
    <published>2014-11-16T21:25:53Z</published>
    <title>Ten Years of Pedestrian Detection, What Have We Learned?</title>
    <summary>  Paper-by-paper results make it easy to miss the forest for the trees.We
analyse the remarkable progress of the last decade by discussing the main ideas
explored in the 40+ detectors currently present in the Caltech pedestrian
detection benchmark. We observe that there exist three families of approaches,
all currently reaching similar detection quality. Based on our analysis, we
study the complementarity of the most promising ideas by combining multiple
published strategies. This new decision forest detector achieves the current
best known performance on the challenging Caltech-USA dataset.
</summary>
    <author>
      <name>Rodrigo Benenson</name>
    </author>
    <author>
      <name>Mohamed Omran</name>
    </author>
    <author>
      <name>Jan Hosang</name>
    </author>
    <author>
      <name>Bernt Schiele</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ECCV 2014 CVRSUAD workshop proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.4304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.4304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4734v4</id>
    <updated>2015-12-17T03:19:36Z</updated>
    <published>2014-11-18T04:49:08Z</published>
    <title>Predicting Depth, Surface Normals and Semantic Labels with a Common
  Multi-Scale Convolutional Architecture</title>
    <summary>  In this paper we address three different computer vision tasks using a single
basic architecture: depth prediction, surface normal estimation, and semantic
labeling. We use a multiscale convolutional network that is able to adapt
easily to each task using only small modifications, regressing from the input
image to the output map directly. Our method progressively refines predictions
using a sequence of scales, and captures many image details without any
superpixels or low-level segmentation. We achieve state-of-the-art performance
on benchmarks for all three tasks.
</summary>
    <author>
      <name>David Eigen</name>
    </author>
    <author>
      <name>Rob Fergus</name>
    </author>
    <link href="http://arxiv.org/abs/1411.4734v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.4734v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.5879v2</id>
    <updated>2014-12-09T18:00:13Z</updated>
    <published>2014-11-18T17:03:20Z</published>
    <title>A Unified Semantic Embedding: Relating Taxonomies and Attributes</title>
    <summary>  We propose a method that learns a discriminative yet semantic space for
object categorization, where we also embed auxiliary semantic entities such as
supercategories and attributes. Contrary to prior work which only utilized them
as side information, we explicitly embed the semantic entities into the same
space where we embed categories, which enables us to represent a category as
their linear combination. By exploiting such a unified model for semantics, we
enforce each category to be represented by a supercategory + sparse combination
of attributes, with an additional exclusive regularization to learn
discriminative composition.
</summary>
    <author>
      <name>Sung Ju Hwang</name>
    </author>
    <author>
      <name>Leonid Sigal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To Appear in NIPS 2014 Learning Semantics Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.5879v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.5879v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.6031v1</id>
    <updated>2014-11-21T21:38:15Z</updated>
    <published>2014-11-21T21:38:15Z</published>
    <title>Finding Action Tubes</title>
    <summary>  We address the problem of action detection in videos. Driven by the latest
progress in object detection from 2D images, we build action models using rich
feature hierarchies derived from shape and kinematic cues. We incorporate
appearance and motion in two ways. First, starting from image region proposals
we select those that are motion salient and thus are more likely to contain the
action. This leads to a significant reduction in the number of regions being
processed and allows for faster computations. Second, we extract
spatio-temporal feature representations to build strong classifiers using
Convolutional Neural Networks. We link our predictions to produce detections
consistent in time, which we call action tubes. We show that our approach
outperforms other techniques in the task of action detection.
</summary>
    <author>
      <name>Georgia Gkioxari</name>
    </author>
    <author>
      <name>Jitendra Malik</name>
    </author>
    <link href="http://arxiv.org/abs/1411.6031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.6067v2</id>
    <updated>2015-04-26T04:07:17Z</updated>
    <published>2014-11-22T03:14:21Z</published>
    <title>Viewpoints and Keypoints</title>
    <summary>  We characterize the problem of pose estimation for rigid objects in terms of
determining viewpoint to explain coarse pose and keypoint prediction to capture
the finer details. We address both these tasks in two different settings - the
constrained setting with known bounding boxes and the more challenging
detection setting where the aim is to simultaneously detect and correctly
estimate pose of objects. We present Convolutional Neural Network based
architectures for these and demonstrate that leveraging viewpoint estimates can
substantially improve local appearance based keypoint predictions. In addition
to achieving significant improvements over state-of-the-art in the above tasks,
we analyze the error modes and effect of object characteristics on performance
to guide future efforts towards this goal.
</summary>
    <author>
      <name>Shubham Tulsiani</name>
    </author>
    <author>
      <name>Jitendra Malik</name>
    </author>
    <link href="http://arxiv.org/abs/1411.6067v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6067v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.6206v1</id>
    <updated>2014-11-23T08:19:11Z</updated>
    <published>2014-11-23T08:19:11Z</published>
    <title>Low-Rank and Sparse Matrix Decomposition with a-priori knowledge for
  Dynamic 3D MRI reconstruction</title>
    <summary>  It has been recently shown that incorporating priori knowledge significantly
improves the performance of basic compressive sensing based approaches. We have
managed to successfully exploit this idea for recovering a matrix as a
summation of a Low-rank and a Sparse component from compressive measurements.
When applied to the problem of construction of 4D Cardiac MR image sequences in
real-time from highly under-sampled $k-$space data, our proposed method
achieves superior reconstruction quality compared to the other state-of-the-art
methods.
</summary>
    <author>
      <name>Dornoosh Zonoobi</name>
    </author>
    <author>
      <name>Shahrooz Faghih Roohi</name>
    </author>
    <author>
      <name>Ashraf A. Kassim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.6206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.7113v1</id>
    <updated>2014-11-26T05:50:02Z</updated>
    <published>2014-11-26T05:50:02Z</published>
    <title>Real time Detection of Lane Markers in Urban Streets</title>
    <summary>  We present a robust and real time approach to lane marker detection in urban
streets. It is based on generating a top view of the road, filtering using
selective oriented Gaussian filters, using RANSAC line fitting to give initial
guesses to a new and fast RANSAC algorithm for fitting Bezier Splines, which is
then followed by a post-processing step. Our algorithm can detect all lanes in
still images of the street in various conditions, while operating at a rate of
50 Hz and achieving comparable results to previous techniques.
</summary>
    <author>
      <name>Mohamed Aly</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IVS.2008.4621152</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IVS.2008.4621152" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Intelligent Vehicles Symposium, Eindhoven, The Netherlands,
  June 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1411.7113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.7113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.7682v1</id>
    <updated>2014-11-27T19:04:30Z</updated>
    <published>2014-11-27T19:04:30Z</published>
    <title>On color image quality assessment using natural image statistics</title>
    <summary>  Color distortion can introduce a significant damage in visual quality
perception, however, most of existing reduced-reference quality measures are
designed for grayscale images. In this paper, we consider a basic extension of
well-known image-statistics based quality assessment measures to color images.
In order to evaluate the impact of color information on the measures
efficiency, two color spaces are investigated: RGB and CIELAB. Results of an
extensive evaluation using TID 2013 benchmark demonstrates that significant
improvement can be achieved for a great number of distortion type when the
CIELAB color representation is used.
</summary>
    <author>
      <name>Mounir Omari</name>
    </author>
    <author>
      <name>Mohammed El Hassouni</name>
    </author>
    <author>
      <name>Hocine Cherifi</name>
    </author>
    <author>
      <name>Abdelkaher Ait Abdelouahad</name>
    </author>
    <link href="http://arxiv.org/abs/1411.7682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.7682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.7715v1</id>
    <updated>2014-11-27T22:39:50Z</updated>
    <published>2014-11-27T22:39:50Z</published>
    <title>Flying Objects Detection from a Single Moving Camera</title>
    <summary>  We propose an approach to detect flying objects such as UAVs and aircrafts
when they occupy a small portion of the field of view, possibly moving against
complex backgrounds, and are filmed by a camera that itself moves.
  Solving such a difficult problem requires combining both appearance and
motion cues. To this end we propose a regression-based approach to motion
stabilization of local image patches that allows us to achieve effective
classification on spatio-temporal image cubes and outperform state-of-the-art
techniques.
  As the problem is relatively new, we collected two challenging datasets for
UAVs and Aircrafts, which can be used as benchmarks for flying objects
detection and vision-guided collision avoidance.
</summary>
    <author>
      <name>Artem Rozantsev</name>
    </author>
    <author>
      <name>Vincent Lepetit</name>
    </author>
    <author>
      <name>Pascal Fua</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CVPR.2015.7299040</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CVPR.2015.7299040" rel="related"/>
    <link href="http://arxiv.org/abs/1411.7715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.7715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.0774v1</id>
    <updated>2014-12-02T03:31:51Z</updated>
    <published>2014-12-02T03:31:51Z</published>
    <title>Feedforward semantic segmentation with zoom-out features</title>
    <summary>  We introduce a purely feed-forward architecture for semantic segmentation. We
map small image elements (superpixels) to rich feature representations
extracted from a sequence of nested regions of increasing extent. These regions
are obtained by "zooming out" from the superpixel all the way to scene-level
resolution. This approach exploits statistical structure in the image and in
the label space without setting up explicit structured prediction mechanisms,
and thus avoids complex and expensive inference. Instead superpixels are
classified by a feedforward multilayer network. Our architecture achieves new
state of the art performance in semantic segmentation, obtaining 64.4% average
accuracy on the PASCAL VOC 2012 test set.
</summary>
    <author>
      <name>Mohammadreza Mostajabi</name>
    </author>
    <author>
      <name>Payman Yadollahpour</name>
    </author>
    <author>
      <name>Gregory Shakhnarovich</name>
    </author>
    <link href="http://arxiv.org/abs/1412.0774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.0774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.0985v3</id>
    <updated>2015-02-11T17:51:12Z</updated>
    <published>2014-12-02T17:18:13Z</published>
    <title>Covariance estimation using conjugate gradient for 3D classification in
  Cryo-EM</title>
    <summary>  Classifying structural variability in noisy projections of biological
macromolecules is a central problem in Cryo-EM. In this work, we build on a
previous method for estimating the covariance matrix of the three-dimensional
structure present in the molecules being imaged. Our proposed method allows for
incorporation of contrast transfer function and non-uniform distribution of
viewing angles, making it more suitable for real-world data. We evaluate its
performance on a synthetic dataset and an experimental dataset obtained by
imaging a 70S ribosome complex.
</summary>
    <author>
      <name>Joakim Andén</name>
    </author>
    <author>
      <name>Eugene Katsevich</name>
    </author>
    <author>
      <name>Amit Singer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISBI.2015.7163849</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISBI.2015.7163849" rel="related"/>
    <link href="http://arxiv.org/abs/1412.0985v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.0985v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.1194v1</id>
    <updated>2014-12-03T05:23:03Z</updated>
    <published>2014-12-03T05:23:03Z</published>
    <title>Gradient Boundary Histograms for Action Recognition</title>
    <summary>  This paper introduces a high efficient local spatiotemporal descriptor,
called gradient boundary histograms (GBH). The proposed GBH descriptor is built
on simple spatio-temporal gradients, which are fast to compute. We demonstrate
that it can better represent local structure and motion than other
gradient-based descriptors, and significantly outperforms them on large
realistic datasets. A comprehensive evaluation shows that the recognition
accuracy is preserved while the spatial resolution is greatly reduced, which
yields both high efficiency and low memory usage.
</summary>
    <author>
      <name>Feng Shi</name>
    </author>
    <author>
      <name>Robert Laganiere</name>
    </author>
    <author>
      <name>Emil Petriu</name>
    </author>
    <link href="http://arxiv.org/abs/1412.1194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.1194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.2067v1</id>
    <updated>2014-11-20T20:13:00Z</updated>
    <published>2014-11-20T20:13:00Z</published>
    <title>An algorithm for improving Non-Local Means operators via low-rank
  approximation</title>
    <summary>  We present a method for improving a Non Local Means operator by computing its
low-rank approximation. The low-rank operator is constructed by applying a
filter to the spectrum of the original Non Local Means operator. This results
in an operator which is less sensitive to noise while preserving important
properties of the original operator. The method is efficiently implemented
based on Chebyshev polynomials and is demonstrated on the application of
natural images denoising. For this application, we provide a comprehensive
comparison of our method with leading denoising methods.
</summary>
    <author>
      <name>Victor May</name>
    </author>
    <author>
      <name>Yosi Keller</name>
    </author>
    <author>
      <name>Nir Sharon</name>
    </author>
    <author>
      <name>Yoel Shkolnisky</name>
    </author>
    <link href="http://arxiv.org/abs/1412.2067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.2067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3684v1</id>
    <updated>2014-12-10T18:23:13Z</updated>
    <published>2014-12-10T18:23:13Z</published>
    <title>Object Recognition Using Deep Neural Networks: A Survey</title>
    <summary>  Recognition of objects using Deep Neural Networks is an active area of
research and many breakthroughs have been made in the last few years. The paper
attempts to indicate how far this field has progressed. The paper briefly
describes the history of research in Neural Networks and describe several of
the recent advances in this field. The performances of recently developed
Neural Network Algorithm over benchmark datasets have been tabulated. Finally,
some the applications of this field have been provided.
</summary>
    <author>
      <name>Soren Goyal</name>
    </author>
    <author>
      <name>Paul Benjamin</name>
    </author>
    <link href="http://arxiv.org/abs/1412.3684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3717v2</id>
    <updated>2015-04-10T12:30:14Z</updated>
    <published>2014-11-18T11:09:01Z</published>
    <title>Unsupervised Neural Architecture for Saliency Detection: Extended
  Version</title>
    <summary>  We propose a novel neural network architecture for visual saliency
detections, which utilizes neurophysiologically plausible mechanisms for
extraction of salient regions. The model has been significantly inspired by
recent findings from neurophysiology and aimed to simulate the bottom-up
processes of human selective attention. Two types of features were analyzed:
color and direction of maximum variance. The mechanism we employ for processing
those features is PCA, implemented by means of normalized Hebbian learning and
the waves of spikes. To evaluate performance of our model we have conducted
psychological experiment. Comparison of simulation results with those of
experiment indicates good performance of our model.
</summary>
    <author>
      <name>Natalia Efremova</name>
    </author>
    <author>
      <name>Sergey Tarasenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 26 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.3717v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3717v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3914v1</id>
    <updated>2014-12-12T08:17:28Z</updated>
    <published>2014-12-12T08:17:28Z</published>
    <title>Edge Preserving Multi-Modal Registration Based On Gradient Intensity
  Self-Similarity</title>
    <summary>  Image registration is a challenging task in the world of medical imaging.
Particularly, accurate edge registration plays a central role in a variety of
clinical conditions. The Modality Independent Neighbourhood Descriptor (MIND)
demonstrates state of the art alignment, based on the image self-similarity.
However, this method appears to be less accurate regarding edge registration.
In this work, we propose a new registration method, incorporating gradient
intensity and MIND self-similarity metric. Experimental results show the
superiority of this method in edge registration tasks, while preserving the
original MIND performance for other image features and textures.
</summary>
    <author>
      <name>Tamar Rott</name>
    </author>
    <author>
      <name>Dorin Shriki</name>
    </author>
    <author>
      <name>Tamir Bendory</name>
    </author>
    <link href="http://arxiv.org/abs/1412.3914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3949v1</id>
    <updated>2014-12-12T11:11:30Z</updated>
    <published>2014-12-12T11:11:30Z</published>
    <title>CITlab ARGUS for historical handwritten documents</title>
    <summary>  We describe CITlab's recognition system for the HTRtS competition attached to
the 14. International Conference on Frontiers in Handwriting Recognition, ICFHR
2014. The task comprises the recognition of historical handwritten documents.
The core algorithms of our system are based on multi-dimensional recurrent
neural networks (MDRNN) and connectionist temporal classification (CTC). The
software modules behind that as well as the basic utility technologies are
essentially powered by PLANET's ARGUS framework for intelligent text
recognition and image processing.
</summary>
    <author>
      <name>Tobias Strauß</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">for the University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <author>
      <name>Tobias Grüning</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">for the University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <author>
      <name>Gundram Leifert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">for the University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <author>
      <name>Roger Labahn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">for the University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1412.3949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 68T10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.4181v2</id>
    <updated>2015-06-28T19:37:56Z</updated>
    <published>2014-12-13T02:30:59Z</published>
    <title>Oriented Edge Forests for Boundary Detection</title>
    <summary>  We present a simple, efficient model for learning boundary detection based on
a random forest classifier. Our approach combines (1) efficient clustering of
training examples based on simple partitioning of the space of local edge
orientations and (2) scale-dependent calibration of individual tree output
probabilities prior to multiscale combination. The resulting model outperforms
published results on the challenging BSDS500 boundary detection benchmark.
Further, on large datasets our model requires substantially less memory for
training and speeds up training time by a factor of 10 over the structured
forest model.
</summary>
    <author>
      <name>Sam Hallman</name>
    </author>
    <author>
      <name>Charless C. Fowlkes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">updated to include contents of CVPR version + new figure showing
  example segmentation results</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.4181v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.4181v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.4183v1</id>
    <updated>2014-12-13T02:39:09Z</updated>
    <published>2014-12-13T02:39:09Z</published>
    <title>A survey of modern optical character recognition techniques</title>
    <summary>  This report explores the latest advances in the field of digital document
recognition. With the focus on printed document imagery, we discuss the major
developments in optical character recognition (OCR) and document image
enhancement/restoration in application to Latin and non-Latin scripts. In
addition, we review and discuss the available technologies for hand-written
document recognition. In this report, we also provide some company-accumulated
benchmark results on available OCR engines.
</summary>
    <author>
      <name>Eugene Borovikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report surveying OCR/ICR and document understanding methods
  as of 2004.It contains 38 pages, numerous figures, 93 references, and
  provides a table of contents</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.4183v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.4183v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62-04" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.7.5; I.4.1; I.5.4; I.4.1; I.4.3; I.4.6; I.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.4205v1</id>
    <updated>2014-12-13T08:57:49Z</updated>
    <published>2014-12-13T08:57:49Z</published>
    <title>The application of the Bayes Ying Yang harmony based GMMs in on-line
  signature verification</title>
    <summary>  In this contribution, a Bayes Ying Yang(BYY) harmony based approach for
on-line signature verification is presented. In the proposed method, a simple
but effective Gaussian Mixture Models(GMMs) is used to represent for each
user's signature model based on the prior information collected. Different from
the early works, in this paper, we use the Bayes Ying Yang machine combined
with the harmony function to achieve Automatic Model Selection(AMS) during the
parameter learning for the GMMs, so that a better approximation of the user
model is assured. Experiments on a database from the First International
Signature Verification Competition(SVC 2004) confirm that this combined
algorithm yields quite satisfactory results.
</summary>
    <author>
      <name>Xiaosha Zhao</name>
    </author>
    <author>
      <name>Mandan Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1412.4205v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.4205v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5322v1</id>
    <updated>2014-12-17T10:21:10Z</updated>
    <published>2014-12-17T10:21:10Z</published>
    <title>An Algebraical Model for Gray Level Images</title>
    <summary>  In this paper we propose a new algebraical model for the gray level images.
It can be used for digital image processing. The model adresses to those images
which are generated in improper light conditions (very low or high level). The
vector space structure is able to illustrate some features into the image using
modified level of contrast and luminosity. Also, the defined structure could be
used in image enhancement. The general approach is presented with experimental
results to demonstrate image enhancement.
</summary>
    <author>
      <name>Vasile Patrascu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 7th International Conference, Exhibition on Optimization of
  Electrical and Electronic Equipment, OPTIM 2000, Bra\c{s}ov, Rom\^ania 11-12
  May, 2000</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.5322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5325v1</id>
    <updated>2014-12-17T10:32:07Z</updated>
    <published>2014-12-17T10:32:07Z</published>
    <title>Color Image Enhancement In the Framework of Logarithmic Models</title>
    <summary>  In this paper, we propose a mathematical model for color image processing. It
is a logarithmical one. We consider the cube (-1,1)x(-1,1)x(-1,1) as the set of
values for the color space. We define two operations: addition &lt;+&gt; and real
scalar multiplication &lt;x&gt;. With these operations the space of colors becomes a
real vector space. Then, defining the scalar product (.|.) and the norm || .
||, we obtain a (logarithmic) Euclidean space. We show how we can use this
model for color image enhancement and we present some experimental results.
</summary>
    <author>
      <name>Vasile Patrascu</name>
    </author>
    <author>
      <name>Vasile Buzuloiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 8th IEEE International Conference on Telecommunications, Vol. 1,
  pp. 199-204, IEEE ICT2001, June 4 - 7, 2001, Bucharest,Romania</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.5325v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5325v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5328v1</id>
    <updated>2014-12-17T10:50:25Z</updated>
    <published>2014-12-17T10:50:25Z</published>
    <title>A Mathematical Model for Logarithmic Image Processing</title>
    <summary>  In this paper, we propose a new mathematical model for image processing. It
is a logarithmical one. We consider the bounded interval (-1, 1) as the set of
gray levels. Firstly, we define two operations: addition &lt;+&gt; and real scalar
multiplication &lt;x&gt;. With these operations, the set of gray levels becomes a
real vector space. Then, defining the scalar product (.|.) and the norm || .
||, we obtain an Euclidean space of the gray levels. Secondly, we extend these
operations and functions for color images. We finally show the effect of
various simple operations on an image.
</summary>
    <author>
      <name>Vasile Patrascu</name>
    </author>
    <author>
      <name>Vasile Buzuloiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 5th World Multi-Conference on Systemics, Cybernetics and
  Informatics, Vol 13, pp. 117-122, SCI2001, July 22-25, 2001, Orlando, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.5328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5764v1</id>
    <updated>2014-12-18T09:09:17Z</updated>
    <published>2014-12-18T09:09:17Z</published>
    <title>Image Dynamic Range Enhancement in the Context of Logarithmic Models</title>
    <summary>  Images of a scene observed under a variable illumination or with a variable
optical aperture are not identical. Does a privileged representant exist? In
which mathematical context? How to obtain it? The authors answer to such
questions in the context of logarithmic models for images. After a short
presentation of the model, the paper presents two image transforms: one
performs an optimal enhancement of the dynamic range, and the other does the
same for the mean dynamic range. Experimental results are shown.
</summary>
    <author>
      <name>Vasile Patrascu</name>
    </author>
    <author>
      <name>Vasile Buzuloiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 11th European Signal Processing Conference, EUSIPCO 2002,
  Toulouse, France, 03-06 september 2002</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.5764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5787v1</id>
    <updated>2014-12-18T10:12:49Z</updated>
    <published>2014-12-18T10:12:49Z</published>
    <title>Gray Level Image Enhancement Using Polygonal Functions</title>
    <summary>  This paper presents a method for enhancing the gray level images. This method
takes part from the category of point transforms and it is based on
interpolation functions. The latter have a graphic represented by polygonal
lines. The interpolation nodes of these functions are calculated taking into
account the statistics of gray levels belonging to the image.
</summary>
    <author>
      <name>Vasile Patrascu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 13th International Conference on Automation, Quality and Testing,
  Robotics, Vol. Robotics, Image and Signal processing, pp. 129-134, May 23-25
  2002, Cluj-Napoca, Romania</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.5787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5802v1</id>
    <updated>2014-12-18T10:59:09Z</updated>
    <published>2014-12-18T10:59:09Z</published>
    <title>Contour Detection Using Contrast Formulas in the Framework of
  Logarithmic Models</title>
    <summary>  In this paper we use a new logarithmic model of image representation,
developed in [1,2], for edge detection. In fact, in the framework of the new
model we obtain the formulas for computing the "contrast of a pixel" and the
"contrast" image is just the "contour" or edge image. In our setting the range
of values is preserved and the quality of the contour is good for high as well
as for low luminosity regions. We present the comparison of our results with
the results using classical edge detection operators.
</summary>
    <author>
      <name>Vasile Patrascu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 8th International Conference, Exhibition on Optimization of
  Electrical and Electronic Equipment, OPTIM 2002, Vol III, pp 751-756, 16 - 17
  May 2002, Brasov, Romania</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.5802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6012v1</id>
    <updated>2014-12-15T06:54:47Z</updated>
    <published>2014-12-15T06:54:47Z</published>
    <title>CITlab ARGUS for historical data tables</title>
    <summary>  We describe CITlab's recognition system for the ANWRESH-2014 competition
attached to the 14. International Conference on Frontiers in Handwriting
Recognition, ICFHR 2014. The task comprises word recognition from segmented
historical documents. The core components of our system are based on
multi-dimensional recurrent neural networks (MDRNN) and connectionist temporal
classification (CTC). The software modules behind that as well as the basic
utility technologies are essentially powered by PLANET's ARGUS framework for
intelligent text recognition and image processing.
</summary>
    <author>
      <name>Gundram Leifert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">for the University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <author>
      <name>Tobias Grüning</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">for the University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <author>
      <name>Tobias Strauß</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">for the University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <author>
      <name>Roger Labahn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">for the University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1412.3949</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.6012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 68T10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6574v4</id>
    <updated>2016-05-09T08:54:31Z</updated>
    <published>2014-12-20T01:32:43Z</published>
    <title>Visual Instance Retrieval with Deep Convolutional Networks</title>
    <summary>  This paper provides an extensive study on the availability of image
representations based on convolutional networks (ConvNets) for the task of
visual instance retrieval. Besides the choice of convolutional layers, we
present an efficient pipeline exploiting multi-scale schemes to extract local
features, in particular, by taking geometric invariance into explicit account,
i.e. positions, scales and spatial consistency. In our experiments using five
standard image retrieval datasets, we demonstrate that generic ConvNet image
representations can outperform other state-of-the-art methods if they are
extracted appropriately.
</summary>
    <author>
      <name>Ali Sharif Razavian</name>
    </author>
    <author>
      <name>Josephine Sullivan</name>
    </author>
    <author>
      <name>Stefan Carlsson</name>
    </author>
    <author>
      <name>Atsuto Maki</name>
    </author>
    <link href="http://arxiv.org/abs/1412.6574v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6574v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6618v3</id>
    <updated>2015-05-03T11:26:34Z</updated>
    <published>2014-12-20T07:08:54Z</published>
    <title>Permutohedral Lattice CNNs</title>
    <summary>  This paper presents a convolutional layer that is able to process sparse
input features. As an example, for image recognition problems this allows an
efficient filtering of signals that do not lie on a dense grid (like pixel
position), but of more general features (such as color values). The presented
algorithm makes use of the permutohedral lattice data structure. The
permutohedral lattice was introduced to efficiently implement a bilateral
filter, a commonly used image processing operation. Its use allows for a
generalization of the convolution type found in current (spatial) convolutional
network architectures.
</summary>
    <author>
      <name>Martin Kiefel</name>
    </author>
    <author>
      <name>Varun Jampani</name>
    </author>
    <author>
      <name>Peter V. Gehler</name>
    </author>
    <link href="http://arxiv.org/abs/1412.6618v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6618v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6847v1</id>
    <updated>2014-12-22T00:10:11Z</updated>
    <published>2014-12-22T00:10:11Z</published>
    <title>A New Way to Factorize Linear Cameras</title>
    <summary>  The implementation details of factorizing the 3x4 projection matrices of
linear cameras into their left matrix factors and the 4x4 homogeneous
central(also parallel for infinite center cases) projection factors are
presented in this work. Any full row rank 3x4 real matrix can be factorized
into such basic matrices which will be called LC factors.
  A further extension to multiple view midpoint triangulation, for both pinhole
and affine camera cases, is also presented based on such camera factorizations.
</summary>
    <author>
      <name>Feng Lu</name>
    </author>
    <author>
      <name>Ziqiang Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1412.6847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7851v1</id>
    <updated>2014-12-25T18:50:31Z</updated>
    <published>2014-12-25T18:50:31Z</published>
    <title>Fractal descriptors based on the probability dimension: a texture
  analysis and classification approach</title>
    <summary>  In this work, we propose a novel technique for obtaining descriptors of
gray-level texture images. The descriptors are provided by applying a
multiscale transform to the fractal dimension of the image estimated through
the probability (Voss) method. The effectiveness of the descriptors is verified
in a classification task using benchmark over texture datasets. The results
obtained demonstrate the efficiency of the proposed method as a tool for the
description and discrimination of texture images.
</summary>
    <author>
      <name>João Batista Florindo</name>
    </author>
    <author>
      <name>Odemir Martinez Bruno</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patrec.2014.01.009</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patrec.2014.01.009" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:1205.2821</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pattern Recognition Letters, Volume 42, Pages 107-114, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1412.7851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7884v1</id>
    <updated>2014-12-26T02:46:41Z</updated>
    <published>2014-12-26T02:46:41Z</published>
    <title>Sparkle Vision: Seeing the World through Random Specular Microfacets</title>
    <summary>  In this paper, we study the problem of reproducing the world lighting from a
single image of an object covered with random specular microfacets on the
surface. We show that such reflectors can be interpreted as a randomized
mapping from the lighting to the image. Such specular objects have very
different optical properties from both diffuse surfaces and smooth specular
objects like metals, so we design special imaging system to robustly and
effectively photograph them. We present simple yet reliable algorithms to
calibrate the proposed system and do the inference. We conduct experiments to
verify the correctness of our model assumptions and prove the effectiveness of
our pipeline.
</summary>
    <author>
      <name>Zhengdong Zhang</name>
    </author>
    <author>
      <name>Phillip Isola</name>
    </author>
    <author>
      <name>Edward H. Adelson</name>
    </author>
    <link href="http://arxiv.org/abs/1412.7884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.8070v1</id>
    <updated>2014-12-27T17:53:06Z</updated>
    <published>2014-12-27T17:53:06Z</published>
    <title>Functional correspondence by matrix completion</title>
    <summary>  In this paper, we consider the problem of finding dense intrinsic
correspondence between manifolds using the recently introduced functional
framework. We pose the functional correspondence problem as matrix completion
with manifold geometric structure and inducing functional localization with the
$L_1$ norm. We discuss efficient numerical procedures for the solution of our
problem. Our method compares favorably to the accuracy of state-of-the-art
correspondence algorithms on non-rigid shape matching benchmarks, and is
especially advantageous in settings when only scarce data is available.
</summary>
    <author>
      <name>Artiom Kovnatsky</name>
    </author>
    <author>
      <name>Michael M. Bronstein</name>
    </author>
    <author>
      <name>Xavier Bresson</name>
    </author>
    <author>
      <name>Pierre Vandergheynst</name>
    </author>
    <link href="http://arxiv.org/abs/1412.8070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.8070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.8341v1</id>
    <updated>2014-12-29T13:47:06Z</updated>
    <published>2014-12-29T13:47:06Z</published>
    <title>Spectral classification using convolutional neural networks</title>
    <summary>  There is a great need for accurate and autonomous spectral classification
methods in astrophysics. This thesis is about training a convolutional neural
network (ConvNet) to recognize an object class (quasar, star or galaxy) from
one-dimension spectra only. Author developed several scripts and C programs for
datasets preparation, preprocessing and postprocessing of the data. EBLearn
library (developed by Pierre Sermanet and Yann LeCun) was used to create
ConvNets. Application on dataset of more than 60000 spectra yielded success
rate of nearly 95%. This thesis conclusively proved great potential of
convolutional neural networks and deep learning methods in astrophysics.
</summary>
    <author>
      <name>Pavel Hála</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">71 pages, 50 figures, Master's thesis, Masaryk University</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.8341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.8341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.8556v3</id>
    <updated>2015-05-05T05:36:05Z</updated>
    <published>2014-12-30T03:49:47Z</published>
    <title>Domain-Size Pooling in Local Descriptors: DSP-SIFT</title>
    <summary>  We introduce a simple modification of local image descriptors, such as SIFT,
based on pooling gradient orientations across different domain sizes, in
addition to spatial locations. The resulting descriptor, which we call
DSP-SIFT, outperforms other methods in wide-baseline matching benchmarks,
including those based on convolutional neural networks, despite having the same
dimension of SIFT and requiring no training.
</summary>
    <author>
      <name>Jingming Dong</name>
    </author>
    <author>
      <name>Stefano Soatto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of the CVPR 2015 paper. Technical Report UCLA CSD
  140022</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.8556v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.8556v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.8656v1</id>
    <updated>2014-12-30T15:28:26Z</updated>
    <published>2014-12-30T15:28:26Z</published>
    <title>A multistep segmentation algorithm for vessel extraction in medical
  imaging</title>
    <summary>  The main contribution of this paper is to propose an iterative procedure for
tubular structure segmentation of 2D images, which combines tight frame of
Curvelet transforms with a SURE technique thresholding which is based on
principle obtained by minimizing Stein Unbiased Risk Estimate for denoising.
This proposed algorithm is mainly based on the TFA proposal presented in [1,
9], which we use eigenvectors of Hessian matrix of image for improving this
iterative part in segmenting unclear and narrow vessels and filling the gap
between separate pieces of detected vessels. The experimental results are
presented to demonstrate the effectiveness of the proposed model.
</summary>
    <author>
      <name>Nasser Aghazadeh</name>
    </author>
    <author>
      <name>Ladan Sharafyan Cigaroudy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages,4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.8656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.8656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.8659v2</id>
    <updated>2015-05-30T16:43:02Z</updated>
    <published>2014-12-30T15:32:18Z</published>
    <title>Deep Roto-Translation Scattering for Object Classification</title>
    <summary>  Dictionary learning algorithms or supervised deep convolution networks have
considerably improved the efficiency of predefined feature representations such
as SIFT. We introduce a deep scattering convolution network, with predefined
wavelet filters over spatial and angular variables. This representation brings
an important improvement to results previously obtained with predefined
features over object image databases such as Caltech and CIFAR. The resulting
accuracy is comparable to results obtained with unsupervised deep learning and
dictionary based representations. This shows that refining image
representations by using geometric priors is a promising direction to improve
image classification and its understanding.
</summary>
    <author>
      <name>Edouard Oyallon</name>
    </author>
    <author>
      <name>Stéphane Mallat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, CVPR 2015 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.8659v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.8659v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.00108v1</id>
    <updated>2014-12-31T10:53:50Z</updated>
    <published>2014-12-31T10:53:50Z</published>
    <title>HSI based colour image equalization using iterative nth root and nth
  power</title>
    <summary>  In this paper an equalization technique for colour images is introduced. The
method is based on nth root and nth power equalization approach but with
optimization of the mean of the image in different colour channels such as RGB
and HSI. The performance of the proposed method has been measured by the means
of peak signal to noise ratio. The proposed algorithm has been compared with
conventional histogram equalization and the visual and quantitative
experimental results are showing that the proposed method over perform the
histogram equalization.
</summary>
    <author>
      <name>Gholamreza Anbarjafari</name>
    </author>
    <link href="http://arxiv.org/abs/1501.00108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.00108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.00834v1</id>
    <updated>2015-01-05T12:20:09Z</updated>
    <published>2015-01-05T12:20:09Z</published>
    <title>Inverse Renormalization Group Transformation in Bayesian Image
  Segmentations</title>
    <summary>  A new Bayesian image segmentation algorithm is proposed by combining a loopy
belief propagation with an inverse real space renormalization group
transformation to reduce the computational time. In results of our experiment,
we observe that the proposed method can reduce the computational time to less
than one-tenth of that taken by conventional Bayesian approaches.
</summary>
    <author>
      <name>Kazuyuki Tanaka</name>
    </author>
    <author>
      <name>Shun Kataoka</name>
    </author>
    <author>
      <name>Muneki Yasuda</name>
    </author>
    <author>
      <name>Masayuki Ohzeki</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.7566/JPSJ.84.045001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.7566/JPSJ.84.045001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of the Physical Society of Japan 84 (2015) 045001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1501.00834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.00834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.00901v2</id>
    <updated>2015-04-29T06:35:50Z</updated>
    <published>2015-01-05T15:53:01Z</published>
    <title>Learning to Recognize Pedestrian Attribute</title>
    <summary>  Learning to recognize pedestrian attributes at far distance is a challenging
problem in visual surveillance since face and body close-shots are hardly
available; instead, only far-view image frames of pedestrian are given. In this
study, we present an alternative approach that exploits the context of
neighboring pedestrian images for improved attribute inference compared to the
conventional SVM-based method. In addition, we conduct extensive experiments to
evaluate the informativeness of background and foreground features for
attribute recognition. Experiments are based on our newly released pedestrian
attribute dataset, which is by far the largest and most diverse of its kind.
</summary>
    <author>
      <name>Yubin Deng</name>
    </author>
    <author>
      <name>Ping Luo</name>
    </author>
    <author>
      <name>Chen Change Loy</name>
    </author>
    <author>
      <name>Xiaoou Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1501.00901v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.00901v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.01548v2</id>
    <updated>2015-01-18T04:18:38Z</updated>
    <published>2015-01-07T16:40:51Z</published>
    <title>Implementation of Auto Monitoring and Short-Message-Service System via
  GSM Modem</title>
    <summary>  Auto-Monitoring and Short-Messaging-Service System is a real-time monitoring
system for any critical operational environments. It detects an undesired event
occurring in the environment, generates an alert with detailed message and
sends it to the user to prevent hazards. This system employs a Friendly ARM as
main controller while, sensors and terminals to interact with the real world. A
GSM network is utilized to bridge the communication between monitoring system
and user. This paper presents details of prototyping the system.
</summary>
    <author>
      <name>Akilan Thangarajah</name>
    </author>
    <author>
      <name>Buddhapala Wongkaew</name>
    </author>
    <author>
      <name>Mongkol Ekpanyapong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.01548v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.01548v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.02393v1</id>
    <updated>2015-01-10T21:12:09Z</updated>
    <published>2015-01-10T21:12:09Z</published>
    <title>Riemannian Metric Learning for Symmetric Positive Definite Matrices</title>
    <summary>  Over the past few years, symmetric positive definite (SPD) matrices have been
receiving considerable attention from computer vision community. Though various
distance measures have been proposed in the past for comparing SPD matrices,
the two most widely-used measures are affine-invariant distance and
log-Euclidean distance. This is because these two measures are true geodesic
distances induced by Riemannian geometry. In this work, we focus on the
log-Euclidean Riemannian geometry and propose a data-driven approach for
learning Riemannian metrics/geodesic distances for SPD matrices. We show that
the geodesic distance learned using the proposed approach performs better than
various existing distance measures when evaluated on face matching and
clustering tasks.
</summary>
    <author>
      <name>Raviteja Vemulapalli</name>
    </author>
    <author>
      <name>David W. Jacobs</name>
    </author>
    <link href="http://arxiv.org/abs/1501.02393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.02393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.02825v1</id>
    <updated>2015-01-12T21:14:56Z</updated>
    <published>2015-01-12T21:14:56Z</published>
    <title>A Survey on Recent Advances of Computer Vision Algorithms for Egocentric
  Video</title>
    <summary>  Recent technological advances have made lightweight, head mounted cameras
both practical and affordable and products like Google Glass show first
approaches to introduce the idea of egocentric (first-person) video to the
mainstream. Interestingly, the computer vision community has only recently
started to explore this new domain of egocentric vision, where research can
roughly be categorized into three areas: Object recognition, activity
detection/recognition, video summarization. In this paper, we try to give a
broad overview about the different problems that have been addressed and
collect and compare evaluation results. Moreover, along with the emergence of
this new domain came the introduction of numerous new and versatile benchmark
datasets, which we summarize and compare as well.
</summary>
    <author>
      <name>Sven Bambach</name>
    </author>
    <link href="http://arxiv.org/abs/1501.02825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.02825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.04782v2</id>
    <updated>2015-07-16T14:57:47Z</updated>
    <published>2015-01-20T12:38:08Z</published>
    <title>Constructing Binary Descriptors with a Stochastic Hill Climbing Search</title>
    <summary>  Binary descriptors of image patches provide processing speed advantages and
require less storage than methods that encode the patch appearance with a
vector of real numbers. We provide evidence that, despite its simplicity, a
stochastic hill climbing bit selection procedure for descriptor construction
defeats recently proposed alternatives on a standard discriminative power
benchmark. The method is easy to implement and understand, has no free
parameters that need fine tuning, and runs fast.
</summary>
    <author>
      <name>Nenad Markuš</name>
    </author>
    <author>
      <name>Igor S. Pandžić</name>
    </author>
    <author>
      <name>Jörgen Ahlberg</name>
    </author>
    <link href="http://arxiv.org/abs/1501.04782v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.04782v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.05759v1</id>
    <updated>2015-01-23T10:19:33Z</updated>
    <published>2015-01-23T10:19:33Z</published>
    <title>Filtered Channel Features for Pedestrian Detection</title>
    <summary>  This paper starts from the observation that multiple top performing
pedestrian detectors can be modelled by using an intermediate layer filtering
low-level features in combination with a boosted decision forest. Based on this
observation we propose a unifying framework and experimentally explore
different filter families. We report extensive results enabling a systematic
analysis.
  Using filtered channel features we obtain top performance on the challenging
Caltech and KITTI datasets, while using only HOG+LUV as low-level features.
When adding optical flow features we further improve detection quality and
report the best known results on the Caltech dataset, reaching 93% recall at 1
FPPI.
</summary>
    <author>
      <name>Shanshan Zhang</name>
    </author>
    <author>
      <name>Rodrigo Benenson</name>
    </author>
    <author>
      <name>Bernt Schiele</name>
    </author>
    <link href="http://arxiv.org/abs/1501.05759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.05759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.05964v1</id>
    <updated>2015-01-23T21:36:55Z</updated>
    <published>2015-01-23T21:36:55Z</published>
    <title>Advances in Human Action Recognition: A Survey</title>
    <summary>  Human action recognition has been an important topic in computer vision due
to its many applications such as video surveillance, human machine interaction
and video retrieval. One core problem behind these applications is
automatically recognizing low-level actions and high-level activities of
interest. The former is usually the basis for the latter. This survey gives an
overview of the most recent advances in human action recognition during the
past several years, following a well-formed taxonomy proposed by a previous
survey. From this state-of-the-art survey, researchers can view a panorama of
progress in this area for future research.
</summary>
    <author>
      <name>Guangchun Cheng</name>
    </author>
    <author>
      <name>Yiwen Wan</name>
    </author>
    <author>
      <name>Abdullah N. Saudagar</name>
    </author>
    <author>
      <name>Kamesh Namuduri</name>
    </author>
    <author>
      <name>Bill P. Buckles</name>
    </author>
    <link href="http://arxiv.org/abs/1501.05964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.05964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.06993v1</id>
    <updated>2015-01-28T05:41:07Z</updated>
    <published>2015-01-28T05:41:07Z</published>
    <title>Feature Sampling Strategies for Action Recognition</title>
    <summary>  Although dense local spatial-temporal features with bag-of-features
representation achieve state-of-the-art performance for action recognition, the
huge feature number and feature size prevent current methods from scaling up to
real size problems. In this work, we investigate different types of feature
sampling strategies for action recognition, namely dense sampling, uniformly
random sampling and selective sampling. We propose two effective selective
sampling methods using object proposal techniques. Experiments conducted on a
large video dataset show that we are able to achieve better average recognition
accuracy using 25% less features, through one of proposed selective sampling
methods, and even remain comparable accuracy while discarding 70% features.
</summary>
    <author>
      <name>Youjie Zhou</name>
    </author>
    <author>
      <name>Hongkai Yu</name>
    </author>
    <author>
      <name>Song Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1501.06993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.06993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.07645v2</id>
    <updated>2015-05-17T03:32:22Z</updated>
    <published>2015-01-30T02:08:51Z</published>
    <title>Hyper-parameter optimization of Deep Convolutional Networks for object
  recognition</title>
    <summary>  Recently sequential model based optimization (SMBO) has emerged as a
promising hyper-parameter optimization strategy in machine learning. In this
work, we investigate SMBO to identify architecture hyper-parameters of deep
convolution networks (DCNs) object recognition. We propose a simple SMBO
strategy that starts from a set of random initial DCN architectures to generate
new architectures, which on training perform well on a given dataset. Using the
proposed SMBO strategy we are able to identify a number of DCN architectures
that produce results that are comparable to state-of-the-art results on object
recognition benchmarks.
</summary>
    <author>
      <name>Sachin S. Talathi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure, 3 tables, Submitted to ICIP 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.07645v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.07645v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.07681v1</id>
    <updated>2015-01-30T07:16:50Z</updated>
    <published>2015-01-30T07:16:50Z</published>
    <title>Vector Quantization by Minimizing Kullback-Leibler Divergence</title>
    <summary>  This paper proposes a new method for vector quantization by minimizing the
Kullback-Leibler Divergence between the class label distributions over the
quantization inputs, which are original vectors, and the output, which is the
quantization subsets of the vector set. In this way, the vector quantization
output can keep as much information of the class label as possible. An
objective function is constructed and we also developed an iterative algorithm
to minimize it. The new method is evaluated on bag-of-features based image
classification problem.
</summary>
    <author>
      <name>Lan Yang</name>
    </author>
    <author>
      <name>Jingbin Wang</name>
    </author>
    <author>
      <name>Yujin Tu</name>
    </author>
    <author>
      <name>Prarthana Mahapatra</name>
    </author>
    <author>
      <name>Nelson Cardoso</name>
    </author>
    <link href="http://arxiv.org/abs/1501.07681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.07681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.07862v1</id>
    <updated>2015-01-30T17:50:41Z</updated>
    <published>2015-01-30T17:50:41Z</published>
    <title>An Analytical Study of different Document Image Binarization Methods</title>
    <summary>  Document image has been the area of research for a couple of decades because
of its potential application in the area of text recognition, line recognition
or any other shape recognition from the image. For most of these purposes
binarization of image becomes mandatory as far as recognition is concerned.
Throughout couple decades standard algorithms have already been developed for
this purpose. Some of these algorithms are applicable to degraded image also.
Our objective behind this work is to study the existing techniques, compare
them in view of advantages and disadvantages and modify some of these
algorithms to optimize time or performance.
</summary>
    <author>
      <name>Mahua Nandy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Pal</arxiv:affiliation>
    </author>
    <author>
      <name>Satadal Saha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">National Conference on Computing and Communication Systems
  (COCOSYS-09), UIT, Burdwan, January 02-04, 2009, pp. 71-76</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.07862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.07862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00558v2</id>
    <updated>2015-02-09T19:38:58Z</updated>
    <published>2015-02-02T17:22:26Z</published>
    <title>Complex-Valued Hough Transforms for Circles</title>
    <summary>  This paper advocates the use of complex variables to represent votes in the
Hough transform for circle detection. Replacing the positive numbers
classically used in the parameter space of the Hough transforms by complex
numbers allows cancellation effects when adding up the votes. Cancellation and
the computation of shape likelihood via a complex number's magnitude square
lead to more robust solutions than the "classic" algorithms, as shown by
computational experiments on synthetic and real datasets.
</summary>
    <author>
      <name>Marcelo Cicconet</name>
    </author>
    <author>
      <name>Davi Geiger</name>
    </author>
    <author>
      <name>Michael Werman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper has been withdrawn since the authors concluded a more
  comprehensive study on the choice of parameters needs to be performed</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.00558v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00558v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00561v2</id>
    <updated>2015-02-09T19:38:39Z</updated>
    <published>2015-02-02T17:33:55Z</published>
    <title>Quantum Pairwise Symmetry: Applications in 2D Shape Analysis</title>
    <summary>  A pair of rooted tangents -- defining a quantum triangle -- with an
associated quantum wave of spin 1/2 is proposed as the primitive to represent
and compute symmetry. Measures of the spin characterize how "isosceles" or how
"degenerate" these triangles are -- which corresponds to their mirror or
parallel symmetry. We also introduce a complex-valued kernel to model
probability errors in the parameter space, which is more robust to noise and
clutter than the classical model.
</summary>
    <author>
      <name>Marcelo Cicconet</name>
    </author>
    <author>
      <name>Davi Geiger</name>
    </author>
    <author>
      <name>Michael Werman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper has been withdrawn since the authors concluded a more
  comprehensive study on the choice of parameters needs to be performed</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.00561v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00561v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02506v1</id>
    <updated>2015-02-09T14:46:40Z</updated>
    <published>2015-02-09T14:46:40Z</published>
    <title>Predicting Alzheimer's disease: a neuroimaging study with 3D
  convolutional neural networks</title>
    <summary>  Pattern recognition methods using neuroimaging data for the diagnosis of
Alzheimer's disease have been the subject of extensive research in recent
years. In this paper, we use deep learning methods, and in particular sparse
autoencoders and 3D convolutional neural networks, to build an algorithm that
can predict the disease status of a patient, based on an MRI scan of the brain.
We report on experiments using the ADNI data set involving 2,265 historical
scans. We demonstrate that 3D convolutional neural networks outperform several
other classifiers reported in the literature and produce state-of-art results.
</summary>
    <author>
      <name>Adrien Payan</name>
    </author>
    <author>
      <name>Giovanni Montana</name>
    </author>
    <link href="http://arxiv.org/abs/1502.02506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02905v1</id>
    <updated>2015-02-10T13:37:49Z</updated>
    <published>2015-02-10T13:37:49Z</published>
    <title>Real Time Implementation of Spatial Filtering On FPGA</title>
    <summary>  Field Programmable Gate Array (FPGA) technology has gained vital importance
mainly because of its parallel processing hardware which makes it ideal for
image and video processing. In this paper, a step by step approach to apply a
linear spatial filter on real time video frame sent by Omnivision OV7670 camera
using Zynq Evaluation and Development board based on Xilinx XC7Z020 has been
discussed. Face detection application was chosen to explain above procedure.
This procedure is applicable to most of the complex image processing algorithms
which needs to be implemented using FPGA.
</summary>
    <author>
      <name>Chaitannya Supe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.02905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.04623v2</id>
    <updated>2015-05-20T15:29:42Z</updated>
    <published>2015-02-16T16:48:56Z</published>
    <title>DRAW: A Recurrent Neural Network For Image Generation</title>
    <summary>  This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural
network architecture for image generation. DRAW networks combine a novel
spatial attention mechanism that mimics the foveation of the human eye, with a
sequential variational auto-encoding framework that allows for the iterative
construction of complex images. The system substantially improves on the state
of the art for generative models on MNIST, and, when trained on the Street View
House Numbers dataset, it generates images that cannot be distinguished from
real data with the naked eye.
</summary>
    <author>
      <name>Karol Gregor</name>
    </author>
    <author>
      <name>Ivo Danihelka</name>
    </author>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Danilo Jimenez Rezende</name>
    </author>
    <author>
      <name>Daan Wierstra</name>
    </author>
    <link href="http://arxiv.org/abs/1502.04623v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04623v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05212v1</id>
    <updated>2015-02-18T13:11:46Z</updated>
    <published>2015-02-18T13:11:46Z</published>
    <title>IAT - Image Annotation Tool: Manual</title>
    <summary>  The annotation of image and video data of large datasets is a fundamental
task in multimedia information retrieval and computer vision applications. In
order to support the users during the image and video annotation process,
several software tools have been developed to provide them with a graphical
environment which helps drawing object contours, handling tracking information
and specifying object metadata. Here we introduce a preliminary version of the
image annotation tools developed at the Imaging and Vision Laboratory.
</summary>
    <author>
      <name>Gianluigi Ciocca</name>
    </author>
    <author>
      <name>Paolo Napoletano</name>
    </author>
    <author>
      <name>Raimondo Schettini</name>
    </author>
    <link href="http://arxiv.org/abs/1502.05212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05224v2</id>
    <updated>2015-02-25T12:13:47Z</updated>
    <published>2015-02-18T13:41:23Z</published>
    <title>Cross-Modality Hashing with Partial Correspondence</title>
    <summary>  Learning a hashing function for cross-media search is very desirable due to
its low storage cost and fast query speed. However, the data crawled from
Internet cannot always guarantee good correspondence among different modalities
which affects the learning for hashing function. In this paper, we focus on
cross-modal hashing with partially corresponded data. The data without full
correspondence are made in use to enhance the hashing performance. The
experiments on Wiki and NUS-WIDE datasets demonstrates that the proposed method
outperforms some state-of-the-art hashing approaches with fewer correspondence
information.
</summary>
    <author>
      <name>Yun Gu</name>
    </author>
    <author>
      <name>Haoyang Xue</name>
    </author>
    <author>
      <name>Jie Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1502.05224v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05224v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.06807v2</id>
    <updated>2016-12-02T15:41:25Z</updated>
    <published>2015-02-24T13:39:55Z</published>
    <title>Hands Deep in Deep Learning for Hand Pose Estimation</title>
    <summary>  We introduce and evaluate several architectures for Convolutional Neural
Networks to predict the 3D joint locations of a hand given a depth map. We
first show that a prior on the 3D pose can be easily introduced and
significantly improves the accuracy and reliability of the predictions. We also
show how to use context efficiently to deal with ambiguities between fingers.
These two contributions allow us to significantly outperform the
state-of-the-art on several challenging benchmarks, both in terms of accuracy
and computation times.
</summary>
    <author>
      <name>Markus Oberweger</name>
    </author>
    <author>
      <name>Paul Wohlhart</name>
    </author>
    <author>
      <name>Vincent Lepetit</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">added link to source https://github.com/moberweger/deep-prior</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of 20th Computer Vision Winter Workshop (CVWW)
  2015, pp. 21-30</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1502.06807v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.06807v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.01420v1</id>
    <updated>2015-04-06T21:27:45Z</updated>
    <published>2015-04-06T21:27:45Z</published>
    <title>Knowledge driven Offline to Online Script Conversion</title>
    <summary>  The problem of offline to online script conversion is a challenging and an
ill-posed problem. The interest in offline to online conversion exists because
there are a plethora of robust algorithms in online script literature which can
not be used on offline scripts. In this paper, we propose a method, based on
heuristics, to extract online script information from offline bitmap image. We
show the performance of the proposed method on a real sample signature offline
image, whose online information is known.
</summary>
    <author>
      <name>Sunil Kopparapu</name>
    </author>
    <author>
      <name> Devanuj</name>
    </author>
    <author>
      <name>Akhilesh Srivastava</name>
    </author>
    <author>
      <name>P. V. S. Rao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures, KBCS 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.01420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.01420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.01488v1</id>
    <updated>2015-04-07T06:31:58Z</updated>
    <published>2015-04-07T06:31:58Z</published>
    <title>On-line Handwritten Devanagari Character Recognition using Fuzzy
  Directional Features</title>
    <summary>  This paper describes a new feature set for use in the recognition of on-line
handwritten Devanagari script based on Fuzzy Directional Features. Experiments
are conducted for the automatic recognition of isolated handwritten character
primitives (sub-character units). Initially we describe the proposed feature
set, called the Fuzzy Directional Features (FDF) and then show how these
features can be effectively utilized for writer independent character
recognition. Experimental results show that FDF set perform well for writer
independent data set at stroke level recognition. The main contribution of this
paper is the introduction of a novel feature set and establish experimentally
its ability in recognition of handwritten Devanagari script.
</summary>
    <author>
      <name>Sunil Kumar Kopparapu</name>
    </author>
    <author>
      <name>Lajish VL</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages; 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.01488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.01488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.01800v2</id>
    <updated>2015-04-14T12:27:57Z</updated>
    <published>2015-04-08T02:02:55Z</published>
    <title>A Multicomponent Approach to Nonrigid Registration of Diffusion Tensor
  Images</title>
    <summary>  We propose a nonrigid registration approach for diffusion tensor images using
a multicomponent information-theoretic measure. Explicit orientation
optimization is enabled by incorporating tensor reorientation, which is
necessary for wrapping diffusion tensor images. Experimental results on
diffusion tensor images indicate the feasibility of the proposed approach and a
much better performance compared to the affine registration method based on
mutual information in terms of registration accuracy in the presence of
geometric distortion.
</summary>
    <author>
      <name>Mohammed Khader</name>
    </author>
    <author>
      <name>A. Ben Hamza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.01800v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.01800v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.01954v1</id>
    <updated>2015-04-08T13:22:13Z</updated>
    <published>2015-04-08T13:22:13Z</published>
    <title>Image Subset Selection Using Gabor Filters and Neural Networks</title>
    <summary>  An automatic method for the selection of subsets of images, both modern and
historic, out of a set of landmark large images collected from the Internet is
presented in this paper. This selection depends on the extraction of dominant
features using Gabor filtering. Features are selected carefully from a
preliminary image set and fed into a neural network as a training data. The
method collects a large set of raw landmark images containing modern and
historic landmark images and non-landmark images. The method then processes
these images to classify them as landmark and non-landmark images. The
classification performance highly depends on the number of candidate features
of the landmark.
</summary>
    <author>
      <name>Heider K. Ali</name>
    </author>
    <author>
      <name>Anthony Whitehead</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.01954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.01954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.01989v1</id>
    <updated>2015-04-08T14:44:20Z</updated>
    <published>2015-04-08T14:44:20Z</published>
    <title>Pixel-wise Deep Learning for Contour Detection</title>
    <summary>  We address the problem of contour detection via per-pixel classifications of
edge point. To facilitate the process, the proposed approach leverages with
DenseNet, an efficient implementation of multiscale convolutional neural
networks (CNNs), to extract an informative feature vector for each pixel and
uses an SVM classifier to accomplish contour detection. In the experiment of
contour detection, we look into the effectiveness of combining per-pixel
features from different CNN layers and verify their performance on BSDS500.
</summary>
    <author>
      <name>Jyh-Jing Hwang</name>
    </author>
    <author>
      <name>Tyng-Luh Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages. arXiv admin note: substantial text overlap with
  arXiv:1412.6857</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.01989v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.01989v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.02174v2</id>
    <updated>2015-12-23T15:48:27Z</updated>
    <published>2015-04-09T02:05:02Z</published>
    <title>Connectivity Preserving Multivalued Functions in Digital Topology</title>
    <summary>  We study connectivity preserving multivalued functions between digital
images. This notion generalizes that of continuous multivalued functions
studied mostly in the setting of the digital plane $Z^2$. We show that
connectivity preserving multivalued functions, like continuous multivalued
functions, are appropriate models for digital morpholological operations.
Connectivity preservation, unlike continuity, is preserved by compositions, and
generalizes easily to higher dimensions and arbitrary adjacency relations.
</summary>
    <author>
      <name>Laurence Boxer</name>
    </author>
    <author>
      <name>P. Christopher Staecker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">small changes</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.02174v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02174v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.02235v1</id>
    <updated>2015-04-09T09:53:44Z</updated>
    <published>2015-04-09T09:53:44Z</published>
    <title>Extraction of Protein Sequence Motif Information using PSO K-Means</title>
    <summary>  The main objective of the paper is to find the motif information.The
functionalities of the proteins are ideally found from their motif information
which is extracted using various techniques like clustering with k-means,
hybrid k-means, self-organising maps, etc., in the literature. In this work
protein sequence information is extracted using optimised k-means algorithm.
The particle swarm optimisation technique is one of the frequently used
optimisation method. In the current work the PSO k-means is used for motif
information extraction. This paper also deals with the comparison between the
motif information obtained from clusters and biclustersusing PSO k-means
algorithm. The motif information acquired is based on the structure homogeneity
of the protein sequence.
</summary>
    <author>
      <name>R. Gowri</name>
    </author>
    <author>
      <name>R. Rathipriya</name>
    </author>
    <link href="http://arxiv.org/abs/1504.02235v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02235v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.02485v1</id>
    <updated>2015-04-09T20:02:03Z</updated>
    <published>2015-04-09T20:02:03Z</published>
    <title>What Do Deep CNNs Learn About Objects?</title>
    <summary>  Deep convolutional neural networks learn extremely powerful image
representations, yet most of that power is hidden in the millions of deep-layer
parameters. What exactly do these parameters represent? Recent work has started
to analyse CNN representations, finding that, e.g., they are invariant to some
2D transformations Fischer et al. (2014), but are confused by particular types
of image noise Nguyen et al. (2014). In this work, we delve deeper and ask: how
invariant are CNNs to object-class variations caused by 3D shape, pose, and
photorealism?
</summary>
    <author>
      <name>Xingchao Peng</name>
    </author>
    <author>
      <name>Baochen Sun</name>
    </author>
    <author>
      <name>Karim Ali</name>
    </author>
    <author>
      <name>Kate Saenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages workshop paper. arXiv admin note: substantial text overlap
  with arXiv:1412.7122</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.02485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.02518v2</id>
    <updated>2015-04-15T23:08:30Z</updated>
    <published>2015-04-09T23:26:26Z</published>
    <title>Unsupervised Feature Learning from Temporal Data</title>
    <summary>  Current state-of-the-art classification and detection algorithms rely on
supervised training. In this work we study unsupervised feature learning in the
context of temporally coherent video data. We focus on feature learning from
unlabeled video data, using the assumption that adjacent video frames contain
semantically similar information. This assumption is exploited to train a
convolutional pooling auto-encoder regularized by slowness and sparsity. We
establish a connection between slow feature learning to metric learning and
show that the trained encoder can be used to define a more temporally and
semantically coherent metric.
</summary>
    <author>
      <name>Ross Goroshin</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Jonathan Tompson</name>
    </author>
    <author>
      <name>David Eigen</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1412.6056</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.02518v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02518v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03083v2</id>
    <updated>2015-04-28T17:24:00Z</updated>
    <published>2015-04-13T07:36:08Z</published>
    <title>Joint Learning of Distributed Representations for Images and Texts</title>
    <summary>  This technical report provides extra details of the deep multimodal
similarity model (DMSM) which was proposed in (Fang et al. 2015,
arXiv:1411.4952). The model is trained via maximizing global semantic
similarity between images and their captions in natural language using the
public Microsoft COCO database, which consists of a large set of images and
their corresponding captions. The learned representations attempt to capture
the combination of various visual concepts and cues.
</summary>
    <author>
      <name>Xiaodong He</name>
    </author>
    <author>
      <name>Rupesh Srivastava</name>
    </author>
    <author>
      <name>Jianfeng Gao</name>
    </author>
    <author>
      <name>Li Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a previous tech report of a part of the work of
  arXiv:1411.4952. In order to avoid confusion, we'd like to withdraw this
  report from arXiv</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.03083v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03083v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03285v1</id>
    <updated>2015-04-13T18:17:12Z</updated>
    <published>2015-04-13T18:17:12Z</published>
    <title>Multiple Measurements and Joint Dimensionality Reduction for Large Scale
  Image Search with Short Vectors - Extended Version</title>
    <summary>  This paper addresses the construction of a short-vector (128D) image
representation for large-scale image and particular object retrieval. In
particular, the method of joint dimensionality reduction of multiple
vocabularies is considered. We study a variety of vocabulary generation
techniques: different k-means initializations, different descriptor
transformations, different measurement regions for descriptor extraction. Our
extensive evaluation shows that different combinations of vocabularies, each
partitioning the descriptor space in a different yet complementary manner,
results in a significant performance improvement, which exceeds the
state-of-the-art.
</summary>
    <author>
      <name>Filip Radenovic</name>
    </author>
    <author>
      <name>Herve Jegou</name>
    </author>
    <author>
      <name>Ondrej Chum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of the ICMR 2015 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.03285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03641v1</id>
    <updated>2015-04-14T17:53:51Z</updated>
    <published>2015-04-14T17:53:51Z</published>
    <title>Learning to Compare Image Patches via Convolutional Neural Networks</title>
    <summary>  In this paper we show how to learn directly from image data (i.e., without
resorting to manually-designed features) a general similarity function for
comparing image patches, which is a task of fundamental importance for many
computer vision problems. To encode such a function, we opt for a CNN-based
model that is trained to account for a wide variety of changes in image
appearance. To that end, we explore and study multiple neural network
architectures, which are specifically adapted to this task. We show that such
an approach can significantly outperform the state-of-the-art on several
problems and benchmark datasets.
</summary>
    <author>
      <name>Sergey Zagoruyko</name>
    </author>
    <author>
      <name>Nikos Komodakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.03641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.04090v1</id>
    <updated>2015-04-16T03:18:28Z</updated>
    <published>2015-04-16T03:18:28Z</published>
    <title>Segmentation of Subspaces in Sequential Data</title>
    <summary>  We propose Ordered Subspace Clustering (OSC) to segment data drawn from a
sequentially ordered union of subspaces. Similar to Sparse Subspace Clustering
(SSC) we formulate the problem as one of finding a sparse representation but
include an additional penalty term to take care of sequential data. We test our
method on data drawn from infrared hyper spectral, video and motion capture
data. Experiments show that our method, OSC, outperforms the state of the art
methods: Spatial Subspace Clustering (SpatSC), Low-Rank Representation (LRR)
and SSC.
</summary>
    <author>
      <name>Stephen Tierney</name>
    </author>
    <author>
      <name>Yi Guo</name>
    </author>
    <author>
      <name>Junbin Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1504.04090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.04090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.06740v1</id>
    <updated>2015-04-25T15:26:38Z</updated>
    <published>2015-04-25T15:26:38Z</published>
    <title>SIFT Vs SURF: Quantifying the Variation in Transformations</title>
    <summary>  This paper studies the robustness of SIFT and SURF against different image
transforms (rigid body, similarity, affine and projective) by quantitatively
analyzing the variations in the extent of transformations. Previous studies
have been comparing the two techniques on absolute transformations rather than
the specific amount of deformation caused by the transformation. The paper
establishes an exhaustive empirical analysis of such deformations and matching
capability of SIFT and SURF with variations in matching parameters and the
amount of tolerance. This is helpful in choosing the specific use case for
applying these techniques.
</summary>
    <author>
      <name>Siddharth Srivastava</name>
    </author>
    <link href="http://arxiv.org/abs/1504.06740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.06897v1</id>
    <updated>2015-04-27T00:46:54Z</updated>
    <published>2015-04-27T00:46:54Z</published>
    <title>Linear Spatial Pyramid Matching Using Non-convex and non-negative Sparse
  Coding for Image Classification</title>
    <summary>  Recently sparse coding have been highly successful in image classification
mainly due to its capability of incorporating the sparsity of image
representation. In this paper, we propose an improved sparse coding model based
on linear spatial pyramid matching(SPM) and Scale Invariant Feature Transform
(SIFT ) descriptors. The novelty is the simultaneous non-convex and
non-negative characters added to the sparse coding model. Our numerical
experiments show that the improved approach using non-convex and non-negative
sparse coding is superior than the original ScSPM[1] on several typical
databases.
</summary>
    <author>
      <name>Chengqiang Bao</name>
    </author>
    <author>
      <name>Liangtian He</name>
    </author>
    <author>
      <name>Yilun Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1504.06897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T45" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.07259v1</id>
    <updated>2015-04-27T20:08:37Z</updated>
    <published>2015-04-27T20:08:37Z</published>
    <title>Image Segmentation and Restoration Using Parametric Contours With Free
  Endpoints</title>
    <summary>  In this paper, we introduce a novel approach for active contours with free
endpoints. A scheme is presented for image segmentation and restoration based
on a discrete version of the Mumford-Shah functional where the contours can be
both closed and open curves. Additional to a flow of the curves in normal
direction, evolution laws for the tangential flow of the endpoints are derived.
Using a parametric approach to describe the evolving contours together with an
edge-preserving denoising, we obtain a fast method for image segmentation and
restoration. The analytical and numerical schemes are presented followed by
numerical experiments with artificial test images and with a real medical
image.
</summary>
    <author>
      <name>Heike Benninghoff</name>
    </author>
    <author>
      <name>Harald Garcke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2016.2529180</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2016.2529180" rel="related"/>
    <link href="http://arxiv.org/abs/1504.07259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.07284v1</id>
    <updated>2015-04-27T21:41:01Z</updated>
    <published>2015-04-27T21:41:01Z</published>
    <title>Mid-level Elements for Object Detection</title>
    <summary>  Building on the success of recent discriminative mid-level elements, we
propose a surprisingly simple approach for object detection which performs
comparable to the current state-of-the-art approaches on PASCAL VOC comp-3
detection challenge (no external data). Through extensive experiments and
ablation analysis, we show how our approach effectively improves upon the
HOG-based pipelines by adding an intermediate mid-level representation for the
task of object detection. This representation is easily interpretable and
allows us to visualize what our object detector "sees". We also discuss the
insights our approach shares with CNN-based methods, such as sharing
representation between categories helps.
</summary>
    <author>
      <name>Aayush Bansal</name>
    </author>
    <author>
      <name>Abhinav Shrivastava</name>
    </author>
    <author>
      <name>Carl Doersch</name>
    </author>
    <author>
      <name>Abhinav Gupta</name>
    </author>
    <link href="http://arxiv.org/abs/1504.07284v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07284v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.07858v1</id>
    <updated>2015-04-29T13:51:04Z</updated>
    <published>2015-04-29T13:51:04Z</published>
    <title>Intelligent Health Recommendation System for Computer Users</title>
    <summary>  The time people spend in front of computers has been increasing steadily due
to the role computers play in modern society. Individuals who sit in front of
computers for an extended period of time, specifically with improper postures
may incur various health issues. In this work, individuals' behaviors in front
of computers are studied using web cameras. By means of non-rigid face tracking
system, data are analyzed to determine the 3D head pose, blink rate and yawn
frequency of computer users. When combining these visual cues, a system of
intelligent personal assistants for computer users is proposed.
</summary>
    <author>
      <name>Qi Guo</name>
    </author>
    <author>
      <name>Zixuan Wang</name>
    </author>
    <author>
      <name>Ming Li</name>
    </author>
    <author>
      <name>Hamid Aghajan</name>
    </author>
    <link href="http://arxiv.org/abs/1504.07858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.07967v1</id>
    <updated>2015-04-29T19:01:30Z</updated>
    <published>2015-04-29T19:01:30Z</published>
    <title>Improved repeatability measures for evaluating performance of feature
  detectors</title>
    <summary>  The most frequently employed measure for performance characterisation of
local feature detectors is repeatability, but it has been observed that this
does not necessarily mirror actual performance. Presented are improved
repeatability formulations which correlate much better with the true
performance of feature detectors. Comparative results for several
state-of-the-art feature detectors are presented using these measures; it is
found that Hessian-based detectors are generally superior at identifying
features when images are subject to various geometric and photometric
transformations.
</summary>
    <author>
      <name>Shoaib Ehsan</name>
    </author>
    <author>
      <name>Nadia Kanwal</name>
    </author>
    <author>
      <name>Adrian F. Clark</name>
    </author>
    <author>
      <name>Klaus D. McDonald-Maier</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronics Letters 8th July 2010 Vol. 46 No. 14</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1504.07967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.08083v2</id>
    <updated>2015-09-27T15:10:14Z</updated>
    <published>2015-04-30T05:13:08Z</published>
    <title>Fast R-CNN</title>
    <summary>  This paper proposes a Fast Region-based Convolutional Network method (Fast
R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently
classify object proposals using deep convolutional networks. Compared to
previous work, Fast R-CNN employs several innovations to improve training and
testing speed while also increasing detection accuracy. Fast R-CNN trains the
very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and
achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains
VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is
implemented in Python and C++ (using Caffe) and is available under the
open-source MIT License at https://github.com/rbgirshick/fast-rcnn.
</summary>
    <author>
      <name>Ross Girshick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICCV 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.08083v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.08083v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.08200v4</id>
    <updated>2015-11-23T21:48:15Z</updated>
    <published>2015-04-30T12:54:39Z</published>
    <title>Predicting People's 3D Poses from Short Sequences</title>
    <summary>  We propose an efficient approach to exploiting motion information from
consecutive frames of a video sequence to recover the 3D pose of people.
Instead of computing candidate poses in individual frames and then linking
them, as is often done, we regress directly from a spatio-temporal block of
frames to a 3D pose in the central one. We will demonstrate that this approach
allows us to effectively overcome ambiguities and to improve upon the
state-of-the-art on challenging sequences.
</summary>
    <author>
      <name>Bugra Tekin</name>
    </author>
    <author>
      <name>Xiaolu Sun</name>
    </author>
    <author>
      <name>Xinchao Wang</name>
    </author>
    <author>
      <name>Vincent Lepetit</name>
    </author>
    <author>
      <name>Pascal Fua</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">superseded by arXiv:1511.06692</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.08200v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.08200v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00066v2</id>
    <updated>2015-09-28T18:17:20Z</updated>
    <published>2015-05-01T01:45:52Z</published>
    <title>Pose Induction for Novel Object Categories</title>
    <summary>  We address the task of predicting pose for objects of unannotated object
categories from a small seed set of annotated object classes. We present a
generalized classifier that can reliably induce pose given a single instance of
a novel category. In case of availability of a large collection of novel
instances, our approach then jointly reasons over all instances to improve the
initial estimates. We empirically validate the various components of our
algorithm and quantitatively show that our method produces reliable pose
estimates. We also show qualitative results on a diverse set of classes and
further demonstrate the applicability of our system for learning shape models
of novel object classes.
</summary>
    <author>
      <name>Shubham Tulsiani</name>
    </author>
    <author>
      <name>João Carreira</name>
    </author>
    <author>
      <name>Jitendra Malik</name>
    </author>
    <link href="http://arxiv.org/abs/1505.00066v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00066v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00171v1</id>
    <updated>2015-05-01T12:55:32Z</updated>
    <published>2015-05-01T12:55:32Z</published>
    <title>SynthCam3D: Semantic Understanding With Synthetic Indoor Scenes</title>
    <summary>  We are interested in automatic scene understanding from geometric cues. To
this end, we aim to bring semantic segmentation in the loop of real-time
reconstruction. Our semantic segmentation is built on a deep autoencoder stack
trained exclusively on synthetic depth data generated from our novel 3D scene
library, SynthCam3D. Importantly, our network is able to segment real world
scenes without any noise modelling. We present encouraging preliminary results.
</summary>
    <author>
      <name>Ankur Handa</name>
    </author>
    <author>
      <name>Viorica Patraucean</name>
    </author>
    <author>
      <name>Vijay Badrinarayanan</name>
    </author>
    <author>
      <name>Simon Stent</name>
    </author>
    <author>
      <name>Roberto Cipolla</name>
    </author>
    <link href="http://arxiv.org/abs/1505.00171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00249v1</id>
    <updated>2015-05-01T19:14:39Z</updated>
    <published>2015-05-01T19:14:39Z</published>
    <title>Image Segmentation by Size-Dependent Single Linkage Clustering of a
  Watershed Basin Graph</title>
    <summary>  We present a method for hierarchical image segmentation that defines a
disaffinity graph on the image, over-segments it into watershed basins, defines
a new graph on the basins, and then merges basins with a modified,
size-dependent version of single linkage clustering. The quasilinear runtime of
the method makes it suitable for segmenting large images. We illustrate the
method on the challenging problem of segmenting 3D electron microscopic brain
images.
</summary>
    <author>
      <name>Aleksandar Zlateski</name>
    </author>
    <author>
      <name>H. Sebastian Seung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.00249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00432v1</id>
    <updated>2015-05-03T14:06:08Z</updated>
    <published>2015-05-03T14:06:08Z</published>
    <title>Object Class Detection and Classification using Multi Scale Gradient and
  Corner Point based Shape Descriptors</title>
    <summary>  This paper presents a novel multi scale gradient and a corner point based
shape descriptors. The novel multi scale gradient based shape descriptor is
combined with generic Fourier descriptors to extract contour and region based
shape information. Shape information based object class detection and
classification technique with a random forest classifier has been optimized.
Proposed integrated descriptor in this paper is robust to rotation, scale,
translation, affine deformations, noisy contours and noisy shapes. The new
corner point based interpolated shape descriptor has been exploited for fast
object detection and classification with higher accuracy.
</summary>
    <author>
      <name>Basura Fernando</name>
    </author>
    <author>
      <name>Sezer Karaoglu</name>
    </author>
    <author>
      <name>Sajib Kumar Saha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.00432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00737v1</id>
    <updated>2015-05-04T18:05:52Z</updated>
    <published>2015-05-04T18:05:52Z</published>
    <title>A Gaussian Scale Space Approach For Exudates Detection, Classification
  And Severity Prediction</title>
    <summary>  In the context of Computer Aided Diagnosis system for diabetic retinopathy,
we present a novel method for detection of exudates and their classification
for disease severity prediction. The method is based on Gaussian scale space
based interest map and mathematical morphology. It makes use of support vector
machine for classification and location information of the optic disc and the
macula region for severity prediction. It can efficiently handle luminance
variation and it is suitable for varied sized exudates. The method has been
probed in publicly available DIARETDB1V2 and e-ophthaEX databases. For exudate
detection the proposed method achieved a sensitivity of 96.54% and prediction
of 98.35% in DIARETDB1V2 database.
</summary>
    <author>
      <name>Mrinal Haloi</name>
    </author>
    <author>
      <name>Samarendra Dandapat</name>
    </author>
    <author>
      <name>Rohit Sinha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in ICIP 2015, Quebec city, Canada</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.00737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T45" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00996v1</id>
    <updated>2015-05-05T13:10:53Z</updated>
    <published>2015-05-05T13:10:53Z</published>
    <title>Fast Guided Filter</title>
    <summary>  The guided filter is a technique for edge-aware image filtering. Because of
its nice visual quality, fast speed, and ease of implementation, the guided
filter has witnessed various applications in real products, such as image
editing apps in phones and stereo reconstruction, and has been included in
official MATLAB and OpenCV. In this note, we remind that the guided filter can
be simply sped up from O(N) time to O(N/s^2) time for a subsampling ratio s. In
a variety of applications, this leads to a speedup of &gt;10x with almost no
visible degradation. We hope this acceleration will improve performance of
current applications and further popularize this filter. Code is released.
</summary>
    <author>
      <name>Kaiming He</name>
    </author>
    <author>
      <name>Jian Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.00996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.01936v1</id>
    <updated>2015-05-08T06:15:42Z</updated>
    <published>2015-05-08T06:15:42Z</published>
    <title>Noise in Structured-Light Stereo Depth Cameras: Modeling and its
  Applications</title>
    <summary>  Depth maps obtained from commercially available structured-light stereo based
depth cameras, such as the Kinect, are easy to use but are affected by
significant amounts of noise. This paper is devoted to a study of the intrinsic
noise characteristics of such depth maps, i.e. the standard deviation of noise
in estimated depth varies quadratically with the distance of the object from
the depth camera. We validate this theoretical model against empirical
observations and demonstrate the utility of this noise model in three popular
applications: depth map denoising, volumetric scan merging for 3D modeling, and
identification of 3D planes in depth maps.
</summary>
    <author>
      <name>Avishek Chatterjee</name>
    </author>
    <author>
      <name>Venu Madhav Govindu</name>
    </author>
    <link href="http://arxiv.org/abs/1505.01936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.01936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.02247v1</id>
    <updated>2015-05-09T09:07:08Z</updated>
    <published>2015-05-09T09:07:08Z</published>
    <title>Performance Evaluation of Vision-Based Algorithms for MAVs</title>
    <summary>  An important focus of current research in the field of Micro Aerial Vehicles
(MAVs) is to increase the safety of their operation in general unstructured
environments. Especially indoors, where GPS cannot be used for localization,
reliable algorithms for localization and mapping of the environment are
necessary in order to keep an MAV airborne safely. In this paper, we compare
vision-based real-time capable methods for localization and mapping and point
out their strengths and weaknesses. Additionally, we describe algorithms for
state estimation, control and navigation, which use the localization and
mapping results of our vision-based algorithms as input.
</summary>
    <author>
      <name>T. Holzmann</name>
    </author>
    <author>
      <name>R. Prettenthaler</name>
    </author>
    <author>
      <name>J. Pestana</name>
    </author>
    <author>
      <name>D. Muschick</name>
    </author>
    <author>
      <name>G. Graber</name>
    </author>
    <author>
      <name>C. Mostegel</name>
    </author>
    <author>
      <name>F. Fraundorfer</name>
    </author>
    <author>
      <name>H. Bischof</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at OAGM Workshop, 2015 (arXiv:1505.01065)</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.02247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.02247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.02496v1</id>
    <updated>2015-05-11T06:26:46Z</updated>
    <published>2015-05-11T06:26:46Z</published>
    <title>Training Deeper Convolutional Networks with Deep Supervision</title>
    <summary>  One of the most promising ways of improving the performance of deep
convolutional neural networks is by increasing the number of convolutional
layers. However, adding layers makes training more difficult and
computationally expensive. In order to train deeper networks, we propose to add
auxiliary supervision branches after certain intermediate layers during
training. We formulate a simple rule of thumb to determine where these branches
should be added. The resulting deeply supervised structure makes the training
much easier and also produces better classification results on ImageNet and the
recently released, larger MIT Places dataset
</summary>
    <author>
      <name>Liwei Wang</name>
    </author>
    <author>
      <name>Chen-Yu Lee</name>
    </author>
    <author>
      <name>Zhuowen Tu</name>
    </author>
    <author>
      <name>Svetlana Lazebnik</name>
    </author>
    <link href="http://arxiv.org/abs/1505.02496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.02496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03344v1</id>
    <updated>2015-05-13T11:56:01Z</updated>
    <published>2015-05-13T11:56:01Z</published>
    <title>A Framework for Fast Face and Eye Detection</title>
    <summary>  Face detection is an essential step in many computer vision applications like
surveillance, tracking, medical analysis, facial expression analysis etc.
Several approaches have been made in the direction of face detection. Among
them, Haar-like features based method is a robust method. In spite of the
robustness, Haar - like features work with some limitations. However, with some
simple modifications in the algorithm, its performance can be made faster and
more robust. The present work refers to the increase in speed of operation of
the original algorithm by down sampling the frames and its analysis with
different scale factors. It also discusses the detection of tilted faces using
an affine transformation of the input image.
</summary>
    <author>
      <name>Anjith George</name>
    </author>
    <author>
      <name>Anirban Dasgupta</name>
    </author>
    <author>
      <name>Aurobinda Routray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages , 10 figures,</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.03344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03493v2</id>
    <updated>2015-05-14T13:08:15Z</updated>
    <published>2015-05-13T18:45:07Z</published>
    <title>Modified Hausdorff Fractal Dimension (MHFD)</title>
    <summary>  The Hausdorff fractal dimension has been a fast-to-calculate method to
estimate complexity of fractal shapes. In this work, a modified version of this
fractal dimension is presented in order to make it more robust when applied in
estimating complexity of non-fractal images. The modified Hausdorff fractal
dimension stands on two features that weaken the requirement of presence of a
shape and also reduce the impact of the noise possibly presented in the input
image. The new algorithm has been evaluated on a set of images of different
character with promising performance.
</summary>
    <author>
      <name>Reza Farrahi Moghaddam</name>
    </author>
    <author>
      <name>Mohamed Cheriet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures, 2 algorithms. Working Paper WP-RFM-15-02,
  (version: 150507)</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.03493v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03493v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03505v3</id>
    <updated>2017-01-28T20:57:56Z</updated>
    <published>2015-05-13T19:38:57Z</published>
    <title>On a spatial-temporal decomposition of the optical flow</title>
    <summary>  In this paper we present a decomposition algorithm for computation of the
spatial-temporal optical flow of a dynamic image sequence. We consider several
applications, such as the extraction of temporal motion features and motion
detection in dynamic sequences under varying illumination conditions, such as
they appear for instance in psychological flickering experiments. For the
numerical implementation we are solving an integro-differential equation by a
fixed point iteration. For comparison purposes we use a standard time dependent
optical flow algorithm, which in contrast to our method, constitutes in solving
a spatial-temporal differential equation.
</summary>
    <author>
      <name>Aniello Raffale Patrone</name>
    </author>
    <author>
      <name>Otmar Scherzer</name>
    </author>
    <link href="http://arxiv.org/abs/1505.03505v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03505v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03578v1</id>
    <updated>2015-05-14T00:22:35Z</updated>
    <published>2015-05-14T00:22:35Z</published>
    <title>Vanishing Point Attracts Eye Movements in Scene Free-viewing</title>
    <summary>  Eye movements are crucial in understanding complex scenes. By predicting
where humans look in natural scenes, we can understand how they percieve scenes
and priotriaze information for further high-level processing. Here, we study
the effect of a particular type of scene structural information known as
vanishing point and show that human gaze is attracted to vanishing point
regions. We then build a combined model of traditional saliency and vanishing
point channel that outperforms state of the art saliency models.
</summary>
    <author>
      <name>Ali Borji</name>
    </author>
    <author>
      <name>Mengyang Feng</name>
    </author>
    <author>
      <name>Huchuan Lu</name>
    </author>
    <link href="http://arxiv.org/abs/1505.03578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03932v1</id>
    <updated>2015-05-15T00:59:48Z</updated>
    <published>2015-05-15T00:59:48Z</published>
    <title>Using Ensemble Models in the Histological Examination of Tissue
  Abnormalities</title>
    <summary>  Classification models for the automatic detection of abnormalities on
histological samples do exists, with an active debate on the cost associated
with false negative diagnosis (underdiagnosis) and false positive diagnosis
(overdiagnosis). Current models tend to underdiagnose, failing to recognize a
potentially fatal disease.
  The objective of this study is to investigate the possibility of
automatically identifying abnormalities in tissue samples through the use of an
ensemble model on data generated by histological examination and to minimize
the number of false negative cases.
</summary>
    <author>
      <name>Giancarlo Crocetti</name>
    </author>
    <author>
      <name>Michael Coakley</name>
    </author>
    <author>
      <name>Phil Dressner</name>
    </author>
    <author>
      <name>Wanda Kellum</name>
    </author>
    <author>
      <name>Tamba Lamin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 tables, 3 figures. Proceedings of 12th Annual Research
  Day, 2014 - Pace University</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.03932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; I.5.3; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.04260v1</id>
    <updated>2015-05-16T10:28:51Z</updated>
    <published>2015-05-16T10:28:51Z</published>
    <title>The color of smiling: computational synaesthesia of facial expressions</title>
    <summary>  This note gives a preliminary account of the transcoding or rechanneling
problem between different stimuli as it is of interest for the natural
interaction or affective computing fields. By the consideration of a simple
example, namely the color response of an affective lamp to a sensed facial
expression, we frame the problem within an information- theoretic perspective.
A full justification in terms of the Information Bottleneck principle promotes
a latent affective space, hitherto surmised as an appealing and intuitive
solution, as a suitable mediator between the different stimuli.
</summary>
    <author>
      <name>Vittorio Cuculo</name>
    </author>
    <author>
      <name>Raffaella Lanzarotti</name>
    </author>
    <author>
      <name>Giuseppe Boccignone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to: 18th International Conference on Image Analysis and
  Processing (ICIAP 2015), 7-11 September 2015, Genova, Italy</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.04260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.04260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.04424v2</id>
    <updated>2016-07-17T10:07:40Z</updated>
    <published>2015-05-17T17:37:14Z</published>
    <title>Improved Microaneurysm Detection using Deep Neural Networks</title>
    <summary>  In this work, we propose a novel microaneurysm (MA) detection for early
diabetic retinopathy screening using color fundus images. Since MA usually the
first lesions to appear as an indicator of diabetic retinopathy, accurate
detection of MA is necessary for treatment. Each pixel of the image is
classified as either MA or non-MA using a deep neural network with dropout
training procedure using maxout activation function. No preprocessing step or
manual feature extraction is required. Substantial improvements over standard
MA detection method based on the pipeline of preprocessing, feature extraction,
classification followed by post processing is achieved. The presented method is
evaluated in publicly available Retinopathy Online Challenge (ROC) and
Diaretdb1v2 database and achieved state-of-the-art accuracy.
</summary>
    <author>
      <name>Mrinal Haloi</name>
    </author>
    <link href="http://arxiv.org/abs/1505.04424v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.04424v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T45" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.04467v1</id>
    <updated>2015-05-17T22:14:27Z</updated>
    <published>2015-05-17T22:14:27Z</published>
    <title>Exploring Nearest Neighbor Approaches for Image Captioning</title>
    <summary>  We explore a variety of nearest neighbor baseline approaches for image
captioning. These approaches find a set of nearest neighbor images in the
training set from which a caption may be borrowed for the query image. We
select a caption for the query image by finding the caption that best
represents the "consensus" of the set of candidate captions gathered from the
nearest neighbor images. When measured by automatic evaluation metrics on the
MS COCO caption evaluation server, these approaches perform as well as many
recent approaches that generate novel captions. However, human studies show
that a method that generates novel captions is still preferred over the nearest
neighbor approach.
</summary>
    <author>
      <name>Jacob Devlin</name>
    </author>
    <author>
      <name>Saurabh Gupta</name>
    </author>
    <author>
      <name>Ross Girshick</name>
    </author>
    <author>
      <name>Margaret Mitchell</name>
    </author>
    <author>
      <name>C. Lawrence Zitnick</name>
    </author>
    <link href="http://arxiv.org/abs/1505.04467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.04467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.05601v1</id>
    <updated>2015-05-21T04:24:48Z</updated>
    <published>2015-05-21T04:24:48Z</published>
    <title>Unsupervised Segmentation of Overlapping Cervical Cell Cytoplasm</title>
    <summary>  Overlapping of cervical cells and poor contrast of cell cytoplasm are the
major issues in accurate detection and segmentation of cervical cells. An
unsupervised cell segmentation approach is presented here. Cell clump
segmentation was carried out using the extended depth of field (EDF) image
created from the images of different focal planes. A modified Otsu method with
prior class weights is proposed for accurate segmentation of nuclei from the
cell clumps. The cell cytoplasm was further segmented from cell clump depending
upon the number of nucleus detected in that cell clump. Level set model was
used for cytoplasm segmentation.
</summary>
    <author>
      <name>S L Happy</name>
    </author>
    <author>
      <name>Swarnadip Chatterjee</name>
    </author>
    <author>
      <name>Debdoot Sheet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.05601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.05601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.05819v1</id>
    <updated>2015-02-24T13:23:58Z</updated>
    <published>2015-02-24T13:23:58Z</published>
    <title>New HSL Distance Based Colour Clustering Algorithm</title>
    <summary>  In this paper, we define a distance for the HSL colour system. Next, the
proposed distance is used for a fuzzy colour clustering algorithm construction.
The presented algorithm is related to the well-known fuzzy c-means algorithm.
Finally, the clustering algorithm is used as colour reduction method. The
obtained experimental results are presented to demonstrate the effectiveness of
our approach.
</summary>
    <author>
      <name>Vasile Patrascu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/2.1.4990.8007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/2.1.4990.8007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 24th Midwest Artificial Intelligence and Cognitive Sciences
  Conference (MAICS 2013), pp. 85-92, New Albany, Indiana. USA, April 13-14,
  2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.05819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.05819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.06079v2</id>
    <updated>2017-07-12T13:58:54Z</updated>
    <published>2015-05-22T13:48:10Z</published>
    <title>Robust Rotation Synchronization via Low-rank and Sparse Matrix
  Decomposition</title>
    <summary>  This paper deals with the rotation synchronization problem, which arises in
global registration of 3D point-sets and in structure from motion. The problem
is formulated in an unprecedented way as a "low-rank and sparse" matrix
decomposition that handles both outliers and missing data. A minimization
strategy, dubbed R-GoDec, is also proposed and evaluated experimentally against
state-of-the-art algorithms on simulated and real data. The results show that
R-GoDec is the fastest among the robust algorithms.
</summary>
    <author>
      <name>Federica Arrigoni</name>
    </author>
    <author>
      <name>Andrea Fusiello</name>
    </author>
    <author>
      <name>Beatrice Rossi</name>
    </author>
    <author>
      <name>Pasqualina Fragneto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cviu.2018.08.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cviu.2018.08.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The material contained in this paper is part of a manuscript
  submitted to CVIU</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Computer Vision and Image Understanding, 174: 95-113, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1505.06079v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.06079v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.06219v1</id>
    <updated>2015-04-07T01:26:06Z</updated>
    <published>2015-04-07T01:26:06Z</published>
    <title>A comparative study between proposed Hyper Kurtosis based Modified
  Duo-Histogram Equalization (HKMDHE) and Contrast Limited Adaptive Histogram
  Equalization (CLAHE) for Contrast Enhancement Purpose of Low Contrast Human
  Brain CT scan images</title>
    <summary>  In this paper, a comparative study between proposed hyper kurtosis based
modified duo-histogram equalization (HKMDHE) algorithm and contrast limited
adaptive histogram enhancement (CLAHE) has been presented for the
implementation of contrast enhancement and brightness preservation of low
contrast human brain CT scan images. In HKMDHE algorithm, contrast enhancement
is done on the hyper-kurtosis based application. The results are very promising
of proposed HKMDHE technique with improved PSNR values and lesser AMMBE values
than CLAHE technique.
</summary>
    <author>
      <name>Sabyasachi Mukhopadhyay</name>
    </author>
    <author>
      <name>Soham Mandal</name>
    </author>
    <author>
      <name>Sawon Pratiher</name>
    </author>
    <author>
      <name>Satyasaran Changdar</name>
    </author>
    <author>
      <name>Ritwik Burman</name>
    </author>
    <author>
      <name>Nirmalya Ghosh</name>
    </author>
    <author>
      <name>Prasanta K. Panigrahi</name>
    </author>
    <link href="http://arxiv.org/abs/1505.06219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.06219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.06800v1</id>
    <updated>2015-05-26T03:52:52Z</updated>
    <published>2015-05-26T03:52:52Z</published>
    <title>Boosting-like Deep Learning For Pedestrian Detection</title>
    <summary>  This paper proposes boosting-like deep learning (BDL) framework for
pedestrian detection. Due to overtraining on the limited training samples,
overfitting is a major problem of deep learning. We incorporate a boosting-like
technique into deep learning to weigh the training samples, and thus prevent
overtraining in the iterative process. We theoretically give the details of
derivation of our algorithm, and report the experimental results on open data
sets showing that BDL achieves a better stable performance than the
state-of-the-arts. Our approach achieves 15.85% and 3.81% reduction in the
average miss rate compared with ACF and JointDeep on the largest Caltech
benchmark dataset, respectively.
</summary>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Baochang Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages,7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.06800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.06800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.06814v1</id>
    <updated>2015-05-26T06:02:05Z</updated>
    <published>2015-05-26T06:02:05Z</published>
    <title>Discrete Independent Component Analysis (DICA) with Belief Propagation</title>
    <summary>  We apply belief propagation to a Bayesian bipartite graph composed of
discrete independent hidden variables and discrete visible variables. The
network is the Discrete counterpart of Independent Component Analysis (DICA)
and it is manipulated in a factor graph form for inference and learning. A full
set of simulations is reported for character images from the MNIST dataset. The
results show that the factorial code implemented by the sources contributes to
build a good generative model for the data that can be used in various
inference modes.
</summary>
    <author>
      <name>Francesco A. N. Palmieri</name>
    </author>
    <author>
      <name>Amedeo Buonanno</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Sumbitted for publication (May 2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.06814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.06814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.07934v1</id>
    <updated>2015-05-29T06:18:31Z</updated>
    <published>2015-05-29T06:18:31Z</published>
    <title>Symbolic Segmentation Using Algorithm Selection</title>
    <summary>  In this paper we present an alternative approach to symbolic segmentation;
instead of implementing a new method we approach symbolic segmentation as an
algorithm selection problem. That is, let there be $n$ available algorithms for
symbolic segmentation, a selection mechanism forms a set of input features and
image attributes and selects on a case by case basis the best algorithm. The
selection mechanism is demonstrated from within an algorithm framework where
the selection is done in a set of various algorithm networks. Two sets of
experiments are performed and in both cases we demonstrate that the algorithm
selection allows to increase the result of the symbolic segmentation by a
considerable amount.
</summary>
    <author>
      <name>Martin Lukac</name>
    </author>
    <author>
      <name>Kamila Abdiyeva</name>
    </author>
    <author>
      <name>Michitaka Kameyama</name>
    </author>
    <link href="http://arxiv.org/abs/1505.07934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.07934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.08070v2</id>
    <updated>2022-01-09T19:51:44Z</updated>
    <published>2015-05-29T14:51:18Z</published>
    <title>General Deformations of Point Configurations Viewed By a Pinhole Model
  Camera</title>
    <summary>  This paper is a theoretical study of the following Non-Rigid Structure from
Motion problem. What can be computed from a monocular view of a parametrically
deforming set of points? We treat various variations of this problem for affine
and polynomial deformations with calibrated and uncalibrated cameras. We show
that in general at least three images with quasi-identical two deformations are
needed in order to have a finite set of solutions of the points' structure and
calculate some simple examples.
</summary>
    <author>
      <name>Yirmeyahu Kaminski</name>
    </author>
    <author>
      <name>Michael Werman</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Mathematical Imaging and Vision, 65, 631-643 (2023)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1505.08070v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.08070v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="53Z99, 53A07, 68T45, 14P05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.08071v1</id>
    <updated>2015-05-29T14:51:23Z</updated>
    <published>2015-05-29T14:51:23Z</published>
    <title>Geometry of Graph Edit Distance Spaces</title>
    <summary>  In this paper we study the geometry of graph spaces endowed with a special
class of graph edit distances. The focus is on geometrical results useful for
statistical pattern recognition. The main result is the Graph Representation
Theorem. It states that a graph is a point in some geometrical space, called
orbit space. Orbit spaces are well investigated and easier to explore than the
original graph space. We derive a number of geometrical results from the orbit
space representation, translate them to the graph space, and indicate their
significance and usefulness in statistical pattern recognition.
</summary>
    <author>
      <name>Brijnesh J. Jain</name>
    </author>
    <link href="http://arxiv.org/abs/1505.08071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.08071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.08153v1</id>
    <updated>2015-05-29T19:09:02Z</updated>
    <published>2015-05-29T19:09:02Z</published>
    <title>Feature Representation for Online Signature Verification</title>
    <summary>  Biometrics systems have been used in a wide range of applications and have
improved people authentication. Signature verification is one of the most
common biometric methods with techniques that employ various specifications of
a signature. Recently, deep learning has achieved great success in many fields,
such as image, sounds and text processing. In this paper, deep learning method
has been used for feature extraction and feature selection.
</summary>
    <author>
      <name>Mohsen Fayyaz</name>
    </author>
    <author>
      <name>Mohammad Hajizadeh_Saffar</name>
    </author>
    <author>
      <name>Mohammad Sabokrou</name>
    </author>
    <author>
      <name>Mahmood Fathy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/AISP.2015.7123528</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/AISP.2015.7123528" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures, Submitted to IEEE Transactions on Information
  Forensics and Security</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.08153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.08153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.00097v1</id>
    <updated>2015-05-30T09:50:19Z</updated>
    <published>2015-05-30T09:50:19Z</published>
    <title>A Review of Feature and Data Fusion with Medical Images</title>
    <summary>  The fusion techniques that utilize multiple feature sets to form new features
that are often more robust and contain useful information for future processing
are referred to as feature fusion. The term data fusion is applied to the class
of techniques used for combining decisions obtained from multiple feature sets
to form global decisions. Feature and data fusion interchangeably represent two
important classes of techniques that have proved to be of practical importance
in a wide range of medical imaging problems
</summary>
    <author>
      <name>Alex Pappachen James</name>
    </author>
    <author>
      <name>Belur Dasarathy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Multisensor Data Fusion: From Algorithm and Architecture Design to
  Applications, CRC Press, 2015. arXiv admin note: substantial text overlap
  with arXiv:1401.0166</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.00097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.00097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.00395v1</id>
    <updated>2015-06-01T08:55:54Z</updated>
    <published>2015-06-01T08:55:54Z</published>
    <title>Hierarchical structure-and-motion recovery from uncalibrated images</title>
    <summary>  This paper addresses the structure-and-motion problem, that requires to find
camera motion and 3D struc- ture from point matches. A new pipeline, dubbed
Samantha, is presented, that departs from the prevailing sequential paradigm
and embraces instead a hierarchical approach. This method has several
advantages, like a provably lower computational complexity, which is necessary
to achieve true scalability, and better error containment, leading to more
stability and less drift. Moreover, a practical autocalibration procedure
allows to process images without ancillary information. Experiments with real
data assess the accuracy and the computational efficiency of the method.
</summary>
    <author>
      <name>Roberto Toldo</name>
    </author>
    <author>
      <name>Riccardo Gherardi</name>
    </author>
    <author>
      <name>Michela Farenzena</name>
    </author>
    <author>
      <name>Andrea Fusiello</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cviu.2015.05.011</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cviu.2015.05.011" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in CVIU</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Vision and Image Understanding, Volume 140, November
  2015, Pages 127-143</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.00395v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.00395v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.00761v1</id>
    <updated>2015-06-02T06:12:41Z</updated>
    <published>2015-06-02T06:12:41Z</published>
    <title>Image Retrieval Based on Binary Signature ang S-kGraph</title>
    <summary>  In this paper, we introduce an optimum approach for querying similar images
on large digital-image databases. Our work is based on RBIR (region-based image
retrieval) method which uses multiple regions as the key to retrieval images.
This method significantly improves the accuracy of queries. However, this also
increases the cost of computing. To reduce this expensive computational cost,
we implement binary signature encoder which maps an image to its identification
in binary. In order to fasten the lookup, binary signatures of images are
classified by the help of S-kGraph. Finally, our work is evaluated on COREL's
images.
</summary>
    <author>
      <name>Thanh The Van</name>
    </author>
    <author>
      <name>Thanh Manh Le</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Annales Univ. Sci. Budapest, Sect. Comp., 43(2014), pp.105-122</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.00761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.00761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.00815v4</id>
    <updated>2015-06-12T01:13:06Z</updated>
    <published>2015-06-02T09:49:45Z</published>
    <title>Classify Images with Conceptor Network</title>
    <summary>  This article demonstrates a new conceptor network based classifier in
classifying images. Mathematical descriptions and analysis are presented.
Various tests are experimented using three benchmark datasets: MNIST, CIFAR-10
and CIFAR-100. The experiments displayed that conceptor network can offer
superior results and flexible configurations than conventional classifiers such
as Softmax Regression and Support Vector Machine (SVM).
</summary>
    <author>
      <name>Yuhuang Hu</name>
    </author>
    <author>
      <name>M. S. Ishwarya</name>
    </author>
    <author>
      <name>Chu Kiong Loo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to a crucial sign
  error in experiments</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.00815v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.00815v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02247v2</id>
    <updated>2016-04-08T10:11:18Z</updated>
    <published>2015-06-07T10:23:58Z</published>
    <title>Well-posedness of a nonlinear integro-differential problem and its
  rearranged formulation</title>
    <summary>  We study the existence and uniqueness of solutions of a nonlinear
integro-differential problem which we reformulate introducing the notion of the
decreasing rearrangement of the solution. A dimensional reduction of the
problem is obtained and a detailed analysis of the properties of the solutions
of the model is provided. Finally, a fast numerical method is devised and
implemented to show the performance of the model when typical image processing
tasks such as filtering and segmentation are performed.
</summary>
    <author>
      <name>Gonzalo Galiano</name>
    </author>
    <author>
      <name>Emanuele Schiavi</name>
    </author>
    <author>
      <name>Julián Velasco</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.nonrwa.2016.03.013</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.nonrwa.2016.03.013" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final version. To appear in Nolinear Analysis Real World Applications
  (2016)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nonlinear Anal Real World App 32 (2016) 74-90</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.02247v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02247v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02345v1</id>
    <updated>2015-06-08T03:47:17Z</updated>
    <published>2015-06-08T03:47:17Z</published>
    <title>Wavelets and continuous wavelet transform for autostereoscopic multiview
  images</title>
    <summary>  Recently, the reference functions for the synthesis and analysis of the
autostereoscopic multiview and integral images in three-dimensional displays we
introduced. In the current paper, we propose the wavelets to analyze such
images. The wavelets are built on the reference functions as on the scaling
functions of the wavelet analysis. The continuous wavelet transform was
successfully applied to the testing wireframe binary objects. The restored
locations correspond to the structure of the testing wireframe binary objects.
</summary>
    <author>
      <name>Vladimir Saveljev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1364/AO.55.006275</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1364/AO.55.006275" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Applied Optics, Vol. 55, Issue 23, pp. 6275-6284 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.02345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02432v1</id>
    <updated>2015-06-08T10:44:36Z</updated>
    <published>2015-06-08T10:44:36Z</published>
    <title>Reflection Invariance: an important consideration of image orientation</title>
    <summary>  In this position paper, we consider the state of computer vision research
with respect to invariance to the horizontal orientation of an image -- what we
term reflection invariance. We describe why we consider reflection invariance
to be an important property and provide evidence where the absence of this
invariance produces surprising inconsistencies in state-of-the-art systems. We
demonstrate inconsistencies in methods of object detection and scene
classification when they are presented with images and the horizontal mirror of
those images. Finally, we examine where some of the invariance is exhibited in
feature detection and descriptors, and make a case for future consideration of
reflection invariance as a measure of quality in computer vision algorithms.
</summary>
    <author>
      <name>Craig Henderson</name>
    </author>
    <author>
      <name>Ebroul Izquierdo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.02432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02515v2</id>
    <updated>2015-12-07T18:11:36Z</updated>
    <published>2015-06-08T14:20:37Z</published>
    <title>Fast ConvNets Using Group-wise Brain Damage</title>
    <summary>  We revisit the idea of brain damage, i.e. the pruning of the coefficients of
a neural network, and suggest how brain damage can be modified and used to
speedup convolutional layers. The approach uses the fact that many efficient
implementations reduce generalized convolutions to matrix multiplications. The
suggested brain damage process prunes the convolutional kernel tensor in a
group-wise fashion by adding group-sparsity regularization to the standard
training process. After such group-wise pruning, convolutions can be reduced to
multiplications of thinned dense matrices, which leads to speedup. In the
comparison on AlexNet, the method achieves very competitive performance.
</summary>
    <author>
      <name>Vadim Lebedev</name>
    </author>
    <author>
      <name>Victor Lempitsky</name>
    </author>
    <link href="http://arxiv.org/abs/1506.02515v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02515v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02776v1</id>
    <updated>2015-06-09T05:03:47Z</updated>
    <published>2015-06-09T05:03:47Z</published>
    <title>Fast Geometric Fit Algorithm for Sphere Using Exact Solution</title>
    <summary>  Sphere fitting is a common problem in almost all science and engineering
disciplines. Most of methods available are iterative in behavior. This involves
fitting of the parameters in a least square sense or in a geometric sense. Here
we extend the methods of Thomas Chan and Landau who fitted the 2D data using
circle. This work closely resemble their work in redefining the error estimate
and solving the sphere fitting problem exactly. The solutions for center and
radius of the sphere can be found exactly and the equations can be hard coded
for high performance. We have also shown some comparison with other popular
methods and how this method behaves.
</summary>
    <author>
      <name>Sumith YD</name>
    </author>
    <link href="http://arxiv.org/abs/1506.02776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.03011v2</id>
    <updated>2015-09-10T15:20:38Z</updated>
    <published>2015-06-09T17:22:17Z</published>
    <title>Learning to Linearize Under Uncertainty</title>
    <summary>  Training deep feature hierarchies to solve supervised learning tasks has
achieved state of the art performance on many problems in computer vision.
However, a principled way in which to train such hierarchies in the
unsupervised setting has remained elusive. In this work we suggest a new
architecture and loss for training deep feature hierarchies that linearize the
transformations observed in unlabeled natural video sequences. This is done by
training a generative model to predict video frames. We also address the
problem of inherent uncertainty in prediction by introducing latent variables
that are non-deterministic functions of the input into the network
architecture.
</summary>
    <author>
      <name>Ross Goroshin</name>
    </author>
    <author>
      <name>Michael Mathieu</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at NIPS 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.03011v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.03011v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.03995v1</id>
    <updated>2015-06-12T11:51:16Z</updated>
    <published>2015-06-12T11:51:16Z</published>
    <title>Technical Report: Image Captioning with Semantically Similar Images</title>
    <summary>  This report presents our submission to the MS COCO Captioning Challenge 2015.
The method uses Convolutional Neural Network activations as an embedding to
find semantically similar images. From these images, the most typical caption
is selected based on unigram frequencies. Although the method received low
scores with automated evaluation metrics and in human assessed average
correctness, it is competitive in the ratio of captions which pass the Turing
test and which are assessed as better or equal to human captions.
</summary>
    <author>
      <name>Martin Kolář</name>
    </author>
    <author>
      <name>Michal Hradiš</name>
    </author>
    <author>
      <name>Pavel Zemčík</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.03995v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.03995v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.04191v1</id>
    <updated>2015-06-12T22:18:08Z</updated>
    <published>2015-06-12T22:18:08Z</published>
    <title>Deep Structured Models For Group Activity Recognition</title>
    <summary>  This paper presents a deep neural-network-based hierarchical graphical model
for individual and group activity recognition in surveillance scenes. Deep
networks are used to recognize the actions of individual people in a scene.
Next, a neural-network-based hierarchical graphical model refines the predicted
labels for each class by considering dependencies between the classes. This
refinement step mimics a message-passing step similar to inference in a
probabilistic graphical model. We show that this approach can be effective in
group activity recognition, with the deep graphical model improving recognition
rates over baseline methods.
</summary>
    <author>
      <name>Zhiwei Deng</name>
    </author>
    <author>
      <name>Mengyao Zhai</name>
    </author>
    <author>
      <name>Lei Chen</name>
    </author>
    <author>
      <name>Yuhao Liu</name>
    </author>
    <author>
      <name>Srikanth Muralidharan</name>
    </author>
    <author>
      <name>Mehrsan Javan Roshtkhari</name>
    </author>
    <author>
      <name>Greg Mori</name>
    </author>
    <link href="http://arxiv.org/abs/1506.04191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.04340v1</id>
    <updated>2015-06-14T00:39:20Z</updated>
    <published>2015-06-14T00:39:20Z</published>
    <title>Deep Secure Encoding: An Application to Face Recognition</title>
    <summary>  In this paper we present Deep Secure Encoding: a framework for secure
classification using deep neural networks, and apply it to the task of
biometric template protection for faces. Using deep convolutional neural
networks (CNNs), we learn a robust mapping of face classes to high entropy
secure codes. These secure codes are then hashed using standard hash functions
like SHA-256 to generate secure face templates. The efficacy of the approach is
shown on two face databases, namely, CMU-PIE and Extended Yale B, where we
achieve state of the art matching performance, along with cancelability and
high security with no unrealistic assumptions. Furthermore, the scheme can work
in both identification and verification modes.
</summary>
    <author>
      <name>Rohit Pandey</name>
    </author>
    <author>
      <name>Yingbo Zhou</name>
    </author>
    <author>
      <name>Venu Govindaraju</name>
    </author>
    <link href="http://arxiv.org/abs/1506.04340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.04721v1</id>
    <updated>2015-06-15T19:36:25Z</updated>
    <published>2015-06-15T19:36:25Z</published>
    <title>Automatic Layer Separation using Light Field Imaging</title>
    <summary>  We propose a novel approach that jointly removes reflection or translucent
layer from a scene and estimates scene depth. The input data are captured via
light field imaging. The problem is couched as minimizing the rank of the
transmitted scene layer via Robust Principle Component Analysis (RPCA). We also
impose regularization based on piecewise smoothness, gradient sparsity, and
layer independence to simultaneously recover 3D geometry of the transmitted
layer. Experimental results on synthetic and real data show that our technique
is robust and reliable, and can handle a broad range of layer separation
problems.
</summary>
    <author>
      <name>Qiaosong Wang</name>
    </author>
    <author>
      <name>Haiting Lin</name>
    </author>
    <author>
      <name>Yi Ma</name>
    </author>
    <author>
      <name>Sing Bing Kang</name>
    </author>
    <author>
      <name>Jingyi Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.04721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.04843v2</id>
    <updated>2016-10-27T12:27:47Z</updated>
    <published>2015-06-16T06:07:21Z</published>
    <title>Evaluation of Denoising Techniques for EOG signals based on SNR
  Estimation</title>
    <summary>  This paper evaluates four algorithms for denoising raw Electrooculography
(EOG) data based on the Signal to Noise Ratio (SNR). The SNR is computed using
the eigenvalue method. The filtering algorithms are a) Finite Impulse Response
(FIR) bandpass filters, b) Stationary Wavelet Transform, c) Empirical Mode
Decomposition (EMD) d) FIR Median Hybrid Filters. An EOG dataset has been
prepared where the subject is asked to perform letter cancelation test on 20
subjects.
</summary>
    <author>
      <name>Anirban Dasgupta</name>
    </author>
    <author>
      <name>Suvodip Chakrborty</name>
    </author>
    <author>
      <name>Aritra Chaudhuri</name>
    </author>
    <author>
      <name>Aurobinda Routray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in IEEE 2016 International Conference on Systems in Medicine and
  Biology (ICSMB)</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.04843v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.04843v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.05274v2</id>
    <updated>2015-12-22T12:57:25Z</updated>
    <published>2015-06-17T10:47:20Z</published>
    <title>Partial Functional Correspondence</title>
    <summary>  In this paper, we propose a method for computing partial functional
correspondence between non-rigid shapes. We use perturbation analysis to show
how removal of shape parts changes the Laplace-Beltrami eigenfunctions, and
exploit it as a prior on the spectral representation of the correspondence.
Corresponding parts are optimization variables in our problem and are used to
weight the functional correspondence; we are looking for the largest and most
regular (in the Mumford-Shah sense) parts that minimize correspondence
distortion. We show that our approach can cope with very challenging
correspondence settings.
</summary>
    <author>
      <name>Emanuele Rodolà</name>
    </author>
    <author>
      <name>Luca Cosmo</name>
    </author>
    <author>
      <name>Michael M. Bronstein</name>
    </author>
    <author>
      <name>Andrea Torsello</name>
    </author>
    <author>
      <name>Daniel Cremers</name>
    </author>
    <link href="http://arxiv.org/abs/1506.05274v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.05274v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06876v1</id>
    <updated>2015-06-23T06:35:08Z</updated>
    <published>2015-06-23T06:35:08Z</published>
    <title>Autonomous 3D Reconstruction Using a MAV</title>
    <summary>  An approach is proposed for high resolution 3D reconstruction of an object
using a Micro Air Vehicle (MAV). A system is described which autonomously
captures images and performs a dense 3D reconstruction via structure from
motion with no prior knowledge of the environment. Only the MAVs own sensors,
the front facing camera and the Inertial Measurement Unit (IMU) are utilized.
Precision agriculture is considered as an example application for the system.
</summary>
    <author>
      <name>Alexander Popov</name>
    </author>
    <author>
      <name>Dimitrios Zermas</name>
    </author>
    <author>
      <name>Nikolaos Papanikolopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.06876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06881v1</id>
    <updated>2015-06-23T07:12:17Z</updated>
    <published>2015-06-23T07:12:17Z</published>
    <title>Automatic vehicle tracking and recognition from aerial image sequences</title>
    <summary>  This paper addresses the problem of automated vehicle tracking and
recognition from aerial image sequences. Motivated by its successes in the
existing literature focus on the use of linear appearance subspaces to describe
multi-view object appearance and highlight the challenges involved in their
application as a part of a practical system. A working solution which includes
steps for data extraction and normalization is described. In experiments on
real-world data the proposed methodology achieved promising results with a high
correct recognition rate and few, meaningful errors (type II errors whereby
genuinely similar targets are sometimes being confused with one another).
Directions for future research and possible improvements of the proposed method
are discussed.
</summary>
    <author>
      <name>Ognjen Arandjelovic</name>
    </author>
    <link href="http://arxiv.org/abs/1506.06881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08006v1</id>
    <updated>2015-06-26T09:20:05Z</updated>
    <published>2015-06-26T09:20:05Z</published>
    <title>Spectral Collaborative Representation based Classification for Hand
  Gestures recognition on Electromyography Signals</title>
    <summary>  In this study, we introduce a novel variant and application of the
Collaborative Representation based Classification in spectral domain for
recognition of the hand gestures using the raw surface Electromyography
signals. The intuitive use of spectral features are explained via circulant
matrices. The proposed Spectral Collaborative Representation based
Classification (SCRC) is able to recognize gestures with higher levels of
accuracy for a fairly rich gesture set. The worst recognition result which is
the best in the literature is obtained as 97.3\% among the four sets of the
experiments for each hand gestures. The recognition results are reported with a
substantial number of experiments and labeling computation.
</summary>
    <author>
      <name>Ali Boyali</name>
    </author>
    <link href="http://arxiv.org/abs/1506.08006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08682v1</id>
    <updated>2015-06-29T15:33:30Z</updated>
    <published>2015-06-29T15:33:30Z</published>
    <title>Human Shape Variation - An Efficient Implementation using Skeleton</title>
    <summary>  It is at times important to detect human presence automatically in secure
environments. This needs a shape recognition algorithm that is robust, fast and
has low error rates. The algorithm needs to process camera images quickly to
detect any human in the range of vision, and generate alerts, especially if the
object under scrutiny is moving in certain directions. We present here a
simple, efficient and fast algorithm using skeletons of the images, and simple
features like posture and length of the object.
</summary>
    <author>
      <name>Dhriti Sengupta</name>
    </author>
    <author>
      <name>Merina Kundu</name>
    </author>
    <author>
      <name>Jayati Ghosh Dastidar</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJACR, Volume 4, Issue 14, March 2014, pp. 145-150</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.08682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08765v1</id>
    <updated>2015-06-29T18:23:17Z</updated>
    <published>2015-06-29T18:23:17Z</published>
    <title>Spectral Motion Synchronization in SE(3)</title>
    <summary>  This paper addresses the problem of motion synchronization (or averaging) and
describes a simple, closed-form solution based on a spectral decomposition,
which does not consider rotation and translation separately but works straight
in SE(3), the manifold of rigid motions. Besides its theoretical interest,
being the first closed form solution in SE(3), experimental results show that
it compares favourably with the state of the art both in terms of precision and
speed.
</summary>
    <author>
      <name>Federica Arrigoni</name>
    </author>
    <author>
      <name>Andrea Fusiello</name>
    </author>
    <author>
      <name>Beatrice Rossi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/16M1060248</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/16M1060248" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In SIAM Journal on Imaging Sciences, 9 (4): 1963-1990, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.08765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08815v1</id>
    <updated>2015-06-29T15:48:02Z</updated>
    <published>2015-06-29T15:48:02Z</published>
    <title>Tracking Direction of Human Movement - An Efficient Implementation using
  Skeleton</title>
    <summary>  Sometimes a simple and fast algorithm is required to detect human presence
and movement with a low error rate in a controlled environment for security
purposes. Here a light weight algorithm has been presented that generates alert
on detection of human presence and its movement towards a certain direction.
The algorithm uses fixed angle CCTV camera images taken over time and relies
upon skeleton transformation of successive images and calculation of difference
in their coordinates.
</summary>
    <author>
      <name>Merina Kundu</name>
    </author>
    <author>
      <name>Dhriti Sengupta</name>
    </author>
    <author>
      <name>Jayati Ghosh Dastidar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/16855-6722</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/16855-6722" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1506.08682</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications 96(13):27-33, June
  2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.08815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.00302v1</id>
    <updated>2015-07-01T17:59:21Z</updated>
    <published>2015-07-01T17:59:21Z</published>
    <title>Pose Embeddings: A Deep Architecture for Learning to Match Human Poses</title>
    <summary>  We present a method for learning an embedding that places images of humans in
similar poses nearby. This embedding can be used as a direct method of
comparing images based on human pose, avoiding potential challenges of
estimating body joint positions. Pose embedding learning is formulated under a
triplet-based distance criterion. A deep architecture is used to allow learning
of a representation capable of making distinctions between different poses.
Experiments on human pose matching and retrieval from video data demonstrate
the potential of the method.
</summary>
    <author>
      <name>Greg Mori</name>
    </author>
    <author>
      <name>Caroline Pantofaru</name>
    </author>
    <author>
      <name>Nisarg Kothari</name>
    </author>
    <author>
      <name>Thomas Leung</name>
    </author>
    <author>
      <name>George Toderici</name>
    </author>
    <author>
      <name>Alexander Toshev</name>
    </author>
    <author>
      <name>Weilong Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1507.00302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.00302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.00410v2</id>
    <updated>2015-09-18T18:01:47Z</updated>
    <published>2015-07-02T02:16:42Z</published>
    <title>Convolutional Color Constancy</title>
    <summary>  Color constancy is the problem of inferring the color of the light that
illuminated a scene, usually so that the illumination color can be removed.
Because this problem is underconstrained, it is often solved by modeling the
statistical regularities of the colors of natural objects and illumination. In
contrast, in this paper we reformulate the problem of color constancy as a 2D
spatial localization task in a log-chrominance space, thereby allowing us to
apply techniques from object detection and structured prediction to the color
constancy problem. By directly learning how to discriminate between correctly
white-balanced images and poorly white-balanced images, our model is able to
improve performance on standard benchmarks by nearly 40%.
</summary>
    <author>
      <name>Jonathan T. Barron</name>
    </author>
    <link href="http://arxiv.org/abs/1507.00410v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.00410v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.00913v1</id>
    <updated>2015-07-03T13:53:26Z</updated>
    <published>2015-07-03T13:53:26Z</published>
    <title>Fine-grained Recognition Datasets for Biodiversity Analysis</title>
    <summary>  In the following paper, we present and discuss challenging applications for
fine-grained visual classification (FGVC): biodiversity and species analysis.
We not only give details about two challenging new datasets suitable for
computer vision research with up to 675 highly similar classes, but also
present first results with localized features using convolutional neural
networks (CNN). We conclude with a list of challenging new research directions
in the area of visual classification for biodiversity research.
</summary>
    <author>
      <name>Erik Rodner</name>
    </author>
    <author>
      <name>Marcel Simon</name>
    </author>
    <author>
      <name>Gunnar Brehm</name>
    </author>
    <author>
      <name>Stephanie Pietsch</name>
    </author>
    <author>
      <name>J. Wolfgang Wägele</name>
    </author>
    <author>
      <name>Joachim Denzler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR FGVC Workshop 2015; dataset available</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.00913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.00913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.03751v1</id>
    <updated>2015-07-14T07:57:39Z</updated>
    <published>2015-07-14T07:57:39Z</published>
    <title>Closed Curves and Elementary Visual Object Identification</title>
    <summary>  For two closed curves on a plane (discrete version) and local criteria for
similarity of points on the curves one gets a potential, which describes the
similarity between curve points. This is the base for a global similarity
measure of closed curves (Fr\'echet distance). I use borderlines of handwritten
digits to demonstrate an area of application. I imagine, measuring the
similarity of closed curves is an essential and elementary task performed by a
visual system. This approach to similarity measures may be used by visual
systems.
</summary>
    <author>
      <name>Manfred Harringer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.03751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.03751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.04835v1</id>
    <updated>2015-07-17T05:24:45Z</updated>
    <published>2015-07-17T05:24:45Z</published>
    <title>Multiscale Adaptive Representation of Signals: I. The Basic Framework</title>
    <summary>  We introduce a framework for designing multi-scale, adaptive, shift-invariant
frames and bi-frames for representing signals. The new framework, called
AdaFrame, improves over dictionary learning-based techniques in terms of
computational efficiency at inference time. It improves classical multi-scale
basis such as wavelet frames in terms of coding efficiency. It provides an
attractive alternative to dictionary learning-based techniques for low level
signal processing tasks, such as compression and denoising, as well as high
level tasks, such as feature extraction for object recognition. Connections
with deep convolutional networks are also discussed. In particular, the
proposed framework reveals a drawback in the commonly used approach for
visualizing the activations of the intermediate layers in convolutional
networks, and suggests a natural alternative.
</summary>
    <author>
      <name>Cheng Tai</name>
    </author>
    <author>
      <name>Weinan E</name>
    </author>
    <link href="http://arxiv.org/abs/1507.04835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.04835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.04844v1</id>
    <updated>2015-07-17T06:21:31Z</updated>
    <published>2015-07-17T06:21:31Z</published>
    <title>Learning Robust Deep Face Representation</title>
    <summary>  With the development of convolution neural network, more and more researchers
focus their attention on the advantage of CNN for face recognition task. In
this paper, we propose a deep convolution network for learning a robust face
representation. The deep convolution net is constructed by 4 convolution
layers, 4 max pooling layers and 2 fully connected layers, which totally
contains about 4M parameters. The Max-Feature-Map activation function is used
instead of ReLU because the ReLU might lead to the loss of information due to
the sparsity while the Max-Feature-Map can get the compact and discriminative
feature vectors. The model is trained on CASIA-WebFace dataset and evaluated on
LFW dataset. The result on LFW achieves 97.77% on unsupervised setting for
single net.
</summary>
    <author>
      <name>Xiang Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1507.04844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.04844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05243v1</id>
    <updated>2015-07-19T03:10:28Z</updated>
    <published>2015-07-19T03:10:28Z</published>
    <title>Hand Gesture Recognition Library</title>
    <summary>  In this paper we have presented a hand gesture recognition library. Various
functions include detecting cluster count, cluster orientation, finger pointing
direction, etc. To use these functions first the input image needs to be
processed into a logical array for which a function has been developed. The
library has been developed keeping flexibility in mind and thus provides
application developers a wide range of options to develop custom gestures.
</summary>
    <author>
      <name>Jonathan Fidelis Paul</name>
    </author>
    <author>
      <name>Dibyabiva Seth</name>
    </author>
    <author>
      <name>Cijo Paul</name>
    </author>
    <author>
      <name>Jayati Ghosh Dastidar</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Science and Applied Information
  Technology, Volume 3, No.2, March - April 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1507.05243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.06332v1</id>
    <updated>2015-07-22T20:21:59Z</updated>
    <published>2015-07-22T20:21:59Z</published>
    <title>Part Localization using Multi-Proposal Consensus for Fine-Grained
  Categorization</title>
    <summary>  We present a simple deep learning framework to simultaneously predict
keypoint locations and their respective visibilities and use those to achieve
state-of-the-art performance for fine-grained classification. We show that by
conditioning the predictions on object proposals with sufficient image support,
our method can do well without complicated spatial reasoning. Instead,
inference methods with robustness to outliers, yield state-of-the-art for
keypoint localization. We demonstrate the effectiveness of our accurate
keypoint localization and visibility prediction on the fine-grained bird
recognition task with and without ground truth bird bounding boxes, and
outperform existing state-of-the-art methods by over 2%.
</summary>
    <author>
      <name>Kevin J. Shih</name>
    </author>
    <author>
      <name>Arun Mallya</name>
    </author>
    <author>
      <name>Saurabh Singh</name>
    </author>
    <author>
      <name>Derek Hoiem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BMVC 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.06332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.06332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08958v1</id>
    <updated>2015-07-31T17:41:55Z</updated>
    <published>2015-07-31T17:41:55Z</published>
    <title>SnowWatch: Snow Monitoring through Acquisition and Analysis of
  User-Generated Content</title>
    <summary>  We present a system for complementing snow phenomena monitoring with virtual
measurements extracted from public visual content. The proposed system
integrates an automatic acquisition and analysis of photographs and webcam
images depicting Alpine mountains. In particular, the technical demonstration
consists in a web portal that interfaces the whole system with the population.
It acts as an entertaining photo-sharing social web site, acquiring at the same
time visual content necessary for environmental monitoring.
</summary>
    <author>
      <name>Roman Fedorov</name>
    </author>
    <author>
      <name>Piero Fraternali</name>
    </author>
    <author>
      <name>Chiara Pasini</name>
    </author>
    <author>
      <name>Marco Tagliasacchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Multimedia and Expo, ICME 2015.
  Accepted and presented technical demo proposal</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.08958v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08958v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.00092v1</id>
    <updated>2015-08-01T07:15:19Z</updated>
    <published>2015-08-01T07:15:19Z</published>
    <title>Land Use Classification in Remote Sensing Images by Convolutional Neural
  Networks</title>
    <summary>  We explore the use of convolutional neural networks for the semantic
classification of remote sensing scenes. Two recently proposed architectures,
CaffeNet and GoogLeNet, are adopted, with three different learning modalities.
Besides conventional training from scratch, we resort to pre-trained networks
that are only fine-tuned on the target data, so as to avoid overfitting
problems and reduce design time. Experiments on two remote sensing datasets,
with markedly different characteristics, testify on the effectiveness and wide
applicability of the proposed solution, which guarantees a significant
performance improvement over all state-of-the-art references.
</summary>
    <author>
      <name>Marco Castelluccio</name>
    </author>
    <author>
      <name>Giovanni Poggi</name>
    </author>
    <author>
      <name>Carlo Sansone</name>
    </author>
    <author>
      <name>Luisa Verdoliva</name>
    </author>
    <link href="http://arxiv.org/abs/1508.00092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.00239v1</id>
    <updated>2015-08-02T14:09:02Z</updated>
    <published>2015-08-02T14:09:02Z</published>
    <title>Partial matching face recognition method for rehabilitation nursing
  robots beds</title>
    <summary>  In order to establish face recognition system in rehabilitation nursing
robots beds and achieve real-time monitor the patient on the bed. We propose a
face recognition method based on partial matching Hu moments which apply for
rehabilitation nursing robots beds. Firstly we using Haar classifier to detect
human faces automatically in dynamic video frames. Secondly we using Otsu
threshold method to extract facial features (eyebrows, eyes, mouth) in the face
image and its Hu moments. Finally, we using Hu moment feature set to achieve
the automatic face recognition. Experimental results show that this method can
efficiently identify face in a dynamic video and it has high practical value
(the accuracy rate is 91% and the average recognition time is 4.3s).
</summary>
    <author>
      <name>Dongmei Liang</name>
    </author>
    <author>
      <name>Wushan Cheng</name>
    </author>
    <link href="http://arxiv.org/abs/1508.00239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.00282v1</id>
    <updated>2015-08-02T20:40:21Z</updated>
    <published>2015-08-02T20:40:21Z</published>
    <title>On Hyperspectral Classification in the Compressed Domain</title>
    <summary>  In this paper, we study the problem of hyperspectral pixel classification
based on the recently proposed architectures for compressive whisk-broom
hyperspectral imagers without the need to reconstruct the complete data cube. A
clear advantage of classification in the compressed domain is its suitability
for real-time on-site processing of the sensed data. Moreover, it is assumed
that the training process also takes place in the compressed domain, thus,
isolating the classification unit from the recovery unit at the receiver's
side. We show that, perhaps surprisingly, using distinct measurement matrices
for different pixels results in more accuracy of the learned classifier and
consistent classification performance, supporting the role of information
diversity in learning.
</summary>
    <author>
      <name>Mohammad Aghagolzadeh</name>
    </author>
    <author>
      <name>Hayder Radha</name>
    </author>
    <link href="http://arxiv.org/abs/1508.00282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.00998v2</id>
    <updated>2015-12-11T14:35:20Z</updated>
    <published>2015-08-05T08:25:27Z</published>
    <title>Single and Multiple Illuminant Estimation Using Convolutional Neural
  Networks</title>
    <summary>  In this paper we present a method for the estimation of the color of the
illuminant in RAW images. The method includes a Convolutional Neural Network
that has been specially designed to produce multiple local estimates. A
multiple illuminant detector determines whether or not the local outputs of the
network must be aggregated into a single estimate. We evaluated our method on
standard datasets with single and multiple illuminants, obtaining lower
estimation errors with respect to those obtained by other general purpose
methods in the state of the art.
</summary>
    <author>
      <name>Simone Bianco</name>
    </author>
    <author>
      <name>Claudio Cusano</name>
    </author>
    <author>
      <name>Raimondo Schettini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.00998v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00998v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01057v2</id>
    <updated>2017-04-19T08:19:29Z</updated>
    <published>2015-08-05T13:02:48Z</published>
    <title>On the convergence of the sparse possibilistic c-means algorithm</title>
    <summary>  In this paper, a convergence proof for the recently proposed sparse
possibilistic c-means (SPCM) algorithm is provided, utilizing the celebrated
Zangwill convergence theorem. It is shown that the iterative sequence generated
by SPCM converges to a stationary point or there exists a subsequence of it
that converges to a stationary point of the cost function of the algorithm.
</summary>
    <author>
      <name>Spyridoula D. Xenaki</name>
    </author>
    <author>
      <name>Konstantinos D. Koutroumbas</name>
    </author>
    <author>
      <name>Athanasios A. Rontogiannis</name>
    </author>
    <link href="http://arxiv.org/abs/1508.01057v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01057v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01667v1</id>
    <updated>2015-08-07T12:11:06Z</updated>
    <published>2015-08-07T12:11:06Z</published>
    <title>Places205-VGGNet Models for Scene Recognition</title>
    <summary>  VGGNets have turned out to be effective for object recognition in still
images. However, it is unable to yield good performance by directly adapting
the VGGNet models trained on the ImageNet dataset for scene recognition. This
report describes our implementation of training the VGGNets on the large-scale
Places205 dataset. Specifically, we train three VGGNet models, namely
VGGNet-11, VGGNet-13, and VGGNet-16, by using a Multi-GPU extension of Caffe
toolbox with high computational efficiency. We verify the performance of
trained Places205-VGGNet models on three datasets: MIT67, SUN397, and
Places205. Our trained models achieve the state-of-the-art performance on these
datasets and are made public available.
</summary>
    <author>
      <name>Limin Wang</name>
    </author>
    <author>
      <name>Sheng Guo</name>
    </author>
    <author>
      <name>Weilin Huang</name>
    </author>
    <author>
      <name>Yu Qiao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.01667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01722v2</id>
    <updated>2016-03-02T19:41:42Z</updated>
    <published>2015-08-07T15:21:19Z</published>
    <title>Unconstrained Face Verification using Deep CNN Features</title>
    <summary>  In this paper, we present an algorithm for unconstrained face verification
based on deep convolutional features and evaluate it on the newly released
IARPA Janus Benchmark A (IJB-A) dataset. The IJB-A dataset includes real-world
unconstrained faces from 500 subjects with full pose and illumination
variations which are much harder than the traditional Labeled Face in the Wild
(LFW) and Youtube Face (YTF) datasets. The deep convolutional neural network
(DCNN) is trained using the CASIA-WebFace dataset. Extensive experiments on the
IJB-A dataset are provided.
</summary>
    <author>
      <name>Jun-Cheng Chen</name>
    </author>
    <author>
      <name>Vishal M. Patel</name>
    </author>
    <author>
      <name>Rama Chellappa</name>
    </author>
    <link href="http://arxiv.org/abs/1508.01722v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01722v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01859v1</id>
    <updated>2015-08-08T06:40:55Z</updated>
    <published>2015-08-08T06:40:55Z</published>
    <title>Simulation of optical flow and fuzzy based obstacle avoidance system for
  mobile robots</title>
    <summary>  Honey bees use optical flow to avoid obstacles effectively. In this research
work similar methodology was tested on a simulated mobile robot. Simulation
framework was based on VRML and Simulink in a 3D world. Optical flow vectors
were calculated from a video scene captured by a virtual camera which was used
as inputs to a fuzzy logic controller. Fuzzy logic controller decided the
locomotion of the robot. Different fuzzy logic rules were evaluated. The robot
was able to navigate through complex static and dynamic environments
effectively, avoiding obstacles on its path.
</summary>
    <author>
      <name>G. D. Illeperuma</name>
    </author>
    <author>
      <name>D. U. J. Sonnadara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Published in 30 April 2015</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Artificial Intelligence and Neural
  Networks, 5-1 (2015) 53-56</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1508.01859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02606v1</id>
    <updated>2015-08-11T14:17:28Z</updated>
    <published>2015-08-11T14:17:28Z</published>
    <title>InAR:Inverse Augmented Reality</title>
    <summary>  Augmented reality is the art to seamlessly fuse virtual objects into real
ones. In this short note, we address the opposite problem, the inverse
augmented reality, that is, given a perfectly augmented reality scene where
human is unable to distinguish real objects from virtual ones, how the machine
could help do the job. We show by structure from motion (SFM), a simple 3D
reconstruction technique from images in computer vision, the real and virtual
objects can be easily separated in the reconstructed 3D scene.
</summary>
    <author>
      <name>Hao Hu</name>
    </author>
    <author>
      <name>Hainan Cui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.02606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02977v1</id>
    <updated>2015-08-12T16:16:10Z</updated>
    <published>2015-08-12T16:16:10Z</published>
    <title>A massively parallel multi-level approach to a domain decomposition
  method for the optical flow estimation with varying illumination</title>
    <summary>  We consider a variational method to solve the optical flow problem with
varying illumination. We apply an adaptive control of the regularization
parameter which allows us to preserve the edges and fine features of the
computed flow. To reduce the complexity of the estimation for high resolution
images and the time of computations, we implement a multi-level parallel
approach based on the domain decomposition with the Schwarz overlapping method.
The second level of parallelism uses the massively parallel solver MUMPS. We
perform some numerical simulations to show the efficiency of our approach and
to validate it on classical and real-world image sequences.
</summary>
    <author>
      <name>Diane Gilliocq-Hirtz</name>
    </author>
    <author>
      <name>Zakaria Belhachmi</name>
    </author>
    <link href="http://arxiv.org/abs/1508.02977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.03710v1</id>
    <updated>2015-08-15T09:03:19Z</updated>
    <published>2015-08-15T09:03:19Z</published>
    <title>A Novel Approach For Finger Vein Verification Based on Self-Taught
  Learning</title>
    <summary>  In this paper, we propose a method for user Finger Vein Authentication (FVA)
as a biometric system. Using the discriminative features for classifying theses
finger veins is one of the main tips that make difference in related works,
Thus we propose to learn a set of representative features, based on
autoencoders. We model the user finger vein using a Gaussian distribution.
Experimental results show that our algorithm perform like a state-of-the-art on
SDUMLA-HMT benchmark.
</summary>
    <author>
      <name>Mohsen Fayyaz</name>
    </author>
    <author>
      <name>Masoud PourReza</name>
    </author>
    <author>
      <name>Mohammad Hajizadeh Saffar</name>
    </author>
    <author>
      <name>Mohammad Sabokrou</name>
    </author>
    <author>
      <name>Mahmood Fathy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures, Submitted Iranian Conference on Machine Vision
  and Image Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.03710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.03710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04198v1</id>
    <updated>2015-08-18T02:33:30Z</updated>
    <published>2015-08-18T02:33:30Z</published>
    <title>Low Rank Representation on Riemannian Manifold of Square Root Densities</title>
    <summary>  In this paper, we present a novel low rank representation (LRR) algorithm for
data lying on the manifold of square root densities. Unlike traditional LRR
methods which rely on the assumption that the data points are vectors in the
Euclidean space, our new algorithm is designed to incorporate the intrinsic
geometric structure and geodesic distance of the manifold. Experiments on
several computer vision datasets showcase its noise robustness and superior
performance on classification and subspace clustering compared to other
state-of-the-art approaches.
</summary>
    <author>
      <name>Yifan Fu</name>
    </author>
    <author>
      <name>Junbin Gao</name>
    </author>
    <author>
      <name>Xia Hong</name>
    </author>
    <author>
      <name>David Tien</name>
    </author>
    <link href="http://arxiv.org/abs/1508.04198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04238v1</id>
    <updated>2015-08-18T08:18:55Z</updated>
    <published>2015-08-18T08:18:55Z</published>
    <title>Preprint ARPPS Augmented Reality Pipeline Prospect System</title>
    <summary>  This is the preprint version of our paper on ICONIP. Outdoor augmented
reality geographic information system (ARGIS) is the hot application of
augmented reality over recent years. This paper concludes the key solutions of
ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),
and respectively realizes the machine vision based pipeline prospect system
(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the
MVBPPS's realization, this paper studies the neural network based 3D features
matching method.
</summary>
    <author>
      <name>Xiaolei Zhang</name>
    </author>
    <author>
      <name>Yong Han</name>
    </author>
    <author>
      <name>DongSheng Hao</name>
    </author>
    <author>
      <name>Zhihan Lv</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the preprint version of our paper on ICONIP</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.04238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04389v1</id>
    <updated>2015-08-18T17:24:09Z</updated>
    <published>2015-08-18T17:24:09Z</published>
    <title>A Deep Pyramid Deformable Part Model for Face Detection</title>
    <summary>  We present a face detection algorithm based on Deformable Part Models and
deep pyramidal features. The proposed method called DP2MFD is able to detect
faces of various sizes and poses in unconstrained conditions. It reduces the
gap in training and testing of DPM on deep features by adding a normalization
layer to the deep convolutional neural network (CNN). Extensive experiments on
four publicly available unconstrained face detection datasets show that our
method is able to capture the meaningful structure of faces and performs
significantly better than many competitive face detection algorithms.
</summary>
    <author>
      <name>Rajeev Ranjan</name>
    </author>
    <author>
      <name>Vishal M. Patel</name>
    </author>
    <author>
      <name>Rama Chellappa</name>
    </author>
    <link href="http://arxiv.org/abs/1508.04389v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04389v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04955v1</id>
    <updated>2015-08-20T11:03:44Z</updated>
    <published>2015-08-20T11:03:44Z</published>
    <title>Introducing Geometry in Active Learning for Image Segmentation</title>
    <summary>  We propose an Active Learning approach to training a segmentation classifier
that exploits geometric priors to streamline the annotation process in 3D image
volumes. To this end, we use these priors not only to select voxels most in
need of annotation but to guarantee that they lie on 2D planar patch, which
makes it much easier to annotate than if they were randomly distributed in the
volume. A simplified version of this approach is effective in natural 2D
images. We evaluated our approach on Electron Microscopy and Magnetic Resonance
image volumes, as well as on natural images. Comparing our approach against
several accepted baselines demonstrates a marked performance increase.
</summary>
    <author>
      <name>Ksenia Konyushkova</name>
    </author>
    <author>
      <name>Raphael Sznitman</name>
    </author>
    <author>
      <name>Pascal Fua</name>
    </author>
    <link href="http://arxiv.org/abs/1508.04955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05995v1</id>
    <updated>2015-08-24T23:29:19Z</updated>
    <published>2015-08-24T23:29:19Z</published>
    <title>An algorithm for Left Atrial Thrombi detection using Transesophageal
  Echocardiography</title>
    <summary>  Transesophageal echocardiography (TEE) is widely used to detect left atrium
(LA)/left atrial appendage (LAA) thrombi. In this paper, the local binary
pattern variance (LBPV) features are extracted from region of interest (ROI).
And the dynamic features are formed by using the information of its neighbor
frames in the sequence. The sequence is viewed as a bag, and the images in the
sequence are considered as the instances. Multiple-instance learning (MIL)
method is employed to solve the LAA thrombi detection. The experimental results
show that the proposed method can achieve better performance than that by using
other methods.
</summary>
    <author>
      <name>Jianrui Ding</name>
    </author>
    <author>
      <name>Min Xian</name>
    </author>
    <author>
      <name>H. D. Cheng</name>
    </author>
    <author>
      <name>Yang Li</name>
    </author>
    <author>
      <name>Fei Xu</name>
    </author>
    <author>
      <name>Yingtao Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1508.05995v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05995v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06725v1</id>
    <updated>2015-08-27T05:36:00Z</updated>
    <published>2015-08-27T05:36:00Z</published>
    <title>Image Type Water Meter Character Recognition Based on Embedded DSP</title>
    <summary>  In the paper, we combined DSP processor with image processing algorithm and
studied the method of water meter character recognition. We collected water
meter image through camera at a fixed angle, and the projection method is used
to recognize those digital images. The experiment results show that the method
can recognize the meter characters accurately and artificial meter reading is
replaced by automatic digital recognition, which improves working efficiency.
</summary>
    <author>
      <name>Ying Liu</name>
    </author>
    <author>
      <name>Yan-bin Han</name>
    </author>
    <author>
      <name>Yu-lin Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1508.06725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.02223v2</id>
    <updated>2016-01-26T10:13:27Z</updated>
    <published>2015-09-07T23:15:51Z</published>
    <title>Diffusion tensor imaging with deterministic error bounds</title>
    <summary>  Errors in the data and the forward operator of an inverse problem can be
handily modelled using partial order in Banach lattices. We present some
existing results of the theory of regularisation in this novel framework, where
errors are represented as bounds by means of the appropriate partial order.
  We apply the theory to Diffusion Tensor Imaging, where correct noise
modelling is challenging: it involves the Rician distribution and the nonlinear
Stejskal-Tanner equation. Linearisation of the latter in the statistical
framework would complicate the noise model even further. We avoid this using
the error bounds approach, which preserves simple error structure under
monotone transformations.
</summary>
    <author>
      <name>Artur Gorokh</name>
    </author>
    <author>
      <name>Yury Korolev</name>
    </author>
    <author>
      <name>Tuomo Valkonen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10851-016-0639-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10851-016-0639-7" rel="related"/>
    <link href="http://arxiv.org/abs/1509.02223v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.02223v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.02441v1</id>
    <updated>2015-09-04T22:03:40Z</updated>
    <published>2015-09-04T22:03:40Z</published>
    <title>Semantic Video Segmentation : Exploring Inference Efficiency</title>
    <summary>  We explore the efficiency of the CRF inference beyond image level semantic
segmentation and perform joint inference in video frames. The key idea is to
combine best of two worlds: semantic co-labeling and more expressive models.
Our formulation enables us to perform inference over ten thousand images within
seconds and makes the system amenable to perform video semantic segmentation
most effectively. On CamVid dataset, with TextonBoost unaries, our proposed
method achieves up to 8% improvement in accuracy over individual semantic image
segmentation without additional time overhead. The source code is available at
https://github.com/subtri/video_inference
</summary>
    <author>
      <name>Subarna Tripathi</name>
    </author>
    <author>
      <name>Serge Belongie</name>
    </author>
    <author>
      <name>Youngbae Hwang</name>
    </author>
    <author>
      <name>Truong Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in proc of ISOCC 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.02441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.02441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.02468v1</id>
    <updated>2015-09-08T17:52:03Z</updated>
    <published>2015-09-08T17:52:03Z</published>
    <title>Accelerated graph-based spectral polynomial filters</title>
    <summary>  Graph-based spectral denoising is a low-pass filtering using the
eigendecomposition of the graph Laplacian matrix of a noisy signal. Polynomial
filtering avoids costly computation of the eigendecomposition by projections
onto suitable Krylov subspaces. Polynomial filters can be based, e.g., on the
bilateral and guided filters. We propose constructing accelerated polynomial
filters by running flexible Krylov subspace based linear and eigenvalue solvers
such as the Block Locally Optimal Preconditioned Conjugate Gradient (LOBPCG)
method.
</summary>
    <author>
      <name>Andrew Knyazev</name>
    </author>
    <author>
      <name>Alexander Malyshev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MLSP.2015.7324315</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MLSP.2015.7324315" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures. Accepted to the 2015 IEEE International Workshop
  on Machine Learning for Signal Processing</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning for Signal Processing (MLSP), 2015 IEEE 25th
  International Workshop on , pp.1-6, 17-20 Sept. 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1509.02468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.02468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03413v1</id>
    <updated>2015-09-11T08:13:35Z</updated>
    <published>2015-09-11T08:13:35Z</published>
    <title>Learning Sparse Feature Representations using Probabilistic Quadtrees
  and Deep Belief Nets</title>
    <summary>  Learning sparse feature representations is a useful instrument for solving an
unsupervised learning problem. In this paper, we present three labeled
handwritten digit datasets, collectively called n-MNIST. Then, we propose a
novel framework for the classification of handwritten digits that learns sparse
representations using probabilistic quadtrees and Deep Belief Nets. On the
MNIST and n-MNIST datasets, our framework shows promising results and
significantly outperforms traditional Deep Belief Networks.
</summary>
    <author>
      <name>Saikat Basu</name>
    </author>
    <author>
      <name>Manohar Karki</name>
    </author>
    <author>
      <name>Sangram Ganguly</name>
    </author>
    <author>
      <name>Robert DiBiano</name>
    </author>
    <author>
      <name>Supratik Mukhopadhyay</name>
    </author>
    <author>
      <name>Ramakrishna Nemani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in the European Symposium on Artificial Neural Networks,
  ESANN 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.03413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03502v2</id>
    <updated>2015-09-25T19:58:34Z</updated>
    <published>2015-09-11T13:34:45Z</published>
    <title>Person Recognition in Personal Photo Collections</title>
    <summary>  Recognising persons in everyday photos presents major challenges (occluded
faces, different clothing, locations, etc.) for machine vision. We propose a
convnet based person recognition system on which we provide an in-depth
analysis of informativeness of different body cues, impact of training data,
and the common failure modes of the system. In addition, we discuss the
limitations of existing benchmarks and propose more challenging ones. Our
method is simple and is built on open source and open data, yet it improves the
state of the art results on a large dataset of social media photos (PIPA).
</summary>
    <author>
      <name>Seong Joon Oh</name>
    </author>
    <author>
      <name>Rodrigo Benenson</name>
    </author>
    <author>
      <name>Mario Fritz</name>
    </author>
    <author>
      <name>Bernt Schiele</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICCV 2015, revised</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.03502v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03502v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.04420v1</id>
    <updated>2015-09-15T07:06:32Z</updated>
    <published>2015-09-15T07:06:32Z</published>
    <title>Neuron detection in stack images: a persistent homology interpretation</title>
    <summary>  Automation and reliability are the two main requirements when computers are
applied in Life Sciences. In this paper we report on an application to neuron
recognition, an important step in our long-term project of providing software
systems to the study of neural morphology and functionality from biomedical
images. Our algorithms have been implemented in an ImageJ plugin called
NeuronPersistentJ, which has been validated experimentally. The soundness and
reliability of our approach are based on the interpretation of our processing
methods with respect to persistent homology, a well-known tool in computational
mathematics.
</summary>
    <author>
      <name>Jónathan Heras</name>
    </author>
    <author>
      <name>Gadea Mata</name>
    </author>
    <author>
      <name>Germán Cuesto</name>
    </author>
    <author>
      <name>Julio Rubio</name>
    </author>
    <author>
      <name>Miguel Morales</name>
    </author>
    <link href="http://arxiv.org/abs/1509.04420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.04420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.04853v2</id>
    <updated>2016-08-11T05:15:48Z</updated>
    <published>2015-09-16T08:43:53Z</published>
    <title>SPECFACE - A Dataset of Human Faces Wearing Spectacles</title>
    <summary>  This paper presents a database of human faces for persons wearing spectacles.
The database consists of images of faces having significant variations with
respect to illumination, head pose, skin color, facial expressions and sizes,
and nature of spectacles. The database contains data of 60 subjects. This
database is expected to be a precious resource for the development and
evaluation of algorithms for face detection, eye detection, head tracking, eye
gaze tracking, etc., for subjects wearing spectacles. As such, this can be a
valuable contribution to the computer vision community.
</summary>
    <author>
      <name>Anirban Dasgupta</name>
    </author>
    <author>
      <name>Shubhobrata Bhattacharya</name>
    </author>
    <author>
      <name>Aurobinda Routray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 9 figures, 1 Table. arXiv admin note: text overlap with
  arXiv:1505.04055</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2016 IEEE Students' Technology Symposium (IEEE TechSym 2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1509.04853v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.04853v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06016v1</id>
    <updated>2015-09-20T13:49:30Z</updated>
    <published>2015-09-20T13:49:30Z</published>
    <title>Image Set Querying Based Localization</title>
    <summary>  Conventional single image based localization methods usually fail to localize
a querying image when there exist large variations between the querying image
and the pre-built scene. To address this, we propose an image-set querying
based localization approach. When the localization by a single image fails to
work, the system will ask the user to capture more auxiliary images. First, a
local 3D model is established for the querying image set. Then, the pose of the
querying image set is estimated by solving a nonlinear optimization problem,
which aims to match the local 3D model against the pre-built scene. Experiments
have shown the effectiveness and feasibility of the proposed approach.
</summary>
    <author>
      <name>Lei Deng</name>
    </author>
    <author>
      <name>Siyuan Huang</name>
    </author>
    <author>
      <name>Yueqi Duan</name>
    </author>
    <author>
      <name>Baohua Chen</name>
    </author>
    <author>
      <name>Jie Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VCIP2015, 4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.06016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.08715v1</id>
    <updated>2015-09-29T12:26:08Z</updated>
    <published>2015-09-29T12:26:08Z</published>
    <title>Retinex filtering of foggy images: generation of a bulk set with
  selection and ranking</title>
    <summary>  In this paper we are proposing the use of GIMP Retinex, a filter of the GNU
Image Manipulation Program, for enhancing foggy images. This filter involves
adjusting four different parameters to find the output image which has to be
preferred according to some specific purposes. Aiming to obtain a processing,
which is able of choosing automatically the best image from a given set, we are
proposing a method for the generation a bulk set of GIMP Retinex filtered
images and a preliminary approach for selecting and ranking them.
</summary>
    <author>
      <name>Roberto Marazzato</name>
    </author>
    <author>
      <name>Amelia Carolina Sparavigna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: GIMP Retinex, GIMP, Image processing, Bulk generation of
  images, Bulk manipulation of images</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.08715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.08715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.08969v1</id>
    <updated>2015-09-29T22:37:21Z</updated>
    <published>2015-09-29T22:37:21Z</published>
    <title>Light Field Reconstruction Using Shearlet Transform</title>
    <summary>  In this article we develop an image based rendering technique based on light
field reconstruction from a limited set of perspective views acquired by
cameras. Our approach utilizes sparse representation of epipolar-plane images
in a directionally sensitive transform domain, obtained by an adapted discrete
shearlet transform. The used iterative thresholding algorithm provides
high-quality reconstruction results for relatively big disparities between
neighboring views. The generated densely sampled light field of a given 3D
scene is thus suitable for all applications which requires light field
reconstruction. The proposed algorithm is compared favorably against state of
the art depth image based rendering techniques.
</summary>
    <author>
      <name>Suren Vagharshakyan</name>
    </author>
    <author>
      <name>Robert Bregovic</name>
    </author>
    <author>
      <name>Atanas Gotchev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.08969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.08969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00479v1</id>
    <updated>2015-10-02T03:19:16Z</updated>
    <published>2015-10-02T03:19:16Z</published>
    <title>Effective Object Tracking in Unstructured Crowd Scenes</title>
    <summary>  In this paper, we are presenting a rotation variant Oriented Texture Curve
(OTC) descriptor based mean shift algorithm for tracking an object in an
unstructured crowd scene. The proposed algorithm works by first obtaining the
OTC features for a manually selected object target, then a visual vocabulary is
created by using all the OTC features of the target. The target histogram is
obtained using codebook encoding method which is then used in mean shift
framework to perform similarity search. Results are obtained on different
videos of challenging scenes and the comparison of the proposed approach with
several state-of-the-art approaches are provided. The analysis shows the
advantages and limitations of the proposed approach for tracking an object in
unstructured crowd scenes.
</summary>
    <author>
      <name>Ishan Jindal</name>
    </author>
    <author>
      <name>Shanmuganathan Raman</name>
    </author>
    <link href="http://arxiv.org/abs/1510.00479v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00479v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00889v1</id>
    <updated>2015-10-04T00:34:56Z</updated>
    <published>2015-10-04T00:34:56Z</published>
    <title>Background Image Generation Using Boolean Operations</title>
    <summary>  Tracking moving objects from a video sequence requires segmentation of these
objects from the background image. However, getting the actual background image
automatically without object detection and using only the video is difficult.
In this paper, we describe a novel algorithm that generates background from
real world images without foreground detection. The algorithm assumes that the
background image is shown in the majority of the video. Given this simple
assumption, the method described in this paper is able to accurately generate,
with high probability, the background image from a video using only a small
number of binary operations.
</summary>
    <author>
      <name>Kardi Teknomo</name>
    </author>
    <author>
      <name>Proceso Fernandez</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Philippine Computing Journal Vol 4 No 2, December 2009, pp. 43-49</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1510.00889v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00889v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02131v1</id>
    <updated>2015-10-07T21:01:34Z</updated>
    <published>2015-10-07T21:01:34Z</published>
    <title>DeepLogo: Hitting Logo Recognition with the Deep Neural Network Hammer</title>
    <summary>  Recently, there has been a flurry of industrial activity around logo
recognition, such as Ditto's service for marketers to track their brands in
user-generated images, and LogoGrab's mobile app platform for logo recognition.
However, relatively little academic or open-source logo recognition progress
has been made in the last four years. Meanwhile, deep convolutional neural
networks (DCNNs) have revolutionized a broad range of object recognition
applications. In this work, we apply DCNNs to logo recognition. We propose
several DCNN architectures, with which we surpass published state-of-art
accuracy on a popular logo recognition dataset.
</summary>
    <author>
      <name>Forrest N. Iandola</name>
    </author>
    <author>
      <name>Anting Shen</name>
    </author>
    <author>
      <name>Peter Gao</name>
    </author>
    <author>
      <name>Kurt Keutzer</name>
    </author>
    <link href="http://arxiv.org/abs/1510.02131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02192v1</id>
    <updated>2015-10-08T03:42:45Z</updated>
    <published>2015-10-08T03:42:45Z</published>
    <title>Simultaneous Deep Transfer Across Domains and Tasks</title>
    <summary>  Recent reports suggest that a generic supervised deep CNN model trained on a
large-scale dataset reduces, but does not remove, dataset bias. Fine-tuning
deep models in a new domain can require a significant amount of labeled data,
which for many applications is simply not available. We propose a new CNN
architecture to exploit unlabeled and sparsely labeled target domain data. Our
approach simultaneously optimizes for domain invariance to facilitate domain
transfer and uses a soft label distribution matching loss to transfer
information between tasks. Our proposed adaptation method offers empirical
performance which exceeds previously published results on two standard
benchmark visual domain adaptation tasks, evaluated across supervised and
semi-supervised adaptation settings.
</summary>
    <author>
      <name>Eric Tzeng</name>
    </author>
    <author>
      <name>Judy Hoffman</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Kate Saenko</name>
    </author>
    <link href="http://arxiv.org/abs/1510.02192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02644v1</id>
    <updated>2015-10-09T12:08:38Z</updated>
    <published>2015-10-09T12:08:38Z</published>
    <title>Free-hand Sketch Synthesis with Deformable Stroke Models</title>
    <summary>  We present a generative model which can automatically summarize the stroke
composition of free-hand sketches of a given category. When our model is fit to
a collection of sketches with similar poses, it discovers and learns the
structure and appearance of a set of coherent parts, with each part represented
by a group of strokes. It represents both consistent (topology) as well as
diverse aspects (structure and appearance variations) of each sketch category.
Key to the success of our model are important insights learned from a
comprehensive study performed on human stroke data. By fitting this model to
images, we are able to synthesize visually similar and pleasant free-hand
sketches.
</summary>
    <author>
      <name>Yi Li</name>
    </author>
    <author>
      <name>Yi-Zhe Song</name>
    </author>
    <author>
      <name>Timothy Hospedales</name>
    </author>
    <author>
      <name>Shaogang Gong</name>
    </author>
    <link href="http://arxiv.org/abs/1510.02644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02710v1</id>
    <updated>2015-10-09T15:47:00Z</updated>
    <published>2015-10-09T15:47:00Z</published>
    <title>Procams-Based Cybernetics</title>
    <summary>  Procams-based cybernetics is a unique, emerging research field, which aims at
enhancing and supporting our activities by naturally connecting human and
computers/machines as a cooperative integrated system via projector-camera
systems (procams). It rests on various research domains such as
virtual/augmented reality, computer vision, computer graphics, projection
display, human computer interface, human robot interaction and so on. This
laboratory presentation provides a brief history including recent achievements
of our procams-based cybernetics project.
</summary>
    <author>
      <name>Kosuke Sato</name>
    </author>
    <author>
      <name>Daisuke Iwai</name>
    </author>
    <author>
      <name>Sei Ikeda</name>
    </author>
    <author>
      <name>Noriko Takemura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures, IEEE VR 2015 Lab/Project presentation</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.02710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02774v1</id>
    <updated>2015-10-09T19:15:52Z</updated>
    <published>2015-10-09T19:15:52Z</published>
    <title>Human Head Pose Estimation by Facial Features Location</title>
    <summary>  We describe a method for estimating human head pose in a color image that
contains enough of information to locate the head silhouette and detect
non-trivial color edges of individual facial features. The method works by
spotting the human head on an arbitrary background, extracting the head
outline, and locating facial features necessary to describe the head
orientation in the 3D space. It is robust enough to work with both color and
gray-level images featuring quasi-frontal views of a human head under variable
lighting conditions.
</summary>
    <author>
      <name>Eugene Borovikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a master's thesis completed at UMCP in 1998, being published
  here given enough of the demand on its contents from the Computer Vision R&amp;D
  community</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.02774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02942v1</id>
    <updated>2015-10-10T14:30:25Z</updated>
    <published>2015-10-10T14:30:25Z</published>
    <title>Evaluation of Joint Multi-Instance Multi-Label Learning For Breast
  Cancer Diagnosis</title>
    <summary>  Multi-instance multi-label (MIML) learning is a challenging problem in many
aspects. Such learning approaches might be useful for many medical diagnosis
applications including breast cancer detection and classification. In this
study subset of digiPATH dataset (whole slide digital breast cancer
histopathology images) are used for training and evaluation of six
state-of-the-art MIML methods.
  At the end, performance comparison of these approaches are given by means of
effective evaluation metrics. It is shown that MIML-kNN achieve the best
performance that is %65.3 average precision, where most of other methods attain
acceptable results as well.
</summary>
    <author>
      <name>Baris Gecer</name>
    </author>
    <author>
      <name>Ozge Yalcinkaya</name>
    </author>
    <author>
      <name>Onur Tasar</name>
    </author>
    <author>
      <name>Selim Aksoy</name>
    </author>
    <link href="http://arxiv.org/abs/1510.02942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.03199v1</id>
    <updated>2015-10-12T09:36:53Z</updated>
    <published>2015-10-12T09:36:53Z</published>
    <title>Interactive multiclass segmentation using superpixel classification</title>
    <summary>  This paper adresses the problem of interactive multiclass segmentation. We
propose a fast and efficient new interactive segmentation method called
Superpixel Classification-based Interactive Segmentation (SCIS). From a few
strokes drawn by a human user over an image, this method extracts relevant
semantic objects. To get a fast calculation and an accurate segmentation, SCIS
uses superpixel over-segmentation and support vector machine classification. In
this paper, we demonstrate that SCIS significantly outperfoms competing
algorithms by evaluating its performances on the reference benchmarks of
McGuinness and Santner.
</summary>
    <author>
      <name>Bérengère Mathieu</name>
    </author>
    <author>
      <name>Alain Crouzil</name>
    </author>
    <author>
      <name>Jean-Baptiste Puel</name>
    </author>
    <link href="http://arxiv.org/abs/1510.03199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.03199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04238v1</id>
    <updated>2015-10-14T18:51:51Z</updated>
    <published>2015-10-14T18:51:51Z</published>
    <title>Dynamical spectral unmixing of multitemporal hyperspectral images</title>
    <summary>  In this paper, we consider the problem of unmixing a time series of
hyperspectral images. We propose a dynamical model based on linear mixing
processes at each time instant. The spectral signatures and fractional
abundances of the pure materials in the scene are seen as latent variables, and
assumed to follow a general dynamical structure. Based on a simplified version
of this model, we derive an efficient spectral unmixing algorithm to estimate
the latent variables by performing alternating minimizations. The performance
of the proposed approach is demonstrated on synthetic and real multitemporal
hyperspectral images.
</summary>
    <author>
      <name>Simon Henrot</name>
    </author>
    <author>
      <name>Jocelyn Chanussot</name>
    </author>
    <author>
      <name>Christian Jutten</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2016.2562562</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2016.2562562" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04563v1</id>
    <updated>2015-10-15T14:51:35Z</updated>
    <published>2015-10-15T14:51:35Z</published>
    <title>Elasticity-based Matching by Minimizing the Symmetric Difference of
  Shapes</title>
    <summary>  We consider the problem of matching two shapes assuming these shapes are
related by an elastic deformation. Using linearized elasticity theory and the
finite element method we seek an elastic deformation that is caused by simple
external boundary forces and accounts for the difference between the two
shapes. Our main contribution is in proposing a cost function and an
optimization procedure to minimize the symmetric difference between the
deformed and the target shapes as an alternative to point matches that guide
the matching in other techniques. We show how to approximate the nonlinear
optimization problem by a sequence of convex problems. We demonstrate the
utility of our method in experiments and compare it to an ICP-like matching
algorithm.
</summary>
    <author>
      <name>Konrad Simon</name>
    </author>
    <author>
      <name>Ronen Basri</name>
    </author>
    <link href="http://arxiv.org/abs/1510.04563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04863v1</id>
    <updated>2015-10-16T12:36:13Z</updated>
    <published>2015-10-16T12:36:13Z</published>
    <title>An Extension to Hough Transform Based on Gradient Orientation</title>
    <summary>  The Hough transform is one of the most common methods for line detection. In
this paper we propose a novel extension of the regular Hough transform. The
proposed extension combines the extension of the accumulator space and the
local gradient orientation resulting in clutter reduction and yielding more
prominent peaks, thus enabling better line identification. We demonstrate
benefits in applications such as visual quality inspection and rectangle
detection.
</summary>
    <author>
      <name>Tomislav Petković</name>
    </author>
    <author>
      <name>Sven Lončarić</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Part of the Proceedings of the Croatian Computer Vision Workshop,
  CCVW 2015, Year 3</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.06925v2</id>
    <updated>2015-12-03T18:38:08Z</updated>
    <published>2015-10-23T13:02:55Z</published>
    <title>Confusing Deep Convolution Networks by Relabelling</title>
    <summary>  Deep convolutional neural networks have become the gold standard for image
recognition tasks, demonstrating many current state-of-the-art results and even
achieving near-human level performance on some tasks. Despite this fact it has
been shown that their strong generalisation qualities can be fooled to
misclassify previously correctly classified natural images and give erroneous
high confidence classifications to nonsense synthetic images. In this paper we
extend that work, by presenting a straightforward way to perturb an image in
such a way as to cause it to acquire any other label from within the dataset
while leaving this perturbed image visually indistinguishable from the
original.
</summary>
    <author>
      <name>Leigh Robinson</name>
    </author>
    <author>
      <name>Benjamin Graham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to BMVC 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.06925v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.06925v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.07182v1</id>
    <updated>2015-10-24T20:26:08Z</updated>
    <published>2015-10-24T20:26:08Z</published>
    <title>Computational models of attention</title>
    <summary>  This chapter reviews recent computational models of visual attention. We
begin with models for the bottom-up or stimulus-driven guidance of attention to
salient visual items, which we examine in seven different broad categories. We
then examine more complex models which address the top-down or goal-oriented
guidance of attention towards items that are more relevant to the task at hand.
</summary>
    <author>
      <name>Laurent Itti</name>
    </author>
    <author>
      <name>Ali Borji</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Cognitive Neuroscience: The Biology of the Mind (Fifth Edition),
  (M. S. Gazzaniga, R. B. Ivry, G. R. Mangun Ed.), pp. 1-10, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1510.07182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.07182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.08893v1</id>
    <updated>2015-10-29T20:34:15Z</updated>
    <published>2015-10-29T20:34:15Z</published>
    <title>A Deep Siamese Network for Scene Detection in Broadcast Videos</title>
    <summary>  We present a model that automatically divides broadcast videos into coherent
scenes by learning a distance measure between shots. Experiments are performed
to demonstrate the effectiveness of our approach by comparing our algorithm
against recent proposals for automatic scene segmentation. We also propose an
improved performance measure that aims to reduce the gap between numerical
evaluation and expected results, and propose and release a new benchmark
dataset.
</summary>
    <author>
      <name>Lorenzo Baraldi</name>
    </author>
    <author>
      <name>Costantino Grana</name>
    </author>
    <author>
      <name>Rita Cucchiara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2733373.2806316</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2733373.2806316" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Multimedia 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.08893v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.08893v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.00513v1</id>
    <updated>2015-11-02T14:23:22Z</updated>
    <published>2015-11-02T14:23:22Z</published>
    <title>Pixel-wise Segmentation of Street with Neural Networks</title>
    <summary>  Pixel-wise street segmentation of photographs taken from a drivers
perspective is important for self-driving cars and can also support other
object recognition tasks. A framework called SST was developed to examine the
accuracy and execution time of different neural networks. The best neural
network achieved an $F_1$-score of 89.5% with a simple feedforward neural
network which trained to solve a regression task.
</summary>
    <author>
      <name>Sebastian Bittel</name>
    </author>
    <author>
      <name>Vitali Kaiser</name>
    </author>
    <author>
      <name>Marvin Teichmann</name>
    </author>
    <author>
      <name>Martin Thoma</name>
    </author>
    <link href="http://arxiv.org/abs/1511.00513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.00871v1</id>
    <updated>2015-11-03T12:09:26Z</updated>
    <published>2015-11-03T12:09:26Z</published>
    <title>Properties of the Sample Mean in Graph Spaces and the
  Majorize-Minimize-Mean Algorithm</title>
    <summary>  One of the most fundamental concepts in statistics is the concept of sample
mean. Properties of the sample mean that are well-defined in Euclidean spaces
become unwieldy or even unclear in graph spaces. Open problems related to the
sample mean of graphs include: non-existence, non-uniqueness, statistical
inconsistency, lack of convergence results of mean algorithms, non-existence of
midpoints, and disparity to midpoints. We present conditions to resolve all six
problems and propose a Majorize-Minimize-Mean (MMM) Algorithm. Experiments on
graph datasets representing images and molecules show that the MMM-Algorithm
best approximates a sample mean of graphs compared to six other mean
algorithms.
</summary>
    <author>
      <name>Brijnesh J. Jain</name>
    </author>
    <link href="http://arxiv.org/abs/1511.00871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.01154v1</id>
    <updated>2015-11-03T23:08:18Z</updated>
    <published>2015-11-03T23:08:18Z</published>
    <title>Robust Registration of Calcium Images by Learned Contrast Synthesis</title>
    <summary>  Multi-modal image registration is a challenging task that is vital to fuse
complementary signals for subsequent analyses. Despite much research into cost
functions addressing this challenge, there exist cases in which these are
ineffective. In this work, we show that (1) this is true for the registration
of in-vivo Drosophila brain volumes visualizing genetically encoded calcium
indicators to an nc82 atlas and (2) that machine learning based contrast
synthesis can yield improvements. More specifically, the number of subjects for
which the registration outright failed was greatly reduced (from 40% to 15%) by
using a synthesized image.
</summary>
    <author>
      <name>John A. Bogovic</name>
    </author>
    <author>
      <name>Philipp Hanslovsky</name>
    </author>
    <author>
      <name>Allan Wong</name>
    </author>
    <author>
      <name>Stephan Saalfeld</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISBI.2016.7493463</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISBI.2016.7493463" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Symposium on Biomedical Imaging, 2016, pages
  1123--1126</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.01154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.01154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.01156v1</id>
    <updated>2015-11-03T23:10:47Z</updated>
    <published>2015-11-03T23:10:47Z</published>
    <title>Robust Large-Scale Localization in 3D Point Clouds Revisited</title>
    <summary>  We tackle the problem of getting a full 6-DOF pose estimation of a query
image inside a given point cloud. This technical report re-evaluates the
algorithms proposed by Y. Li et al. "Worldwide Pose Estimation using 3D Point
Cloud". Our code computes poses from 3 or 4 points, with both known and unknown
focal length. The results can easily be displayed and analyzed with Meshlab. We
found both advantages and shortcomings of the methods proposed. Furthermore,
additional priors and parameters for point selection, RANSAC and pose quality
estimate (inlier test) are proposed and applied.
</summary>
    <author>
      <name>Fabian Tschopp</name>
    </author>
    <author>
      <name>Marco Zorzi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages; technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.01156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.01156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10; I.3.5; I.4.1; I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.01168v1</id>
    <updated>2015-11-04T00:34:29Z</updated>
    <published>2015-11-04T00:34:29Z</published>
    <title>Cell identification in whole-brain multiview images of neural activation</title>
    <summary>  We present a scalable method for brain cell identification in multiview
confocal light sheet microscopy images. Our algorithmic pipeline includes a
hierarchical registration approach and a novel multiview version of semantic
deconvolution that simultaneously enhance visibility of fluorescent cell
bodies, equalize their contrast, and fuses adjacent views into a single 3D
images on which cell identification is performed with mean shift.
  We present empirical results on a whole-brain image of an adult Arc-dVenus
mouse acquired at 4micron resolution. Based on an annotated test volume
containing 3278 cells, our algorithm achieves an $F_1$ measure of 0.89.
</summary>
    <author>
      <name>Marco Paciscopi</name>
    </author>
    <author>
      <name>Ludovico Silvestri</name>
    </author>
    <author>
      <name>Francesco Saverio Pavone</name>
    </author>
    <author>
      <name>Paolo Frasconi</name>
    </author>
    <link href="http://arxiv.org/abs/1511.01168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.01168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.02282v1</id>
    <updated>2015-11-07T02:06:11Z</updated>
    <published>2015-11-07T02:06:11Z</published>
    <title>Fingertip in the Eye: A cascaded CNN pipeline for the real-time
  fingertip detection in egocentric videos</title>
    <summary>  We introduce a new pipeline for hand localization and fingertip detection.
For RGB images captured from an egocentric vision mobile camera, hand and
fingertip detection remains a challenging problem due to factors like
background complexity and hand shape variety. To address these issues
accurately and robustly, we build a large scale dataset named Ego-Fingertip and
propose a bi-level cascaded pipeline of convolutional neural networks, namely,
Attention-based Hand Detector as well as Multi-point Fingertip Detector. The
proposed method significantly tackles challenges and achieves satisfactorily
accurate prediction and real-time performance compared to previous hand and
fingertip detection methods.
</summary>
    <author>
      <name>Xiaorui Liu</name>
    </author>
    <author>
      <name>Yichao Huang</name>
    </author>
    <author>
      <name>Xin Zhang</name>
    </author>
    <author>
      <name>Lianwen Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.02282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.02282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.02999v1</id>
    <updated>2015-11-10T06:08:42Z</updated>
    <published>2015-11-10T06:08:42Z</published>
    <title>Improvised Salient Object Detection and Manipulation</title>
    <summary>  In case of salient subject recognition, computer algorithms have been heavily
relied on scanning of images from top-left to bottom-right systematically and
apply brute-force when attempting to locate objects of interest. Thus, the
process turns out to be quite time consuming. Here a novel approach and a
simple solution to the above problem is discussed. In this paper, we implement
an approach to object manipulation and detection through segmentation map,
which would help to desaturate or, in other words, wash out the background of
the image. Evaluation for the performance is carried out using the Jaccard
index against the well-known Ground-truth target box technique.
</summary>
    <author>
      <name>Abhishek Maity</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.02999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.02999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.03296v2</id>
    <updated>2016-07-22T19:12:04Z</updated>
    <published>2015-11-10T21:19:34Z</published>
    <title>The Fast Bilateral Solver</title>
    <summary>  We present the bilateral solver, a novel algorithm for edge-aware smoothing
that combines the flexibility and speed of simple filtering approaches with the
accuracy of domain-specific optimization algorithms. Our technique is capable
of matching or improving upon state-of-the-art results on several different
computer vision tasks (stereo, depth superresolution, colorization, and
semantic segmentation) while being 10-1000 times faster than competing
approaches. The bilateral solver is fast, robust, straightforward to generalize
to new domains, and simple to integrate into deep learning pipelines.
</summary>
    <author>
      <name>Jonathan T. Barron</name>
    </author>
    <author>
      <name>Ben Poole</name>
    </author>
    <link href="http://arxiv.org/abs/1511.03296v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03296v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.03464v1</id>
    <updated>2015-11-11T11:38:22Z</updated>
    <published>2015-11-11T11:38:22Z</published>
    <title>A Directional Diffusion Algorithm for Inpainting</title>
    <summary>  The problem of inpainting involves reconstructing the missing areas of an
image. Inpainting has many applications, such as reconstructing old damaged
photographs or removing obfuscations from images. In this paper we present the
directional diffusion algorithm for inpainting. Typical diffusion algorithms
are bad at propagating edges from the image into the unknown masked regions.
The directional diffusion algorithm improves on the regular diffusion algorithm
by reconstructing edges more accurately. It scores better than regular
diffusion when reconstructing images that are obfuscated by a text mask.
</summary>
    <author>
      <name>Jan Deriu</name>
    </author>
    <author>
      <name>Rolf Jagerman</name>
    </author>
    <author>
      <name>Kai-En Tsay</name>
    </author>
    <link href="http://arxiv.org/abs/1511.03464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.03690v1</id>
    <updated>2015-11-11T21:30:10Z</updated>
    <published>2015-11-11T21:30:10Z</published>
    <title>Deep Multimodal Semantic Embeddings for Speech and Images</title>
    <summary>  In this paper, we present a model which takes as input a corpus of images
with relevant spoken captions and finds a correspondence between the two
modalities. We employ a pair of convolutional neural networks to model visual
objects and speech signals at the word level, and tie the networks together
with an embedding and alignment model which learns a joint semantic space over
both modalities. We evaluate our model using image search and annotation tasks
on the Flickr8k dataset, which we augmented by collecting a corpus of 40,000
spoken captions using Amazon Mechanical Turk.
</summary>
    <author>
      <name>David Harwath</name>
    </author>
    <author>
      <name>James Glass</name>
    </author>
    <link href="http://arxiv.org/abs/1511.03690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04003v1</id>
    <updated>2015-11-12T18:55:20Z</updated>
    <published>2015-11-12T18:55:20Z</published>
    <title>Human Curation and Convnets: Powering Item-to-Item Recommendations on
  Pinterest</title>
    <summary>  This paper presents Pinterest Related Pins, an item-to-item recommendation
system that combines collaborative filtering with content-based ranking. We
demonstrate that signals derived from user curation, the activity of users
organizing content, are highly effective when used in conjunction with
content-based ranking. This paper also demonstrates the effectiveness of visual
features, such as image or object representations learned from convnets, in
improving the user engagement rate of our item-to-item recommendation system.
</summary>
    <author>
      <name>Dmitry Kislyuk</name>
    </author>
    <author>
      <name>Yuchen Liu</name>
    </author>
    <author>
      <name>David Liu</name>
    </author>
    <author>
      <name>Eric Tzeng</name>
    </author>
    <author>
      <name>Yushi Jing</name>
    </author>
    <link href="http://arxiv.org/abs/1511.04003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04242v1</id>
    <updated>2015-11-13T11:25:50Z</updated>
    <published>2015-11-13T11:25:50Z</published>
    <title>Volume-based Semantic Labeling with Signed Distance Functions</title>
    <summary>  Research works on the two topics of Semantic Segmentation and SLAM
(Simultaneous Localization and Mapping) have been following separate tracks.
Here, we link them quite tightly by delineating a category label fusion
technique that allows for embedding semantic information into the dense map
created by a volume-based SLAM algorithm such as KinectFusion. Accordingly, our
approach is the first to provide a semantically labeled dense reconstruction of
the environment from a stream of RGB-D images. We validate our proposal using a
publicly available semantically annotated RGB-D dataset and a) employing ground
truth labels, b) corrupting such annotations with synthetic noise, c) deploying
a state of the art semantic segmentation algorithm based on Convolutional
Neural Networks.
</summary>
    <author>
      <name>Tommaso Cavallari</name>
    </author>
    <author>
      <name>Luigi Di Stefano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to PSIVT2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.04242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04491v2</id>
    <updated>2016-11-11T08:40:53Z</updated>
    <published>2015-11-14T02:21:50Z</published>
    <title>Deeply-Recursive Convolutional Network for Image Super-Resolution</title>
    <summary>  We propose an image super-resolution method (SR) using a deeply-recursive
convolutional network (DRCN). Our network has a very deep recursive layer (up
to 16 recursions). Increasing recursion depth can improve performance without
introducing new parameters for additional convolutions. Albeit advantages,
learning a DRCN is very hard with a standard gradient descent method due to
exploding/vanishing gradients. To ease the difficulty of training, we propose
two extensions: recursive-supervision and skip-connection. Our method
outperforms previous methods by a large margin.
</summary>
    <author>
      <name>Jiwon Kim</name>
    </author>
    <author>
      <name>Jung Kwon Lee</name>
    </author>
    <author>
      <name>Kyoung Mu Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2016 Oral</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.04491v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04491v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04670v1</id>
    <updated>2015-11-15T07:57:41Z</updated>
    <published>2015-11-15T07:57:41Z</published>
    <title>Uncovering Temporal Context for Video Question and Answering</title>
    <summary>  In this work, we introduce Video Question Answering in temporal domain to
infer the past, describe the present and predict the future. We present an
encoder-decoder approach using Recurrent Neural Networks to learn temporal
structures of videos and introduce a dual-channel ranking loss to answer
multiple-choice questions. We explore approaches for finer understanding of
video content using question form of "fill-in-the-blank", and managed to
collect 109,895 video clips with duration over 1,000 hours from TACoS, MPII-MD,
MEDTest 14 datasets, while the corresponding 390,744 questions are generated
from annotations. Extensive experiments demonstrate that our approach
significantly outperforms the compared baselines.
</summary>
    <author>
      <name>Linchao Zhu</name>
    </author>
    <author>
      <name>Zhongwen Xu</name>
    </author>
    <author>
      <name>Yi Yang</name>
    </author>
    <author>
      <name>Alexander G. Hauptmann</name>
    </author>
    <link href="http://arxiv.org/abs/1511.04670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04901v1</id>
    <updated>2015-11-16T10:31:18Z</updated>
    <published>2015-11-16T10:31:18Z</published>
    <title>Coarse-to-fine Face Alignment with Multi-Scale Local Patch Regression</title>
    <summary>  Facial landmark localization plays an important role in face recognition and
analysis applications. In this paper, we give a brief introduction to a
coarse-to-fine pipeline with neural networks and sequential regression. First,
a global convolutional network is applied to the holistic facial image to give
an initial landmark prediction. A pyramid of multi-scale local image patches is
then cropped to feed to a new network for each landmark to refine the
prediction. As the refinement network outputs a more accurate position
estimation than the input, such procedure could be repeated several times until
the estimation converges. We evaluate our system on the 300-W dataset [11] and
it outperforms the recent state-of-the-arts.
</summary>
    <author>
      <name>Zhiao Huang</name>
    </author>
    <author>
      <name>Erjin Zhou</name>
    </author>
    <author>
      <name>Zhimin Cao</name>
    </author>
    <link href="http://arxiv.org/abs/1511.04901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04902v1</id>
    <updated>2015-11-16T10:34:25Z</updated>
    <published>2015-11-16T10:34:25Z</published>
    <title>Graph-based denoising for time-varying point clouds</title>
    <summary>  Noisy 3D point clouds arise in many applications. They may be due to errors
when constructing a 3D model from images or simply to imprecise depth sensors.
Point clouds can be given geometrical structure using graphs created from the
similarity information between points. This paper introduces a technique that
uses this graph structure and convex optimization methods to denoise 3D point
clouds. A short discussion presents how those methods naturally generalize to
time-varying inputs such as 3D point cloud time series.
</summary>
    <author>
      <name>Yann Schoenenberger</name>
    </author>
    <author>
      <name>Johan Paratte</name>
    </author>
    <author>
      <name>Pierre Vandergheynst</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/3DTV.2015.7169366</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/3DTV.2015.7169366" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, 3DTV-Con 2015</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">3DTV-Conference: The True Vision - Capture, Transmission and
  Display of 3D Video (3DTV-CON) (2015) 1-4</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.04902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06078v2</id>
    <updated>2016-04-14T03:10:04Z</updated>
    <published>2015-11-19T07:17:49Z</published>
    <title>Learning Deep Structure-Preserving Image-Text Embeddings</title>
    <summary>  This paper proposes a method for learning joint embeddings of images and text
using a two-branch neural network with multiple layers of linear projections
followed by nonlinearities. The network is trained using a large margin
objective that combines cross-view ranking constraints with within-view
neighborhood structure preservation constraints inspired by metric learning
literature. Extensive experiments show that our approach gains significant
improvements in accuracy for image-to-text and text-to-image retrieval. Our
method achieves new state-of-the-art results on the Flickr30K and MSCOCO
image-sentence datasets and shows promise on the new task of phrase
localization on the Flickr30K Entities dataset.
</summary>
    <author>
      <name>Liwei Wang</name>
    </author>
    <author>
      <name>Yin Li</name>
    </author>
    <author>
      <name>Svetlana Lazebnik</name>
    </author>
    <link href="http://arxiv.org/abs/1511.06078v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06078v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06106v2</id>
    <updated>2015-11-27T03:09:58Z</updated>
    <published>2015-11-19T09:57:14Z</published>
    <title>Quantitative Analysis of Particles Segregation</title>
    <summary>  Segregation is a popular phenomenon. It has considerable effects on material
performance. To the author's knowledge, there is still no automated objective
quantitative indicator for segregation. In order to full fill this task,
segregation of particles is analyzed. Edges of the particles are extracted from
the digital picture. Then, the whole picture of particles is splintered to
small rectangles with the same shape. Statistical index of the edges in each
rectangle is calculated. Accordingly, segregation between the indexes
corresponding to the rectangles is evaluated. The results show coincident with
subjective evaluated results. Further more, it can be implemented as an
automated system, which would facilitate the materials quality control
mechanism during production process.
</summary>
    <author>
      <name>Ting Peng</name>
    </author>
    <author>
      <name>Aiping Qu</name>
    </author>
    <author>
      <name>Xiaoling Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1511.06106v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06106v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06181v2</id>
    <updated>2015-12-01T10:56:34Z</updated>
    <published>2015-11-19T14:25:04Z</published>
    <title>What Players do with the Ball: A Physically Constrained Interaction
  Modeling</title>
    <summary>  Tracking the ball is critical for video-based analysis of team sports.
However, it is difficult, especially in low-resolution images, due to the small
size of the ball, its speed that creates motion blur, and its often being
occluded by players. In this paper, we propose a generic and principled
approach to modeling the interaction between the ball and the players while
also imposing appropriate physical constraints on the ball's trajectory. We
show that our approach, formulated in terms of a Mixed Integer Program, is more
robust and more accurate than several state-of-the-art approaches on real-life
volleyball, basketball, and soccer sequences.
</summary>
    <author>
      <name>Andrii Maksai</name>
    </author>
    <author>
      <name>Xinchao Wang</name>
    </author>
    <author>
      <name>Pascal Fua</name>
    </author>
    <link href="http://arxiv.org/abs/1511.06181v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06181v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06276v1</id>
    <updated>2015-11-19T17:41:08Z</updated>
    <published>2015-11-19T17:41:08Z</published>
    <title>Faster method for Deep Belief Network based Object classification using
  DWT</title>
    <summary>  A Deep Belief Network (DBN) requires large, multiple hidden layers with high
number of hidden units to learn good features from the raw pixels of large
images. This implies more training time as well as computational complexity. By
integrating DBN with Discrete Wavelet Transform (DWT), both training time and
computational complexity can be reduced. The low resolution images obtained
after application of DWT are used to train multiple DBNs. The results obtained
from these DBNs are combined using a weighted voting algorithm. The performance
of this method is found to be competent and faster in comparison with that of
traditional DBNs.
</summary>
    <author>
      <name>Saurabh Sihag</name>
    </author>
    <author>
      <name>Pranab Kumar Dutta</name>
    </author>
    <link href="http://arxiv.org/abs/1511.06276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06494v1</id>
    <updated>2015-11-20T05:23:12Z</updated>
    <published>2015-11-20T05:23:12Z</published>
    <title>Bidirectional Warping of Active Appearance Model</title>
    <summary>  Active Appearance Model (AAM) is a commonly used method for facial image
analysis with applications in face identification and facial expression
recognition. This paper proposes a new approach based on image alignment for
AAM fitting called bidirectional warping. Previous approaches warp either the
input image or the appearance template. We propose to warp both the input
image, using incremental update by an affine transformation, and the appearance
template, using an inverse compositional approach. Our experimental results on
Multi-PIE face database show that the bidirectional approach outperforms
state-of-the-art inverse compositional fitting approaches in extracting
landmark points of faces with shape and pose variations.
</summary>
    <author>
      <name>Ali Mollahosseini</name>
    </author>
    <author>
      <name>Mohammad H. Mahoor</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CVPRW.2013.129</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CVPRW.2013.129" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2013 IEEE Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.06494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06702v2</id>
    <updated>2016-08-02T10:14:07Z</updated>
    <published>2015-11-20T17:34:17Z</published>
    <title>Multi-view 3D Models from Single Images with a Convolutional Network</title>
    <summary>  We present a convolutional network capable of inferring a 3D representation
of a previously unseen object given a single image of this object. Concretely,
the network can predict an RGB image and a depth map of the object as seen from
an arbitrary view. Several of these depth maps fused together give a full point
cloud of the object. The point cloud can in turn be transformed into a surface
mesh. The network is trained on renderings of synthetic 3D models of cars and
chairs. It successfully deals with objects on cluttered background and
generates reasonable predictions for real images of cars.
</summary>
    <author>
      <name>Maxim Tatarchenko</name>
    </author>
    <author>
      <name>Alexey Dosovitskiy</name>
    </author>
    <author>
      <name>Thomas Brox</name>
    </author>
    <link href="http://arxiv.org/abs/1511.06702v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06702v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06911v1</id>
    <updated>2015-11-21T17:55:14Z</updated>
    <published>2015-11-21T17:55:14Z</published>
    <title>Screen Content Image Segmentation Using Sparse-Smooth Decomposition</title>
    <summary>  Sparse decomposition has been extensively used for different applications
including signal compression and denoising and document analysis. In this
paper, sparse decomposition is used for image segmentation. The proposed
algorithm separates the background and foreground using a sparse-smooth
decomposition technique such that the smooth and sparse components correspond
to the background and foreground respectively. This algorithm is tested on
several test images from HEVC test sequences and is shown to have superior
performance over other methods, such as the hierarchical k-means clustering in
DjVu. This segmentation algorithm can also be used for text extraction, video
compression and medical image segmentation.
</summary>
    <author>
      <name>Shervin Minaee</name>
    </author>
    <author>
      <name>Amirali Abdolrashidi</name>
    </author>
    <author>
      <name>Yao Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACSSC.2015.7421331</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACSSC.2015.7421331" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Asilomar Conference on Signals, Systems and Computers, IEEE, 2015,
  (to Appear)</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.06911v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06911v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.07394v2</id>
    <updated>2016-01-10T13:26:23Z</updated>
    <published>2015-11-23T20:17:18Z</published>
    <title>Where To Look: Focus Regions for Visual Question Answering</title>
    <summary>  We present a method that learns to answer visual questions by selecting image
regions relevant to the text-based query. Our method exhibits significant
improvements in answering questions such as "what color," where it is necessary
to evaluate a specific location, and "what room," where it selectively
identifies informative image regions. Our model is tested on the VQA dataset
which is the largest human-annotated visual question answering dataset to our
knowledge.
</summary>
    <author>
      <name>Kevin J. Shih</name>
    </author>
    <author>
      <name>Saurabh Singh</name>
    </author>
    <author>
      <name>Derek Hoiem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to CVPR2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.07394v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.07394v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.08177v1</id>
    <updated>2015-11-25T19:45:03Z</updated>
    <published>2015-11-25T19:45:03Z</published>
    <title>Exploring Person Context and Local Scene Context for Object Detection</title>
    <summary>  In this paper we explore two ways of using context for object detection. The
first model focusses on people and the objects they commonly interact with,
such as fashion and sports accessories. The second model considers more general
object detection and uses the spatial relationships between objects and between
objects and scenes. Our models are able to capture precise spatial
relationships between the context and the object of interest, and make
effective use of the appearance of the contextual region. On the newly released
COCO dataset, our models provide relative improvements of up to 5% over
CNN-based state-of-the-art detectors, with the gains concentrated on hard cases
such as small objects (10% relative improvement).
</summary>
    <author>
      <name>Saurabh Gupta</name>
    </author>
    <author>
      <name>Bharath Hariharan</name>
    </author>
    <author>
      <name>Jitendra Malik</name>
    </author>
    <link href="http://arxiv.org/abs/1511.08177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.08177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.08951v1</id>
    <updated>2015-11-29T00:47:19Z</updated>
    <published>2015-11-29T00:47:19Z</published>
    <title>MidRank: Learning to rank based on subsequences</title>
    <summary>  We present a supervised learning to rank algorithm that effectively orders
images by exploiting the structure in image sequences. Most often in the
supervised learning to rank literature, ranking is approached either by
analyzing pairs of images or by optimizing a list-wise surrogate loss function
on full sequences. In this work we propose MidRank, which learns from
moderately sized sub-sequences instead. These sub-sequences contain useful
structural ranking information that leads to better learnability during
training and better generalization during testing. By exploiting sub-sequences,
the proposed MidRank improves ranking accuracy considerably on an extensive
array of image ranking applications and datasets.
</summary>
    <author>
      <name>Basura Fernando</name>
    </author>
    <author>
      <name>Efstratios Gavves</name>
    </author>
    <author>
      <name>Damien Muselet</name>
    </author>
    <author>
      <name>Tinne Tuytelaars</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICCV 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.08951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.08951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.09030v1</id>
    <updated>2015-11-29T15:52:00Z</updated>
    <published>2015-11-29T15:52:00Z</published>
    <title>On-line Recognition of Handwritten Mathematical Symbols</title>
    <summary>  Finding the name of an unknown symbol is often hard, but writing the symbol
is easy. This bachelor's thesis presents multiple systems that use the pen
trajectory to classify handwritten symbols. Five preprocessing steps, one data
augmentation algorithm, five features and five variants for multilayer
Perceptron training were evaluated using 166898 recordings which were collected
with two crowdsourcing projects. The evaluation results of these 21 experiments
were used to create an optimized recognizer which has a TOP1 error of less than
17.5% and a TOP3 error of 4.0%. This is an improvement of 18.5% for the TOP1
error and 29.7% for the TOP3 error.
</summary>
    <author>
      <name>Martin Thoma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5445/IR/1000048047</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5445/IR/1000048047" rel="related"/>
    <link href="http://arxiv.org/abs/1511.09030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.09030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.00607v1</id>
    <updated>2015-12-02T08:25:23Z</updated>
    <published>2015-12-02T08:25:23Z</published>
    <title>Double Sparse Multi-Frame Image Super Resolution</title>
    <summary>  A large number of image super resolution algorithms based on the sparse
coding are proposed, and some algorithms realize the multi-frame super
resolution. In multi-frame super resolution based on the sparse coding, both
accurate image registration and sparse coding are required. Previous study on
multi-frame super resolution based on sparse coding firstly apply block
matching for image registration, followed by sparse coding to enhance the image
resolution. In this paper, these two problems are solved by optimizing a single
objective function. The results of numerical experiments support the
effectiveness of the proposed approch.
</summary>
    <author>
      <name>Toshiyuki Kato</name>
    </author>
    <author>
      <name>Hideitsu Hino</name>
    </author>
    <author>
      <name>Noboru Murata</name>
    </author>
    <link href="http://arxiv.org/abs/1512.00607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.00607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.00717v1</id>
    <updated>2015-12-02T14:49:12Z</updated>
    <published>2015-12-02T14:49:12Z</published>
    <title>MMSE Estimation for Poisson Noise Removal in Images</title>
    <summary>  Poisson noise suppression is an important preprocessing step in several
applications, such as medical imaging, microscopy, and astronomical imaging. In
this work, we propose a novel patch-wise Poisson noise removal strategy, in
which the MMSE estimator is utilized in order to produce the denoising result
for each image patch. Fast and accurate computation of the MMSE estimator is
carried out using k-d tree search followed by search in the K-nearest neighbor
graph. Our experiments show that the proposed method is the preferable choice
for low signal-to-noise ratios.
</summary>
    <author>
      <name>Stanislav Pyatykh</name>
    </author>
    <author>
      <name>Jürgen Hesser</name>
    </author>
    <link href="http://arxiv.org/abs/1512.00717v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.00717v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.00747v1</id>
    <updated>2015-12-02T15:57:59Z</updated>
    <published>2015-12-02T15:57:59Z</published>
    <title>Active Learning for Delineation of Curvilinear Structures</title>
    <summary>  Many recent delineation techniques owe much of their increased effectiveness
to path classification algorithms that make it possible to distinguish
promising paths from others. The downside of this development is that they
require annotated training data, which is tedious to produce.
  In this paper, we propose an Active Learning approach that considerably
speeds up the annotation process. Unlike standard ones, it takes advantage of
the specificities of the delineation problem. It operates on a graph and can
reduce the training set size by up to 80% without compromising the
reconstruction quality.
  We will show that our approach outperforms conventional ones on various
biomedical and natural image datasets, thus showing that it is broadly
applicable.
</summary>
    <author>
      <name>Agata Mosinska</name>
    </author>
    <author>
      <name>Raphael Sznitman</name>
    </author>
    <author>
      <name>Przemysław Głowacki</name>
    </author>
    <author>
      <name>Pascal Fua</name>
    </author>
    <link href="http://arxiv.org/abs/1512.00747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.00747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01515v1</id>
    <updated>2015-12-04T19:14:57Z</updated>
    <published>2015-12-04T19:14:57Z</published>
    <title>ASIST: Automatic Semantically Invariant Scene Transformation</title>
    <summary>  We present ASIST, a technique for transforming point clouds by replacing
objects with their semantically equivalent counterparts. Transformations of
this kind have applications in virtual reality, repair of fused scans, and
robotics. ASIST is based on a unified formulation of semantic labeling and
object replacement; both result from minimizing a single objective. We present
numerical tools for the efficient solution of this optimization problem. The
method is experimentally assessed on new datasets of both synthetic and real
point clouds, and is additionally compared to two recent works on object
replacement on data from the corresponding papers.
</summary>
    <author>
      <name>Or Litany</name>
    </author>
    <author>
      <name>Tal Remez</name>
    </author>
    <author>
      <name>Daniel Freedman</name>
    </author>
    <author>
      <name>Lior Shapira</name>
    </author>
    <author>
      <name>Alex Bronstein</name>
    </author>
    <author>
      <name>Ran Gal</name>
    </author>
    <link href="http://arxiv.org/abs/1512.01515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01774v1</id>
    <updated>2015-12-06T10:59:36Z</updated>
    <published>2015-12-06T10:59:36Z</published>
    <title>Image reconstruction from dense binary pixels</title>
    <summary>  Recently, the dense binary pixel Gigavision camera had been introduced,
emulating a digital version of the photographic film. While seems to be a
promising solution for HDR imaging, its output is not directly usable and
requires an image reconstruction process. In this work, we formulate this
problem as the minimization of a convex objective combining a
maximum-likelihood term with a sparse synthesis prior. We present MLNet - a
novel feed-forward neural network, producing acceptable output quality at a
fixed complexity and is two orders of magnitude faster than iterative
algorithms. We present state of the art results in the abstract.
</summary>
    <author>
      <name>Or Litany</name>
    </author>
    <author>
      <name>Tal Remez</name>
    </author>
    <author>
      <name>Alex Bronstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Signal Processing with Adaptive Sparse Structured Representations
  (SPARS 2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.01774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01815v2</id>
    <updated>2016-04-10T14:17:18Z</updated>
    <published>2015-12-06T18:30:28Z</published>
    <title>PatchBatch: a Batch Augmented Loss for Optical Flow</title>
    <summary>  We propose a new pipeline for optical flow computation, based on Deep
Learning techniques. We suggest using a Siamese CNN to independently, and in
parallel, compute the descriptors of both images. The learned descriptors are
then compared efficiently using the L2 norm and do not require network
processing of patch pairs. The success of the method is based on an innovative
loss function that computes higher moments of the loss distributions for each
training batch. Combined with an Approximate Nearest Neighbor patch matching
method and a flow interpolation technique, state of the art performance is
obtained on the most challenging and competitive optical flow benchmarks.
</summary>
    <author>
      <name>David Gadot</name>
    </author>
    <author>
      <name>Lior Wolf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.01815v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01815v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02072v1</id>
    <updated>2015-12-07T14:50:43Z</updated>
    <published>2015-12-07T14:50:43Z</published>
    <title>On The Continuous Steering of the Scale of Tight Wavelet Frames</title>
    <summary>  In analogy with steerable wavelets, we present a general construction of
adaptable tight wavelet frames, with an emphasis on scaling operations. In
particular, the derived wavelets can be "dilated" by a procedure comparable to
the operation of steering steerable wavelets. The fundamental aspects of the
construction are the same: an admissible collection of Fourier multipliers is
used to extend a tight wavelet frame, and the "scale" of the wavelets is
adapted by scaling the multipliers. As an application, the proposed wavelets
can be used to improve the frequency localization. Importantly, the localized
frequency bands specified by this construction can be scaled efficiently using
matrix multiplication.
</summary>
    <author>
      <name>Zsuzsanna Püspöki</name>
    </author>
    <author>
      <name>John Paul Ward</name>
    </author>
    <author>
      <name>Daniel Sage</name>
    </author>
    <author>
      <name>Michael Unser</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/15M1033885</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/15M1033885" rel="related"/>
    <link href="http://arxiv.org/abs/1512.02072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02167v2</id>
    <updated>2015-12-15T05:17:49Z</updated>
    <published>2015-12-07T19:00:54Z</published>
    <title>Simple Baseline for Visual Question Answering</title>
    <summary>  We describe a very simple bag-of-words baseline for visual question
answering. This baseline concatenates the word features from the question and
CNN features from the image to predict the answer. When evaluated on the
challenging VQA dataset [2], it shows comparable performance to many recent
approaches using recurrent neural networks. To explore the strength and
weakness of the trained model, we also provide an interactive web demo and
open-source code. .
</summary>
    <author>
      <name>Bolei Zhou</name>
    </author>
    <author>
      <name>Yuandong Tian</name>
    </author>
    <author>
      <name>Sainbayar Sukhbaatar</name>
    </author>
    <author>
      <name>Arthur Szlam</name>
    </author>
    <author>
      <name>Rob Fergus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">One comparison method's scores are put into the correct column, and a
  new experiment of generating attention map is added</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.02167v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02167v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02355v1</id>
    <updated>2015-12-08T06:48:39Z</updated>
    <published>2015-12-08T06:48:39Z</published>
    <title>Is Hamming distance the only way for matching binary image feature
  descriptors?</title>
    <summary>  Brute force matching of binary image feature descriptors is conventionally
performed using the Hamming distance. This paper assesses the use of
alternative metrics in order to see whether they can produce feature
correspondences that yield more accurate homography matrices. Two statistical
tests, namely ANOVA (Analysis of Variance) and McNemar's test were employed for
evaluation. Results show that Jackard-Needham and Dice metrics can display
better performance for some descriptors. Yet, these performance differences
were not found to be statistically significant.
</summary>
    <author>
      <name>Erkan Bostanci</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1049/el.2014.0773</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1049/el.2014.0773" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronics Letters (2014), vol. 50, iss. 11, pp. 806-808</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1512.02355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02949v1</id>
    <updated>2015-12-09T17:17:29Z</updated>
    <published>2015-12-09T17:17:29Z</published>
    <title>Video captioning with recurrent networks based on frame- and video-level
  features and visual content classification</title>
    <summary>  In this paper, we describe the system for generating textual descriptions of
short video clips using recurrent neural networks (RNN), which we used while
participating in the Large Scale Movie Description Challenge 2015 in ICCV 2015.
Our work builds on static image captioning systems with RNN based language
models and extends this framework to videos utilizing both static image
features and video-specific features. In addition, we study the usefulness of
visual content classifiers as a source of additional information for caption
generation. With experimental results we show that utilizing keyframe based
features, dense trajectory video features and content classifier outputs
together gives better performance than any one of them individually.
</summary>
    <author>
      <name>Rakshith Shetty</name>
    </author>
    <author>
      <name>Jorma Laaksonen</name>
    </author>
    <link href="http://arxiv.org/abs/1512.02949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03131v1</id>
    <updated>2015-12-10T03:23:54Z</updated>
    <published>2015-12-10T03:23:54Z</published>
    <title>Deep Learning Algorithms with Applications to Video Analytics for A
  Smart City: A Survey</title>
    <summary>  Deep learning has recently achieved very promising results in a wide range of
areas such as computer vision, speech recognition and natural language
processing. It aims to learn hierarchical representations of data by using deep
architecture models. In a smart city, a lot of data (e.g. videos captured from
many distributed sensors) need to be automatically processed and analyzed. In
this paper, we review the deep learning algorithms applied to video analytics
of smart city in terms of different research topics: object detection, object
tracking, face recognition, image classification and scene labeling.
</summary>
    <author>
      <name>Li Wang</name>
    </author>
    <author>
      <name>Dennis Sng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 18 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.03131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03424v1</id>
    <updated>2015-12-10T06:31:59Z</updated>
    <published>2015-12-10T06:31:59Z</published>
    <title>Evaluation of Object Detection Proposals Under Condition Variations</title>
    <summary>  Object detection is a fundamental task in many computer vision applications,
therefore the importance of evaluating the quality of object detection is well
acknowledged in this domain. This process gives insight into the capabilities
of methods in handling environmental changes. In this paper, a new method for
object detection is introduced that combines the Selective Search and
EdgeBoxes. We tested these three methods under environmental variations. Our
experiments demonstrate the outperformance of the combination method under
illumination and view point variations.
</summary>
    <author>
      <name>Fahimeh Rezazadegan</name>
    </author>
    <author>
      <name>Sareh Shirazi</name>
    </author>
    <author>
      <name>Michael Milford</name>
    </author>
    <author>
      <name>Ben Upcroft</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 6 figures, CVPR Workshop, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.03424v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03424v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03617v1</id>
    <updated>2015-12-11T12:17:35Z</updated>
    <published>2015-12-11T12:17:35Z</published>
    <title>Robust Dictionary based Data Representation</title>
    <summary>  The robustness to noise and outliers is an important issue in linear
representation in real applications. We focus on the problem that samples are
grossly corrupted, which is also the 'sample specific' corruptions problem. A
reasonable assumption is that corrupted samples cannot be represented by the
dictionary while clean samples can be well represented. This assumption is
enforced in this paper by investigating the coefficients of corrupted samples.
Concretely, we require the coefficients of corrupted samples be zero. In this
way, the representation quality of clean data can be assured without the effect
of corrupted data. At last, a robust dictionary based data representation
approach and its sparse representation version are proposed, which have
directive significance for future applications.
</summary>
    <author>
      <name>Wei-Ya Ren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. 2015.12.10</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.03617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03958v1</id>
    <updated>2015-12-12T20:24:43Z</updated>
    <published>2015-12-12T20:24:43Z</published>
    <title>RNN Fisher Vectors for Action Recognition and Image Annotation</title>
    <summary>  Recurrent Neural Networks (RNNs) have had considerable success in classifying
and predicting sequences. We demonstrate that RNNs can be effectively used in
order to encode sequences and provide effective representations. The
methodology we use is based on Fisher Vectors, where the RNNs are the
generative probabilistic models and the partial derivatives are computed using
backpropagation. State of the art results are obtained in two central but
distant tasks, which both rely on sequences: video action recognition and image
annotation. We also show a surprising transfer learning result from the task of
image annotation to the task of video action recognition.
</summary>
    <author>
      <name>Guy Lev</name>
    </author>
    <author>
      <name>Gil Sadeh</name>
    </author>
    <author>
      <name>Benjamin Klein</name>
    </author>
    <author>
      <name>Lior Wolf</name>
    </author>
    <link href="http://arxiv.org/abs/1512.03958v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03958v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.03993v1</id>
    <updated>2015-12-13T02:58:56Z</updated>
    <published>2015-12-13T02:58:56Z</published>
    <title>Deep Tracking: Visual Tracking Using Deep Convolutional Networks</title>
    <summary>  In this paper, we study a discriminatively trained deep convolutional network
for the task of visual tracking. Our tracker utilizes both motion and
appearance features that are extracted from a pre-trained dual stream deep
convolution network. We show that the features extracted from our dual-stream
network can provide rich information about the target and this leads to
competitive performance against state of the art tracking methods on a visual
tracking benchmark.
</summary>
    <author>
      <name>Meera Hahn</name>
    </author>
    <author>
      <name>Si Chen</name>
    </author>
    <author>
      <name>Afshin Dehghan</name>
    </author>
    <link href="http://arxiv.org/abs/1512.03993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.03993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.04219v1</id>
    <updated>2015-12-14T08:50:44Z</updated>
    <published>2015-12-14T08:50:44Z</published>
    <title>On the Relation between two Rotation Metrics</title>
    <summary>  In their work "Global Optimization through Rotation Space Search", Richard
Hartley and Fredrik Kahl introduce a global optimization strategy for problems
in geometric computer vision, based on rotation space search using a
branch-and-bound algorithm. In its core, Lemma 2 of their publication is the
important foundation for a class of global optimization algorithms, which is
adopted over a wide range of problems in subsequent publications. This lemma
relates a metric on rotations represented by rotation matrices with a metric on
rotations in axis-angle representation. This work focuses on a proof for this
relationship, which is based on Rodrigues' Rotation Theorem for the composition
of rotations in axis-angle representation.
</summary>
    <author>
      <name>Thomas Ruland</name>
    </author>
    <link href="http://arxiv.org/abs/1512.04219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.04219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.05653v1</id>
    <updated>2015-12-16T10:45:17Z</updated>
    <published>2015-12-16T10:45:17Z</published>
    <title>Effects of GIMP Retinex Filtering Evaluated by the Image Entropy</title>
    <summary>  A GIMP Retinex filtering can be used for enhancing images, with good results
on foggy images, as recently discussed. Since this filter has some parameters
that can be adjusted to optimize the output image, several approaches can be
decided according to desired results. Here, as a criterion for optimizing the
filtering parameters, we consider the maximization of the image entropy. We
use, besides the Shannon entropy, also a generalized entropy.
</summary>
    <author>
      <name>A. C. Sparavigna</name>
    </author>
    <author>
      <name>R. Marazzato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: Image Processing, Foggy Images, Retinex, Shannon Entropy,
  Generalized Entropies, Kaniadakis Entropy</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.05653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.05653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.06790v2</id>
    <updated>2016-06-17T11:58:47Z</updated>
    <published>2015-12-21T20:01:53Z</published>
    <title>Car Segmentation and Pose Estimation using 3D Object Models</title>
    <summary>  Image segmentation and 3D pose estimation are two key cogs in any algorithm
for scene understanding. However, state-of-the-art CRF-based models for image
segmentation rely mostly on 2D object models to construct top-down high-order
potentials. In this paper, we propose new top-down potentials for image
segmentation and pose estimation based on the shape and volume of a 3D object
model. We show that these complex top-down potentials can be easily decomposed
into standard forms for efficient inference in both the segmentation and pose
estimation tasks. Experiments on a car dataset show that knowledge of
segmentation helps perform pose estimation better and vice versa.
</summary>
    <author>
      <name>Siddharth Mahendran</name>
    </author>
    <author>
      <name>René Vidal</name>
    </author>
    <link href="http://arxiv.org/abs/1512.06790v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.06790v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.07080v1</id>
    <updated>2015-12-22T13:35:10Z</updated>
    <published>2015-12-22T13:35:10Z</published>
    <title>Cost-based Feature Transfer for Vehicle Occupant Classification</title>
    <summary>  Knowledge of human presence and interaction in a vehicle is of growing
interest to vehicle manufacturers for design and safety purposes. We present a
framework to perform the tasks of occupant detection and occupant
classification for automatic child locks and airbag suppression. It operates
for all passenger seats, using a single overhead camera. A transfer learning
technique is introduced to make full use of training data from all seats whilst
still maintaining some control over the bias, necessary for a system designed
to penalize certain misclassifications more than others. An evaluation is
performed on a challenging dataset with both weighted and unweighted
classifiers, demonstrating the effectiveness of the transfer process.
</summary>
    <author>
      <name>Toby Perrett</name>
    </author>
    <author>
      <name>Majid Mirmehdi</name>
    </author>
    <author>
      <name>Eduardo Dias</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.07080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.07080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00212v1</id>
    <updated>2016-01-02T19:24:24Z</updated>
    <published>2016-01-02T19:24:24Z</published>
    <title>Supervised Texture Segmentation: A Comparative Study</title>
    <summary>  This paper aims to compare between four different types of feature extraction
approaches in terms of texture segmentation. The feature extraction methods
that were used for segmentation are Gabor filters (GF), Gaussian Markov random
fields (GMRF), run-length matrix (RLM) and co-occurrence matrix (GLCM). It was
shown that the GF performed best in terms of quality of segmentation while the
GLCM localises the texture boundaries better as compared to the other methods.
</summary>
    <author>
      <name>Omar S. Al-Kadi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/AEECT.2011.6132529</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/AEECT.2011.6132529" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Jordan Conf. on Applied Electrical Engineering and Computing
  Technologies, Jordan, 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.00212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00311v4</id>
    <updated>2017-04-19T10:04:51Z</updated>
    <published>2016-01-03T16:51:14Z</published>
    <title>How can one sample images with sampling rates close to the theoretical
  minimum?</title>
    <summary>  A problem is addressed of minimization of the number of measurements needed
for digital image acquisition and reconstruction with a given accuracy. A
sampling theory based method of image sampling and reconstruction is suggested
that allows to draw near the minimal rate of image sampling defined by the
sampling theory. Presented and discussed are also results of experimental
verification of the method and its possible applicability extensions.
</summary>
    <author>
      <name>Leonid Yaroslavsky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/2040-8986/aa65b7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/2040-8986/aa65b7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Optics, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.00311v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00311v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01145v2</id>
    <updated>2016-08-07T09:21:08Z</updated>
    <published>2016-01-06T11:25:36Z</published>
    <title>Image-based Vehicle Analysis using Deep Neural Network: A Systematic
  Study</title>
    <summary>  We address the vehicle detection and classification problems using Deep
Neural Networks (DNNs) approaches. Here we answer to questions that are
specific to our application including how to utilize DNN for vehicle detection,
what features are useful for vehicle classification, and how to extend a model
trained on a limited size dataset, to the cases of extreme lighting condition.
Answering these questions we propose our approach that outperforms
state-of-the-art methods, and achieves promising results on image with extreme
lighting conditions.
</summary>
    <author>
      <name>Yiren Zhou</name>
    </author>
    <author>
      <name>Hossein Nejati</name>
    </author>
    <author>
      <name>Thanh-Toan Do</name>
    </author>
    <author>
      <name>Ngai-Man Cheung</name>
    </author>
    <author>
      <name>Lynette Cheah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.01145v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01145v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01339v1</id>
    <updated>2016-01-06T22:06:58Z</updated>
    <published>2016-01-06T22:06:58Z</published>
    <title>Quality Adaptive Low-Rank Based JPEG Decoding with Applications</title>
    <summary>  Small compression noises, despite being transparent to human eyes, can
adversely affect the results of many image restoration processes, if left
unaccounted for. Especially, compression noises are highly detrimental to
inverse operators of high-boosting (sharpening) nature, such as deblurring and
superresolution against a convolution kernel. By incorporating the non-linear
DCT quantization mechanism into the formulation for image restoration, we
propose a new sparsity-based convex programming approach for joint compression
noise removal and image restoration. Experimental results demonstrate
significant performance gains of the new approach over existing image
restoration methods.
</summary>
    <author>
      <name>Xiao Shu</name>
    </author>
    <author>
      <name>Xiaolin Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1601.01339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01422v1</id>
    <updated>2016-01-07T07:29:45Z</updated>
    <published>2016-01-07T07:29:45Z</published>
    <title>Stochastic Dykstra Algorithms for Metric Learning on Positive
  Semi-Definite Cone</title>
    <summary>  Recently, covariance descriptors have received much attention as powerful
representations of set of points. In this research, we present a new metric
learning algorithm for covariance descriptors based on the Dykstra algorithm,
in which the current solution is projected onto a half-space at each iteration,
and runs at O(n^3) time. We empirically demonstrate that randomizing the order
of half-spaces in our Dykstra-based algorithm significantly accelerates the
convergence to the optimal solution. Furthermore, we show that our approach
yields promising experimental results on pattern recognition tasks.
</summary>
    <author>
      <name>Tomoki Matsuzawa</name>
    </author>
    <author>
      <name>Raissa Relator</name>
    </author>
    <author>
      <name>Jun Sese</name>
    </author>
    <author>
      <name>Tsuyoshi Kato</name>
    </author>
    <link href="http://arxiv.org/abs/1601.01422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01885v1</id>
    <updated>2016-01-08T14:25:20Z</updated>
    <published>2016-01-08T14:25:20Z</published>
    <title>Visual Script and Language Identification</title>
    <summary>  In this paper we introduce a script identification method based on
hand-crafted texture features and an artificial neural network. The proposed
pipeline achieves near state-of-the-art performance for script identification
of video-text and state-of-the-art performance on visual language
identification of handwritten text. More than using the deep network as a
classifier, the use of its intermediary activations as a learned metric
demonstrates remarkable results and allows the use of discriminative models on
unknown classes. Comparative experiments in video-text and text in the wild
datasets provide insights on the internals of the proposed deep network.
</summary>
    <author>
      <name>Anguelos Nicolaou</name>
    </author>
    <author>
      <name>Andrew Bagdanov</name>
    </author>
    <author>
      <name>Lluis Gomez-Bigorda</name>
    </author>
    <author>
      <name>Dimosthenis Karatzas</name>
    </author>
    <link href="http://arxiv.org/abs/1601.01885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.03642v1</id>
    <updated>2016-01-12T23:28:07Z</updated>
    <published>2016-01-12T23:28:07Z</published>
    <title>Creativity in Machine Learning</title>
    <summary>  Recent machine learning techniques can be modified to produce creative
results. Those results did not exist before; it is not a trivial combination of
the data which was fed into the machine learning system. The obtained results
come in multiple forms: As images, as text and as audio.
  This paper gives a high level overview of how they are created and gives some
examples. It is meant to be a summary of the current work and give people who
are new to machine learning some starting points.
</summary>
    <author>
      <name>Martin Thoma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.03642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04589v1</id>
    <updated>2016-01-18T16:31:37Z</updated>
    <published>2016-01-18T16:31:37Z</published>
    <title>Combining Markov Random Fields and Convolutional Neural Networks for
  Image Synthesis</title>
    <summary>  This paper studies a combination of generative Markov random field (MRF)
models and discriminatively trained deep convolutional neural networks (dCNNs)
for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNN
feature pyramid, controling the image layout at an abstract level. We apply the
method to both photographic and non-photo-realistic (artwork) synthesis tasks.
The MRF regularizer prevents over-excitation artifacts and reduces implausible
feature mixtures common to previous dCNN inversion approaches, permitting
synthezing photographic content with increased visual plausibility. Unlike
standard MRF-based texture synthesis, the combined system can both match and
adapt local features with considerable variability, yielding results far out of
reach of classic generative MRF methods.
</summary>
    <author>
      <name>Chuan Li</name>
    </author>
    <author>
      <name>Michael Wand</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.04589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04871v2</id>
    <updated>2016-01-24T08:01:37Z</updated>
    <published>2016-01-19T11:02:26Z</published>
    <title>Eye detection in digital images: challenges and solutions</title>
    <summary>  Eye Detection has an important role in the field of biometric identification
and known as one method of person's identification. In recent years, many
efforts have been done which can detect eye automatically and with different
image conditions. However, each method has its own drawbacks which can control
some of these conditions. In this paper, different methods of eye detection
will be categorized and explained. In each category, the advantages and
disadvantages of each method will be presented.
</summary>
    <author>
      <name>Mitra Montazeri</name>
    </author>
    <author>
      <name>Mahdieh Montazeri</name>
    </author>
    <author>
      <name>Saeid Saryazdi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2th National Conference of Electrical Engineering (NEEC2011),2011,
  Esfehan, Iran, in Persian</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.04871v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04871v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.05116v1</id>
    <updated>2016-01-19T22:15:48Z</updated>
    <published>2016-01-19T22:15:48Z</published>
    <title>A Theory of Local Matching: SIFT and Beyond</title>
    <summary>  Why has SIFT been so successful? Why its extension, DSP-SIFT, can further
improve SIFT? Is there a theory that can explain both? How can such theory
benefit real applications? Can it suggest new algorithms with reduced
computational complexity or new descriptors with better accuracy for matching?
We construct a general theory of local descriptors for visual matching. Our
theory relies on concepts in energy minimization and heat diffusion. We show
that SIFT and DSP-SIFT approximate the solution the theory suggests. In
particular, DSP-SIFT gives a better approximation to the theoretical solution;
justifying why DSP-SIFT outperforms SIFT. Using the developed theory, we derive
new descriptors that have fewer parameters and are potentially better in
handling affine deformations.
</summary>
    <author>
      <name>Hossein Mobahi</name>
    </author>
    <author>
      <name>Stefano Soatto</name>
    </author>
    <link href="http://arxiv.org/abs/1601.05116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.05116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01125v2</id>
    <updated>2016-10-03T13:25:00Z</updated>
    <published>2016-02-02T21:43:15Z</published>
    <title>Fitting a 3D Morphable Model to Edges: A Comparison Between Hard and
  Soft Correspondences</title>
    <summary>  We propose a fully automatic method for fitting a 3D morphable model to
single face images in arbitrary pose and lighting. Our approach relies on
geometric features (edges and landmarks) and, inspired by the iterated closest
point algorithm, is based on computing hard correspondences between model
vertices and edge pixels. We demonstrate that this is superior to previous work
that uses soft correspondences to form an edge-derived cost surface that is
minimised by nonlinear optimisation.
</summary>
    <author>
      <name>Anil Bas</name>
    </author>
    <author>
      <name>William A. P. Smith</name>
    </author>
    <author>
      <name>Timo Bolkart</name>
    </author>
    <author>
      <name>Stefanie Wuhrer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-54427-4_28</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-54427-4_28" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ACCV 2016 Workshop on Facial Informatics</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.01125v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01125v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01228v1</id>
    <updated>2016-02-03T09:11:45Z</updated>
    <published>2016-02-03T09:11:45Z</published>
    <title>Image and Information</title>
    <summary>  A well-known old adage says that {\em "A picture is worth a thousand words!"}
(attributed to the Chinese philosopher Confucius ca 500 years BC). But more
precisely, what do we mean by information in images? And how can it be
retrieved effectively by machines? We briefly highlight these puzzling
questions in this column. But first of all, let us start by defining more
precisely what is meant by an "Image."
</summary>
    <author>
      <name>Frank Nielsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures. to be published in french by Belin publisher for
  a collaborative book project on "Image and Communication"</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.01228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01728v1</id>
    <updated>2016-02-04T16:20:26Z</updated>
    <published>2016-02-04T16:20:26Z</published>
    <title>NeRD: a Neural Response Divergence Approach to Visual Salience Detection</title>
    <summary>  In this paper, a novel approach to visual salience detection via Neural
Response Divergence (NeRD) is proposed, where synaptic portions of deep neural
networks, previously trained for complex object recognition, are leveraged to
compute low level cues that can be used to compute image region
distinctiveness. Based on this concept , an efficient visual salience detection
framework is proposed using deep convolutional StochasticNets. Experimental
results using CSSD and MSRA10k natural image datasets show that the proposed
NeRD approach can achieve improved performance when compared to
state-of-the-art image saliency approaches, while the attaining low
computational complexity necessary for near-real-time computer vision
applications.
</summary>
    <author>
      <name>M. J. Shafiee</name>
    </author>
    <author>
      <name>P. Siva</name>
    </author>
    <author>
      <name>C. Scharfenberger</name>
    </author>
    <author>
      <name>P. Fieguth</name>
    </author>
    <author>
      <name>A. Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.01728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01895v1</id>
    <updated>2016-02-05T00:17:18Z</updated>
    <published>2016-02-05T00:17:18Z</published>
    <title>Generate Image Descriptions based on Deep RNN and Memory Cells for
  Images Features</title>
    <summary>  Generating natural language descriptions for images is a challenging task.
The traditional way is to use the convolutional neural network (CNN) to extract
image features, followed by recurrent neural network (RNN) to generate
sentences. In this paper, we present a new model that added memory cells to
gate the feeding of image features to the deep neural network. The intuition is
enabling our model to memorize how much information from images should be fed
at each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed
that our model outperforms other state-of-the-art models with higher BLEU
scores.
</summary>
    <author>
      <name>Shijian Tang</name>
    </author>
    <author>
      <name>Song Han</name>
    </author>
    <link href="http://arxiv.org/abs/1602.01895v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01895v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.02822v1</id>
    <updated>2016-02-09T00:07:05Z</updated>
    <published>2016-02-09T00:07:05Z</published>
    <title>Parameterizing Region Covariance: An Efficient Way To Apply Sparse Codes
  On Second Order Statistics</title>
    <summary>  Sparse representations have been successfully applied to signal processing,
computer vision and machine learning. Currently there is a trend to learn
sparse models directly on structure data, such as region covariance. However,
such methods when combined with region covariance often require complex
computation. We present an approach to transform a structured sparse model
learning problem to a traditional vectorized sparse modeling problem by
constructing a Euclidean space representation for region covariance matrices.
Our new representation has multiple advantages. Experiments on several vision
tasks demonstrate competitive performance with the state-of-the-art methods.
</summary>
    <author>
      <name>Xiyang Dai</name>
    </author>
    <author>
      <name>Sameh Khamis</name>
    </author>
    <author>
      <name>Yangmuzi Zhang</name>
    </author>
    <author>
      <name>Larry S. Davis</name>
    </author>
    <link href="http://arxiv.org/abs/1602.02822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.02822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.03256v1</id>
    <updated>2016-02-10T03:52:11Z</updated>
    <published>2016-02-10T03:52:11Z</published>
    <title>Improved Eigenfeature Regularization for Face Identification</title>
    <summary>  In this work, we propose to divide each class (a person) into subclasses
using spatial partition trees which helps in better capturing the
intra-personal variances arising from the appearances of the same individual.
We perform a comprehensive analysis on within-class and within-subclass
eigenspectrums of face images and propose a novel method of eigenspectrum
modeling which extracts discriminative features of faces from both
within-subclass and total or between-subclass scatter matrices. Effective
low-dimensional face discriminative features are extracted for face recognition
(FR) after performing discriminant evaluation in the entire eigenspace.
Experimental results on popular face databases (AR, FERET) and the challenging
unconstrained YouTube Face database show the superiority of our proposed
approach on all three databases.
</summary>
    <author>
      <name>Bappaditya Mandal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, ICIP 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.03256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.03256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.03308v1</id>
    <updated>2016-02-10T09:45:38Z</updated>
    <published>2016-02-10T09:45:38Z</published>
    <title>Gabor Wavelets in Image Processing</title>
    <summary>  This work shows the use of a two-dimensional Gabor wavelets in image
processing. Convolution with such a two-dimensional wavelet can be separated
into two series of one-dimensional ones. The key idea of this work is to
utilize a Gabor wavelet as a multiscale partial differential operator of a
given order. Gabor wavelets are used here to detect edges, corners and blobs. A
performance of such an interest point detector is compared to detectors
utilizing a Haar wavelet and a derivative of a Gaussian function. The proposed
approach may be useful when a fast implementation of the Gabor transform is
available or when the transform is already precomputed.
</summary>
    <author>
      <name>David Barina</name>
    </author>
    <link href="http://arxiv.org/abs/1602.03308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.03308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.03418v2</id>
    <updated>2016-03-13T18:06:34Z</updated>
    <published>2016-02-10T15:48:47Z</published>
    <title>Triplet Similarity Embedding for Face Verification</title>
    <summary>  In this work, we present an unconstrained face verification algorithm and
evaluate it on the recently released IJB-A dataset that aims to push the
boundaries of face verification methods. The proposed algorithm couples a deep
CNN-based approach with a low-dimensional discriminative embedding learnt using
triplet similarity constraints in a large margin fashion. Aside from yielding
performance improvement, this embedding provides significant advantages in
terms of memory and post-processing operations like hashing and visualization.
Experiments on the IJB-A dataset show that the proposed algorithm outperforms
state of the art methods in verification and identification metrics, while
requiring less training time.
</summary>
    <author>
      <name>Swami Sankaranarayanan</name>
    </author>
    <author>
      <name>Azadeh Alavi</name>
    </author>
    <author>
      <name>Rama Chellappa</name>
    </author>
    <link href="http://arxiv.org/abs/1602.03418v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.03418v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.04868v1</id>
    <updated>2016-02-16T00:14:22Z</updated>
    <published>2016-02-16T00:14:22Z</published>
    <title>Deep Feature-based Face Detection on Mobile Devices</title>
    <summary>  We propose a deep feature-based face detector for mobile devices to detect
user's face acquired by the front facing camera. The proposed method is able to
detect faces in images containing extreme pose and illumination variations as
well as partial faces. The main challenge in developing deep feature-based
algorithms for mobile devices is the constrained nature of the mobile platform
and the non-availability of CUDA enabled GPUs on such devices. Our
implementation takes into account the special nature of the images captured by
the front-facing camera of mobile devices and exploits the GPUs present in
mobile devices without CUDA-based frameorks, to meet these challenges.
</summary>
    <author>
      <name>Sayantan Sarkar</name>
    </author>
    <author>
      <name>Vishal M. Patel</name>
    </author>
    <author>
      <name>Rama Chellappa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISBA 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.04868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.04868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05168v1</id>
    <updated>2016-02-16T20:28:16Z</updated>
    <published>2016-02-16T20:28:16Z</published>
    <title>An Approach for Noise Removal on Depth Images</title>
    <summary>  Image based rendering is a fundamental problem in computer vision and
graphics. Modern techniques often rely on depth image for the 3D construction.
However for most of the existing depth cameras, the large and unpredictable
noises can be problematic, which can cause noticeable artifacts in the rendered
results. In this paper, we proposed an efficacious method for depth image noise
removal that can be applied for most RGBD systems. The proposed solution will
benefit many subsequent vision problems such as 3D reconstruction, novel view
rendering, object recognition. Our experimental results demonstrate the
efficacy and accuracy.
</summary>
    <author>
      <name>Rashi Chaudhary</name>
    </author>
    <author>
      <name>Himanshu Dasgupta</name>
    </author>
    <link href="http://arxiv.org/abs/1602.05168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05256v1</id>
    <updated>2016-02-17T00:41:58Z</updated>
    <published>2016-02-17T00:41:58Z</published>
    <title>2D SEM images turn into 3D object models</title>
    <summary>  The scanning electron microscopy (SEM) is probably one the most fascinating
examination approach that has been used since more than two decades to detailed
inspection of micro scale objects. Most of the scanning electron microscopes
could only produce 2D images that could not assist operational analysis of
microscopic surface properties. Computer vision algorithms combined with very
advanced geometry and mathematical approaches turn any SEM into a full 3D
measurement device. This work focuses on a methodical literature review for
automatic 3D surface reconstruction of scanning electron microscope images.
</summary>
    <author>
      <name>Wichai Shanklin</name>
    </author>
    <link href="http://arxiv.org/abs/1602.05256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05439v1</id>
    <updated>2016-02-17T14:47:32Z</updated>
    <published>2016-02-17T14:47:32Z</published>
    <title>Cell segmentation with random ferns and graph-cuts</title>
    <summary>  The progress in imaging techniques have allowed the study of various aspect
of cellular mechanisms. To isolate individual cells in live imaging data, we
introduce an elegant image segmentation framework that effectively extracts
cell boundaries, even in the presence of poor edge details. Our approach works
in two stages. First, we estimate pixel interior/border/exterior class
probabilities using random ferns. Then, we use an energy minimization framework
to compute boundaries whose localization is compliant with the pixel class
probabilities. We validate our approach on a manually annotated dataset.
</summary>
    <author>
      <name>Arnaud Browet</name>
    </author>
    <author>
      <name>Christophe De Vleeschouwer</name>
    </author>
    <author>
      <name>Laurent Jacques</name>
    </author>
    <author>
      <name>Navrita Mathiah</name>
    </author>
    <author>
      <name>Bechara Saykali</name>
    </author>
    <author>
      <name>Isabelle Migeotte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to ICIP</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.05439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05941v1</id>
    <updated>2016-02-18T20:50:50Z</updated>
    <published>2016-02-18T20:50:50Z</published>
    <title>Multi-resolution Compressive Sensing Reconstruction</title>
    <summary>  We consider the problem of reconstructing an image from compressive
measurements using a multi-resolution grid. In this context, the reconstructed
image is divided into multiple regions, each one with a different resolution.
This problem arises in situations where the image to reconstruct contains a
certain region of interest (RoI) that is more important than the rest. Through
a theoretical analysis and simulation experiments we show that the
multi-resolution reconstruction provides a higher quality of the RoI compared
to the traditional single-resolution approach.
</summary>
    <author>
      <name>Adriana Gonzalez</name>
    </author>
    <author>
      <name>Hong Jiang</name>
    </author>
    <author>
      <name>Gang Huang</name>
    </author>
    <author>
      <name>Laurent Jacques</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages; 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.05941v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05941v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
