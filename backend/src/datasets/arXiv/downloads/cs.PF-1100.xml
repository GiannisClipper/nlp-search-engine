<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acs.PF%26id_list%3D%26start%3D0%26max_results%3D1100" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:cs.PF&amp;id_list=&amp;start=0&amp;max_results=1100</title>
  <id>http://arxiv.org/api/CQkZ69LQbR4d5Daibklo/aZaBe8</id>
  <updated>2025-05-27T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">4471</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1100</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/0710.3283v2</id>
    <updated>2007-10-20T08:21:37Z</updated>
    <published>2007-10-17T12:32:29Z</published>
    <title>Effects of Non-Identical Rayleigh Fading on Differential Unitary
  Space-Time Modulation</title>
    <summary>  This paper has been withdrawn by the author.
</summary>
    <author>
      <name>Meixia Tao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/0710.3283v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.3283v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.2347v1</id>
    <updated>2014-12-07T12:21:58Z</updated>
    <published>2014-12-07T12:21:58Z</published>
    <title>Proceedings of the 5th International Workshop on Adaptive Self-tuning
  Computing Systems 2015 (ADAPT'15)</title>
    <summary>  This is the proceedings of the 5th International Workshop on Adaptive
Self-tuning Computing Systems 2015 (ADAPT'15).
</summary>
    <author>
      <name>Christophe Dubach</name>
    </author>
    <author>
      <name>Grigori Fursin</name>
    </author>
    <link href="http://arxiv.org/abs/1412.2347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.2347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.02926v1</id>
    <updated>2019-01-09T20:47:20Z</updated>
    <published>2019-01-09T20:47:20Z</published>
    <title>Three Other Models of Computer System Performance</title>
    <summary>  This note argues for more use of simple models beyond Amdahl's Law:
Bottleneck Analysis, Little's Law, and a M/M/1 Queue.
</summary>
    <author>
      <name>Mark D. Hill</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages + references; 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.02926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.02926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.3100v1</id>
    <updated>2008-08-22T15:27:34Z</updated>
    <published>2008-08-22T15:27:34Z</published>
    <title>Optimizing Compiler for Engineering Problems</title>
    <summary>  New information technologies provide a lot of prospects for performance
improvement. One of them is "Dynamic Source Code Generation and Compilation".
This article shows how this way provides high performance for engineering
problems.
</summary>
    <author>
      <name>Petr R. Ivankov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.3100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.3100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.0880v2</id>
    <updated>2012-02-07T06:35:38Z</updated>
    <published>2011-06-05T06:37:32Z</published>
    <title>Performance Analysis of Sequential Method for HandOver in Cognitive
  Radio Networks</title>
    <summary>  This paper has been withdrawn by the author due to a crucial problem in Lemma
3. This equation must be changed.
</summary>
    <author>
      <name>Hossein Shokri</name>
    </author>
    <author>
      <name>Mohammad Mozaffari</name>
    </author>
    <author>
      <name>Adnan Gavili</name>
    </author>
    <author>
      <name>Masoumeh Nasiri-Kenari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.0880v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.0880v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0093v2</id>
    <updated>2014-09-03T11:40:44Z</updated>
    <published>2014-08-30T09:05:04Z</published>
    <title>Proceedings of the 1st OMNeT++ Community Summit, Hamburg, Germany,
  September 2, 2014</title>
    <summary>  This is the Proceedings of the 1st OMNeT++ Community Summit, which was held
in Hamburg, Germany, September 2, 2014.
</summary>
    <author>
      <name>Anna Förster</name>
    </author>
    <author>
      <name>Christoph Sommer</name>
    </author>
    <author>
      <name>Till Steinbach</name>
    </author>
    <author>
      <name>Matthias Wählisch</name>
    </author>
    <link href="http://arxiv.org/abs/1409.0093v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0093v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6; C.2.0; C.4; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0820v2</id>
    <updated>2015-01-31T23:37:31Z</updated>
    <published>2014-09-02T18:44:48Z</published>
    <title>Network calculus for parallel processing</title>
    <summary>  In this note, we present preliminary results on the use of "network calculus"
for parallel processing systems, specifically MapReduce.
</summary>
    <author>
      <name>G. Kesidis</name>
    </author>
    <author>
      <name>B. Urgaonkar</name>
    </author>
    <author>
      <name>Y. Shan</name>
    </author>
    <author>
      <name>S. Kamarava</name>
    </author>
    <author>
      <name>J. Liebeherr</name>
    </author>
    <link href="http://arxiv.org/abs/1409.0820v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0820v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.09185v2</id>
    <updated>2015-11-12T20:43:19Z</updated>
    <published>2015-10-30T18:32:52Z</published>
    <title>Analysis of the Energy-Performance Tradeoff for Delayed Mobile
  Offloading</title>
    <summary>  This paper has been withdrawn by the author
</summary>
    <author>
      <name>Huaming Wu</name>
    </author>
    <author>
      <name>Katinka Wolter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.09185v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.09185v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.04556v2</id>
    <updated>2019-02-01T20:52:22Z</updated>
    <published>2018-11-12T05:09:04Z</published>
    <title>HPS: A C++11 High Performance Serialization Library</title>
    <summary>  Data serialization is a common and crucial component in high performance
computing. In this paper, I present a C++11 based serialization library for
performance critical systems. It provides an interface similar to Boost but up
to 150% faster and beats several popular serialization libraries.
</summary>
    <author>
      <name>Junhao Li</name>
    </author>
    <link href="http://arxiv.org/abs/1811.04556v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.04556v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.10750v1</id>
    <updated>2022-02-22T09:08:42Z</updated>
    <published>2022-02-22T09:08:42Z</published>
    <title>Queue input estimation from discrete workload observations</title>
    <summary>  This note considers the problem of statistical inference of the parameters of
the input process to a queue from periodic workload observations. The main
focus is the open problem of constructing statistically efficient estimators
for a given observation scheme, in the sense of minimizing the asymptotic
variance of the estimation error.
</summary>
    <author>
      <name>Liron Ravner</name>
    </author>
    <link href="http://arxiv.org/abs/2202.10750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.10750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.14171v1</id>
    <updated>2023-11-23T19:22:49Z</updated>
    <published>2023-11-23T19:22:49Z</published>
    <title>OpenMP behavior in low resource and high stress mobile environment</title>
    <summary>  This paper investigates the use of OpenMP for parallel post processing in
obejct detection on personal Android devices, where resources like
computational power, memory, and battery are limited. Specifically, it explores
various configurations of thread count, CPU affinity, and chunk size on a Redmi
Note 10 Pro with an ARM Cortex A76 CPU. The study finds that using four threads
offers a maximum post processing speedup of 2.3x but increases overall
inference time by 2.7x. A balanced configuration of two threads achieves a 1.8x
speedup in post processing and a 2% improvement in overall program performance.
</summary>
    <author>
      <name>Kaijun Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.14171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.14171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS, cs.PF, cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0301005v1</id>
    <updated>2003-01-09T05:46:32Z</updated>
    <published>2003-01-09T05:46:32Z</published>
    <title>Modelling Delay Jitter in Voice over IP</title>
    <summary>  It has been suggested in voice over IP that an appropriate choice of the
distribution used in modeling the delay jitters, can improve the play-out
algorithm. In this paper, we propose a tool using which, one can determine, at
a given instance, which distribution model best explains the jitter
distribution. This is done using Expectation Maximization, to choose amongst
possible distribution models which include, the i.i.d exponential distribution,
the gamma distribution etc.
</summary>
    <author>
      <name>R. Ganesh</name>
    </author>
    <author>
      <name>B. Kaushik</name>
    </author>
    <author>
      <name>R. Sadhu</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0301005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0301005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.2;C.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.1896v2</id>
    <updated>2013-01-10T06:39:34Z</updated>
    <published>2012-12-09T16:00:33Z</published>
    <title>Power Consumption Analysis of a Modern Smartphone</title>
    <summary>  This paper presents observations about power consumption of a latest
smartphone. Modern smartphones are powerful devices with different choices of
data connections and other functional modes. This paper provides analysis of
power utilization for these different operation modes. Also, we present power
consumption by vital operating system (OS) components.
</summary>
    <author>
      <name>Muhammad Yasir Malik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.1896v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.1896v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.6469v1</id>
    <updated>2013-08-29T13:54:39Z</updated>
    <published>2013-08-29T13:54:39Z</published>
    <title>Using Chip Multithreading to Speed Up Scenario-Based Design Space
  Exploration</title>
    <summary>  To cope with the complex embedded system design, early design space
exploration (DSE) is used to make design decisions early in the design phase.
For early DSE it is crucial that the running time of the exploration is as
small as possible. In this paper, we describe both the porting of our
scenario-based DSE to the SPARC T3-4 server and the analysis of its performance
behavior.
</summary>
    <author>
      <name>P. van Stralen</name>
    </author>
    <link href="http://arxiv.org/abs/1308.6469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.6469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U07" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.05135v1</id>
    <updated>2015-04-27T18:29:58Z</updated>
    <published>2015-04-27T18:29:58Z</published>
    <title>Network Simulator - Visão Geral da Ferramenta de Simulação de
  Redes</title>
    <summary>  This paper describes NS - Network Simulator, the computer networks simulation
tool. We offer an overview NS, and also analyze its characteristics and
functions. Finally, we present in detail all steps for preparing a simulation
of a simple model in NS.
</summary>
    <author>
      <name>Marcos Portnoi</name>
    </author>
    <author>
      <name>Rafael Gonçalves Bezerra de Araújo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Portuguese, Semin\'ario Estudantil de Produ\c{c}\~ao Acad\^emica,
  2002</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.05135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.05135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.04631v1</id>
    <updated>2015-07-16T16:06:18Z</updated>
    <published>2015-07-16T16:06:18Z</published>
    <title>Window Flow Control Systems with Random Service</title>
    <summary>  We present an extension of the window flow control analysis by R. Agrawal
et.al. (Reference [1]), C.-S. Chang (Reference [6]), and C.-S. Chang et. al.
(Reference [8]) to a system with random service time and fixed feedback delay.
We consider two network service models. In the first model, the network service
process itself has no time correlations. The second model addresses a two-state
Markov-modulated service.
</summary>
    <author>
      <name>Alireza Shekaramiz</name>
    </author>
    <author>
      <name>Jorg Liebeherr</name>
    </author>
    <author>
      <name>Almut Burchard</name>
    </author>
    <link href="http://arxiv.org/abs/1507.04631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.04631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03284v2</id>
    <updated>2015-09-13T20:55:35Z</updated>
    <published>2015-09-03T22:09:10Z</published>
    <title>Proceedings of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015</title>
    <summary>  This is the Proceedings of the 2nd OMNeT++ Community Summit, which was held
at IBM Research - Zurich, Switzerland on September 3-4, 2015.
</summary>
    <author>
      <name>Anna Förster</name>
    </author>
    <author>
      <name>Cyriel Minkenberg</name>
    </author>
    <author>
      <name>German Rodriguez Herrera</name>
    </author>
    <author>
      <name>Michael Kirsche</name>
    </author>
    <link href="http://arxiv.org/abs/1509.03284v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03284v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6; C.2.0; C.4; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05592v1</id>
    <updated>2017-04-19T02:48:49Z</updated>
    <published>2017-04-19T02:48:49Z</published>
    <title>Testing Docker Performance for HPC Applications</title>
    <summary>  The main goal for this article is to compare performance penalties when using
KVM virtualization and Docker containers for creating isolated environments for
HPC applications. The article provides both data obtained using commonly
accepted synthetic tests (High Performance Linpack) and real life applications
(OpenFOAM). The article highlights the influence on resulting application
performance of major infrastructure configuration options: CPU type presented
to VM, networking connection type used.
</summary>
    <author>
      <name>Alexey Ermakov</name>
    </author>
    <author>
      <name>Alexey Vasyukov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 12 figures, 13 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.05592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07730v1</id>
    <updated>2017-05-14T10:12:14Z</updated>
    <published>2017-05-14T10:12:14Z</published>
    <title>Application of the Computer Capacity to the Analysis of Processors
  Evolution</title>
    <summary>  The notion of computer capacity was proposed in 2012, and this quantity has
been estimated for computers of different kinds.
  In this paper we show that, when designing new processors, the manufacturers
change the parameters that affect the computer capacity. This allows us to
predict the values of parameters of future processors. As the main example we
use Intel processors, due to the accessibility of detailed description of all
their technical characteristics.
</summary>
    <author>
      <name>Boris Ryabko</name>
    </author>
    <author>
      <name>Anton Rakitskiy</name>
    </author>
    <link href="http://arxiv.org/abs/1705.07730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01477v1</id>
    <updated>2017-09-05T16:22:47Z</updated>
    <published>2017-09-05T16:22:47Z</published>
    <title>Queueing systems with renovation vs. queues with RED. Supplementary
  Material</title>
    <summary>  In this note we consider M/D/1/N queue with renovation and derive analytic
expressions for the following performance characteristics: stationary loss
rate, moments of the number in the system. Moments of consecutive losses,
waiting/sojourn time are out of scope. The motivation for studying these
characteristics is in the comparison of renovation with known active queue
mechanisms like RED.
</summary>
    <author>
      <name>Mikhail Konovalov</name>
    </author>
    <author>
      <name>Rostislav Razumchik</name>
    </author>
    <link href="http://arxiv.org/abs/1709.01477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03561v1</id>
    <updated>2017-09-27T11:05:34Z</updated>
    <published>2017-09-27T11:05:34Z</published>
    <title>Ciw: An open source discrete event simulation library</title>
    <summary>  This paper introduces Ciw, an open source library for conducting discrete
event simulations that has been developed in Python. The strengths of the
library are illustrated in terms of best practice and reproducibility for
computational research. An analysis of Ciw's performance and comparison to
several alternative discrete event simulation frameworks is presented.
</summary>
    <author>
      <name>Geraint I. Palmer</name>
    </author>
    <author>
      <name>Vincent A. Knight</name>
    </author>
    <author>
      <name>Paul R. Harper</name>
    </author>
    <author>
      <name>Asyl L. Hawa</name>
    </author>
    <link href="http://arxiv.org/abs/1710.03561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.12341v1</id>
    <updated>2018-11-29T17:41:05Z</updated>
    <published>2018-11-29T17:41:05Z</published>
    <title>Linux-Tomcat Application Performance on Amazon AWS</title>
    <summary>  The need for Linux system administrators to do performance management has
returned with a vengeance. Why? The cloud. Resource consumption in the cloud is
all about pay-as-you-go. This article shows you how performance models can find
the most cost-effective deployment of an application on Amazon's cloud.
</summary>
    <author>
      <name>Neil J. Gunther</name>
    </author>
    <author>
      <name>Mohit Chawla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 25 figures. To appear in Linux Magazin (in German),
  February 2, 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.12341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.12341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.2; C.4; C.2.4; C.5.5; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.12535v4</id>
    <updated>2020-12-12T12:40:32Z</updated>
    <published>2019-12-28T22:20:07Z</published>
    <title>On the Asymptotic Optimality of Work-Conserving Disciplines in
  Completion Time Minimization</title>
    <summary>  In this paper, we prove that under mild stochastic assumptions,
work-conserving disciplines are asymptotic optimal for minimizing total
completion time. As a byproduct of our analysis, we obtain tight upper bound on
the competitive ratios of work-conserving disciplines on minimizing the metric
of flow time.
</summary>
    <author>
      <name>Wenxin Li</name>
    </author>
    <link href="http://arxiv.org/abs/1912.12535v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.12535v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.00532v1</id>
    <updated>2020-03-01T17:44:08Z</updated>
    <published>2020-03-01T17:44:08Z</published>
    <title>High Performance Code Generation in MLIR: An Early Case Study with GEMM</title>
    <summary>  This article is primarily meant to present an early case study on using MLIR,
a new compiler intermediate representation infrastructure, for high-performance
code generation. Aspects of MLIR covered in particular include memrefs, the
affine dialect, and polyhedral utilities and pass infrastructure surrounding
those. This article is also aimed at showing the role compiler infrastructure
could play in generating code that is competitive with highly tuned manually
developed libraries, albeit in a more modular, reusable, and automatable way.
</summary>
    <author>
      <name>Uday Bondhugula</name>
    </author>
    <link href="http://arxiv.org/abs/2003.00532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.00532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.07152v1</id>
    <updated>2020-12-13T21:10:11Z</updated>
    <published>2020-12-13T21:10:11Z</published>
    <title>On Non-Markovian Performance Models</title>
    <summary>  We present an approach that can be useful when the network or system
performance is described by a model that is not Markovian. Although most
performance models are based on Markov chains or Markov processes, in some
cases the Markov property does not hold. This can occur, for example, when the
system exhibits long range dependencies. For such situations, and other
non-Markovian cases, our method may provide useful help.
</summary>
    <author>
      <name>Andras Farago</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.07152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.07152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.07458v1</id>
    <updated>2021-03-24T07:06:26Z</updated>
    <published>2021-03-24T07:06:26Z</published>
    <title>Comparison of the FCFS and PS discipline in Redundancy Systems</title>
    <summary>  We consider the c.o.c. redundancy system with $N$ parallel servers where
incoming jobs are immediately replicated to $d$ servers chosen uniformly at
random (without replacement). A job finishes service as soon as the first
replica is completed, after which all the remaining replicas are abandoned. We
compare the performance of the first-come first-served (FCFS) and
processor-sharing (PS) discipline based on the stability condition, the tail
behavior of the latency and the expected latency.
</summary>
    <author>
      <name>Youri Raaijmakers</name>
    </author>
    <link href="http://arxiv.org/abs/2104.07458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.07458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.09298v1</id>
    <updated>2022-11-17T02:23:33Z</updated>
    <published>2022-11-17T02:23:33Z</published>
    <title>Asymptotic behaviour of a conservative reaction-diffusion system
  associated with a Markov process algebra model</title>
    <summary>  This paper demonstrates a lower and upper solution method to investigate the
asymptotic behaviour of the conservative reaction-diffusion systems associated
with Markovian process algebra models. In particular, we have proved the
uniform convergence of the solution to its constant equilibrium for a case
study as time tends to infinity, together with experimental results
illustrations.
</summary>
    <author>
      <name>Jie Ding</name>
    </author>
    <author>
      <name>Ruiming Ma</name>
    </author>
    <author>
      <name>Zhigui Lin</name>
    </author>
    <author>
      <name>Zhi Ling</name>
    </author>
    <link href="http://arxiv.org/abs/2211.09298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.09298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.07945v1</id>
    <updated>2022-12-13T19:48:20Z</updated>
    <published>2022-12-13T19:48:20Z</published>
    <title>Queues on interacting networks</title>
    <summary>  Interacting networks are different in nature to single networks. The study of
queuing processes on interacting networks is underdeveloped. It presents new
mathematical challenges and is of importance to applications. This area of
operations research deserves careful study: queuing theory needs to incorporate
high-order network interactions in the performance analysis of a queuing
system.
</summary>
    <author>
      <name>Maria Vlasiou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11134-022-09771-w</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11134-022-09771-w" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Queueing Systems volume 100, 553-555 (2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2212.07945v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.07945v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.11425v1</id>
    <updated>2024-05-19T02:11:30Z</updated>
    <published>2024-05-19T02:11:30Z</published>
    <title>Enabling full-speed random access to the entire memory on the A100 GPU</title>
    <summary>  We describe some features of the A100 memory architecture. In particular, we
give a technique to reverse-engineer some hardware layout information. Using
this information, we show how to avoid TLB issues to obtain full-speed random
HBM access to the entire memory, as long as we constrain any particular thread
to a reduced access window of less than 64GB.
</summary>
    <author>
      <name>Alden Walker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.11425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.11425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; B.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412027v1</id>
    <updated>2004-12-07T15:27:47Z</updated>
    <published>2004-12-07T15:27:47Z</published>
    <title>Correlated dynamics in human printing behavior</title>
    <summary>  Arrival times of requests to print in a student laboratory were analyzed.
Inter-arrival times between subsequent requests follow a universal scaling law
relating time intervals and the size of the request, indicating a scale
invariant dynamics with respect to the size. The cumulative distribution of
file sizes is well-described by a modified power law often seen in
non-equilibrium critical systems. For each user, waiting times between their
individual requests show long range dependence and are broadly distributed from
seconds to weeks. All results are incompatible with Poisson models, and may
provide evidence of critical dynamics associated with voluntary thought
processes in the brain.
</summary>
    <author>
      <name>Uli Harder</name>
    </author>
    <author>
      <name>Maya Paczuski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0412027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.other" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502012v1</id>
    <updated>2005-02-02T04:43:33Z</updated>
    <published>2005-02-02T04:43:33Z</published>
    <title>Sequential File Programming Patterns and Performance with .NET</title>
    <summary>  Programming patterns for sequential file access in the .NET Framework are
described and the performance is measured. The default behavior provides
excellent performance on a single disk - 50 MBps both reading and writing.
Using large request sizes and doing file pre-allocation when possible have
quantifiable benefits. When one considers disk arrays, .NET unbuffered IO
delivers 800 MBps on a 16-disk array, but buffered IO delivers about 12% of
that performance. Consequently, high-performance file and database utilities
are still forced to use unbuffered IO for maximum sequential performance. The
report is accompanied by downloadable source code that demonstrates the
concepts and code that was used to obtain these measurements.
</summary>
    <author>
      <name>Peter Kukol</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0502012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0507073v1</id>
    <updated>2005-07-29T19:33:13Z</updated>
    <published>2005-07-29T19:33:13Z</published>
    <title>Software Performance Analysis</title>
    <summary>  The key to speeding up applications is often understanding where the elapsed
time is spent, and why. This document reviews in depth the full array of
performance analysis tools and techniques available on Linux for this task,
from the traditional tools like gcov and gprof, to the more advanced tools
still under development like oprofile and the Linux Trace Toolkit. The focus is
more on the underlying data collection and processing algorithms, and their
overhead and precision, than on the cosmetic details of the graphical user
interface frontends.
</summary>
    <author>
      <name>Michel R. Dagenais</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dept. of Computer Engineering, Ecole Polytechnique, Montreal, Canada</arxiv:affiliation>
    </author>
    <author>
      <name>Karim Yaghmour</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Opersys, Montreal, Canada</arxiv:affiliation>
    </author>
    <author>
      <name>Charles Levert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Ericsson Research, Montreal, Canada</arxiv:affiliation>
    </author>
    <author>
      <name>Makan Pourzandi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Ericsson Research, Montreal, Canada</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/cs/0507073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0507073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0508063v1</id>
    <updated>2005-08-12T16:13:32Z</updated>
    <published>2005-08-12T16:13:32Z</published>
    <title>Disks, Partitions, Volumes and RAID Performance with the Linux Operating
  System</title>
    <summary>  Block devices in computer operating systems typically correspond to disks or
disk partitions, and are used to store files in a filesystem. Disks are not the
only real or virtual device which adhere to the block accessible stream of
bytes block device model. Files, remote devices, or even RAM may be used as a
virtual disks. This article examines several common combinations of block
device layers used as virtual disks in the Linux operating system: disk
partitions, loopback files, software RAID, Logical Volume Manager, and Network
Block Devices. It measures their relative performance using different
filesystems: Ext2, Ext3, ReiserFS, JFS, XFS,NFS.
</summary>
    <author>
      <name>Michel R. Dagenais</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dept. of Computer Engineering, Ecole Polytechnique, Montreal, Canada</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/cs/0508063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0508063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603099v1</id>
    <updated>2006-03-26T20:19:59Z</updated>
    <published>2006-03-26T20:19:59Z</published>
    <title>Benchmark Problems for Constraint Solving</title>
    <summary>  Constraint Programming is roughly a new software technology introduced by
Jaffar and Lassez in 1987 for description and effective solving of large,
particularly combinatorial, problems especially in areas of planning and
scheduling. In the following we define three problems for constraint solving
from the domain of electrical networks; based on them we define 43 related
problems. For the defined set of problems we benchmarked five systems: ILOG
OPL, AMPL, GAMS, Mathematica and UniCalc. As expected some of the systems
performed very well for some problems while others performed very well on
others.
</summary>
    <author>
      <name>Alin Suciu</name>
    </author>
    <author>
      <name>Rodica Potolea</name>
    </author>
    <author>
      <name>Tudor Muresan</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0603099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0608102v1</id>
    <updated>2006-08-25T18:12:08Z</updated>
    <published>2006-08-25T18:12:08Z</published>
    <title>Analysis of a Reputation System for Mobile Ad-Hoc Networks with Liars</title>
    <summary>  The application of decentralized reputation systems is a promising approach
to ensure cooperation and fairness, as well as to address random failures and
malicious attacks in Mobile Ad-Hoc Networks. However, they are potentially
vulnerable to liars. With our work, we provide a first step to analyzing
robustness of a reputation system based on a deviation test. Using a mean-field
approach to our stochastic process model, we show that liars have no impact
unless their number exceeds a certain threshold (phase transition). We give
precise formulae for the critical values and thus provide guidelines for an
optimal choice of parameters.
</summary>
    <author>
      <name>Jochen Mundinger</name>
    </author>
    <author>
      <name>Jean-Yves Le Boudec</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0608102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0608102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0610173v1</id>
    <updated>2006-10-31T10:09:51Z</updated>
    <published>2006-10-31T10:09:51Z</published>
    <title>On Degree-Based Decentralized Search in Complex Networks</title>
    <summary>  Decentralized search aims to find the target node in a large network by using
only local information. The applications of it include peer-to-peer file
sharing, web search and anything else that requires locating a specific target
in a complex system. In this paper, we examine the degree-based decentralized
search method. Specifically, we evaluate the efficiency of the method in
different cases with different amounts of available local information. In
addition, we propose a simple refinement algorithm for significantly shortening
the length of the route that has been found. Some insights useful for the
future developments of efficient decentralized search schemes have been
achieved.
</summary>
    <author>
      <name>Shi Xiao</name>
    </author>
    <author>
      <name>Gaoxi Xiao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figs, shortly published by ECCS'06</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0610173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0610173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612141v1</id>
    <updated>2006-12-28T19:35:56Z</updated>
    <published>2006-12-28T19:35:56Z</published>
    <title>Exact Failure Frequency Calculations for Extended Systems</title>
    <summary>  This paper shows how the steady-state availability and failure frequency can
be calculated in a single pass for very large systems, when the availability is
expressed as a product of matrices. We apply the general procedure to
$k$-out-of-$n$:G and linear consecutive $k$-out-of-$n$:F systems, and to a
simple ladder network in which each edge and node may fail. We also give the
associated generating functions when the components have identical
availabilities and failure rates. For large systems, the failure rate of the
whole system is asymptotically proportional to its size. This paves the way to
ready-to-use formulae for various architectures, as well as proof that the
differential operator approach to failure frequency calculations is very useful
and straightforward.
</summary>
    <author>
      <name>Annie Druault-Vicard</name>
    </author>
    <author>
      <name>Christian Tanguy</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0612141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701001v1</id>
    <updated>2007-01-02T14:14:11Z</updated>
    <published>2007-01-02T14:14:11Z</published>
    <title>On High Spatial Reuse Link Scheduling in STDMA Wireless Ad Hoc Networks</title>
    <summary>  Graph-based algorithms for point-to-point link scheduling in Spatial reuse
Time Division Multiple Access (STDMA) wireless ad hoc networks often result in
a significant number of transmissions having low Signal to Interference and
Noise density Ratio (SINR) at intended receivers, leading to low throughput. To
overcome this problem, we propose a new algorithm for STDMA link scheduling
based on a graph model of the network as well as SINR computations. The
performance of our algorithm is evaluated in terms of spatial reuse and
computational complexity. Simulation results demonstrate that our algorithm
achieves better performance than existing algorithms.
</summary>
    <author>
      <name>Ashutosh Deepak Gore</name>
    </author>
    <author>
      <name>Srikanth Jagabathula</name>
    </author>
    <author>
      <name>Abhay Karandikar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages (double column), 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0701001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.1; C.2.5; F.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701005v1</id>
    <updated>2006-12-30T17:26:42Z</updated>
    <published>2006-12-30T17:26:42Z</published>
    <title>Exact solutions for the two- and all-terminal reliabilities of the
  Brecht-Colbourn ladder and the generalized fan</title>
    <summary>  The two- and all-terminal reliabilities of the Brecht-Colbourn ladder and the
generalized fan have been calculated exactly for arbitrary size as well as
arbitrary individual edge and node reliabilities, using transfer matrices of
dimension four at most. While the all-terminal reliabilities of these graphs
are identical, the special case of identical edge ($p$) and node ($\rho$)
reliabilities shows that their two-terminal reliabilities are quite distinct,
as demonstrated by their generating functions and the locations of the zeros of
the reliability polynomials, which undergo structural transitions at $\rho =
\displaystyle {1/2}$.
</summary>
    <author>
      <name>Christian Tanguy</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703086v2</id>
    <updated>2007-06-03T06:50:30Z</updated>
    <published>2007-03-15T21:06:06Z</published>
    <title>A Technical Report On Grid Benchmarking using SEE V.O</title>
    <summary>  Grids include heterogeneous resources, which are based on different hardware
and software architectures or components. In correspondence with this diversity
of the infrastructure, the execution time of any single job, as well as the
total grid performance can both be affected substantially, which can be
demonstrated by measurements. Running a simple benchmarking suite can show this
heterogeneity and give us results about the differences over the grid sites.
</summary>
    <author>
      <name>John Kouvakis</name>
    </author>
    <author>
      <name>Fotis Georgatos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 12 figures including tables of results, SEE VO</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0703086v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703086v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.0861v1</id>
    <updated>2007-04-06T08:50:34Z</updated>
    <published>2007-04-06T08:50:34Z</published>
    <title>Empirical analysis and statistical modeling of attack processes based on
  honeypots</title>
    <summary>  Honeypots are more and more used to collect data on malicious activities on
the Internet and to better understand the strategies and techniques used by
attackers to compromise target systems. Analysis and modeling methodologies are
needed to support the characterization of attack processes based on the data
collected from the honeypots. This paper presents some empirical analyses based
on the data collected from the Leurr{\'e}.com honeypot platforms deployed on
the Internet and presents some preliminary modeling studies aimed at fulfilling
such objectives.
</summary>
    <author>
      <name>Mohamed Kaaniche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS</arxiv:affiliation>
    </author>
    <author>
      <name>Y. Deswarte</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Alata</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS</arxiv:affiliation>
    </author>
    <author>
      <name>Marc Dacier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SC</arxiv:affiliation>
    </author>
    <author>
      <name>Vincent Nicomette</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/IFIP International Conference on Dependable Systems and
  Networks (DSN-2006) (25/06/2006) 119-124</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0704.0861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.0861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4633v1</id>
    <updated>2007-10-25T08:07:46Z</updated>
    <published>2007-10-25T08:07:46Z</published>
    <title>Nano-Sim: A Step Wise Equivalent Conductance based Statistical Simulator
  for Nanotechnology Circuit Design</title>
    <summary>  New nanotechnology based devices are replacing CMOS devices to overcome CMOS
technology's scaling limitations. However, many such devices exhibit
non-monotonic I-V characteristics and uncertain properties which lead to the
negative differential resistance (NDR) problem and the chaotic performance.
This paper proposes a new circuit simulation approach that can effectively
simulate nanotechnology devices with uncertain input sources and negative
differential resistance (NDR) problem. The experimental results show a 20-30
times speedup comparing with existing simulators.
</summary>
    <author>
      <name>Bharat Sukhwani</name>
    </author>
    <author>
      <name>Uday Padmanabhan</name>
    </author>
    <author>
      <name>Janet M. Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DATE.2005.221</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DATE.2005.221" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4633v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4633v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.2532v1</id>
    <updated>2008-09-15T14:11:34Z</updated>
    <published>2008-09-15T14:11:34Z</published>
    <title>Multidimensional Visualization of Oracle Performance Using Barry007</title>
    <summary>  Most generic performance tools display only system-level performance data
using 2-dimensional plots or diagrams and this limits the informational detail
that can be displayed. Moreover, a modern relational database system, like
Oracle, can concurrently serve thousands of client processes with different
workload characteristics, so that generic performance-data displays inevitably
hide important information. Drawing on our previous work, this paper
demonstrates the application of Barry007 multidimensional visualization to the
analysis of Oracle end-user, session-level, performance data, showing both
collective trends and individual performance anomalies.
</summary>
    <author>
      <name>Tanel Poder</name>
    </author>
    <author>
      <name>Neil J. Gunther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Proc. CMG International Conference, Las Vegas,
  Nevada, December 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.2532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.2532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8; C.4; D.4.8; H.2.4; H.2.7; H.3.4; H.5.1; K.8.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.0135v1</id>
    <updated>2008-10-01T15:02:16Z</updated>
    <published>2008-10-01T15:02:16Z</published>
    <title>Occupancy distributions of homogeneous queueing systems under
  opportunistic scheduling</title>
    <summary>  We analyze opportunistic schemes for transmission scheduling from one of $n$
homogeneous queues whose channel states fluctuate independently. Considered
schemes consist of the LCQ policy, which transmits from a longest connected
queue in the entire system, and its low-complexity variants that transmit from
a longest queue within a randomly chosen subset of connected queues. A
Markovian model is studied where mean packet transmission time is $n^{-1}$ and
packet arrival rate is $\lambda&lt;1$ per queue. Transient and equilibrium
distributions of queue occupancies are obtained in the limit as the system size
$n$ tends to infinity.
</summary>
    <author>
      <name>Murat Alanyali</name>
    </author>
    <author>
      <name>Maxim Dashouk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for possible publication</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.0135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.0135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.3468v1</id>
    <updated>2008-10-20T08:29:21Z</updated>
    <published>2008-10-20T08:29:21Z</published>
    <title>A Call-Graph Profiler for GNU Octave</title>
    <summary>  We report the design and implementation of a call-graph profiler for GNU
Octave, a numerical computing platform. GNU Octave simplifies matrix
computation for use in modeling or simulation. Our work provides a call-graph
profiler, which is an improvement on the flat profiler. We elaborate design
constraints of building a profiler for numerical computation, and benchmark the
profiler by comparing it to the rudimentary timer start-stop (tic-toc)
measurements, for a similar set of programs. The profiler code provides clean
interfaces to internals of GNU Octave, for other (newer) profiling tools on GNU
Octave.
</summary>
    <author>
      <name>Muthiah Annamalai</name>
    </author>
    <author>
      <name>Leela Velusamy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, 1 table. Fix typos</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.3468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.3468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.0904v1</id>
    <updated>2008-12-04T11:45:48Z</updated>
    <published>2008-12-04T11:45:48Z</published>
    <title>An Approximation of the Outage Probability for Multi-hop AF Fixed Gain
  Relay</title>
    <summary>  In this letter, we present a closed-form approximation of the outage
probability for the multi-hop amplify-and-forward (AF) relaying systems with
fixed gain in Rayleigh fading channel. The approximation is derived from the
outage event for each hop. The simulation results show the tightness of the
proposed approximation in low and high signal-to-noise ratio (SNR) region.
</summary>
    <author>
      <name>Jun Kyoung Lee</name>
    </author>
    <author>
      <name>Janghoon Yang</name>
    </author>
    <author>
      <name>Dong Ku Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 3 figures, Submitted to IEEE Communication Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/0812.0904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.0904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.2119v1</id>
    <updated>2009-03-12T10:16:13Z</updated>
    <published>2009-03-12T10:16:13Z</published>
    <title>Adaptive Mesh Approach for Predicting Algorithm Behavior with
  Application to Visibility Culling in Computer Graphics</title>
    <summary>  We propose a concise approximate description, and a method for efficiently
obtaining this description, via adaptive random sampling of the performance
(running time, memory consumption, or any other profileable numerical quantity)
of a given algorithm on some low-dimensional rectangular grid of inputs. The
formal correctness is proven under reasonable assumptions on the algorithm
under consideration; and the approach's practical benefit is demonstrated by
predicting for which observer positions and viewing directions an occlusion
culling algorithm yields a net performance benefit or loss compared to a simple
brute force renderer.
</summary>
    <author>
      <name>Matthias Fischer</name>
    </author>
    <author>
      <name>Claudius Jähn</name>
    </author>
    <author>
      <name>Martin Ziegler</name>
    </author>
    <link href="http://arxiv.org/abs/0903.2119v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.2119v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; I.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.4898v1</id>
    <updated>2009-03-27T20:05:17Z</updated>
    <published>2009-03-27T20:05:17Z</published>
    <title>Asymptotic Optimality of the Static Frequency Caching in the Presence of
  Correlated Requests</title>
    <summary>  It is well known that the static caching algorithm that keeps the most
frequently requested documents in the cache is optimal in case when documents
are of the same size and requests are independent and equally distributed.
However, it is hard to develop explicit and provably optimal caching algorithms
when requests are statistically correlated. In this paper, we show that keeping
the most frequently requested documents in the cache is still optimal for large
cache sizes even if the requests are strongly correlated.
</summary>
    <author>
      <name>Predrag R. Jelenkovic</name>
    </author>
    <author>
      <name>Ana Radovanovic</name>
    </author>
    <link href="http://arxiv.org/abs/0903.4898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.4898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.0792v1</id>
    <updated>2009-05-06T10:55:04Z</updated>
    <published>2009-05-06T10:55:04Z</published>
    <title>Introducing a Performance Model for Bandwidth-Limited Loop Kernels</title>
    <summary>  We present a performance model for bandwidth limited loop kernels which is
founded on the analysis of modern cache based microarchitectures. This model
allows an accurate performance prediction and evaluation for existing
instruction codes. It provides an in-depth understanding of how performance for
different memory hierarchy levels is made up. The performance of raw memory
load, store and copy operations and a stream vector triad are analyzed and
benchmarked on three modern x86-type quad-core architectures in order to
demonstrate the capabilities of the model.
</summary>
    <author>
      <name>Jan Treibig</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0905.0792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.0792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.4865v1</id>
    <updated>2009-10-26T12:48:02Z</updated>
    <published>2009-10-26T12:48:02Z</published>
    <title>Multi-core architectures: Complexities of performance prediction and the
  impact of cache topology</title>
    <summary>  The balance metric is a simple approach to estimate the performance of
bandwidth-limited loop kernels. However, applying the method to in-cache
situations and modern multi-core architectures yields unsatisfactory results.
This paper analyzes the in uence of cache hierarchy design on performance
predictions for bandwidth-limited loop kernels on current mainstream
processors. We present a diagnostic model with improved predictive power,
correcting the limitations of the simple balance metric. The importance of code
execution overhead even in bandwidth-bound situations is emphasized. Finally we
analyze the impact of synchronization overhead on multi-threaded performance
with a special emphasis on the in uence of cache topology.
</summary>
    <author>
      <name>Jan Treibig</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0910.4865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.4865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.4506v1</id>
    <updated>2009-12-22T20:44:59Z</updated>
    <published>2009-12-22T20:44:59Z</published>
    <title>Multicore-aware parallel temporal blocking of stencil codes for shared
  and distributed memory</title>
    <summary>  New algorithms and optimization techniques are needed to balance the
accelerating trend towards bandwidth-starved multicore chips. It is well known
that the performance of stencil codes can be improved by temporal blocking,
lessening the pressure on the memory interface. We introduce a new pipelined
approach that makes explicit use of shared caches in multicore environments and
minimizes synchronization and boundary overhead. For clusters of shared-memory
nodes we demonstrate how temporal blocking can be employed successfully in a
hybrid shared/distributed-memory environment.
</summary>
    <author>
      <name>Markus Wittmann</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IPDPSW.2010.5470813</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IPDPSW.2010.5470813" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0912.4506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.4506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.2604v1</id>
    <updated>2010-01-15T03:12:55Z</updated>
    <published>2010-01-15T03:12:55Z</published>
    <title>On the Model Transform in Stochastic Network Calculus</title>
    <summary>  Stochastic network calculus requires special care in the search of proper
stochastic traffic arrival models and stochastic service models. Tradeoff must
be considered between the feasibility for the analysis of performance bounds,
the usefulness of performance bounds, and the ease of their numerical
calculation. In theory, transform between different traffic arrival models and
transform between different service models are possible. Nevertheless, the
impact of the model transform on performance bounds has not been thoroughly
investigated. This paper is to investigate the effect of the model transform
and to provide practical guidance in the model selection in stochastic network
calculus.
</summary>
    <author>
      <name>Kui Wu</name>
    </author>
    <author>
      <name>Yuming Jiang</name>
    </author>
    <author>
      <name>Jie Li</name>
    </author>
    <link href="http://arxiv.org/abs/1001.2604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.2604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.5068v1</id>
    <updated>2010-03-26T08:06:37Z</updated>
    <published>2010-03-26T08:06:37Z</published>
    <title>On the stability of flow-aware CSMA</title>
    <summary>  We consider a wireless network where each flow (instead of each link) runs
its own CSMA (Carrier Sense Multiple Access) algorithm. Specifically, each flow
attempts to access the radio channel after some random time and transmits a
packet if the channel is sensed idle. We prove that, unlike the standard CSMA
algorithm, this simple distributed access scheme is optimal in the sense that
the network is stable for all traffic intensities in the capacity region of the
network.
</summary>
    <author>
      <name>T. Bonald</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Telecom ParisTech, Paris, France</arxiv:affiliation>
    </author>
    <author>
      <name>M. Feuillet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA, Rocquencourt, France</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.peva.2010.08.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.peva.2010.08.001" rel="related"/>
    <link href="http://arxiv.org/abs/1003.5068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.5068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.1741v1</id>
    <updated>2010-04-10T20:56:21Z</updated>
    <published>2010-04-10T20:56:21Z</published>
    <title>Efficient multicore-aware parallelization strategies for iterative
  stencil computations</title>
    <summary>  Stencil computations consume a major part of runtime in many scientific
simulation codes. As prototypes for this class of algorithms we consider the
iterative Jacobi and Gauss-Seidel smoothers and aim at highly efficient
parallel implementations for cache-based multicore architectures. Temporal
cache blocking is a known advanced optimization technique, which can reduce the
pressure on the memory bus significantly. We apply and refine this optimization
for a recently presented temporal blocking strategy designed to explicitly
utilize multicore characteristics. Especially for the case of Gauss-Seidel
smoothers we show that simultaneous multi-threading (SMT) can yield substantial
performance improvements for our optimized algorithm.
</summary>
    <author>
      <name>Jan Treibig</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jocs.2011.01.010</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jocs.2011.01.010" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computational Science 2, 130-137 (2011)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.1741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.1741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3560v1</id>
    <updated>2010-04-20T20:21:18Z</updated>
    <published>2010-04-20T20:21:18Z</published>
    <title>Comparison of the Performance of Two Service Disciplines for a Shared
  Bus Multiprocessor with Private Caches</title>
    <summary>  In this paper, we compare two analytical models for evaluation of cache
coherence overhead of a shared bus multiprocessor with private caches. The
models are based on a closed queuing network with different service
disciplines. We find that the priority discipline can be used as a lower-level
bound. Some numerical results are shown graphically.
</summary>
    <author>
      <name>Angel Vassilev Nikolov</name>
    </author>
    <author>
      <name>Lerato Lerato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues online at
  http://ijcsi.org/articles/Comparison-of-the-Performance-of-Two-Service-Disciplines-for-a-Shared-Bus-Multiprocessor-with-Private-Caches.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI, Volume 7, Issue 2, March 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.3560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4286v2</id>
    <updated>2010-04-27T12:52:46Z</updated>
    <published>2010-04-24T15:17:56Z</published>
    <title>Space-efficient scheduling of stochastically generated tasks</title>
    <summary>  We study the problem of scheduling tasks for execution by a processor when
the tasks can stochastically generate new tasks. Tasks can be of different
types, and each type has a fixed, known probability of generating other tasks.
We present results on the random variable S^sigma modeling the maximal space
needed by the processor to store the currently active tasks when acting under
the scheduler sigma. We obtain tail bounds for the distribution of S^sigma for
both offline and online schedulers, and investigate the expected value of
S^sigma.
</summary>
    <author>
      <name>Tomáš Brázdil</name>
    </author>
    <author>
      <name>Javier Esparza</name>
    </author>
    <author>
      <name>Stefan Kiefer</name>
    </author>
    <author>
      <name>Michael Luttenberger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">technical report accompanying an ICALP'10 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.4286v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4286v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.2992v1</id>
    <updated>2011-06-15T15:18:27Z</updated>
    <published>2011-06-15T15:18:27Z</published>
    <title>A Characterization of the SPARC T3-4 System</title>
    <summary>  This technical report covers a set of experiments on the 64-core SPARC T3-4
system, comparing it to two similar AMD and Intel systems. Key characteristics
as maximum integer and floating point arithmetic throughput are measured as
well as memory throughput, showing the scalability of the SPARC T3-4 system.
The performance of POSIX threads primitives is characterized and compared in
detail, such as thread creation and mutex synchronization. Scalability tests
with a fine grained multithreaded runtime are performed, showing problems with
atomic CAS operations on such physically highly parallel systems.
</summary>
    <author>
      <name>Michiel W. van Tol</name>
    </author>
    <link href="http://arxiv.org/abs/1106.2992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.2992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.4851v1</id>
    <updated>2011-07-25T06:43:39Z</updated>
    <published>2011-07-25T06:43:39Z</published>
    <title>AWRP: Adaptive Weight Ranking Policy for Improving Cache Performance</title>
    <summary>  Due to the huge difference in performance between the computer memory and
processor, the virtual memory management plays a vital role in system
performance. A Cache memory is the fast memory which is used to compensate the
speed difference between the memory and processor. This paper gives an adaptive
replacement policy over the traditional policy which has low overhead, better
performance and is easy to implement. Simulations show that our algorithm
performs better than Least-Recently-Used (LRU), First-In-First-Out (FIFO) and
Clock with Adaptive Replacement (CAR).
</summary>
    <author>
      <name>Debabala Swain</name>
    </author>
    <author>
      <name>Bijay Paikaray</name>
    </author>
    <author>
      <name>Debabrata Swain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages; Journal of Computing, Volume 3, Issue 2, February 2011, ISSN
  2151-9617; http://www.journalofcomputing.org</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.4851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.4851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.2034v1</id>
    <updated>2012-01-10T11:59:37Z</updated>
    <published>2012-01-10T11:59:37Z</published>
    <title>Early Performance Prediction of Web Services</title>
    <summary>  Web Service is an interface which implements business logic. Performance is
an important quality aspect of Web services because of their distributed
nature. Predicting the performance of web services during early stages of
software development is significant. In this paper we model web service using
Unified Modeling Language, Use Case Diagram, Sequence Diagram, Deployment
Diagram. We obtain the Performance metrics by simulating the web services model
using a simulation tool Simulation of Multi-Tier Queuing Architecture. We have
identified the bottle neck resources.
</summary>
    <author>
      <name>Ch Ram Mohan Reddy</name>
    </author>
    <author>
      <name>D. Evangelin Geetha</name>
    </author>
    <author>
      <name>K. G. Srinivasa</name>
    </author>
    <author>
      <name>T. V. Suresh Kumar</name>
    </author>
    <author>
      <name>K. Rajani Kanth</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijwsc.2011.2303</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijwsc.2011.2303" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.2034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.2034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.4655v1</id>
    <updated>2012-01-23T09:51:15Z</updated>
    <published>2012-01-23T09:51:15Z</published>
    <title>Reengineering multi tiered enterprise business applications for
  performance enhancement and reciprocal or rectangular hyperbolic relation of
  variation of data transportation time with row pre-fetch size of relational
  database drivers</title>
    <summary>  Reengineering multi tiered enterprise business applications for performance
enhancement and reciprocal or rectangular hyperbolic relation of variation of
data transportation time with row pre-fetch size of relational database drivers
</summary>
    <author>
      <name>Sridhar Sowmiyanarayanan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">i) A general approach for Re engineering a N tier Java EE
  applications for performance improvements, A Case study of performance
  enhancement iii) Elaborates on rectangular hyperbolic relationship between
  total elapsed time to transport a set of records from database server to
  application server vs the pre fetch size configurable for database drivers;
  http://www.IJCSI.org ISSN (Online): 1694-0814</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 6, No 3, November 2011, 393-412</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1201.4655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.4655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.6402v1</id>
    <updated>2012-01-30T23:03:45Z</updated>
    <published>2012-01-30T23:03:45Z</published>
    <title>A Note on Disk Drag Dynamics</title>
    <summary>  The electrical power consumed by typical magnetic hard disk drives (HDD) not
only increases linearly with the number of spindles but, more significantly, it
increases as very fast power-laws of speed (RPM) and diameter. Since the
theoretical basis for this relationship is neither well-known nor readily
accessible in the literature, we show how these exponents arise from
aerodynamic disk drag and discuss their import for green storage capacity
planning.
</summary>
    <author>
      <name>Neil J. Gunther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.6402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.6402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.class-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.3.2; C.4; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.1581v1</id>
    <updated>2012-11-07T16:00:28Z</updated>
    <published>2012-11-07T16:00:28Z</published>
    <title>Data-parallel programming with Intel Array Building Blocks (ArBB)</title>
    <summary>  Intel Array Building Blocks is a high-level data-parallel programming
environment designed to produce scalable and portable results on existing and
upcoming multi- and many-core platforms.
  We have chosen several mathematical kernels - a dense matrix-matrix
multiplication, a sparse matrix-vector multiplication, a 1-D complex FFT and a
conjugate gradients solver - as synthetic benchmarks and representatives of
scientific codes and ported them to ArBB. This whitepaper describes the ArBB
ports and presents performance and scaling measurements on the Westmere-EX
based system SuperMIG at LRZ in comparison with OpenMP and MKL.
</summary>
    <author>
      <name>Volker Weinberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures, PRACE Whitepaper</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.1581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.1581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.2554v3</id>
    <updated>2014-03-17T15:50:53Z</updated>
    <published>2013-04-09T12:37:41Z</published>
    <title>Throughput Optimal Scheduling Policies in Networks of Interacting Queues</title>
    <summary>  This report considers a fairly general model of constrained queuing networks
that allows us to represent both MMBP (Markov Modulated Bernoulli Processes)
arrivals and time-varying service constraints. We derive a set of sufficient
conditions for throughput optimality of scheduling policies that encompass and
generalize all the previously obtained results in the field. This leads to the
definition of new classes of (non diagonal) throughput optimal scheduling
policies. We prove the stability of queues by extending the traditional
Lyapunov drift criteria methodology.
</summary>
    <author>
      <name>Emilio Leonardi</name>
    </author>
    <link href="http://arxiv.org/abs/1304.2554v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.2554v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.3536v1</id>
    <updated>2013-05-15T16:32:59Z</updated>
    <published>2013-05-15T16:32:59Z</published>
    <title>Analysis of a non-work conserving Generalized Processor Sharing queue</title>
    <summary>  We consider in this paper a non work-conserving Generalized Processor Sharing
(GPS) system composed of two queues with Poisson arrivals and exponential
service times. Using general results due to Fayolle \emph{et al}, we first
establish the stability condition for this system. We then determine the
functional equation satisfied by the generating function of the numbers of jobs
in both queues and the associated Riemann-Hilbert problem. We prove the
existence and the uniqueness of the solution. This allows us to completely
characterize the system, in particular to compute the empty queue probability.
We finally derive the tail asymptotics of the number of jobs in one queue.
</summary>
    <author>
      <name>Fabrice Guillemin</name>
    </author>
    <link href="http://arxiv.org/abs/1305.3536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.3536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.1894v1</id>
    <updated>2013-09-07T18:49:59Z</updated>
    <published>2013-09-07T18:49:59Z</published>
    <title>Software Autotuning for Sustainable Performance Portability</title>
    <summary>  Scientific software applications are increasingly developed by large
interdiscplinary teams operating on functional modules organized around a
common software framework, which is capable of integrating new functional
capabilities without modifying the core of the framework. In such environment,
software correctness and modularity take precedence at the expense of code
performance, which is an important concern during execution on supercomputing
facilities, where the allocation of core-hours is a valuable resource. To
alleviate the performance problems, we propose automated performance tuning
(autotuning) of software to extract the maximum performance on a given hardware
platform and to enable performance portability across heterogeneous hardware
platforms. The resulting code remains generic without committing to a
particular software stack and yet is compile-time specializable for maximal
sustained performance.
</summary>
    <author>
      <name>Azamat Mametjanov</name>
    </author>
    <author>
      <name>Boyana Norris</name>
    </author>
    <link href="http://arxiv.org/abs/1309.1894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.1894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.0184v1</id>
    <updated>2014-06-01T17:44:59Z</updated>
    <published>2014-06-01T17:44:59Z</published>
    <title>Parallelism Via Concurrency at Multiple Levels</title>
    <summary>  In this paper we examine the key elements determining the best performance of
computing by increasing the frequency of a single chip and to get the minimum
latency during execution of the programs to achieve best possible output. It is
not enough to provide concurrent improvements in the hardware as Software also
have to introduce concurrency in order to exploit the parallelism. The software
parallelism is defined by the control and data dependency of programs whereas
Hardware refers to the type of parallelism defined by the machine architecture
and hardware multiplicity.
</summary>
    <author>
      <name>Kamran Latif</name>
    </author>
    <link href="http://arxiv.org/abs/1406.0184v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.0184v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2067v1</id>
    <updated>2014-06-09T03:47:53Z</updated>
    <published>2014-06-09T03:47:53Z</published>
    <title>Extended Differential Aggregations in Process Algebra for Performance
  and Biology</title>
    <summary>  We study aggregations for ordinary differential equations induced by fluid
semantics for Markovian process algebra which can capture the dynamics of
performance models and chemical reaction networks. Whilst previous work has
required perfect symmetry for exact aggregation, we present approximate fluid
lumpability, which makes nearby processes perfectly symmetric after a
perturbation of their parameters. We prove that small perturbations yield
nearby differential trajectories. Numerically, we show that many heterogeneous
processes can be aggregated with negligible errors.
</summary>
    <author>
      <name>Max Tschaikowski</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Southampton</arxiv:affiliation>
    </author>
    <author>
      <name>Mirco Tribastone</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Southampton</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.154.3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.154.3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings QAPL 2014, arXiv:1406.1567</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 154, 2014, pp. 34-47</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.2067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.3463v1</id>
    <updated>2014-09-11T14:53:47Z</updated>
    <published>2014-09-11T14:53:47Z</published>
    <title>Heavy Traffic Limits for GI/H/n Queues: Theory and Application</title>
    <summary>  We consider a GI/H/n queueing system. In this system, there are multiple
servers in the queue. The inter-arrival time is general and independent, and
the service time follows hyper-exponential distribution. Instead of stochastic
differential equations, we propose two heavy traffic limits for this system,
which can be easily applied in practical systems. In applications, we show how
to use these heavy traffic limits to design a power efficient cloud computing
environment based on different QoS requirements.
</summary>
    <author>
      <name>Yousi Zheng</name>
    </author>
    <author>
      <name>Ness Shroff</name>
    </author>
    <author>
      <name>Prasun Sinha</name>
    </author>
    <link href="http://arxiv.org/abs/1409.3463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.3463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.07908v1</id>
    <updated>2015-04-29T16:03:40Z</updated>
    <published>2015-04-29T16:03:40Z</published>
    <title>Inhomogeneous CTMC Model of a Call Center with Balking and Abandonment</title>
    <summary>  This paper considers a nonstationary multiserver queuing model with
abandonment and balking for inbound call centers. We present a continuous time
Markov chain (CTMC) model which captures the important characteristics of an
inbound call center and obtain a numerical solution for its transient state
probabilities using uniformization method with steady-state detection.
Keywords: call center, transient, Markov processes, numerical methods,
uniformization, abandonment, balking
</summary>
    <author>
      <name>Maciej Rafal Burak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to Studia Informatica (http://studiainformatica.polsl.pl/).
  arXiv admin note: substantial text overlap with arXiv:1410.0804</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Studia Informatica, Vol 36, No 2 (2015):23-34, jun 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1504.07908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.03997v1</id>
    <updated>2015-06-12T12:09:03Z</updated>
    <published>2015-06-12T12:09:03Z</published>
    <title>Short Note on Costs of Floating Point Operations on current x86-64
  Architectures: Denormals, Overflow, Underflow, and Division by Zero</title>
    <summary>  Simple floating point operations like addition or multiplication on
normalized floating point values can be computed by current AMD and Intel
processors in three to five cycles. This is different for denormalized numbers,
which appear when an underflow occurs and the value can no longer be
represented as a normalized floating-point value. Here the costs are about two
magnitudes higher.
</summary>
    <author>
      <name>Markus Wittmann</name>
    </author>
    <author>
      <name>Thomas Zeiser</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <link href="http://arxiv.org/abs/1506.03997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.03997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04951v1</id>
    <updated>2016-04-18T01:24:48Z</updated>
    <published>2016-04-18T01:24:48Z</published>
    <title>A Hybrid Performance Analysis Technique for Distributed Real-Time
  Embedded Systems</title>
    <summary>  It remains a challenging problem to tightly estimate the worst case response
time of an application in a distributed embedded system, especially when there
are dependencies between tasks. We discovered that the state-of-the art
techniques considering task dependencies either fail to obtain a conservative
bound or produce a loose upper bound. We propose a novel conservative
performance analysis, called hybrid performance analysis, combining the
response time analysis technique and the scheduling time bound analysis
technique to compute a tighter bound fast. Through extensive experiments with
randomly generated graphs, superior performance of our proposed approach
compared with previous methods is confirmed.
</summary>
    <author>
      <name>Junchul Choi</name>
    </author>
    <author>
      <name>Hyunok Oh</name>
    </author>
    <author>
      <name>Soonhoi Ha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages (25 page paper, 4 page appendix), 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04997v1</id>
    <updated>2016-04-18T05:49:56Z</updated>
    <published>2016-04-18T05:49:56Z</published>
    <title>A Unified, Hardware-Fitted, Cross-GPU Performance Model</title>
    <summary>  We present a mechanism to symbolically gather performance-relevant operation
counts from numerically-oriented subprograms (`kernels') expressed in the Loopy
programming system, and apply these counts in a simple, linear model of kernel
run time. We use a series of `performance-instructive' kernels to fit the
parameters of a unified model to the performance characteristics of GPU
hardware from multiple hardware generations and vendors. We evaluate the
predictive power of the model on a broad array of computational kernels
relevant to scientific computing. In terms of the geometric mean, our simple,
vendor- and GPU-type-independent model achieves relative accuracy comparable to
that of previously published work using hardware specific models.
</summary>
    <author>
      <name>James Stevens</name>
    </author>
    <author>
      <name>Andreas Klöckner</name>
    </author>
    <link href="http://arxiv.org/abs/1604.04997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06763v3</id>
    <updated>2017-09-04T08:06:33Z</updated>
    <published>2016-04-22T18:10:10Z</published>
    <title>Balanced Fair Resource Sharing in Computer Clusters</title>
    <summary>  We represent a computer cluster as a multi-server queue with some arbitrary
bipartite graph of compatibilities between jobs and servers. Each server
processes its jobs sequentially in FCFS order. The service rate of a job at any
given time is the sum of the service rates of all servers processing this job.
We show that the corresponding queue is quasi-reversible and use this property
to design a scheduling algorithm achieving balanced fair sharing of the service
capacity.
</summary>
    <author>
      <name>Thomas Bonald</name>
    </author>
    <author>
      <name>Céline Comte</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.peva.2017.08.006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.peva.2017.08.006" rel="related"/>
    <link href="http://arxiv.org/abs/1604.06763v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06763v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03068v1</id>
    <updated>2016-05-10T15:44:54Z</updated>
    <published>2016-05-10T15:44:54Z</published>
    <title>On Stability and Sojourn Time of Peer-to-Peer Queuing Systems</title>
    <summary>  Recent development of peer-to-peer (P2P) services (e.g. streaming, file
sharing, and storage) systems introduces a new type of queue systems that
receive little attention before, where both job and server arrive and depart
randomly. Current study on these models focuses on the stability condition,
under exponential workload assumption. This paper extends existing result in
two aspects. In the first part of the paper we relax the exponential workload
assumption, and study the stability of systems with general workload
distribution. The second part of the paper focuses on the job sojourn time. An
upper bound and a lower bound for job sojourn time are investigated. We
evaluate tightness of the bounds by numerical analysis.
</summary>
    <author>
      <name>Taoyu Li</name>
    </author>
    <author>
      <name>Minghua Chen</name>
    </author>
    <author>
      <name>Tony Lee</name>
    </author>
    <author>
      <name>Xing Li</name>
    </author>
    <link href="http://arxiv.org/abs/1605.03068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05753v2</id>
    <updated>2017-08-26T03:57:02Z</updated>
    <published>2016-05-18T20:35:11Z</published>
    <title>Delay Bounds for Multiclass FIFO</title>
    <summary>  FIFO is perhaps the simplest scheduling discipline. For single-class FIFO,
its delay guarantee performance has been extensively studied: The well-known
results include a stochastic delay bound for $GI/GI/1$ by Kingman and a
deterministic delay bound for $D/D/1$ by Cruz. However, for multiclass FIFO,
few such results are available. To fill the gap, we prove delay bounds for
multiclass FIFO in this work, considering both deterministic and stochastic
cases. Specifically, delay bounds are presented for multiclass D/D/1, GI/GI/1
and G/G/1. In addition, examples are provided for several basic settings to
demonstrate the obtained bounds in more explicit forms, which are also compared
with simulation results.
</summary>
    <author>
      <name>Yuming Jiang</name>
    </author>
    <author>
      <name>Vishal Misra</name>
    </author>
    <link href="http://arxiv.org/abs/1605.05753v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05753v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00636v1</id>
    <updated>2016-08-01T22:52:28Z</updated>
    <published>2016-08-01T22:52:28Z</published>
    <title>A survey of sparse matrix-vector multiplication performance on large
  matrices</title>
    <summary>  We contribute a third-party survey of sparse matrix-vector (SpMV) product
performance on industrial-strength, large matrices using: (1) The SpMV
implementations in Intel MKL, the Trilinos project (Tpetra subpackage), the
CUSPARSE library, and the CUSP library, each running on modern architectures.
(2) NVIDIA GPUs and Intel multi-core CPUs (supported by each software package).
(3) The CSR, BSR, COO, HYB, and ELL matrix formats (supported by each software
package).
</summary>
    <author>
      <name>Max Grossman</name>
    </author>
    <author>
      <name>Christopher Thiele</name>
    </author>
    <author>
      <name>Mauricio Araya-Polo</name>
    </author>
    <author>
      <name>Florian Frank</name>
    </author>
    <author>
      <name>Faruk O. Alpak</name>
    </author>
    <author>
      <name>Vivek Sarkar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Rice Oil &amp; Gas High Performance Computing Workshop. March 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04295v1</id>
    <updated>2016-08-15T15:02:13Z</updated>
    <published>2016-08-15T15:02:13Z</published>
    <title>Robust benchmarking in noisy environments</title>
    <summary>  We propose a benchmarking strategy that is robust in the presence of timer
error, OS jitter and other environmental fluctuations, and is insensitive to
the highly nonideal statistics produced by timing measurements. We construct a
model that explains how these strongly nonideal statistics can arise from
environmental fluctuations, and also justifies our proposed strategy. We
implement this strategy in the BenchmarkTools Julia package, where it is used
in production continuous integration (CI) pipelines for developing the Julia
language and its ecosystem.
</summary>
    <author>
      <name>Jiahao Chen</name>
    </author>
    <author>
      <name>Jarrett Revels</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures, Proceedings of the 20th Annual IEEE High
  Performance Extreme Computing Conference, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68N30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.1; D.2.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07046v1</id>
    <updated>2016-08-25T08:15:40Z</updated>
    <published>2016-08-25T08:15:40Z</published>
    <title>Transient performance analysis of zero-attracting LMS</title>
    <summary>  Zero-attracting least-mean-square (ZA-LMS) algorithm has been widely used for
online sparse system identification. It combines the LMS framework and
$\ell_1$-norm regularization to promote sparsity, and relies on subgradient
iterations. Despite the significant interest in ZA-LMS, few works analyzed its
transient behavior. The main difficulty lies in the nonlinearity of the update
rule. In this work, a detailed analysis in the mean and mean-square sense is
carried out in order to examine the behavior of the algorithm. Simulation
results illustrate the accuracy of the model and highlight its performance
through comparisons with an existing model.
</summary>
    <author>
      <name>Jie Chen</name>
    </author>
    <author>
      <name>Cedric Richard</name>
    </author>
    <author>
      <name>Yingying Song</name>
    </author>
    <author>
      <name>David Brie</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2016.2616890</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2016.2616890" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.07046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.06307v1</id>
    <updated>2016-10-20T07:23:33Z</updated>
    <published>2016-10-20T07:23:33Z</published>
    <title>Breakdown of a Benchmark Score Without Internal Analysis of Benchmarking
  Program</title>
    <summary>  A breakdown of a benchmark score is how much each aspect of the system
performance affects the score. Existing methods require internal analysis on
the benchmarking program and then involve the following problems: (1) require a
certain amount of labor for code analysis, profiling, simulation, and so on and
(2) require the benchmarking program itself. In this paper, we present a method
for breaking down a benchmark score without internal analysis of the
benchmarking program. The method utilizes regression analysis of benchmark
scores on a number of systems. Experimental results with 3 benchmarks on 15
Android smartphones showed that our method could break down those benchmark
scores even though there is room for improvement in accuracy.
</summary>
    <author>
      <name>Naoki Matagawa</name>
    </author>
    <author>
      <name>Kazuyuki Shudo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.06307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.06307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.06415v1</id>
    <updated>2017-01-20T16:21:41Z</updated>
    <published>2017-01-20T16:21:41Z</published>
    <title>Steady state availability general equations of decision and sequential
  processes in Continuous Time Markov Chain models</title>
    <summary>  Continuous Time Markov Chain (CMTC) is widely used to describe and analyze
systems in several knowledge areas. Steady state availability is one important
analysis that can be made through Markov chain formalism that allows
researchers generate equations for several purposes, such as channel capacity
estimation in wireless networks as well as system performance estimations. The
problem with this kind of analysis is the complex process to generating these
equations. In this letter, we have developed general equations for decision and
sequential processes of CMTC Models, aiming to help researchers to develop
steady state availability equations. We also have developed the general
equation here termed as Closed Decision Process.
</summary>
    <author>
      <name>Eduardo M. Vasconcelos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 3 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.06415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.06415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02968v1</id>
    <updated>2017-02-09T20:11:26Z</updated>
    <published>2017-02-09T20:11:26Z</published>
    <title>Comparative benchmarking of cloud computing vendors with High
  Performance Linpack</title>
    <summary>  We present a comparative analysis of the maximum performance achieved by the
Linpack benchmark on compute intensive hardware publicly available from
multiple cloud providers. We study both performance within a single compute
node, and speedup for distributed memory calculations with up to 32 nodes or at
least 512 computing cores. We distinguish between hyper-threaded and
non-hyper-threaded scenarios and estimate the performance per single computing
core. We also compare results with a traditional supercomputing system for
reference. Our findings provide a way to rank the cloud providers and
demonstrate the viability of the cloud for high performance computing
applications.
</summary>
    <author>
      <name>Mohammad Mohammadi</name>
    </author>
    <author>
      <name>Timur Bazhirov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3195612.3195613</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3195612.3195613" rel="related"/>
    <link href="http://arxiv.org/abs/1702.02968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01070v2</id>
    <updated>2017-05-16T15:32:58Z</updated>
    <published>2017-05-02T16:52:31Z</published>
    <title>Correcting for Non-Markovian Asymptotic Effects using Markovian
  Representation</title>
    <summary>  Asymptotic properties of Markov Processes, such as steady state probabilities
or hazard rate for absorbing states can be efficiently calculated by means of
linear algebra even for large-scale problems. This paper discusses the methods
for adjusting parameters of the Markov models to account for non-constant
transition rates. In particular, transitions with fixed delays are considered
along with the transitions that follow Weibull and lognormal distributions.
Procedures for both steady-state solutions in the absence of an absorbing
state, and for hazard rates to an absorbing state are provided and demonstrated
on several examples.
</summary>
    <author>
      <name>Vitali Volovoi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Added background, convergence discussion, additional examples. 9
  pages (double column format), 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01070v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01070v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.10738v1</id>
    <updated>2017-05-19T14:46:14Z</updated>
    <published>2017-05-19T14:46:14Z</published>
    <title>Approximation of LRU Caches Miss Rate: Application to Power-law
  Popularities</title>
    <summary>  Building on the 1977 pioneering work of R. Fagin, we give a closed-form
expression for the approximated Miss Rate (MR) of LRU Caches assuming a
power-law popularity. Asymptotic behavior of this expression is an already
known result when power-law parameter is above 1. It is extended to any value
of the parameter. In addition, we bring a new analysis of the conditions (cache
relative size, popularity parameter) under which the ratio of LRU MR to Static
MR is worst-case.
</summary>
    <author>
      <name>Christian Berthet</name>
    </author>
    <link href="http://arxiv.org/abs/1705.10738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01859v2</id>
    <updated>2017-09-07T14:08:50Z</updated>
    <published>2017-07-06T16:41:07Z</published>
    <title>Efficient Strategy Iteration for Mean Payoff in Markov Decision
  Processes</title>
    <summary>  Markov decision processes (MDPs) are standard models for probabilistic
systems with non-deterministic behaviours. Mean payoff (or long-run average
reward) provides a mathematically elegant formalism to express performance
related properties. Strategy iteration is one of the solution techniques
applicable in this context. While in many other contexts it is the technique of
choice due to advantages over e.g. value iteration, such as precision or
possibility of domain-knowledge-aware initialization, it is rarely used for
MDPs, since there it scales worse than value iteration. We provide several
techniques that speed up strategy iteration by orders of magnitude for many
MDPs, eliminating the performance disadvantage while preserving all its
advantages.
</summary>
    <author>
      <name>Jan Křetínský</name>
    </author>
    <author>
      <name>Tobias Meggendorfer</name>
    </author>
    <link href="http://arxiv.org/abs/1707.01859v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01859v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03264v1</id>
    <updated>2017-09-11T07:03:23Z</updated>
    <published>2017-09-11T07:03:23Z</published>
    <title>Report: Performance comparison between C2075 and P100 GPU cards using
  cosmological correlation functions</title>
    <summary>  In this report, some cosmological correlation functions are used to evaluate
the differential performance between C2075 and P100 GPU cards. In the past, the
correlation functions used in this work have been widely studied and exploited
on some previous GPU architectures. The analysis of the performance indicates
that a speedup in the range from 13 to 15 is achieved without any additional
optimization process for the P100 card.
</summary>
    <author>
      <name>Miguel Cárdenas-Montes</name>
    </author>
    <author>
      <name>Iván Méndez-Jiménez</name>
    </author>
    <author>
      <name>Juan José Rodríguez-Vázquez</name>
    </author>
    <author>
      <name>José María Hernández Calama</name>
    </author>
    <link href="http://arxiv.org/abs/1709.03264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08774v1</id>
    <updated>2017-10-24T13:51:36Z</updated>
    <published>2017-10-24T13:51:36Z</published>
    <title>High-Performance Code Generation though Fusion and Vectorization</title>
    <summary>  We present a technique for automatically transforming kernel-based
computations in disparate, nested loops into a fused, vectorized form that can
reduce intermediate storage needs and lead to improved performance on
contemporary hardware.
  We introduce representations for the abstract relationships and data
dependencies of kernels in loop nests and algorithms for manipulating them into
more efficient form; we similarly introduce techniques for determining data
access patterns for stencil-like array accesses and show how this can be used
to elide storage and improve vectorization.
  We discuss our prototype implementation of these ideas---named HFAV---and its
use of a declarative, inference-based front-end to drive transformations, and
we present results for some prominent codes in HPC.
</summary>
    <author>
      <name>Jason Sewall</name>
    </author>
    <author>
      <name>Simon J. Pennycook</name>
    </author>
    <link href="http://arxiv.org/abs/1710.08774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.11471v4</id>
    <updated>2019-02-09T22:33:13Z</updated>
    <published>2017-10-28T10:46:22Z</published>
    <title>Distributed Server Allocation for Content Delivery Networks</title>
    <summary>  We propose a dynamic formulation of file-sharing networks in terms of an
average cost Markov decision process with constraints. By analyzing a
Whittle-like relaxation thereof, we propose an index policy in the spirit of
Whittle and compare it by simulations with other natural heuristics.
</summary>
    <author>
      <name>Sarath Pattathil</name>
    </author>
    <author>
      <name>Vivek S. Borkar</name>
    </author>
    <author>
      <name>Gaurav S. Kasbekar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.11471v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.11471v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.01254v3</id>
    <updated>2018-04-14T14:16:59Z</updated>
    <published>2018-02-05T03:35:27Z</published>
    <title>A Measurement Theory of Locality</title>
    <summary>  Locality is a fundamental principle used extensively in program and system
optimization. It can be measured in many ways. This paper formalizes the
metrics of locality into a measurement theory. The new theory includes the
precise definition of locality metrics based on access frequency, reuse time,
reuse distance, working set, footprint, and the cache miss ratio. It gives the
formal relation between these definitions and the proofs of equivalence or
non-equivalence. It provides the theoretical justification for four successful
locality models in operating systems, programming languages, and computer
architectures which were developed empirically.
</summary>
    <author>
      <name>Liang Yuan</name>
    </author>
    <author>
      <name>Chen Ding</name>
    </author>
    <author>
      <name>Peter Denning</name>
    </author>
    <author>
      <name>Yunquan Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1802.01254v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.01254v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.10553v1</id>
    <updated>2018-03-28T12:10:26Z</updated>
    <published>2018-03-28T12:10:26Z</published>
    <title>Effect of payload size on mean response time when message segmentations
  occur using $\rm{M}^{\rm X}/\rm{G}/1$ queueing model</title>
    <summary>  This paper proposes the $\rm{M}^{\rm X}/\rm{G}/1$ queueing model to represent
arrivals of segmented packets when message segmentations occur. This queueing
model enables us to derive the closed form of mean response time, given payload
size, message size distribution and message arrival rate. From a numerical
result, we show that the mean response time is more convex in payload sizes if
message arrival rate is larger in a scenario where Web objects are delivered
over a physical link.
</summary>
    <author>
      <name>Takashi Ikegawa</name>
    </author>
    <link href="http://arxiv.org/abs/1803.10553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.10553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00443v3</id>
    <updated>2018-10-11T01:15:39Z</updated>
    <published>2018-08-01T17:46:04Z</published>
    <title>Relative Age of Information: Maintaining Freshness while Considering the
  Most Recently Generated Information</title>
    <summary>  A queueing system handling a sequence of message arrivals is considered where
each message obsoletes all previous messages. The objective is to assess the
freshness of the latest message/information that has been successfully
transmitted, i.e., "age of information" (AoI). We study a variation of
traditional AoI, the "Relative AoI", here defined so as to account for the
presence of newly arrived messages/information to the queue to be transmitted.
</summary>
    <author>
      <name>George Kesidis</name>
    </author>
    <author>
      <name>Takis Konstantopoulos</name>
    </author>
    <author>
      <name>Michael Zazanis</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00443v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00443v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04413v1</id>
    <updated>2018-10-10T08:34:56Z</updated>
    <published>2018-10-10T08:34:56Z</published>
    <title>Performance analysis and optimization of the JOREK code for many-core
  CPUs</title>
    <summary>  This report investigates the performance of the JOREK code on the Intel
Knights Landing and Skylake processor architectures. The OpenMP scaling of the
matrix construction part of the code was analyzed and improved synchronization
methods were implemented. A new switch was implemented to control the number of
threads used for the linear equation solver independently from other parts of
the code. The matrix construction subroutine was vectorized, and the data
locality was also improved. These steps led to a factor of two speedup for the
matrix construction.
</summary>
    <author>
      <name>T. B. Fehér</name>
    </author>
    <author>
      <name>M. Hölzl</name>
    </author>
    <author>
      <name>G. Latu</name>
    </author>
    <author>
      <name>G. T. A. Huijsmans</name>
    </author>
    <link href="http://arxiv.org/abs/1810.04413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.11588v1</id>
    <updated>2019-03-20T09:06:13Z</updated>
    <published>2019-03-20T09:06:13Z</published>
    <title>Algorithms of evaluation of the waiting time and the modelling of the
  terminal activity</title>
    <summary>  This paper approaches the application of the waiting model with Poisson
inputs and priorities in the port activity. The arrival of ships in the
maritime terminal is numerically modelled, and specific parameters for the
distribution functions of service and of inputs are determined, in order to
establish the waiting time of ships in the seaport and a stationary process.
The modelling is based on waiting times and on the traffic coefficient.
</summary>
    <author>
      <name>Gh. Miscoi</name>
    </author>
    <author>
      <name>A. Costea</name>
    </author>
    <author>
      <name>R. I. Ţicu</name>
    </author>
    <author>
      <name>C. Pomazan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.21506/j.ponte.2016.8.17</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.21506/j.ponte.2016.8.17" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ponte Journal, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.11588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.11588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.07141v1</id>
    <updated>2019-04-11T22:59:06Z</updated>
    <published>2019-04-11T22:59:06Z</published>
    <title>Defence Efficiency</title>
    <summary>  In order to automate actions, such as defences against network attacks, one
needs to quantify their efficiency. This can subsequently be used in
post-evaluation, learning, etc. In order to quantify the defence efficiency as
a function of the impact of the defence and its total cost, we present several
natural requirements from such a definition of efficiency and provide a natural
definition that complies with these requirements. Next, we precisely
characterize our definition of efficiency by the axiomatic approach; namely, we
strengthen the original requirements from such a definition and prove that the
given definition is the unique definition that satisfies those requirements.
Finally, we generalize the definition to the case of any number of input
variables in two natural ways, and compare these generalizations.
</summary>
    <author>
      <name>Gleb Polevoy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.07141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.07141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.03427v1</id>
    <updated>2019-07-08T07:16:36Z</updated>
    <published>2019-07-08T07:16:36Z</published>
    <title>Guidelines for benchmarking of optimization approaches for fitting
  mathematical models</title>
    <summary>  Insufficient performance of optimization approaches for fitting of
mathematical models is still a major bottleneck in systems biology. In this
manuscript, the reasons and methodological challenges are summarized as well as
their impact in benchmark studies. Important aspects for increasing evidence of
outcomes of benchmark analyses are discussed. Based on general guidelines for
benchmarking in computational biology, a collection of tailored guidelines is
presented for performing informative and unbiased benchmarking of
optimization-based fitting approaches. Comprehensive benchmark studies based on
these recommendations are urgently required for establishing of a robust and
reliable methodology for the systems biology community.
</summary>
    <author>
      <name>Clemens Kreutz</name>
    </author>
    <link href="http://arxiv.org/abs/1907.03427v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.03427v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.03653v2</id>
    <updated>2019-08-19T12:05:35Z</updated>
    <published>2019-08-09T23:06:31Z</published>
    <title>Performance of Devito on HPC-Optimised ARM Processors</title>
    <summary>  We evaluate the performance of Devito, a domain specific language (DSL) for
finite differences on Arm ThunderX2 processors. Experiments with two common
seismic computational kernels demonstrate that Arm processors can deliver
competitive performance compared to other Intel Xeon processors.
</summary>
    <author>
      <name>Hermes Senger</name>
    </author>
    <author>
      <name>Jaime F. de Souza</name>
    </author>
    <author>
      <name>Edson S. Gomi</name>
    </author>
    <author>
      <name>Fabio Luporini</name>
    </author>
    <author>
      <name>Gerard J. Gorman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, one figure, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.03653v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.03653v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.04236v1</id>
    <updated>2019-08-12T16:45:41Z</updated>
    <published>2019-08-12T16:45:41Z</published>
    <title>MLP Aware Scheduling Techniques in Multithreaded Processors</title>
    <summary>  Major chip manufacturers have all introduced Multithreaded processors. These
processors are used for running a variety of workloads. Efficient resource
utilization is an important design aspect in such processors. Particularly, it
is important to take advantage of available memory-level parallelism(MLP). In
this paper I propose a MLP aware operating system (OS) scheduling algorithm for
Multithreaded Multi-core processors. By observing the MLP available in each
thread and by balancing it with available MLP resources in the system the OS
will come up with a new schedule of threads for the next quantum that could
potentially improve overall performance. We do a qualitative comparison of our
solution with other hardware and software techniques. This work can be extended
by doing a quantitative evaluation and by further refining the scheduling
optimization.
</summary>
    <author>
      <name>Murthy Durbhakula</name>
    </author>
    <link href="http://arxiv.org/abs/1908.04236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.04379v1</id>
    <updated>2019-11-18T17:45:56Z</updated>
    <published>2019-11-18T17:45:56Z</published>
    <title>General Matrix-Matrix Multiplication Using SIMD features of the PIII</title>
    <summary>  Generalised matrix-matrix multiplication forms the kernel of many
mathematical algorithms. A faster matrix-matrix multiply immediately benefits
these algorithms. In this paper we implement efficient matrix multiplication
for large matrices using the floating point Intel Pentium SIMD (Single
Instruction Multiple Data) architecture. A description of the issues and our
solution is presented, paying attention to all levels of the memory hierarchy.
Our results demonstrate an average performance of 2.09 times faster than the
leading public domain matrix-matrix multiply routines.
</summary>
    <author>
      <name>Douglas Aberdeen</name>
    </author>
    <author>
      <name>Jonathan Baxter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/3-540-44520-X_138</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/3-540-44520-X_138" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1911.05181</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Euro-Par '00 Proceedings from the 6th International Euro-Par
  Conference on Parallel Processing (2000) Pages 980-983</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1912.04379v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.04379v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.08368v1</id>
    <updated>2019-12-18T03:50:13Z</updated>
    <published>2019-12-18T03:50:13Z</published>
    <title>Real-Time Prediction of Delay Distribution in Service Systems using
  Mixture Density Networks</title>
    <summary>  Motivated by interest in providing more efficient services in customer
service systems, we use statistical learning methods and delay history
information to predict the conditional distribution of the customers' waiting
times in queueing systems. From the predicted distributions, descriptive
statistics of the system such as the mean, variance and percentiles of the
waiting times can be obtained, which can be used for delay announcements, SLA
conformance and better system management. We model the conditional
distributions by mixtures of Gaussians, parameters of which can be estimated
using Mixture Density Networks. The evaluations show that exploiting more delay
history information can result in much more accurate predictions under
realistic time-varying arrival assumptions.
</summary>
    <author>
      <name>Majid Raeis</name>
    </author>
    <author>
      <name>Ali Tizghadam</name>
    </author>
    <author>
      <name>Alberto Leon-Garcia</name>
    </author>
    <link href="http://arxiv.org/abs/1912.08368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.08368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.04416v1</id>
    <updated>2020-02-11T14:39:13Z</updated>
    <published>2020-02-11T14:39:13Z</published>
    <title>Reproducibility Report for the Paper: Modeling of Request Cloning in
  Cloud Server Systems using Processor Sharing</title>
    <summary>  The authors have uploaded their artifact on Zenodo, which ensures a long-term
retention of the artifact. The code is suitably documented, and some examples
are given. A minimalistic overall description of the engine is provided. The
artifact allows to setup the environment quite quickly, and the dependencies
are well documented. The process to regenerate data for the figures in the
paper completes, and all results are reproducible.
  This paper can thus receive the Artifacts Available badge and the Artifacts
Evaluated-Functional. Given the high quality of the artifact, also the
Artifacts Evaluated-Reusable badge can be assigned.
</summary>
    <author>
      <name>Alessandro Pellegrini</name>
    </author>
    <link href="http://arxiv.org/abs/2002.04416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.04416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.14639v1</id>
    <updated>2020-04-30T08:56:48Z</updated>
    <published>2020-04-30T08:56:48Z</published>
    <title>Communication-Aware Scheduling of Precedence-Constrained Tasks on
  Related Machines</title>
    <summary>  Scheduling precedence-constrained tasks is a classical problem that has been
studied for more than fifty years. However, little progress has been made in
the setting where there are communication delays between tasks. Results for the
case of identical machines were derived nearly thirty years ago, and yet no
results for related machines have followed. In this work, we propose a new
scheduler, Generalized Earliest Time First (GETF), and provide the first
provable, worst-case approximation guarantees for the goals of minimizing both
the makespan and total weighted completion time of tasks with precedence
constraints on related machines with machine-dependent communication times.
</summary>
    <author>
      <name>Yu Su</name>
    </author>
    <author>
      <name>Xiaoqi Ren</name>
    </author>
    <author>
      <name>Shai Vardi</name>
    </author>
    <author>
      <name>Adam Wierman</name>
    </author>
    <link href="http://arxiv.org/abs/2004.14639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.14639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.13144v1</id>
    <updated>2020-05-27T03:53:41Z</updated>
    <published>2020-05-27T03:53:41Z</published>
    <title>A review of analytical performance modeling and its role in computer
  engineering and science</title>
    <summary>  This article is a review of analytical performance modeling for computer
systems. It discusses the motivation for this area of research, examines key
issues, introduces some ideas, illustrates how it is applied, and points out a
role that it can play in developing Computer Science.
</summary>
    <author>
      <name>Y. C. Tay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Some parts of this article appeared in "Lessons from Teaching
  Analytical Performance Modeling", Proc. ICPE Workshop on Education and
  Practice of Performance Engineering, Mumbai, India (April 2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.13144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.13144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.05979v1</id>
    <updated>2020-06-10T17:49:16Z</updated>
    <published>2020-06-10T17:49:16Z</published>
    <title>Product Forms for FCFS Queueing Models with Arbitrary Server-Job
  Compatibilities: An Overview</title>
    <summary>  In recent years a number of models involving different compatibilities
between jobs and servers in queueing systems, or between agents and resources
in matching systems, have been studied, and, under Markov assumptions and
appropriate stability conditions, the stationary distributions have been shown
to have product forms. We survey these results and show how, under an
appropriate detailed description of the state, many are corollaries of similar
results for the Order Independent Queue. We also discuss how to use the product
form results to determine distributions for steady-state response times.
</summary>
    <author>
      <name>Kristen Gardner</name>
    </author>
    <author>
      <name>Rhonda Righter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.05979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.05979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.13951v1</id>
    <updated>2020-07-28T02:16:34Z</updated>
    <published>2020-07-28T02:16:34Z</published>
    <title>Analytical Performance Modeling of NoCs under Priority Arbitration and
  Bursty Traffic</title>
    <summary>  Networks-on-Chip (NoCs) used in commercial many-core processors typically
incorporate priority arbitration. Moreover, they experience bursty traffic due
to application workloads. However, most state-of-the-art NoC analytical
performance analysis techniques assume fair arbitration and simple traffic
models. To address these limitations, we propose an analytical modeling
technique for priority-aware NoCs under bursty traffic. Experimental
evaluations with synthetic and bursty traffic show that the proposed approach
has less than 10% modeling error with respect to cycle-accurate NoC simulator.
</summary>
    <author>
      <name>Sumit K. Mandal</name>
    </author>
    <author>
      <name>Raid Ayoub</name>
    </author>
    <author>
      <name>Michael Kishinevsky</name>
    </author>
    <author>
      <name>Mohammad M. Islam</name>
    </author>
    <author>
      <name>Umit Y. Ogras</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will appear in a future issue of IEEE Embedded Systems
  Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.13951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.13951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.03695v1</id>
    <updated>2020-12-07T13:49:37Z</updated>
    <published>2020-12-07T13:49:37Z</published>
    <title>Non-Asymptotic Performance Analysis of Size-Based Routing Policies</title>
    <summary>  We investigate the performance of two size-based routing policies: the Size
Interval Task Assignment (SITA) and Task Assignment based on Guessing Size
(TAGS). We consider a system with two servers and Bounded Pareto distributed
job sizes with tail parameter 1 where the difference between the size of the
largest and the smallest job is finite. We show that the ratio between the mean
waiting time of TAGS over the mean waiting time of SITA is unbounded when the
largest job size is large and the arrival rate times the largest job size is
less than one. We provide numerical experiments that show that our theoretical
findings extend to Bounded Pareto distributed job sizes with tail parameter
different to 1.
</summary>
    <author>
      <name>E. Bachmat</name>
    </author>
    <author>
      <name>J. Doncel</name>
    </author>
    <link href="http://arxiv.org/abs/2012.03695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.03695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.03187v4</id>
    <updated>2021-10-18T09:40:59Z</updated>
    <published>2021-04-07T15:29:12Z</published>
    <title>A Preliminary Proposal for an Analytical Model for Evaluating the Impact
  on Performance of Data Access Patterns in Transaction Execution</title>
    <summary>  We present a preliminary proposal for an analytical model for evaluating the
impact on performance of data access patterns in concurrent transaction
execution. We consider the case of concurrency control protocols that use
locking to ensure isolation in the execution of transactions. We analyse
scenarios where transactions access one or more sets of data items in the same
order or in different order.
</summary>
    <author>
      <name>Pierangelo Di Sanzo</name>
    </author>
    <link href="http://arxiv.org/abs/2104.03187v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.03187v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.02926v1</id>
    <updated>2021-05-06T19:47:37Z</updated>
    <published>2021-05-06T19:47:37Z</published>
    <title>Download time analysis for distributed storage systems with node
  failures</title>
    <summary>  We consider a distributed storage system which stores several hot (popular)
and cold (less popular) data files across multiple nodes or servers. Hot files
are stored using repetition codes while cold files are stored using erasure
codes. The nodes are prone to failure and hence at any given time, we assume
that only a fraction of the nodes are available. Using a cavity process based
mean field framework, we analyze the download time for users accessing hot or
cold data in the presence of failed nodes. Our work also illustrates the impact
of the choice of the storage code on the download time performance of users in
the system.
</summary>
    <author>
      <name>Tim Hellemans</name>
    </author>
    <author>
      <name>Arti Yardi</name>
    </author>
    <author>
      <name>Tejas Bodas</name>
    </author>
    <link href="http://arxiv.org/abs/2105.02926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.02926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.11104v1</id>
    <updated>2021-05-24T05:55:03Z</updated>
    <published>2021-05-24T05:55:03Z</published>
    <title>Conditional Waiting Time Analysis in Tandem Polling Queues</title>
    <summary>  We analyze a tandem network of polling queues with two product types and two
stations. We assume that external arrivals to the network follow a Poisson
process, and service times at each station are exponentially distributed. For
this system, we determine the mean conditional waiting time for an arriving
customer using a sample path analysis approach. The approach classifies system
state upon arrival into scenarios and exploits an inherent structure in the
sequence of events that occur till the customer departs to obtain conditional
waiting time estimates. We conduct numerical studies to show both the accuracy
of our conditional waiting time estimates and their practical importance.
</summary>
    <author>
      <name>Ravi Suman</name>
    </author>
    <author>
      <name>Ananth Krishnamurthy</name>
    </author>
    <link href="http://arxiv.org/abs/2105.11104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.11104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.08473v1</id>
    <updated>2021-06-15T22:43:55Z</updated>
    <published>2021-06-15T22:43:55Z</published>
    <title>Age of Information for Small Buffer Systems</title>
    <summary>  Consider a message processing system whose objective is to produce the most
current information as measured by the quantity known as "age of information".
We have argued in previous papers that if we are allowed to design the message
processing policy ad libitum, we should keep a small buffer and operate
according to a LIFO policy. In this small note we provide an analysis for the
AoI of the P_m system which uses a buffer of size m, a single server, operating
without service preemption and in a LIFO manner for stored messages. Analytical
expressions for the mean (or even distribution) of the AoI in steady-state are
possible but with the aid computer algebra. We explain the the analysis for
m=3.
</summary>
    <author>
      <name>George Kesidis</name>
    </author>
    <author>
      <name>Takis Konstantopoulos</name>
    </author>
    <author>
      <name>Michael Zazanis</name>
    </author>
    <link href="http://arxiv.org/abs/2106.08473v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.08473v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.09787v1</id>
    <updated>2021-06-17T20:00:16Z</updated>
    <published>2021-06-17T20:00:16Z</published>
    <title>Comparing the behavior of OpenMP Implementations with various
  Applications on two different Fujitsu A64FX platforms</title>
    <summary>  The development of the A64FX processor by Fujitsu has been a massive
innovation in vectorized processors and led to Fugaku: the current world's
fastest supercomputer. We use a variety of tools to analyze the behavior and
performance of several OpenMP applications with different compilers, and how
these applications scale on the different A64FX processors on clusters at Stony
Brook University and RIKEN.
</summary>
    <author>
      <name>Benjamin Michalowicz</name>
    </author>
    <author>
      <name>Eric Raut</name>
    </author>
    <author>
      <name>Yan Kang</name>
    </author>
    <author>
      <name>Tony Curtis</name>
    </author>
    <author>
      <name>Barbara Chapman</name>
    </author>
    <author>
      <name>Dossay Oryspayev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3437359.3465592</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3437359.3465592" rel="related"/>
    <link href="http://arxiv.org/abs/2106.09787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.09787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.10334v2</id>
    <updated>2021-06-29T00:40:28Z</updated>
    <published>2021-06-18T19:49:46Z</published>
    <title>AutoTune: Improving End-to-end Performance and Resource Efficiency for
  Microservice Applications</title>
    <summary>  Most large web-scale applications are now built by composing collections
(from a few up to 100s or 1000s) of microservices. Operators need to decide how
many resources are allocated to each microservice, and these allocations can
have a large impact on application performance. Manually determining
allocations that are both cost-efficient and meet performance requirements is
challenging, even for experienced operators. In this paper we present AutoTune,
an end-to-end tool that automatically minimizes resource utilization while
maintaining good application performance.
</summary>
    <author>
      <name>Michael Alan Chang</name>
    </author>
    <author>
      <name>Aurojit Panda</name>
    </author>
    <author>
      <name>Hantao Wang</name>
    </author>
    <author>
      <name>Yuancheng Tsai</name>
    </author>
    <author>
      <name>Rahul Balakrishnan</name>
    </author>
    <author>
      <name>Scott Shenker</name>
    </author>
    <link href="http://arxiv.org/abs/2106.10334v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.10334v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.07656v1</id>
    <updated>2021-08-17T14:39:37Z</updated>
    <published>2021-08-17T14:39:37Z</published>
    <title>On the equivalence of holding cost and response time for evaluating
  performance of queues</title>
    <summary>  This self-contained discussion relates the long-run average holding cost per
unit time to the long-run average response time per customer in a $G/G/1$ queue
with no assumption made on the order of service. The only restriction
established is that the system be ergodic. This is achieved using standard
queuing theory. The practical relevance of such a result is discussed in the
context of simulation output analysis as well as through an application to
formulating a Markov Decision Process that minimises long-run average response
time per customer.
</summary>
    <author>
      <name>Dylan Solms</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Discussion paper with 4 figures and 2 examples</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.07656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.07656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.12223v1</id>
    <updated>2021-08-27T11:31:15Z</updated>
    <published>2021-08-27T11:31:15Z</published>
    <title>On the Representation of Correlated Exponential Distributions by Phase
  Type Distributions</title>
    <summary>  In this paper we present results for bivariate exponential distributions
which are represented by phase type distributions. The paper extends results
from previous publications [5, 14] on this topic by introducing new
representations that require a smaller number of phases to reach some
correlation coefficient and introduces different ways to describe correlation
between exponentially distributed random variables. Furthermore, it is shown
how Markovian Arrival Processes (MAPs) with exponential marginal distribution
can be generated from the phase type representations of exponential
distributions and how the results for exponential distributions can be applied
to define correlated hyperexponential or Erlang distributions. As application
examples we analyze two queueing models with correlated inter-arrival and
service times.
</summary>
    <author>
      <name>Peter Buchholz</name>
    </author>
    <link href="http://arxiv.org/abs/2108.12223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.12223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.12914v1</id>
    <updated>2021-08-29T20:51:02Z</updated>
    <published>2021-08-29T20:51:02Z</published>
    <title>Leveraging Transprecision Computing for Machine Vision Applications at
  the Edge</title>
    <summary>  Machine vision tasks present challenges for resource constrained edge
devices, particularly as they execute multiple tasks with variable workloads. A
robust approach that can dynamically adapt in runtime while maintaining the
maximum quality of service (QoS) within resource constraints, is needed. The
paper presents a lightweight approach that monitors the runtime workload
constraint and leverages accuracy-throughput trade-off. Optimisation techniques
are included which find the configurations for each task for optimal accuracy,
energy and memory and manages transparent switching between configurations. For
an accuracy drop of 1%, we show a 1.6x higher achieved frame processing rate
with further improvements possible at lower accuracy.
</summary>
    <author>
      <name>Umar Ibrahim Minhas</name>
    </author>
    <author>
      <name>Lev Mukhanov</name>
    </author>
    <author>
      <name>Georgios Karakonstantis</name>
    </author>
    <author>
      <name>Hans Vandierendonck</name>
    </author>
    <author>
      <name>Roger Woods</name>
    </author>
    <link href="http://arxiv.org/abs/2108.12914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.12914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.04148v1</id>
    <updated>2021-09-09T10:17:32Z</updated>
    <published>2021-09-09T10:17:32Z</published>
    <title>Automatic Timing-Coherent Transactor Generation for Mixed-level
  Simulations</title>
    <summary>  In this paper we extend the concept of the traditional transactor, which
focuses on correct content transfer, to a new timing-coherent transactor that
also accurately aligns the timing of each transaction boundary so that
designers can perform precise concurrent system behavior analysis in
mixed-abstraction-level system simulations which are essential to increasingly
complex system designs. To streamline the process, we also developed an
automatic approach for timing-coherent transactor generation. Our approach is
actually applied in mixed-level simulations and the results show that it
achieves 100% timing accuracy while the conventional approach produces results
of 25% to 44% error rate.
</summary>
    <author>
      <name>Li-Chun Chen</name>
    </author>
    <author>
      <name>Hsin-I Wu</name>
    </author>
    <author>
      <name>Ren-Song Tsay</name>
    </author>
    <link href="http://arxiv.org/abs/2109.04148v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.04148v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.12567v1</id>
    <updated>2021-09-26T11:35:39Z</updated>
    <published>2021-09-26T11:35:39Z</published>
    <title>An Analysis into the Performance and Memory Usage of MATLAB Strings</title>
    <summary>  MATLAB is a mathematical computing environment used by many engineers,
mathematicians, and students to process and understand their data. Important to
all data science is the managing of textual data. MATLAB supports two textual
data containers: (1) cell arrays of characters and (2) string arrays. This
research showcases the strengths of string arrays over cell arrays by
quantifying their performance, memory contiguity, syntax readability, interface
fluidity, and autocomplete capabilities. These results demonstrate that string
arrays often run 2x to 40x faster than cell arrays for common string
benchmarks, are optimized for data locality by reducing metadata overhead, and
offer a more expressive syntax due to their automatic data type conversions and
vectorized methods.
</summary>
    <author>
      <name>Travis Near</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 10 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.12567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.12567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01058v1</id>
    <updated>2021-12-02T08:39:58Z</updated>
    <published>2021-12-02T08:39:58Z</published>
    <title>A Foreground-Background queueing model with speed or capacity modulation</title>
    <summary>  The models studied in the steady state involve two queues which are served
either by a single server whose speed depends on the number of jobs present, or
by several parallel servers whose number may be controlled dynamically. Job
service times have a two-phase Coxian distribution and the second phase is
given lower priority than the first. The trade-offs between holding costs and
energy consumption costs are examined by means of a suitable cost functions.
Two different two-dimensional Markov process are solved exactly. The solutions
are used in several numerical experiments. Some counter-intuitive results are
observed.
</summary>
    <author>
      <name>Andrea Marin</name>
    </author>
    <author>
      <name>Isi Mitrani</name>
    </author>
    <link href="http://arxiv.org/abs/2112.01058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.11270v1</id>
    <updated>2021-12-21T14:50:25Z</updated>
    <published>2021-12-21T14:50:25Z</published>
    <title>Adding semantics to measurements: Ontology-guided, systematic
  performance analysis</title>
    <summary>  The design and operation of modern software systems exhibit a shift towards
virtualization, containerization and service-based orchestration. Performance
capacity engineering and resource utilization tuning become priority
requirements in such environments.
  Measurement-based performance evaluation is the cornerstone of capacity
engineering and designing for performance. Moreover, the increasing complexity
of systems necessitates rigorous performance analysis approaches. However,
empirical performance analysis lacks sophisticated model-based support similar
to the functional design of the system.
  The paper proposes an ontology-based approach for facilitating and guiding
the empirical evaluation throughout its various steps. Hyperledger Fabric
(HLF), an open-source blockchain platform by the Linux Foundation, is modelled
and evaluated as a pilot example of the approach, using the standard TPC-C
performance benchmark workload.
</summary>
    <author>
      <name>Attila Klenik</name>
    </author>
    <author>
      <name>András Pataricza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.11270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.11270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.1915v3</id>
    <updated>2007-06-03T06:57:27Z</updated>
    <published>2007-05-14T11:39:52Z</published>
    <title>A Technical Report On Grid Benchmarking using ATLAS V.O</title>
    <summary>  Grids include heterogeneous resources, which are based on different hardware
and software architectures or components. In correspondence with this diversity
of the infrastructure, the execution time of any single job, as well as the
total grid performance can both be affected substantially, which can be
demonstrated by measurements. Running a simple benchmarking suite can show this
heterogeneity and give us results about the differences over the grid sites.
</summary>
    <author>
      <name>John Kouvakis</name>
    </author>
    <author>
      <name>Fotis Georgatos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 35 figures, including charts and results of benchmarking
  over the grid, ATLAS V.O</arxiv:comment>
    <link href="http://arxiv.org/abs/0705.1915v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.1915v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.8191v2</id>
    <updated>2012-11-13T08:59:49Z</updated>
    <published>2012-10-30T22:45:46Z</published>
    <title>Performance Indicator for MIMO MMSE Receivers in the Presence of Channel
  Estimation Error</title>
    <summary>  We present the derivation of post-processing SNR for
Minimum-Mean-Squared-Error (MMSE) receivers with imperfect channel estimates,
and show that it is an accurate indicator of the error rate performance of MIMO
systems in the presence of channel estimation error. Simulation results show
the tightness of the analysis.
</summary>
    <author>
      <name>Eren Eraslan</name>
    </author>
    <author>
      <name>Babak Daneshrad</name>
    </author>
    <author>
      <name>Chung-Yu Lou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/WCL.2013.012513.120824</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/WCL.2013.012513.120824" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures. Submitted to IEEE Wireless Communications Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.8191v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.8191v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.4773v1</id>
    <updated>2013-06-20T07:28:39Z</updated>
    <published>2013-06-20T07:28:39Z</published>
    <title>Performance Bounds for Multiclass FIFO in Communication Networks: A
  Deterministic Case</title>
    <summary>  Multiclass FIFO is used in communication networks such as in input-queueing
routers/switches and in wireless networks. For the concern of providing service
guarantees in such networks, it is crucial to have analytical results, e.g.
bounds, on the performance of multi-class FIFO. Surprisingly, there are few
such results in the literature. This paper is devoted to filling the gap.
Specifically, a single hop deterministic case is studied, for which, delay and
backlog bounds are derived, in addition to guaranteed rate and service curve
characterizations that may be exploited to extend the analysis to network
cases.
</summary>
    <author>
      <name>Yuming Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/1306.4773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.4773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06382v1</id>
    <updated>2015-03-22T04:00:45Z</updated>
    <published>2015-03-22T04:00:45Z</published>
    <title>On the tradeoff of average delay, average service cost, and average
  utility for single server queues with monotone policies</title>
    <summary>  In this thesis, we study the optimal tradeoff of average delay, average
service cost, and average utility for single server queueing models, with and
without admission control. The continuous time and discrete time queueing
models that we consider are motivated by cross-layer models for noisy
point-to-point links, with random packet arrivals. We study the above tradeoff
problem for a class of admissible policies, which are monotone and stationary
and obtain an asymptotic characterization of the minimum average delay as a
function of the average service cost and average utility constraints.
</summary>
    <author>
      <name>Vineeth Bala Sukumaran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ph.D. thesis, Department of Electrical Communication Engineering,
  Indian Institute of Science</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.06382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.07693v2</id>
    <updated>2015-04-02T15:12:31Z</updated>
    <published>2015-03-26T11:38:29Z</published>
    <title>Communication Patterns in Mean Field Models for Wireless Sensor Networks</title>
    <summary>  Wireless sensor networks are usually composed of a large number of nodes, and
with the increasing processing power and power consumption efficiency they are
expected to run more complex protocols in the future. These pose problems in
the field of verification and performance evaluation of wireless networks. In
this paper, we tailor the mean-field theory as a modeling technique to analyze
their behavior. We apply this method to the slotted ALOHA protocol, and
establish results on the long term trends of the protocol within a very large
network, specially regarding the stability of ALOHA-type protocols.
</summary>
    <author>
      <name>Mahmoud Talebi</name>
    </author>
    <author>
      <name>Jan Friso Groote</name>
    </author>
    <author>
      <name>Jean-Paul Linnartz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, in LNCS format, Submitted to QEST'15</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.07693v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.07693v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02867v2</id>
    <updated>2018-07-28T06:51:27Z</updated>
    <published>2018-05-08T07:34:17Z</published>
    <title>Online normalizer calculation for softmax</title>
    <summary>  The Softmax function is ubiquitous in machine learning, multiple previous
works suggested faster alternatives for it. In this paper we propose a way to
compute classical Softmax with fewer memory accesses and hypothesize that this
reduction in memory accesses should improve Softmax performance on actual
hardware. The benchmarks confirm this hypothesis: Softmax accelerates by up to
1.3x and Softmax+TopK combined and fused by up to 5x.
</summary>
    <author>
      <name>Maxim Milakov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NVIDIA</arxiv:affiliation>
    </author>
    <author>
      <name>Natalia Gimelshein</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NVIDIA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1) Added link to the benchmark code, 2) Benchmarked Safe Softmax +
  Top-K fused and attributed part of 5x explicitly to fusion in sections 5.2
  and 6, 3) Stylistic changes, 4) Minor clarifications</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.02867v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02867v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09642v1</id>
    <updated>2018-05-23T14:46:46Z</updated>
    <published>2018-05-23T14:46:46Z</published>
    <title>An infinite-server queueing model MMAPkGk in semi-Markov random
  environment with marked MAP arrival and subject to catastrophes</title>
    <summary>  In the present paper the infinite-server MMAPkGk queueing model with random
resource vector of customers, marked MAP arrival and semi-Markov (SM) arrival
of catastrophes is considered. The joint generating functions (PGF) of
transient and stationary distributions of number of busy servers and numbers of
different types served customers, as well as Laplace transformations (LT) of
joint distributions of total accumulated resources in the model at moment and
total accumulated resources of served customers during time interval are found.
The basic differential and renewal equations for transient and stationary PGF
of queue sizes of customers are found.
</summary>
    <author>
      <name>K. Kerobyan</name>
    </author>
    <author>
      <name>R. Covington</name>
    </author>
    <author>
      <name>R. Kerobyan</name>
    </author>
    <author>
      <name>K. Enakoutsa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.10663v4</id>
    <updated>2020-03-30T20:32:53Z</updated>
    <published>2018-09-27T17:45:07Z</published>
    <title>Is Your Load Generator Launching Web Requests in Bunches?</title>
    <summary>  One problem with load test quality, almost always overlooked, is the
potential for the load generator's user thread pool to sync up and dispatch
queries in bunches rather than independently from each other like real users
initiate their requests. A spiky launch pattern misrepresents workload flow as
well as yields erroneous application response time statistics. This paper
describes what a real user request timing pattern looks like, illustrates how
to identify it in the load generation environment, and exercises a free
downloadable tool which measures how well the load generator is mimicking the
timing pattern of real web user requests.
</summary>
    <author>
      <name>James F Brady</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">One link was updated</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.10663v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.10663v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.07778v1</id>
    <updated>2018-12-19T06:51:24Z</updated>
    <published>2018-12-19T06:51:24Z</published>
    <title>AdaptMemBench: Application-Specific MemorySubsystem Benchmarking</title>
    <summary>  Optimizing scientific applications to take full advan-tage of modern memory
subsystems is a continual challenge forapplication and compiler developers.
Factors beyond working setsize affect performance. A benchmark framework that
exploresthe performance in an application-specific manner is essential
tocharacterize memory performance and at the same time informmemory-efficient
coding practices. We present AdaptMemBench,a configurable benchmark framework
that measures achievedmemory performance by emulating application-specific
accesspatterns with a set of kernel-independent driver templates. Thisframework
can explore the performance characteristics of a widerange of access patterns
and can be used as a testbed for potentialoptimizations due to the flexibility
of polyhedral code generation.We demonstrate the effectiveness of AdaptMemBench
with casestudies on commonly used computational kernels such as triadand
multidimensional stencil patterns.
</summary>
    <author>
      <name>Mahesh Lakshminarasimhan</name>
    </author>
    <author>
      <name>Catherine Olschanowsky</name>
    </author>
    <link href="http://arxiv.org/abs/1812.07778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.07778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.09560v1</id>
    <updated>2019-06-23T06:53:15Z</updated>
    <published>2019-06-23T06:53:15Z</published>
    <title>Retrial Queueing Models: A Survey on Theory and Applications</title>
    <summary>  Retrial phenomenon naturally arises in various systems such as call centers,
cellular networks and random access protocols in local area networks. This
paper gives a comprehensive survey on theory and applications of retrial queues
in these systems. We investigate the state of the art of the theoretical
researches including exact solutions, stability, asymptotic analyses and
multidimensional models. We present an overview on retrial models arising from
real world applications. Some open problems and promising research directions
are also discussed.
</summary>
    <author>
      <name>Tuan Phung-Duc</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages. In: Stochastic Operations Research in Business and Industry
  (eds. by Tadashi Dohi, Katsunori Ano and Shoji Kasahara)</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.09560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.09560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.05811v2</id>
    <updated>2020-01-17T08:37:19Z</updated>
    <published>2020-01-16T14:17:28Z</published>
    <title>Duet Benchmarking: Improving Measurement Accuracy in the Cloud</title>
    <summary>  We investigate the duet measurement procedure, which helps improve the
accuracy of performance comparison experiments conducted on shared machines by
executing the measured artifacts in parallel and evaluating their relative
performance together, rather than individually. Specifically, we analyze the
behavior of the procedure in multiple cloud environments and use experimental
evidence to answer multiple research questions concerning the assumption
underlying the procedure. We demonstrate improvements in accuracy ranging from
2.3x to 12.5x (5.03x on average) for the tested ScalaBench (and DaCapo)
workloads, and from 23.8x to 82.4x (37.4x on average) for the SPEC CPU 2017
workloads.
</summary>
    <author>
      <name>Lubomír Bulej</name>
    </author>
    <author>
      <name>Vojtěch Horký</name>
    </author>
    <author>
      <name>Petr Tůma</name>
    </author>
    <author>
      <name>François Farquet</name>
    </author>
    <author>
      <name>Aleksandar Prokopec</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3358960.3379132</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3358960.3379132" rel="related"/>
    <link href="http://arxiv.org/abs/2001.05811v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05811v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.06823v1</id>
    <updated>2020-08-16T02:50:19Z</updated>
    <published>2020-08-16T02:50:19Z</published>
    <title>Erlang Redux: An Ansatz Method for Solving the M/M/m Queue</title>
    <summary>  This exposition presents a novel approach to solving an M/M/m queue for the
waiting time and the residence time. The motivation comes from an algebraic
solution for the residence time of the M/M/1 queue. The key idea is the
introduction of an ansatz transformation, defined in terms of the Erlang B
function, that avoids the more opaque derivation based on applied probability
theory. The only prerequisite is an elementary knowledge of the Poisson
distribution, which is already necessary for understanding the M/M/1 queue. The
approach described here supersedes our earlier approximate morphing
transformation.
</summary>
    <author>
      <name>Neil J. Gunther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures, 2 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.06823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.06823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.2; C.4; C.2.1; C.2.4; C.5.5; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.01659v1</id>
    <updated>2020-08-27T22:09:18Z</updated>
    <published>2020-08-27T22:09:18Z</published>
    <title>Analysis of an M/G/1 system for the optimization of the RTG performances
  in the delivery of containers in Abidjan Terminal</title>
    <summary>  In front of the major challenges to increase its productivity while
satisfying its customer, it is today important to establish in advance the
operational performances of the RTG Abidjan Terminal. In this article, by using
an M/G/1 retrial queue system, we obtained the average number of parked
delivery trucks and as well as their waiting time. Finally, we used Matlab to
represent them graphically then analyze the RTG performances according to the
traffic rate.
</summary>
    <author>
      <name>Bakary Kone</name>
    </author>
    <author>
      <name>Salimata Gueye Diagne</name>
    </author>
    <author>
      <name>Dethie Dione</name>
    </author>
    <author>
      <name>Coumba Diallo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJAAMM (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2009.01659v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.01659v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="15B52" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.00754v1</id>
    <updated>2020-10-02T02:39:07Z</updated>
    <published>2020-10-02T02:39:07Z</published>
    <title>P = FS: Parallel is Just Fast Serial</title>
    <summary>  We prove that parallel processing with homogeneous processors is logically
equivalent to fast serial processing. The reverse proposition can also be used
to identify obscure opportunities for applying parallelism. To our knowledge,
this theorem has not been previously reported in the queueing theory
literature. A plausible explanation is offered for why this might be. The basic
homogeneous theorem is also extended to optimizing the latency of heterogenous
parallel arrays.
</summary>
    <author>
      <name>Neil J. Gunther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, 1 Table</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.00754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.00754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.2; C.4; C.2.1; C.2.4; C.5.5; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.09263v1</id>
    <updated>2020-10-19T07:13:39Z</updated>
    <published>2020-10-19T07:13:39Z</published>
    <title>Trade-off between accuracy and tractability of network calculus in FIFO
  networks</title>
    <summary>  Computing accurate deterministic performance bounds is a strong need for
communication technologies having strong requirements on latency and
reliability. Beyond new scheduling protocols such as TSN, the FIFO policy
remains at work within each class of communication.
  In this paper, we focus on computing deterministic performance bounds in FIFO
networks in the network calculus framework. We propose a new algorithm based on
linear programming that presents a trade-off between accuracy and tractability.
This algorithm is first presented for tree networks. In a second time, we
generalize our approach and present a linear program for computing performance
bounds for arbitrary topologies, including cyclic dependencies. Finally, we
provide numerical results, both of toy examples and real topologies, to assess
the interest of our approach.
</summary>
    <author>
      <name>Anne Bouillard</name>
    </author>
    <link href="http://arxiv.org/abs/2010.09263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.07479v1</id>
    <updated>2020-11-15T08:32:29Z</updated>
    <published>2020-11-15T08:32:29Z</published>
    <title>Performance Analysis of an Interference-Limited RIS-Aided Network</title>
    <summary>  In this work, the performance of reconfigurable intelligent surface
(RIS)-aided communication systems corrupted by the co-channel interference
(CCI) at the destination is investigated. Assuming Rayleigh fading and
equal-power CCI, we present the analysis for the outage probability (OP),
average bit error rate (BER), and ergodic capacity. In addition, an asymptotic
outage analysis is carried in order to obtain further insights. Our analysis
shows that the number of reflecting elements as well as the number of
interferers have a great impact on the overall system performance.
</summary>
    <author>
      <name>Liang Yang</name>
    </author>
    <author>
      <name>Yin Yang</name>
    </author>
    <author>
      <name>Daniel Benevides da Costa</name>
    </author>
    <author>
      <name>Imene Trigui</name>
    </author>
    <link href="http://arxiv.org/abs/2011.07479v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.07479v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.09951v1</id>
    <updated>2020-11-19T16:51:39Z</updated>
    <published>2020-11-19T16:51:39Z</published>
    <title>A Bounded Multi-Vacation Queue Model for Multi-stage Sleep Control 5G
  Base station</title>
    <summary>  Modelling and control of energy consumption is an important problem in
telecommunication systems.To model such systems, this paper publishes a bounded
multi-vacation queue model. The energy consumption predicted by the model shows
an average error rate of 0.0177 and the delay predicted by the model shows an
average error rate of 0.0655 over 99 test instances.Subsequently, an
optimization algorithm is proposed to minimize the energy consumption while not
violate the delay bound. Furthermore, given current state of art 5G base
station system configuration, numerical results shows that with the increase of
traffic load, energy saving rate becomes less.
</summary>
    <author>
      <name>Jie Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2011.09951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.09951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.10886v1</id>
    <updated>2020-11-21T22:39:42Z</updated>
    <published>2020-11-21T22:39:42Z</published>
    <title>Optimal Transaction Queue Waiting in Blockchain Mining</title>
    <summary>  Blockchain systems are being used in a wide range of application domains.
They can support trusted transactions in time critical applications. In this
paper, we study how miners should pick up transactions from a transaction pool
so as to minimize the average waiting time per transaction. We derive an
expression for the average transaction waiting time of the proposed mining
scheme and determine the optimum decision rule. Numerical results show that the
average waiting time per transaction can be reduced by about 10% compared to
the traditional no-wait scheme in which miners immediately start the next
mining round using all transactions waiting in the pool.
</summary>
    <author>
      <name>Gholamreza Ramezan</name>
    </author>
    <author>
      <name>Cyril Leung</name>
    </author>
    <author>
      <name>Chunyan Miao</name>
    </author>
    <link href="http://arxiv.org/abs/2011.10886v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.10886v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.10045v1</id>
    <updated>2022-02-21T08:28:55Z</updated>
    <published>2022-02-21T08:28:55Z</published>
    <title>Analysis of Two-Station Polling Queues with Setups using Continuous Time
  Markov Chain</title>
    <summary>  The paper analyzes the performance of tandem network of polling queue with
setups. For a system with two-products and two-stations, we propose a new
approach based on a partially-collapsible state-space characterization to
reduce state-space complexity. In this approach, the size of the state-space is
varied depending on the information needed to determine buffer levels and
waiting times. We evaluate system performance under different system setting
and comment on the numerical accuracy of the approach as well as provide
managerial insights. Numerical results show that approach yields reliable
estimates of the performance measures. We also show how product and station
asymmetry significantly affect the systems performance.
</summary>
    <author>
      <name>Ravi Suman</name>
    </author>
    <author>
      <name>Ananth Krishnamurthy</name>
    </author>
    <link href="http://arxiv.org/abs/2202.10045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.10045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.14286v2</id>
    <updated>2022-06-30T10:48:01Z</updated>
    <published>2022-06-28T20:53:25Z</published>
    <title>TPU-KNN: K Nearest Neighbor Search at Peak FLOP/s</title>
    <summary>  This paper presents a novel nearest neighbor search algorithm achieving TPU
(Google Tensor Processing Unit) peak performance, outperforming
state-of-the-art GPU algorithms with similar level of recall. The design of the
proposed algorithm is motivated by an accurate accelerator performance model
that takes into account both the memory and instruction bottlenecks. Our
algorithm comes with an analytical guarantee of recall in expectation and does
not require maintaining sophisticated index data structure or tuning, making it
suitable for applications with frequent updates. Our work is available in the
open-source package of Jax and Tensorflow on TPU.
</summary>
    <author>
      <name>Felix Chern</name>
    </author>
    <author>
      <name>Blake Hechtman</name>
    </author>
    <author>
      <name>Andy Davis</name>
    </author>
    <author>
      <name>Ruiqi Guo</name>
    </author>
    <author>
      <name>David Majnemer</name>
    </author>
    <author>
      <name>Sanjiv Kumar</name>
    </author>
    <link href="http://arxiv.org/abs/2206.14286v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.14286v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.14505v1</id>
    <updated>2022-06-29T09:54:49Z</updated>
    <published>2022-06-29T09:54:49Z</published>
    <title>Rate Lifting for Stochastic Process Algebra: Exploiting Structural
  Properties</title>
    <summary>  This report presents an algorithm for determining the unknown rates in the
sequential processes of a Stochastic Process Algebra model, provided that the
rates in the combined flat model are given. Such a rate lifting is useful for
model reengineering and model repair. Technically, the algorithm works by
solving systems of nonlinear equations and, if necessary, adjusting the model`s
synchronisation structure without changing its transition system. This report
contains the complete pseudo-code of the algorithm. The approach taken by the
algorithm exploits some structural properties of Stochastic Process Algebra
systems, which are formulated here for the first time and could be very
beneficial also in other contexts.
</summary>
    <author>
      <name>Markus Siegle</name>
    </author>
    <author>
      <name>Amin Soltanieh</name>
    </author>
    <link href="http://arxiv.org/abs/2206.14505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.14505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.03022v1</id>
    <updated>2022-08-05T07:24:58Z</updated>
    <published>2022-08-05T07:24:58Z</published>
    <title>A Probabilistic Bound for Peak Age of Information Guarantee</title>
    <summary>  This paper considers the distribution of a general peak age of information
(AoI) model and develops a general analysis approach for probabilistic
performance guarantee from the time-domain perspective. Firstly, a general
relationship between the peak AoI and the inter-arrival and service times of
packets is revealed. With the help of martingale theory, a probabilistic bound
on the peak AoI is then derived for the general case of endogenous
independently and identically distributed increments in information generation
and transmission processes. Thereafter, the application of the obtained bound
is illustrated with the M/M/1 and D/M/1 queuing models. The validity of the
proposed bound is finally examined with numerical results.
</summary>
    <author>
      <name>Ailing Zhong</name>
    </author>
    <author>
      <name>Zhidu Li</name>
    </author>
    <author>
      <name>Tong Tang</name>
    </author>
    <author>
      <name>Dapeng Wu</name>
    </author>
    <author>
      <name>Ruyan Wang</name>
    </author>
    <author>
      <name>Yuming Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/2208.03022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.03022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.04220v1</id>
    <updated>2022-09-09T10:12:28Z</updated>
    <published>2022-09-09T10:12:28Z</published>
    <title>A Software Package for Queueing Networks and Markov Chains analysis</title>
    <summary>  Queueing networks and Markov chains are widely used for conducting
performance and reliability studies. In this paper we describe the queueing
package, a free software package for queueing networks and Markov chain
analysis for GNU Octave. The queueing package provides implementations of
numerical algorithms for computing transient and steady-state performance
measures of discrete and continuous Markov chains, and for steady-state
analysis of single-station queueing systems and queueing networks. We
illustrate the design principles of the queueing package, describe its most
salient features and provide some usage examples.
</summary>
    <author>
      <name>Moreno Marzolla</name>
    </author>
    <link href="http://arxiv.org/abs/2209.04220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.04220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="58-04" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.10430v1</id>
    <updated>2022-09-21T15:25:05Z</updated>
    <published>2022-09-21T15:25:05Z</published>
    <title>Real-Time Guarantees in Routerless Networks-on-Chip</title>
    <summary>  This paper considers the use of routerless networks-on-chip as an alternative
on-chip interconnect for multiprocessor systems requiring hard real-time
guarantees for inter-processor communication. It presents a novel analytical
framework that can provide latency upper bounds to real-time packet flows sent
over routerless networks-on-chip, and it uses that framework to evaluate the
ability of such networks to provide real-time guarantees. Extensive comparative
analysis is provided, considering different architectures for routerless
networks and a state-of-the-art wormhole network based on priority-preemptive
routers as a baseline.
</summary>
    <author>
      <name>Leandro Soares Indrusiak</name>
    </author>
    <author>
      <name>Alan Burns</name>
    </author>
    <link href="http://arxiv.org/abs/2209.10430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.10430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.03724v2</id>
    <updated>2022-11-07T08:53:47Z</updated>
    <published>2022-10-07T17:51:02Z</published>
    <title>PMT: Power Measurement Toolkit</title>
    <summary>  Efficient use of energy is essential for today's supercomputing systems, as
energy cost is generally a major component of their operational cost. Research
into "green computing" is needed to reduce the environmental impact of running
these systems. As such, several scientific communities are evaluating the
trade-off between time-to-solution and energy-to-solution. While the runtime of
an application is typically easy to measure, power consumption is not.
Therefore, we present the Power Measurement Toolkit (PMT), a high-level
software library capable of collecting power consumption measurements on
various hardware. The library provides a standard interface to easily measure
the energy use of devices such as CPUs and GPUs in critical application
sections.
</summary>
    <author>
      <name>Stefano Corda</name>
    </author>
    <author>
      <name>Bram Veenboer</name>
    </author>
    <author>
      <name>Emma Tolley</name>
    </author>
    <link href="http://arxiv.org/abs/2210.03724v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.03724v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.13757v2</id>
    <updated>2022-11-23T03:00:39Z</updated>
    <published>2022-10-25T03:55:14Z</published>
    <title>Workload Similarity Analysis using Machine Learning Techniques</title>
    <summary>  Finding the similarity between two workload behaviors is helpful in 1.
creating proxy workloads 2. characterizing an unknown workload's behavior by
matching its behavior against known workloads. In this article, we propose a
method to measure the similarity between two workloads using machine
learning-based analysis of the performance telemetry data collected for the
execution runs of the two workloads. We also demonstrate the accuracy of the
technique by measuring the similarity between a variety of know benchmark
workloads.
</summary>
    <author>
      <name>Ashish Ledalla</name>
    </author>
    <author>
      <name>Vineet Singh</name>
    </author>
    <author>
      <name>Deepak Mishra</name>
    </author>
    <link href="http://arxiv.org/abs/2210.13757v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.13757v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.00367v1</id>
    <updated>2022-11-01T10:29:59Z</updated>
    <published>2022-11-01T10:29:59Z</published>
    <title>Towards Maximizing Nonlinear Delay Sensitive Rewards in Queuing Systems</title>
    <summary>  We consider maximizing the long-term average reward in a single server queue,
where the reward obtained for a job is a non-increasing function of its sojourn
time. The motivation behind this work comes from multiple applications,
including quantum information processing and multimedia streaming. We introduce
a new service discipline, shortest predicted sojourn time (SPST), which, in
simulations, performs better than well-known disciplines. We also present some
limited analytical guarantees for this highly intricate problem.
</summary>
    <author>
      <name>Sushmitha Shree S</name>
    </author>
    <author>
      <name>Avijit Mandal</name>
    </author>
    <author>
      <name>Avhishek Chatterjee</name>
    </author>
    <author>
      <name>Krishna Jagannathan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.00367v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.00367v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.17746v1</id>
    <updated>2023-10-26T19:37:46Z</updated>
    <published>2023-10-26T19:37:46Z</published>
    <title>Memory Efficient Multithreaded Incremental Segmented Sieve Algorithm</title>
    <summary>  Prime numbers are fundamental in number theory and play a significant role in
various areas, from pure mathematics to practical applications, including
cryptography. In this contribution, we introduce a multithreaded implementation
of the Segmented Sieve algorithm. In our implementation, instead of handling
large prime ranges in one iteration, the sieving process is broken down
incrementally, which theoretically eliminates the challenges of working with
large numbers, and can reduce memory usage, providing overall more efficient
multi-core utilization over extended computations.
</summary>
    <author>
      <name>Evan Ning</name>
    </author>
    <author>
      <name>David Kaeli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.17746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.17746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.19816v1</id>
    <updated>2023-10-19T17:09:06Z</updated>
    <published>2023-10-19T17:09:06Z</published>
    <title>Benchmarking GPUs on SVBRDF Extractor Model</title>
    <summary>  With the maturity of deep learning, its use is emerging in every field. Also,
as different types of GPUs are becoming more available in the markets, it
creates a difficult decision for users. How can users select GPUs to achieve
optimal performance for a specific task? Analysis of GPU architecture is well
studied, but existing works that benchmark GPUs do not study tasks for networks
with significantly larger input. In this work, we tried to differentiate the
performance of different GPUs on neural network models that operate on bigger
input images (256x256).
</summary>
    <author>
      <name>Narayan Kandel</name>
    </author>
    <author>
      <name>Melanie Lambert</name>
    </author>
    <link href="http://arxiv.org/abs/2310.19816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.19816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.17704v2</id>
    <updated>2024-02-14T18:20:49Z</updated>
    <published>2023-11-29T15:09:24Z</published>
    <title>An Efficient Algorithm for Unbalanced 1D Transportation</title>
    <summary>  Optimal transport (OT) and unbalanced optimal transport (UOT) are central in
many machine learning, statistics and engineering applications. 1D OT is easily
solved, with complexity O(n log n), but no efficient algorithm was known for 1D
UOT. We present a new approach that leverages the successive shortest path
algorithm for the corresponding network flow problem. By employing a suitable
representation, we bundle together multiple steps that do not change the cost
of the shortest path. We prove that our algorithm solves 1D UOT in O(n log n),
closing the gap.
</summary>
    <author>
      <name>Gabriel Gouvine</name>
    </author>
    <link href="http://arxiv.org/abs/2311.17704v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.17704v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.1; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.08879v1</id>
    <updated>2024-06-13T07:31:36Z</updated>
    <published>2024-06-13T07:31:36Z</published>
    <title>Modeling Common Cause Failure in Dynamic PRA</title>
    <summary>  In this paper we propose a dynamic model of Common Cause Failures (CCF) that
allows to generate common cause events in time. The proposed model is a
generalization of Binomial Failure Rate Model (Atwood model) that can generate
staggered failures of multiple components due to a common cause. We implement
the model using statechart formalism, a similar implementation can be adopted
in other modeling languages like Petri Nets or Hybrid Stochastic Automata. The
presented model was integrated in a Dynamic PRA study.
</summary>
    <author>
      <name>Claudia Picoco</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">EDF R\&amp;D</arxiv:affiliation>
    </author>
    <author>
      <name>Valentin Rychkov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">EDF R\&amp;D</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2406.08879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.08879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.09441v1</id>
    <updated>2024-06-11T23:14:19Z</updated>
    <published>2024-06-11T23:14:19Z</published>
    <title>Comment on paper: Position: Rethinking Post-Hoc Search-Based Neural
  Approaches for Solving Large-Scale Traveling Salesman Problems</title>
    <summary>  We identify two major issues in the SoftDist paper (Xia et al.): (1) the
failure to run all steps of different baselines on the same hardware
environment, and (2) the use of inconsistent time measurements when comparing
to other baselines. These issues lead to flawed conclusions. When all steps are
executed in the same hardware environment, the primary claim made in SoftDist
is no longer supported.
</summary>
    <author>
      <name>Yimeng Min</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">comment on arXiv:2406.03503, 4 pages, 1 figure and 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.09441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.09441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.16607v1</id>
    <updated>2024-08-29T15:17:34Z</updated>
    <published>2024-08-29T15:17:34Z</published>
    <title>ppOpen-AT: A Directive-base Auto-tuning Language</title>
    <summary>  ppOpen-AT is a domain-specific language designed to ease the workload for
developers creating libraries with auto-tuning (AT) capabilities. It consists
of a set of directives that allow for the automatic generation of code
necessary for AT by placing annotations in the source program. This approach
significantly reduces the effort required by numerical library developers. This
technical report details the implementation of the AT software and its extended
functions, and provides an explanation of the internal specifications of
ppOpen-AT.
</summary>
    <author>
      <name>Takahiro Katagiri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Source code is available at:
  https://github.com/Post-Peta-Crest/ppOpenHPC/tree/AT</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.16607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.16607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.19156v1</id>
    <updated>2024-09-27T21:44:37Z</updated>
    <published>2024-09-27T21:44:37Z</published>
    <title>ZERNIPAX: A Fast and Accurate Zernike Polynomial Calculator in Python</title>
    <summary>  Zernike Polynomials serve as an orthogonal basis on the unit disc, and have
been proven to be effective in optics simulations, astrophysics, and more
recently in plasma simulations. Unlike Bessel functions, they maintain finite
values at the disc center, ensuring inherent analyticity along the axis. We
developed ZERNIPAX, an open-source Python package capable of utilizing
CPU/GPUs, leveraging Google's JAX package and available on
https://github.com/PlasmaControl/FastZernike.git as well as PyPI. Our
implementation of the recursion relation between Jacobi polynomials
significantly improves computation time compared to alternative methods by use
of parallel computing while still preserving accuracy for mode numbers n&gt;100.
</summary>
    <author>
      <name>Yigit Gunsur Elmacioglu</name>
    </author>
    <author>
      <name>Rory Conlin</name>
    </author>
    <author>
      <name>Daniel W. Dudt</name>
    </author>
    <author>
      <name>Dario Panici</name>
    </author>
    <author>
      <name>Egemen Kolemen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.amc.2025.129534</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.amc.2025.129534" rel="related"/>
    <link href="http://arxiv.org/abs/2409.19156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.19156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.15813v1</id>
    <updated>2024-10-21T09:28:08Z</updated>
    <published>2024-10-21T09:28:08Z</published>
    <title>Industry 4.0 Connectors -- A Performance Experiment with Modbus/TCP</title>
    <summary>  For Industry 4.0 applications, communication protocols and data formats even
for legacy devices are fundamental. In this paper, we focus on the Modbus/TCP
protocol, which is, e.g., used in energy metering. Allowing Industry 4.0
applications to include data from such protocols without need for programming
would increase flexibility and, in turn, improve development efficiency. As one
particular approach, we discuss the automated generation of Modbus/TCP
connectors for our Open Source oktoflow platform and compare the performance of
handcrafted as well as generated connectors in different settings, including
industrial energy metering devices.
</summary>
    <author>
      <name>Christian Nikolajew</name>
    </author>
    <author>
      <name>Holger Eichelberger</name>
    </author>
    <link href="http://arxiv.org/abs/2410.15813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.15813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.19730v1</id>
    <updated>2024-11-29T14:23:25Z</updated>
    <published>2024-11-29T14:23:25Z</published>
    <title>Ten Ways in which Virtual Reality Differs from Video Streaming</title>
    <summary>  Virtual Reality (VR) applications have a number of unique characteristics
that set them apart from traditional video streaming. These characteristics
have major implications on the design of VR rendering, adaptation, prefetching,
caching, and transport mechanisms. This paper contrasts VR to video streaming,
stored 2D video streaming in particular, and discusses how to rethink system
and network support for VR.
</summary>
    <author>
      <name>Gustavo de Veciana</name>
    </author>
    <author>
      <name>Sonia Fahmy</name>
    </author>
    <author>
      <name>George Kesidis</name>
    </author>
    <author>
      <name>Voicu Popescu</name>
    </author>
    <link href="http://arxiv.org/abs/2411.19730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.19730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.07846v1</id>
    <updated>2025-02-11T09:51:25Z</updated>
    <published>2025-02-11T09:51:25Z</published>
    <title>Memory Analysis on the Training Course of DeepSeek Models</title>
    <summary>  We present a theoretical analysis of GPU memory consumption during the
training of DeepSeek models such as DeepSeek-v2 and DeepSeek-v3. Our primary
objective is to clarify the device-level memory requirements associated with
various distributed training configurations. Specifically, we examine critical
factors influencing memory usage, including micro-batch size, activation
recomputation policies, 3D parallelism, and ZeRO optimizations. It is important
to emphasize that the training policies discussed in this report are not
representative of DeepSeek's official configurations. Instead, they are
explored to provide a deeper understanding of memory dynamics in training of
large-scale mixture-of-experts model.
</summary>
    <author>
      <name>Ping Zhang</name>
    </author>
    <author>
      <name>Lei Su</name>
    </author>
    <link href="http://arxiv.org/abs/2502.07846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.07846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.09217v1</id>
    <updated>2025-02-13T11:49:33Z</updated>
    <published>2025-02-13T11:49:33Z</published>
    <title>Modular Stochastic Rewritable Petri Nets</title>
    <summary>  Petri Nets (PN) are widely used for modeling concurrent and distributed
systems, but face challenges in modeling adaptive systems. To address this, we
have formalized "rewritable" PT nets (RwPT) using Maude, a declarative language
with sound rewriting logic semantics. Recently, we introduced a modular
approach that utilizes algebraic operators to construct large RwPT models. This
technique employs composite node labeling to outline symmetries in hierarchical
organization, preserved through net rewrites. Once stochastic parameters are
added to the formalism, we present an automated process to derive a lumped CTMC
from the quotient graph generated by an RwPT.
</summary>
    <author>
      <name>Lorenzo Capra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.416.11</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.416.11" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings ICLP 2024, arXiv:2502.08453</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 416, 2025, pp. 128-134</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2502.09217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.09217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.12592v1</id>
    <updated>2025-02-18T07:06:39Z</updated>
    <published>2025-02-18T07:06:39Z</published>
    <title>Efficient Hybrid Amplitude-Phase Quantization for Multi-Antenna Relay
  System</title>
    <summary>  This letter explores relay quantization in multi-antenna quantize-forward
(QF) relay systems. Existing methods, such as uniform phase quantization (U-PQ)
and uniform amplitude-phase quantization (U-APQ), suffer from performance
saturation and high memory demands. To overcome these limitations, we propose
hybrid amplitude-phase quantization (H-APQ), which adaptively quantizes
received signal amplitudes based on their relative magnitudes while applying
uniform quantization to individual phases. H-APQ significantly reduces memory
consumption at the relay while maintaining strong overall performance, offering
an efficient solution for multiple-input multiple-output (MIMO) QF relay
systems.
</summary>
    <author>
      <name>Changdae Kim</name>
    </author>
    <author>
      <name>Xianglan Jin</name>
    </author>
    <link href="http://arxiv.org/abs/2502.12592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.12592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.01348v1</id>
    <updated>2025-03-03T09:38:20Z</updated>
    <published>2025-03-03T09:38:20Z</published>
    <title>Performance Optimization of 3D Stencil Computation on ARM Scalable
  Vector Extension</title>
    <summary>  Stencil computation is essential in high-performance computing, especially
for large-scale tasks like liquid simulation and weather forecasting.
Optimizing its performance can reduce both energy consumption and computation
time, which is critical in disaster prediction. This paper explores
optimization techniques for 7-point 3D stencil computation on ARM's Scalable
Vector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.
We evaluate software optimizations such as vectorization and tiling, as well as
hardware adjustments in ARM SVE vector lengths and cache configurations. The
study also examines performance, power consumption, and chip area trade-offs to
identify optimal configurations for ARM-based systems.
</summary>
    <author>
      <name>Hongguang Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2503.01348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.01348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.09650v1</id>
    <updated>2025-03-12T09:04:59Z</updated>
    <published>2025-03-12T09:04:59Z</published>
    <title>A Review on Proprietary Accelerators for Large Language Models</title>
    <summary>  With the advancement of Large Language Models (LLMs), the importance of
accelerators that efficiently process LLM computations has been increasing.
This paper discusses the necessity of LLM accelerators and provides a
comprehensive analysis of the hardware and software characteristics of the main
commercial LLM accelerators. Based on this analysis, we propose considerations
for the development of next-generation LLM accelerators and suggest future
research directions.
</summary>
    <author>
      <name>Sihyeong Park</name>
    </author>
    <author>
      <name>Jemin Lee</name>
    </author>
    <author>
      <name>Byung-Soo Kim</name>
    </author>
    <author>
      <name>Seokhun Jeon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, accepted in AICompS 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.09650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.09650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.04754v1</id>
    <updated>2025-05-07T19:36:42Z</updated>
    <published>2025-05-07T19:36:42Z</published>
    <title>Multiserver-job Response Time under Multilevel Scaling</title>
    <summary>  We study the multiserver-job setting in the load-focused multilevel scaling
limit, where system load approaches capacity much faster than the growth of the
number of servers $n$.
  We specifically consider the ``1 and $n$'' system, where each job requires
either one server or all $n$ servers. Within the multilevel scaling limit, we
examine three regimes: load dominated by $n$-server jobs, 1-server jobs, or
balanced. In each regime, we characterize the asymptotic growth rate of the
boundary of the stability region and the scaled mean queue length.
  We numerically verify our asymptotic results against exact formulae.
</summary>
    <author>
      <name>Isaac Grosof</name>
    </author>
    <author>
      <name>Hayriye Ayhan</name>
    </author>
    <link href="http://arxiv.org/abs/2505.04754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.04754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.14294v1</id>
    <updated>2025-05-20T12:43:30Z</updated>
    <published>2025-05-20T12:43:30Z</published>
    <title>Heterogeneous Memory Pool Tuning</title>
    <summary>  We present a lightweight tool for the analysis and tuning of application data
placement in systems with heterogeneous memory pools. The tool allows
non-intrusively identifying, analyzing, and controlling the placement of
individual allocations of the application. We use the tool to analyze a set of
benchmarks running on the Intel Sapphire Rapids platform with both HBM and DDR
memory. The paper also contains an analysis of the performance of both memory
subsystems in terms of read/write bandwidth and latency. The key part of the
analysis is to focus on performance if both subsystems are used together. We
show that only about 60% to 75% of the data must be placed in HBM memory to
achieve 90% of the potential performance of the platform on those benchmarks.
</summary>
    <author>
      <name>Filip Vaverka</name>
    </author>
    <author>
      <name>Ondrej Vysocky</name>
    </author>
    <author>
      <name>Lubomir Riha</name>
    </author>
    <link href="http://arxiv.org/abs/2505.14294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.14294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.16095v1</id>
    <updated>2025-05-22T00:34:41Z</updated>
    <published>2025-05-22T00:34:41Z</published>
    <title>Towards Stream-Based Monitoring for EVM Networks</title>
    <summary>  We believe that leveraging real-time blockchain operational data is of
particular interest in the context of the current rapid expansion of rollup
networks in the Ethereum ecosystem. Given the compatible but also competing
ground that rollups offer for applications, stream-based monitoring can be of
use both to developers and to EVM networks governance. In this paper, we
discuss this perspective and propose a basic monitoring pipeline.
</summary>
    <author>
      <name>Emanuel Onica</name>
    </author>
    <author>
      <name>Claudiu-Nicu Bărbieru</name>
    </author>
    <author>
      <name>Andrei Arusoaie</name>
    </author>
    <author>
      <name>Oana-Otilia Captarencu</name>
    </author>
    <author>
      <name>Ciprian Amariei</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3701717.3733230</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3701717.3733230" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for the 19th ACM International Conference on Distributed and
  Event-based Systems, DEBS 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.16095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.16095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.1090v2</id>
    <updated>2012-02-07T06:29:58Z</updated>
    <published>2012-01-05T09:26:40Z</published>
    <title>Distributed Multiuser Sequential Channel Sensing Schemes in Multichannel
  Cognitive Radio Networks</title>
    <summary>  This paper has been withdrawn by the author due to a crucial problem
associated with Figs. 2 and 3.
</summary>
    <author>
      <name>Hossein Shokri-Ghadikolaei</name>
    </author>
    <author>
      <name>Fatemeh Sheikholeslami</name>
    </author>
    <author>
      <name>Masoumeh Nasiri-Kenari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.1090v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.1090v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.11607v1</id>
    <updated>2021-09-24T14:59:32Z</updated>
    <published>2021-09-24T14:59:32Z</published>
    <title>Proceedings of the 8th OMNeT++ Community Summit, Virtual Summit,
  September 8-10, 2021</title>
    <summary>  These are the Proceedings of the 8th OMNeT++ Community Summit, which was held
virtually on September 8-10, 2021.
</summary>
    <author>
      <name>Marcel Marek</name>
    </author>
    <author>
      <name>Giovanni Nardini</name>
    </author>
    <author>
      <name>Vladimír Veselý</name>
    </author>
    <link href="http://arxiv.org/abs/2109.11607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.11607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.01250v1</id>
    <updated>2022-06-02T19:00:11Z</updated>
    <published>2022-06-02T19:00:11Z</published>
    <title>Proceedings of the 2022 Workshop on Resource AWareness of Systems and
  Society (RAW)</title>
    <summary>  Proceedings of the 2022 Workshop on Resource AWareness of Systems and Society
(RAW), colocated with ICT4S 2022 in Plovdiv, Bulgaria on 13th of June 2022.
</summary>
    <author>
      <name>Rafal Graczyk</name>
    </author>
    <author>
      <name>Padma Iyenghar</name>
    </author>
    <link href="http://arxiv.org/abs/2206.01250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.01250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.16589v2</id>
    <updated>2023-07-04T18:37:18Z</updated>
    <published>2023-06-28T22:41:14Z</published>
    <title>Collective-Optimized FFTs</title>
    <summary>  This paper measures the impact of the various alltoallv methods. Results are
analyzed within Beatnik, a Z-model solver that is bottlenecked by HeFFTe and
representative of applications that rely on FFTs.
</summary>
    <author>
      <name>Evelyn Namugwanya</name>
    </author>
    <author>
      <name>Amanda Bienz</name>
    </author>
    <author>
      <name>Derek Schafer</name>
    </author>
    <author>
      <name>Anthony Skjellum</name>
    </author>
    <link href="http://arxiv.org/abs/2306.16589v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.16589v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611037v2</id>
    <updated>2007-04-17T04:38:31Z</updated>
    <published>2006-11-09T03:57:33Z</published>
    <title>On Conditional Branches in Optimal Decision Trees</title>
    <summary>  The decision tree is one of the most fundamental programming abstractions. A
commonly used type of decision tree is the alphabetic binary tree, which uses
(without loss of generality) ``less than'' versus ''greater than or equal to''
tests in order to determine one of $n$ outcome events. The process of finding
an optimal alphabetic binary tree for a known probability distribution on
outcome events usually has the underlying assumption that the cost (time) per
decision is uniform and thus independent of the outcome of the decision. This
assumption, however, is incorrect in the case of software to be optimized for a
given microprocessor, e.g., in compiling switch statements or in fine-tuning
program bottlenecks. The operation of the microprocessor generally means that
the cost for the more likely decision outcome can or will be less -- often far
less -- than the less likely decision outcome. Here we formulate a variety of
$O(n^3)$-time $O(n^2)$-space dynamic programming algorithms to solve such
optimal binary decision tree problems, optimizing for the behavior of
processors with predictive branch capabilities, both static and dynamic. In the
static case, we use existing results to arrive at entropy-based performance
bounds. Solutions to this formulation are often faster in practice than
``optimal'' decision trees as formulated in the literature, and, for small
problems, are easily worth the extra complexity in finding the better solution.
This can be applied in fast implementation of decoding Huffman codes.
</summary>
    <author>
      <name>Michael B. Baer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 illustrations; conference version of cs.PF/0604016,
  accepted to ISIT 2007</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0611037v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611037v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.1.4; C.0; C.1.1; D.3.4; E.1; F.2.2; G.3; H.3.3; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0512043v1</id>
    <updated>2005-12-10T08:04:32Z</updated>
    <published>2005-12-10T08:04:32Z</published>
    <title>Random Walks with Anti-Correlated Steps</title>
    <summary>  We conjecture the expected value of random walks with anti-correlated steps
to be exactly 1. We support this conjecture with 2 plausibility arguments and
experimental data. The experimental analysis includes the computation of the
expected values of random walks for steps up to 22. The result shows the
expected value asymptotically converging to 1.
</summary>
    <author>
      <name>Dirk Wagner</name>
    </author>
    <author>
      <name>John Noga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 chart, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0512043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0512043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0606120v1</id>
    <updated>2006-06-29T09:53:05Z</updated>
    <published>2006-06-29T09:53:05Z</published>
    <title>On symmetric sandpiles</title>
    <summary>  A symmetric version of the well-known SPM model for sandpiles is introduced.
We prove that the new model has fixed point dynamics. Although there might be
several fixed points, a precise description of the fixed points is given.
Moreover, we provide a simple closed formula for counting the number of fixed
points originated by initial conditions made of a single column of grains.
</summary>
    <author>
      <name>Enrico Formenti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Benoît Masson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Theophilos Pisokas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Will be presented at ACRI2006 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0606120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; G.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701161v1</id>
    <updated>2007-01-25T23:51:22Z</updated>
    <published>2007-01-25T23:51:22Z</published>
    <title>Thousands of DebitCredit Transactions-Per-Second: Easy and Inexpensive</title>
    <summary>  A $2k computer can execute about 8k transactions per second. This is 80x more
than one of the largest US bank's 1970's traffic - it approximates the total US
1970's financial transaction volume. Very modest modern computers can easily
solve yesterday's problems.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Charles Levine</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701162v1</id>
    <updated>2007-01-25T23:57:15Z</updated>
    <published>2007-01-25T23:57:15Z</published>
    <title>A Measure of Transaction Processing 20 Years Later</title>
    <summary>  This provides a retrospective of the paper "A Measure of Transaction
Processing" published in 1985. It shows that transaction processing peak
performance and price-peformance have improved about 100,000x respectively and
that sort/sequential performance has approximately doubled each year (so a
million fold improvement) even though processor performance plateaued in 1995.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article appeared in the IEEE Data Engineering, Fall 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0701162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701191v1</id>
    <updated>2007-01-30T15:20:07Z</updated>
    <published>2007-01-30T15:20:07Z</published>
    <title>The parallel implementation of the Astrée static analyzer</title>
    <summary>  The Astr\'{e}e static analyzer is a specialized tool that can prove the
absence of runtime errors, including arithmetic overflows, in large critical
programs. Keeping analysis times reasonable for industrial use is one of the
design objectives. In this paper, we discuss the parallel implementation of the
analysis.
</summary>
    <author>
      <name>David Monniaux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIENS</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/11575467_7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/11575467_7" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">APLAS: Programming languages and systems (2005) 86-96</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0701191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.2596v2</id>
    <updated>2009-12-06T01:34:28Z</updated>
    <published>2008-11-16T19:10:43Z</published>
    <title>An Enhanced Mathematical Model for Performance Evaluation of Optical
  Burst Switched Networks</title>
    <summary>  This paper has been withdrawn by the authors.
</summary>
    <author>
      <name>Mohamed H. S. Morsy</name>
    </author>
    <author>
      <name>Mohamad Y. S. Sowailem</name>
    </author>
    <author>
      <name>Hossam M. H. Shalaby</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the authors</arxiv:comment>
    <link href="http://arxiv.org/abs/0811.2596v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.2596v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.0192v2</id>
    <updated>2009-12-06T01:14:22Z</updated>
    <published>2008-12-01T00:55:14Z</published>
    <title>A Simple Performance Analysis of a Core Node in an Optical Burst
  Switched Network</title>
    <summary>  This paper has been withdrawn
</summary>
    <author>
      <name>Mohamed H. S. Morsy</name>
    </author>
    <author>
      <name>Mohammad Y. S. Sowailem</name>
    </author>
    <author>
      <name>Hossam M. H. Shalaby</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/0812.0192v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.0192v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.4217v1</id>
    <updated>2009-06-23T14:43:07Z</updated>
    <published>2009-06-23T14:43:07Z</published>
    <title>A Performance Analysis of HICCUPS - a Steganographic System for WLAN</title>
    <summary>  The paper presents an analysis of performance features of the HICCUPS (HIdden
Communication system for CorrUPted networkS) including the efficiency and the
cost of the system in WLANs (Wireless Local Area Networks). The analysis relies
on the original CSMA/CA (Carrier Sense Multiple Access with Collision
Avoidance) 802.11 Markov chain-based model.
</summary>
    <author>
      <name>Krzysztof Szczypiorski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, 3 tables. Submitted to the First International
  Workshop on Network Steganography - IWNS 2009, November 18-20, 2009, Wuhan
  (China)</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.4217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.4217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.5819v2</id>
    <updated>2015-06-25T20:38:29Z</updated>
    <published>2009-10-30T09:25:43Z</published>
    <title>Undecidability of performance equivalence of Petri nets</title>
    <summary>  We investigate bisimulation equivalence on Petri nets under durational
semantics. Our motivation was to verify the conjecture that in durational
setting, the bisimulation equivalence checking problem becomes more tractable
than in ordinary setting (which is the case, e.g., over communication-free
nets). We disprove this conjecture in three of four proposed variants of
durational semantics. The fourth variant remains an intriguing open problem.
</summary>
    <author>
      <name>Slawomir Lasota</name>
    </author>
    <author>
      <name>Marcin Poturalski</name>
    </author>
    <link href="http://arxiv.org/abs/0910.5819v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.5819v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.4078v1</id>
    <updated>2011-04-20T17:22:34Z</updated>
    <published>2011-04-20T17:22:34Z</published>
    <title>A Note on Parallel Algorithmic Speedup Bounds</title>
    <summary>  A parallel program can be represented as a directed acyclic graph. An
important performance bound is the time to execute the critical path through
the graph. We show how this performance metric is related to Amdahl speedup and
the degree of average parallelism. These bounds formally exclude superlinear
performance.
</summary>
    <author>
      <name>Neil J. Gunther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.4078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.4078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8; C.4; C.5.5; D.4.8; F.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.6255v1</id>
    <updated>2012-04-27T15:48:38Z</updated>
    <published>2012-04-27T15:48:38Z</published>
    <title>Revisiting the D-iteration method: runtime comparison</title>
    <summary>  In this paper, we revisit the D-iteration algorithm in order to better
explain different performance results that were observed for the numerical
computation of the eigenvector associated to the PageRank score. We revisit
here the practical computation cost based on the execution runtime compared to
the theoretical number of iterations.
</summary>
    <author>
      <name>Dohy Hong</name>
    </author>
    <author>
      <name>Gérard Burnside</name>
    </author>
    <author>
      <name>Philippe Raoult</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.6255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.6255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.3; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.0678v1</id>
    <updated>2013-08-03T10:17:12Z</updated>
    <published>2013-08-03T10:17:12Z</published>
    <title>Performance comparison of IEEE 802.11g and IEEE 802.11n in the presence
  of interference from 802.15.4 networks</title>
    <summary>  In this paper we compare the packet error rate (PER) and maximum throughput
of IEEE 802.11n and IEEE 802.11g under interference from IEEE 802.15.4 by using
MATLAB to simulate the IEEE PHY for 802.11n and 802.11g networks.
</summary>
    <author>
      <name>Syed Haani Masood</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.0678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.0678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.07986v3</id>
    <updated>2015-11-12T20:42:09Z</updated>
    <published>2015-10-27T17:01:32Z</published>
    <title>A Novel Offloading Partitioning Algorithm in Mobile Cloud Computing</title>
    <summary>  This paper has been withdrawn by the author
</summary>
    <author>
      <name>Huaming Wu</name>
    </author>
    <author>
      <name>Daniel Seidenstücker</name>
    </author>
    <author>
      <name>Yi Sun</name>
    </author>
    <author>
      <name>Carlos Martín Nieto</name>
    </author>
    <author>
      <name>William Knottenbelt</name>
    </author>
    <author>
      <name>Katinka Wolter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.07986v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.07986v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07629v1</id>
    <updated>2016-09-24T14:47:00Z</updated>
    <published>2016-09-24T14:47:00Z</published>
    <title>Proceedings of the 3rd OMNeT++ Community Summit, Brno University of
  Technology - Czech Republic, September 15-16, 2016</title>
    <summary>  These are the Proceedings of the 3rd OMNeT++ Community Summit, which was held
at the University of Technology in Brno - Czech Republic - on September 15-16,
2016.
</summary>
    <author>
      <name>Anna Förster</name>
    </author>
    <author>
      <name>Vladimír Veselý</name>
    </author>
    <author>
      <name>Antonio Virdis</name>
    </author>
    <author>
      <name>Michael Kirsche</name>
    </author>
    <link href="http://arxiv.org/abs/1609.07629v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07629v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6; C.2.0; C.4; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03745v1</id>
    <updated>2017-09-12T08:59:09Z</updated>
    <published>2017-09-12T08:59:09Z</published>
    <title>Proceedings of the 4th OMNeT++ Community Summit, University of Bremen -
  Germany, September 7-8, 2017</title>
    <summary>  These are the Proceedings of the 4th OMNeT++ Community Summit, which was held
at the University of Bremen - Germany - on September 7-8, 2017.
</summary>
    <author>
      <name>Anna Förster</name>
    </author>
    <author>
      <name>Asanga Udugama</name>
    </author>
    <author>
      <name>Andreas Könsgen</name>
    </author>
    <author>
      <name>Antonio Virdis</name>
    </author>
    <author>
      <name>Michael Kirsche</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 4th OMNeT++ Community Summit</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.03745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6; I.6.5; C.2.0; C.4; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09108v1</id>
    <updated>2017-09-26T16:11:43Z</updated>
    <published>2017-09-26T16:11:43Z</published>
    <title>Tensors Come of Age: Why the AI Revolution will help HPC</title>
    <summary>  This article discusses how the automation of tensor algorithms, based on A
Mathematics of Arrays and Psi Calculus, and a new way to represent numbers,
Unum Arithmetic, enables mechanically provable, scalable, portable, and more
numerically accurate software.
</summary>
    <author>
      <name>John L. Gustafson</name>
    </author>
    <author>
      <name>Lenore M. Mullin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in this years 30th anniversary edition of HPCwire</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.09108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00056v1</id>
    <updated>2018-01-11T03:34:10Z</updated>
    <published>2018-01-11T03:34:10Z</published>
    <title>diagnoseIT: Expertengestützte automatische Diagnose von
  Performance-Probleme in Enterprise-Anwendungen (Abschlussbericht)</title>
    <summary>  This is the final report of the collaborative research project diagnoseIT on
expert-guided automatic diagnosis of performance problems in enterprise
applications.
</summary>
    <author>
      <name>Christoph Heger</name>
    </author>
    <author>
      <name>André van Hoorn</name>
    </author>
    <author>
      <name>Dušan Okanovic</name>
    </author>
    <author>
      <name>Stefan Siegl</name>
    </author>
    <author>
      <name>Christian Vögele</name>
    </author>
    <author>
      <name>Alexander Wert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The language of the report is German</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.00056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.08040v2</id>
    <updated>2018-06-29T14:08:25Z</updated>
    <published>2018-04-21T22:45:21Z</published>
    <title>A Fluid-Flow Interpretation of SCED Scheduling</title>
    <summary>  We show that a fluid-flow interpretation of Service Curve Earliest Deadline
First (SCED) scheduling simplifies deadline derivations for this scheduler. By
exploiting the recently reported isomorphism between min-plus and max-plus
network calculus, and expressing deadlines in a max-plus algebra, deadline
computations no longer require pseudo-inverse computations. SCED deadlines are
provided for general convex or concave piecewise linear service curves.
</summary>
    <author>
      <name>Jorg Liebeherr</name>
    </author>
    <link href="http://arxiv.org/abs/1804.08040v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08040v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.04875v2</id>
    <updated>2019-02-01T20:52:11Z</updated>
    <published>2018-11-12T17:43:53Z</published>
    <title>Comparing Spark vs MPI/OpenMP On Word Count MapReduce</title>
    <summary>  Spark provides an in-memory implementation of MapReduce that is widely used
in the big data industry. MPI/OpenMP is a popular framework for high
performance parallel computing. This paper presents a high performance
MapReduce design in MPI/OpenMP and uses that to compare with Spark on the
classic word count MapReduce task. My result shows that the MPI/OpenMP
MapReduce outperforms Apache Spark by about 300%.
</summary>
    <author>
      <name>Junhao Li</name>
    </author>
    <link href="http://arxiv.org/abs/1811.04875v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.04875v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.10707v1</id>
    <updated>2021-01-26T11:03:02Z</updated>
    <published>2021-01-26T11:03:02Z</published>
    <title>Enhancing Application Performance by Memory Partitioning in Android
  Platforms</title>
    <summary>  This paper suggests a new memory partitioning scheme that can enhance process
lifecycle, while avoiding Low Memory Killer and Out-of-Memory Killer operations
on mobile devices. Our proposed scheme offers the complete concept of virtual
memory nodes in operating systems of Android devices.
</summary>
    <author>
      <name>Geunsik Lim</name>
    </author>
    <author>
      <name>Changwoo Min</name>
    </author>
    <author>
      <name>Young Ik Eom</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCE.2013.6487055</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCE.2013.6487055" rel="related"/>
    <link href="http://arxiv.org/abs/2101.10707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.10707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.06210v1</id>
    <updated>2021-04-13T14:01:58Z</updated>
    <published>2021-04-13T14:01:58Z</published>
    <title>A simple proof of the Moore-Hodgson Algorithm for minimizing the number
  of late jobs</title>
    <summary>  The Moore-Hodgson Algorithm minimizes the number of late jobs on a single
machine. That is, it finds an optimal schedule for the classical problem
$1~|\;|~\sum{U_j}$. Several proofs of the correctness of this algorithm have
been published. We present a new short proof.
</summary>
    <author>
      <name>Joseph Cheriyan</name>
    </author>
    <author>
      <name>R. Ravi</name>
    </author>
    <author>
      <name>Martin Skutella</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.06210v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.06210v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90B35, 68W40" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.09541v1</id>
    <updated>2021-09-20T13:52:19Z</updated>
    <published>2021-09-20T13:52:19Z</published>
    <title>Scaling TensorFlow to 300 million predictions per second</title>
    <summary>  We present the process of transitioning machine learning models to the
TensorFlow framework at a large scale in an online advertising ecosystem. In
this talk we address the key challenges we faced and describe how we
successfully tackled them; notably, implementing the models in TF and serving
them efficiently with low latency using various optimization techniques.
</summary>
    <author>
      <name>Jan Hartman</name>
    </author>
    <author>
      <name>Davorin Kopič</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3460231.3474605</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3460231.3474605" rel="related"/>
    <link href="http://arxiv.org/abs/2109.09541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.09541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.09840v1</id>
    <updated>2021-10-19T10:48:06Z</updated>
    <published>2021-10-19T10:48:06Z</published>
    <title>Stability analysis of two-class retrial systems with constant retrial
  rates and general service times</title>
    <summary>  We establish stability criterion for a two-class retrial system with Poisson
inputs, general class-dependent service times and class-dependent constant
retrial rates. We also characterise an interesting phenomenon of partial
stability when one orbit is tight but the other orbit goes to infinity in
probability. All theoretical results are illustrated by numerical experiments.
</summary>
    <author>
      <name>Konstantin Avrachenkov</name>
    </author>
    <author>
      <name>Evsey Morozov</name>
    </author>
    <author>
      <name>Ruslana Nekrasova</name>
    </author>
    <link href="http://arxiv.org/abs/2110.09840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.09840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.09407v1</id>
    <updated>2017-11-26T15:24:05Z</updated>
    <published>2017-11-26T15:24:05Z</published>
    <title>A note on using performance and data profilesfor training algorithms</title>
    <summary>  It is shown how to use the performance and data profile benchmarking tools to
improve algorithms' performance. An illustration for the BFO derivative-free
optimizer suggests that the obtained gains are potentially significant.
</summary>
    <author>
      <name>Margherita Porcelli</name>
    </author>
    <author>
      <name>Philippe L. Toint</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 tables, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Transactions of the AMS on Mathematical Software, vol. 45(2), 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.09407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.09407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65K05, 90C56, 90C90" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.4; D.2.2; D.2.8; G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10435v1</id>
    <updated>2017-11-20T16:33:31Z</updated>
    <published>2017-11-20T16:33:31Z</published>
    <title>LP-Based Power Grid Enhancement Methodology</title>
    <summary>  In this paper, we explored the opportunity to enhance power grid robustness
after routing stage, and propose a linear programming based algorithm that
maximizes the improvement of power grid strengthening with given available
routing resource. We further discussed some techniques to leverage tradeoffs
between runtime and optimality of the solutions. Experimental results show
substantial power integrity improvement with "zero cost".
</summary>
    <author>
      <name>Tapio Bohn</name>
    </author>
    <author>
      <name>Paul Salmi</name>
    </author>
    <author>
      <name>Albert Milner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.10435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01369v1</id>
    <updated>2018-09-05T07:56:11Z</updated>
    <published>2018-09-05T07:56:11Z</published>
    <title>Towards quantitative methods to assess network generative models</title>
    <summary>  Assessing generative models is not an easy task. Generative models should
synthesize graphs which are not replicates of real networks but show
topological features similar to real graphs. We introduce an approach for
assessing graph generative models using graph classifiers. The inability of an
established graph classifier for distinguishing real and synthesized graphs
could be considered as a performance measurement for graph generators.
</summary>
    <author>
      <name>Vahid Mostofi</name>
    </author>
    <author>
      <name>Sadegh Aliakbary</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.02018v1</id>
    <updated>2018-09-06T14:40:10Z</updated>
    <published>2018-09-06T14:40:10Z</published>
    <title>Scalable Load Balancing Algorithms in Networked Systems</title>
    <summary>  A fundamental challenge in large-scale networked systems viz., data centers
and cloud networks is to distribute tasks to a pool of servers, using minimal
instantaneous state information, while providing excellent delay performance.
In this thesis we design and analyze load balancing algorithms that aim to
achieve a highly efficient distribution of tasks, optimize server utilization,
and minimize communication overhead.
</summary>
    <author>
      <name>Debankur Mukherjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ph.D. thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.02018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.02018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.00260v1</id>
    <updated>2019-08-31T18:34:35Z</updated>
    <published>2019-08-31T18:34:35Z</published>
    <title>SCALABLE INTERNETWORKING: Final Technical Report</title>
    <summary>  This document describes the work completed at the University of California,
Santa Cruz under the project Scalable Internetworking sponsored by ARPA under
Contract No. F19628-93-C-0175. This report covers work performed from 1 April
1993 to 31 December 1995. Results on routing and multicasting for large-scale
internets are summarized. The technical material discussed assumes familiarity
with the content of our proposal and previous quarterly reports submitted in
this project.
</summary>
    <author>
      <name>JJ Garcia-Luna-Aceves</name>
    </author>
    <author>
      <name>A. Varma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.00260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.03715v1</id>
    <updated>2020-09-01T15:17:26Z</updated>
    <published>2020-09-01T15:17:26Z</published>
    <title>Latency and Throughput Optimization in Modern Networks: A Comprehensive
  Survey</title>
    <summary>  Modern applications are highly sensitive to communication delays and
throughput. This paper surveys major attempts on reducing latency and
increasing the throughput. These methods are surveyed on different networks and
surroundings such as wired networks, wireless networks, application layer
transport control, Remote Direct Memory Access, and machine learning based
transport control.
</summary>
    <author>
      <name>Amir Mirzaeinnia</name>
    </author>
    <author>
      <name>Mehdi Mirzaeinia</name>
    </author>
    <author>
      <name>Abdelmounaam Rezgui</name>
    </author>
    <link href="http://arxiv.org/abs/2009.03715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.03715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.11224v1</id>
    <updated>2020-09-23T15:39:12Z</updated>
    <published>2020-09-23T15:39:12Z</published>
    <title>Applying the Roofline model for Deep Learning performance optimizations</title>
    <summary>  In this paper We present a methodology for creating Roofline models
automatically for Non-Unified Memory Access (NUMA) using Intel Xeon as an
example. Finally, we present an evaluation of highly efficient deep learning
primitives as implemented in the Intel oneDNN Library.
</summary>
    <author>
      <name>Jacek Czaja</name>
    </author>
    <author>
      <name>Michal Gallus</name>
    </author>
    <author>
      <name>Joanna Wozna</name>
    </author>
    <author>
      <name>Adam Grygielski</name>
    </author>
    <author>
      <name>Luo Tao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">oneDNN library analysis with roofline model</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.11224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.10039v1</id>
    <updated>2023-11-16T17:36:28Z</updated>
    <published>2023-11-16T17:36:28Z</published>
    <title>Software Dependability Measurement at the Age Of 36</title>
    <summary>  Thirty-six years after the first edition of IEEE standard 982.1, Measures of
the Software Aspects of Dependability, the third edition focuses on the
measurement of in-service software dependability. This article explains how
this new point of view evolved and shaped the third edition's guidance for
software dependability measurement.
</summary>
    <author>
      <name>Robert V. Binder</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MC.2023.3327668</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MC.2023.3327668" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 4 figures. Accepted for publication in IEEE Computer, April
  2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.10039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.10039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.08142v1</id>
    <updated>2024-02-13T00:33:43Z</updated>
    <published>2024-02-13T00:33:43Z</published>
    <title>Optimal Size-Aware Dispatching Rules via Value Iteration and Some
  Numerical Investigations</title>
    <summary>  This technical report explains how optimal size-aware dispatching policies
can be determined numerically using value iteration. It also contains some
numerical examples that shed light to the nature of the optimal policies
itself. The report complements our ``Towards the Optimal Dynamic Size-aware
Dispatching'' article that will appear in Elsevier's Performance Evaluation in
2024.
</summary>
    <author>
      <name>Esa Hyytiä</name>
    </author>
    <author>
      <name>Rhonda Righter</name>
    </author>
    <link href="http://arxiv.org/abs/2402.08142v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.08142v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.05377v1</id>
    <updated>2024-03-08T15:09:27Z</updated>
    <published>2024-03-08T15:09:27Z</published>
    <title>Scalable Software as a Service Architecture</title>
    <summary>  This paper explores the architecture of Software as a Service (SaaS)
platforms, emphasizing scalability and maintainability. SaaS, a flexible
software distribution model suitable for individuals and organizations, has
become prevalent with the advent of Cloud services. This paper aims to provide
a high-level design reference for establishing a scalable and maintainable SaaS
architecture.
</summary>
    <author>
      <name>Ardy Dedase</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 12 figures, self-published</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.05377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.05377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.16917v1</id>
    <updated>2024-05-27T08:09:07Z</updated>
    <published>2024-05-27T08:09:07Z</published>
    <title>LRAMM -- Low precision approximates GEMM via RSVD</title>
    <summary>  Matrix multiplication computation acceleration has been a research hotspot
across various domains. Due to the characteristics of some applications,
approximate matrix multiplication can achieve significant performance
improvements without losing much precision.
  In this paper, we propose LRAMM - a high-performance matrix multiplication
approximation algorithm that combines mixed-precision quantized matrix
multiplication with RSVD techniques, further enhancing efficiency within the
error range of low-precision matrix multiplication by utilizing matrix low-rank
decomposition technology.
</summary>
    <author>
      <name>Hongyaoxing Gu</name>
    </author>
    <link href="http://arxiv.org/abs/2405.16917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.16917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.05858v1</id>
    <updated>2024-06-09T17:03:56Z</updated>
    <published>2024-06-09T17:03:56Z</published>
    <title>Comments on "Federated Learning with Differential Privacy: Algorithms
  and Performance Analysis"</title>
    <summary>  In the paper by Wei et al. ("Federated Learning with Differential Privacy:
Algorithms and Performance Analysis"), the convergence performance of the
proposed differential privacy algorithm in federated learning (FL), known as
Noising before Model Aggregation FL (NbAFL), was studied. However, the
presented convergence upper bound of NbAFL (Theorem 2) is incorrect. This
comment aims to present the correct form of the convergence upper bound for
NbAFL.
</summary>
    <author>
      <name>Mahtab Talaei</name>
    </author>
    <author>
      <name>Iman Izadi</name>
    </author>
    <link href="http://arxiv.org/abs/2406.05858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.05858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.00643v2</id>
    <updated>2024-08-24T10:32:09Z</updated>
    <published>2024-06-30T10:05:20Z</published>
    <title>A Power-Consumption Analysis for Different IPoWDM Network Architectures
  with ZR/ZR+ and Long-Haul Muxponders</title>
    <summary>  We quantify and compare the power consumption of four IPoWDM transport
network architectures employing ZR/ZR+ modules, considering different grooming,
regeneration, and optical bypass capabilities. Results show that optical bypass
is still the most power-efficient solution, reducing consumption by up to 30%.
</summary>
    <author>
      <name>Qiaolun Zhang</name>
    </author>
    <author>
      <name>Annalisa Morea</name>
    </author>
    <author>
      <name>Massimo Tornatore</name>
    </author>
    <link href="http://arxiv.org/abs/2407.00643v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.00643v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006015v2</id>
    <updated>2000-06-13T22:07:28Z</updated>
    <published>2000-06-08T17:02:51Z</published>
    <title>UNIX Resource Managers: Capacity Planning and Resource Issues</title>
    <summary>  The latest implementations of commercial UNIX to offer mainframe style
capacity management on enterprise servers include: AIX Workload Manager (WLM),
HP-UX Process Resource Manager (PRM), Solaris Resource Manager (SRM), as well
as SGI and Compaq. The ability to manage server capacity is achieved by making
significant modifications to the standard UNIX operating system so that
processes are inherently tied to specific users. Those users, in turn, are
granted only a certain fraction of system resources. Resource usage is
monitored and compared with each users grant to ensure that the assigned
entitlement constraints are met. In this paper, we begin by clearing up some of
the confusion that has surrounded the motivation and the terminology behind the
new technology. The common theme across each of the commercial implementations
is the introduction of the fair-share scheduler. After reviewing some potential
performance pitfalls, we present capacity planning guidelines for migrating to
automated UNIX resource management.
</summary>
    <author>
      <name>Neil J. Gunther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages. Fixed formatting problem. To be presented at the SAGE-AU
  Conference, Bond University, Gold Coast, Australia, July 7, 2000</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0006015v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006015v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4;D.4.1;D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006016v1</id>
    <updated>2000-06-08T18:08:19Z</updated>
    <published>2000-06-08T18:08:19Z</published>
    <title>The X-Files: Investigating Alien Performance in a Thin-client World</title>
    <summary>  Many scientific applications use the X11 window environment; an open source
windows GUI standard employing a client/server architecture. X11 promotes:
distributed computing, thin-client functionality, cheap desktop displays,
compatibility with heterogeneous servers, remote services and administration,
and greater maturity than newer web technologies. This paper details the
author's investigations into close encounters with alien performance in
X11-based seismic applications running on a 200-node cluster, backed by 2 TB of
mass storage. End-users cited two significant UFOs (Unidentified Faulty
Operations) i) long application launch times and ii) poor interactive response
times. The paper is divided into three major sections describing Close
Encounters of the 1st Kind: citings of UFO experiences, the 2nd Kind: recording
evidence of a UFO, and the 3rd Kind: contact and analysis. UFOs do exist and
this investigation presents a real case study for evaluating workload analysis
and other diagnostic tools.
</summary>
    <author>
      <name>Neil J. Gunther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages; Invited Lecture at the High Performance Computing
  Conference, University of Tromso, Norway, June 27-30, 1999</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. Hiper'99 Vol.1, p.156</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0006016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4;C.4;D.4.8;D.4.9;H.3.4;H.5.2;I.6.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0007027v1</id>
    <updated>2000-07-14T17:44:41Z</updated>
    <published>2000-07-14T17:44:41Z</published>
    <title>Efficient cache use for stencil operations on structured discretization
  grids</title>
    <summary>  We derive tight bounds on cache misses for evaluation of explicit stencil
operators on structured grids. Our lower bound is based on the isoperimetrical
property of the discrete octahedron. Our upper bound is based on good surface
to volume ratio of a parallelepiped spanned by a reduced basis of the inter-
ference lattice of a grid. Measurements show that our algorithm typically
reduces the number of cache misses by factor of three relative to a compiler
optimized code. We show that stencil calculations on grids whose interference
lattice have a short vector feature abnormally high numbers of cache misses. We
call such grids unfavorable and suggest to avoid these in computations by
appropriate padding. By direct measurements on MIPS R10000 we show a good
correlation of abnormally high cache misses and unfavorable three-dimensional
grids.
</summary>
    <author>
      <name>Michael A. Frumkin</name>
    </author>
    <author>
      <name>Rob F. Van der Wijngaart</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">tex .tar.gz file, including ps file, 16 pagest, 5 figures, 2
  Appendicies</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0007027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0007027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4;B.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0012010v1</id>
    <updated>2000-12-15T14:04:28Z</updated>
    <published>2000-12-15T14:04:28Z</published>
    <title>The Role of Commutativity in Constraint Propagation Algorithms</title>
    <summary>  Constraint propagation algorithms form an important part of most of the
constraint programming systems. We provide here a simple, yet very general
framework that allows us to explain several constraint propagation algorithms
in a systematic way. In this framework we proceed in two steps. First, we
introduce a generic iteration algorithm on partial orderings and prove its
correctness in an abstract setting. Then we instantiate this algorithm with
specific partial orderings and functions to obtain specific constraint
propagation algorithms.
  In particular, using the notions commutativity and semi-commutativity, we
show that the {\tt AC-3}, {\tt PC-2}, {\tt DAC} and {\tt DPC} algorithms for
achieving (directional) arc consistency and (directional) path consistency are
instances of a single generic algorithm. The work reported here extends and
simplifies that of Apt \citeyear{Apt99b}.
</summary>
    <author>
      <name>Krzysztof R. Apt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages. To appear in ACM TOPLAS</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0012010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0012010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.3;I.1.2;I.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0012022v1</id>
    <updated>2000-12-26T22:42:39Z</updated>
    <published>2000-12-26T22:42:39Z</published>
    <title>Performance and Scalability Models for a Hypergrowth e-Commerce Web Site</title>
    <summary>  The performance of successful Web-based e-commerce services has all the
allure of a roller-coaster ride: accelerated fiscal growth combined with the
ever-present danger of running out of server capacity. This chapter presents a
case study based on the author's own capacity planning engagement with one of
the hottest e-commerce Web sites in the world. Several spreadsheet techniques
are presented for forecasting both short-term and long-term trends in the
consumption of server capacity. Two new performance metrics are introduced for
site planning and procurement: the effective demand, and the doubling period.
</summary>
    <author>
      <name>Neil J. Gunther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages; To appear in the book entitled "Performance Engineering -
  State of the Art and Current Trends," Lecture Notes in Computer Science,
  Springer-Verlag Heidelberg, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0012022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0012022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4;D.2.8;D.4.8;H.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0103014v1</id>
    <updated>2001-03-12T18:23:57Z</updated>
    <published>2001-03-12T18:23:57Z</published>
    <title>Faster-than-light effects and negative group delays in optics and
  electronics, and their applications</title>
    <summary>  Recent manifestations of apparently faster-than-light effects confirmed our
predictions that the group velocity in transparent optical media can exceed c.
Special relativity is not violated by these phenomena. Moreover, in the
electronic domain, the causality principle does not forbid negative group
delays of analytic signals in electronic circuits, in which the peak of an
output pulse leaves the exit port of a circuit before the peak of the input
pulse enters the input port. Furthermore, pulse distortion for these
superluminal analytic signals can be negligible in both the optical and
electronic domains. Here we suggest an extension of these ideas to the
microelectronic domain. The underlying principle is that negative feedback can
be used to produce negative group delays. Such negative group delays can be
used to cancel out the positive group delays due to transistor latency (e.g.,
the finite RC rise time of MOSFETS caused by their intrinsic gate capacitance),
as well as the propagation delays due to the interconnects between transistors.
Using this principle, it is possible to speed up computer systems.
</summary>
    <author>
      <name>Raymond Y. Chiao</name>
    </author>
    <author>
      <name>Jandir M. Hickmann</name>
    </author>
    <author>
      <name>Daniel Solli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.432562</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.432562" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures, 2001 Photonic West Plenary Talk</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0103014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0103014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.7.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0107001v2</id>
    <updated>2001-11-06T19:14:57Z</updated>
    <published>2001-07-02T08:21:15Z</published>
    <title>Analysis of Network Traffic in Switched Ethernet Systems</title>
    <summary>  A 100 Mbps Ethernet link between a college campus and the outside world was
monitored with a dedicated PC and the measured data analysed for its
statistical properties. Similar measurements were taken at an internal node of
the network. The networks in both cases are a full-duplex switched Ethernet.
Inter-event interval histograms and power spectra of the throughput aggregated
for 10ms bins were used to analyse the measured traffic. For most investigated
cases both methods reveal that the traffic behaves according to a power law.
The results will be used in later studies to parameterise models for network
traffic.
</summary>
    <author>
      <name>Tony Field</name>
    </author>
    <author>
      <name>Uli Harder</name>
    </author>
    <author>
      <name>Peter Harrison</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 14 figures, using infocom.cls (included) and graphicx (not
  included); all power spectrum plots replaced with new ones; minor typos in
  figure captions corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0107001v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0107001v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.5; C.4; D.2.8; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0111034v1</id>
    <updated>2001-11-09T22:33:58Z</updated>
    <published>2001-11-09T22:33:58Z</published>
    <title>Experiences with advanced CORBA services</title>
    <summary>  The Common Object Request Broker Architecture (CORBA) is successfully used in
many control systems (CS) for data transfer and device modeling. Communication
rates below 1 millisecond, high reliability, scalability, language independence
and other features make it very attractive. For common types of applications
like error logging, alarm messaging or slow monitoring, one can benefit from
standard CORBA services that are implemented by third parties and save
tremendous amount of developing time. We have started using few CORBA services
on our previous CORBA-based control system for the light source ANKA [1] and
use now several CORBA services for the ALMA Common Software (ACS) [2], the core
of the control system of the Atacama Large Millimeter Array. Our experiences
with the interface repository (IFR), the implementation repository, the naming
service, the property service, telecom log service and the notify service from
different vendors are presented. Performance and scalability benchmarks have
been performed.
</summary>
    <author>
      <name>G. Milcinski</name>
    </author>
    <author>
      <name>M. Plesko</name>
    </author>
    <author>
      <name>M. Sekoranja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Poster THAP005, ICALEPS 2001, 3 pages (ZIP and WORD file)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">eConf C011127 (2001) THAP005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0111034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0111034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0202019v2</id>
    <updated>2022-03-04T02:06:29Z</updated>
    <published>2002-02-16T21:46:14Z</published>
    <title>Hypernets -- Good (G)news for Gnutella</title>
    <summary>  Criticism of Gnutella network scalability has rested on the bandwidth
attributes of the original interconnection topology: a Cayley tree. Trees, in
general, are known to have lower aggregate bandwidth than higher dimensional
topologies e.g., hypercubes, meshes and tori. Gnutella was intended to support
thousands to millions of peers. Studies of interconnection topologies in the
literature, however, have focused on hardware implementations which are limited
by cost to a few thousand nodes. Since the Gnutella network is virtual,
hyper-topologies are relatively unfettered by such constraints. We present
performance models for several plausible hyper-topologies and compare their
query throughput up to millions of peers. The virtual hypercube and the virtual
hypertorus are shown to offer near linear scalability subject to the number of
peer TCP/IP connections that can be simultaneously kept open.
</summary>
    <author>
      <name>N. J. Gunther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, 3 tables. Updated text and replaced original
  2002 GIF figures with PDFs</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0202019v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0202019v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4; C.4; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0211024v1</id>
    <updated>2002-11-20T22:32:56Z</updated>
    <published>2002-11-20T22:32:56Z</published>
    <title>Narses: A Scalable Flow-Based Network Simulator</title>
    <summary>  Most popular, modern network simulators, such as ns, are targeted towards
simulating low-level protocol details. These existing simulators are not
intended for simulating large distributed applications with many hosts and many
concurrent connections over long periods of simulated time. We introduce a new
simulator, Narses, targeted towards large distributed applications. The goal of
Narses is to simulate and validate large applications efficiently using network
models of varying levels of detail. We introduce several simplifying
assumptions that allow our simulator to scale to the needs of large distributed
applications while maintaining a reasonable degree of accuracy. Initial results
show up to a 45 times speedup while consuming 28% of the memory used by ns.
Narses maintains a reasonable degree of accuracy -- within 8% on average.
</summary>
    <author>
      <name>TJ Giuli</name>
    </author>
    <author>
      <name>Mary Baker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0211024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0211024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.8; I.6.5; I.6.3; C.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0304015v1</id>
    <updated>2003-04-10T14:01:26Z</updated>
    <published>2003-04-10T14:01:26Z</published>
    <title>A Performance Study of Monitoring and Information Services for
  Distributed Systems</title>
    <summary>  Monitoring and information services form a key component of a distributed
system, or Grid. A quantitative study of such services can aid in understanding
the performance limitations, advise in the deployment of the systems, and help
evaluate future development work. To this end, we study the performance of
three monitoring and information services for distributed systems: the Globus
Toolkit's Monitoring and Discovery Service (MDS), the European Data Grid
Relational Grid Monitoring Architecture (R-GMA), and Hawkeye, part of the
Condor project. We perform experiments to test their scalability with respect
to number of users, number of resources, and amount of data collected. Our
study shows that each approach has different behaviors, often due to their
different design goals. In the four sets of experiments we conducted to
evaluate the performance of the service components under different
circumstances, we found a strong advantage to caching or prefetching the data,
as well as the need to have primary components at well connected sites due to
high load seen by all systems.
</summary>
    <author>
      <name>Xuehai Zhang</name>
    </author>
    <author>
      <name>Jeffrey Freschl</name>
    </author>
    <author>
      <name>Jennifer M. Schopf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 20 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0304015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0304015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.4; C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0305060v1</id>
    <updated>2003-05-30T10:14:02Z</updated>
    <published>2003-05-30T10:14:02Z</published>
    <title>Performance comparison between iSCSI and other hardware and software
  solutions</title>
    <summary>  We report on our investigations on some technologies that can be used to
build disk servers and networks of disk servers using commodity hardware and
software solutions. It focuses on the performance that can be achieved by these
systems and gives measured figures for different configurations.
  It is divided into two parts : iSCSI and other technologies and hardware and
software RAID solutions.
  The first part studies different technologies that can be used by clients to
access disk servers using a gigabit ethernet network. It covers block access
technologies (iSCSI, hyperSCSI, ENBD). Experimental figures are given for
different numbers of clients and servers.
  The second part compares a system based on 3ware hardware RAID controllers, a
system using linux software RAID and IDE cards and a system mixing both
hardware RAID and software RAID. Performance measurements for reading and
writing are given for different RAID levels.
</summary>
    <author>
      <name>Mathias Gug</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper associated to a poster from the 2003 Computing in High Energy
  and Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages, LaTeX.
  PSN TUDP001</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ECONFC0303241:TUDP001,2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0305060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0305060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306090v1</id>
    <updated>2003-06-14T10:46:34Z</updated>
    <published>2003-06-14T10:46:34Z</published>
    <title>Worldwide Fast File Replication on Grid Datafarm</title>
    <summary>  The Grid Datafarm architecture is designed for global petascale
data-intensive computing. It provides a global parallel filesystem with online
petascale storage, scalable I/O bandwidth, and scalable parallel processing,
and it can exploit local I/O in a grid of clusters with tens of thousands of
nodes. One of features is that it manages file replicas in filesystem metadata
for fault tolerance and load balancing.
  This paper discusses and evaluates several techniques to support
long-distance fast file replication. The Grid Datafarm manages a ranked group
of files as a Gfarm file, each file, called a Gfarm file fragment, being stored
on a filesystem node, or replicated on several filesystem nodes. Each Gfarm
file fragment is replicated independently and in parallel using rate-controlled
HighSpeed TCP with network striping. On a US-Japan testbed with 10,000 km
distance, we achieve 419 Mbps using 2 nodes on each side, and 741 Mbps using 4
nodes out of 893 Mbps with two transpacific networks.
</summary>
    <author>
      <name>Osamu Tatebe</name>
    </author>
    <author>
      <name>Satoshi Sekiguchi</name>
    </author>
    <author>
      <name>Youhei Morita</name>
    </author>
    <author>
      <name>Satoshi Matsuoka</name>
    </author>
    <author>
      <name>Noriyuki Soda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 6 pages, LaTeX, 7 eps figures. PSN
  THAT06</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4; C.2.5; C.4; D.4.3; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307058v1</id>
    <updated>2003-07-25T21:45:15Z</updated>
    <published>2003-07-25T21:45:15Z</published>
    <title>Efficient Instrumentation for Performance Profiling</title>
    <summary>  Performance profiling consists of tracing a software system during execution
and then analyzing the obtained traces. However, traces themselves affect the
performance of the system distorting its execution. Therefore, there is a need
to minimize the effect of the tracing on the underlying system's performance.
To achieve this, the trace set needs to be optimized according to the
performance profiling problem being solved. Our position is that such
minimization can be achieved only by adding the software trace design and
implementation to the overall software development process. In such a process,
the performance analyst supplies the knowledge of performance measurement
requirements, while the software developer supplies the knowledge of the
software. Both of these are needed for an optimal trace placement.
</summary>
    <author>
      <name>Edu Metz</name>
    </author>
    <author>
      <name>Raimondas Lencevicius</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure, published in proceedings of WODA'2003, ICSE
  Workshop on Dynamic Analysis</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0307058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; D.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404001v1</id>
    <updated>2004-04-01T16:54:42Z</updated>
    <published>2004-04-01T16:54:42Z</published>
    <title>On the Practicality of Intrinsic Reconfiguration As a Fault Recovery
  Method in Analog Systems</title>
    <summary>  Evolvable hardware combines the powerful search capability of evolutionary
algorithms with the flexibility of reprogrammable devices, thereby providing a
natural framework for reconfiguration. This framework has generated an interest
in using evolvable hardware for fault-tolerant systems because reconfiguration
can effectively deal with hardware faults whenever it is impossible to provide
spares. But systems cannot tolerate faults indefinitely, which means
reconfiguration does have a deadline. The focus of previous evolvable hardware
research relating to fault-tolerance has been primarily restricted to restoring
functionality, with no real consideration of time constraints. In this paper we
are concerned with evolvable hardware performing reconfiguration under deadline
constraints. In particular, we investigate reconfigurable hardware that
undergoes intrinsic evolution. We show that fault recovery done by intrinsic
reconfiguration has some restrictions, which designers cannot ignore.
</summary>
    <author>
      <name>Garrison W. Greenwood</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0404001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404035v3</id>
    <updated>2005-05-16T21:22:10Z</updated>
    <published>2004-04-16T16:03:05Z</published>
    <title>Elements for Response Time Statistics in ERP Transaction Systems</title>
    <summary>  We present some measurements and ideas for response time statistics in ERP
systems. It is shown that the response time distribution of a given transaction
in a given system is generically a log-normal distribution or, in some
situations, a sum of two or more log-normal distributions. We present some
arguments for this form of the distribution based on heuristic rules for
response times, and we show data from performance measurements in actual
systems to support the log-normal form. Deviations of the log-normal form can
often be traced back to performance problems in the system. Consequences for
the interpretation of response time data and for service level agreements are
discussed.
</summary>
    <author>
      <name>Andreas Mielke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.peva.2005.05.006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.peva.2005.05.006" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">revtex, twocolumn, 8 pages, 13 figures. figures replaced by coloured
  versions</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Performance Evaluation 64, 635-653 (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0404035v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404035v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.8; C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404043v1</id>
    <updated>2004-04-21T20:07:07Z</updated>
    <published>2004-04-21T20:07:07Z</published>
    <title>Benchmarking Blunders and Things That Go Bump in the Night</title>
    <summary>  Benchmarking; by which I mean any computer system that is driven by a
controlled workload, is the ultimate in performance testing and simulation.
Aside from being a form of institutionalized cheating, it also offer countless
opportunities for systematic mistakes in the way the workloads are applied and
the resulting measurements interpreted. Right test, wrong conclusion is a
ubiquitous mistake that happens because test engineers tend to treat data as
divine. Such reverence is not only misplaced, it's also a sure ticket to
production hell when the application finally goes live. I demonstrate how such
mistakes can be avoided by means of two war stories that are real WOPRs. (a)
How to resolve benchmark flaws over the psychic hotline and (b) How benchmarks
can go flat with too much Java juice. In each case I present simple performance
models and show how they can be applied to correctly assess benchmark data.
</summary>
    <author>
      <name>Neil J. Gunther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invited presentation at the Workshop On Software Performance and
  Reliability (WOPR2) Menlo Park, California, April 15-17 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0404043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.1;B.8.2;D.2.5;K.7.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410012v1</id>
    <updated>2004-10-05T22:38:14Z</updated>
    <published>2004-10-05T22:38:14Z</published>
    <title>DiPerF: an automated DIstributed PERformance testing Framework</title>
    <summary>  We present DiPerF, a distributed performance testing framework, aimed at
simplifying and automating service performance evaluation. DiPerF coordinates a
pool of machines that test a target service, collects and aggregates
performance metrics, and generates performance statistics. The aggregate data
collected provide information on service throughput, on service "fairness" when
serving multiple clients concurrently, and on the impact of network latency on
service performance. Furthermore, using this data, it is possible to build
predictive models that estimate a service performance given the service load.
We have tested DiPerF on 100+ machines on two testbeds, Grid3 and PlanetLab,
and explored the performance of job submission services (pre WS GRAM and WS
GRAM) included with Globus Toolkit 3.2.
</summary>
    <author>
      <name>Catalin Dumitrescu</name>
    </author>
    <author>
      <name>Ioan Raicu</name>
    </author>
    <author>
      <name>Matei Ripeanu</name>
    </author>
    <author>
      <name>Ian Foster</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/GRID.2004.21</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/GRID.2004.21" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures, will appear in IEEE/ACM Grid2004, November 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0410012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0511008v1</id>
    <updated>2005-11-02T14:23:48Z</updated>
    <published>2005-11-02T14:23:48Z</published>
    <title>Analysis of Stochastic Service Guarantees in Communication Networks: A
  Basic Calculus</title>
    <summary>  A basic calculus is presented for stochastic service guarantee analysis in
communication networks. Central to the calculus are two definitions,
maximum-(virtual)-backlog-centric (m.b.c) stochastic arrival curve and
stochastic service curve, which respectively generalize arrival curve and
service curve in the deterministic network calculus framework. With m.b.c
stochastic arrival curve and stochastic service curve, various basic results
are derived under the (min, +) algebra for the general case analysis, which are
crucial to the development of stochastic network calculus. These results
include (i) superposition of flows, (ii) concatenation of servers, (iii) output
characterization, (iv) per-flow service under aggregation, and (v) stochastic
backlog and delay guarantees. In addition, to perform independent case
analysis, stochastic strict server is defined, which uses an ideal service
process and an impairment process to characterize a server. The concept of
stochastic strict server not only allows us to improve the basic results (i) --
(v) under the independent case, but also provides a convenient way to find the
stochastic service curve of a serve. Moreover, an approach is introduced to
find the m.b.c stochastic arrival curve of a flow and the stochastic service
curve of a server.
</summary>
    <author>
      <name>Yuming Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0511008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0511008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0601097v2</id>
    <updated>2006-01-24T03:33:32Z</updated>
    <published>2006-01-23T11:47:06Z</published>
    <title>Compression Scheme for Faster and Secure Data Transmission Over Internet</title>
    <summary>  Compression algorithms reduce the redundancy in data representation to
decrease the storage required for that data. Data compression offers an
attractive approach to reducing communication costs by using available
bandwidth effectively. Over the last decade there has been an unprecedented
explosion in the amount of digital data transmitted via the Internet,
representing text, images, video, sound, computer programs, etc. With this
trend expected to continue, it makes sense to pursue research on developing
algorithms that can most effectively use available network bandwidth by
maximally compressing data. It is also important to consider the security
aspects of the data being transmitted while compressing it, as most of the text
data transmitted over the Internet is very much vulnerable to a multitude of
attacks. This paper is focused on addressing this problem of lossless
compression of text files with an added security.
</summary>
    <author>
      <name>B. S. Shajeemohan</name>
    </author>
    <author>
      <name>Dr. V. K. Govindan</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0601097v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0601097v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611087v1</id>
    <updated>2006-11-17T23:43:40Z</updated>
    <published>2006-11-17T23:43:40Z</published>
    <title>A Combined LIFO-Priority Scheme for Overload Control of E-commerce Web
  Servers</title>
    <summary>  E-commerce Web-servers often face overload conditions during which
revenue-generating requests may be dropped or abandoned due to an increase in
the browsing requests. In this paper we present a simple, yet effective,
mechanism for overload control of E-commerce Web-servers. We develop an
E-commerce workload model that separates the browsing requests from
revenue-generating transaction requests. During overload, we apply LIFO
discipline in the browsing queues and use a dynamic priority model to service
them. The transaction queues are given absolute priority over the browsing
queues. This is called the LIFO-Pri scheduling discipline. Experimental results
show that LIFO-Pri dramatically improves the overall Web-server throughput
while also increasing the completion rate of revenue-generating requests. The
Web-server was able to operate at nearly 60% of its maximum capacity even when
offered load was 1.5 times its capacity. Further, when compared to a single
queue FIFO system, there was a seven-fold increase in the number of completed
revenue-generating requests during overload.
</summary>
    <author>
      <name>Naresh Singhmar</name>
    </author>
    <author>
      <name>Vipul Mathur</name>
    </author>
    <author>
      <name>Varsha Apte</name>
    </author>
    <author>
      <name>D. Manjunath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures, presented at the International Infrastructure
  Survivability Workshop (affiliated with the 25th IEEE International Real-Time
  Systems Symposium), Lisbon, Portugal, December 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0611087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612143v1</id>
    <updated>2006-12-28T19:43:26Z</updated>
    <published>2006-12-28T19:43:26Z</published>
    <title>Exact solutions for the two- and all-terminal reliabilities of a simple
  ladder network</title>
    <summary>  The exact calculation of network reliability in a probabilistic context has
been a long-standing issue of practical importance, but a difficult one, even
for planar graphs, with perfect nodes and with edges of identical reliability
p. Many approaches (determination of bounds, sums of disjoint products
algorithms, Monte Carlo evaluations, studies of the reliability polynomials,
etc.) can only provide approximations when the network's size increases. We
consider here a ladder graph of arbitrary size corresponding to real-life
network configurations, and give the exact, analytical solutions for the all-
and two-terminal reliabilities. These solutions use transfer matrices, in which
individual reliabilities of edges and nodes are taken into account. The special
case of identical edge and node reliabilities -- p and rho, respectively -- is
solved. We show that the zeros of the two-terminal reliability polynomial
exhibit structures which differ substantially for seemingly similar networks,
and we compare the sensitivity of various edges. We discuss how the present
work may be further extended to lead to a catalog of exactly solvable networks
in terms of reliability, which could be useful as elementary bricks for a new
and improved set of bounds or benchmarks in the general case.
</summary>
    <author>
      <name>Christian Tanguy</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0612143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.0860v1</id>
    <updated>2007-04-06T08:24:47Z</updated>
    <published>2007-04-06T08:24:47Z</published>
    <title>Availability assessment of SunOS/Solaris Unix Systems based on Syslogd
  and wtmpx logfiles : a case study</title>
    <summary>  This paper presents a measurement-based availability assessment study using
field data collected during a 4-year period from 373 SunOS/Solaris Unix
workstations and servers interconnected through a local area network. We focus
on the estimation of machine uptimes, downtimes and availability based on the
identification of failures that caused total service loss. Data corresponds to
syslogd event logs that contain a large amount of information about the normal
activity of the studied systems as well as their behavior in the presence of
failures. It is widely recognized that the information contained in such event
logs might be incomplete or imperfect. The solution investigated in this paper
to address this problem is based on the use of auxiliary sources of data
obtained from wtmpx files maintained by the SunOS/Solaris Unix operating
system. The results obtained suggest that the combined use of wtmpx and syslogd
log files provides more complete information on the state of the target systems
that is useful to provide availability estimations that better reflect reality.
</summary>
    <author>
      <name>Cristina Simache</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS</arxiv:affiliation>
    </author>
    <author>
      <name>Mohamed Kaaniche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 2005 IEEE Pacific Rim International Symposium on Dependable
  Computing (PRDC'2005), Changsha, Hunan (Chine), 12-14 D{\'e}cembre 2005
  (18/12/2005) 49-56</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0704.0860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.0860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.0865v1</id>
    <updated>2007-04-06T09:33:06Z</updated>
    <published>2007-04-06T09:33:06Z</published>
    <title>An architecture-based dependability modeling framework using AADL</title>
    <summary>  For efficiency reasons, the software system designers' will is to use an
integrated set of methods and tools to describe specifications and designs, and
also to perform analyses such as dependability, schedulability and performance.
AADL (Architecture Analysis and Design Language) has proved to be efficient for
software architecture modeling. In addition, AADL was designed to accommodate
several types of analyses. This paper presents an iterative dependency-driven
approach for dependability modeling using AADL. It is illustrated on a small
example. This approach is part of a complete framework that allows the
generation of dependability analysis and evaluation models from AADL models to
support the analysis of software and system architectures, in critical
application domains.
</summary>
    <author>
      <name>Ana-Elena Rugina</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS</arxiv:affiliation>
    </author>
    <author>
      <name>Karama Kanoun</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS</arxiv:affiliation>
    </author>
    <author>
      <name>Mohamed Kaaniche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 10th IASTED International Conference on Software Engineering
  and Applications (SEA'2006), Dallas (USA), 13-15 November2006 (13/11/2006)
  222-227</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0704.0865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.0865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.2887v1</id>
    <updated>2007-10-15T17:53:49Z</updated>
    <published>2007-10-15T17:53:49Z</published>
    <title>Implementation, Compilation, Optimization of Object-Oriented Languages,
  Programs and Systems - Report on the Workshop ICOOOLPS'2006 at ECOOP'06</title>
    <summary>  ICOOOLPS'2006 was the first edition of ECOOP-ICOOOLPS workshop. It intended
to bring researchers and practitioners both from academia and industry
together, with a spirit of openness, to try and identify and begin to address
the numerous and very varied issues of optimization. This succeeded, as can be
seen from the papers, the attendance and the liveliness of the discussions that
took place during and after the workshop, not to mention a few new cooperations
or postdoctoral contracts. The 22 talented people from different groups who
participated were unanimous to appreciate this first edition and recommend that
ICOOOLPS be continued next year. A community is thus beginning to form, and
should be reinforced by a second edition next year, with all the improvements
this first edition made emerge.
</summary>
    <author>
      <name>Roland Ducournau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIRMM</arxiv:affiliation>
    </author>
    <author>
      <name>Etienne Gagnon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">RACE LAB</arxiv:affiliation>
    </author>
    <author>
      <name>Chandra Krintz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">RACE LAB</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Mulet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">S3L</arxiv:affiliation>
    </author>
    <author>
      <name>Jan Vitek</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">S3L</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Zendra</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-540-71774-4_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-540-71774-4_1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The original publication is available at http://www.springerlink.com</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Object-Oriented Technology. ECOOP 2006 Workshop Reader - ECOOP
  2006 Workshops, Nantes, France, July 3-7, 2006, Final Reports Springer Berlin
  / Heidelberg (Ed.) (2007) 1-14</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.2887v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.2887v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4701v1</id>
    <updated>2007-10-25T09:27:03Z</updated>
    <published>2007-10-25T09:27:03Z</published>
    <title>A Prediction Packetizing Scheme for Reducing Channel Traffic in
  Transaction-Level Hardware/Software Co-Emulation</title>
    <summary>  This paper presents a scheme for efficient channel usage between simulator
and accelerator where the accelerator models some RTL sub-blocks in the
accelerator-based hardware/software co-simulation while the simulator runs
transaction-level model of the remaining part of the whole chip being verified.
With conventional simulation accelerator, evaluations of simulator and
accelerator alternate at every valid simulation time, which results in poor
simulation performance due to startup overhead of simulator-accelerator channel
access. The startup overhead can be reduced by merging multiple transactions on
the channel into a single burst traffic. We propose a predictive packetizing
scheme for reducing channel traffic by merging as many transactions into a
burst traffic as possible based on 'prediction and rollback.' Under ideal
condition with 100% prediction accuracy, the proposed method shows a
performance gain of 1500% compared to the conventional one.
</summary>
    <author>
      <name>Jae-Gon Lee</name>
    </author>
    <author>
      <name>Moo-Kyoung Chung</name>
    </author>
    <author>
      <name>Ki-Yong Ahn</name>
    </author>
    <author>
      <name>Sang-Heon Lee</name>
    </author>
    <author>
      <name>Chong-Min Kyung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4723v1</id>
    <updated>2007-10-25T09:37:18Z</updated>
    <published>2007-10-25T09:37:18Z</published>
    <title>Simulation Methodology for Analysis of Substrate Noise Impact on Analog
  / RF Circuits Including Interconnect Resistance</title>
    <summary>  This paper reports a novel simulation methodology for analysis and prediction
of substrate noise impact on analog / RF circuits taking into account the role
of the parasitic resistance of the on-chip interconnect in the impact
mechanism. This methodology allows investigation of the role of the separate
devices (also parasitic devices) in the analog / RF circuit in the overall
impact. This way is revealed which devices have to be taken care of (shielding,
topology change) to protect the circuit against substrate noise. The developed
methodology is used to analyze impact of substrate noise on a 3 GHz LC-tank
Voltage Controlled Oscillator (VCO) designed in a high-ohmic 0.18 $\mu$m 1PM6
CMOS technology. For this VCO (in the investigated frequency range from DC to
15 MHz) impact is mainly caused by resistive coupling of noise from the
substrate to the non-ideal on-chip ground interconnect, resulting in analog
ground bounce and frequency modulation. Hence, the presented test-case reveals
the important role of the on-chip interconnect in the phenomenon of substrate
noise impact.
</summary>
    <author>
      <name>C. Soens</name>
    </author>
    <author>
      <name>G. Van Der Plas</name>
    </author>
    <author>
      <name>P. Wambacq</name>
    </author>
    <author>
      <name>S. Donnay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.0811v2</id>
    <updated>2007-12-19T08:05:54Z</updated>
    <published>2007-12-05T19:55:16Z</published>
    <title>The Fast Fibonacci Decompression Algorithm</title>
    <summary>  Data compression has been widely applied in many data processing areas.
Compression methods use variable-size codes with the shorter codes assigned to
symbols or groups of symbols that appear in the data frequently. Fibonacci
coding, as a representative of these codes, is used for compressing small
numbers. Time consumption of a decompression algorithm is not usually as
important as the time of a compression algorithm. However, efficiency of the
decompression may be a critical issue in some cases. For example, a real-time
compression of tree data structures follows this issue. Tree's pages are
decompressed during every reading from a secondary storage into the main
memory. In this case, the efficiency of a decompression algorithm is extremely
important. We have developed a Fast Fibonacci decompression for this purpose.
Our approach is up to $3.5\times$ faster than the original implementation.
</summary>
    <author>
      <name>R. Baca</name>
    </author>
    <author>
      <name>V. Snasel</name>
    </author>
    <author>
      <name>J. Platos</name>
    </author>
    <author>
      <name>M. Kratky</name>
    </author>
    <author>
      <name>E. El-Qawasmeh</name>
    </author>
    <link href="http://arxiv.org/abs/0712.0811v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.0811v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.2949v1</id>
    <updated>2008-05-19T19:45:31Z</updated>
    <published>2008-05-19T19:45:31Z</published>
    <title>Performability Aspects of the Atlas Vo; Using Lmbench Suite</title>
    <summary>  The ATLAS Virtual Organization is grid's largest Virtual Organization which
is currently in full production stage. Hereby a case is being made that a user
working within that VO is going to face a wide spectrum of different systems,
whose heterogeneity is enough to count as "orders of magnitude" according to a
number of metrics; including integer/float operations, memory throughput
(STREAM) and communication latencies. Furthermore, the spread of performance
does not appear to follow any known distribution pattern, which is demonstrated
in graphs produced during May 2007 measurements. It is implied that the current
practice where either "all-WNs-are-equal" or, the alternative of SPEC-based
rating used by LCG/EGEE is an oversimplification which is inappropriate and
expensive from an operational point of view, therefore new techniques are
needed for optimal grid resources allocation.
</summary>
    <author>
      <name>Fotis Georgatos</name>
    </author>
    <author>
      <name>John Kouvakis</name>
    </author>
    <author>
      <name>John Kouretis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 7 figures; Particular attention should be given to graph
  int64mul; A related presentation on the subject matter is available by the
  authors upon request</arxiv:comment>
    <link href="http://arxiv.org/abs/0805.2949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.2949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.0626v1</id>
    <updated>2008-07-03T19:44:36Z</updated>
    <published>2008-07-03T19:44:36Z</published>
    <title>Asymptotic Mean Time To Failure and Higher Moments for Large, Recursive
  Networks</title>
    <summary>  This paper deals with asymptotic expressions of the Mean Time To Failure
(MTTF) and higher moments for large, recursive, and non-repairable systems in
the context of two-terminal reliability. Our aim is to extend the well-known
results of the series and parallel cases. We first consider several exactly
solvable configurations of identical components with exponential failure-time
distribution functions to illustrate different (logarithmic or power-law)
behaviors as the size of the system, indexed by an integer n, increases. The
general case is then addressed: it provides a simple interpretation of the
origin of the power-law exponent and an efficient asymptotic expression for the
total reliability of large, recursive systems. Finally, we assess the influence
of the non-exponential character of the component reliability on the
n-dependence of the MTTF.
</summary>
    <author>
      <name>Christian Tanguy</name>
    </author>
    <link href="http://arxiv.org/abs/0807.0626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.0626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.0993v1</id>
    <updated>2008-07-07T11:20:39Z</updated>
    <published>2008-07-07T11:20:39Z</published>
    <title>WCET analysis of multi-level set-associative instruction caches</title>
    <summary>  With the advent of increasingly complex hardware in real-time embedded
systems (processors with performance enhancing features such as pipelines,
cache hierarchy, multiple cores), many processors now have a set-associative L2
cache. Thus, there is a need for considering cache hierarchies when validating
the temporal behavior of real-time systems, in particular when estimating
tasks' worst-case execution times (WCETs). To the best of our knowledge, there
is only one approach for WCET estimation for systems with cache hierarchies
[Mueller, 1997], which turns out to be unsafe for set-associative caches. In
this paper, we highlight the conditions under which the approach described in
[Mueller, 1997] is unsafe. A safe static instruction cache analysis method is
then presented. Contrary to [Mueller, 1997] our method supports set-associative
and fully associative caches. The proposed method is experimented on
medium-size and large programs. We show that the method is most of the time
tight. We further show that in all cases WCET estimations are much tighter when
considering the cache hierarchy than when considering only the L1 cache. An
evaluation of the analysis time is conducted, demonstrating that analysing the
cache hierarchy has a reasonable computation time.
</summary>
    <author>
      <name>Damien Hardy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRISA</arxiv:affiliation>
    </author>
    <author>
      <name>Isabelle Puaut</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRISA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0807.0993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.0993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.1162v1</id>
    <updated>2008-07-08T03:51:17Z</updated>
    <published>2008-07-08T03:51:17Z</published>
    <title>Measurement and Evaluation of ENUM Server Performance</title>
    <summary>  ENUM is a DNS-based protocol standard for mapping E.164 telephone numbers to
Internet Uniform Resource Identifiers (URIs). It places unique requirements on
the existing DNS infrastructure, such as data scalability, query throughput,
response time, and database update rates. This paper measures and evaluates the
performance of existing name server implementation as ENUM servers. We compared
PowerDNS (PDNS), BIND and Navitas. Results show that BIND is not suitable for
ENUM due to its poor scaling property. Both PDNS and Navitas can serve ENUM.
However, Navitas turns out to be highly optimized and clearly outperforms PDNS
in all aspects we have tested. We also instrumented the PDNS server to identify
its performance bottleneck and investigated ways to improve it.
</summary>
    <author>
      <name>Charles Shen</name>
    </author>
    <author>
      <name>Henning Schulzrinne</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of IEEE ICC 2007 p. 1967-1972</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0807.1162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.1162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.6.2; D.4.8; C.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.1228v1</id>
    <updated>2008-07-08T13:04:05Z</updated>
    <published>2008-07-08T13:04:05Z</published>
    <title>Restricted Mobility Improves Delay-Throughput Trade-offs in Mobile
  Ad-Hoc Networks</title>
    <summary>  In this paper, we analyze asymptotic delay-throughput trade-offs in mobile
ad-hoc networks comprising heterogeneous nodes with restricted mobility. We
show that node spatial heterogeneity has the ability to drastically improve
upon existing scaling laws established under the assumption that nodes are
identical and uniformly visit the entire network area. In particular, we
consider the situation in which each node moves around its own home-point
according to a restricted mobility process which results into a spatial
stationary distribution that decays as a power law of exponent delta with the
distance from the home-point. For such restricted mobility model, we propose a
novel class of scheduling and routing schemes, which significantly outperforms
all delay-throughput results previously obtained in the case of identical
nodes. In particular, for delta = 2 it is possible to achieve almost constant
delay and almost constant per-node throughput (except for a poly-logarithmic
factor) as the number of nodes increases, even without resorting to
sophisticated coding or signal processing techniques.
</summary>
    <author>
      <name>Michele Garetto</name>
    </author>
    <author>
      <name>Emilio Leonardi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0807.1228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.1228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.1431v2</id>
    <updated>2008-08-25T16:20:42Z</updated>
    <published>2008-08-11T00:06:16Z</published>
    <title>A General Theory of Computational Scalability Based on Rational
  Functions</title>
    <summary>  The universal scalability law of computational capacity is a rational
function C_p = P(p)/Q(p) with P(p) a linear polynomial and Q(p) a second-degree
polynomial in the number of physical processors p, that has been long used for
statistical modeling and prediction of computer system performance. We prove
that C_p is equivalent to the synchronous throughput bound for a
machine-repairman with state-dependent service rate. Simpler rational
functions, such as Amdahl's law and Gustafson speedup, are corollaries of this
queue-theoretic bound. C_p is further shown to be both necessary and sufficient
for modeling all practical characteristics of computational scalability.
</summary>
    <author>
      <name>Neil J. Gunther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures; several typos corrected, 1 reference updated,
  page number reduced with 10 pt font</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.1431v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.1431v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8; C.4; C.5.5; D.4.8; F.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.2541v1</id>
    <updated>2008-09-15T14:51:04Z</updated>
    <published>2008-09-15T14:51:04Z</published>
    <title>Getting in the Zone for Successful Scalability</title>
    <summary>  The universal scalability law (USL) is an analytic model used to quantify
application scaling. It is universal because it subsumes Amdahl's law and
Gustafson linearized scaling as special cases. Using simulation, we show: (i)
that the USL is equivalent to synchronous queueing in a load-dependent machine
repairman model and (ii) how USL, Amdahl's law, and Gustafson scaling can be
regarded as boundaries defining three scalability zones. Typical throughput
measurements lie across all three zones. Simulation scenarios provide deeper
insight into queueing effects and thus provide a clearer indication of which
application features should be tuned to get into the optimal performance zone.
</summary>
    <author>
      <name>Jim Holtman</name>
    </author>
    <author>
      <name>Neil J. Gunther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 15 figures. To appear in Proc. CMG International
  Conference, Las Vegas, Nevada, December 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.2541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.2541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8; C.4; C.5.5; D.4.8; F.1.2; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.0394v1</id>
    <updated>2008-10-02T11:44:47Z</updated>
    <published>2008-10-02T11:44:47Z</published>
    <title>Mobility Management Framework</title>
    <summary>  This paper investigates mobility management strategies from the point of view
of their need of signalling and processing resources on the backbone network
and load on the air interface. A method is proposed to model the serving
network and mobile node mobility in order to be able to compare the different
types of mobility management algorithms. To obtain a good description of the
network we calculate descriptive parameters from given topologies. Most
mobility approaches derived from existing protocols are analyzed and their
performances are numerically compared in various network and mobility
scenarios. We developed a mobility management framework that is able to give
general designing guidelines for the next generation mobility managements on
given network, technology and mobility properties. With our model an operator
can design the network and tune the parameters to obtain the optimal
implementation of course revising existing systems is also possible. We present
a vertical handover decision method as a special application of our model
framework.
</summary>
    <author>
      <name>Peter Fulop</name>
    </author>
    <author>
      <name>Benedek Kovacs</name>
    </author>
    <author>
      <name>Sandor Imre</name>
    </author>
    <link href="http://arxiv.org/abs/0810.0394v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.0394v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.1151v1</id>
    <updated>2008-11-07T14:33:10Z</updated>
    <published>2008-11-07T14:33:10Z</published>
    <title>A Model for Probabilistic Reasoning on Assume/Guarantee Contracts</title>
    <summary>  In this paper, we present a probabilistic adaptation of an Assume/Guarantee
contract formalism. For the sake of generality, we assume that the extended
state machines used in the contracts and implementations define sets of runs on
a given set of variables, that compose by intersection over the common
variables. In order to enable probabilistic reasoning, we consider that the
contracts dictate how certain input variables will behave, being either
non-deterministic, or probabilistic; the introduction of probabilistic
variables leading us to tune the notions of implementation, refinement and
composition. As shown in the report, this probabilistic adaptation of the
Assume/Guarantee contract theory preserves compositionality and therefore
allows modular reliability analysis, either with a top-down or a bottom-up
approach.
</summary>
    <author>
      <name>Benoît Delahaye</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRISA</arxiv:affiliation>
    </author>
    <author>
      <name>Benoît Caillaud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRISA, Irisa, Irisa</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0811.1151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.1151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.3712v1</id>
    <updated>2008-11-22T21:29:47Z</updated>
    <published>2008-11-22T21:29:47Z</published>
    <title>Performance Modeling and Evaluation for Information-Driven Networks</title>
    <summary>  Information-driven networks include a large category of networking systems,
where network nodes are aware of information delivered and thus can not only
forward data packets but may also perform information processing. In many
situations, the quality of service (QoS) in information-driven networks is
provisioned with the redundancy in information. Traditional performance models
generally adopt evaluation measures suitable for packet-oriented service
guarantee, such as packet delay, throughput, and packet loss rate. These
performance measures, however, do not align well with the actual need of
information-driven networks. New performance measures and models for
information-driven networks, despite their importance, have been mainly blank,
largely because information processing is clearly application dependent and
cannot be easily captured within a generic framework. To fill the vacancy, we
present a new performance evaluation framework particularly tailored for
information-driven networks, based on the recent development of stochastic
network calculus. We analyze the QoS with respect to information delivery and
study the scheduling problem with the new performance metrics. Our analytical
framework can be used to calculate the network capacity in information delivery
and in the meantime to help transmission scheduling for a large body of systems
where QoS is stochastically guaranteed with the redundancy in information.
</summary>
    <author>
      <name>Kui Wu</name>
    </author>
    <author>
      <name>Yuming Jiang</name>
    </author>
    <author>
      <name>Guoqiang Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0811.3712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.3712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; H.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.1884v1</id>
    <updated>2009-02-11T13:51:27Z</updated>
    <published>2009-02-11T13:51:27Z</published>
    <title>A Proof of Concept for Optimizing Task Parallelism by Locality Queues</title>
    <summary>  Task parallelism as employed by the OpenMP task construct, although ideal for
tackling irregular problems or typical producer/consumer schemes, bears some
potential for performance bottlenecks if locality of data access is important,
which is typically the case for memory-bound code on ccNUMA systems. We present
a programming technique which ameliorates adverse effects of dynamic task
distribution by sorting tasks into locality queues, each of which is preferably
processed by threads that belong to the same locality domain. Dynamic
scheduling is fully preserved inside each domain, and is preferred over
possible load imbalance even if non-local access is required. The effectiveness
of the approach is demonstrated using a blocked six-point stencil solver as a
toy model.
</summary>
    <author>
      <name>Markus Wittmann</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0902.1884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.1884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.3065v1</id>
    <updated>2009-02-18T07:48:34Z</updated>
    <published>2009-02-18T07:48:34Z</published>
    <title>The Multi-Branched Method of Moments for Queueing Networks</title>
    <summary>  We propose a new exact solution algorithm for closed multiclass product-form
queueing networks that is several orders of magnitude faster and less memory
consuming than established methods for multiclass models, such as the Mean
Value Analysis (MVA) algorithm. The technique is an important generalization of
the recently proposed Method of Moments (MoM) which, differently from MVA,
recursively computes higher-order moments of queue-lengths instead of mean
values.
  The main contribution of this paper is to prove that the information used in
the MoM recursion can be increased by considering multiple recursive branches
that evaluate models with different number of queues. This reformulation allows
to formulate a simpler matrix difference equation which leads to large
computational savings with respect to the original MoM recursion. Computational
analysis shows several cases where the proposed algorithm is between 1,000 and
10,000 times faster and less memory consuming than the original MoM, thus
extending the range of multiclass models where exact solutions are feasible.
</summary>
    <author>
      <name>Giuliano Casale</name>
    </author>
    <link href="http://arxiv.org/abs/0902.3065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.3065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.0771v1</id>
    <updated>2009-04-05T13:38:07Z</updated>
    <published>2009-04-05T13:38:07Z</published>
    <title>Effect of cell residence time variance on the performance of an advanced
  paging algorithm</title>
    <summary>  The use of advanced sequential paging algorithms has been suggested as a
means to reduce the signaling cost in future mobile cellular networks. In a
proposed algorithm (Koukoutsidis and Theologou, 2003), the system can use the
additional information of the last interaction cell combined with a mobility
model to predict the short-term location probabilities at the time of an
incoming call arrival. The short-term location probabilities reduce the
uncertainty in mobile user position and thus greatly improve the search. In
this paper, an analytical model is derived that allows for a general
distribution of cell residence times. By considering a Gamma distribution, we
study the effect of the variance of cell residence times and derive useful
results on the performance of the algorithm.
</summary>
    <author>
      <name>Ioannis Koukoutsidis</name>
    </author>
    <author>
      <name>Petros Papaioannou</name>
    </author>
    <author>
      <name>Michael E. Theologou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0904.0771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.0771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.1680v1</id>
    <updated>2009-06-09T09:54:49Z</updated>
    <published>2009-06-09T09:54:49Z</published>
    <title>Methodology for assessing system performance loss within a proactive
  maintenance framework</title>
    <summary>  Maintenance plays now a critical role in manufacturing for achieving
important cost savings and competitive advantage while preserving product
conditions. It suggests moving from conventional maintenance practices to
predictive strategy. Indeed the maintenance action has to be done at the right
time based on the system performance and component Remaining Useful Life (RUL)
assessed by a prognostic process. In that way, this paper proposes a
methodology in order to evaluate the performance loss of the system according
to the degradation of component and the deviations of system input flows. This
methodology is supported by the neuro-fuzzy tool ANFIS (Adaptive Neuro-Fuzzy
Inference Systems) that allows to integrate knowledge from two different
sources: expertise and real data. The feasibility and added value of such
methodology is then highlighted through an application case extracted from the
TELMA platform used for education and research.
</summary>
    <author>
      <name>Pierre Cocheteux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <author>
      <name>Alexandre Voisin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Levrat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <author>
      <name>Benoît Iung</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">13th IFAC Symposium on Information Control Problems in
  Manufacturing, INCOM'09, Moscow : Russie (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0906.1680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.1680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.1057v1</id>
    <updated>2009-08-07T13:57:15Z</updated>
    <published>2009-08-07T13:57:15Z</published>
    <title>Transmission Performance Analysis of Digital Wire and Wireless Optical
  Links in Local and Wide Areas Optical Networks</title>
    <summary>  In the present paper, the transmission performance analysis of digital wire
and wireless optical links in local and wide areas optical networks have been
modeled and parametrically investigated over wide range of the affecting
parameters. Moreover, we have analyzed the basic equations of the comparative
study of the performance of digital fiber optic links with wire and wireless
optical links. The development of optical wireless communication systems is
accelerating as a high cost effective to wire fiber optic links. The optical
wireless technology is used mostly in wide bandwidth data transmission
applications. Finally, we have investigated the maximum transmission distance
and data transmission bit rates that can be achieved within digital wire and
wireless optical links for local and wide areas optical network applications.
</summary>
    <author>
      <name>Abd El Naser A. Mohamed</name>
    </author>
    <author>
      <name>Mohamed M. E. El Halawany</name>
    </author>
    <author>
      <name>Ahmed Nabih Zaki Rashed</name>
    </author>
    <author>
      <name>Amina E. M. El Nabawy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages IEEE Format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Internation Journal of Computer Science and Information Security,
  IJCSIS, Vol. 3, No.1, July 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0908.1057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.1057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.4256v1</id>
    <updated>2009-08-28T18:32:22Z</updated>
    <published>2009-08-28T18:32:22Z</published>
    <title>Experimental Performances Analysis of Load Balancing Algorithms in IEEE
  802.11</title>
    <summary>  In IEEE 802.11, load balancing algorithms (LBA) consider only the associated
stations to balance the load of the available access points (APs). However,
although the APs are balanced, it causes a bad situation if the AP has a lower
signal length (SNR) less than the neighbor APs. So, balance the load and
associate one mobile station to an access point without care about the signal
to noise ratio (SNR) of the AP cause possibly an unforeseen QoS, such as the
bit rate, the end to end delay, the packet loss. In this way, we study an
improvement load balancing algorithm with SNR integration at the selection
policy.
</summary>
    <author>
      <name>Hamdi Salah</name>
    </author>
    <author>
      <name>Soudani Adel</name>
    </author>
    <author>
      <name>Tourki Rached</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact factor
  0.423,http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 4, No. 1 &amp; 2., August 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0908.4256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.4256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1780v1</id>
    <updated>2009-09-09T18:10:26Z</updated>
    <published>2009-09-09T18:10:26Z</published>
    <title>uFLIP: Understanding Flash IO Patterns</title>
    <summary>  Does the advent of flash devices constitute a radical change for secondary
storage? How should database systems adapt to this new form of secondary
storage? Before we can answer these questions, we need to fully understand the
performance characteristics of flash devices. More specifically, we want to
establish what kind of IOs should be favored (or avoided) when designing
algorithms and architectures for flash-based systems. In this paper, we focus
on flash IO patterns, that capture relevant distribution of IOs in time and
space, and our goal is to quantify their performance. We define uFLIP, a
benchmark for measuring the response time of flash IO patterns. We also present
a benchmarking methodology which takes into account the particular
characteristics of flash devices. Finally, we present the results obtained by
measuring eleven flash devices, and derive a set of design hints that should
drive the development of flash-based systems on current devices.
</summary>
    <author>
      <name>Luc Bouganim</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA</arxiv:affiliation>
    </author>
    <author>
      <name>Björn Jónsson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Reykjavík University</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Bonnet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer Science, University of Copenhagen</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIDR 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.1780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.4836v1</id>
    <updated>2009-10-26T09:31:38Z</updated>
    <published>2009-10-26T09:31:38Z</published>
    <title>Performance limitations for sparse matrix-vector multiplications on
  current multicore environments</title>
    <summary>  The increasing importance of multicore processors calls for a reevaluation of
established numerical algorithms in view of their ability to profit from this
new hardware concept. In order to optimize the existent algorithms, a detailed
knowledge of the different performance-limiting factors is mandatory. In this
contribution we investigate sparse matrix-vector multiplication, which is the
dominant operation in many sparse eigenvalue solvers. Two conceptually
different storage schemes and computational kernels have been conceived in the
past to target cache-based and vector architectures, respectively. Starting
from a series of microbenchmarks we apply the gained insight on optimized
sparse MVM implementations, whose serial and OpenMP-parallel performance we
review on state-of-the-art multicore systems.
</summary>
    <author>
      <name>Gerald Schubert</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Holger Fehske</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-13872-0_2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-13872-0_2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">High Performance Computing in Science and Engineering,
  Garching/Munich 2009. Springer, (2010), 13-26</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0910.4836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.4836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.3852v1</id>
    <updated>2009-12-19T01:18:05Z</updated>
    <published>2009-12-19T01:18:05Z</published>
    <title>Sharp utilization thresholds for some real-time scheduling problems</title>
    <summary>  Scheduling policies for real-time systems exhibit threshold behavior that is
related to the utilization of the task set they schedule, and in some cases
this threshold is sharp. For the rate monotonic scheduling policy, we show that
periodic workload with utilization less than a threshold $U_{RM}^{*}$ can be
scheduled almost surely and that all workload with utilization greater than
$U_{RM}^{*}$ is almost surely not schedulable. We study such sharp threshold
behavior in the context of processor scheduling using static task priorities,
not only for periodic real-time tasks but for aperiodic real-time tasks as
well. The notion of a utilization threshold provides a simple schedulability
test for most real-time applications. These results improve our understanding
of scheduling policies and provide an interesting characterization of the
typical behavior of policies. The threshold is sharp (small deviations around
the threshold cause schedulability, as a property, to appear or disappear) for
most policies; this is a happy consequence that can be used to address the
limitations of existing utilization-based tests for schedulability. We
demonstrate the use of such an approach for balancing power consumption with
the need to meet deadlines in web servers.
</summary>
    <author>
      <name>Sathish Gopalakrishnan</name>
    </author>
    <link href="http://arxiv.org/abs/0912.3852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.3852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.1860v2</id>
    <updated>2010-02-01T13:37:17Z</updated>
    <published>2010-01-12T12:08:53Z</published>
    <title>OMI4papps: Optimisation, Modelling and Implementation for Highly
  Parallel Applications</title>
    <summary>  This article reports on first results of the KONWIHR-II project OMI4papps at
the Leibniz Supercomputing Centre (LRZ). The first part describes Apex-MAP, a
tunable synthetic benchmark designed to simulate the performance of typical
scientific applications. Apex-MAP mimics common memory access patterns and
different computational intensity of scientific codes. An approach for
modelling LRZ's application mix is given whichh makes use of performance
counter measurements of real applications running on "HLRB II", an SGI Altix
system based on 9728 Intel Montecito dual-cores.
  The second part will show how the Apex-MAP benchmark could be used to
simulate the performance of two mathematical kernels frequently used in
scientific applications: a dense matrix-matrix multiplication and a sparse
matrix-vector multiplication. The performance of both kernels has been
intensively studied on x86 cores and hardware accelerators. We will compare the
predicted performance with measured data to validate our Apex-MAP approach.
</summary>
    <author>
      <name>Volker Weinberg</name>
    </author>
    <author>
      <name>Matthias Brehm</name>
    </author>
    <author>
      <name>Iris Christadler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures, talk given at the HLRB, KONWIHR and
  Linux-Cluster Review and Results Workshop, Garching b. Muenchen, Germany, 8-9
  Dec 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.1860v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.1860v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.1902v2</id>
    <updated>2010-02-19T16:55:00Z</updated>
    <published>2010-01-12T18:11:23Z</published>
    <title>RapidMind: Portability across Architectures and its Limitations</title>
    <summary>  Recently, hybrid architectures using accelerators like GPGPUs or the Cell
processor have gained much interest in the HPC community. The RapidMind
Multi-Core Development Platform is a programming environment that allows
generating code which is able to seamlessly run on hardware accelerators like
GPUs or the Cell processor and multicore CPUs both from AMD and Intel. This
paper describes the ports of three mathematical kernels to RapidMind which are
chosen as synthetic benchmarks and representatives of scientific codes.
Performance of these kernels has been measured on various RapidMind backends
(cuda, cell and x86) and compared to other hardware-specific implementations
(using CUDA, Cell SDK and Intel MKL). The results give an insight in the degree
of portability of RapidMind code and code performance across different
architectures.
</summary>
    <author>
      <name>Iris Christadler</name>
    </author>
    <author>
      <name>Volker Weinberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.1902v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.1902v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3756v1</id>
    <updated>2010-01-21T10:18:43Z</updated>
    <published>2010-01-21T10:18:43Z</published>
    <title>Fault Tolerant Real Time Systems</title>
    <summary>  Real time systems are systems in which there is a commitment for timely
response by the computer to external stimuli. Real time applications have to
function correctly even in presence of faults. Fault tolerance can be achieved
by either hardware or software or time redundancy. Safety-critical applications
have strict time and cost constraints, which means that not only faults have to
be tolerated but also the constraints should be satisfied. Deadline scheduling
means that the taskwith the earliest required response time is processed. The
most common scheduling algorithms are :Rate Monotonic(RM) and Earliest deadline
first(EDF).This paper deals with the interaction between the fault tolerant
strategy and the EDF real time scheduling strategy.
</summary>
    <author>
      <name>A. Christy Persya</name>
    </author>
    <author>
      <name>T. R. Gopalakrishnan Nair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Next Generation Software Application,
  pp 177-180, 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.3756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.1154v1</id>
    <updated>2010-02-05T08:51:44Z</updated>
    <published>2010-02-05T08:51:44Z</published>
    <title>Performance Analysis of Software to Hardware Task Migration in Codesign</title>
    <summary>  The complexity of multimedia applications in terms of intensity of
computation and heterogeneity of treated data led the designers to embark them
on multiprocessor systems on chip. The complexity of these systems on one hand
and the expectations of the consumers on the other hand complicate the
designers job to conceive and supply strong and successful systems in the
shortest deadlines. They have to explore the different solutions of the design
space and estimate their performances in order to deduce the solution that
respects their design constraints. In this context, we propose the modeling of
one of the design space possible solutions: the software to hardware task
migration. This modeling exploits the synchronous dataflow graphs to take into
account the different migration impacts and estimate their performances in
terms of throughput.
</summary>
    <author>
      <name>Dorsaf Sebai</name>
    </author>
    <author>
      <name>Abderrazak Jemai</name>
    </author>
    <author>
      <name>Imed Bennour</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 1, January 2010,
  http://ijcsi.org/articles/Performance-Analysis-of-Software-to-Hardware-Task-Migration-in-Codesign.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 1, January 2010,
  http://ijcsi.org/articles/Performance-Analysis-of-Software-to-Hardware-Task-Migration-in-Codesign.php</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.1154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.1154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.2450v1</id>
    <updated>2010-02-11T23:20:40Z</updated>
    <published>2010-02-11T23:20:40Z</published>
    <title>Modeling the Probability of Failure on LDAP Binding Operations in
  Iplanet Web Proxy 3.6 Server</title>
    <summary>  This paper is devoted to the theoretical analysis of a problem derived from
interaction between two Iplanet products: Web Proxy Server and the Directory
Server. In particular, a probabilistic and stochastic-approximation model is
proposed to minimize the occurrence of LDAP connection failures in Iplanet Web
Proxy 3.6 Server. The proposed model serves not only to provide a
parameterization of the aforementioned phenomena, but also to provide
meaningful insights illustrating and supporting these theoretical results. In
addition, we shall also address practical considerations when estimating the
parameters of the proposed model from experimental data. Finally, we shall
provide some interesting results from real-world data collected from our
customers.
</summary>
    <author>
      <name>Alejandro Chinea Manrique de Lara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, Published in Sun MicroSystems Laboratories April
  2002</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.2450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.2450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4064v1</id>
    <updated>2010-03-22T05:17:46Z</updated>
    <published>2010-03-22T05:17:46Z</published>
    <title>Measuring Bandwidth for Super Computer Workloads</title>
    <summary>  Parallel computing plays a major role in almost all the fields from research
to major concern problem solving purposes. Many researches are till now
focusing towards the area of parallel processing. Nowadays it extends its usage
towards the end user application such as GPU as well as multi-core processor
development. The bandwidth measurement is essential for resource management and
for studying the various performance factors of the existing super computer
systems which will be helpful for better system utilization since super
computers are very few and their resources should be properly utilized. In this
paper the real workload trace of one of the super computers LANL is taken and
shown how the bandwidth is estimated with the given parameters.
</summary>
    <author>
      <name>A. Neela Madheswari</name>
    </author>
    <author>
      <name>R. S. D. Wahida Banu</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 2, Issue 3, March 2010,
  https://sites.google.com/site/journalofcomputing/</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.1680v1</id>
    <updated>2010-04-10T04:14:15Z</updated>
    <published>2010-04-10T04:14:15Z</published>
    <title>Magnetohydrodynamics on Heterogeneous architectures: a performance
  comparison</title>
    <summary>  We present magneto-hydrodynamic simulation results for heterogeneous systems.
Heterogeneous architectures combine high floating point performance many-core
units hosted in conventional server nodes. Examples include Graphics Processing
Units (GPU's) and Cell. They have potentially large gains in performance, at
modest power and monetary cost. We implemented a magneto-hydrodynamic (MHD)
simulation code on a variety of heterogeneous and multi-core architectures ---
multi-core x86, Cell, Nvidia and ATI GPU --- in different languages, FORTRAN,
C, Cell, CUDA and OpenCL. We present initial performance results for these
systems. To our knowledge, this is the widest comparison of heterogeneous
systems for MHD simulations. We review the different challenges faced in each
architecture, and potential bottlenecks. We conclude that substantial gains in
performance over traditional systems are possible, and in particular that is
possible to extract a greater percentage of peak theoretical performance from
some systems when compared to x86 architectures.
</summary>
    <author>
      <name>Bijia Pang</name>
    </author>
    <author>
      <name>Ue-li Pen</name>
    </author>
    <author>
      <name>Michael Perrone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.1680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.1680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.2637v1</id>
    <updated>2010-04-15T14:31:32Z</updated>
    <published>2010-04-15T14:31:32Z</published>
    <title>Performance Evaluation of Components Using a Granularity-based Interface
  Between Real-Time Calculus and Timed Automata</title>
    <summary>  To analyze complex and heterogeneous real-time embedded systems, recent works
have proposed interface techniques between real-time calculus (RTC) and timed
automata (TA), in order to take advantage of the strengths of each technique
for analyzing various components. But the time to analyze a state-based
component modeled by TA may be prohibitively high, due to the state space
explosion problem. In this paper, we propose a framework of granularity-based
interfacing to speed up the analysis of a TA modeled component. First, we
abstract fine models to work with event streams at coarse granularity. We
perform analysis of the component at multiple coarse granularities and then
based on RTC theory, we derive lower and upper bounds on arrival patterns of
the fine output streams using the causality closure algorithm. Our framework
can help to achieve tradeoffs between precision and analysis time.
</summary>
    <author>
      <name>Karine Altisen</name>
    </author>
    <author>
      <name>Yanhong Liu</name>
    </author>
    <author>
      <name>Matthieu Moy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">QAPL 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.2637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.2637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3109v1</id>
    <updated>2010-04-19T07:13:00Z</updated>
    <published>2010-04-19T07:13:00Z</published>
    <title>Applying Stochastic Network Calculus to 802.11 Backlog and Delay
  Analysis</title>
    <summary>  Stochastic network calculus provides an elegant way to characterize traffic
and service processes. However, little effort has been made on applying it to
multi-access communication systems such as 802.11. In this paper, we take the
first step to apply it to the backlog and delay analysis of an 802.11 wireless
local network. In particular, we address the following questions: In applying
stochastic network calculus, under what situations can we derive stable backlog
and delay bounds? How to derive the backlog and delay bounds of an 802.11
wireless node? And how tight are these bounds when compared with simulations?
To answer these questions, we first derive the general stability condition of a
wireless node (not restricted to 802.11). From this, we give the specific
stability condition of an 802.11 wireless node. Then we derive the backlog and
delay bounds of an 802.11 node based on an existing model of 802.11. We observe
that the derived bounds are loose when compared with ns-2 simulations,
indicating that improvements are needed in the current version of stochastic
network calculus.
</summary>
    <author>
      <name>Yue Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1004.3109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3254v1</id>
    <updated>2010-04-19T17:42:16Z</updated>
    <published>2010-04-19T17:42:16Z</published>
    <title>Automatic Mapping Tasks to Cores - Evaluating AMTHA Algorithm in
  Multicore Architectures</title>
    <summary>  The AMTHA (Automatic Mapping Task on Heterogeneous Architectures) algorithm
for task-to-processors assignment and the MPAHA (Model of Parallel Algorithms
on Heterogeneous Architectures) model are presented. The use of AMTHA is
analyzed for multicore processor-based architectures, considering the
communication model among processes in use. The results obtained in the tests
carried out are presented, comparing the real execution times on multicores of
a set of synthetic applications with the predictions obtained with AMTHA.
Finally current lines of research are presented, focusing on clusters of
multicores and hybrid programming paradigms.
</summary>
    <author>
      <name>Laura De Giusti</name>
    </author>
    <author>
      <name>Franco Chichizola</name>
    </author>
    <author>
      <name>Marcelo Naiouf</name>
    </author>
    <author>
      <name>Armando De Giusti</name>
    </author>
    <author>
      <name>Emilio Luque</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://ijcsi.org/articles/Automatic-Mapping-Tasks-to-Cores-Evaluating-AMTHA-Algorithm-in-Multicore-Architectures.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI, Volume 7, Issue 2, March 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.3254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.0806v1</id>
    <updated>2010-05-05T18:33:28Z</updated>
    <published>2010-05-05T18:33:28Z</published>
    <title>A New Benchmark For Evaluation Of Graph-Theoretic Algorithms</title>
    <summary>  We propose a new graph-theoretic benchmark in this paper. The benchmark is
developed to address shortcomings of an existing widely-used graph benchmark.
We thoroughly studied a large number of traditional and contemporary graph
algorithms reported in the literature to have clear understanding of their
algorithmic and run-time characteristics. Based on this study, we designed a
suite of kernels, each of which represents a specific class of graph
algorithms. The kernels are designed to capture the typical run-time behavior
of target algorithms accurately, while limiting computational and spatial
overhead to ensure its computation finishes in reasonable time. We expect that
the developed benchmark will serve as a much needed tool for evaluating
different architectures and programming models to run graph algorithms.
</summary>
    <author>
      <name>Andy B. Yoo</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Sheila Vaidya</name>
    </author>
    <author>
      <name>Stephen Poole</name>
    </author>
    <link href="http://arxiv.org/abs/1005.0806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.0806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.1992v1</id>
    <updated>2010-05-12T05:29:40Z</updated>
    <published>2010-05-12T05:29:40Z</published>
    <title>Analyzing the Performance of Active Queue Management Algorithms</title>
    <summary>  Congestion is an important issue which researchers focus on in the
Transmission Control Protocol (TCP) network environment. To keep the stability
of the whole network, congestion control algorithms have been extensively
studied. Queue management method employed by the routers is one of the
important issues in the congestion control study. Active queue management (AQM)
has been proposed as a router-based mechanism for early detection of congestion
inside the network. In this paper we analyzed several active queue management
algorithms with respect to their abilities of maintaining high resource
utilization, identifying and restricting disproportionate bandwidth usage, and
their deployment complexity. We compare the performance of FRED, BLUE, SFB, and
CHOKe based on simulation results, using RED and Drop Tail as the evaluation
baseline. The characteristics of different algorithms are also discussed and
compared. Simulation is done by using Network Simulator(NS2) and the graphs are
drawn using X- graph.
</summary>
    <author>
      <name>G. F. Ali Ahammed</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Ghousia college of Engg.Ramanagaram, India</arxiv:affiliation>
    </author>
    <author>
      <name>Reshma Banu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Ghousia college of Engg.Ramanagaram, India</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcnc.2010.2201</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcnc.2010.2201" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 Pages, IJCNC 2010</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Networks &amp; Communications 2.2
  (2010) 1-19</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.1992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.1992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.2581v3</id>
    <updated>2011-05-16T16:39:03Z</updated>
    <published>2010-05-14T17:41:53Z</published>
    <title>A Performance Comparison of CUDA and OpenCL</title>
    <summary>  CUDA and OpenCL are two different frameworks for GPU programming. OpenCL is
an open standard that can be used to program CPUs, GPUs, and other devices from
different vendors, while CUDA is specific to NVIDIA GPUs. Although OpenCL
promises a portable language for GPU programming, its generality may entail a
performance penalty. In this paper, we use complex, near-identical kernels from
a Quantum Monte Carlo application to compare the performance of CUDA and
OpenCL. We show that when using NVIDIA compiler tools, converting a CUDA kernel
to an OpenCL kernel involves minimal modifications. Making such a kernel
compile with ATI's build tools involves more modifications. Our performance
tests measure and compare data transfer times to and from the GPU, kernel
execution times, and end-to-end application execution times for both CUDA and
OpenCL.
</summary>
    <author>
      <name>Kamran Karimi</name>
    </author>
    <author>
      <name>Neil G. Dickson</name>
    </author>
    <author>
      <name>Firas Hamze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 Tables, 5 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.2581v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.2581v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.5241v1</id>
    <updated>2010-05-28T08:55:12Z</updated>
    <published>2010-05-28T08:55:12Z</published>
    <title>Simulation de traces réelles d'E/S disque de PC</title>
    <summary>  Under Windows operating system, existing I/O benchmarking tools does not
allow a developer to efficiently define a file access strategy according to the
applications' constraints. This is essentially due to the fact that the
existing tools do allow only a restricted set of I/O workloads that does not
generally correspond to the target applications. To cope with this problem, we
designed and implemented a precise I/O simulator allowing to simulate whatever
real I/O trace on a given defined architecture, and in which most of file and
disk cache strategies, their interactions and the detailed storage system
architecture are implemented. Simulation results on different workloads and
architectures show a very high degree of precision. In fact, the mean error
rate as compared to real measures is of about 6% with a maximum of 10% on
global throughput.
</summary>
    <author>
      <name>Jalil Boukhobza</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LESTER</arxiv:affiliation>
    </author>
    <author>
      <name>Timsit Claude</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">RenPar'17 / SympA'2006 / CFSE'5 / JC'2006, Canet en Roussillon :
  France (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.5241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.5241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.5095v1</id>
    <updated>2010-06-26T03:03:05Z</updated>
    <published>2010-06-26T03:03:05Z</published>
    <title>Performance Evaluation of Components Using a Granularity-based Interface
  Between Real-Time Calculus and Timed Automata</title>
    <summary>  To analyze complex and heterogeneous real-time embedded systems, recent works
have proposed interface techniques between real-time calculus (RTC) and timed
automata (TA), in order to take advantage of the strengths of each technique
for analyzing various components. But the time to analyze a state-based
component modeled by TA may be prohibitively high, due to the state space
explosion problem. In this paper, we propose a framework of granularity-based
interfacing to speed up the analysis of a TA modeled component. First, we
abstract fine models to work with event streams at coarse granularity. We
perform analysis of the component at multiple coarse granularities and then
based on RTC theory, we derive lower and upper bounds on arrival patterns of
the fine output streams using the causality closure algorithm. Our framework
can help to achieve tradeoffs between precision and analysis time.
</summary>
    <author>
      <name>Karine Altisen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Verimag</arxiv:affiliation>
    </author>
    <author>
      <name>Yanhong Liu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Verimag</arxiv:affiliation>
    </author>
    <author>
      <name>Matthieu Moy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Verimag</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.28.2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.28.2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 28, 2010, pp. 16-33</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.5095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.5095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3; C.4; D.4.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.5104v1</id>
    <updated>2010-06-26T03:03:55Z</updated>
    <published>2010-06-26T03:03:55Z</published>
    <title>A new tool for the performance analysis of massively parallel computer
  systems</title>
    <summary>  We present a new tool, GPA, that can generate key performance measures for
very large systems. Based on solving systems of ordinary differential equations
(ODEs), this method of performance analysis is far more scalable than
stochastic simulation. The GPA tool is the first to produce higher moment
analysis from differential equation approximation, which is essential, in many
cases, to obtain an accurate performance prediction. We identify so-called
switch points as the source of error in the ODE approximation. We investigate
the switch point behaviour in several large models and observe that as the
scale of the model is increased, in general the ODE performance prediction
improves in accuracy. In the case of the variance measure, we are able to
justify theoretically that in the limit of model scale, the ODE approximation
can be expected to tend to the actual variance of the model.
</summary>
    <author>
      <name>Anton Stefanek</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Imperial College London</arxiv:affiliation>
    </author>
    <author>
      <name>Richard Hayden</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Imperial College London</arxiv:affiliation>
    </author>
    <author>
      <name>Jeremy Bradley</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Imperial College London</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.28.11</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.28.11" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 28, 2010, pp. 159-181</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.5104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.5104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.4853v2</id>
    <updated>2010-08-03T14:10:38Z</updated>
    <published>2010-07-27T23:47:58Z</published>
    <title>Performance bounds in wormhole routing, a network calculus approach</title>
    <summary>  We present a model of performance bound calculus on feedforward networks
where data packets are routed under wormhole routing discipline. We are
interested in determining maximum end-to-end delays and backlogs of messages or
packets going from a source node to a destination node, through a given virtual
path in the network. Our objective here is to give a network calculus approach
for calculating the performance bounds. First we propose a new concept of
curves that we call packet curves. The curves permit to model constraints on
packet lengths of a given data flow, when the lengths are allowed to be
different. Second, we use this new concept to propose an approach for
calculating residual services for data flows served under non preemptive
service disciplines. Third, we model a binary switch (with two input ports and
two output ports), where data is served under wormhole discipline. We present
our approach for computing the residual services and deduce the worst case
bounds for flows passing through a wormhole binary switch. Finally, we
illustrate this approach in numerical examples, and show how to extend it to
feedforward networks.
</summary>
    <author>
      <name>Nadir Farhi</name>
    </author>
    <author>
      <name>Bruno Gaujal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1007.4853v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.4853v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.1571v1</id>
    <updated>2010-08-09T19:23:10Z</updated>
    <published>2010-08-09T19:23:10Z</published>
    <title>Scaling Turbo Boost to a 1000 cores</title>
    <summary>  The Intel Core i7 processor code named Nehalem provides a feature named Turbo
Boost which opportunistically varies the frequencies of the processor's cores.
The frequency of a core is determined by core temperature, the number of active
cores, the estimated power consumption, the estimated current consumption, and
operating system frequency scaling requests. For a chip multi-processor(CMP)
that has a small number of physical cores and a small set of performance
states, deciding the Turbo Boost frequency to use on a given core might not be
difficult. However, we do not know the complexity of this decision making
process in the context of a large number of cores, scaling to the 100s, as
predicted by researchers in the field.
</summary>
    <author>
      <name>Ananth Narayan S</name>
    </author>
    <author>
      <name>Somsubhra Sharangi</name>
    </author>
    <author>
      <name>Alexandra Fedorova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, short paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1008.1571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.1571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.1708v1</id>
    <updated>2010-09-09T08:37:34Z</updated>
    <published>2010-09-09T08:37:34Z</published>
    <title>On the Performance Evaluation and Analysis of the Hybridised Bittorrent
  Protocol with Partial Mobility Characteristics</title>
    <summary>  Engaging mobility with file sharing is considered very promising in today's
run Anywhere, Anytime, Anything (3As) environments. The Bittorrent file sharing
protocol can be rarely combined with the mobility scenario framework since
resources are not available due to the dynamically changing topology network.
As a result, mobility in P2P-oriented file sharing platforms, degrades the
end-to-end efficiency and the system's performance. This work proposes a new
hybridized model, which takes into account the mobility characteristics of the
combined Bittorrent protocol in a centralized manner enabling partial mobility
characteristics, where the clients of the network use a distinct technique to
differentiate between mobile and static nodes. Many parameters were taken into
consideration like the round trip delays, the diffusion process, and the
seeding techniques, targeting the maximization of the average throughput in the
clustered swarms containing mobile peers. Partial mobility characteristics are
set in a peer-tracker and peer-peer communication enhancement schema with
partial mobility, allowing an optimistic approach to attain high availability
and throughput response as simulation results show.
</summary>
    <author>
      <name>George C. Violaris</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Science Department, University of Nicosia Cyprus</arxiv:affiliation>
    </author>
    <author>
      <name>Constandinos X. Mavromoustakis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Science Department, University of Nicosia Cyprus</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The Fourth International Conference on Mobile Ubiquitous Computing,
  Systems, Services and Technologies
  http://www.iaria.org/conferences2010/ProgramUBICOMM10.html UBICOMM 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.1708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.1708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.5878v1</id>
    <updated>2010-09-29T13:28:13Z</updated>
    <published>2010-09-29T13:28:13Z</published>
    <title>Performance analysis of Xen virtual machines in real-world scenarios</title>
    <summary>  This paper presents results of the performance benchmarks of the Open Source
hypervisor Xen. The study focuses on the network related performance as well as
on the application related performance of multiple virtual machines that were
running on the same Xen hypervisor. The comparison was carried out using a
self-developed benchmark suite that consists of easily available Open Source
tools. The goal is to measure the performance of the hypervisor in typical
real-world application scenarios when used for "mass virtual hosting", such as
hosting solutions of so called virtual private servers for small-to-medium
sized businesses environments. The results of the benchmarks show, that the
tested Xen setup offers good performance with respect to network traffic stress
tests, but only 75% of the performance of the non-virtualized reference
environment. This application performance score decreases as more virtual
machines are running simultaneously.
</summary>
    <author>
      <name>Adrian Heissler</name>
    </author>
    <link href="http://arxiv.org/abs/1009.5878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.5878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.6218v1</id>
    <updated>2010-09-30T18:57:10Z</updated>
    <published>2010-09-30T18:57:10Z</published>
    <title>Low Power Reversible Parallel Binary Adder/Subtractor</title>
    <summary>  In recent years, Reversible Logic is becoming more and more prominent
technology having its applications in Low Power CMOS, Quantum Computing,
Nanotechnology, and Optical Computing. Reversibility plays an important role
when energy efficient computations are considered. In this paper, Reversible
eight-bit Parallel Binary Adder/Subtractor with Design I, Design II and Design
III are proposed. In all the three design approaches, the full Adder and
Subtractors are realized in a single unit as compared to only full Subtractor
in the existing design. The performance analysis is verified using number
reversible gates, Garbage input/outputs and Quantum Cost. It is observed that
Reversible eight-bit Parallel Binary Adder/Subtractor with Design III is
efficient compared to Design I, Design II and existing design.
</summary>
    <author>
      <name>H G Rangaraju</name>
    </author>
    <author>
      <name>U. Venugopal</name>
    </author>
    <author>
      <name>K N Muralidhara</name>
    </author>
    <author>
      <name>K B Raja</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/vlsic.2010.1303</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/vlsic.2010.1303" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages,VLSICS Journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of VLSI Design &amp; Communication Systems,
  1.3(2010),pp-23-34</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.6218v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.6218v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.0019v1</id>
    <updated>2010-09-30T21:02:04Z</updated>
    <published>2010-09-30T21:02:04Z</published>
    <title>Mantis: Predicting System Performance through Program Analysis and
  Modeling</title>
    <summary>  We present Mantis, a new framework that automatically predicts program
performance with high accuracy. Mantis integrates techniques from programming
language and machine learning for performance modeling, and is a radical
departure from traditional approaches. Mantis extracts program features, which
are information about program execution runs, through program instrumentation.
It uses machine learning techniques to select features relevant to performance
and creates prediction models as a function of the selected features. Through
program analysis, it then generates compact code slices that compute these
feature values for prediction. Our evaluation shows that Mantis can achieve
more than 93% accuracy with less than 10% training data set, which is a
significant improvement over models that are oblivious to program features. The
system generates code slices that are cheap to compute feature values.
</summary>
    <author>
      <name>Byung-Gon Chun</name>
    </author>
    <author>
      <name>Ling Huang</name>
    </author>
    <author>
      <name>Sangmin Lee</name>
    </author>
    <author>
      <name>Petros Maniatis</name>
    </author>
    <author>
      <name>Mayur Naik</name>
    </author>
    <link href="http://arxiv.org/abs/1010.0019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.0019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.8; C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.2313v3</id>
    <updated>2011-05-09T22:37:08Z</updated>
    <published>2010-11-10T08:38:46Z</published>
    <title>Weighted Centroid Algorithm for Estimating Primary User Location:
  Theoretical Analysis and Distributed Implementation</title>
    <summary>  Information about primary transmitter location is crucial in enabling several
key capabilities in cognitive radio networks, including improved
spatio-temporal sensing, intelligent location-aware routing, as well as aiding
spectrum policy enforcement. Compared to other proposed non-interactive
localization algorithms, the weighted centroid localization (WCL) scheme uses
only the received signal strength information, which makes it simple to
implement and robust to variations in the propagation environment. In this
paper we present the first theoretical framework for WCL performance analysis
in terms of its localization error distribution parameterized by node density,
node placement, shadowing variance, correlation distance and inaccuracy of
sensor node positioning. Using this analysis, we quantify the robustness of WCL
to various physical conditions and provide design guidelines, such as node
placement and spacing, for the practical deployment of WCL. We also propose a
power-efficient method for implementing WCL through a distributed cluster-based
algorithm, that achieves comparable accuracy with its centralized counterpart.
</summary>
    <author>
      <name>Jun Wang</name>
    </author>
    <author>
      <name>Paulo Urriza</name>
    </author>
    <author>
      <name>Yuxing Han</name>
    </author>
    <author>
      <name>Danijela Čabrić</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TWC.2011.11.102209</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TWC.2011.11.102209" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 10 figures, resubmitted with major revisions to IEEE
  Transactions on Wireless Communications</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.2313v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.2313v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.6031v1</id>
    <updated>2010-11-28T11:13:40Z</updated>
    <published>2010-11-28T11:13:40Z</published>
    <title>A framework to experiment optimizations for real-time and embedded
  software</title>
    <summary>  Typical constraints on embedded systems include code size limits, upper
bounds on energy consumption and hard or soft deadlines. To meet these
requirements, it may be necessary to improve the software by applying various
kinds of transformations like compiler optimizations, specific mapping of code
and data in the available memories, code compression, etc. However, a
transformation that aims at improving the software with respect to a given
criterion might engender side effects on other criteria and these effects must
be carefully analyzed. For this purpose, we have developed a common framework
that makes it possible to experiment various code transfor-mations and to
evaluate their impact of various criteria. This work has been carried out
within the French ANR MORE project.
</summary>
    <author>
      <name>Hugues Cassé</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Karine Heydemann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIP6</arxiv:affiliation>
    </author>
    <author>
      <name>Haluk Ozaktas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIP6</arxiv:affiliation>
    </author>
    <author>
      <name>Jonathan Ponroy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Christine Rochange</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Zendra</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Embedded Real Time Software and Systems
  (ERTS2), Toulouse : France (2010)</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.6031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.6031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.0091v1</id>
    <updated>2010-12-30T14:21:25Z</updated>
    <published>2010-12-30T14:21:25Z</published>
    <title>Parallel sparse matrix-vector multiplication as a test case for hybrid
  MPI+OpenMP programming</title>
    <summary>  We evaluate optimized parallel sparse matrix-vector operations for two
representative application areas on widespread multicore-based cluster
configurations. First the single-socket baseline performance is analyzed and
modeled with respect to basic architectural properties of standard multicore
chips. Going beyond the single node, parallel sparse matrix-vector operations
often suffer from an unfavorable communication to computation ratio. Starting
from the observation that nonblocking MPI is not able to hide communication
cost using standard MPI implementations, we demonstrate that explicit overlap
of communication and computation can be achieved by using a dedicated
communication thread, which may run on a virtual core. We compare our approach
to pure MPI and the widely used "vector-like" hybrid programming strategy.
</summary>
    <author>
      <name>Gerald Schubert</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Holger Fehske</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IPDPS.2011.332</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IPDPS.2011.332" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.0091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.0091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.5335v1</id>
    <updated>2011-01-10T06:26:56Z</updated>
    <published>2011-01-10T06:26:56Z</published>
    <title>Accurate Performance Analysis of Opportunistic Decode-and-Forward
  Relaying</title>
    <summary>  In this paper, we investigate an opportunistic relaying scheme where the
selected relay assists the source-destination (direct) communication. In our
study, we consider a regenerative opportunistic relaying scheme in which the
direct path can be considered unusable, and takes into account the effect of
the possible erroneously detected and transmitted data at the best relay. We
first derive statistics based on exact probability density function (PDF) of
each hop. Then, the PDFs are used to determine accurate closed form expressions
for end-to-end bit-error rate (BER) of binary phase-shift keying (BPSK)
modulation. Furthermore, we evaluate the asymptotical performance analysis and
the diversity order is deduced. Finally, we validate our analysis by showing
that performance simulation results coincide with our analytical results over
different network architectures.
</summary>
    <author>
      <name>Kamel Tourki</name>
    </author>
    <author>
      <name>Hong-Chuan Yang</name>
    </author>
    <author>
      <name>Mohamed-Slim Alouni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures, Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.5335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.5335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.3059v1</id>
    <updated>2011-02-15T13:07:53Z</updated>
    <published>2011-02-15T13:07:53Z</published>
    <title>Profit-Aware Server Allocation for Green Internet Services</title>
    <summary>  A server farm is examined, where a number of servers are used to offer a
service to impatient customers. Every completed request generates a certain
amount of profit, running servers consume electricity for power and cooling,
while waiting customers might leave the system before receiving service if they
experience excessive delays. A dynamic allocation policy aiming at satisfying
the conflicting goals of maximizing the quality of users' experience while
minimizing the cost for the provider is introduced and evaluated. The results
of several experiments are described, showing that the proposed scheme performs
well under different traffic conditions.
</summary>
    <author>
      <name>Michele Mazzucco</name>
    </author>
    <author>
      <name>Dmytro Dyachuk</name>
    </author>
    <author>
      <name>Marios Dikaiakos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">18th Annual IEEE/ACM International Symposium on Modeling, Analysis
  and Simulation of Computer and Telecommunication Systems, 2010, pp 277-284</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1102.3059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.3059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.3703v1</id>
    <updated>2011-02-17T21:20:06Z</updated>
    <published>2011-02-17T21:20:06Z</published>
    <title>Allocation and Admission Policies for Service Streams</title>
    <summary>  A service provisioning system is examined, where a number of servers are used
to offer different types of services to paying customers. A customer is charged
for the execution of a stream of jobs; the number of jobs in the stream and the
rate of their submission is specified. On the other hand, the provider promises
a certain quality of service (QoS), measured by the average waiting time of the
jobs in the stream. A penalty is paid if the agreed QoS requirement is not met.
The objective is to maximize the total average revenue per unit time. Dynamic
policies for making server allocation and stream admission decisions are
introduced and evaluated. The results of several simulations are described.
</summary>
    <author>
      <name>Michele Mazzucco</name>
    </author>
    <author>
      <name>Isi Mitrani</name>
    </author>
    <author>
      <name>Mike Fisher</name>
    </author>
    <author>
      <name>Paul McKee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, 16th International Symposium on Modeling,
  Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS
  2008), pp155-162 (Best Paper Award)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">16th International Symposium on Modeling, Analysis, and Simulation
  of Computer and Telecommunication Systems (MASCOTS 2008), pp155-162</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1102.3703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.3703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.3099v2</id>
    <updated>2011-03-19T12:58:09Z</updated>
    <published>2011-03-16T05:37:18Z</published>
    <title>Optimal Power Cost Management Using Stored Energy in Data Centers</title>
    <summary>  Since the electricity bill of a data center constitutes a significant portion
of its overall operational costs, reducing this has become important. We
investigate cost reduction opportunities that arise by the use of uninterrupted
power supply (UPS) units as energy storage devices. This represents a deviation
from the usual use of these devices as mere transitional fail-over mechanisms
between utility and captive sources such as diesel generators. We consider the
problem of opportunistically using these devices to reduce the time average
electric utility bill in a data center. Using the technique of Lyapunov
optimization, we develop an online control algorithm that can optimally exploit
these devices to minimize the time average cost. This algorithm operates
without any knowledge of the statistics of the workload or electricity cost
processes, making it attractive in the presence of workload and pricing
uncertainties. An interesting feature of our algorithm is that its deviation
from optimality reduces as the storage capacity is increased. Our work opens up
a new area in data center power management.
</summary>
    <author>
      <name>Rahul Urgaonkar</name>
    </author>
    <author>
      <name>Bhuvan Urgaonkar</name>
    </author>
    <author>
      <name>Michael J. Neely</name>
    </author>
    <author>
      <name>Anand Sivasubramaniam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of Sigmetrics 2011 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.3099v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.3099v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.3225v1</id>
    <updated>2011-03-16T16:37:49Z</updated>
    <published>2011-03-16T16:37:49Z</published>
    <title>Measuring NUMA effects with the STREAM benchmark</title>
    <summary>  Modern high-end machines feature multiple processor packages, each of which
contains multiple independent cores and integrated memory controllers connected
directly to dedicated physical RAM. These packages are connected via a shared
bus, creating a system with a heterogeneous memory hierarchy. Since this shared
bus has less bandwidth than the sum of the links to memory, aggregate memory
bandwidth is higher when parallel threads all access memory local to their
processor package than when they access memory attached to a remote package.
  But, the impact of this heterogeneous memory architecture is not easily
understood from vendor benchmarks. Even where these measurements are available,
they provide only best-case memory throughput. This work presents a series of
modifications to the well-known STREAM benchmark to measure the effects of NUMA
on both a 48-core AMD Opteron machine and a 32-core Intel Xeon machine.
</summary>
    <author>
      <name>Lars Bergstrom</name>
    </author>
    <link href="http://arxiv.org/abs/1103.3225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.3225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.1729v1</id>
    <updated>2011-04-09T17:50:30Z</updated>
    <published>2011-04-09T17:50:30Z</published>
    <title>Expression Templates Revisited: A Performance Analysis of the Current ET
  Methodology</title>
    <summary>  In the last decade, Expression Templates (ET) have gained a reputation as an
efficient performance optimization tool for C++ codes. This reputation builds
on several ET-based linear algebra frameworks focused on combining both elegant
and high-performance C++ code. However, on closer examination the assumption
that ETs are a performance optimization technique cannot be maintained. In this
paper we demonstrate and explain the inability of current ET-based frameworks
to deliver high performance for dense and sparse linear algebra operations, and
introduce a new "smart" ET implementation that truly allows the combination of
high performance code with the elegance and maintainability of a
domain-specific language.
</summary>
    <author>
      <name>Klaus Iglberger</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Jan Treibig</name>
    </author>
    <author>
      <name>Ulrich Ruede</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/110830125</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/110830125" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIAM Journal on Scientific Computing 34(2), C42-C69 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1104.1729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.1729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.1202v1</id>
    <updated>2011-07-06T17:54:58Z</updated>
    <published>2011-07-06T17:54:58Z</published>
    <title>A Stochastic Broadcast Pi-Calculus</title>
    <summary>  In this paper we propose a stochastic broadcast PI-calculus which can be used
to model server-client based systems where synchronization is always governed
by only one participant. Therefore, there is no need to determine the joint
synchronization rates. We also take immediate transitions into account which is
useful to model behaviors with no impact on the temporal properties of a
system. Since immediate transitions may introduce non-determinism, we will show
how these non-determinism can be resolved, and as result a valid CTMC will be
obtained finally. Also some practical examples are given to show the
application of this calculus.
</summary>
    <author>
      <name>Lei Song</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IT University of Copenhagen, Denmark</arxiv:affiliation>
    </author>
    <author>
      <name>Flemming Nielson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technical University of Denmark</arxiv:affiliation>
    </author>
    <author>
      <name>Bo Friis Nielsen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technical University of Denmark</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.57.6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.57.6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings QAPL 2011, arXiv:1107.0746</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 57, 2011, pp. 74-88</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1107.1202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.1202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.3087v1</id>
    <updated>2011-07-15T15:22:45Z</updated>
    <published>2011-07-15T15:22:45Z</published>
    <title>Non-equilibrium Information Envelopes and the
  Capacity-Delay-Error-Tradeoff of Source Coding</title>
    <summary>  This paper develops an envelope-based approach to establish a link between
information and queueing theory. Unlike classical, equilibrium information
theory, information envelopes focus on the dynamics of sources and coders,
using functions of time that bound the number of bits generated. In the limit
the information envelopes converge to the average behavior and recover the
entropy of a source, respectively, the average codeword length of a coder. In
contrast, on short time scales and for sources with memory it is shown that
large deviations from known equilibrium results occur with non-negligible
probability. These can cause significant network delays. Compared to well-known
traffic models from queueing theory, information envelopes consider the
functioning of information sources and coders, avoiding a priori assumptions,
such as exponential traffic, or empirical, trace-based traffic models. Using
results from the stochastic network calculus, the envelopes yield a
characterization of the operating points of source coders by the triplet of
capacity, delay, and error. In the limit, assuming an optimal coder the
required capacity approaches the entropy with arbitrarily small probability of
error if infinitely large delays are permitted. We derive a corresponding
characterization of channels and prove that the model has the desirable
property of additivity, that allows analyzing coders and channels separately.
</summary>
    <author>
      <name>Ralf Lübben</name>
    </author>
    <author>
      <name>Markus Fidler</name>
    </author>
    <link href="http://arxiv.org/abs/1107.3087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.3087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.1554v1</id>
    <updated>2011-08-07T16:52:18Z</updated>
    <published>2011-08-07T16:52:18Z</published>
    <title>A Stochastic Calculus for Network Systems with Renewable Energy Sources</title>
    <summary>  We consider the performance modeling and evaluation of network systems
powered with renewable energy sources such as solar and wind energy. Such
energy sources largely depend on environmental conditions, which are hard to
predict accurately. As such, it may only make sense to require the network
systems to support a soft quality of service (QoS) guarantee, i.e., to
guarantee a service requirement with a certain high probability. In this paper,
we intend to build a solid mathematical foundation to help better understand
the stochastic energy constraint and the inherent correlation between QoS and
the uncertain energy supply. We utilize a calculus approach to model the
cumulative amount of charged energy and the cumulative amount of consumed
energy. We derive upper and lower bounds on the remaining energy level based on
a stochastic energy charging rate and a stochastic energy discharging rate. By
building the bridge between energy consumption and task execution (i.e.,
service), we study the QoS guarantee under the constraint of uncertain energy
sources. We further show how performance bounds can be improved if some strong
assumptions can be made.
</summary>
    <author>
      <name>Kui Wu</name>
    </author>
    <author>
      <name>Yuming Jiang</name>
    </author>
    <author>
      <name>Dimitri Marinakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.1554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.1554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.2753v2</id>
    <updated>2012-05-06T03:53:44Z</updated>
    <published>2011-10-12T18:47:23Z</published>
    <title>Stability of a Peer-to-Peer Communication System</title>
    <summary>  This paper focuses on the stationary portion of file download in an
unstructured peer-to-peer network, which typically follows for many hours after
a flash crowd initiation. The model includes the case that peers can have some
pieces at the time of arrival. The contribution of the paper is to identify how
much help is needed from the seeds, either fixed seeds or peer seeds (which are
peers remaining in the system after obtaining a complete collection) to
stabilize the system. The dominant cause for instability is the missing piece
syndrome, whereby one piece becomes very rare in the network. It is shown that
stability can be achieved with only a small amount of help from peer
seeds--even with very little help from a fixed seed, peers need dwell as peer
seeds on average only long enough to upload one additional piece. The region of
stability is insensitive to the piece selection policy. Network coding can
substantially increase the region of stability in case a portion of the new
peers arrive with randomly coded pieces.
</summary>
    <author>
      <name>Ji Zhu</name>
    </author>
    <author>
      <name>Bruce Hajek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.2753v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.2753v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.3711v3</id>
    <updated>2011-11-18T08:32:50Z</updated>
    <published>2011-10-17T15:54:48Z</published>
    <title>Optimization strategies for parallel CPU and GPU implementations of a
  meshfree particle method</title>
    <summary>  Much of the current focus in high performance computing (HPC) for
computational fluid dynamics (CFD) deals with grid based methods. However,
parallel implementations for new meshfree particle methods such as Smoothed
Particle Hydrodynamics (SPH) are less studied. In this work, we present
optimizations for both central processing unit (CPU) and graphics processing
unit (GPU) of a SPH method. These optimization strategies can be further
applied to many other meshfree methods. The obtained performance for each
architecture and a comparison between the most efficient implementations for
CPU and GPU are shown.
</summary>
    <author>
      <name>Jose M. Domínguez</name>
    </author>
    <author>
      <name>Alejandro J. C. Crespo</name>
    </author>
    <author>
      <name>Moncho Gómez-Gesteira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 21 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.3711v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.3711v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Uxx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.4245v1</id>
    <updated>2011-10-19T11:33:43Z</updated>
    <published>2011-10-19T11:33:43Z</published>
    <title>An Improved Analytical Expression for Write Amplification in NAND Flash</title>
    <summary>  Agarwal et al. gave an closed-form expression for write amplification in NAND
flash memory by finding the probability of a page being valid over the whole
flash memory. This paper gives an improved analytic expression for write
amplification in NAND flash memory by finding the probability of a page being
invalid over the block selected for garbage collection. The improved expression
uses Lambert W function. Through asymptotic analysis, write amplification is
shown to depend on overprovisioning factor only, consistent with the previous
work. Comparison with numerical simulations shows that the improved expression
achieves a more accurate prediction of write amplification. For example, when
the overprovisioning factor is 0.3, the expression proposed by this paper gives
a write amplification of 2.36 whereas that of the previous work gives 2.17,
when the actual value is 2.35.
</summary>
    <author>
      <name>Luojie Xiang</name>
    </author>
    <author>
      <name>Brian Kurkoski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, accepted by ICNC 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.4245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.4245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.4535v1</id>
    <updated>2011-10-20T14:09:02Z</updated>
    <published>2011-10-20T14:09:02Z</published>
    <title>A Survey on Delay-Aware Resource Control for Wireless Systems --- Large
  Deviation Theory, Stochastic Lyapunov Drift and Distributed Stochastic
  Learning</title>
    <summary>  In this tutorial paper, a comprehensive survey is given on several major
systematic approaches in dealing with delay-aware control problems, namely the
equivalent rate constraint approach, the Lyapunov stability drift approach and
the approximate Markov Decision Process (MDP) approach using stochastic
learning. These approaches essentially embrace most of the existing literature
regarding delay-aware resource control in wireless systems. They have their
relative pros and cons in terms of performance, complexity and implementation
issues. For each of the approaches, the problem setup, the general solution and
the design methodology are discussed. Applications of these approaches to
delay-aware resource allocation are illustrated with examples in single-hop
wireless networks. Furthermore, recent results regarding delay-aware multi-hop
routing designs in general multi-hop networks are elaborated. Finally, the
delay performance of the various approaches are compared through simulations
using an example of the uplink OFDMA systems.
</summary>
    <author>
      <name>Ying Cui</name>
    </author>
    <author>
      <name>Vincent K. N. Lau</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
    <author>
      <name>Huang Huang</name>
    </author>
    <author>
      <name>Shunqing Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIT.2011.2178150</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIT.2011.2178150" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">58 pages, 8 figures; IEEE Transactions on Information Theory, 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.4535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.4535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.6544v2</id>
    <updated>2011-11-24T13:24:26Z</updated>
    <published>2011-10-29T18:22:36Z</published>
    <title>A Generalized Loss Network Model with Overflow for Capacity Planning of
  a Perinatal Network</title>
    <summary>  We develop a generalized loss network framework for capacity planning of a
perinatal network in the UK. Decomposing the network by hospitals, each unit is
analyzed with a GI/G/c/0 overflow loss network model. A two-moment
approximation is performed to obtain the steady state solution of the GI/G/c/0
loss systems, and expressions for rejection probability and overflow
probability have been derived. Using the model framework, the number of
required cots can be estimated based on the rejection probability at each level
of care of the neonatal units in a network. The generalization ensures that the
model can be applied to any perinatal network for renewal arrival and discharge
processes.
</summary>
    <author>
      <name>Md Asaduzzaman</name>
    </author>
    <author>
      <name>Thierry J Chaussalet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 4 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.6544v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.6544v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90B22, 68M20, 60K25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.1926v2</id>
    <updated>2012-01-02T08:56:16Z</updated>
    <published>2011-11-08T14:54:57Z</published>
    <title>Performance Analysis of Sequential Method for Handover in Cognitive
  Radio Systems</title>
    <summary>  Powerful spectrum handover schemes enable cognitive radios (CRs) to use
transmission opportunities in primary users' channels appropriately. In this
paper, we consider the cognitive access of primary channels by a secondary
user. We evaluate the average detection time and the maximum achievable average
throughput of the secondary user when the sequential method for hand-over
(SMHO) is used. We assume that a prior knowledge of the primary users' presence
and absence probabilities are available. When investigating the maximum
achievable throughput of the secondary user, we end into an optimization
problem, in which the optimum value of sensing time must be selected. In our
optimization problem, we take into account the spectrum hand over due to false
detection of the primary user. We also propose a weighted based hand-over
(WBHO) scheme in which the impacts of channels conditions and primary users'
presence probability are considered. This Spectrum handover scheme provides
higher average throughput for the SU than the SMHO method. The tradeoff between
the maximum achievable throughput and consumed energy is discussed, and finally
an energy efficient optimization formulation for finding a proper sensing time
is provided.
</summary>
    <author>
      <name>Hossein Shokri-Ghadikolaei</name>
    </author>
    <author>
      <name>Mohammad Mozaffari</name>
    </author>
    <author>
      <name>Masoumeh Nasiri-Kenari</name>
    </author>
    <link href="http://arxiv.org/abs/1111.1926v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.1926v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.4287v1</id>
    <updated>2011-11-18T05:29:44Z</updated>
    <published>2011-11-18T05:29:44Z</published>
    <title>Parametric Estimation of the Ultimate Size of Hypercomputers</title>
    <summary>  The performance of the emerging petaflops-scale supercomputers of the nearest
future (hypercomputers) will be governed not only by the clock frequency of the
processing nodes or by the width of the system bus, but also by such factors as
the overall power consumption and the geometric size. In this paper, we study
the influence of such parameters on one of the most important characteristics
of a general purpose computer - on the degree of multithreading that must be
present in an application to make the use of the hypercomputer justifiable. Our
major finding is that for the class of applications with purely random memory
access patterns "super-fast computing" and "high-performance computing" are
essentially synonyms for "massively-parallel computing."
</summary>
    <author>
      <name>Dmitry Zinoviev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, published in Proc. 6th World Multiconference on
  Systemics, Cybernetics and Informatics, 2002</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.4287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.4287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.4624v3</id>
    <updated>2012-01-02T09:09:27Z</updated>
    <published>2011-11-20T11:26:29Z</published>
    <title>Sensing Matrix Setting Schemes for Cognitive Networks and Their
  Performance Analysis</title>
    <summary>  Powerful spectrum decision schemes enable cognitive radios (CRs) to find
transmission opportunities in spectral resources allocated exclusively to the
primary users. One of the key effecting factor on the CR network throughput is
the spectrum sensing sequence used by each secondary user. In this paper,
secondary users' throughput maximization through finding an appropriate sensing
matrix (SM) is investigated. To this end, first the average throughput of the
CR network is evaluated for a given SM. Then, an optimization problem based on
the maximization of the network throughput is formulated in order to find the
optimal SM. As the optimum solution is very complicated, to avoid its major
challenges, three novel sub optimum solutions for finding an appropriate SM are
proposed for various cases including perfect and non-perfect sensing. Despite
of having less computational complexities as well as lower consumed energies,
the proposed solutions perform quite well compared to the optimum solution (the
optimum SM). The structure and performance of the proposed SM setting schemes
are discussed in detail and a set of illustrative simulation results is
presented to validate their efficiencies.
</summary>
    <author>
      <name>Hossein Shokri-Ghadikolaei</name>
    </author>
    <author>
      <name>Masoumeh Nasiri-Kenari</name>
    </author>
    <link href="http://arxiv.org/abs/1111.4624v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.4624v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.6374v2</id>
    <updated>2012-06-17T17:42:58Z</updated>
    <published>2011-11-28T08:52:04Z</published>
    <title>Solving Dense Generalized Eigenproblems on Multi-threaded Architectures</title>
    <summary>  We compare two approaches to compute a portion of the spectrum of dense
symmetric definite generalized eigenproblems: one is based on the reduction to
tridiagonal form, and the other on the Krylov-subspace iteration. Two
large-scale applications, arising in molecular dynamics and material science,
are employed to investigate the contributions of the application, architecture,
and parallelism of the method to the performance of the solvers. The
experimental results on a state-of-the-art 8-core platform, equipped with a
graphics processing unit (GPU), reveal that in real applications, iterative
Krylov-subspace methods can be a competitive approach also for the solution of
dense problems.
</summary>
    <author>
      <name>José I. Aliaga</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Depto. de Ingeniería y Ciencia de Computadores, Universidad Jaume I</arxiv:affiliation>
    </author>
    <author>
      <name>Paolo Bientinesi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">RWTH-Aachen University</arxiv:affiliation>
    </author>
    <author>
      <name>Davor Davidović</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institut Ruder Boskovíc, Centarza Informatiku i Racunarstvo</arxiv:affiliation>
    </author>
    <author>
      <name>Edoardo Di Napoli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">JSC, Forschungszentrum Jülich</arxiv:affiliation>
    </author>
    <author>
      <name>Francisco D. Igual</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Depto. de Ingeniería y Ciencia de Computadores, Universidad Jaume I</arxiv:affiliation>
    </author>
    <author>
      <name>Enrique S. Quintana-Ortí</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Depto. de Ingeniería y Ciencia de Computadores, Universidad Jaume I</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.amc.2012.05.020</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.amc.2012.05.020" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 tables and 4 figures. In press by Applied Mathematics and
  Computation. Accepted version</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.6374v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.6374v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.3496v1</id>
    <updated>2012-01-17T12:39:04Z</updated>
    <published>2012-01-17T12:39:04Z</published>
    <title>Optimizing the Performance of Streaming Numerical Kernels on the IBM
  Blue Gene/P PowerPC 450 Processor</title>
    <summary>  Several emerging petascale architectures use energy-efficient processors with
vectorized computational units and in-order thread processing. On these
architectures the sustained performance of streaming numerical kernels,
ubiquitous in the solution of partial differential equations, represents a
challenge despite the regularity of memory access. Sophisticated optimization
techniques are required to fully utilize the Central Processing Unit (CPU).
  We propose a new method for constructing streaming numerical kernels using a
high-level assembly synthesis and optimization framework. We describe an
implementation of this method in Python targeting the IBM Blue Gene/P
supercomputer's PowerPC 450 core. This paper details the high-level design,
construction, simulation, verification, and analysis of these kernels utilizing
a subset of the CPU's instruction set.
  We demonstrate the effectiveness of our approach by implementing several
three-dimensional stencil kernels over a variety of cached memory scenarios and
analyzing the mechanically scheduled variants, including a 27-point stencil
achieving a 1.7x speedup over the best previously published results.
</summary>
    <author>
      <name>Tareq M. Malas</name>
    </author>
    <author>
      <name>Aron J. Ahmadia</name>
    </author>
    <author>
      <name>Jed Brown</name>
    </author>
    <author>
      <name>John A. Gunnels</name>
    </author>
    <author>
      <name>David E. Keyes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1177/1094342012444795</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1177/1094342012444795" rel="related"/>
    <link href="http://arxiv.org/abs/1201.3496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.3496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.6304v1</id>
    <updated>2012-04-27T19:21:30Z</updated>
    <published>2012-04-27T19:21:30Z</published>
    <title>Model for Predicting End User Web Page Response Time</title>
    <summary>  Perceived responsiveness of a web page is one of the most important and least
understood metrics of web page design, and is critical for attracting and
maintaining a large audience. Web pages can be designed to meet performance
SLAs early in the product lifecycle if there is a way to predict the apparent
responsiveness of a particular page layout. Response time of a web page is
largely influenced by page layout and various network characteristics. Since
the network characteristics vary widely from country to country, accurately
modeling and predicting the perceived responsiveness of a web page from the end
user's perspective has traditionally proven very difficult. We propose a model
for predicting end user web page response time based on web page, network,
browser download and browser rendering characteristics. We start by
understanding the key parameters that affect perceived response time. We then
model each of these parameters individually using experimental tests and
statistical techniques. Finally, we demonstrate the effectiveness of this model
by conducting an experimental study with Yahoo! web pages in two countries and
compare it with 3rd party measurement application.
</summary>
    <author>
      <name>Sathya Narayanan Nagarajan</name>
    </author>
    <author>
      <name>Srijith Ravikumar</name>
    </author>
    <link href="http://arxiv.org/abs/1204.6304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.6304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.2678v1</id>
    <updated>2012-02-08T15:00:32Z</updated>
    <published>2012-02-08T15:00:32Z</published>
    <title>Evaluation of Proactive, Reactive and Hybrid Ad hoc Routing Protocol for
  various Battery models in VANET using Qualnet</title>
    <summary>  In VANET high speed is the real characteristics which leads frequent
breakdown, interference etc. In this paper we studied various Ad hoc routing
protocols, Reactive, Proactive &amp; Hybrid, taking into consideration various
VANET parameters like speed, altitude etc in real traffic scenario and
evaluated them for various battery models for energy conservation.. The AODV
and DYMO (Reactive), OLSR (Proactive) and ZRP (hybrid) protocols are compared
for battery models Duracell AA(MX- 1500),Duracell AAA(MN-2400),Duracell
AAA(MX-2400), Duracell C-MN(MN-1400),Panasonic AA standard using Qualnet as a
Simulation tool. Since Energy conservation is main focus area nowadays. Hence
performance of the protocols with various battery models counts and helps to
make a right selection. Varying parameters of VANET shows that in the real
traffic scenarios proactive protocol performs more efficiently for energy
conservation.
</summary>
    <author>
      <name>Manish Sharma</name>
    </author>
    <author>
      <name>Gurpadam Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages, 5 Figures. arXiv admin note: substantial text overlap with
  arXiv:1202.1720</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Smart Sensors and Ad Hoc Networks
  (IJSSAN) ISSN No. 2248-9738 Vol.1(2) 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1205.2678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.2678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.3846v1</id>
    <updated>2012-05-17T04:27:30Z</updated>
    <published>2012-05-17T04:27:30Z</published>
    <title>A Method for the Characterisation of Observer Effects and its
  Application to OML</title>
    <summary>  In all measurement campaigns, one needs to assert that the instrumentation
tools do not significantly impact the system being monitored. This is critical
to future claims based on the collected data and is sometimes overseen in
experimental studies. We propose a method to evaluate the potential "observer
effect" of an instrumentation system, and apply it to the OMF Measurement
Library (OML). OML allows the instrumentation of almost any software to collect
any type of measurements. As it is increasingly being used in networking
research, it is important to characterise possible biases it may introduce in
the collected metrics. Thus, we study its effect on multiple types of reports
from various applications commonly used in wireless research. To this end, we
designed experiments comparing OML-instrumented software with their original
flavours. Our analyses of the results from these experiments show that, with an
appropriate reporting setup, OML has no significant impact on the instrumented
applications, and may even improve some of their performances in specifics
cases. We discuss our methodology and the implication of using OML, and provide
guidelines on instrumenting off-the-shelf software.
</summary>
    <author>
      <name>Olivier Mehani</name>
    </author>
    <author>
      <name>Guillaume Jourjon</name>
    </author>
    <author>
      <name>Thierry Rakotoarivelo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, submitted to ACM MSWIM 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.3846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.3846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.1264v1</id>
    <updated>2012-06-06T16:07:58Z</updated>
    <published>2012-06-06T16:07:58Z</published>
    <title>Heavy Traffic Optimal Resource Allocation Algorithms for Cloud Computing
  Clusters</title>
    <summary>  Cloud computing is emerging as an important platform for business, personal
and mobile computing applications. In this paper, we study a stochastic model
of cloud computing, where jobs arrive according to a stochastic process and
request resources like CPU, memory and storage space. We consider a model where
the resource allocation problem can be separated into a routing or load
balancing problem and a scheduling problem. We study the
join-the-shortest-queue routing and power-of-two-choices routing algorithms
with MaxWeight scheduling algorithm. It was known that these algorithms are
throughput optimal. In this paper, we show that these algorithms are queue
length optimal in the heavy traffic limit.
</summary>
    <author>
      <name>Siva Theja Maguluri</name>
    </author>
    <author>
      <name>R Srikant</name>
    </author>
    <author>
      <name>Lei Ying</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report corresponding to the paper of same title to be
  presented at International Teletraffic Conference 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.1264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.1264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.3738v1</id>
    <updated>2012-06-17T09:53:53Z</updated>
    <published>2012-06-17T09:53:53Z</published>
    <title>Best practices for HPM-assisted performance engineering on modern
  multicore processors</title>
    <summary>  Many tools and libraries employ hardware performance monitoring (HPM) on
modern processors, and using this data for performance assessment and as a
starting point for code optimizations is very popular. However, such data is
only useful if it is interpreted with care, and if the right metrics are chosen
for the right purpose. We demonstrate the sensible use of hardware performance
counters in the context of a structured performance engineering approach for
applications in computational science. Typical performance patterns and their
respective metric signatures are defined, and some of them are illustrated
using case studies. Although these generic concepts do not depend on specific
tools or environments, we restrict ourselves to modern x86-based multicore
processors and use the likwid-perfctr tool under the Linux OS.
</summary>
    <author>
      <name>Jan Treibig</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-36949-0_50</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-36949-0_50" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.3738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.3738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.5217v3</id>
    <updated>2012-08-27T13:38:10Z</updated>
    <published>2012-07-22T12:04:53Z</published>
    <title>Hierarchical Performance Modeling for Ranking Dense Linear Algebra
  Algorithms</title>
    <summary>  A large class of dense linear algebra operations, such as LU decomposition or
inversion of a triangular matrix, are usually performed by blocked algorithms.
For one such operation, typically, not only one but many algorithmic variants
exist; depending on computing architecture, libraries and problem size, each
variant attains a different performances. We propose methods and tools to rank
the algorithmic variants according to their performance for a given scenario
without executing them.
  For this purpose, we identify the routines upon which the algorithms are
built. A first tool - the Sampler - measures the performance of these routines.
Using the Sampler, a second tool models their performance. The generated models
are then used to predict the performance of the considered algorithms. For a
given scenario, these predictions allow us to correctly rank the algorithms
according to their performance without executing them. With the help of the
same tools, algorithmic parameters such as block-size can be optimally tuned.
</summary>
    <author>
      <name>Elmar Peise</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">AICES, RWTH Aachen</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Master's Thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.5217v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.5217v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.6295v2</id>
    <updated>2014-05-11T06:32:57Z</updated>
    <published>2012-07-26T15:17:06Z</published>
    <title>Characterizing the Impact of the Workload on the Value of Dynamic
  Resizing in Data Centers</title>
    <summary>  Energy consumption imposes a significant cost for data centers; yet much of
that energy is used to maintain excess service capacity during periods of
predictably low load. Resultantly, there has recently been interest in
developing designs that allow the service capacity to be dynamically resized to
match the current workload. However, there is still much debate about the value
of such approaches in real settings. In this paper, we show that the value of
dynamic resizing is highly dependent on statistics of the workload process. In
particular, both slow time-scale non-stationarities of the workload (e.g., the
peak-to-mean ratio) and the fast time-scale stochasticity (e.g., the burstiness
of arrivals) play key roles. To illustrate the impact of these factors, we
combine optimization-based modeling of the slow time-scale with stochastic
modeling of the fast time scale. Within this framework, we provide both
analytic and numerical results characterizing when dynamic resizing does (and
does not) provide benefits.
</summary>
    <author>
      <name>Kai Wang</name>
    </author>
    <author>
      <name>Minghong Lin</name>
    </author>
    <author>
      <name>Florin Ciucu</name>
    </author>
    <author>
      <name>Adam Wierman</name>
    </author>
    <author>
      <name>Chuang Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 27 fugures</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.6295v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.6295v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.0505v1</id>
    <updated>2012-08-02T14:38:24Z</updated>
    <published>2012-08-02T14:38:24Z</published>
    <title>Criticality of Large Delay Tolerant Networks via Directed Continuum
  Percolation in Space-Time</title>
    <summary>  We study delay tolerant networking (DTN) and in particular, its capacity to
store, carry and forward messages so that the messages eventually reach their
final destinations. We approach this broad question in the framework of
percolation theory. To this end, we assume an elementary mobility model, where
nodes arrive to an infinite plane according to a Poisson point process, move a
certain distance L, and then depart. In this setting, we characterize the mean
density of nodes required to support DTN style networking. In particular, under
the given assumptions, we show that DTN is feasible when the mean node degree
is greater than 4 e(g), where parameter g=L/d is the ratio of the distance L to
the transmission range d, and e(g) is the critical reduced number density of
tilted cylinders in a directed continuum percolation model. By means of Monte
Carlo simulations, we give numerical values for e(g). The asymptotic behavior
of e(g) when g tends to infinity is also derived from a fluid flow analysis.
</summary>
    <author>
      <name>Esa Hyytiä</name>
    </author>
    <author>
      <name>Jörg Ott</name>
    </author>
    <link href="http://arxiv.org/abs/1208.0505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.0505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; C.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.0525v2</id>
    <updated>2013-05-17T21:42:57Z</updated>
    <published>2012-08-02T16:00:45Z</published>
    <title>An Upper Bound on the Convergence Time for Distributed Binary Consensus</title>
    <summary>  The problem addressed in this paper is the analysis of a distributed
consensus algorithm for arbitrary networks, proposed by B\'en\'ezit et al.. In
the initial setting, each node in the network has one of two possible states
("yes" or "no"). Nodes can update their states by communicating with their
neighbors via a 2-bit message in an asynchronous clock setting. Eventually, all
nodes reach consensus on the majority states. We use the theory of electric
networks, random walks, and couplings of Markov chains to derive an O(N4 logN)
upper bound for the expected convergence time on an arbitrary graph of size N.
</summary>
    <author>
      <name>Shang Shang</name>
    </author>
    <author>
      <name>Paul W. Cuff</name>
    </author>
    <author>
      <name>Sanjeev R. Kulkarni</name>
    </author>
    <author>
      <name>Pan Hui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15th International Conference on Information Fusion, July 2012, 7
  pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.0525v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.0525v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.3315v1</id>
    <updated>2012-09-14T20:22:23Z</updated>
    <published>2012-09-14T20:22:23Z</published>
    <title>Storage Workload Modelling by Hidden Markov Models: Application to FLASH
  Memory</title>
    <summary>  A workload analysis technique is presented that processes data from operation
type traces and creates a Hidden Markov Model (HMM) to represent the workload
that generated those traces. The HMM can be used to create representative
traces for performance models, such as simulators, avoiding the need to
repeatedly acquire suitable traces. It can also be used to estimate directly
the transition probabilities and rates of a Markov modulated arrival process,
for use as input to an analytical performance model of Flash memory. The HMMs
obtained from industrial workloads are validated by comparing their
autocorrelation functions and other statistics with those of the corresponding
monitored time series. Further, the performance model applications are
illustrated by numerical examples.
</summary>
    <author>
      <name>P. G. Harrison</name>
    </author>
    <author>
      <name>S. K. Harrison</name>
    </author>
    <author>
      <name>N. M. Patel</name>
    </author>
    <author>
      <name>S. Zertal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 18 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Performance Evaluation 69, 1 (2012), 17-40</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.3315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.3315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.0313v1</id>
    <updated>2012-11-01T21:30:52Z</updated>
    <published>2012-11-01T21:30:52Z</published>
    <title>Multiple Antenna Cyclostationary Spectrum Sensing Based on the Cyclic
  Correlation Significance Test</title>
    <summary>  In this paper, we propose and analyze a spectrum sensing method based on
cyclostationarity specifically targeted for receivers with multiple antennas.
This detection method is used for determining the presence or absence of
primary users in cognitive radio networks based on the eigenvalues of the
cyclic covariance matrix of received signals. In particular, the cyclic
correlation significance test is used to detect a specific signal-of-interest
by exploiting knowledge of its cyclic frequencies. Analytical expressions for
the probability of detection and probability of false-alarm under both
spatially uncorrelated or spatially correlated noise are derived and verified
by simulation. The detection performance in a Rayleigh flat-fading environment
is found and verified through simulations. One of the advantages of the
proposed method is that the detection threshold is shown to be independent of
both the number of samples and the noise covariance, effectively eliminating
the dependence on accurate noise estimation. The proposed method is also shown
to provide higher detection probability and better robustness to noise
uncertainty than existing multiple-antenna cyclostationary-based spectrum
sensing algorithms under both AWGN as well as a quasi-static Rayleigh fading
channel.
</summary>
    <author>
      <name>Paulo Urriza</name>
    </author>
    <author>
      <name>Eric Rebeiz</name>
    </author>
    <author>
      <name>Danijela Cabric</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JSAC.2013.131118</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JSAC.2013.131118" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 8 figures, submitted to IEEE JSAC: Cognitive Radio Series.
  arXiv admin note: substantial text overlap with arXiv:1210.8176</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.0313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.0313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.0557v1</id>
    <updated>2012-11-02T20:23:23Z</updated>
    <published>2012-11-02T20:23:23Z</published>
    <title>Stochastic Superoptimization</title>
    <summary>  We formulate the loop-free, binary superoptimization task as a stochastic
search problem. The competing constraints of transformation correctness and
performance improvement are encoded as terms in a cost function, and a Markov
Chain Monte Carlo sampler is used to rapidly explore the space of all possible
programs to find one that is an optimization of a given target program.
Although our method sacrifices com- pleteness, the scope of programs we are
able to reason about, and the quality of the programs we produce, far exceed
those of existing superoptimizers. Beginning from binaries com- piled by llvm
-O0 for 64-bit X86, our prototype implemen- tation, STOKE, is able to produce
programs which either match or outperform the code sequences produced by gcc
with full optimizations enabled, and, in some cases, expert handwritten
assembly.
</summary>
    <author>
      <name>Eric Schkufza</name>
    </author>
    <author>
      <name>Rahul Sharma</name>
    </author>
    <author>
      <name>Alex Aiken</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ASPLOS 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.0557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.0557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.5738v1</id>
    <updated>2012-11-25T07:56:54Z</updated>
    <published>2012-11-25T07:56:54Z</published>
    <title>Modeling the resilience of large and evolving systems</title>
    <summary>  This paper summarizes the state of knowledge and ongoing research on methods
and techniques for resilience evaluation, taking into account the
resilience-scaling challenges and properties related to the ubiquitous
computerized systems. We mainly focus on quantitative evaluation approaches
and, in particular, on model-based evaluation techniques that are commonly used
to evaluate and compare, from the dependability point of view, different
architecture alternatives at the design stage. We outline some of the main
modeling techniques aiming at mastering the largeness of analytical
dependability models at the construction level. Actually, addressing the model
largeness problem is important with respect to the investigation of the
scalability of current techniques to meet the complexity challenges of
ubiquitous systems. Finally we present two case studies in which some of the
presented techniques are applied for modeling web services and General Packet
Radio Service (GPRS) mobile telephone networks, as prominent examples of large
and evolving systems.
</summary>
    <author>
      <name>Mohamed Kaaniche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS</arxiv:affiliation>
    </author>
    <author>
      <name>Paolo Lollini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Florence</arxiv:affiliation>
    </author>
    <author>
      <name>Andrea Bondavalli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Florence</arxiv:affiliation>
    </author>
    <author>
      <name>Karama Kanoun</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Performability Engineering 4, 2 (2008)
  153-168</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.5738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.4846v1</id>
    <updated>2012-12-19T21:00:28Z</updated>
    <published>2012-12-19T21:00:28Z</published>
    <title>Operational semantics for product-form solution</title>
    <summary>  In this paper we present product-form solutions from the point of view of
stochastic process algebra. In previous work we have shown how to derive
product-form solutions for a formalism called Labelled Markov Automata (LMA).
LMA are very useful as their relation with the Continuous Time Markov Chains is
very direct. The disadvantage of using LMA is that the proofs of properties are
cumbersome. In fact, in LMA it is not possible to use the inductive structure
of the language in a proof. In this paper we consider a simple stochastic
process algebra that has the great advantage of simplifying the proofs. This
simple language has been inspired by PEPA, however, detailed analysis of the
semantics of cooperation will show the differences between the two formalisms.
It will also be shown that the semantics of the cooperation in process algebra
influences the correctness of the derivation of the product-form solutions.
</summary>
    <author>
      <name>Maria Grazia Vigliotti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.4846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.4846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.1078v1</id>
    <updated>2013-02-05T16:06:15Z</updated>
    <published>2013-02-05T16:06:15Z</published>
    <title>Performance Evaluation of Sparse Matrix Multiplication Kernels on Intel
  Xeon Phi</title>
    <summary>  Intel Xeon Phi is a recently released high-performance coprocessor which
features 61 cores each supporting 4 hardware threads with 512-bit wide SIMD
registers achieving a peak theoretical performance of 1Tflop/s in double
precision. Many scientific applications involve operations on large sparse
matrices such as linear solvers, eigensolver, and graph mining algorithms. The
core of most of these applications involves the multiplication of a large,
sparse matrix with a dense vector (SpMV). In this paper, we investigate the
performance of the Xeon Phi coprocessor for SpMV. We first provide a
comprehensive introduction to this new architecture and analyze its peak
performance with a number of micro benchmarks. Although the design of a Xeon
Phi core is not much different than those of the cores in modern processors,
its large number of cores and hyperthreading capability allow many application
to saturate the available memory bandwidth, which is not the case for many
cutting-edge processors. Yet, our performance studies show that it is the
memory latency not the bandwidth which creates a bottleneck for SpMV on this
architecture. Finally, our experiments show that Xeon Phi's sparse kernel
performance is very promising and even better than that of cutting-edge general
purpose processors and GPUs.
</summary>
    <author>
      <name>Erik Saule</name>
    </author>
    <author>
      <name>Kamer Kaya</name>
    </author>
    <author>
      <name>Umit V. Catalyurek</name>
    </author>
    <link href="http://arxiv.org/abs/1302.1078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.1078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5171v1</id>
    <updated>2013-02-21T03:59:28Z</updated>
    <published>2013-02-21T03:59:28Z</published>
    <title>Software model refactoring based on performance analysis: better working
  on software or performance side?</title>
    <summary>  Several approaches have been introduced in the last few years to tackle the
problem of interpreting model-based performance analysis results and
translating them into architectural feedback. Typically the interpretation can
take place by browsing either the software model or the performance model. In
this paper, we compare two approaches that we have recently introduced for this
goal: one based on the detection and solution of performance antipatterns, and
another one based on bidirectional model transformations between software and
performance models. We apply both approaches to the same example in order to
illustrate the differences in the obtained performance results. Thereafter, we
raise the level of abstraction and we discuss the pros and cons of working on
the software side and on the performance side.
</summary>
    <author>
      <name>Davide Arcelli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DISIM</arxiv:affiliation>
    </author>
    <author>
      <name>Vittorio Cortellessa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DISIM</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.108.3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.108.3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings FESCA 2013, arXiv:1302.4780</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 108, 2013, pp. 33-47</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.5171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Performance, Experimentation" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5514v1</id>
    <updated>2013-02-22T08:16:37Z</updated>
    <published>2013-02-22T08:16:37Z</published>
    <title>Blind Estimation of Primary User Traffic Parameters Under Sensing Errors</title>
    <summary>  In this work we investigate the bounds on the estimation accuracy of Primary
User (PU) traffic parameters with exponentially distributed busy and idle
times. We derive closed-form expressions for the Cramer-Rao bounds on the mean
squared estimation error for the blind joint estimation of the PU traffic
parameters, specifically, the duty cycle, and the mean arrival and departure
rates. Moreover, we present the corresponding maximum-likelihood estimators for
the traffic parameters. In addition, we derive a modified likelihood function
for the joint estimation of traffic parameters when spectrum sensing errors are
considered, and we present the impact of spectrum sensing errors on the
estimation error via simulations. Finally, we consider a duty cycle estimator,
common in traffic estimation literature, that is based on averaging the traffic
samples. We derive, in closed-form, the mean squared estimation error of the
considered estimator under spectrum sensing errors.
</summary>
    <author>
      <name>Wesam Gabran</name>
    </author>
    <author>
      <name>Przemysław Pawełczak</name>
    </author>
    <author>
      <name>Chun-Hao Liu</name>
    </author>
    <author>
      <name>Danijela Cabric</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICC.2013.6654889</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICC.2013.6654889" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to IEEE ICC 2013 [Extended version, with appendices]</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.5514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.1561v1</id>
    <updated>2013-03-06T22:24:37Z</updated>
    <published>2013-03-06T22:24:37Z</published>
    <title>Queuing Theoretic Analysis of Power-performance Tradeoff in
  Power-efficient Computing</title>
    <summary>  In this paper we study the power-performance relationship of power-efficient
computing from a queuing theoretic perspective. We investigate the interplay of
several system operations including processing speed, system on/off decisions,
and server farm size. We identify that there are oftentimes "sweet spots" in
power-efficient operations: there exist optimal combinations of processing
speed and system settings that maximize power efficiency. For the single server
case, a widely deployed threshold mechanism is studied. We show that there
exist optimal processing speed and threshold value pairs that minimize the
power consumption. This holds for the threshold mechanism with job batching.
For the multi-server case, it is shown that there exist best processing speed
and server farm size combinations.
</summary>
    <author>
      <name>Yanpei Liu</name>
    </author>
    <author>
      <name>Stark C. Draper</name>
    </author>
    <author>
      <name>Nam Sung Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper published in CISS 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.1561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.1561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.1651v2</id>
    <updated>2013-05-06T07:43:45Z</updated>
    <published>2013-03-07T11:40:27Z</published>
    <title>Model-guided Performance Analysis of the Sparse Matrix-Matrix
  Multiplication</title>
    <summary>  Achieving high efficiency with numerical kernels for sparse matrices is of
utmost importance, since they are part of many simulation codes and tend to use
most of the available compute time and resources. In addition, especially in
large scale simulation frameworks the readability and ease of use of
mathematical expressions are essential components for the continuous
maintenance, modification, and extension of software. In this context, the
sparse matrix-matrix multiplication is of special interest. In this paper we
thoroughly analyze the single-core performance of sparse matrix-matrix
multiplication kernels in the Blaze Smart Expression Template (SET) framework.
We develop simple models for estimating the achievable maximum performance, and
use them to assess the efficiency of our implementations. Additionally, we
compare these kernels with several commonly used SET-based C++ libraries,
which, just as Blaze, aim at combining the requirements of high performance
with an elegant user interface. For the different sparse matrix structures
considered here, we show that our implementations are competitive or faster
than those of the other SET libraries for most problem sizes on a current Intel
multicore processor.
</summary>
    <author>
      <name>Tobias Scharpff</name>
    </author>
    <author>
      <name>Klaus Iglberger</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Ulrich Ruede</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 12 figures. Small corrections w.r.t. previous version</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.1651v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.1651v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.3026v1</id>
    <updated>2013-03-12T20:59:46Z</updated>
    <published>2013-03-12T20:59:46Z</published>
    <title>Stochastic Service Curve and Delay Bound Analysis: A Single Node Case</title>
    <summary>  A packet-switched network node with constant capacity (in bps) is considered,
where packets within each flow are served in the first in first out (FIFO)
manner. While this single node system is perhaps the simplest computer
communication system, its stochastic service curve characterization and
independent case analysis in the context of stochastic network calculus
(snetcal) are still basic and many crucial questions surprisingly remain open.
Specifically, when the input is a single flow, what stochastic service curve
and delay bound does the node provide? When the considered flow shares the node
with another flow, what stochastic service curve and delay bound does the node
provide to the considered flow, and if the two flows are independent, can this
independence be made use of and how? The aim of this paper is to provide
answers to these fundamental questions.
</summary>
    <author>
      <name>Yuming Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/1303.3026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.3026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.4114v2</id>
    <updated>2013-07-22T13:43:49Z</updated>
    <published>2013-03-17T22:14:36Z</published>
    <title>Sharp Bounds in Stochastic Network Calculus</title>
    <summary>  The practicality of the stochastic network calculus (SNC) is often questioned
on grounds of potential looseness of its performance bounds. In this paper it
is uncovered that for bursty arrival processes (specifically Markov-Modulated
On-Off (MMOO)), whose amenability to \textit{per-flow} analysis is typically
proclaimed as a highlight of SNC, the bounds can unfortunately indeed be very
loose (e.g., by several orders of magnitude off). In response to this uncovered
weakness of SNC, the (Standard) per-flow bounds are herein improved by deriving
a general sample-path bound, using martingale based techniques, which
accommodates FIFO, SP, EDF, and GPS scheduling. The obtained (Martingale)
bounds gain an exponential decay factor of ${\mathcal{O}}(e^{-\alpha n})$ in
the number of flows $n$. Moreover, numerical comparisons against simulations
show that the Martingale bounds are remarkably accurate for FIFO, SP, and EDF
scheduling; for GPS scheduling, although the Martingale bounds substantially
improve the Standard bounds, they are numerically loose, demanding for
improvements in the core SNC analysis of GPS.
</summary>
    <author>
      <name>Florin Ciucu</name>
    </author>
    <author>
      <name>Felix Poloczek</name>
    </author>
    <author>
      <name>Jens Schmitt</name>
    </author>
    <link href="http://arxiv.org/abs/1303.4114v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.4114v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.6485v2</id>
    <updated>2013-08-27T15:14:26Z</updated>
    <published>2013-03-26T13:31:34Z</published>
    <title>Identifying Compiler Options to Minimise Energy Consumption for Embedded
  Platforms</title>
    <summary>  This paper presents an analysis of the energy consumption of an extensive
number of the optimisations a modern compiler can perform. Using GCC as a test
case, we evaluate a set of ten carefully selected benchmarks for five different
embedded platforms.
  A fractional factorial design is used to systematically explore the large
optimisation space (2^82 possible combinations), whilst still accurately
determining the effects of optimisations and optimisation combinations.
Hardware power measurements on each platform are taken to ensure all
architectural effects on the energy consumption are captured.
  We show that fractional factorial design can find more optimal combinations
than relying on built in compiler settings. We explore the relationship between
run-time and energy consumption, and identify scenarios where they are and are
not correlated.
  A further conclusion of this study is the structure of the benchmark has a
larger effect than the hardware architecture on whether the optimisation will
be effective, and that no single optimisation is universally beneficial for
execution time or energy consumption.
</summary>
    <author>
      <name>James Pallister</name>
    </author>
    <author>
      <name>Simon Hollis</name>
    </author>
    <author>
      <name>Jeremy Bennett</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/comjnl/bxt129</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/comjnl/bxt129" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.6485v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.6485v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3767v1</id>
    <updated>2013-04-13T03:48:53Z</updated>
    <published>2013-04-13T03:48:53Z</published>
    <title>A Taxonomy of Performance Assurance Methodologies and its Application in
  High Performance Computer Architectures</title>
    <summary>  This paper presents a systematic approach to the complex problem of high
confidence performance assurance of high performance architectures based on
methods used over several generations of industrial microprocessors. A taxonomy
is presented for performance assurance through three key stages of a product
life cycle-high level performance, RTL performance, and silicon performance.
The proposed taxonomy includes two components-independent performance assurance
space for each stage and a correlation performance assurance space between
stages. It provides a detailed insight into the performance assurance space in
terms of coverage provided taking into account capabilities and limitations of
tools and methodologies used at each stage. An application of the taxonomy to
cases described in the literature and to high performance Intel architectures
is shown. The proposed work should be of interest to manufacturers of high
performance microprocessor/chipset architectures and has not been discussed in
the literature.
</summary>
    <author>
      <name>Hemant Rotithor</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijsea.2013.4201</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijsea.2013.4201" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 5 figures, 3 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Software Engineering &amp; Applications
  (IJSEA), Vol.4, No.2, March 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.3767v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3767v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.3490v1</id>
    <updated>2013-05-15T14:18:11Z</updated>
    <published>2013-05-15T14:18:11Z</published>
    <title>Stationary analysis of the Shortest Queue First service policy</title>
    <summary>  We analyze the so-called Shortest Queue First (SQF) queueing discipline
whereby a unique server addresses queues in parallel by serving at any time
that queue with the smallest workload. Considering a stationary system composed
of two parallel queues and assuming Poisson arrivals and general service time
distributions, we first establish the functional equations satisfied by the
Laplace transforms of the workloads in each queue. We further specialize these
equations to the so-called "symmetric case", with same arrival rates and
identical exponential service time distributions at each queue; we then obtain
a functional equation $$ M(z) = q(z) \cdot M \circ h(z) + L(z) $$ for unknown
function $M$, where given functions $q$, $L$ and $h$ are related to one branch
of a cubic polynomial equation. We study the analyticity domain of function $M$
and express it by a series expansion involving all iterates of function $h$.
This allows us to determine empty queue probabilities along with the tail of
the workload distribution in each queue. This tail appears to be identical to
that of the Head-of-Line preemptive priority system, which is the key feature
desired for the SQF discipline.
</summary>
    <author>
      <name>Fabrice Guillemin</name>
    </author>
    <author>
      <name>Alain Simonian</name>
    </author>
    <link href="http://arxiv.org/abs/1305.3490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.3490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.3496v1</id>
    <updated>2013-05-15T14:39:03Z</updated>
    <published>2013-05-15T14:39:03Z</published>
    <title>Stationary analysis of the "Shortest Queue First" service policy: the
  asymmetric case</title>
    <summary>  As a follow-up to a recent paper considering two symmetric queues, the
\textit{Shortest Queue First} service discipline is presently analysed for two
general asymmetric queues. Using the results previously established and
assuming exponentially distributed service times, the bivariate Laplace
transform of workloads in each queue is shown to depend on the solution
$\mathbf{M}$ to a two-dimensional functional equation $$ \mathbf{M} = Q_1 \cdot
\mathbf{M}\circ h_1 + Q_2 \cdot \mathbf{M}\circ h_2 + \mathbf{L} $$ with given
matrices $Q_1$, $Q_2$ and vector $\mathbf{L}$ and where functions $h_1$ and
$h_2$ are defined each on some rational curve; solution $\mathbf{M}$ can then
represented by a series expansion involving the semi-group $&lt; h_1, h_2 &gt;$
generated by these two functions. The empty queue probabilities along with the
tail behaviour of the workload distribution at each queue are characterised.
</summary>
    <author>
      <name>Fabrice Guillemin</name>
    </author>
    <author>
      <name>Alain Simonian</name>
    </author>
    <link href="http://arxiv.org/abs/1305.3496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.3496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.6249v2</id>
    <updated>2016-03-22T17:58:44Z</updated>
    <published>2013-05-27T15:07:54Z</published>
    <title>A class of equivalent idle-time-order-based routing policies for
  heterogeneous multi-server systems</title>
    <summary>  We consider an M/M/N/K/FCFS system (N&gt;0, K&gt;=N), where the servers operate at
(possibly) heterogeneous service rates. In this situation, the steady state
behavior depends on the routing policy that is used to select which idle server
serves the next job in queue. We define a class of idle-time-order-based
policies (including, for example, Longest Idle Server First (LISF)) and show
that all policies in this class result in the same steady state behavior. In
particular, they are all equivalent to the naive Random routing policy.
</summary>
    <author>
      <name>Sherwin Doroudi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tepper School of Business, Carnegie Mellon University</arxiv:affiliation>
    </author>
    <author>
      <name>Ragavendran Gopalakrishnan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computing and Mathematical Sciences, California Institute of Technology</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has now been merged with the revised version of article
  arXiv:1402.3606 titled "Routing and Staffing when Servers are Strategic"</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.6249v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.6249v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60K25, 68M20, 90B22, 90B36" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.8; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.1217v1</id>
    <updated>2013-07-04T06:23:19Z</updated>
    <published>2013-07-04T06:23:19Z</published>
    <title>Toward a Unified Performance and Power Consumption NAND Flash Memory
  Model of Embedded and Solid State Secondary Storage Systems</title>
    <summary>  This paper presents a set of models dedicated to describe a flash storage
subsystem structure, functions, performance and power consumption behaviors.
These models cover a large range of today's NAND flash memory applications.
They are designed to be implemented in simulation tools allowing to estimate
and compare performance and power consumption of I/O requests on flash memory
based storage systems. Such tools can also help in designing and validating new
flash storage systems and management mechanisms. This work is integrated in a
global project aiming to build a framework simulating complex flash storage
hierarchies for performance and power consumption analysis. This tool will be
highly configurable and modular with various levels of usage complexity
according to the required aim: from a software user point of view for
simulating storage systems, to a developer point of view for designing, testing
and validating new flash storage management systems.
</summary>
    <author>
      <name>Pierre Olivier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Jalil Boukhobza</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Senn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">GDR Soc-Sip 2013 Meeting, Lyon : France (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.1217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.1217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.1743v1</id>
    <updated>2013-07-06T02:02:57Z</updated>
    <published>2013-07-06T02:02:57Z</published>
    <title>"The tail wags the dog": A study of anomaly detection in commercial
  application performance</title>
    <summary>  The IT industry needs systems management models that leverage available
application information to detect quality of service, scalability and health of
service. Ideally this technique would be common for varying application types
with different n-tier architectures under normal production conditions of
varying load, user session traffic, transaction type, transaction mix, and
hosting environment.
  This paper shows that a whole of service measurement paradigm utilizing a
black box M/M/1 queuing model and auto regression curve fitting of the
associated CDF are an accurate model to characterize system performance
signatures. This modeling method is also used to detect application slow down
events. The technique was shown to work for a diverse range of workloads
ranging from 76 Tx/ 5min to 19,025 Tx/ 5min. The method did not rely on
customizations specific to the n-tier architecture of the systems being
analyzed and so the performance anomaly detection technique was shown to be
platform and configuration agnostic.
</summary>
    <author>
      <name>Richard Gow</name>
    </author>
    <author>
      <name>Srikumar Venugopal</name>
    </author>
    <author>
      <name>Pradeep Ray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages; Longer version of the short paper accepted for MASCOTS 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.1743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.1743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.7271v1</id>
    <updated>2013-07-27T15:27:18Z</updated>
    <published>2013-07-27T15:27:18Z</published>
    <title>On the Catalyzing Effect of Randomness on the Per-Flow Throughput in
  Wireless Networks</title>
    <summary>  This paper investigates the throughput capacity of a flow crossing a
multi-hop wireless network, whose geometry is characterized by general
randomness laws including Uniform, Poisson, Heavy-Tailed distributions for both
the nodes' densities and the number of hops. The key contribution is to
demonstrate \textit{how} the \textit{per-flow throughput} depends on the
distribution of 1) the number of nodes $N_j$ inside hops' interference sets, 2)
the number of hops $K$, and 3) the degree of spatial correlations. The
randomness in both $N_j$'s and $K$ is advantageous, i.e., it can yield larger
scalings (as large as $\Theta(n)$) than in non-random settings. An interesting
consequence is that the per-flow capacity can exhibit the opposite behavior to
the network capacity, which was shown to suffer from a logarithmic decrease in
the presence of randomness. In turn, spatial correlations along the end-to-end
path are detrimental by a logarithmic term.
</summary>
    <author>
      <name>Florin Ciucu</name>
    </author>
    <author>
      <name>Jens Schmitt</name>
    </author>
    <link href="http://arxiv.org/abs/1307.7271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.7271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.7584v2</id>
    <updated>2013-08-01T15:50:37Z</updated>
    <published>2013-07-29T13:54:57Z</published>
    <title>Towards a System Theoretic Approach to Wireless Network Capacity in
  Finite Time and Space</title>
    <summary>  In asymptotic regimes, both in time and space (network size), the derivation
of network capacity results is grossly simplified by brushing aside queueing
behavior in non-Jackson networks. This simplifying double-limit model, however,
lends itself to conservative numerical results in finite regimes. To properly
account for queueing behavior beyond a simple calculus based on average rates,
we advocate a system theoretic methodology for the capacity problem in finite
time and space regimes. This methodology also accounts for spatial correlations
arising in networks with CSMA/CA scheduling and it delivers rigorous
closed-form capacity results in terms of probability distributions. Unlike
numerous existing asymptotic results, subject to anecdotal practical concerns,
our transient one can be used in practical settings: for example, to compute
the time scales at which multi-hop routing is more advantageous than single-hop
routing.
</summary>
    <author>
      <name>Florin Ciucu</name>
    </author>
    <author>
      <name>Ramin Khalili</name>
    </author>
    <author>
      <name>Yuming Jiang</name>
    </author>
    <author>
      <name>Liu Yang</name>
    </author>
    <author>
      <name>Yong Cui</name>
    </author>
    <link href="http://arxiv.org/abs/1307.7584v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.7584v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.7943v1</id>
    <updated>2013-07-30T12:34:49Z</updated>
    <published>2013-07-30T12:34:49Z</published>
    <title>The Implications of Diverse Applications and Scalable Data Sets in
  Benchmarking Big Data Systems</title>
    <summary>  Now we live in an era of big data, and big data applications are becoming
more and more pervasive. How to benchmark data center computer systems running
big data applications (in short big data systems) is a hot topic. In this
paper, we focus on measuring the performance impacts of diverse applications
and scalable volumes of data sets on big data systems. For four typical data
analysis applications---an important class of big data applications, we find
two major results through experiments: first, the data scale has a significant
impact on the performance of big data systems, so we must provide scalable
volumes of data sets in big data benchmarks. Second, for the four applications,
even all of them use the simple algorithms, the performance trends are
different with increasing data scales, and hence we must consider not only
variety of data sets but also variety of applications in benchmarking big data
systems.
</summary>
    <author>
      <name>Zhen Jia</name>
    </author>
    <author>
      <name>Runlin Zhou</name>
    </author>
    <author>
      <name>Chunge Zhu</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Wanling Gao</name>
    </author>
    <author>
      <name>Yingjie Shi</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <author>
      <name>Lixin Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.7943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.7943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.3123v1</id>
    <updated>2013-08-14T13:46:37Z</updated>
    <published>2013-08-14T13:46:37Z</published>
    <title>First experiences with the Intel MIC architecture at LRZ</title>
    <summary>  With the rapidly growing demand for computing power new accelerator based
architectures have entered the world of high performance computing since around
5 years. In particular GPGPUs have recently become very popular, however
programming GPGPUs using programming languages like CUDA or OpenCL is
cumbersome and error-prone. Trying to overcome these difficulties, Intel
developed their own Many Integrated Core (MIC) architecture which can be
programmed using standard parallel programming techniques like OpenMP and MPI.
In the beginning of 2013, the first production-level cards named Intel Xeon Phi
came on the market. LRZ has been considered by Intel as a leading research
centre for evaluating coprocessors based on the MIC architecture since 2010
under strict NDA. Since the Intel Xeon Phi is now generally available, we can
share our experience on programming Intel's new MIC architecture.
</summary>
    <author>
      <name>Volker Weinberg</name>
    </author>
    <author>
      <name>Momme Allalen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.3123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.5174v2</id>
    <updated>2013-09-28T16:16:03Z</updated>
    <published>2013-08-23T16:29:27Z</published>
    <title>BEEBS: Open Benchmarks for Energy Measurements on Embedded Platforms</title>
    <summary>  This paper presents and justifies an open benchmark suite named BEEBS,
targeted at evaluating the energy consumption of embedded processors.
  We explore the possible sources of energy consumption, then select individual
benchmarks from contemporary suites to cover these areas. Version one of BEEBS
is presented here and contains 10 benchmarks that cover a wide range of typical
embedded applications. The benchmark suite is portable across diverse
architectures and is freely available.
  The benchmark suite is extensively evaluated, and the properties of its
constituent programs are analysed. Using real hardware platforms we show case
examples which illustrate the difference in power dissipation between three
processor architectures and their related ISAs. We observe significant
differences in the average instruction dissipation between the architectures of
4.4x, specifically 170uW/MHz (ARM Cortex-M0), 65uW/MHz (Adapteva Epiphany) and
88uW/MHz (XMOS XS1-L1).
</summary>
    <author>
      <name>James Pallister</name>
    </author>
    <author>
      <name>Simon Hollis</name>
    </author>
    <author>
      <name>Jeremy Bennett</name>
    </author>
    <link href="http://arxiv.org/abs/1308.5174v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.5174v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.1613v1</id>
    <updated>2013-09-06T11:48:02Z</updated>
    <published>2013-09-06T11:48:02Z</published>
    <title>An Aggregation Technique For Large-Scale PEPA Models With Non-Uniform
  Populations</title>
    <summary>  Performance analysis based on modelling consists of two major steps: model
construction and model analysis. Formal modelling techniques significantly aid
model construction but can exacerbate model analysis. In particular, here we
consider the analysis of large-scale systems which consist of one or more
entities replicated many times to form large populations. The replication of
entities in such models can cause their state spaces to grow exponentially to
the extent that their exact stochastic analysis becomes computationally
expensive or even infeasible.
  In this paper, we propose a new approximate aggregation algorithm for a class
of large-scale PEPA models. For a given model, the method quickly checks if it
satisfies a syntactic condition, indicating that the model may be solved
approximately with high accuracy. If so, an aggregated CTMC is generated
directly from the model description. This CTMC can be used for efficient
derivation of an approximate marginal probability distribution over some of the
model's populations. In the context of a large-scale client-server system, we
demonstrate the usefulness of our method.
</summary>
    <author>
      <name>Alireza Pourranjbar</name>
    </author>
    <author>
      <name>Jane Hillston</name>
    </author>
    <link href="http://arxiv.org/abs/1309.1613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.1613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.2949v1</id>
    <updated>2013-12-10T05:15:32Z</updated>
    <published>2013-12-10T05:15:32Z</published>
    <title>A Survey of Embedded Software Profiling Methodologies</title>
    <summary>  Embedded Systems combine one or more processor cores with dedicated logic
running on an ASIC or FPGA to meet design goals at reasonable cost. It is
achieved by profiling the application with variety of aspects like performance,
memory usage, cache hit versus cache miss, energy consumption, etc. Out of
these, performance estimation is more important than others. With ever
increasing system complexities, it becomes quite necessary to carry out
performance estimation of embedded software implemented in a particular
processor for fast design space exploration. Such profiled data also guides the
designer how to partition the system for Hardware (HW) and Software (SW)
environments. In this paper, we propose a classification for currently
available Embedded Software Profiling Tools, and we present different academic
and industrial approaches in this context. Based on these observations, it will
be easy to identify such common principles and needs which are required for a
true Software Profiling Tool for a particular application.
</summary>
    <author>
      <name>Rajendra Patel</name>
    </author>
    <author>
      <name>Arvind Rajwat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijesa.2011.1203</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijesa.2011.1203" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 Pages, 14 Figures, 2 Tables, International Journal of Embedded
  Systems and Applications (IJESA)</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.2949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.3824v1</id>
    <updated>2014-01-16T04:18:47Z</updated>
    <published>2014-01-16T04:18:47Z</published>
    <title>Power Aware Wireless File Downloading: A Constrained Restless Bandit
  Approach</title>
    <summary>  This paper treats power-aware throughput maximization in a multi-user file
downloading system. Each user can receive a new file only after its previous
file is finished. The file state processes for each user act as coupled Markov
chains that form a generalized restless bandit system. First, an optimal
algorithm is derived for the case of one user. The algorithm maximizes
throughput subject to an average power constraint. Next, the one-user algorithm
is extended to a low complexity heuristic for the multi-user problem. The
heuristic uses a simple online index policy and its effectiveness is shown via
simulation. For simple 3-user cases where the optimal solution can be computed
offline, the heuristic is shown to be near-optimal for a wide range of
parameters.
</summary>
    <author>
      <name>Xiaohan Wei</name>
    </author>
    <author>
      <name>Michael J. Neely</name>
    </author>
    <link href="http://arxiv.org/abs/1401.3824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.3824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.6020v1</id>
    <updated>2014-01-23T15:51:32Z</updated>
    <published>2014-01-23T15:51:32Z</published>
    <title>A Brief Review on Models for Performance Evaluation in DSS Architecture</title>
    <summary>  Distributed Software Systems are used these days by many people in the real
time operations and modern enterprise applications. One of the most important
and essential attributes of measurements for the quality of service of
distributed software is performance. Performance models can be employed at
early stages of the software development cycle to characterize the quantitative
behavior of software systems. In this research, performance models based on
fuzzy logic approach, queuing network approach and Petri net approach have been
reviewed briefly. One of the most common ways in performance analysis of
distributed software systems is translating the UML diagrams to mathematical
modeling languages for the description of distributed systems such as queuing
networks or Petri nets. In this paper, some of these approaches are reviewed
briefly. Attributes which are used for performance modeling in the literature
are mostly machine based. On the other hand, end users and client parameters
for performance evaluation are not covered extensively. In this way, future
research could be based on developing hybrid models to capture user decision
variables which make system performance evaluation more user driven.
</summary>
    <author>
      <name>Ghassem Tofighi</name>
    </author>
    <author>
      <name>Kaamran Raahemifar</name>
    </author>
    <author>
      <name>Anastasios N. Venetsanopoulos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.7321/jscse.v3.n3.68</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.7321/jscse.v3.n3.68" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Find it online here: http://www.jscse.com/papers/?vol=3&amp;no=3&amp;n=68</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.6020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.6020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.5194v1</id>
    <updated>2014-02-21T03:03:16Z</updated>
    <published>2014-02-21T03:03:16Z</published>
    <title>On Big Data Benchmarking</title>
    <summary>  Big data systems address the challenges of capturing, storing, managing,
analyzing, and visualizing big data. Within this context, developing benchmarks
to evaluate and compare big data systems has become an active topic for both
research and industry communities. To date, most of the state-of-the-art big
data benchmarks are designed for specific types of systems. Based on our
experience, however, we argue that considering the complexity, diversity, and
rapid evolution of big data systems, for the sake of fairness, big data
benchmarks must include diversity of data and workloads. Given this motivation,
in this paper, we first propose the key requirements and challenges in
developing big data benchmarks from the perspectives of generating data with 4V
properties (i.e. volume, velocity, variety and veracity) of big data, as well
as generating tests with comprehensive workloads for big data systems. We then
present the methodology on big data benchmarking designed to address these
challenges. Next, the state-of-the-art are summarized and compared, following
by our vision for future research directions.
</summary>
    <author>
      <name>Rui Han</name>
    </author>
    <author>
      <name>Xiaoyi Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures, 2 tables, accepted in BPOE-04
  (http://prof.ict.ac.cn/bpoe_4_asplos/)</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.5194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.5194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.5987v1</id>
    <updated>2014-02-24T21:19:39Z</updated>
    <published>2014-02-24T21:19:39Z</published>
    <title>Exact Analysis of TTL Cache Networks: The Case of Caching Policies
  driven by Stopping Times</title>
    <summary>  TTL caching models have recently regained significant research interest,
largely due to their ability to fit popular caching policies such as LRU. This
paper advances the state-of-the-art analysis of TTL-based cache networks by
developing two exact methods with orthogonal generality and computational
complexity. The first method generalizes existing results for line networks
under renewal requests to the broad class of caching policies whereby evictions
are driven by stopping times. The obtained results are further generalized,
using the second method, to feedforward networks with Markov arrival processes
(MAP) requests. MAPs are particularly suitable for non-line networks because
they are closed not only under superposition and splitting, as known, but also
under input-output caching operations as proven herein for phase-type TTL
distributions. The crucial benefit of the two closure properties is that they
jointly enable the first exact analysis of feedforward networks of TTL caches
in great generality.
</summary>
    <author>
      <name>Daniel S. Berger</name>
    </author>
    <author>
      <name>Philipp Gland</name>
    </author>
    <author>
      <name>Sahil Singla</name>
    </author>
    <author>
      <name>Florin Ciucu</name>
    </author>
    <link href="http://arxiv.org/abs/1402.5987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.5987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.3480v1</id>
    <updated>2014-03-14T03:06:34Z</updated>
    <published>2014-03-14T03:06:34Z</published>
    <title>Performance Benefits of DataMPI: A Case Study with BigDataBench</title>
    <summary>  Apache Hadoop and Spark are gaining prominence in Big Data processing and
analytics. Both of them are widely deployed on Internet companies. On the other
hand, high-performance data analysis requirements are causing academical and
industrial communities to adopt state-of-the-art technologies in HPC to solve
Big Data problems. Recently, we have proposed a key-value pair based
communication library, DataMPI, which is extending MPI to support
Hadoop/Spark-like Big Data Computing jobs. In this paper, we use BigDataBench,
a Big Data benchmark suite, to do comprehensive studies on performance and
resource utilization characterizations of Hadoop, Spark and DataMPI. From our
experiments, we observe that the job execution time of DataMPI has up to 55%
and 39% speedups compared with those of Hadoop and Spark, respectively. Most of
the benefits come from the high-efficiency communication mechanisms in DataMPI.
We also notice that the resource (CPU, memory, disk and network I/O)
utilizations of DataMPI are also more efficient than those of the other two
frameworks.
</summary>
    <author>
      <name>Fan Liang</name>
    </author>
    <author>
      <name>Chen Feng</name>
    </author>
    <author>
      <name>Xiaoyi Lu</name>
    </author>
    <author>
      <name>Zhiwei Xu</name>
    </author>
    <link href="http://arxiv.org/abs/1403.3480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.3480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.2266v1</id>
    <updated>2014-04-08T09:44:11Z</updated>
    <published>2014-04-08T09:44:11Z</published>
    <title>Enhanced Cluster Computing Performance Through Proportional Fairness</title>
    <summary>  The performance of cluster computing depends on how concurrent jobs share
multiple data center resource types like CPU, RAM and disk storage. Recent
research has discussed efficiency and fairness requirements and identified a
number of desirable scheduling objectives including so-called dominant resource
fairness (DRF). We argue here that proportional fairness (PF), long recognized
as a desirable objective in sharing network bandwidth between ongoing flows, is
preferable to DRF. The superiority of PF is manifest under the realistic
modelling assumption that the population of jobs in progress is a stochastic
process. In random traffic the strategy-proof property of DRF proves
unimportant while PF is shown by analysis and simulation to offer a
significantly better efficiency-fairness tradeoff.
</summary>
    <author>
      <name>Thomas Bonald</name>
    </author>
    <author>
      <name>James Roberts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Performance 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.2266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.2266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.4865v2</id>
    <updated>2014-04-21T15:09:46Z</updated>
    <published>2014-04-18T19:34:24Z</published>
    <title>On Time-Sensitive Revenue Management and Energy Scheduling in Green Data
  Centers</title>
    <summary>  In this paper, we design an analytically and experimentally better online
energy and job scheduling algorithm with the objective of maximizing net profit
for a service provider in green data centers. We first study the previously
known algorithms and conclude that these online algorithms have provable poor
performance against their worst-case scenarios. To guarantee an online
algorithm's performance in hindsight, we design a randomized algorithm to
schedule energy and jobs in the data centers and prove the algorithm's expected
competitive ratio in various settings. Our algorithm is theoretical-sound and
it outperforms the previously known algorithms in many settings using both real
traces and simulated data. An optimal offline algorithm is also implemented as
an empirical benchmark.
</summary>
    <author>
      <name>Huangxin Wang</name>
    </author>
    <author>
      <name>Jean X. Zhang</name>
    </author>
    <author>
      <name>Fei Li</name>
    </author>
    <link href="http://arxiv.org/abs/1404.4865v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.4865v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5121v1</id>
    <updated>2014-04-21T06:12:22Z</updated>
    <published>2014-04-21T06:12:22Z</published>
    <title>SleepScale: Runtime Joint Speed Scaling and Sleep States Management for
  Power Efficient Data Centers</title>
    <summary>  Power consumption in data centers has been growing significantly in recent
years. To reduce power, servers are being equipped with increasingly
sophisticated power management mechanisms. Different mechanisms offer
dramatically different trade-offs between power savings and performance
penalties. Considering the complexity, variety, and temporally varying nature
of the applications hosted in a typical data center, intelligently determining
which power management policy to use and when is a complicated task.
  In this paper we analyze a system model featuring both performance scaling
and low-power states. We reveal the interplay between performance scaling and
low-power states via intensive simulation and analytic verification. Based on
the observations, we present SleepScale, a runtime power management tool
designed to efficiently exploit existing power control mechanisms. At run time,
SleepScale characterizes power consumption and quality-of-service (QoS) for
each low-power state and frequency setting, and selects the best policy for a
given QoS constraint. We evaluate SleepScale using workload traces from data
centers and achieve significant power savings relative to conventional power
management strategies.
</summary>
    <author>
      <name>Yanpei Liu</name>
    </author>
    <author>
      <name>Stark C. Draper</name>
    </author>
    <author>
      <name>Nam Sung Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACM/IEEE International Symposium on Computer Architecture
  (ISCA) 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.5121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5406v1</id>
    <updated>2014-04-22T07:40:51Z</updated>
    <published>2014-04-22T07:40:51Z</published>
    <title>Degradation Analysis of Probabilistic Parallel Choice Systems</title>
    <summary>  Degradation analysis is used to analyze the useful lifetimes of systems,
their failure rates, and various other system parameters like mean time to
failure (MTTF), mean time between failures (MTBF), and the system failure rate
(SFR). In many systems, certain possible parallel paths of execution that have
greater chances of success are preferred over others. Thus we introduce here
the concept of probabilistic parallel choice. We use binary and $n$-ary
probabilistic choice operators in describing the selections of parallel paths.
These binary and $n$-ary probabilistic choice operators are considered so as to
represent the complete system (described as a series-parallel system) in terms
of the probabilities of selection of parallel paths and their relevant
parameters. Our approach allows us to derive new and generalized formulae for
system parameters like MTTF, MTBF, and SFR. We use a generalized exponential
distribution, allowing distinct installation times for individual components,
and use this model to derive expressions for such system parameters.
</summary>
    <author>
      <name>Avinash Saxena</name>
    </author>
    <author>
      <name>Shrisha Rao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0218539314500120</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0218539314500120" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Reliability, Quality and Safety
  Engineering, vol. 21(3), June 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.5406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90B25, 60K10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.3084v4</id>
    <updated>2016-02-22T13:11:19Z</updated>
    <published>2014-06-12T00:01:54Z</published>
    <title>Exact Solutions for M/M/c/Setup Queues</title>
    <summary>  Recently multiserver queues with setup times have been extensively studied
because they have applications in power-saving data centers. The most
challenging model is the M/M/$c$/Setup queue where a server is turned off when
it is idle and is turned on if there are some waiting jobs. Recently, Gandhi et
al.~(SIGMETRICS 2013, QUESTA 2014) present the recursive renewal reward
approach as a new mathematical tool to analyze the model. In this paper, we
derive exact solutions for the same model using two alternative methodologies:
generating function approach and matrix analytic method. The former yields
several theoretical insights into the systems while the latter provides an
exact recursive algorithm to calculate the joint stationary distribution and
then some performance measures so as to give new application insights.
</summary>
    <author>
      <name>Tuan Phung-Duc</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for review</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.3084v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3084v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.7527v1</id>
    <updated>2014-06-29T17:23:26Z</updated>
    <published>2014-06-29T17:23:26Z</published>
    <title>Dealing with Zero Density Using Piecewise Phase-type Approximation</title>
    <summary>  Every probability distribution can be approximated up to a given precision by
a phase-type distribution, i.e. a distribution encoded by a continuous time
Markov chain (CTMC). However, an excessive number of states in the
corresponding CTMC is needed for some standard distributions, in particular
most distributions with regions of zero density such as uniform or shifted
distributions. Addressing this class of distributions, we suggest an
alternative representation by CTMC extended with discrete-time transitions.
Using discrete-time transitions we split the density function into multiple
intervals. Within each interval, we then approximate the density with standard
phase-type fitting. We provide an experimental evidence that our method
requires only a moderate number of states to approximate such distributions
with regions of zero density. Furthermore, the usage of CTMC with discrete-time
transitions is supported by a number of techniques for their analysis. Thus,
our results promise an efficient approach to the transient analysis of a class
of non-Markovian models.
</summary>
    <author>
      <name>Ľuboš Korenčiak</name>
    </author>
    <author>
      <name>Jan Krčál</name>
    </author>
    <author>
      <name>Vojtěch Řehák</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended version of paper with same name accepted to 11th European
  Workshop on Performance Engineering (EPEW 2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.7527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.7527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.7539v1</id>
    <updated>2014-06-29T19:23:49Z</updated>
    <published>2014-06-29T19:23:49Z</published>
    <title>Exploring Task Mappings on Heterogeneous MPSoCs using a Bias-Elitist
  Genetic Algorithm</title>
    <summary>  Exploration of task mappings plays a crucial role in achieving high
performance in heterogeneous multi-processor system-on-chip (MPSoC) platforms.
The problem of optimally mapping a set of tasks onto a set of given
heterogeneous processors for maximal throughput has been known, in general, to
be NP-complete. The problem is further exacerbated when multiple applications
(i.e., bigger task sets) and the communication between tasks are also
considered. Previous research has shown that Genetic Algorithms (GA) typically
are a good choice to solve this problem when the solution space is relatively
small. However, when the size of the problem space increases, classic genetic
algorithms still suffer from the problem of long evolution times. To address
this problem, this paper proposes a novel bias-elitist genetic algorithm that
is guided by domain-specific heuristics to speed up the evolution process.
Experimental results reveal that our proposed algorithm is able to handle large
scale task mapping problems and produces high-quality mapping solutions in only
a short time period.
</summary>
    <author>
      <name>Wei Quan</name>
    </author>
    <author>
      <name>Andy D. Pimentel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 11 figures, uses algorithm2e.sty</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.7539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.7539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.3267v2</id>
    <updated>2016-06-01T01:20:43Z</updated>
    <published>2014-07-10T06:49:42Z</published>
    <title>Analysis and Approximation of Dual Tandem Queues with Finite Buffer
  Capacity</title>
    <summary>  Tandem queues with finite buffer capacity commonly exist in practical
applications. By viewing a tandem queue as an integrated system, an innovative
approach has been developed to analyze its performance through the insight from
reduction method. In our approach, the starvation at the bottleneck caused by
service time randomness is modeled and captured by interruptions. Fundamental
properties of tandem queues with finite buffer capacity are examined. We show
that in general system service rate of a dual tandem queue with finite buffer
capacity is equal or smaller than its bottleneck service rate, and virtual
interruptions, which are the extra idle period at the bottleneck caused by the
non-bottlenecks, depend on arrival rates. Hence, system service rate is a
function of arrival rate when the buffer capacity of a tandem queue is finite.
Approximation for the mean queue time of a dual tandem queue has been developed
through the concept of virtual interruptions.
</summary>
    <author>
      <name>Kan Wu</name>
    </author>
    <author>
      <name>Ning Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proposition 4 and Corollary 5 are wrong. The proofs have obvious
  defects</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.3267v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.3267v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.4777v4</id>
    <updated>2016-04-15T15:36:11Z</updated>
    <published>2014-07-17T19:10:57Z</published>
    <title>Optimizing Performance of Continuous-Time Stochastic Systems using
  Timeout Synthesis</title>
    <summary>  We consider parametric version of fixed-delay continuous-time Markov chains
(or equivalently deterministic and stochastic Petri nets, DSPN) where
fixed-delay transitions are specified by parameters, rather than concrete
values. Our goal is to synthesize values of these parameters that, for a given
cost function, minimise expected total cost incurred before reaching a given
set of target states. We show that under mild assumptions, optimal values of
parameters can be effectively approximated using translation to a Markov
decision process (MDP) whose actions correspond to discretized values of these
parameters.
</summary>
    <author>
      <name>Tomáš Brázdil</name>
    </author>
    <author>
      <name>Ľuboš Korenčiak</name>
    </author>
    <author>
      <name>Jan Krčál</name>
    </author>
    <author>
      <name>Petr Novotný</name>
    </author>
    <author>
      <name>Vojtěch Řehák</name>
    </author>
    <link href="http://arxiv.org/abs/1407.4777v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.4777v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.4964v1</id>
    <updated>2014-08-21T11:23:54Z</updated>
    <published>2014-08-21T11:23:54Z</published>
    <title>High Level Programming for Heterogeneous Architectures</title>
    <summary>  This work presents an effort to bridge the gap between abstract high level
programming and OpenCL by extending an existing high level Java programming
framework (APARAPI), based on OpenCL, so that it can be used to program FPGAs
at a high level of abstraction and increased ease of programmability. We run
several real world algorithms to assess the performance of the framework on
both a low end and a high end system. On the low end and high end systems
respectively we observed up to 78-80 percent power reduction and 4.8X-5.3X
speed increase running NBody simulation, as well as up to 65-80 percent power
reduction and 6.2X-7X speed increase for a KMeans, MapReduce algorithm running
on top of the Hadoop framework and APARAPI.
</summary>
    <author>
      <name>Oren Segal</name>
    </author>
    <author>
      <name>Martin Margala</name>
    </author>
    <author>
      <name>Sai Rahul Chalamalasetti</name>
    </author>
    <author>
      <name>Mitch Wright</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at First International Workshop on FPGAs for Software
  Programmers (FSP 2014) (arXiv:1408.4423)</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.4964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.4964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.6721v2</id>
    <updated>2015-02-22T23:57:45Z</updated>
    <published>2014-08-26T15:20:55Z</published>
    <title>Performance Analysis of Linear-Equality-Constrained Least-Squares
  Estimation</title>
    <summary>  We analyze the performance of a linear-equality-constrained least-squares
(CLS) algorithm and its relaxed version, called rCLS, that is obtained via the
method of weighting. The rCLS algorithm solves an unconstrained least-squares
problem that is augmented by incorporating a weighted form of the linear
constraints. As a result, unlike the CLS algorithm, the rCLS algorithm is
amenable to our approach to performance analysis presented here, which is akin
to the energy-conservation-based methodology. Therefore, we initially inspect
the convergence properties and evaluate the precision of estimation as well as
satisfaction of the constraints for the rCLS algorithm in both mean and
mean-square senses. Afterwards, we examine the performance of the CLS algorithm
by evaluating the limiting performance of the rCLS algorithm as the relaxation
parameter (weight) approaches infinity. Numerical examples verify the accuracy
of the theoretical findings.
</summary>
    <author>
      <name>Reza Arablouei</name>
    </author>
    <author>
      <name>Kutluyıl Doğançay</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2015.2424199</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2015.2424199" rel="related"/>
    <link href="http://arxiv.org/abs/1408.6721v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.6721v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0792v1</id>
    <updated>2014-09-01T10:57:16Z</updated>
    <published>2014-09-01T10:57:16Z</published>
    <title>Characterizing and Subsetting Big Data Workloads</title>
    <summary>  Big data benchmark suites must include a diversity of data and workloads to
be useful in fairly evaluating big data systems and architectures. However,
using truly comprehensive benchmarks poses great challenges for the
architecture community. First, we need to thoroughly understand the behaviors
of a variety of workloads. Second, our usual simulation-based research methods
become prohibitively expensive for big data. As big data is an emerging field,
more and more software stacks are being proposed to facilitate the development
of big data applications, which aggravates hese challenges. In this paper, we
first use Principle Component Analysis (PCA) to identify the most important
characteristics from 45 metrics to characterize big data workloads from
BigDataBench, a comprehensive big data benchmark suite. Second, we apply a
clustering technique to the principle components obtained from the PCA to
investigate the similarity among big data workloads, and we verify the
importance of including different software stacks for big data benchmarking.
Third, we select seven representative big data workloads by removing redundant
ones and release the BigDataBench simulation version, which is publicly
available from http://prof.ict.ac.cn/BigDataBench/simulatorversion/.
</summary>
    <author>
      <name>Zhen Jia</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Rui Han</name>
    </author>
    <author>
      <name>Sally A. McKee</name>
    </author>
    <author>
      <name>Qiang Yang</name>
    </author>
    <author>
      <name>Chunjie Luo</name>
    </author>
    <author>
      <name>Jingwei Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IISWC.2014.6983058</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IISWC.2014.6983058" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures, 2014 IEEE International Symposium on Workload
  Characterization</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.5622v1</id>
    <updated>2014-09-19T12:26:05Z</updated>
    <published>2014-09-19T12:26:05Z</published>
    <title>Instability of Sharing Systems in the Presence of Retransmissions</title>
    <summary>  Retransmissions represent a primary failure recovery mechanism on all layers
of communication network architecture. Similarly, fair sharing, e.g. processor
sharing (PS), is a widely accepted approach to resource allocation among
multiple users. Recent work has shown that retransmissions in failure-prone,
e.g. wireless ad hoc, networks can cause heavy tails and long delays. In this
paper, we discover a new phenomenon showing that PS-based scheduling induces
complete instability with zero throughput in the presence of retransmissions,
regardless of how low the traffic load may be. This phenomenon occurs even when
the job sizes are bounded/fragmented, e.g. deterministic. Our analytical
results are further validated via simulation experiments. Moreover, our work
demonstrates that scheduling one job at a time, such as first-come-first-serve,
achieves stability and should be preferred in these systems.
</summary>
    <author>
      <name>Predrag R. Jelenković</name>
    </author>
    <author>
      <name>Evangelia D. Skiani</name>
    </author>
    <link href="http://arxiv.org/abs/1409.5622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.5622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.6775v2</id>
    <updated>2014-09-27T06:48:49Z</updated>
    <published>2014-09-23T23:52:36Z</published>
    <title>A Queueing Network Approach to the Analysis and Control of
  Mobility-On-Demand Systems</title>
    <summary>  This paper presents a queueing network approach to the analysis and control
of mobility-on-demand (MoD) systems for urban personal transportation. A MoD
system consists of a fleet of vehicles providing one-way car sharing service
and a team of drivers to rebalance such vehicles. The drivers then rebalance
themselves by driving select customers similar to a taxi service. We model the
MoD system as two coupled closed Jackson networks with passenger loss. We show
that the system can be approximately balanced by solving two decoupled linear
programs and exactly balanced through nonlinear optimization. The rebalancing
techniques are applied to a system sizing example using taxi data in three
neighborhoods of Manhattan, which suggests that the optimal vehicle-to-driver
ratio in a MoD system is between 3 and 5. Lastly, we formulate a real-time
closed-loop rebalancing policy for drivers and demonstrate its stability (in
terms of customer wait times) for typical system loads.
</summary>
    <author>
      <name>Rick Zhang</name>
    </author>
    <author>
      <name>Marco Pavone</name>
    </author>
    <link href="http://arxiv.org/abs/1409.6775v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.6775v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.0804v3</id>
    <updated>2014-10-11T09:11:41Z</updated>
    <published>2014-10-03T10:13:03Z</published>
    <title>Multi-step Uniformization with Steady-State Detection in Nonstationary
  M/M/s Queuing Systems</title>
    <summary>  A new approach to the steady state detection in the uniformization method of
solving continuous time Markov chains is introduced. The method is particularly
useful in solving inhomogenous CTMC's in multiple steps, where the desired
error bound of the whole solution can be distributed not proportionally to the
lengths of the respective intervals, but rather in a way, that maximizes the
chances of detecting a steady state. Additionally, the convergence properties
of the underlying DTMC are used to further enhance the computational savings
due to the steady state detection. The method is applied to the problem of
modeling a Call Center using inhomogenous CTMC model of a M(t)/M(t)/s(t)
queuing system.
</summary>
    <author>
      <name>Maciej Burak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">added DOI information in References</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.0804v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.0804v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.4168v1</id>
    <updated>2014-10-15T18:57:12Z</updated>
    <published>2014-10-15T18:57:12Z</published>
    <title>Efficient HTTP based I/O on very large datasets for high performance
  computing with the libdavix library</title>
    <summary>  Remote data access for data analysis in high performance computing is
commonly done with specialized data access protocols and storage systems. These
protocols are highly optimized for high throughput on very large datasets,
multi-streams, high availability, low latency and efficient parallel I/O. The
purpose of this paper is to describe how we have adapted a generic protocol,
the Hyper Text Transport Protocol (HTTP) to make it a competitive alternative
for high performance I/O and data analysis applications in a global computing
grid: the Worldwide LHC Computing Grid. In this work, we first analyze the
design differences between the HTTP protocol and the most common high
performance I/O protocols, pointing out the main performance weaknesses of
HTTP. Then, we describe in detail how we solved these issues. Our solutions
have been implemented in a toolkit called davix, available through several
recent Linux distributions. Finally, we describe the results of our benchmarks
where we compare the performance of davix against a HPC specific protocol for a
data analysis use case.
</summary>
    <author>
      <name>Adrien Devresse</name>
    </author>
    <author>
      <name>Fabrizio Furano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at: Very large Data Bases (VLDB) 2014, Hangzhou</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.4168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.4168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.5010v2</id>
    <updated>2015-01-17T14:07:26Z</updated>
    <published>2014-10-18T21:49:45Z</published>
    <title>Quantifying performance bottlenecks of stencil computations using the
  Execution-Cache-Memory model</title>
    <summary>  Stencil algorithms on regular lattices appear in many fields of computational
science, and much effort has been put into optimized implementations. Such
activities are usually not guided by performance models that provide estimates
of expected speedup. Understanding the performance properties and bottlenecks
by performance modeling enables a clear view on promising optimization
opportunities. In this work we refine the recently developed
Execution-Cache-Memory (ECM) model and use it to quantify the performance
bottlenecks of stencil algorithms on a contemporary Intel processor. This
includes applying the model to arrive at single-core performance and
scalability predictions for typical corner case stencil loop kernels. Guided by
the ECM model we accurately quantify the significance of "layer conditions,"
which are required to estimate the data traffic through the memory hierarchy,
and study the impact of typical optimization approaches such as spatial
blocking, strength reduction, and temporal blocking for their expected
benefits. We also compare the ECM model to the widely known Roofline model.
</summary>
    <author>
      <name>Holger Stengel</name>
    </author>
    <author>
      <name>Jan Treibig</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2751205.2751240</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2751205.2751240" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures. Added Roofline comparison and other minor
  improvements</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.5010v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5010v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.5102v1</id>
    <updated>2014-10-19T18:32:37Z</updated>
    <published>2014-10-19T18:32:37Z</published>
    <title>On Bootstrapping Machine Learning Performance Predictors via Analytical
  Models</title>
    <summary>  Performance modeling typically relies on two antithetic methodologies: white
box models, which exploit knowledge on system's internals and capture its
dynamics using analytical approaches, and black box techniques, which infer
relations among the input and output variables of a system based on the
evidences gathered during an initial training phase. In this paper we
investigate a technique, which we name Bootstrapping, which aims at reconciling
these two methodologies and at compensating the cons of the one with the pros
of the other. We thoroughly analyze the design space of this gray box modeling
technique, and identify a number of algorithmic and parametric trade-offs which
we evaluate via two realistic case studies, a Key-Value Store and a Total Order
Broadcast service.
</summary>
    <author>
      <name>Diego Didona</name>
    </author>
    <author>
      <name>Paolo Romano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.5102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.5561v1</id>
    <updated>2014-10-21T07:35:32Z</updated>
    <published>2014-10-21T07:35:32Z</published>
    <title>Towards energy efficiency and maximum computational intensity for
  stencil algorithms using wavefront diamond temporal blocking</title>
    <summary>  We study the impact of tunable parameters on computational intensity (i.e.,
inverse code balance) and energy consumption of multicore-optimized wavefront
diamond temporal blocking (MWD) applied to different stencil-based update
schemes. MWD combines the concepts of diamond tiling and multicore-aware
wavefront blocking in order to achieve lower cache size requirements than
standard single-core wavefront temporal blocking. We analyze the impact of the
cache block size on the theoretical and observed code balance, introduce loop
tiling in the leading dimension to widen the range of applicable diamond sizes,
and show performance results on a contemporary Intel CPU. The impact of code
balance on power dissipation on the CPU and in the DRAM is investigated and
shows that DRAM power is a decisive factor for energy consumption, which is
strongly influenced by the code balance. Furthermore we show that highest
performance does not necessarily lead to lowest energy even if the clock speed
is fixed.
</summary>
    <author>
      <name>Tareq Malas</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Hatem Ltaief</name>
    </author>
    <author>
      <name>David Keyes</name>
    </author>
    <link href="http://arxiv.org/abs/1410.5561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.2047v1</id>
    <updated>2014-11-07T22:05:16Z</updated>
    <published>2014-11-07T22:05:16Z</published>
    <title>Investigation of the relationship between code change set n-grams and
  change in energy consumption</title>
    <summary>  The amount of software running on mobile devices is constantly growing as
consumers and industry purchase more battery powered devices. On the other
hand, tools that provide developers with feed- back on how their software
changes affect battery life are not widely available. This work employs Green
Mining, the study of the rela- tionship between energy consumption and software
changesets, and n-gram language models to evaluate if source code changeset
perplex- ity correlates with change in energy consumption. A correlation be-
tween perplexity and change in energy consumption would permit the development
of a tool that predicts the impact a code changeset may have on a software
applications energy consumption. The case study results show that there is weak
to no correlation between cross en- tropy and change in energy consumption.
Therefore, future areas of investigation are proposed.
</summary>
    <author>
      <name>Stephen Romansky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.2047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.2867v1</id>
    <updated>2014-11-11T16:13:32Z</updated>
    <published>2014-11-11T16:13:32Z</published>
    <title>Buffer occupancy asymptotics in rate proportional sharing networks with
  heterogeneous long-tailed inputs</title>
    <summary>  In this paper, we consider a network of rate proportional processor sharing
servers in which sessions with long-tailed duration arrive as Poisson
processes. In particular, we assume that a session of type $n$ transmits at a
rate $r_n$ bits per unit time and lasts for a random time $\tau_n$ with a
generalized Pareto distribution given by $P \{\tau_n &gt; x\} \sim \alpha_n
x^{-(1+\beta_n)}$ for large $x$, where $\alpha_n, \beta_n &gt; 0$. The weights are
taken to be the rates of the flows. The network is assumed to be loop-free with
respect to source-destination routes. We characterize the order $O-$asymptotics
of the complementary buffer occupancy distribution at each node in terms of the
input characteristics of the sessions. In particular, we show that the
distributions obey a power law whose exponent can be calculated via solving a
fixed point and deterministic knapsack problem. The paper concludes with some
canonical examples.
</summary>
    <author>
      <name>Oczan Ozturk</name>
    </author>
    <author>
      <name>Ravi R. Mazumdar</name>
    </author>
    <author>
      <name>Nikolay B. Likhanov</name>
    </author>
    <link href="http://arxiv.org/abs/1411.2867v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2867v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 60J25, Secondary 68M20, 90B18" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4733v1</id>
    <updated>2014-11-18T04:40:20Z</updated>
    <published>2014-11-18T04:40:20Z</published>
    <title>On The Modeling of OpenFlow-based SDNs: The Single Node Case</title>
    <summary>  OpenFlow is one of the most commonly used protocols for communication between
the controller and the forwarding element in a software defined network (SDN).
A model based on M/M/1 queues is proposed in [1] to capture the communication
between the forwarding element and the controller. Albeit the model provides
useful insight, it is accurate only for the case when the probability of
expecting a new flow is small. Secondly, it is not straight forward to extend
the model in [1] to more than one forwarding element in the data plane. In this
work we propose a model which addresses both these challenges. The model is
based on Jackson assumption but with corrections tailored to the OpenFlow based
SDN network. Performance analysis using the proposed model indicates that the
model is accurate even for the case when the probability of new flow is quite
large. Further we show by a toy example that the model can be extended to more
than one node in the data plane.
</summary>
    <author>
      <name>Kashif Mahmood</name>
    </author>
    <author>
      <name>Ameen Chilwan</name>
    </author>
    <author>
      <name>Olav N. Østerbø</name>
    </author>
    <author>
      <name>Michael Jarschel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/csit.2014.41120</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/csit.2014.41120" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Proceedings of CS &amp; IT for NeCOM 2014</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Computer Science and Information Technology (CS &amp;
  IT), Vol.4, 2014, pp 207-214</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1411.4733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.4733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4759v4</id>
    <updated>2016-12-15T13:26:13Z</updated>
    <published>2014-11-18T08:20:39Z</published>
    <title>Modeling LRU caches with Shot Noise request processes</title>
    <summary>  In this paper we analyze Least Recently Used (LRU) caches operating under the
Shot Noise requests Model (SNM). The SNM was recently proposed to better
capture the main characteristics of today Video on Demand (VoD) traffic. We
investigate the validity of Che's approximation through an asymptotic analysis
of the cache eviction time. In particular, we provide a large deviation
principle, a law of large numbers and a central limit theorem for the cache
eviction time, as the cache size grows large. Finally, we derive upper and
lower bounds for the "hit" probability in tandem networks of caches under Che's
approximation.
</summary>
    <author>
      <name>Emilio Leonardi</name>
    </author>
    <author>
      <name>Giovanni Luca Torrisi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in SIAP - A preliminary, incomplete version of this work
  has appeared in Proceedings of IEEE Infocom 2015, HK</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.4759v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.4759v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.7224v1</id>
    <updated>2014-11-26T13:42:41Z</updated>
    <published>2014-11-26T13:42:41Z</published>
    <title>Efficient analysis of caching strategies under dynamic content
  popularity</title>
    <summary>  In this paper we develop a novel technique to analyze both isolated and
interconnected caches operating under different caching strategies and
realistic traffic conditions. The main strength of our approach is the ability
to consider dynamic contents which are constantly added into the system
catalogue, and whose popularity evolves over time according to desired
profiles. We do so while preserving the simplicity and computational efficiency
of models developed under stationary popularity conditions, which are needed to
analyze several caching strategies. Our main achievement is to show that the
impact of content popularity dynamics on cache performance can be effectively
captured into an analytical model based on a fixed content catalogue (i.e., a
catalogue whose size and objects' popularity do not change over time).
</summary>
    <author>
      <name>Michele Garetto</name>
    </author>
    <author>
      <name>Emilio Leonardi</name>
    </author>
    <author>
      <name>Stefano Traverso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear at Infocom 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.7224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.7224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.04552v1</id>
    <updated>2015-01-19T16:48:00Z</updated>
    <published>2015-01-19T16:48:00Z</published>
    <title>Solving the Klein-Gordon equation using Fourier spectral methods: A
  benchmark test for computer performance</title>
    <summary>  The cubic Klein-Gordon equation is a simple but non-trivial partial
differential equation whose numerical solution has the main building blocks
required for the solution of many other partial differential equations. In this
study, the library 2DECOMP&amp;FFT is used in a Fourier spectral scheme to solve
the Klein-Gordon equation and strong scaling of the code is examined on
thirteen different machines for a problem size of 512^3. The results are useful
in assessing likely performance of other parallel fast Fourier transform based
programs for solving partial differential equations. The problem is chosen to
be large enough to solve on a workstation, yet also of interest to solve
quickly on a supercomputer, in particular for parametric studies. Unlike other
high performance computing benchmarks, for this problem size, the time to
solution will not be improved by simply building a bigger supercomputer.
</summary>
    <author>
      <name>S. Aseeri</name>
    </author>
    <author>
      <name>O. Batrašev</name>
    </author>
    <author>
      <name>M. Icardi</name>
    </author>
    <author>
      <name>B. Leu</name>
    </author>
    <author>
      <name>A. Liu</name>
    </author>
    <author>
      <name>N. Li</name>
    </author>
    <author>
      <name>B. K. Muite</name>
    </author>
    <author>
      <name>E. Müller</name>
    </author>
    <author>
      <name>B. Palen</name>
    </author>
    <author>
      <name>M. Quell</name>
    </author>
    <author>
      <name>H. Servat</name>
    </author>
    <author>
      <name>P. Sheth</name>
    </author>
    <author>
      <name>R. Speck</name>
    </author>
    <author>
      <name>M. Van Moer</name>
    </author>
    <author>
      <name>J. Vienne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.04552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.04552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.06223v1</id>
    <updated>2015-01-26T00:14:20Z</updated>
    <published>2015-01-26T00:14:20Z</published>
    <title>A Roofline Visualization Framework</title>
    <summary>  The Roofline Model and its derivatives provide an intuitive representation of
the best achievable performance on a given architecture. The Roofline Toolkit
project is a collaboration among researchers at Argonne National Laboratory,
Lawrence Berkeley National Laboratory, and the University of Oregon and
consists of three main parts: hardware characterization, software
characterization, and data manipulation and visualization interface. These
components address the different aspects of performance data acquisition and
manipulation required for performance analysis, modeling and optimization of
codes on existing and emerging architectures. In this paper we introduce an
initial implementation of the third component, a system for visualizing
roofline charts and managing roofline performance analysis data. We discuss the
implementation and rationale for the integration of the roofline visualization
system into the Eclipse IDE. An overview of our continuing efforts and goals in
the development of this project is provided.
</summary>
    <author>
      <name>Wyatt Spear</name>
    </author>
    <author>
      <name>Boyana Norris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.06223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.06223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.04074v1</id>
    <updated>2015-04-16T00:43:27Z</updated>
    <published>2015-04-16T00:43:27Z</published>
    <title>Power Aware Wireless File Downloading: A Lyapunov Indexing Approach to A
  Constrained Restless Bandit Problem</title>
    <summary>  This paper treats power-aware throughput maxi-mization in a multi-user file
downloading system. Each user can receive a new file only after its previous
file is finished. The file state processes for each user act as coupled Markov
chains that form a generalized restless bandit system. First, an optimal
algorithm is derived for the case of one user. The algorithm maximizes
throughput subject to an average power constraint. Next, the one-user algorithm
is extended to a low complexity heuristic for the multi-user problem. The
heuristic uses a simple online index policy. In a special case with no
power-constraint, the multi-user heuristic is shown to be throughput optimal.
Simulations are used to demonstrate effectiveness of the heuristic in the
general case. For simple cases where the optimal solution can be computed
offline, the heuristic is shown to be near-optimal for a wide range of
parameters.
</summary>
    <author>
      <name>Xiaohan Wei</name>
    </author>
    <author>
      <name>Michael J. Neely</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version submitted to IEEE Trans. Network. arXiv admin note:
  substantial text overlap with arXiv:1401.3824</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.04074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.04074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.08035v1</id>
    <updated>2015-04-29T21:58:50Z</updated>
    <published>2015-04-29T21:58:50Z</published>
    <title>The ELAPS Framework: Experimental Linear Algebra Performance Studies</title>
    <summary>  Optimal use of computing resources requires extensive coding, tuning and
benchmarking. To boost developer productivity in these time consuming tasks, we
introduce the Experimental Linear Algebra Performance Studies framework
(ELAPS), a multi-platform open source environment for fast yet powerful
performance experimentation with dense linear algebra kernels, algorithms, and
libraries. ELAPS allows users to construct experiments to investigate how
performance and efficiency vary depending on factors such as caching,
algorithmic parameters, problem size, and parallelism. Experiments are designed
either through Python scripts or a specialized GUI, and run on the whole
spectrum of architectures, ranging from laptops to clusters, accelerators, and
supercomputers. The resulting experiment reports provide various metrics and
statistics that can be analyzed both numerically and visually. We demonstrate
the use of ELAPS in four concrete application scenarios and in as many
computing environments, illustrating its practical value in supporting critical
performance decisions.
</summary>
    <author>
      <name>Elmar Peise</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">AICES, RWTH Aachen</arxiv:affiliation>
    </author>
    <author>
      <name>Paolo Bientinesi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">AICES, RWTH Aachen</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to SC15</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.08035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.08035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.02586v1</id>
    <updated>2015-05-11T12:34:54Z</updated>
    <published>2015-05-11T12:34:54Z</published>
    <title>Performance analysis of the Kahan-enhanced scalar product on current
  multicore processors</title>
    <summary>  We investigate the performance characteristics of a numerically enhanced
scalar product (dot) kernel loop that uses the Kahan algorithm to compensate
for numerical errors, and describe efficient SIMD-vectorized implementations on
recent Intel processors. Using low-level instruction analysis and the
execution-cache-memory (ECM) performance model we pinpoint the relevant
performance bottlenecks for single-core and thread-parallel execution, and
predict performance and saturation behavior. We show that the Kahan-enhanced
scalar product comes at almost no additional cost compared to the naive
(non-Kahan) scalar product if appropriate low-level optimizations, notably SIMD
vectorization and unrolling, are applied. We also investigate the impact of
architectural changes across four generations of Intel Xeon processors.
</summary>
    <author>
      <name>Johannes Hofmann</name>
    </author>
    <author>
      <name>Dietmar Fey</name>
    </author>
    <author>
      <name>Jan Eitzinger</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-32149-3_7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-32149-3_7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.02586v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.02586v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.03374v3</id>
    <updated>2017-05-12T13:09:43Z</updated>
    <published>2015-05-13T13:21:25Z</published>
    <title>Data dependent energy modelling for worst case energy consumption
  analysis</title>
    <summary>  Safely meeting Worst Case Energy Consumption (WCEC) criteria requires
accurate energy modeling of software. We investigate the impact of instruction
operand values upon energy consumption in cacheless embedded processors.
Existing instruction-level energy models typically use measurements from random
input data, providing estimates unsuitable for safe WCEC analysis.
  We examine probabilistic energy distributions of instructions and propose a
model for composing instruction sequences using distributions, enabling WCEC
analysis on program basic blocks. The worst case is predicted with statistical
analysis. Further, we verify that the energy of embedded benchmarks can be
characterised as a distribution, and compare our proposed technique with other
methods of estimating energy consumption.
</summary>
    <author>
      <name>James Pallister</name>
    </author>
    <author>
      <name>Steve Kerrison</name>
    </author>
    <author>
      <name>Jeremy Morse</name>
    </author>
    <author>
      <name>Kerstin Eder</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3078659.3078666</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3078659.3078666" rel="related"/>
    <link href="http://arxiv.org/abs/1505.03374v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.03374v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.01494v1</id>
    <updated>2015-06-04T07:48:37Z</updated>
    <published>2015-06-04T07:48:37Z</published>
    <title>Benchmarking Big Data Systems: State-of-the-Art and Future Directions</title>
    <summary>  The great prosperity of big data systems such as Hadoop in recent years makes
the benchmarking of these systems become crucial for both research and industry
communities. The complexity, diversity, and rapid evolution of big data systems
gives rise to various new challenges about how we design generators to produce
data with the 4V properties (i.e. volume, velocity, variety and veracity), as
well as implement application-specific but still comprehensive workloads.
However, most of the existing big data benchmarks can be described as attempts
to solve specific problems in benchmarking systems. This article investigates
the state-of-the-art in benchmarking big data systems along with the future
challenges to be addressed to realize a successful and efficient benchmark.
</summary>
    <author>
      <name>Rui Han</name>
    </author>
    <author>
      <name>Zhen Jia</name>
    </author>
    <author>
      <name>Wanling Gao</name>
    </author>
    <author>
      <name>Xinhui Tian</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:1402.5194</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.01494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.01494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06011v1</id>
    <updated>2015-06-19T14:12:00Z</updated>
    <published>2015-06-19T14:12:00Z</published>
    <title>A Markovian Analysis of IEEE 802.11 Broadcast Transmission Networks with
  Buffering</title>
    <summary>  The purpose of this paper is to analyze the so-called back-off technique of
the IEEE 802.11 protocol in broadcast mode with waiting queues. In contrast to
existing models, packets arriving when a station (or node) is in back-off state
are not discarded, but are stored in a buffer of infinite capacity. As in
previous studies, the key point of our analysis hinges on the assumption that
the time on the channel is viewed as a random succession of transmission slots
(whose duration corresponds to the length of a packet) and mini-slots during
which the back-o? of the station is decremented. These events occur
independently, with given probabilities. The state of a node is represented by
a two-dimensional Markov chain in discrete-time, formed by the back-off counter
and the number of packets at the station. Two models are proposed both of which
are shown to cope reasonably well with the physical principles of the protocol.
The stabillity (ergodicity) conditions are obtained and interpreted in terms of
maximum throughput. Several approximations related to these models are also
discussed.
</summary>
    <author>
      <name>Guy Fayolle</name>
    </author>
    <author>
      <name>Paul Muhlethaler</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1017/S0269964816000036</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1017/S0269964816000036" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Prob. Eng. Inf. Sci. 30 (2016) 326-344</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.06011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 60J10, secondary 30D05, 30E99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.00095v1</id>
    <updated>2015-08-31T23:55:54Z</updated>
    <published>2015-08-31T23:55:54Z</published>
    <title>Brewing Analytics Quality for Cloud Performance</title>
    <summary>  Cloud computing has become increasingly popular. Many options of cloud
deployments are available. Testing cloud performance would enable us to choose
a cloud deployment based on the requirements. In this paper, we present an
innovative process, implemented in software, to allow us to assess the quality
of the cloud performance data. The process combines performance data from
multiple machines, spanning across user experience data, workload performance
metrics, and readily available system performance data. Furthermore, we discuss
the major challenges of bringing raw data into tidy data formats in order to
enable subsequent analysis, and describe how our process has several layers of
assessment to validate the quality of the data processing procedure. We present
a case study to demonstrate the effectiveness of our proposed process, and
conclude our paper with several future research directions worth investigating.
</summary>
    <author>
      <name>Li Chen</name>
    </author>
    <author>
      <name>Pooja Jain</name>
    </author>
    <author>
      <name>Kingsum Chow</name>
    </author>
    <author>
      <name>Emad Guirguis</name>
    </author>
    <author>
      <name>Tony Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1509.00095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.00095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03573v1</id>
    <updated>2015-09-11T16:03:55Z</updated>
    <published>2015-09-11T16:03:55Z</published>
    <title>Invited Abstract: A Simulation Package for Energy Consumption of Content
  Delivery Networks (CDNs)</title>
    <summary>  Content Delivery Networks (CDNs) are becoming an integral part of the future
generation Internet. Traditionally, these networks have been designed with the
goals of traffic offload and the improvement of users' quality of experience
(QoE), but the energy consumption is also becoming an indispensable design
factor for CDNs to be a sustainable solution. To study and improve the CDN
architectures using this new design metric, we are planning to develop a
generic and flexible simulation package in OMNet++. This package is aimed to
render a holistic view about the CDN energy consumption behaviour by
incorporating the state-of-the-art energy consumption models proposed for the
individual elements of CDNs (e.g. servers, routers, wired and wireless links,
wireless devices, etc.) and for the various Internet contents (web pages,
files, streaming video, etc.).
</summary>
    <author>
      <name>Mohammadhassan Safavi</name>
    </author>
    <author>
      <name>Saeed Bastani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in: A. F\"orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015, arXiv:1509.03284, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.03573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03778v2</id>
    <updated>2015-11-05T13:07:53Z</updated>
    <published>2015-09-12T20:47:29Z</published>
    <title>Automatic Loop Kernel Analysis and Performance Modeling With Kerncraft</title>
    <summary>  Analytic performance models are essential for understanding the performance
characteristics of loop kernels, which consume a major part of CPU cycles in
computational science. Starting from a validated performance model one can
infer the relevant hardware bottlenecks and promising optimization
opportunities. Unfortunately, analytic performance modeling is often tedious
even for experienced developers since it requires in-depth knowledge about the
hardware and how it interacts with the software. We present the "Kerncraft"
tool, which eases the construction of analytic performance models for streaming
kernels and stencil loop nests. Starting from the loop source code, the problem
size, and a description of the underlying hardware, Kerncraft can ideally
predict the single-core performance and scaling behavior of loops on multicore
processors using the Roofline or the Execution-Cache-Memory (ECM) model. We
describe the operating principles of Kerncraft with its capabilities and
limitations, and we show how it may be used to quickly gain insights by
accelerated analytic modeling.
</summary>
    <author>
      <name>Julian Hammer</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Jan Eitzinger</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2832087.2832092</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2832087.2832092" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures, 8 listings</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.03778v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03778v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06506v1</id>
    <updated>2015-09-22T08:40:00Z</updated>
    <published>2015-09-22T08:40:00Z</published>
    <title>Latency Analysis of an Aerial Video Tracking System Using Fiacre and
  Tina</title>
    <summary>  We describe our experience with modeling a video tracking system used to
detect and follow moving targets from an airplane. We provide a formal model
that takes into account the real-time properties of the system and use it to
compute the worst and best-case end to end latency. We also compute a lower
bound on the delay between the loss of two frames. Our approach is based on the
model-checking tool Tina, that provides state-space generation and
model-checking algorithms for an extension of Time Petri Nets with data and
priorities. We propose several models divided in two main categories: first
Time Petri Net models, which are used to study the behavior of the system in
the most basic way; then models based on the Fiacre specification language,
where we take benefit of richer data structures to directly model the buffering
of video information and the use of an unbounded number of frame identifiers.
</summary>
    <author>
      <name>Silvano Dal Zilio</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS-VERTICS</arxiv:affiliation>
    </author>
    <author>
      <name>Bernard Berthomieu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS-VERTICS</arxiv:affiliation>
    </author>
    <author>
      <name>Didier Le Botlan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS-VERTICS</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work was presented at the FMTV verification challenge of WATERS
  2015 (https://waters2015.inria.fr/challenge/) , the 6th International
  Workshop on Analysis Tools and Methodologies for Embedded and Real-time
  Systems. Submissions were not published in the workshop proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.06506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06889v1</id>
    <updated>2015-09-23T08:57:32Z</updated>
    <published>2015-09-23T08:57:32Z</published>
    <title>Throughput capacity of two-hop relay MANETs under finite buffers</title>
    <summary>  Since the seminal work of Grossglauser and Tse [1], the two-hop relay
algorithm and its variants have been attractive for mobile ad hoc networks
(MANETs) due to their simplicity and efficiency. However, most literature
assumed an infinite buffer size for each node, which is obviously not
applicable to a realistic MANET. In this paper, we focus on the exact
throughput capacity study of two-hop relay MANETs under the practical finite
relay buffer scenario. The arrival process and departure process of the relay
queue are fully characterized, and an ergodic Markov chain-based framework is
also provided. With this framework, we obtain the limiting distribution of the
relay queue and derive the throughput capacity under any relay buffer size.
Extensive simulation results are provided to validate our theoretical framework
and explore the relationship among the throughput capacity, the relay buffer
size and the number of nodes.
</summary>
    <author>
      <name>Jia Liu</name>
    </author>
    <author>
      <name>Min Sheng</name>
    </author>
    <author>
      <name>Yang Xu</name>
    </author>
    <author>
      <name>Hongguang Sun</name>
    </author>
    <author>
      <name>Xijun Wang</name>
    </author>
    <author>
      <name>Xiaohong Jiang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/PIMRC.2014.7136358</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/PIMRC.2014.7136358" rel="related"/>
    <link href="http://arxiv.org/abs/1509.06889v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06889v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00761v1</id>
    <updated>2015-10-03T00:18:57Z</updated>
    <published>2015-10-03T00:18:57Z</published>
    <title>On the Rate of Convergence of Mean-Field Models: Stein's Method Meets
  the Perturbation Theory</title>
    <summary>  This paper studies the rate of convergence of a family of continuous-time
Markov chains (CTMC) to a mean-field model. When the mean-field model is a
finite-dimensional dynamical system with a unique equilibrium point, an
analysis based on Stein's method and the perturbation theory shows that under
some mild conditions, the stationary distributions of CTMCs converge (in the
mean-square sense) to the equilibrium point of the mean-field model if the
mean-field model is globally asymptotically stable and locally exponentially
stable. In particular, the mean square difference between the $M$th CTMC in the
steady state and the equilibrium point of the mean-field system is $O(1/M),$
where $M$ is the size of the $M$th CTMC. This approach based on Stein's method
provides a new framework for studying the convergence of CTMCs to their
mean-field limit by mainly looking into the stability of the mean-field model,
which is a deterministic system and is often easier to analyze than the CTMCs.
More importantly, this approach quantifies the rate of convergence, which
reveals the approximation error of using mean-field models for approximating
finite-size systems.
</summary>
    <author>
      <name>Lei Ying</name>
    </author>
    <link href="http://arxiv.org/abs/1510.00761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02238v1</id>
    <updated>2015-10-08T08:48:28Z</updated>
    <published>2015-10-08T08:48:28Z</published>
    <title>On the Maximal Shortest Path in a Connected Component in V2V</title>
    <summary>  In this work, a VANET (Vehicular Ad-hoc NETwork) is considered to operate on
a simple lane, without infrastructure. The arrivals of vehicles are assumed to
be general with any traffic and speed assumptions. The vehicles communicate
through the shortest path. In this paper, we study the probability distribution
of the number of hops on the maximal shortest path in a connected component of
vehicles. The general formulation is given for any assumption of road traffic.
Then, it is applied to calculate the z-transform of this distribution for
medium and dense networks in the Poisson case. Our model is validated with the
Madrid road traces of the Universitat Polit\`ecnica de Catalunya. These results
may be useful for example when evaluating diffusion protocols through the
shortest path in a VANET, where not only the mean but also the other moments
are needed to derive accurate results.
</summary>
    <author>
      <name>Michel Marot</name>
    </author>
    <author>
      <name>Adel Mounir Saïd</name>
    </author>
    <author>
      <name>Hossam Afifi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.peva.2015.09.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.peva.2015.09.003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Performance Evaluation</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.02238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.02494v3</id>
    <updated>2016-01-11T02:20:27Z</updated>
    <published>2015-11-08T15:06:10Z</published>
    <title>A lightweight optimization selection method for Sparse Matrix-Vector
  Multiplication</title>
    <summary>  In this paper, we propose an optimization selection methodology for the
ubiquitous sparse matrix-vector multiplication (SpMV) kernel. We propose two
models that attempt to identify the major performance bottleneck of the kernel
for every instance of the problem and then select an appropriate optimization
to tackle it. Our first model requires online profiling of the input matrix in
order to detect its most prevailing performance issue, while our second model
only uses comprehensive structural features of the sparse matrix. Our method
delivers high performance stability for SpMV across different platforms and
sparse matrices, due to its application and architecture awareness. Our
experimental results demonstrate that a) our approach is able to distinguish
and appropriately optimize special matrices in multicore platforms that fall
out of the standard class of memory bandwidth bound matrices, and b) lead to a
significant performance gain of 29% in a manycore platform compared to an
architecture-centric optimization, as a result of the successful selection of
the appropriate optimization for the great majority of the matrices. With a
runtime overhead equivalent to a couple dozen SpMV operations, our approach is
practical for use in iterative numerical solvers of real-life applications.
</summary>
    <author>
      <name>Athena Elafrou</name>
    </author>
    <author>
      <name>Georgios Goumas</name>
    </author>
    <author>
      <name>Nectarios Koziris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.02494v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.02494v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05771v2</id>
    <updated>2015-11-20T14:15:41Z</updated>
    <published>2015-11-18T13:37:18Z</published>
    <title>Toward Transparent Heterogeneous Systems</title>
    <summary>  Heterogeneous parallel systems are widely spread nowadays. Despite their
availability, their usage and adoption are still limited, and even more rarely
they are used to full power. Indeed, compelling new technologies are constantly
developed and keep changing the technological landscape, but each of them
targets a limited sub-set of supported devices, and nearly all of them require
new programming paradigms and specific toolsets. Software, however, can hardly
keep the pace with the growing number of computational capabilities, and
developers are less and less motivated in learning skills that could quickly
become obsolete. In this paper we present our effort in the direction of a
transparent system optimization based on automatic code profiling and
Just-In-Time compilation, that resulted in a fully-working embedded prototype
capable of dynamically detect computing-intensive code blocks and automatically
dispatch them to different computation units. Experimental results show that
our system allows gains up to 32x in performance --- after an initial warm-up
phase --- without requiring any human intervention.
</summary>
    <author>
      <name>Baptiste Delporte</name>
    </author>
    <author>
      <name>Roberto Rigamonti</name>
    </author>
    <author>
      <name>Alberto Dassatti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.05771v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05771v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.08635v1</id>
    <updated>2015-11-27T11:59:41Z</updated>
    <published>2015-11-27T11:59:41Z</published>
    <title>HPA: An Opportunistic Approach to Embedded Energy Efficiency</title>
    <summary>  Reducing energy consumption is a challenge that is faced on a daily basis by
teams from the High-Performance Computing as well as the Embedded domain. This
issue is mostly attacked from an hardware perspective, by devising
architectures that put energy efficiency as a primary target, often at the cost
of processing power. Lately, computing platforms have become more and more
heterogeneous, but the exploitation of these additional capabilities is so
complex from the application developer's perspective that they are left unused
most of the time, resulting therefore in a supplemental waste of energy rather
than in faster processing times.
  In this paper we present a transparent, on-the-fly optimization scheme that
allows a generic application to automatically exploit the available computing
units to partition its computational load. We have called our approach
Heterogeneous Platform Accelerator (HPA). The idea is to use profiling to
automatically select a computing-intensive candidate for acceleration, and then
distribute the computations to the different units by off-loading blocks of
code to them.
  Using an NVIDIA Jetson TK1 board, we demonstrate that not only HPA results in
faster processing speed, but also in a considerable reduction in the total
energy absorbed.
</summary>
    <author>
      <name>Baptiste Delporte</name>
    </author>
    <author>
      <name>Roberto Rigamonti</name>
    </author>
    <author>
      <name>Alberto Dassatti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.08635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.08635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.08354v1</id>
    <updated>2015-12-28T09:27:32Z</updated>
    <published>2015-12-28T09:27:32Z</published>
    <title>Non-Asymptotic Delay Bounds for (k,l) Fork-Join Systems and Multi-Stage
  Fork-Join Networks</title>
    <summary>  Parallel systems have received increasing attention with numerous recent
applications such as fork-join systems, load-balancing, and l-out-of-k
redundancy. Common to these systems is a join or resequencing stage, where
tasks that have finished service may have to wait for the completion of other
tasks so that they leave the system in a predefined order. These
synchronization constraints make the analysis of parallel systems challenging
and few explicit results are known. In this work, we model parallel systems
using a max-plus approach that enables us to derive statistical bounds of
waiting and sojourn times. Taking advantage of max-plus system theory, we also
show end-to-end delay bounds for multi-stage fork-join networks. We contribute
solutions for basic G|G|1 fork-join systems, parallel systems with
load-balancing, as well as general (k,l) fork-join systems with redundancy. Our
results provide insights into the respective advantages of l-out-of-k
redundancy vs. load-balancing.
</summary>
    <author>
      <name>Markus Fidler</name>
    </author>
    <author>
      <name>Yuming Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/1512.08354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.08354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07623v1</id>
    <updated>2016-02-24T18:08:43Z</updated>
    <published>2016-02-24T18:08:43Z</published>
    <title>Spatial multi-LRU Caching for Wireless Networks with Coverage Overlaps</title>
    <summary>  This article introduces a novel family of decentralised caching policies,
applicable to wireless networks with finite storage at the edge-nodes
(stations). These policies are based on the Least-Recently-Used replacement
principle, and are, here, referred to as spatial multi-LRU. Based on these,
cache inventories are updated in a way that provides content diversity to users
who are covered by, and thus have access to, more than one station. Two
variations are proposed, namely the multi-LRU-One and -All, which differ in the
number of replicas inserted in the involved caches. By introducing spatial
approximations, we propose a Che-like method to predict the hit probability,
which gives very accurate results under the Independent Reference Model (IRM).
It is shown that the performance of multi-LRU increases the more the
multi-coverage areas increase, and it approaches the performance of other
proposed centralised policies, when multi-coverage is sufficient. For IRM
traffic multi-LRU-One outperforms multi-LRU-All, whereas when the traffic
exhibits temporal locality the -All variation can perform better.
</summary>
    <author>
      <name>Anastasios Giovanidis</name>
    </author>
    <author>
      <name>Apostolos Avranas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2896377.2901483</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2896377.2901483" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2-column, 10 Figures (13 sub-figures in all), part of the
  results in ACM SIGMETRICS/IFIP Performance 2016, Antibes, France</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07978v1</id>
    <updated>2016-02-25T16:13:23Z</updated>
    <published>2016-02-25T16:13:23Z</published>
    <title>Contrasting Effects of Replication in Parallel Systems: From Overload to
  Underload and Back</title>
    <summary>  Task replication has recently been advocated as a practical solution to
reduce latencies in parallel systems. In addition to several convincing
empirical studies, some others provide analytical results, yet under some
strong assumptions such as Poisson arrivals, exponential service times, or
independent service times of the replicas themselves, which may lend themselves
to some contrasting and perhaps contriving behavior. For instance, under the
second assumption, an overloaded system can be stabilized by a replication
factor, but can be sent back in overload through further replication. In turn,
under the third assumption, strictly larger stability regions of replication
systems do not necessarily imply smaller delays.
  Motivated by the need to dispense with such common and restricting
assumptions, which may additionally cause unexpected behavior, we develop a
unified and general theoretical framework to compute tight bounds on the
distribution of response times in general replication systems. These results
immediately lend themselves to the optimal number of replicas minimizing
response time quantiles, depending on the parameters of the system (e.g., the
degree of correlation amongst replicas). As a concrete application of our
framework, we design a novel replication policy which can improve the stability
region of classical fork-join queueing systems by $\mathcal{O}(\ln K)$, in the
number of servers $K$.
</summary>
    <author>
      <name>Felix Poloczek</name>
    </author>
    <author>
      <name>Florin Ciucu</name>
    </author>
    <link href="http://arxiv.org/abs/1602.07978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01316v2</id>
    <updated>2016-05-14T11:37:22Z</updated>
    <published>2016-03-04T00:05:18Z</published>
    <title>Performance Assessment of WhatsApp and IMO on Android Operating System
  (Lollipop and KitKat) during VoIP calls using 3G or WiFi</title>
    <summary>  This paper assesses the performance of mobile messaging and VoIP connections.
We investigate the CPU usage of WhatsApp and IMO under different scenarios.
This analysis also enabled a comparison of the performance of these
applications on two Android operating system (OS) versions: KitKat or Lollipop.
Two models of smartphones were considered, viz. Galaxy Note 4 and Galaxy S4.
The applications behavior was statistically investigated for both sending and
receiving VoIP calls. Connections have been examined over 3G and WiFi. The
handset model plays a decisive role in CPU usage of the application. t-tests
showed that IMO has a better performance that WhatsApp whatever be the Android
at a significance level 1%, on Galaxy Note 4. In contrast, WhatsApp requires
less CPU than IMO on Galaxy S4 whatever be the OS and access (3G/WiFi). Galaxy
Note 4 using WiFi always outperformed S4 in terms of processing efficiency.
</summary>
    <author>
      <name>R. C. de Oliveira</name>
    </author>
    <author>
      <name>H. M. de Oliveira</name>
    </author>
    <author>
      <name>R. A. Ramalho</name>
    </author>
    <author>
      <name>L. P. S. Viana</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, Number of floats/tables/figures: 5</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Global Journal of Computer Science and Technology, Vol 16, No 1-A
  (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.01316v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01316v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01890v1</id>
    <updated>2016-04-07T07:04:19Z</updated>
    <published>2016-04-07T07:04:19Z</published>
    <title>Performance analysis of the Kahan-enhanced scalar product on current
  multi- and manycore processors</title>
    <summary>  We investigate the performance characteristics of a numerically enhanced
scalar product (dot) kernel loop that uses the Kahan algorithm to compensate
for numerical errors, and describe efficient SIMD-vectorized implementations on
recent multi- and manycore processors. Using low-level instruction analysis and
the execution-cache-memory (ECM) performance model we pinpoint the relevant
performance bottlenecks for single-core and thread-parallel execution, and
predict performance and saturation behavior. We show that the Kahan-enhanced
scalar product comes at almost no additional cost compared to the naive
(non-Kahan) scalar product if appropriate low-level optimizations, notably SIMD
vectorization and unrolling, are applied. The ECM model is extended
appropriately to accommodate not only modern Intel multicore chips but also the
Intel Xeon Phi "Knights Corner" coprocessor and an IBM POWER8 CPU. This allows
us to discuss the impact of processor features on the performance across four
modern architectures that are relevant for high performance computing.
</summary>
    <author>
      <name>Johannes Hofmann</name>
    </author>
    <author>
      <name>Dietmar Fey</name>
    </author>
    <author>
      <name>Michael Riedmann</name>
    </author>
    <author>
      <name>Jan Eitzinger</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/cpe.3921</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/cpe.3921" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Concurrency Computat.: Pract. Exper., 29: e3921 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.01890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02097v3</id>
    <updated>2017-04-07T19:49:58Z</updated>
    <published>2016-04-06T00:35:18Z</published>
    <title>On the Duration and Intensity of Competitions in Nonlinear Pólya Urn
  Processes with Fitness</title>
    <summary>  Cumulative advantage (CA) refers to the notion that accumulated resources
foster the accumulation of further resources in competitions, a phenomenon that
has been empirically observed in various contexts. The oldest and arguably
simplest mathematical model that embodies this general principle is the P\'olya
urn process, which finds applications in a myriad of problems. The original
model captures the dynamics of competitions between two equally fit agents
under linear CA effects, which can be readily generalized to incorporate
different fitnesses and nonlinear CA effects. We study two statistics of
competitions under the generalized model, namely duration (i.e., time of the
last tie) and intensity (i.e., number of ties). We give rigorous mathematical
characterizations of the tail distributions of both duration and intensity
under the various regimes for fitness and nonlinearity, which reveal very
interesting behaviors. For example, fitness superiority induces much shorter
competitions in the sublinear regime while much longer competitions in the
superlinear regime. Our findings can shed light on the application of P\'olya
urn processes in more general contexts where fitness and nonlinearity may be
present.
</summary>
    <author>
      <name>Bo Jiang</name>
    </author>
    <author>
      <name>Daniel R. Figueiredo</name>
    </author>
    <author>
      <name>Bruno Ribeiro</name>
    </author>
    <author>
      <name>Don Towsley</name>
    </author>
    <link href="http://arxiv.org/abs/1604.02097v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02097v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08004v1</id>
    <updated>2016-04-27T10:08:53Z</updated>
    <published>2016-04-27T10:08:53Z</published>
    <title>An Analytical Solution for Probabilistic Guarantees of Reservation Based
  Soft Real-Time Systems</title>
    <summary>  We show a methodology for the computation of the probability of deadline miss
for a periodic real-time task scheduled by a resource reservation algorithm. We
propose a modelling technique for the system that reduces the computation of
such a probability to that of the steady state probability of an infinite state
Discrete Time Markov Chain with a periodic structure. This structure is
exploited to develop an efficient numeric solution where different
accuracy/computation time trade-offs can be obtained by operating on the
granularity of the model. More importantly we offer a closed form conservative
bound for the probability of a deadline miss. Our experiments reveal that the
bound remains reasonably close to the experimental probability in one real-time
application of practical interest. When this bound is used for the optimisation
of the overall Quality of Service for a set of tasks sharing the CPU, it
produces a good sub-optimal solution in a small amount of time.
</summary>
    <author>
      <name>Luigi Palopoli</name>
    </author>
    <author>
      <name>Daniele Fontanelli</name>
    </author>
    <author>
      <name>Luca Abeni</name>
    </author>
    <author>
      <name>Bernardo Villalba Frías</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPDS.2015.2416732</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPDS.2015.2416732" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Parallel and Distributed Systems, Volume:27,
  Issue: 3, March 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00559v1</id>
    <updated>2016-05-02T16:45:39Z</updated>
    <published>2016-05-02T16:45:39Z</published>
    <title>Age-of-Information in the Presence of Error</title>
    <summary>  We consider the peak age-of-information (PAoI) in an M/M/1 queueing system
with packet delivery error, i.e., update packets can get lost during
transmissions to their destination. We focus on two types of policies, one is
to adopt Last-Come-First-Served (LCFS) scheduling, and the other is to utilize
retransmissions, i.e., keep transmitting the most recent packet. Both policies
can effectively avoid the queueing delay of a busy channel and ensure a small
PAoI. Exact PAoI expressions under both policies with different error
probabilities are derived, including First-Come-First-Served (FCFS), LCFS with
preemptive priority, LCFS with non-preemptive priority, Retransmission with
preemptive priority, and Retransmission with non-preemptive priority. Numerical
results obtained from analysis and simulation are presented to validate our
results.
</summary>
    <author>
      <name>Kun Chen</name>
    </author>
    <author>
      <name>Longbo Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.00559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08146v1</id>
    <updated>2016-05-26T05:19:26Z</updated>
    <published>2016-05-26T05:19:26Z</published>
    <title>On Pollaczek-Khinchine Formula for Peer-to-Peer Networks</title>
    <summary>  The performance analysis of peer-to-peer (P2P) networks calls for a new kind
of queueing model, in which jobs and service stations arrive randomly. Except
in some simple special cases, in general, the queueing model with varying
service rate is mathematically intractable. Motivated by the P-K formula for
M/G/1 queue, we developed a limiting analysis approach based on the connection
between the fluctuation of service rate and the mean queue length. Considering
the two extreme service rates, we proved the conjecture on the lower bound and
upper bound of mean queue length previously postulated. Furthermore, an
approximate P-K formula to estimate the mean queue length is derived from the
convex combination of these two bounds and the conditional mean queue length
under the overload condition. We confirmed the accuracy of our approximation by
extensive simulation studies with different system parameters. We also verified
that all limiting cases of the system behavior are consistent with the
predictions of our formula.
</summary>
    <author>
      <name>Jian Zhang</name>
    </author>
    <author>
      <name>Tony T. Lee</name>
    </author>
    <author>
      <name>Tong Ye</name>
    </author>
    <author>
      <name>Weisheng Hu</name>
    </author>
    <link href="http://arxiv.org/abs/1605.08146v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08146v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00396v1</id>
    <updated>2016-06-01T18:52:53Z</updated>
    <published>2016-06-01T18:52:53Z</published>
    <title>DINAMITE: A modern approach to memory performance profiling</title>
    <summary>  Diagnosing and fixing performance problems on multicore machines with deep
memory hierarchies is extremely challenging. Certain problems are best
addressed when we can analyze the entire trace of program execution, e.g.,
every memory access. Unfortunately such detailed execution logs are very large
and cannot be analyzed by direct inspection. We present DINAMITE: a toolkit for
Dynamic INstrumentation and Analysis for MassIve Trace Exploration. DINAMITE is
a collection of tools for end-to-end performance analysis: from the LLVM
compiler pass that instruments the program to plug-and-play tools that use a
modern data analytics engine Spark Streaming for trace introspection. Using
DINAMITE we found opportunities to improve data layout in several applications
that resulted in 15-20% performance improvements and found a shared-variable
bottleneck in a popular key-value store, whose elimination improved performance
by 20x.
</summary>
    <author>
      <name>Svetozar Miucin</name>
    </author>
    <author>
      <name>Conor Brady</name>
    </author>
    <author>
      <name>Alexandra Fedorova</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02900v3</id>
    <updated>2018-02-12T06:55:22Z</updated>
    <published>2016-06-09T10:45:45Z</published>
    <title>On Continuous-space Embedding of Discrete-parameter Queueing Systems</title>
    <summary>  Motivated by the problem of discrete-parameter simulation optimization (DPSO)
of queueing systems, we consider the problem of embedding the discrete
parameter space into a continuous one so that descent-based continuous-space
methods could be directly applied for efficient optimization. We show that a
randomization of the simulation model itself can be used to achieve such an
embedding when the objective function is a long-run average measure. Unlike
spatial interpolation, the computational cost of this embedding is independent
of the number of parameters in the system, making the approach ideally suited
to high-dimensional problems. We describe in detail the application of this
technique to discrete-time queues for embedding queue capacities, number of
servers and server-delay parameters into continuous space and empirically show
that the technique can produce smooth interpolations of the objective function.
Through an optimization case-study of a queueing network with $10^7$ design
points, we demonstrate that existing continuous optimizers can be effectively
applied over such an embedding to find good solutions.
</summary>
    <author>
      <name>Neha Karanjkar</name>
    </author>
    <author>
      <name>Madhav P. Desai</name>
    </author>
    <author>
      <name>Shalabh Bhatnagar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to a journal and is under review</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02900v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02900v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09530v1</id>
    <updated>2016-06-30T15:07:31Z</updated>
    <published>2016-06-30T15:07:31Z</published>
    <title>Modeling and Predicting DNS Server Load</title>
    <summary>  The DNS relies on caching to ensure high scalability and good performance. In
optimizing caching, TTL adjustment provides a means of balancing between query
load and TTL-dependent performances such as data consistency, load balancing,
migration time, etc. To gain the desired balance, TTL adjustment depends on
predictions of query loads under alternative TTLs. This paper proposes a model
of DNS server load, which employs the uniform aggregate caching model to
simplify the complexity of modeling clients' requests and their caching. A
method of predicting DNS server load is developed using that model. The
prediction method is solely based on the unilateral measurements or
observations at authoritative servers. Without reliance on lots of multi-point
measurements nor distributed measuring facilities, the method is best suited
for DNS authoritative operators. The proposed model and prediction method are
validated through extensive simulations. Finally, global sensibility analysis
is conducted to evaluate the impacts of measurement uncertainties or errors on
the predictions.
</summary>
    <author>
      <name>Zheng Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1606.09530v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09530v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05356v2</id>
    <updated>2016-09-12T03:06:52Z</updated>
    <published>2016-07-18T23:57:15Z</published>
    <title>How to Emulate Web Traffic Using Standard Load Testing Tools</title>
    <summary>  Conventional load-testing tools are based on a fifty-year old time-share
computer paradigm where a finite number of users submit requests and respond in
a synchronized fashion. Conversely, modern web traffic is essentially
asynchronous and driven by an unknown number of users. This difference presents
a conundrum for testing the performance of modern web applications. Even when
the difference is recognized, performance engineers often introduce
modifications to their test scripts based on folklore or hearsay published in
various Internet fora, much of which can lead to wrong results. We present a
coherent methodology, based on two fundamental principles, for emulating web
traffic using a standard load-test environment.
</summary>
    <author>
      <name>James F. Brady</name>
    </author>
    <author>
      <name>Neil J. Gunther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 12 figures. To appear in the proceedings of CMG imPACt, La
  Jolla, CA, Nov. 7-10, 2016. v2 has new Figs. 1 and 5, as well as major text
  reformatting</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05356v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05356v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; D.2; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08102v1</id>
    <updated>2016-07-27T14:15:26Z</updated>
    <published>2016-07-27T14:15:26Z</published>
    <title>Statistical Delay Bound for WirelessHART Networks</title>
    <summary>  In this paper we provide a performance analysis framework for wireless
industrial networks by deriving a service curve and a bound on the delay
violation probability. For this purpose we use the (min,x) stochastic network
calculus as well as a recently presented recursive formula for an end-to-end
delay bound of wireless heterogeneous networks. The derived results are mapped
to WirelessHART networks used in process automation and were validated via
simulations. In addition to WirelessHART, our results can be applied to any
wireless network whose physical layer conforms the IEEE 802.15.4 standard,
while its MAC protocol incorporates TDMA and channel hopping, like e.g.
ISA100.11a or TSCH-based networks. The provided delay analysis is especially
useful during the network design phase, offering further research potential
towards optimal routing and power management in QoS-constrained wireless
industrial networks.
</summary>
    <author>
      <name>Neda Petreska</name>
    </author>
    <author>
      <name>Hussein Al-Zubaidy</name>
    </author>
    <author>
      <name>Barbara Staehle</name>
    </author>
    <author>
      <name>Rudi Knorr</name>
    </author>
    <author>
      <name>James Gross</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at PE-WASUN 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08450v1</id>
    <updated>2016-07-28T13:34:40Z</updated>
    <published>2016-07-28T13:34:40Z</published>
    <title>Overlay Secondary Spectrum Sharing with Independent Re-attempts in
  Cognitive Radios</title>
    <summary>  Opportunistic spectrum access (OSA) is a promising reform paradigm envisioned
to address the issue of spectrum scarcity in cognitive radio networks (CRNs).
While current models consider various aspects of the OSA scheme, the impact of
retrial phenomenon in multi-channel CRNs has not yet been analyzed. In this
work, we present a continuous-time Markov chain (CTMC) model in which the
blocked/preempted secondary users (SUs) enter a finite retrial group (or orbit)
and re-attempt independently for service in an exponentially distributed random
manner. Taking into account the inherent retrial tendency of SUs, we
numerically assess the performance of the proposed scheme in terms of dropping
probability and throughput of SUs.
</summary>
    <author>
      <name>Muthukrishnan Senthil Kumar</name>
    </author>
    <author>
      <name>Aresh Dadlani</name>
    </author>
    <author>
      <name>Kiseon Kim</name>
    </author>
    <author>
      <name>Richard O. Afolabi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SARNOF.2016.7846750</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SARNOF.2016.7846750" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Sarnoff Symposium 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.1; C.4; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02800v1</id>
    <updated>2016-08-09T13:40:59Z</updated>
    <published>2016-08-09T13:40:59Z</published>
    <title>LITMUS: An Open Extensible Framework for Benchmarking RDF Data
  Management Solutions</title>
    <summary>  Developments in the context of Open, Big, and Linked Data have led to an
enormous growth of structured data on the Web. To keep up with the pace of
efficient consumption and management of the data at this rate, many data
Management solutions have been developed for specific tasks and applications.
We present LITMUS, a framework for benchmarking data management solutions.
LITMUS goes beyond classical storage benchmarking frameworks by allowing for
analysing the performance of frameworks across query languages. In this
position paper we present the conceptual architecture of LITMUS as well as the
considerations that led to this architecture.
</summary>
    <author>
      <name>Harsh Thakkar</name>
    </author>
    <author>
      <name>Mohnish Dubey</name>
    </author>
    <author>
      <name>Gezim Sejdiu</name>
    </author>
    <author>
      <name>Axel-Cyrille Ngonga Ngomo</name>
    </author>
    <author>
      <name>Jeremy Debattista</name>
    </author>
    <author>
      <name>Christoph Lange</name>
    </author>
    <author>
      <name>Jens Lehmann</name>
    </author>
    <author>
      <name>Sören Auer</name>
    </author>
    <author>
      <name>Maria-Esther Vidal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure, position paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03347v1</id>
    <updated>2016-09-12T11:18:58Z</updated>
    <published>2016-09-12T11:18:58Z</published>
    <title>An ECM-based energy-efficiency optimization approach for
  bandwidth-limited streaming kernels on recent Intel Xeon processors</title>
    <summary>  We investigate an approach that uses low-level analysis and the
execution-cache-memory (ECM) performance model in combination with tuning of
hardware parameters to lower energy requirements of memory-bound applications.
The ECM model is extended appropriately to deal with software optimizations
such as non-temporal stores. Using incremental steps and the ECM model, we
analytically quantify the impact of various single-core optimizations and
pinpoint microarchitectural improvements that are relevant to energy
consumption. Using a 2D Jacobi solver as example that can serve as a blueprint
for other memory-bound applications, we evaluate our approach on the four most
recent Intel Xeon E5 processors (Sandy Bridge-EP, Ivy Bridge-EP, Haswell-EP,
and Broadwell-EP). We find that chip energy consumption can be reduced in the
range of 2.0-2.4$\times$ on the examined processors.
</summary>
    <author>
      <name>Johannes Hofmann</name>
    </author>
    <author>
      <name>Dietmar Fey</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04603v1</id>
    <updated>2016-09-15T12:41:41Z</updated>
    <published>2016-09-15T12:41:41Z</published>
    <title>Automating Large-Scale Simulation and Data Analysis with OMNeT++:
  Lession Learned and Future Perspectives</title>
    <summary>  Simulation is widely adopted in the study of modern computer networks. In
this context, OMNeT++ provides a set of very effective tools that span from the
definition of the network, to the automation of simulation execution and quick
result representation. However, as network models become more and more complex
to cope with the evolution of network systems, the amount of simulation
factors, the number of simulated nodes and the size of results grow
consequently, leading to simulations with larger scale. In this work, we
perform a critical analysis of the tools provided by OMNeT++ in case of such
large-scale simulations. We then propose a unified and flexible software
architecture to support simulation automation.
</summary>
    <author>
      <name>Antonio Virdis</name>
    </author>
    <author>
      <name>Carlo Vallati</name>
    </author>
    <author>
      <name>Giovanni Nardini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in: A. Foerster, V. Vesely, A. Virdis, M. Kirsche (Eds.),
  Proc. of the 3rd OMNeT++ Community Summit, Brno University of Technology -
  Czech Republic - September 15-16, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.05750v1</id>
    <updated>2016-09-16T06:09:55Z</updated>
    <published>2016-09-16T06:09:55Z</published>
    <title>Infinite Server Queueing Networks with Deadline Based Routing</title>
    <summary>  Motivated by timeouts in Internet services, we consider networks of infinite
server queues in which routing decisions are based on deadlines. Specifically,
at each node in the network, the total service time equals the minimum of
several independent service times (e.g. the minimum of the amount of time
required to complete a transaction and a deadline). Furthermore, routing
decisions depend on which of the independent service times achieves the minimum
(e.g. exceeding a deadline will require the customer to be routed so they can
re-attempt the transaction). Because current routing decisions are dependent on
past service times, much of the existing theory on product-form queueing
networks does not apply. In spite of this, we are able to show that such
networks have product-form equilibrium distributions. We verify our analytic
characterization with a simulation of a simple network. We also discuss
extensions of this work to more general settings.
</summary>
    <author>
      <name>Neal Master</name>
    </author>
    <author>
      <name>Nicholas Bambos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2016 IEEE Conference on Decision and Control (Accepted)</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.05750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.05750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09294v1</id>
    <updated>2016-09-29T10:41:26Z</updated>
    <published>2016-09-29T10:41:26Z</published>
    <title>DynIMS: A Dynamic Memory Controller for In-memory Storage on HPC Systems</title>
    <summary>  In order to boost the performance of data-intensive computing on HPC systems,
in-memory computing frameworks, such as Apache Spark and Flink, use local DRAM
for data storage. Optimizing the memory allocation to data storage is critical
to delivering performance to traditional HPC compute jobs and throughput to
data-intensive applications sharing the HPC resources. Current practices that
statically configure in-memory storage may leave inadequate space for compute
jobs or lose the opportunity to utilize more available space for data-intensive
applications. In this paper, we explore techniques to dynamically adjust
in-memory storage and make the right amount of space for compute jobs. We have
developed a dynamic memory controller, DynIMS, which infers memory demands of
compute tasks online and employs a feedback-based control model to adapt the
capacity of in-memory storage. We test DynIMS using mixed HPCC and Spark
workloads on a HPC cluster. Experimental results show that DynIMS can achieve
up to 5X performance improvement compared to systems with static memory
allocations.
</summary>
    <author>
      <name>Pengfei Xuan</name>
    </author>
    <author>
      <name>Feng Luo</name>
    </author>
    <author>
      <name>Rong Ge</name>
    </author>
    <author>
      <name>Pradip K Srimani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 8 figures, short paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.2; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.00149v1</id>
    <updated>2016-10-01T15:29:52Z</updated>
    <published>2016-10-01T15:29:52Z</published>
    <title>Data-Unit-Size Distribution Model with Retransmitted Packet Size
  Preservation Property and Its Application to Goodput Analysis for
  Stop-and-Wait Protocol: Case of Independent Packet Losses</title>
    <summary>  This paper proposes a data-unit-size distribution model to represent the
retransmitted packet size preservation (RPSP) property in a scenario where
independently lost packets are retransmitted by a stop-and-wait protocol. RPSP
means that retransmitted packets with the same sequence number are equal in
size to the packet of the original transmission, which is identical to the
packet generated from a message through the segmentation function, namely,
generated packet. Furthermore, we derive goodput formula using an approach to
derive the data-unit-size distribution. We investigate the effect of RPSP on
frame size distributions and goodput in a simple case when no collision happens
over the bit-error prone wireless network equipped with IEEE 802.11 Distributed
Coordination Function, which is a typical example of the stop-and-wait
protocol. Numerical results show that the effect gets stronger as bit error
rate increases and the maximum size of the generated packets is larger than the
mean size for large enough packet retry limits because longer packets will be
repeatedly corrupted and retransmitted more times as a result of RPSP.
</summary>
    <author>
      <name>Takashi Ikegawa</name>
    </author>
    <link href="http://arxiv.org/abs/1610.00149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.00149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.00560v3</id>
    <updated>2019-10-24T10:06:44Z</updated>
    <published>2016-10-03T14:05:04Z</published>
    <title>Poly-Symmetry in Processor-Sharing Systems</title>
    <summary>  We consider a system of processor-sharing queues with state-dependent service
rates. These are allocated according to balanced fairness within a polymatroid
capacity set. Balanced fairness is known to be both insensitive and
Pareto-efficient in such systems, which ensures that the performance metrics,
when computable, will provide robust insights into the real performance of the
system considered. We first show that these performance metrics can be
evaluated with a complexity that is polynomial in the system size if the system
is partitioned into a finite number of parts, so that queues are exchangeable
within each part and asymmetric across different parts. This in turn allows us
to derive stochastic bounds for a larger class of systems which satisfy less
restrictive symmetry assumptions. These results are applied to practical
examples of tree data networks, such as backhaul networks of Internet service
providers, and computer clusters.
</summary>
    <author>
      <name>Thomas Bonald</name>
    </author>
    <author>
      <name>Céline Comte</name>
    </author>
    <author>
      <name>Virag Shah</name>
    </author>
    <author>
      <name>Gustavo de Veciana</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11134-017-9525-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11134-017-9525-2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Queueing Systems 86.3-4 (2017) 327-359</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.00560v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.00560v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60K25 Queueing theory, 68M20 Performance evaluation, queueing,&#10;  scheduling, 90B15 Network models, stochastic" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.06309v1</id>
    <updated>2016-10-20T07:39:23Z</updated>
    <published>2016-10-20T07:39:23Z</published>
    <title>Non-Asymptotic Delay Bounds for Multi-Server Systems with
  Synchronization Constraints</title>
    <summary>  Multi-server systems have received increasing attention with important
implementations such as Google MapReduce, Hadoop, and Spark. Common to these
systems are a fork operation, where jobs are first divided into tasks that are
processed in parallel, and a later join operation, where completed tasks wait
until the results of all tasks of a job can be combined and the job leaves the
system. The synchronization constraint of the join operation makes the analysis
of fork-join systems challenging and few explicit results are known. In this
work, we model fork-join systems using a max-plus server model that enables us
to derive statistical bounds on waiting and sojourn times for general arrival
and service time processes. We contribute end-to-end delay bounds for
multi-stage fork-join networks that grow in $\mathcal{O}(h \ln k)$ for $h$
fork-join stages, each with $k$ parallel servers. We perform a detailed
comparison of different multi-server configurations and highlight their pros
and cons. We also include an analysis of single-queue fork-join systems that
are non-idling and achieve a fundamental performance gain, and compare these
results to both simulation and a live Spark system.
</summary>
    <author>
      <name>Markus Fidler</name>
    </author>
    <author>
      <name>Brenton Walker</name>
    </author>
    <author>
      <name>Yuming Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1512.08354</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.06309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.06309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.08172v1</id>
    <updated>2016-10-26T05:00:54Z</updated>
    <published>2016-10-26T05:00:54Z</published>
    <title>Evaluating load balancing policies for performance and energy-efficiency</title>
    <summary>  Nowadays, more and more increasingly hard computations are performed in
challenging fields like weather forecasting, oil and gas exploration, and
cryptanalysis. Many of such computations can be implemented using a computer
cluster with a large number of servers. Incoming computation requests are then,
via a so-called load balancing policy, distributed over the servers to ensure
optimal performance. Additionally, being able to switch-off some servers during
low period of workload, gives potential to reduced energy consumption.
Therefore, load balancing forms, albeit indirectly, a trade-off between
performance and energy consumption. In this paper, we introduce a syntax for
load-balancing policies to dynamically select a server for each request based
on relevant criteria, including the number of jobs queued in servers, power
states of servers, and transition delays between power states of servers. To
evaluate many policies, we implement two load balancers in: (i) iDSL, a
language and tool-chain for evaluating service-oriented systems, and (ii) a
simulation framework in AnyLogic. Both implementations are successfully
validated by comparison of the results.
</summary>
    <author>
      <name>Freek van den Berg</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Twente</arxiv:affiliation>
    </author>
    <author>
      <name>Björn F. Postema</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Twente</arxiv:affiliation>
    </author>
    <author>
      <name>Boudewijn R. Haverkort</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Twente</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.227.7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.227.7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings QAPL'16, arXiv:1610.07696</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 227, 2016, pp. 98-117</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.08172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.08172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07409v1</id>
    <updated>2016-11-22T16:50:42Z</updated>
    <published>2016-11-22T16:50:42Z</published>
    <title>A Metric for Performance Portability</title>
    <summary>  The term "performance portability" has been informally used in computing to
refer to a variety of notions which generally include: 1) the ability to run
one application across multiple hardware platforms; and 2) achieving some
notional level of performance on these platforms. However, there has been a
noticeable lack of consensus on the precise meaning of the term, and authors'
conclusions regarding their success (or failure) to achieve performance
portability have thus been subjective. Comparing one approach to performance
portability with another has generally been marked with vague claims and
verbose, qualitative explanation of the comparison. This paper presents a
concise definition for performance portability, along with a simple metric that
accurately captures the performance and portability of an application across
different platforms. The utility of this metric is then demonstrated with a
retroactive application to previous work.
</summary>
    <author>
      <name>S. J. Pennycook</name>
    </author>
    <author>
      <name>J. D. Sewall</name>
    </author>
    <author>
      <name>V. W. Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, in Proceedings of the 7th International Workshop in
  Performance Modeling, Benchmarking and Simulation of High Performance
  Computer Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.07409v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07409v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07556v1</id>
    <updated>2016-11-22T22:19:54Z</updated>
    <published>2016-11-22T22:19:54Z</published>
    <title>Cultivating Software Performance in Cloud Computing</title>
    <summary>  There exist multitudes of cloud performance metrics, including workload
performance, application placement, software/hardware optimization,
scalability, capacity, reliability, agility and so on. In this paper, we
consider jointly optimizing the performance of the software applications in the
cloud. The challenges lie in bringing a diversity of raw data into tidy data
format, unifying performance data from multiple systems based on timestamps,
and assessing the quality of the processed performance data. Even after
verifying the quality of cloud performance data, additional challenges block
optimizing cloud computing. In this paper, we identify the challenges of cloud
computing from the perspectives of computing environment, data collection,
performance analytics and production environment.
</summary>
    <author>
      <name>Li Chen</name>
    </author>
    <author>
      <name>Colin Cunningham</name>
    </author>
    <author>
      <name>Pooja Jain</name>
    </author>
    <author>
      <name>Chenggang Qin</name>
    </author>
    <author>
      <name>Kingsum Chow</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pacific NW Software Quality Conference 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.07556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.03709v1</id>
    <updated>2016-12-12T14:37:34Z</updated>
    <published>2016-12-12T14:37:34Z</published>
    <title>Geographical Load Balancing across Green Datacenters</title>
    <summary>  "Geographic Load Balancing" is a strategy for reducing the energy cost of
data centers spreading across different terrestrial locations. In this paper,
we focus on load balancing among micro-datacenters powered by renewable energy
sources. We model via a Markov Chain the problem of scheduling jobs by
prioritizing datacenters where renewable energy is currently available. Not
finding a convenient closed form solution for the resulting chain, we use mean
field techniques to derive an asymptotic approximate model which instead is
shown to have an extremely simple and intuitive steady state solution. After
proving, using both theoretical and discrete event simulation results, that the
system performance converges to the asymptotic model for an increasing number
of datacenters, we exploit the simple closed form model's solution to
investigate relationships and trade-offs among the various system parameters.
</summary>
    <author>
      <name>Giovanni Neglia</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MAESTRO</arxiv:affiliation>
    </author>
    <author>
      <name>Matteo Sereno</name>
    </author>
    <author>
      <name>Giuseppe Bianchi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3003977.3003998</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3003977.3003998" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIGMETRICS Performance Evaluation Review, Jun 2016, Juan les Pins,
  France. 44 (2), pp.64 - 69, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1612.03709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.03709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.09532v1</id>
    <updated>2016-12-04T00:03:23Z</updated>
    <published>2016-12-04T00:03:23Z</published>
    <title>M/g/c/c state dependent queueing model for road traffic simulation</title>
    <summary>  In this paper, we present a stochastic queuing model for the road traffic,
which captures the stationary density-flow relationships in both uncongested
and congestion conditions. The proposed model is based on the $M/g/c/c$ state
dependent queuing model of Jain and Smith, and is inspired from the
deterministic Godunov scheme for the road traffic simulation. We first propose
a reformulation of the $M/g/c/c$ state dependent model that works with
density-flow fundamental diagrams rather than density-speed relationships. We
then extend this model in order to consider upstream traffic demand as well as
downstream traffic supply. Finally, we calculate the speed and travel time
distributions for the $M/g/c/c$ state dependent queuing model and for the
proposed model, and derive stationary performance measures (expected number of
cars, blocking probability, expected travel time, and throughput). A comparison
with results predicted by the $M/g/c/c$ state dependent queuing model shows
that the proposed model correctly represents the dynamics of traffic and gives
good performances measures. The results illustrate the good accuracy of the
proposed model.
</summary>
    <author>
      <name>Nacira Guerrouahane</name>
    </author>
    <author>
      <name>Djamil Aissani</name>
    </author>
    <author>
      <name>Louiza Bouallouche-Medjkoune</name>
    </author>
    <author>
      <name>Nadir Farhi</name>
    </author>
    <link href="http://arxiv.org/abs/1612.09532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.09532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.01328v1</id>
    <updated>2016-12-07T22:14:56Z</updated>
    <published>2016-12-07T22:14:56Z</published>
    <title>An Infinite Dimensional Model for a Many Server Priority Queue</title>
    <summary>  We consider a Markovian many server queueing system in which customers are
preemptively scheduled according to exogenously assigned priority levels. The
priority levels are randomly assigned from a continuous probability measure
rather than a discrete one and hence, the queue is modeled by an infinite
dimensional stochastic process. We analyze the equilibrium behavior of the
system and provide several results. We derive the Radon-Nikodym derivative
(with respect to Lebesgue measure) of the measure that describes the average
distribution of customer priority levels in the system; we provide a formula
for the expected sojourn time of a customer as a function of his priority
level; and we provide a formula for the expected waiting time of a customer as
a function of his priority level. We verify our theoretical analysis with
discrete-event simulations. We discuss how each of our results generalizes
previous work on infinite dimensional models for single server priority queues.
</summary>
    <author>
      <name>Neal Master</name>
    </author>
    <author>
      <name>Zhengyuan Zhou</name>
    </author>
    <author>
      <name>Nicholas Bambos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages. arXiv admin note: substantial text overlap with
  arXiv:1609.07996</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.01328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.01328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05308v2</id>
    <updated>2018-06-13T13:49:11Z</updated>
    <published>2017-01-19T06:23:00Z</published>
    <title>GPGPU Performance Estimation with Core and Memory Frequency Scaling</title>
    <summary>  Graphics Processing Units (GPUs) support dynamic voltage and frequency
scaling (DVFS) in order to balance computational performance and energy
consumption. However, there still lacks simple and accurate performance
estimation of a given GPU kernel under different frequency settings on real
hardware, which is important to decide best frequency configuration for energy
saving. This paper reveals a fine-grained model to estimate the execution time
of GPU kernels with both core and memory frequency scaling. Over a 2.5x range
of both core and memory frequencies among 12 GPU kernels, our model achieves
accurate results (within 3.5\%) on real hardware. Compared with the cycle-level
simulators, our model only needs some simple micro-benchmark to extract a set
of hardware parameters and performance counters of the kernels to produce this
high accuracy.
</summary>
    <author>
      <name>Qiang Wang</name>
    </author>
    <author>
      <name>Xiaowen Chu</name>
    </author>
    <link href="http://arxiv.org/abs/1701.05308v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05308v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.06004v1</id>
    <updated>2017-01-21T10:00:27Z</updated>
    <published>2017-01-21T10:00:27Z</published>
    <title>Light traffic behavior under the power-of-two load balancing strategy:
  The case of heterogeneous servers</title>
    <summary>  We consider a multi-server queueing system under the power-of-two policy with
Poisson job arrivals, heterogeneous servers and a general job requirement
distribution; each server operates under the first-come first-serve policy and
there are no buffer constraints. We analyze the performance of this system in
light traffic by evaluating the first two light traffic derivatives of the
average job response time. These expressions point to several interesting
structural features associated with server heterogeneity in light traffic: For
unequal capacities, the average job response time is seen to decrease for small
values of the arrival rate, and the more diverse the server speeds, the greater
the gain in performance. These theoretical findings are assessed through
limited simulations.
</summary>
    <author>
      <name>Ane Izagirre</name>
    </author>
    <author>
      <name>Armand M. Makowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.06004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.06004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00629v2</id>
    <updated>2017-07-11T14:37:00Z</updated>
    <published>2017-02-02T11:41:32Z</published>
    <title>gearshifft - The FFT Benchmark Suite for Heterogeneous Platforms</title>
    <summary>  Fast Fourier Transforms (FFTs) are exploited in a wide variety of fields
ranging from computer science to natural sciences and engineering. With the
rising data production bandwidths of modern FFT applications, judging best
which algorithmic tool to apply, can be vital to any scientific endeavor. As
tailored FFT implementations exist for an ever increasing variety of high
performance computer hardware, choosing the best performing FFT implementation
has strong implications for future hardware purchase decisions, for resources
FFTs consume and for possibly decisive financial and time savings ahead of the
competition. This paper therefor presents gearshifft, which is an open-source
and vendor agnostic benchmark suite to process a wide variety of problem sizes
and types with state-of-the-art FFT implementations (fftw, clfft and cufft).
gearshifft provides a reproducible, unbiased and fair comparison on a wide
variety of hardware to explore which FFT variant is best for a given problem
size.
</summary>
    <author>
      <name>Peter Steinbach</name>
    </author>
    <author>
      <name>Matthias Werner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-58667-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-58667-0" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">High Performance Computing, Theoretical Computer Science and
  General Issues, Vol. 10266, Springer International Publishing. (ISC High
  Performance 2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.00629v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00629v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07554v1</id>
    <updated>2017-02-24T12:17:21Z</updated>
    <published>2017-02-24T12:17:21Z</published>
    <title>An analysis of core- and chip-level architectural features in four
  generations of Intel server processors</title>
    <summary>  This paper presents a survey of architectural features among four generations
of Intel server processors (Sandy Bridge, Ivy Bridge, Haswell, and Broad- well)
with a focus on performance with floating point workloads. Starting on the core
level and going down the memory hierarchy we cover instruction throughput for
floating-point instructions, L1 cache, address generation capabilities, core
clock speed and its limitations, L2 and L3 cache bandwidth and latency, the
impact of Cluster on Die (CoD) and cache snoop modes, and the Uncore clock
speed. Using microbenchmarks we study the influence of these factors on code
performance. This insight can then serve as input for analytic performance
models. We show that the energy efficiency of the LINPACK and HPCG benchmarks
can be improved considerably by tuning the Uncore clock speed without
sacrificing performance, and that the Graph500 benchmark performance may profit
from a suitable choice of cache snoop mode settings.
</summary>
    <author>
      <name>Johannes Hofmann</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <author>
      <name>Dietmar Fey</name>
    </author>
    <link href="http://arxiv.org/abs/1702.07554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08823v1</id>
    <updated>2017-03-26T15:13:36Z</updated>
    <published>2017-03-26T15:13:36Z</published>
    <title>Groups of Repairmen and Repair-based Load Balancing in Supermarket
  Models with Repairable Servers</title>
    <summary>  Supermarket models are a class of interesting parallel queueing networks with
dynamic randomized load balancing and real-time resource management. When the
parallel servers are subject to breakdowns and repairs, analysis of such a
supermarket model becomes more difficult and challenging. In this paper, we
apply the mean-field theory to studying four interrelated supermarket models
with repairable servers, and numerically indicate impact of the different
repairman groups on performance of the systems. First, we set up the systems of
mean-field equations for the supermarket models with repairable servers. Then
we prove the asymptotic independence of the supermarket models through the
operator semi-group and the mean-field limit. Furthermore, we show that the
fixed points of the supermarket models satisfy the systems of nonlinear
equations. Finally, we use the fixed points to give numerical computation for
performer analysis, and provide valuable observations on model improvement.
Therefore, this paper provides a new and effective method in the study of
complex supermarket models.
</summary>
    <author>
      <name>Na Li</name>
    </author>
    <author>
      <name>Quan-Lin Li</name>
    </author>
    <author>
      <name>Zhe George Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">48 pages; 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.08823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M20, 90B22, 90B18" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; C.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01722v3</id>
    <updated>2018-04-11T09:42:06Z</updated>
    <published>2017-04-06T06:23:45Z</published>
    <title>On the equivalence between multiclass processor sharing and random order
  scheduling policies</title>
    <summary>  Consider a single server system serving a multiclass population. Some popular
scheduling policies for such system are the discriminatory processor sharing
(DPS), discriminatory random order service (DROS), generalized processor
sharing (GPS) and weighted fair queueing (WFQ). In this paper, we propose two
classes of policies, namely MPS (multiclass processor sharing) and MROS
(multiclass random order service), that generalize the four policies mentioned
above. For the special case when the multiclass population arrive according to
Poisson processes and have independent and exponential service requirement with
parameter $\mu$, we show that the tail of the sojourn time distribution for a
class $i$ customer in a system with the MPS policy is a constant multiple of
the tail of the waiting time distribution of a class $i$ customer in a system
with the MROS policy. This result implies that for a class $i$ customer, the
tail of the sojourn time distribution in a system with the DPS (GPS) scheduling
policy is a constant multiple of the tail of the waiting time distribution in a
system with the DROS (respectively WFQ) policy.
</summary>
    <author>
      <name>Konstantin Avrachenkov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NEO</arxiv:affiliation>
    </author>
    <author>
      <name>Tejas Bodas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS-SARA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1704.01722v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01722v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02003v2</id>
    <updated>2017-05-17T01:44:23Z</updated>
    <published>2017-04-06T19:48:37Z</published>
    <title>A Comparison of Parallel Graph Processing Implementations</title>
    <summary>  The rapidly growing number of large network analysis problems has led to the
emergence of many parallel and distributed graph processing systems---one
survey in 2014 identified over 80. Since then, the landscape has evolved; some
packages have become inactive while more are being developed. Determining the
best approach for a given problem is infeasible for most developers. To enable
easy, rigorous, and repeatable comparison of the capabilities of such systems,
we present an approach and associated software for analyzing the performance
and scalability of parallel, open-source graph libraries. We demonstrate our
approach on five graph processing packages: GraphMat, the Graph500, the Graph
Algorithm Platform Benchmark Suite, GraphBIG, and PowerGraph using synthetic
and real-world datasets. We examine previously overlooked aspects of parallel
graph processing performance such as phases of execution and energy usage for
three algorithms: breadth first search, single source shortest paths, and
PageRank and compare our results to Graphalytics.
</summary>
    <author>
      <name>Samuel Pollard</name>
    </author>
    <author>
      <name>Boyana Norris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures, Submitted to EuroPar 2017 and rejected. Revised
  and submitted to IEEE Cluster 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.02003v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02003v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04849v8</id>
    <updated>2019-10-22T17:22:52Z</updated>
    <published>2017-04-17T02:18:59Z</published>
    <title>Stationary Distribution of a Generalized LRU-MRU Content Cache</title>
    <summary>  Many different caching mechanisms have been previously proposed, exploring
different insertion and eviction policies and their performance individually
and as part of caching networks. We obtain a novel closed-form stationary
invariant distribution for a generalization of LRU and MRU caching nodes under
a reference Markov model. Numerical comparisons are made with an "Incremental
Rank Progress" (IRP a.k.a. CLIMB) and random eviction (a.k.a. random
replacement) methods under a steady-state Zipf popularity distribution. The
range of cache hit probabilities is smaller under MRU and larger under IRP
compared to LRU. We conclude with the invariant distribution for a special case
of a random-eviction caching tree-network and associated discussion.
</summary>
    <author>
      <name>George Kesidis</name>
    </author>
    <link href="http://arxiv.org/abs/1704.04849v8" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04849v8" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04857v1</id>
    <updated>2017-04-17T03:20:13Z</updated>
    <published>2017-04-17T03:20:13Z</published>
    <title>On the Capacity Requirement for Arbitrary End-to-End Deadline and
  Reliability Guarantees in Multi-hop Networks</title>
    <summary>  It has been shown that it is impossible to achieve both stringent end-to-end
deadline and reliability guarantees in a large network without having complete
information of all future packet arrivals. In order to maintain desirable
performance in the presence of uncertainty of future packet arrivals, common
practice is to add redundancy by increasing link capacities. This paper studies
the amount of capacity needed to provide stringent performance guarantees. We
propose a low-complexity online algorithm and prove that it only requires a
small amount of redundancy to guarantee both end-to-end deadline and
reliability. Further, we show that in large networks with very high reliability
requirements, the redundancy needed by our policy is at most twice as large as
a theoretical lower bound. Also, for practical implementation, we propose a
fully distributed protocol based on the previous centralized policy. Without
adding redundancy, we further propose a low-complexity order-optimal online
policy for the network. Simulation results also show that our policy achieves
much better performance than other state-of-the-art policies.
</summary>
    <author>
      <name>Han Deng</name>
    </author>
    <author>
      <name>I-Hong Hou</name>
    </author>
    <link href="http://arxiv.org/abs/1704.04857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05867v4</id>
    <updated>2023-03-08T17:08:15Z</updated>
    <published>2017-04-19T18:05:04Z</published>
    <title>A note on integrating products of linear forms over the unit simplex</title>
    <summary>  Integrating a product of linear forms over the unit simplex can be done in
polynomial time if the number of variables n is fixed (V. Baldoni et al.,
2011). In this note, we highlight that this problem is equivalent to obtaining
the normalizing constant of state probabilities for a popular class of Markov
processes used in queueing network theory. In light of this equivalence, we
survey existing computational algorithms developed in queueing theory that can
be used for exact integration. For example, under some regularity conditions,
queueing theory algorithms can exactly integrate a product of linear forms of
total degree N by solving N systems of linear equations.
</summary>
    <author>
      <name>Giuliano Casale</name>
    </author>
    <link href="http://arxiv.org/abs/1704.05867v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05867v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; G.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08657v2</id>
    <updated>2017-05-29T17:39:27Z</updated>
    <published>2017-04-27T17:01:07Z</published>
    <title>Accelerating Discrete Wavelet Transforms on Parallel Architectures</title>
    <summary>  The 2-D discrete wavelet transform (DWT) can be found in the heart of many
image-processing algorithms. Until recently, several studies have compared the
performance of such transform on various shared-memory parallel architectures,
especially on graphics processing units (GPUs). All these studies, however,
considered only separable calculation schemes. We show that corresponding
separable parts can be merged into non-separable units, which halves the number
of steps. In addition, we introduce an optional optimization approach leading
to a reduction in the number of arithmetic operations. The discussed schemes
were adapted on the OpenCL framework and pixel shaders, and then evaluated
using GPUs of two biggest vendors. We demonstrate the performance of the
proposed non-separable methods by comparison with existing separable schemes.
The non-separable schemes outperform their separable counterparts on numerous
setups, especially considering the pixel shaders.
</summary>
    <author>
      <name>David Barina</name>
    </author>
    <author>
      <name>Michal Kula</name>
    </author>
    <author>
      <name>Michal Matysek</name>
    </author>
    <author>
      <name>Pavel Zemcik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted on WSCG 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08657v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08657v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.06102v5</id>
    <updated>2018-12-30T13:25:14Z</updated>
    <published>2017-05-17T11:43:08Z</published>
    <title>Scheduling Distributed Resources in Heterogeneous Private Clouds</title>
    <summary>  We first consider the static problem of allocating resources to ( i.e. ,
scheduling) multiple distributed application framework s, possibly with
different priorities and server preferences , in a private cloud with
heterogeneous servers. Several fai r scheduling mechanisms have been proposed
for this purpose. We extend pr ior results on max-min and proportional fair
scheduling to t his constrained multiresource and multiserver case for generi c
fair scheduling criteria. The task efficiencies (a metric r elated to
proportional fairness) of max-min fair allocations found b y progressive
filling are compared by illustrative examples . They show that "server
specific" fairness criteria and those that are b ased on residual (unreserved)
resources are more efficient.
</summary>
    <author>
      <name>George Kesidis</name>
    </author>
    <author>
      <name>Yuquan Shan</name>
    </author>
    <author>
      <name>Yujia Wang</name>
    </author>
    <author>
      <name>Bhuvan Urgaonkar</name>
    </author>
    <author>
      <name>Jalal Khamse-Ashari</name>
    </author>
    <author>
      <name>Ioanns Lambadaris</name>
    </author>
    <link href="http://arxiv.org/abs/1705.06102v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.06102v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07400v1</id>
    <updated>2017-05-21T05:51:21Z</updated>
    <published>2017-05-21T05:51:21Z</published>
    <title>MITHRIL: Mining Sporadic Associations for Cache Prefetching</title>
    <summary>  The growing pressure on cloud application scalability has accentuated storage
performance as a critical bottle- neck. Although cache replacement algorithms
have been extensively studied, cache prefetching - reducing latency by
retrieving items before they are actually requested remains an underexplored
area. Existing approaches to history-based prefetching, in particular, provide
too few benefits for real systems for the resources they cost. We propose
MITHRIL, a prefetching layer that efficiently exploits historical patterns in
cache request associations. MITHRIL is inspired by sporadic association rule
mining and only relies on the timestamps of requests. Through evaluation of 135
block-storage traces, we show that MITHRIL is effective, giving an average of a
55% hit ratio increase over LRU and PROBABILITY GRAPH, a 36% hit ratio gain
over AMP at reasonable cost. We further show that MITHRIL can supplement any
cache replacement algorithm and be readily integrated into existing systems.
Furthermore, we demonstrate the improvement comes from MITHRIL being able to
capture mid-frequency blocks.
</summary>
    <author>
      <name>Juncheng Yang</name>
    </author>
    <author>
      <name>Reza Karimi</name>
    </author>
    <author>
      <name>Trausti Sæmundsson</name>
    </author>
    <author>
      <name>Avani Wildani</name>
    </author>
    <author>
      <name>Ymir Vigfusson</name>
    </author>
    <link href="http://arxiv.org/abs/1705.07400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07575v1</id>
    <updated>2017-05-22T06:31:22Z</updated>
    <published>2017-05-22T06:31:22Z</published>
    <title>Mira: A Framework for Static Performance Analysis</title>
    <summary>  The performance model of an application can pro- vide understanding about its
runtime behavior on particular hardware. Such information can be analyzed by
developers for performance tuning. However, model building and analyzing is
frequently ignored during software development until perfor- mance problems
arise because they require significant expertise and can involve many
time-consuming application runs. In this paper, we propose a fast, accurate,
flexible and user-friendly tool, Mira, for generating performance models by
applying static program analysis, targeting scientific applications running on
supercomputers. We parse both the source code and binary to estimate
performance attributes with better accuracy than considering just source or
just binary code. Because our analysis is static, the target program does not
need to be executed on the target architecture, which enables users to perform
analysis on available machines instead of conducting expensive exper- iments on
potentially expensive resources. Moreover, statically generated models enable
performance prediction on non-existent or unavailable architectures. In
addition to flexibility, because model generation time is significantly reduced
compared to dynamic analysis approaches, our method is suitable for rapid
application performance analysis and improvement. We present several scientific
application validation results to demonstrate the current capabilities of our
approach on small benchmarks and a mini application.
</summary>
    <author>
      <name>Kewen Meng</name>
    </author>
    <author>
      <name>Boyana Norris</name>
    </author>
    <link href="http://arxiv.org/abs/1705.07575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07779v1</id>
    <updated>2017-05-22T14:47:06Z</updated>
    <published>2017-05-22T14:47:06Z</published>
    <title>Cost-Performance Tradeoffs in Fusing Unreliable Computational Units</title>
    <summary>  We investigate fusing several unreliable computational units that perform the
same task. We model an unreliable computational outcome as an additive
perturbation to its error-free result in terms of its fidelity and cost. We
analyze performance of repetition-based strategies that distribute cost across
several unreliable units and fuse their outcomes. When the cost is a convex
function of fidelity, the optimal repetition-based strategy in terms of
incurred cost while achieving a target mean-square error (MSE) performance may
fuse several computational units. For concave and linear costs, a single more
reliable unit incurs lower cost compared to fusion of several lower cost and
less reliable units while achieving the same MSE performance. We show how our
results give insight into problems from theoretical neuroscience, circuits, and
crowdsourcing.
</summary>
    <author>
      <name>Mehmet A. Donmez</name>
    </author>
    <author>
      <name>Maxim Raginsky</name>
    </author>
    <author>
      <name>Andrew C. Singer</name>
    </author>
    <author>
      <name>Lav R. Varshney</name>
    </author>
    <link href="http://arxiv.org/abs/1705.07779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.10756v1</id>
    <updated>2017-05-30T17:09:34Z</updated>
    <published>2017-05-30T17:09:34Z</published>
    <title>Tracking System Behaviour from Resource Usage Data</title>
    <summary>  Resource usage data, collected using tools such as TACC Stats, capture the
resource utilization by nodes within a high performance computing system. We
present methods to analyze the resource usage data to understand the system
performance and identify performance anomalies. The core idea is to model the
data as a three-way tensor corresponding to the compute nodes, usage metrics,
and time. Using the reconstruction error between the original tensor and the
tensor reconstructed from a low rank tensor decomposition, as a scalar
performance metric, enables us to monitor the performance of the system in an
online fashion. This error statistic is then used for anomaly detection that
relies on the assumption that the normal/routine behavior of the system can be
captured using a low rank approx- imation of the original tensor. We evaluate
the performance of the algorithm using information gathered from system logs
and show that the performance anomalies identified by the proposed method
correlates with critical errors reported in the system logs. Results are shown
for data collected for 2013 from the Lonestar4 system at the Texas Advanced
Computing Center (TACC)
</summary>
    <author>
      <name>Niyazi Sorkunlu</name>
    </author>
    <author>
      <name>Varun Chandola</name>
    </author>
    <author>
      <name>Abani Patra</name>
    </author>
    <link href="http://arxiv.org/abs/1705.10756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00522v1</id>
    <updated>2017-06-01T23:35:30Z</updated>
    <published>2017-06-01T23:35:30Z</published>
    <title>On the Scalability of Data Reduction Techniques in Current and Upcoming
  HPC Systems from an Application Perspective</title>
    <summary>  We implement and benchmark parallel I/O methods for the fully-manycore driven
particle-in-cell code PIConGPU. Identifying throughput and overall I/O size as
a major challenge for applications on today's and future HPC systems, we
present a scaling law characterizing performance bottlenecks in
state-of-the-art approaches for data reduction. Consequently, we propose,
implement and verify multi-threaded data-transformations for the I/O library
ADIOS as a feasible way to trade underutilized host-side compute potential on
heterogeneous systems for reduced I/O latency.
</summary>
    <author>
      <name>Axel Huebl</name>
    </author>
    <author>
      <name>Rene Widera</name>
    </author>
    <author>
      <name>Felix Schmitt</name>
    </author>
    <author>
      <name>Alexander Matthes</name>
    </author>
    <author>
      <name>Norbert Podhorszki</name>
    </author>
    <author>
      <name>Jong Youl Choi</name>
    </author>
    <author>
      <name>Scott Klasky</name>
    </author>
    <author>
      <name>Michael Bussmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-67630-2_2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-67630-2_2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures, accepted for DRBSD-1 in conjunction with ISC'17</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J.M. Kunkel et al. (Eds.): ISC High Performance Workshops 2017,
  LNCS 10524, pp. 15-29, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.00522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.8; B.4.3; I.6.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01341v1</id>
    <updated>2017-06-01T21:03:06Z</updated>
    <published>2017-06-01T21:03:06Z</published>
    <title>Performance Modeling and Prediction for Dense Linear Algebra</title>
    <summary>  This dissertation introduces measurement-based performance modeling and
prediction techniques for dense linear algebra algorithms. As a core principle,
these techniques avoid executions of such algorithms entirely, and instead
predict their performance through runtime estimates for the underlying compute
kernels. For a variety of operations, these predictions allow to quickly select
the fastest algorithm configurations from available alternatives. We consider
two scenarios that cover a wide range of computations:
  To predict the performance of blocked algorithms, we design
algorithm-independent performance models for kernel operations that are
generated automatically once per platform. For various matrix operations,
instantaneous predictions based on such models both accurately identify the
fastest algorithm, and select a near-optimal block size.
  For performance predictions of BLAS-based tensor contractions, we propose
cache-aware micro-benchmarks that take advantage of the highly regular
structure inherent to contraction algorithms. At merely a fraction of a
contraction's runtime, predictions based on such micro-benchmarks identify the
fastest combination of tensor traversal and compute kernel.
</summary>
    <author>
      <name>Elmar Peise</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">HPAC, RWTH Aachen</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1706.01341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09606v3</id>
    <updated>2019-03-29T07:37:47Z</updated>
    <published>2017-06-29T07:30:01Z</published>
    <title>Theoretical Performance Analysis of Vehicular Broadcast Communications
  at Intersection and their Optimization</title>
    <summary>  In this paper, we propose an optimization method for the broadcast rate in
vehicle-to-vehicle (V2V) broadcast communications at an intersection on the
basis of theoretical analysis. We consider a model in which locations of
vehicles are modeled separately as queuing and running segments and derive key
performance metrics of V2V broadcast communications via a stochastic geometry
approach. Since these theoretical expressions are mathematically intractable,
we developed closed-form approximate formulae for them. Using them, we optimize
the broadcast rate such that the mean number of successful receivers per unit
time is maximized. Because of the closed form approximation, the optimal rate
can be used as a guideline for a real-time control-method, which is not
achieved through time-consuming simulations. We evaluated our method through
numerical examples and demonstrated the effectiveness of our method.
</summary>
    <author>
      <name>Tatsuaki Kimura</name>
    </author>
    <author>
      <name>Hiroshi Saito</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09606v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09606v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00516v1</id>
    <updated>2017-07-03T12:57:54Z</updated>
    <published>2017-07-03T12:57:54Z</published>
    <title>A Linear Algebra Approach to Fast DNA Mixture Analysis Using GPUs</title>
    <summary>  Analysis of DNA samples is an important step in forensics, and the speed of
analysis can impact investigations. Comparison of DNA sequences is based on the
analysis of short tandem repeats (STRs), which are short DNA sequences of 2-5
base pairs. Current forensics approaches use 20 STR loci for analysis. The use
of single nucleotide polymorphisms (SNPs) has utility for analysis of complex
DNA mixtures. The use of tens of thousands of SNPs loci for analysis poses
significant computational challenges because the forensic analysis scales by
the product of the loci count and number of DNA samples to be analyzed. In this
paper, we discuss the implementation of a DNA sequence comparison algorithm by
re-casting the algorithm in terms of linear algebra primitives. By developing
an overloaded matrix multiplication approach to DNA comparisons, we can
leverage advances in GPU hardware and algoithms for Dense Generalized
Matrix-Multiply (DGEMM) to speed up DNA sample comparisons. We show that it is
possible to compare 2048 unknown DNA samples with 20 million known samples in
under 6 seconds using a NVIDIA K80 GPU.
</summary>
    <author>
      <name>Siddharth Samsi</name>
    </author>
    <author>
      <name>Brian Helfer</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Albert Reuther</name>
    </author>
    <author>
      <name>Darrell O. Ricke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HPEC.2017.8091027</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HPEC.2017.8091027" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at the 2017 IEEE High Performance Extreme
  Computing conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.00516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06204v4</id>
    <updated>2018-07-09T22:19:02Z</updated>
    <published>2017-07-19T17:12:58Z</published>
    <title>On the Convergence of the TTL Approximation for an LRU Cache under
  Independent Stationary Request Processes</title>
    <summary>  The modeling and analysis of an LRU cache is extremely challenging as exact
results for the main performance metrics (e.g. hit rate) are either lacking or
cannot be used because of their high computational complexity for large caches.
As a result, various approximations have been proposed. The state-of-the-art
method is the so-called TTL approximation, first proposed and shown to be
asymptotically exact for IRM requests by Fagin. It has been applied to various
other workload models and numerically demonstrated to be accurate but without
theoretical justification. In this paper we provide theoretical justification
for the approximation in the case where distinct contents are described by
independent stationary and ergodic processes. We show that this approximation
is exact as the cache size and the number of contents go to infinity. This
extends earlier results for the independent reference model. Moreover, we
establish results not only for the aggregate cache hit probability but also for
every individual content. Last, we obtain bounds on the rate of convergence.
</summary>
    <author>
      <name>Bo Jiang</name>
    </author>
    <author>
      <name>Philippe Nain</name>
    </author>
    <author>
      <name>Don Towsley</name>
    </author>
    <link href="http://arxiv.org/abs/1707.06204v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06204v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07175v2</id>
    <updated>2017-07-26T09:09:00Z</updated>
    <published>2017-07-22T14:49:24Z</published>
    <title>Asymptotic Performance Evaluation of Battery Swapping and Charging
  Station for Electric Vehicles</title>
    <summary>  A battery swapping and charging station (BSCS) is an energy refueling
station, where i) electric vehicles (EVs) with depleted batteries (DBs) can
swap their DBs for fully-charged ones, and ii) the swapped DBs are then charged
until they are fully-charged. Successful deployment of a BSCS system
necessitates a careful planning of swapping- and charging-related
infrastructures, and thus a comprehensive performance evaluation of the BSCS is
becoming crucial. This paper studies such a performance evaluation problem with
a novel mixed queueing network (MQN) model and validates this model with
extensive numerical simulation. We adopt the EVs' blocking probability as our
quality-of-service measure and focus on studying the impact of the key
parameters of the BSCS (e.g., the numbers of parking spaces, swapping islands,
chargers, and batteries) on the blocking probability. We prove a necessary and
sufficient condition for showing the ergodicity of the MQN when the number of
batteries approaches infinity, and further prove that the blocking probability
has two different types of asymptotic behaviors. Meanwhile, for each type of
asymptotic behavior, we analytically derive the asymptotic lower bound of the
blocking probability.
</summary>
    <author>
      <name>Xiaoqi Tan</name>
    </author>
    <author>
      <name>Bo Sun</name>
    </author>
    <author>
      <name>Yuan Wu</name>
    </author>
    <author>
      <name>Danny H. K. Tsang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.07175v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07175v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09642v2</id>
    <updated>2017-09-03T16:53:21Z</updated>
    <published>2017-07-30T17:19:26Z</published>
    <title>Adaptive Performance Optimization under Power Constraint in Multi-thread
  Applications with Diverse Scalability</title>
    <summary>  In modern data centers, energy usage represents one of the major factors
affecting operational costs. Power capping is a technique that limits the power
consumption of individual systems, which allows reducing the overall power
demand at both cluster and data center levels. However, literature power
capping approaches do not fit well the nature of important applications based
on first-class multi-thread technology. For these applications performance may
not grow linearly as a function of the thread-level parallelism because of the
need for thread synchronization while accessing shared resources, such as
shared data. In this paper we consider the problem of maximizing the
application performance under a power cap by dynamically tuning the
thread-level parallelism and the power state of the CPU-cores. Based on
experimental observations, we design an adaptive technique that selects in
linear time the optimal combination of thread-level parallelism and CPU-core
power state for the specific workload profile of the multi-threaded
application. We evaluate our proposal by relying on different benchmarks,
configured to use different thread synchronization methods, and compare its
effectiveness to different state-of-the-art techniques.
</summary>
    <author>
      <name>Stefano Conoci</name>
    </author>
    <author>
      <name>Pierangelo Di Sanzo</name>
    </author>
    <author>
      <name>Bruno Ciciani</name>
    </author>
    <author>
      <name>Francesco Quaglia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 18 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09642v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09642v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04567v4</id>
    <updated>2018-02-03T12:26:15Z</updated>
    <published>2017-08-15T15:54:19Z</published>
    <title>GARDENIA: A Domain-specific Benchmark Suite for Next-generation
  Accelerators</title>
    <summary>  This paper presents the Graph Analytics Repository for Designing
Next-generation Accelerators (GARDENIA), a benchmark suite for studying
irregular algorithms on massively parallel accelerators. Existing generic
benchmarks for accelerators have mainly focused on high performance computing
(HPC) applications with limited control and data irregularity, while available
graph analytics benchmarks do not apply state-of-the-art algorithms and/or
optimization techniques. GARDENIA includes emerging irregular applications in
big-data and machine learning domains which mimic massively multithreaded
commercial programs running on modern large-scale datacenters. Our
characterization shows that GARDENIA exhibits irregular microarchitectural
behavior which is quite different from structured workloads and
straightforward-implemented graph benchmarks.
</summary>
    <author>
      <name>Zhen Xu</name>
    </author>
    <author>
      <name>Xuhao Chen</name>
    </author>
    <author>
      <name>Jie Shen</name>
    </author>
    <author>
      <name>Yang Zhang</name>
    </author>
    <author>
      <name>Cheng Chen</name>
    </author>
    <author>
      <name>Canqun Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures, journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.04567v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04567v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05847v1</id>
    <updated>2017-08-19T14:06:03Z</updated>
    <published>2017-08-19T14:06:03Z</published>
    <title>Unbounded product-form Petri nets</title>
    <summary>  Computing steady-state distributions in infinite-state stochastic systems is
in general a very dificult task. Product-form Petri nets are those Petri nets
for which the steady-state distribution can be described as a natural product
corresponding, up to a normalising constant, to an exponentiation of the
markings. However, even though some classes of nets are known to have a
product-form distribution, computing the normalising constant can be hard. The
class of (closed) {\Pi}3-nets has been proposed in an earlier work, for which
it is shown that one can compute the steady-state distribution efficiently.
However these nets are bounded. In this paper, we generalise queuing Markovian
networks and closed {\Pi}3-nets to obtain the class of open {\Pi}3-nets, that
generate infinite-state systems. We show interesting properties of these nets:
(1) we prove that liveness can be decided in polynomial time, and that
reachability in live {\Pi}3-nets can be decided in polynomial time; (2) we show
that we can decide ergodicity of such nets in polynomial time as well; (3) we
provide a pseudo-polynomial time algorithm to compute the normalising constant.
</summary>
    <author>
      <name>Patricia Bouyer</name>
    </author>
    <author>
      <name>Serge Haddad</name>
    </author>
    <author>
      <name>Vincent Jugé</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.05847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.07036v4</id>
    <updated>2018-01-24T02:00:47Z</updated>
    <published>2017-08-23T15:04:57Z</published>
    <title>Optimal Threshold Policies for Robust Data Center Control</title>
    <summary>  With the simultaneous rise of energy costs and demand for cloud computing,
efficient control of data centers becomes crucial. In the data center control
problem, one needs to plan at every time step how many servers to switch on or
off in order to meet stochastic job arrivals while trying to minimize
electricity consumption. This problem becomes particularly challenging when
servers can be of various types and jobs from different classes can only be
served by certain types of server, as it is often the case in real data
centers. We model this problem as a robust Markov Decision Process (i.e., the
transition function is not assumed to be known precisely). We give sufficient
conditions (which seem to be reasonable and satisfied in practice) guaranteeing
that an optimal threshold policy exists. This property can then be exploited in
the design of an efficient solving method, which we provide. Finally, we
present some experimental results demonstrating the practicability of our
approach and compare with a previous related approach based on model predictive
control.
</summary>
    <author>
      <name>Paul Weng</name>
    </author>
    <author>
      <name>Zeqi Qiu</name>
    </author>
    <author>
      <name>John Costanzo</name>
    </author>
    <author>
      <name>Xiaoqi Yin</name>
    </author>
    <author>
      <name>Bruno Sinopoli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Long version of paper accepted at AETA 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.07036v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07036v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.09328v1</id>
    <updated>2017-08-30T15:46:58Z</updated>
    <published>2017-08-30T15:46:58Z</published>
    <title>Insensitivity of the mean-field Limit of Loss Systems Under Power-of-d
  Routing</title>
    <summary>  In this paper, we study large multi-server loss models under power-of-$d$
routing scheme when service time distributions are general with finite mean.
Previous works have addressed the exponential service time case when the number
of servers goes to infinity giving rise to a mean field model. The fixed point
of limiting mean field equations (MFE) was shown to be insensitive to the
service time distribution through simulation. Showing insensitivity to general
service time distributions has remained an open problem. Obtaining the MFE in
this case poses a challenge due to the resulting Markov description of the
system being in positive orthant as opposed to a finite chain in the
exponential case. In this paper, we first obtain the MFE and then show that the
MFE has a unique fixed point that coincides with the fixed point in the
exponential case thus establishing insensitivity. The approach is via a
measure-valued Markov process representation and the martingale problem to
establish the mean-field limit. The techniques can be applied to other queueing
models.
</summary>
    <author>
      <name>Thirupathaiah Vasantam</name>
    </author>
    <author>
      <name>Arpan Mukhopadhyay</name>
    </author>
    <author>
      <name>Ravi R Mazumdar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Advances in Applied Probability</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.09328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.09328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60K35 (Primary) 60F10, 60J10, 62F15 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.2; C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08951v1</id>
    <updated>2017-09-26T11:42:34Z</updated>
    <published>2017-09-26T11:42:34Z</published>
    <title>Report from GI-Dagstuhl Seminar 16394: Software Performance Engineering
  in the DevOps World</title>
    <summary>  This report documents the program and the outcomes of GI-Dagstuhl Seminar
16394 "Software Performance Engineering in the DevOps World".
  The seminar addressed the problem of performance-aware DevOps. Both, DevOps
and performance engineering have been growing trends over the past one to two
years, in no small part due to the rise in importance of identifying
performance anomalies in the operations (Ops) of cloud and big data systems and
feeding these back to the development (Dev). However, so far, the research
community has treated software engineering, performance engineering, and cloud
computing mostly as individual research areas. We aimed to identify
cross-community collaboration, and to set the path for long-lasting
collaborations towards performance-aware DevOps.
  The main goal of the seminar was to bring together young researchers (PhD
students in a later stage of their PhD, as well as PostDocs or Junior
Professors) in the areas of (i) software engineering, (ii) performance
engineering, and (iii) cloud computing and big data to present their current
research projects, to exchange experience and expertise, to discuss research
challenges, and to develop ideas for future collaborations.
</summary>
    <author>
      <name>Andre van Hoorn</name>
    </author>
    <author>
      <name>Pooyan Jamshidi</name>
    </author>
    <author>
      <name>Philipp Leitner</name>
    </author>
    <author>
      <name>Ingo Weber</name>
    </author>
    <link href="http://arxiv.org/abs/1709.08951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00414v1</id>
    <updated>2017-10-01T21:09:29Z</updated>
    <published>2017-10-01T21:09:29Z</published>
    <title>Straggler Mitigation by Delayed Relaunch of Tasks</title>
    <summary>  Redundancy for straggler mitigation, originally in data download and more
recently in distributed computing context, has been shown to be effective both
in theory and practice. Analysis of systems with redundancy has drawn
significant attention and numerous papers have studied pain and gain of
redundancy under various service models and assumptions on the straggler
characteristics. We here present a cost (pain) vs. latency (gain) analysis of
using simple replication or erasure coding for straggler mitigation in
executing jobs with many tasks. We quantify the effect of the tail of task
execution times and discuss tail heaviness as a decisive parameter for the cost
and latency of using redundancy. Specifically, we find that coded redundancy
achieves better cost vs. latency tradeoff than simple replication and can yield
reduction in both cost and latency under less heavy tailed execution times. We
show that delaying redundancy is not effective in reducing cost and that
delayed relaunch of stragglers can yield significant reduction in cost and
latency. We validate these observations by comparing with the simulations that
use empirical distributions extracted from Google cluster data.
</summary>
    <author>
      <name>Mehmet Fatih Aktas</name>
    </author>
    <author>
      <name>Pei Peng</name>
    </author>
    <author>
      <name>Emina Soljanin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for IFIP WG 7.3 Performance 2017. Nov. 14-16, 2017, New
  York, NY USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.00414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00748v1</id>
    <updated>2017-10-02T16:04:27Z</updated>
    <published>2017-10-02T16:04:27Z</published>
    <title>Effective Straggler Mitigation: Which Clones Should Attack and When?</title>
    <summary>  Redundancy for straggler mitigation, originally in data download and more
recently in distributed computing context, has been shown to be effective both
in theory and practice. Analysis of systems with redundancy has drawn
significant attention and numerous papers have studied pain and gain of
redundancy under various service models and assumptions on the straggler
characteristics. We here present a cost (pain) vs. latency (gain) analysis of
using simple replication or erasure coding for straggler mitigation in
executing jobs with many tasks. We quantify the effect of the tail of task
execution times and discuss tail heaviness as a decisive parameter for the cost
and latency of using redundancy. Specifically, we find that coded redundancy
achieves better cost vs. latency and allows for greater achievable latency and
cost tradeoff region compared to replication and can yield reduction in both
cost and latency under less heavy tailed execution times. We show that delaying
redundancy is not effective in reducing cost.
</summary>
    <author>
      <name>Mehmet Fatih Aktas</name>
    </author>
    <author>
      <name>Pei Peng</name>
    </author>
    <author>
      <name>Emina Soljanin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at MAMA Workshop in conjunction with ACM Sigmetrics, June
  5, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.00748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01079v2</id>
    <updated>2018-11-02T10:56:03Z</updated>
    <published>2017-10-03T11:25:24Z</published>
    <title>Optimal DNN Primitive Selection with Partitioned Boolean Quadratic
  Programming</title>
    <summary>  Deep Neural Networks (DNNs) require very large amounts of computation both
for training and for inference when deployed in the field. Many different
algorithms have been proposed to implement the most computationally expensive
layers of DNNs. Further, each of these algorithms has a large number of
variants, which offer different trade-offs of parallelism, data locality,
memory footprint, and execution time. In addition, specific algorithms operate
much more efficiently on specialized data layouts and formats.
  We state the problem of optimal primitive selection in the presence of data
format transformations, and show that it is NP-hard by demonstrating an
embedding in the Partitioned Boolean Quadratic Assignment problem (PBQP).
  We propose an analytic solution via a PBQP solver, and evaluate our approach
experimentally by optimizing several popular DNNs using a library of more than
70 DNN primitives, on an embedded platform and a general purpose platform. We
show experimentally that significant gains are possible versus the state of the
art vendor libraries by using a principled analytic solution to the problem of
layout selection in the presence of data format transformations.
</summary>
    <author>
      <name>Andrew Anderson</name>
    </author>
    <author>
      <name>David Gregg</name>
    </author>
    <link href="http://arxiv.org/abs/1710.01079v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01079v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01113v1</id>
    <updated>2017-10-03T12:49:43Z</updated>
    <published>2017-10-03T12:49:43Z</published>
    <title>Relocation in Car Sharing Systems with Shared Stackable Vehicles:
  Modelling Challenges and Outlook</title>
    <summary>  Car sharing is expected to reduce traffic congestion and pollution in cities
while at the same time improving accessibility to public transport. However,
the most popular form of car sharing, one-way car sharing, still suffers from
the vehicle unbalance problem. Innovative solutions to this issue rely on
custom vehicles with stackable capabilities: customers or operators can drive a
train of vehicles if necessary, thus efficiently bringing several cars from an
area with few requests to an area with many requests. However, how to model a
car sharing system with stackable vehicles is an open problem in the related
literature. In this paper, we propose a queueing theoretical model to fill this
gap, and we use this model to derive an upper-bound on user-based relocation
capabilities. We also validate, for the first time in the related literature,
legacy queueing theoretical models against a trace of real car sharing data.
Finally, we present preliminary results about the impact, on car availability,
of simple user-based relocation heuristics with stackable vehicles. Our results
indicate that user-based relocation schemes that exploit vehicle stackability
can significantly improve car availability at stations.
</summary>
    <author>
      <name>Chiara Boldrini</name>
    </author>
    <author>
      <name>Riccardo Incaini</name>
    </author>
    <author>
      <name>Raffaele Bruno</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE ITSC 2017: 20th International Conference on Intelligent
  Transportation Systems, Yokohama, Japan. October 16 - 19, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.01113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02282v2</id>
    <updated>2018-08-07T07:12:41Z</updated>
    <published>2017-10-06T06:05:58Z</published>
    <title>The Quest for Scalability and Accuracy in the Simulation of the Internet
  of Things: an Approach based on Multi-Level Simulation</title>
    <summary>  This paper presents a methodology for simulating the Internet of Things (IoT)
using multi-level simulation models. With respect to conventional simulators,
this approach allows us to tune the level of detail of different parts of the
model without compromising the scalability of the simulation. As a use case, we
have developed a two-level simulator to study the deployment of smart services
over rural territories. The higher level is base on a coarse grained,
agent-based adaptive parallel and distributed simulator. When needed, this
simulator spawns OMNeT++ model instances to evaluate in more detail the issues
concerned with wireless communications in restricted areas of the simulated
world. The performance evaluation confirms the viability of multi-level
simulations for IoT environments.
</summary>
    <author>
      <name>Stefano Ferretti</name>
    </author>
    <author>
      <name>Gabriele D'Angelo</name>
    </author>
    <author>
      <name>Vittorio Ghini</name>
    </author>
    <author>
      <name>Moreno Marzolla</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DISTRA.2017.8167672</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DISTRA.2017.8167672" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the IEEE/ACM International Symposium on Distributed
  Simulation and Real Time Applications (DS-RT 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.02282v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02282v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02575v1</id>
    <updated>2017-10-06T20:25:59Z</updated>
    <published>2017-10-06T20:25:59Z</published>
    <title>Agile Calibration Process of Full-Stack Simulation Frameworks for V2X
  Communications</title>
    <summary>  Computer simulations and real-world car trials are essential to investigate
the performance of Vehicle-to-Everything (V2X) networks. However, simulations
are imperfect models of the physical reality and can be trusted only when they
indicate agreement with the real-world. On the other hand, trials lack
reproducibility and are subject to uncertainties and errors. In this paper, we
will illustrate a case study where the interrelationship between trials,
simulation, and the reality-of-interest is presented. Results are then compared
in a holistic fashion. Our study will describe the procedure followed to
macroscopically calibrate a full-stack network simulator to conduct
high-fidelity full-stack computer simulations.
</summary>
    <author>
      <name>Ioannis Mavromatis</name>
    </author>
    <author>
      <name>Andrea Tassi</name>
    </author>
    <author>
      <name>Robert J. Piechocki</name>
    </author>
    <author>
      <name>Andrew Nix</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VTCSpring.2019.8746302</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VTCSpring.2019.8746302" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEEE VNC 2017, Torino, IT</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.02575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06189v1</id>
    <updated>2017-10-17T10:00:08Z</updated>
    <published>2017-10-17T10:00:08Z</published>
    <title>Computation of gray-level co-occurrence matrix based on CUDA and its
  optimization</title>
    <summary>  As in various fields like scientific research and industrial application, the
computation time optimization is becoming a task that is of increasing
importance because of its highly parallel architecture. The graphics processing
unit is regarded as a powerful engine for application programs that demand
fairly high computation capabilities. Based on this, an algorithm was
introduced in this paper to optimize the method used to compute the gray-level
co-occurrence matrix (GLCM) of an image, and strategies (e.g., "copying",
"image partitioning", etc.) were proposed to optimize the parallel algorithm.
Results indicate that without losing the computational accuracy, the speed-up
ratio of the GLCM computation of images with different resolutions by GPU by
the use of CUDA was 50 times faster than that of the GLCM computation by CPU,
which manifested significantly improved performance.
</summary>
    <author>
      <name>Huichao Hong</name>
    </author>
    <author>
      <name>Lixin Zheng</name>
    </author>
    <author>
      <name>Shuwan Pan</name>
    </author>
    <link href="http://arxiv.org/abs/1710.06189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09506v1</id>
    <updated>2017-10-26T01:33:21Z</updated>
    <published>2017-10-26T01:33:21Z</published>
    <title>Analysis of the Leakage Queue: A Queueing Model for Energy Storage
  Systems with Self-discharge</title>
    <summary>  Energy storage is a crucial component of the smart grid, since it provides
the ability to buffer transient fluctuations of the energy supply from
renewable sources. Even without a load, energy storage systems experience a
reduction of the stored energy through self-discharge. In some storage
technologies, the rate of self-discharge can exceed 50% of the stored energy
per day. In this paper, we investigate the self-discharge phenomenon in energy
storage using a queueing system model, which we refer to as leakage queue. When
the average net charge is positive, we discover that the leakage queue operates
in one of two regimes: a leakage-dominated regime and a capacity-dominated
regime. We find that in the leakage-dominated regime, the stored energy
stabilizes at a point that is below the storage capacity. Under suitable
independence assumptions for energy supply and demand, the stored energy in
this regime closely follows a normal distribution. We present two methods for
computing probabilities of underflow and overflow at a leakage queue. The
methods are validated in a numerical example where the energy supply resembles
a wind energy source.
</summary>
    <author>
      <name>Majid Raeis</name>
    </author>
    <author>
      <name>Almut Burchard</name>
    </author>
    <author>
      <name>Jorg Liebeherr</name>
    </author>
    <link href="http://arxiv.org/abs/1710.09506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10325v1</id>
    <updated>2017-10-27T20:40:50Z</updated>
    <published>2017-10-27T20:40:50Z</published>
    <title>Power Modelling for Heterogeneous Cloud-Edge Data Centers</title>
    <summary>  Existing power modelling research focuses not on the method used for
developing models but rather on the model itself. This paper aims to develop a
method for deploying power models on emerging processors that will be used, for
example, in cloud-edge data centers. Our research first develops a hardware
counter selection method that appropriately selects counters most correlated to
power on ARM and Intel processors. Then, we propose a two stage power model
that works across multiple architectures. The key results are: (i) the
automated hardware performance counter selection method achieves comparable
selection to the manual selection methods reported in literature, and (ii) the
two stage power model can predict dynamic power more accurately on both ARM and
Intel processors when compared to classic power models.
</summary>
    <author>
      <name>Kai Chen</name>
    </author>
    <author>
      <name>Blesson Varghese</name>
    </author>
    <author>
      <name>Peter Kilpatrick</name>
    </author>
    <author>
      <name>Dimitrios S. Nikolopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,10 figures,conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10325v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10325v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00790v2</id>
    <updated>2018-02-17T18:47:58Z</updated>
    <published>2017-12-03T16:14:29Z</published>
    <title>SOAP: One Clean Analysis of All Age-Based Scheduling Policies</title>
    <summary>  We consider an extremely broad class of M/G/1 scheduling policies called
SOAP: Schedule Ordered by Age-based Priority. The SOAP policies include almost
all scheduling policies in the literature as well as an infinite number of
variants which have never been analyzed, or maybe not even conceived. SOAP
policies range from classic policies, like first-come, first-serve (FCFS),
foreground-background (FB), class-based priority, and shortest remaining
processing time (SRPT); to much more complicated scheduling rules, such as the
famously complex Gittins index policy and other policies in which a job's
priority changes arbitrarily with its age. While the response time of policies
in the former category is well understood, policies in the latter category have
resisted response time analysis. We present a universal analysis of all SOAP
policies, deriving the mean and Laplace-Stieltjes transform of response time.
</summary>
    <author>
      <name>Ziv Scully</name>
    </author>
    <author>
      <name>Mor Harchol-Balter</name>
    </author>
    <author>
      <name>Alan Scheller-Wolf</name>
    </author>
    <link href="http://arxiv.org/abs/1712.00790v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00790v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03209v1</id>
    <updated>2017-12-08T18:34:15Z</updated>
    <published>2017-12-08T18:34:15Z</published>
    <title>Task Scheduling for Heterogeneous Multicore Systems</title>
    <summary>  In recent years, as the demand for low energy and high performance computing
has steadily increased, heterogeneous computing has emerged as an important and
promising solution. Because most workloads can typically run most efficiently
on certain types of cores, mapping tasks on the best available resources can
not only save energy but also deliver high performance. However, optimal task
scheduling for performance and/or energy is yet to be solved for heterogeneous
platforms. The work presented herein mathematically formulates the optimal
heterogeneous system task scheduling as an optimization problem using queueing
theory. We analytically solve for the common case of two processor types, e.g.,
CPU+GPU, and give an optimal policy (CAB). We design the GrIn heuristic to
efficiently solve for near-optimal policy for any number of processor types
(within 1.6% of the optimal). Both policies work for any task size distribution
and processing order, and are therefore, general and practical. We extensively
simulate and validate the theory, and implement the proposed policy in a
CPU-GPU real platform to show the optimal throughput and energy improvement.
Comparing to classic policies like load-balancing, our results range from
1.08x~2.24x better performance or 1.08x~2.26x better energy efficiency in
simulations, and 2.37x~9.07x better performance in experiments.
</summary>
    <author>
      <name>Zhuo Chen</name>
    </author>
    <author>
      <name>Diana Marculescu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">heterogeneous systems; scheduling; performance modeling; queueing
  theory</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.03209v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03209v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03246v1</id>
    <updated>2017-12-08T19:02:48Z</updated>
    <published>2017-12-08T19:02:48Z</published>
    <title>Priority-Aware Near-Optimal Scheduling for Heterogeneous Multi-Core
  Systems with Specialized Accelerators</title>
    <summary>  To deliver high performance in power limited systems, architects have turned
to using heterogeneous systems, either CPU+GPU or mixed CPU-hardware systems.
However, in systems with different processor types and task affinities,
scheduling tasks becomes more challenging than in homogeneous multi-core
systems or systems without task affinities. The problem is even more complex
when specialized accelerators and task priorities are included. In this paper,
we provide a formal proof for the optimal scheduling policy for heterogeneous
systems with arbitrary number of resource types, including specialized
accelerators, independent of the task arrival rate, task size distribution, and
resource processing order. We transform the optimal scheduling policy to a
nonlinear integer optimization problem and propose a fast, near-optimal
algorithm. An additional heuristic is proposed for the case of priority-aware
scheduling. Our experimental results demonstrate that the proposed algorithm is
only 0.3% from the optimal and superior to conventional scheduling policies.
</summary>
    <author>
      <name>Zhuo Chen</name>
    </author>
    <author>
      <name>Diana Marculescu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">heterogeneous systems, performance modeling, queueing theory, optimal
  scheduling</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.03246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08285v1</id>
    <updated>2017-12-22T02:31:16Z</updated>
    <published>2017-12-22T02:31:16Z</published>
    <title>Grand Challenge: Optimized Stage Processing for Anomaly Detection on
  Numerical Data Streams</title>
    <summary>  The 2017 Grand Challenge focused on the problem of automatic detection of
anomalies for manufacturing equipment. This paper reports the technical details
of a solution focused on particular optimizations of the processing stages.
These included customized input parsing, fine tuning of a k-means clustering
algorithm and probability analysis using a lazy flavor of a Markov chain. We
have observed in our custom implementation that carefully tweaking these
processing stages at single node level by leveraging various data stream
characteristics can yield good performance results. We start the paper with
several observations concerning the input data stream, following with our
solution description with details on particular optimizations, and we conclude
with evaluation and a discussion of obtained results.
</summary>
    <author>
      <name>Ciprian Amariei</name>
    </author>
    <author>
      <name>Paul Diac</name>
    </author>
    <author>
      <name>Emanuel Onica</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3093742.3095101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3093742.3095101" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">DEBS 2017, Proceedings of the 11th ACM International Conference on
  Distributed and Event-based Systems, Pages 286-291</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.08285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08738v3</id>
    <updated>2018-04-27T02:19:42Z</updated>
    <published>2017-12-23T08:44:28Z</published>
    <title>Protecting real-time GPU kernels on integrated CPU-GPU SoC platforms</title>
    <summary>  Integrated CPU-GPU architecture provides excellent acceleration capabilities
for data parallel applications on embedded platforms while meeting the size,
weight and power (SWaP) requirements. However, sharing of main memory between
CPU applications and GPU kernels can severely affect the execution of GPU
kernels and diminish the performance gain provided by GPU. For example, in the
NVIDIA Tegra K1 platform which has the integrated CPU-GPU architecture, we
noticed that in the worst case scenario, the GPU kernels can suffer as much as
4X slowdown in the presence of co-running memory intensive CPU applications
compared to their solo execution. In this paper, we propose a software
mechanism, which we call BWLOCK++, to protect the performance of GPU kernels
from co-scheduled memory intensive CPU applications.
</summary>
    <author>
      <name>Waqar Ali</name>
    </author>
    <author>
      <name>Heechul Yun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will be published at ECRTS-2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.08738v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08738v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09431v1</id>
    <updated>2017-12-26T21:49:40Z</updated>
    <published>2017-12-26T21:49:40Z</published>
    <title>The L-CSC cluster: greenest supercomputer in the world in Green500 list
  of November 2014</title>
    <summary>  The L-CSC (Lattice Computer for Scientific Computing) is a general purpose
compute cluster built of commodity hardware installed at GSI. Its main
operational purpose is Lattice QCD (LQCD) calculations for physics simulations.
Quantum Chromo Dynamics (QCD) is the physical theory describing the strong
force, one of the four known fundamental interactions in the universe. L-CSC
leverages a multi-GPU design accommodating the huge demand of LQCD for memory
bandwidth. In recent years, heterogeneous clusters with accelerators such as
GPUs have become more and more powerful while supercomputers in general have
shown enormous increases in power consumption making electricity costs and
cooling a significant factor in the total cost of ownership. Using mainly GPUs
for processing, L-CSC is very power efficient, and its architecture was
optimized to provide the greatest possible power efficiency. This paper
presents the cluster design as well as optimizations to improve the power
efficiency. It examines the power measurements performed for the Green500 list
of the most power efficient supercomputers in the world which led to the number
1 position as the greenest supercomputer in November 2014.
</summary>
    <author>
      <name>D. Rohr</name>
    </author>
    <author>
      <name>G. Neskovic</name>
    </author>
    <author>
      <name>M. Radtke</name>
    </author>
    <author>
      <name>V. Lindenstruth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, proceedings to Supercomputing Frontiers conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.09431v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09431v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04329v2</id>
    <updated>2018-01-16T19:08:36Z</updated>
    <published>2018-01-12T21:57:14Z</published>
    <title>Effect of Meltdown and Spectre Patches on the Performance of HPC
  Applications</title>
    <summary>  In this work we examine how the updates addressing Meltdown and Spectre
vulnerabilities impact the performance of HPC applications. To study this we
use the application kernel module of XDMoD to test the performance before and
after the application of the vulnerability patches. We tested the performance
difference for multiple application and benchmarks including: NWChem, NAMD,
HPCC, IOR, MDTest and IMB. The results show that although some specific
functions can have performance decreased by as much as 74%, the majority of
individual metrics indicates little to no decrease in performance. The
real-world applications show a 2-3% decrease in performance for single node
jobs and a 5-11% decrease for parallel multi node jobs.
</summary>
    <author>
      <name>Nikolay A. Simakov</name>
    </author>
    <author>
      <name>Martins D. Innus</name>
    </author>
    <author>
      <name>Matthew D. Jones</name>
    </author>
    <author>
      <name>Joseph P. White</name>
    </author>
    <author>
      <name>Steven M. Gallo</name>
    </author>
    <author>
      <name>Robert L. DeLeon</name>
    </author>
    <author>
      <name>Thomas R. Furlani</name>
    </author>
    <link href="http://arxiv.org/abs/1801.04329v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04329v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06153v1</id>
    <updated>2018-01-18T18:06:09Z</updated>
    <published>2018-01-18T18:06:09Z</published>
    <title>LCD: Low Latency Command Dissemination for A Platoon of Vehicles</title>
    <summary>  In a vehicular platoon, a lead vehicle that is responsible for managing the
platoon's moving directions and velocity periodically disseminates control
commands to following vehicles based on vehicle-to-vehicle communications.
However, reducing command dissemination latency with multiple vehicles while
ensuring successful message delivery to the tail vehicle is challenging. We
propose a new linear dynamic programming algorithm using backward induction and
interchange arguments to minimize the dissemination latency of the vehicles.
Furthermore, a closed form of dissemination latency in vehicular platoon is
obtained by utilizing Markov chain with M/M/1 queuing model. Simulation results
confirm that the proposed dynamic programming algorithm improves the
dissemination rate by at least 50.9%, compared to similar algorithms in the
literature. Moreover, it also approximates the best performance with the
maximum gap of up to 0.2 second in terms of latency.
</summary>
    <author>
      <name>Kai Li</name>
    </author>
    <author>
      <name>Wei Ni</name>
    </author>
    <author>
      <name>Eduardo Tovar</name>
    </author>
    <author>
      <name>Mohsen Guizani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, accepted in IEEE International Conference on
  Communications (ICC), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.06153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00922v3</id>
    <updated>2018-04-20T23:11:23Z</updated>
    <published>2018-03-02T15:54:52Z</published>
    <title>Online Scheduling of Spark Workloads with Mesos using Different Fair
  Allocation Algorithms</title>
    <summary>  In the following, we present example illustrative and experimental results
comparing fair schedulers allocating resources from multiple servers to
distributed application frameworks. Resources are allocated so that at least
one resource is exhausted in every server. Schedulers considered include DRF
(DRFH) and Best-Fit DRF (BF-DRF), TSF, and PS-DSF. We also consider server
selection under Randomized Round Robin (RRR) and based on their residual
(unreserved) resources. In the following, we consider cases with frameworks of
equal priority and without server-preference constraints. We first give typical
results of a illustrative numerical study and then give typical results of a
study involving Spark workloads on Mesos which we have modified and
open-sourced to prototype different schedulers.
</summary>
    <author>
      <name>Yuquan Shan</name>
    </author>
    <author>
      <name>Aman Jain</name>
    </author>
    <author>
      <name>George Kesidis</name>
    </author>
    <author>
      <name>Bhuvan Urgaonkar</name>
    </author>
    <author>
      <name>Jalal Khamse-Ashari</name>
    </author>
    <author>
      <name>Ioannis Lambadaris</name>
    </author>
    <link href="http://arxiv.org/abs/1803.00922v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00922v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.01618v1</id>
    <updated>2018-03-05T11:53:24Z</updated>
    <published>2018-03-05T11:53:24Z</published>
    <title>On the accuracy and usefulness of analytic energy models for
  contemporary multicore processors</title>
    <summary>  This paper presents refinements to the execution-cache-memory performance
model and a previously published power model for multicore processors. The
combination of both enables a very accurate prediction of performance and
energy consumption of contemporary multicore processors as a function of
relevant parameters such as number of active cores as well as core and Uncore
frequencies. Model validation is performed on the Sandy Bridge-EP and
Broadwell-EP microarchitectures. Production-related variations in chip quality
are demonstrated through a statistical analysis of the fit parameters obtained
on one hundred Broadwell-EP CPUs of the same model. Insights from the models
are used to explain the performance- and energy-related behavior of the
processors for scalable as well as saturating (i.e., memory-bound) codes. In
the process we demonstrate the models' capability to identify optimal operating
points with respect to highest performance, lowest energy-to-solution, and
lowest energy-delay product and identify a set of best practices for
energy-efficient execution.
</summary>
    <author>
      <name>Johannes Hofmann</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Dietmar Fey</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-92040-5_2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-92040-5_2" rel="related"/>
    <link href="http://arxiv.org/abs/1803.01618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.08121v1</id>
    <updated>2018-03-21T20:41:00Z</updated>
    <published>2018-03-21T20:41:00Z</published>
    <title>A Markov Chain Monte Carlo Approach to Cost Matrix Generation for
  Scheduling Performance Evaluation</title>
    <summary>  In high performance computing, scheduling of tasks and allocation to machines
is very critical especially when we are dealing with heterogeneous execution
costs. Simulations can be performed with a large variety of environments and
application models. However, this technique is sensitive to bias when it relies
on random instances with an uncontrolled distribution. We use methods from the
literature to provide formal guarantee on the distribution of the instance. In
particular, it is desirable to ensure a uniform distribution among the
instances with a given task and machine heterogeneity. In this article, we
propose a method that generates instances (cost matrices) with a known
distribution where tasks are scheduled on machines with heterogeneous execution
costs.
</summary>
    <author>
      <name>Louis-Claude Canon</name>
    </author>
    <author>
      <name>Mohamad El Sayah</name>
    </author>
    <author>
      <name>Pierre-Cyrille Héam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.08121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.08121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.08981v1</id>
    <updated>2018-03-23T21:08:36Z</updated>
    <published>2018-03-23T21:08:36Z</published>
    <title>Unix Memory Allocations are Not Poisson</title>
    <summary>  In multitasking operating systems, requests for free memory are traditionally
modeled as a stochastic counting process with independent,
exponentially-distributed interarrival times because of the analytic simplicity
such Poisson models afford. We analyze the distribution of several million unix
page commits to show that although this approach could be valid over relatively
long timespans, the behavior of the arrival process over shorter periods is
decidedly not Poisson. We find that this result holds regardless of the
originator of the request: unlike network packets, there is little difference
between system- and user-level page-request distributions. We believe this to
be due to the bursty nature of page allocations, which tend to occur in either
small or extremely large increments. Burstiness and persistent variance have
recently been found in self-similar processes in computer networks, but we show
that although page commits are both bursty and possess high variance over long
timescales, they are probably not self-similar. These results suggest that
altogether different models are needed for fine-grained analysis of memory
systems, an important consideration not only for understanding behavior but
also for the design of online control systems.
</summary>
    <author>
      <name>James Garnett</name>
    </author>
    <author>
      <name>Elizabeth Bradley</name>
    </author>
    <link href="http://arxiv.org/abs/1803.08981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.08981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01783v2</id>
    <updated>2018-04-07T18:08:16Z</updated>
    <published>2018-04-05T11:16:56Z</published>
    <title>Dynamic Load Balancing with Tokens</title>
    <summary>  Efficiently exploiting the resources of data centers is a complex task that
requires efficient and reliable load balancing and resource allocation
algorithms. The former are in charge of assigning jobs to servers upon their
arrival in the system, while the latter are responsible for sharing server
resources between their assigned jobs. These algorithms should take account of
various constraints, such as data locality, that restrict the feasible job
assignments. In this paper, we propose a token-based mechanism that efficiently
balances load between servers without requiring any knowledge on job arrival
rates and server capacities. Assuming a balanced fair sharing of the server
resources, we show that the resulting dynamic load balancing is insensitive to
the job size distribution. Its performance is compared to that obtained under
the best static load balancing and in an ideal system that would constantly
optimize the resource utilization.
</summary>
    <author>
      <name>Céline Comte</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23919/IFIPNetworking.2018.8697018</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23919/IFIPNetworking.2018.8697018" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2018 IFIP Networking Conference (IFIP Networking) and Workshops</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1804.01783v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01783v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01972v1</id>
    <updated>2018-04-05T17:48:29Z</updated>
    <published>2018-04-05T17:48:29Z</published>
    <title>A Survey of Miss-Ratio Curve Construction Techniques</title>
    <summary>  Miss-ratio curve (MRC), or equivalently hit-ratio curve (HRC), construction
techniques have recently gathered the attention of many researchers. Recent
advancements have allowed for approximating these curves in constant time,
allowing for online working-set-size (WSS) measurement. Techniques span the
algorithmic design paradigm from classic dynamic programming to artificial
intelligence inspired techniques. Our survey produces broad classification of
the current techniques primarily based on \emph{what} locality metric is being
recorded and \emph{how} that metric is stored for processing.
  Applications of theses curves span from dynamic cache partitioning in the
processor, to improving block allocation at the operating system level. Our
survey will give an overview of the historical, exact MRC construction methods,
and compare them with the state-of-the-art methods present in today's
literature. In addition, we will show where there are still open areas of
research and remain excited to see what this domain can produce with a strong
theoretical background.
</summary>
    <author>
      <name>Daniel Byrne</name>
    </author>
    <link href="http://arxiv.org/abs/1804.01972v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01972v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03965v1</id>
    <updated>2018-04-11T13:04:02Z</updated>
    <published>2018-04-11T13:04:02Z</published>
    <title>A Comparative Evaluation of Log-Based Process Performance Analysis
  Techniques</title>
    <summary>  Process mining has gained traction over the past decade and an impressive
body of research has resulted in the introduction of a variety of process
mining approaches measuring process performance. Having this set of techniques
available, organizations might find it difficult to identify which approach is
best suited considering context, performance indicator, and data availability.
In light of this challenge, this paper aims at introducing a framework for
categorizing and selecting performance analysis approaches based on existing
research. We start from a systematic literature review for identifying the
existing works discussing how to measure process performance based on
information retrieved from event logs. Then, the proposed framework is built
starting from the information retrieved from these studies taking into
consideration different aspects of performance analysis.
</summary>
    <author>
      <name>Fredrik Milani</name>
    </author>
    <author>
      <name>Fabrizio M. Maggi</name>
    </author>
    <link href="http://arxiv.org/abs/1804.03965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.05671v1</id>
    <updated>2018-04-12T17:54:46Z</updated>
    <published>2018-04-12T17:54:46Z</published>
    <title>Pliant: Leveraging Approximation to Improve Datacenter Resource
  Efficiency</title>
    <summary>  Cloud multi-tenancy is typically constrained to a single interactive service
colocated with one or more batch, low-priority services, whose performance can
be sacrificed when deemed necessary. Approximate computing applications offer
the opportunity to enable tighter colocation among multiple applications whose
performance is important. We present Pliant, a lightweight cloud runtime that
leverages the ability of approximate computing applications to tolerate some
loss in their output quality to boost the utilization of shared servers. During
periods of high resource contention, Pliant employs incremental and
interference-aware approximation to reduce contention in shared resources, and
prevent QoS violations for co-scheduled interactive, latency-critical services.
We evaluate Pliant across different interactive and approximate computing
applications, and show that it preserves QoS for all co-scheduled workloads,
while incurring a 2.1\% loss in output quality, on average.
</summary>
    <author>
      <name>Neeraj Kulkarni</name>
    </author>
    <author>
      <name>Feng Qi</name>
    </author>
    <author>
      <name>Christina Delimitrou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.05671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06139v2</id>
    <updated>2019-06-19T23:19:55Z</updated>
    <published>2018-04-17T09:52:54Z</published>
    <title>A General Formula for the Stationary Distribution of the Age of
  Information and Its Application to Single-Server Queues</title>
    <summary>  This paper considers the stationary distribution of the age of information
(AoI) in information update systems. We first derive a general formula for the
stationary distribution of the AoI, which holds for a wide class of information
update systems. The formula indicates that the stationary distribution of the
AoI is given in terms of the stationary distributions of the system delay and
the peak AoI. To demonstrate its applicability and usefulness, we analyze the
AoI in single-server queues with four different service disciplines: first-come
first-served (FCFS), preemptive last-come first-served (LCFS), and two variants
of non-preemptive LCFS service disciplines. For the FCFS and the preemptive
LCFS service disciplines, the GI/GI/1, M/GI/1, and GI/M/1 queues are
considered, and for the non-preemptive LCFS service disciplines, the M/GI/1 and
GI/M/1 queues are considered. With these results, we further show comparison
results for the mean AoI's in the M/GI/1 and GI/M/1 queues under those service
disciplines.
</summary>
    <author>
      <name>Yoshiaki Inoue</name>
    </author>
    <author>
      <name>Hiroyuki Masuyama</name>
    </author>
    <author>
      <name>Tetsuya Takine</name>
    </author>
    <author>
      <name>Toshiyuki Tanaka</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIT.2019.2938171</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIT.2019.2938171" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Transactions on Information Theory</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.06139v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06139v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.10590v1</id>
    <updated>2018-04-27T16:59:02Z</updated>
    <published>2018-04-27T16:59:02Z</published>
    <title>Queuing Theoretic Models for Multicast and Coded-Caching in Downlink
  Wireless Systems</title>
    <summary>  We consider a server connected to $L$ users over a shared finite capacity
link. Each user is equipped with a cache. File requests at the users are
generated as independent Poisson processes according to a popularity profile
from a library of $M$ files. The server has access to all the files in the
library. Users can store parts of the files or full files from the library in
their local caches. The server should send missing parts of the files requested
by the users. The server attempts to fulfill the pending requests with minimal
transmissions exploiting multicasting and coding opportunities among the
pending requests. We study the performance of this system in terms of queuing
delays for the naive multicasting and several coded multicasting schemes
proposed in the literature. We also provide approximate expressions for the
mean queuing delay for these models and establish their effectiveness with
simulations.
</summary>
    <author>
      <name>Mahadesh Panju</name>
    </author>
    <author>
      <name>Ramkumar Raghu</name>
    </author>
    <author>
      <name>Vinod Sharma</name>
    </author>
    <author>
      <name>Rajesh Ramachandran</name>
    </author>
    <link href="http://arxiv.org/abs/1804.10590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.10973v2</id>
    <updated>2018-09-30T14:37:35Z</updated>
    <published>2018-04-29T18:22:13Z</published>
    <title>A Basic Result on the Superposition of Arrival Processes in
  Deterministic Networks</title>
    <summary>  Time-Sensitive Networking (TSN) and Deterministic Networking (DetNet) are
emerging standards to enable deterministic, delay-critical communication in
such networks. This naturally (re-)calls attention to the network calculus
theory (NC), since a rich set of results for delay guarantee analysis have
already been developed there. One could anticipate an immediate adoption of
those existing network calculus results to TSN and DetNet. However, the
fundamental difference between the traffic specification adopted in TSN and
DetNet and those traffic models in NC makes this difficult, let alone that
there is a long-standing open challenge in NC. To address them, this paper
considers an arrival time function based max-plus NC traffic model. In
particular, for the former, the mapping between the TSN / DetNet and the NC
traffic model is proved. For the latter, the superposition property of the
arrival time function based NC traffic model is found and proved. Appealingly,
the proved superposition property shows a clear analogy with that of a
well-known counterpart traffic model in NC. These results help make an
important step towards the development of a system theory for delay guarantee
analysis of TSN / DetNet networks.
</summary>
    <author>
      <name>Yuming Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/1804.10973v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10973v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01360v1</id>
    <updated>2018-05-26T08:32:39Z</updated>
    <published>2018-05-26T08:32:39Z</published>
    <title>Evaluating Impact of Human Errors on the Availability of Data Storage
  Systems</title>
    <summary>  In this paper, we investigate the effect of incorrect disk replacement
service on the availability of data storage systems. To this end, we first
conduct Monte Carlo simulations to evaluate the availability of disk subsystem
by considering disk failures and incorrect disk replacement service. We also
propose a Markov model that corroborates the Monte Carlo simulation results. We
further extend the proposed model to consider the effect of automatic disk
fail-over policy. The results obtained by the proposed model show that
overlooking the impact of incorrect disk replacement can result up to three
orders of magnitude unavailability underestimation. Moreover, this study
suggests that by considering the effect of human errors, the conventional
believes about the dependability of different RAID mechanisms should be
revised. The results show that in the presence of human errors, RAID1 can
result in lower availability compared to RAID5.
</summary>
    <author>
      <name>Mostafa Kishani</name>
    </author>
    <author>
      <name>Reza Eftekhari</name>
    </author>
    <author>
      <name>Hossein Asadi</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01374v1</id>
    <updated>2018-06-04T20:28:01Z</updated>
    <published>2018-06-04T20:28:01Z</published>
    <title>Improving rewards in overloaded real-time systems</title>
    <summary>  Competitive analysis of online algorithms has commonly been applied to
understand the behaviour of real-time systems during overload conditions. While
competitive analysis provides insight into the behaviour of certain algorithms,
it is hard to make inferences about the performance of those algorithms in
practice. Other approaches to dealing with overload resort to heuristics that
seem to perform well but are hard to prove as being good. Further, most work on
handling overload in real-time systems does not consider using information
regarding the distribution of arrival rates of jobs and execution times to make
scheduling decisions. We present an scheduling policy (obtained through
stochastic approximation, and using information about the workload) to handle
overload in real-time systems and improve the revenue earned when each
successful job completion results in revenue accrual. We prove that the policy
we outline does lead to increased revenue when compared to a class of
scheduling policies that make static resource allocations to different service
classes. We also use empirical evidence to underscore the fact that this policy
performs better than a variety of other scheduling policies. The ideas
presented can be applied to several soft real-time systems, specifically
systems with multiple service classes.
</summary>
    <author>
      <name>Sathish Gopalakrishnan</name>
    </author>
    <link href="http://arxiv.org/abs/1806.01374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07300v1</id>
    <updated>2018-06-19T15:11:17Z</updated>
    <published>2018-06-19T15:11:17Z</published>
    <title>Forest Packing: Fast, Parallel Decision Forests</title>
    <summary>  Machine learning has an emerging critical role in high-performance computing
to modulate simulations, extract knowledge from massive data, and replace
numerical models with efficient approximations. Decision forests are a critical
tool because they provide insight into model operation that is critical to
interpreting learned results. While decision forests are trivially
parallelizable, the traversals of tree data structures incur many random memory
accesses and are very slow. We present memory packing techniques that
reorganize learned forests to minimize cache misses during classification. The
resulting layout is hierarchical. At low levels, we pack the nodes of multiple
trees into contiguous memory blocks so that each memory access fetches data for
multiple trees. At higher levels, we use leaf cardinality to identify the most
popular paths through a tree and collocate those paths in cache lines. We
extend this layout with out-of-order execution and cache-line prefetching to
increase memory throughput. Together, these optimizations increase the
performance of classification in ensembles by a factor of four over an
optimized C++ implementation and a actor of 50 over a popular R language
implementation.
</summary>
    <author>
      <name>James Browne</name>
    </author>
    <author>
      <name>Tyler M. Tomita</name>
    </author>
    <author>
      <name>Disa Mhembere</name>
    </author>
    <author>
      <name>Randal Burns</name>
    </author>
    <author>
      <name>Joshua T. Vogelstein</name>
    </author>
    <link href="http://arxiv.org/abs/1806.07300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08299v1</id>
    <updated>2018-06-21T15:50:20Z</updated>
    <published>2018-06-21T15:50:20Z</published>
    <title>Optimising finite-difference methods for PDEs through parameterised
  time-tiling in Devito</title>
    <summary>  Finite-difference methods are widely used in solving partial differential
equations. In a large problem set, approximations can take days or weeks to
evaluate, yet the bulk of computation may occur within a single loop nest. The
modelling process for researchers is not straightforward either, requiring
models with differential equations to be translated into stencil kernels, then
optimised separately. One tool that seeks to speed up and eliminate mistakes
from this tedious procedure is Devito, used to efficiently employ
finite-difference methods.
  In this work, we implement time-tiling, a loop nest optimisation, in Devito
yielding a decrease in runtime of up to 45%, and at least 20% across stencils
from the acoustic wave equation family, widely used in Devito's target domain
of seismic imaging. We present an estimator for arithmetic intensity under
time-tiling and a model to predict runtime improvements in stencil
computations. We also consider generalisation of time-tiling to imperfect loop
nests, a less widely studied problem.
</summary>
    <author>
      <name>Nicholas Sim</name>
    </author>
    <link href="http://arxiv.org/abs/1806.08299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00146v1</id>
    <updated>2018-06-30T09:03:39Z</updated>
    <published>2018-06-30T09:03:39Z</published>
    <title>Measuring and comparing the scaling behaviour of a high-performance CFD
  code on different supercomputing infrastructures</title>
    <summary>  Parallel code design is a challenging task especially when addressing
petascale systems for massive parallel processing (MPP), i.e. parallel
computations on several hundreds of thousands of cores. An in-house
computational fluid dynamics code, developed by our group, was designed for
such high-fidelity runs in order to exhibit excellent scalability values. Basis
for this code is an adaptive hierarchical data structure together with an
efficient communication and (numerical) computation scheme that supports MPP.
For a detailled scalability analysis, we performed several experiments on two
of Germany's national supercomputers up to 140,000 processes. In this paper, we
will show the results of those experiments and discuss any bottlenecks that
could be observed while solving engineering-based problems such as porous media
flows or thermal comfort assessments for problem sizes up to several hundred
billion degrees of freedom.
</summary>
    <author>
      <name>Jérôme Frisch</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">RWTH Aachen University, Aachen, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Ralf-Peter Mundani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technische Universität München, Munich, Germany</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SYNASC.2015.63</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SYNASC.2015.63" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 17th International Symposium on Symbolic and
  Numeric Algorithms for Scientific Computing (2015) 371-378</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.00146v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00146v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.3, B.8.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06534v1</id>
    <updated>2018-07-03T17:40:39Z</updated>
    <published>2018-07-03T17:40:39Z</published>
    <title>Design and optimisation of an efficient HDF5 I/O kernel for massive
  parallel fluid flow simulations</title>
    <summary>  More and more massive parallel codes running on several hundreds of thousands
of cores enter the computational science and engineering domain, allowing
high-fidelity computations on up to trillions of unknowns for very detailed
analyses of the underlying problems. During such runs, typically gigabytes of
data are being produced, hindering both efficient storage and (interactive)
data exploration. Here, advanced approaches based on inherently distributed
data formats such as HDF5 become necessary in order to avoid long latencies
when storing the data and to support fast (random) access when retrieving the
data for visual processing. Avoiding file locking and using collective
buffering, write bandwidths to a single file close to the theoretical peak on a
modern supercomputing cluster were achieved. The structure of the output file
supports a very fast interactive visualisation and introduces additional
steering functionality.
</summary>
    <author>
      <name>Christoph Ertl</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technische Universität München, Munich, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Frisch</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">RWTH Aachen University, Aachen, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Ralf-Peter Mundani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technische Universität München, Munich, Germany</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/cpe.4165</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/cpe.4165" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Concurrency and Computation: Practice and Experience 29 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.06534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; E.1; D.1.3; I.6.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08585v1</id>
    <updated>2018-07-20T11:39:12Z</updated>
    <published>2018-07-20T11:39:12Z</published>
    <title>A refined mean field approximation of synchronous discrete-time
  population models</title>
    <summary>  Mean field approximation is a popular method to study the behaviour of
stochastic models composed of a large number of interacting objects. When the
objects are asynchronous, the mean field approximation of a population model
can be expressed as an ordinary differential equation. When the objects are
(clock-) synchronous the mean field approximation is a discrete time dynamical
system. We focus on the latter.We study the accuracy of mean field
approximation when this approximation is a discrete-time dynamical system. We
extend a result that was shown for the continuous time case and we prove that
expected performance indicators estimated by mean field approximation are
$O(1/N)$-accurate. We provide simple expressions to effectively compute the
asymptotic error of mean field approximation, for finite time-horizon and
steady-state, and we use this computed error to propose what we call a
\emph{refined} mean field approximation. We show, by using a few numerical
examples, that this technique improves the quality of approximation compared to
the classical mean field approximation, especially for relatively small
population sizes.
</summary>
    <author>
      <name>Nicolas Gast</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">POLARIS</arxiv:affiliation>
    </author>
    <author>
      <name>Diego Latella</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISTI</arxiv:affiliation>
    </author>
    <author>
      <name>Mieke Massink</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISTI</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.peva.2018.05.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.peva.2018.05.002" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Performance Evaluation, Elsevier, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.08585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08586v1</id>
    <updated>2018-07-19T21:59:19Z</updated>
    <published>2018-07-19T21:59:19Z</published>
    <title>A Queuing Model for CPU Functional Unit and Issue Queue Configuration</title>
    <summary>  In a superscalar processor, instructions of various types flow through an
execution pipeline, traversing hardware resources which are mostly shared among
many different instruction types. A notable exception to shared pipeline
resources is the collection of functional units, the hardware that performs
specific computations. In a trade-off of cost versus performance, a pipeline
designer must decide how many of each type of functional unit to place in a
processor's pipeline. In this paper, we model a superscalar processor's issue
queue and functional units as a novel queuing network. We treat the issue queue
as a finite-sized waiting area and the functional units as servers. In addition
to common queuing problems, customers of the network share the queue but wait
for specific servers to become ready (e.g., addition instructions wait for
adders). Furthermore, the customers in this queue are not necessary ready for
service, since instructions may be waiting for operands. In this paper we model
a novel queuing network that provides a solution to the expected queue length
of each type of instruction. This network and its solution can also be
generalized to other problems, notably other resource-allocation issues that
arise in superscalar pipelines.
</summary>
    <author>
      <name>Shane Carroll</name>
    </author>
    <author>
      <name>Wei-Ming Ling</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08586v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08586v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09313v1</id>
    <updated>2018-07-24T19:22:25Z</updated>
    <published>2018-07-24T19:22:25Z</published>
    <title>Time-efficient Garbage Collection in SSDs</title>
    <summary>  SSDs are currently replacing magnetic disks in many application areas. A
challenge of the underlying flash technology is that data cannot be updated
in-place. A block consisting of many pages must be completely erased before a
single page can be rewritten. This victim block can still contain valid pages
which need to be copied to other blocks before erasure. The objective of
garbage collection strategies is to minimize write amplification induced by
copying valid pages from victim blocks while minimizing the performance
overhead of the victim selection. Victim selection strategies minimizing write
amplification, like the cost-benefit approach, have linear runtime, while the
write amplifications of time-efficient strategies, like the greedy strategy,
significantly reduce the lifetime of SSDs. In this paper, we propose two
strategies which optimize the performance of cost-benefit, while (almost)
preserving its write amplification. Trace-driven simulations for single- and
multi-channel SSDs show that the optimizations help to keep the write
amplification low while improving the runtime by up to 24-times compared to the
original cost-benefit strategy, so that the new strategies can be used in
multi-TByte SSDs.
</summary>
    <author>
      <name>Lars Nagel</name>
    </author>
    <author>
      <name>Tim Süß</name>
    </author>
    <author>
      <name>Kevin Kremer</name>
    </author>
    <author>
      <name>M. Umar Hameed</name>
    </author>
    <author>
      <name>Lingfang Zeng</name>
    </author>
    <author>
      <name>André Brinkmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages (excluding references), 13 pages (including references), 13
  figures, 1 algorithm</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02120v2</id>
    <updated>2021-01-12T02:13:59Z</updated>
    <published>2018-08-06T21:17:14Z</published>
    <title>Heavy-Traffic Insensitive Bounds for Weighted Proportionally Fair
  Bandwidth Sharing Policies</title>
    <summary>  We consider a connection-level model proposed by Massouli\'{e} and Roberts
for bandwidth sharing among file transfer flows in a communication network. We
study weighted proportionally fair sharing policies and establish explicit-form
bounds on the weighted sum of the expected numbers of flows on different routes
in heavy traffic. The bounds are linear in the number of critically loaded
links in the network, and they hold for a class of phase-type file-size
distributions; i.e., the bounds are heavy-traffic insensitive to the
distributions in this class. Our approach is Lyapunov-drift based, which is
different from the widely used diffusion approximation approach. A key
technique we develop is to construct a novel inner product in the state space,
which then allows us to obtain a multiplicative type of state-space collapse in
steady state. Furthermore, this state-space collapse result implies the
interchange of limits as a by-product for the diffusion approximation of the
equal-weight case under phase-type file-size distributions, demonstrating the
heavy-traffic insensitivity of the stationary distribution.
</summary>
    <author>
      <name>Weina Wang</name>
    </author>
    <author>
      <name>Siva Theja Maguluri</name>
    </author>
    <author>
      <name>R. Srikant</name>
    </author>
    <author>
      <name>Lei Ying</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02120v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02120v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03518v1</id>
    <updated>2018-08-01T15:11:47Z</updated>
    <published>2018-08-01T15:11:47Z</published>
    <title>MARS: Memory Aware Reordered Source</title>
    <summary>  Memory bandwidth is critical in today's high performance computing systems.
The bandwidth is particularly paramount for GPU workloads such as 3D Gaming,
Imaging and Perceptual Computing, GPGPU due to their data-intensive nature. As
the number of threads and data streams in the GPUs increases with each
generation, along with a high available memory bandwidth, memory efficiency is
also crucial in order to achieve desired performance. In presence of multiple
concurrent data streams, the inherent locality in a single data stream is often
lost as these streams are interleaved while moving through multiple levels of
memory system. In DRAM based main memory, the poor request locality reduces
row-buffer reuse resulting in underutilized and inefficient memory bandwidth.
  In this paper we propose Memory-Aware Reordered Source (\textit{MARS})
architecture to address memory inefficiency arising from highly interleaved
data streams. The key idea of \textit{MARS} is that with a sufficiently large
lookahead before the main memory, data streams can be reordered based on their
row-buffer address to regain the lost locality and improve memory efficiency.
We show that \textit{MARS} improves achieved memory bandwidth by 11\% for a set
of synthetic microbenchmarks. Moreover, MARS does so without any specific
knowledge of the memory configuration.
</summary>
    <author>
      <name>Ishwar Bhati</name>
    </author>
    <author>
      <name>Udit Dhawan</name>
    </author>
    <author>
      <name>Jayesh Gaur</name>
    </author>
    <author>
      <name>Sreenivas Subramoney</name>
    </author>
    <author>
      <name>Hong Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06074v1</id>
    <updated>2018-08-18T11:39:54Z</updated>
    <published>2018-08-18T11:39:54Z</published>
    <title>Compiler Enhanced Scheduling for OpenMP for Heterogeneous
  Multiprocessors</title>
    <summary>  Scheduling in Asymmetric Multicore Processors (AMP), a special case of
Heterogeneous Multiprocessors, is a widely studied topic. The scheduling
techniques which are mostly runtime do not usually consider parallel
programming pattern used in parallel programming frameworks like OpenMP. On the
other hand, current compilers for these parallel programming platforms are
hardware oblivious which prevent any compile-time optimization for platforms
like big.LITTLE and has to completely rely on runtime optimization. In this
paper, we propose a hardware-aware Compiler Enhanced Scheduling (CES) where the
common compiler transformations are coupled with compiler added scheduling
commands to take advantage of the hardware asymmetry and improve the runtime
efficiency. We implement a compiler for OpenMP and demonstrate its efficiency
in Samsung Exynos with big.LITTLE architecture. On an average, we see 18%
reduction in runtime and 14% reduction in energy consumption in standard NPB
and FSU benchmarks with CES across multiple frequencies and core configurations
in big.LITTLE.
</summary>
    <author>
      <name>Jyothi Krishna V S</name>
    </author>
    <author>
      <name>Shankar Balachandran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages, 4 figures, Presented in 2nd EEHCO (Energy Efficiency with
  Heterogenous Computing) Workshop in Prague 2016 (Part of HiPEAC event 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08650v1</id>
    <updated>2018-08-27T01:19:44Z</updated>
    <published>2018-08-27T01:19:44Z</published>
    <title>Persistent Stochastic Non-Interference</title>
    <summary>  In this paper we present an information flow security property for
stochastic, cooperating, processes expressed as terms of the Performance
Evaluation Process Algebra (PEPA). We introduce the notion of Persistent
Stochastic Non-Interference (PSNI) based on the idea that every state reachable
by a process satisfies a basic Stochastic Non-Interference (SNI) property. The
structural operational semantics of PEPA allows us to give two
characterizations of PSNI: the first involves a single bisimulation-like
equivalence check, while the second is formulated in terms of unwinding
conditions. The observation equivalence at the base of our definition relies on
the notion of lumpability and ensures that, for a secure process P, the steady
state probability of observing the system being in a specific state P' is
independent from its possible high level interactions.
</summary>
    <author>
      <name>Jane Hillston</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Edinburgh, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Carla Piazza</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Università di Udine, Italy</arxiv:affiliation>
    </author>
    <author>
      <name>Sabina Rossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Università Ca` Foscari Venezia, Italy</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.276.6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.276.6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings EXPRESS/SOS 2018, arXiv:1808.08071</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 276, 2018, pp. 53-68</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.08650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04509v1</id>
    <updated>2018-10-10T13:22:47Z</updated>
    <published>2018-10-10T13:22:47Z</published>
    <title>LIRS: Enabling efficient machine learning on NVM-based storage via a
  lightweight implementation of random shuffling</title>
    <summary>  Machine learning algorithms, such as Support Vector Machine (SVM) and Deep
Neural Network (DNN), have gained a lot of interests recently. When training a
machine learning algorithm, randomly shuffle all the training data can improve
the testing accuracy and boost the convergence rate. Nevertheless, realizing
training data random shuffling in a real system is not a straightforward
process due to the slow random accesses in hard disk drive (HDD). To avoid
frequent random disk access, the effect of random shuffling is often limited in
existing approaches. With the emerging non-volatile memory-based storage
device, such as Intel Optane SSD, which provides fast random accesses, we
propose a lightweight implementation of random shuffling (LIRS) to randomly
shuffle the indexes of the entire training dataset, and the selected training
instances are directly accessed from the storage and packed into batches.
Experimental results show that LIRS can reduce the total training time of SVM
and DNN by 49.9% and 43.5% on average, and improve the final testing accuracy
on DNN by 1.01%.
</summary>
    <author>
      <name>Zhi-Lin Ke</name>
    </author>
    <author>
      <name>Hsiang-Yun Cheng</name>
    </author>
    <author>
      <name>Chia-Lin Yang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.6342/NTU201803514</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.6342/NTU201803514" rel="related"/>
    <link href="http://arxiv.org/abs/1810.04509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.11772v2</id>
    <updated>2019-02-26T00:20:20Z</updated>
    <published>2018-10-28T07:19:23Z</published>
    <title>Learning with Analytical Models</title>
    <summary>  To understand and predict the performance of scientific applications, several
analytical and machine learning approaches have been proposed, each having its
advantages and disadvantages. In this paper, we propose and validate a hybrid
approach for performance modeling and prediction, which combines analytical and
machine learning models. The proposed hybrid model aims to minimize prediction
cost while providing reasonable prediction accuracy. Our validation results
show that the hybrid model is able to learn and correct the analytical models
to better match the actual performance. Furthermore, the proposed hybrid model
improves the prediction accuracy in comparison to pure machine learning
techniques while using small training datasets, thus making it suitable for
hardware and workload changes.
</summary>
    <author>
      <name>Huda Ibeid</name>
    </author>
    <author>
      <name>Siping Meng</name>
    </author>
    <author>
      <name>Oliver Dobon</name>
    </author>
    <author>
      <name>Luke Olson</name>
    </author>
    <author>
      <name>William Gropp</name>
    </author>
    <link href="http://arxiv.org/abs/1810.11772v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.11772v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.12092v1</id>
    <updated>2018-10-29T13:16:39Z</updated>
    <published>2018-10-29T13:16:39Z</published>
    <title>On Coding for Reliable VNF Chaining in DCNs</title>
    <summary>  We study how erasure coding can improve service reliability in Data Center
Networks (DCN). To this end, we find that coding can be best deployed in
systems, where i) traffic is split into multiple parallel sub-flows, ii) each
sub-flow is encoded; iii) SFC along with their corresponding Virtual Network
Functions (VNF) concatenated are replicated into at least as many VNF instances
as there are sub-flows, resulting in parallel sub- SFCs; and iv) all coded
sub-flows are distributed over parallel paths and processed in parallel. We
study service reliability as function of the level of parallelization within
DCN and the resulting amount of redundancy. Based on the probability theory and
by considering failures of path segments, VNF and server failures, we
analytically derive the probability that parallel subflows are successfully
processed by the parallelized SFC and that the original serial traffic can be
successfully recovered without service interruptions.We compare the proposed
failure protection with coding and the standard backup protection and evaluate
the related overhead of both methods, including decoding, traffic redirection
and VNF migration. The results not only show the benefit of our scheme for
reliability, but also a reduced overhead required in comparison to backup
protection.
</summary>
    <author>
      <name>Anna Engelmann</name>
    </author>
    <author>
      <name>Admela Jukan</name>
    </author>
    <author>
      <name>Rastin Pries</name>
    </author>
    <link href="http://arxiv.org/abs/1810.12092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.12092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.01412v2</id>
    <updated>2018-11-20T16:53:49Z</updated>
    <published>2018-11-04T18:18:27Z</published>
    <title>Measuring Software Performance on Linux</title>
    <summary>  Measuring and analyzing the performance of software has reached a high
complexity, caused by more advanced processor designs and the intricate
interaction between user programs, the operating system, and the processor's
microarchitecture. In this report, we summarize our experience about how
performance characteristics of software should be measured when running on a
Linux operating system and a modern processor. In particular, (1) We provide a
general overview about hardware and operating system features that may have a
significant impact on timing and how they interact, (2) we identify sources of
errors that need to be controlled in order to obtain unbiased measurement
results, and (3) we propose a measurement setup for Linux to minimize errors.
Although not the focus of this report, we describe the measurement process
using hardware performance counters, which can faithfully reflect the real
bottlenecks on a given processor. Our experiments confirm that our measurement
setup has a large impact on the results. More surprisingly, however, they also
suggest that the setup can be negligible for certain analysis methods.
Furthermore, we found that our setup maintains significantly better performance
under background load conditions, which means it can be used to improve
software in high-performance applications.
</summary>
    <author>
      <name>Martin Becker</name>
    </author>
    <author>
      <name>Samarjit Chakraborty</name>
    </author>
    <link href="http://arxiv.org/abs/1811.01412v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.01412v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.01611v1</id>
    <updated>2018-11-05T11:04:08Z</updated>
    <published>2018-11-05T11:04:08Z</published>
    <title>Stabilizing the virtual response time in single-server processor sharing
  queues with slowly time-varying arrival rates</title>
    <summary>  Motivated by the work of Whitt, who studied stabilization of the mean virtual
waiting time (excluding service time) in a $GI_t/GI_t/1/FCFS$ queue, this paper
investigates the stabilization of the mean virtual response time in a
single-server processor sharing (PS) queueing system with a time-varying
arrival rate and a service rate control (a $GI_t/GI_t/1/PS$ queue). We propose
and compare a modified square-root (SR) control and a difference-matching (DM)
control to stabilize the mean virtual response time of a $GI_t/GI_t/1/PS$
queue. Extensive simulation studies with various settings of arrival processes
and service times show that the DM control outperforms the SR control for
heavy-traffic conditions, and that the SR control performs better for
light-traffic conditions.
</summary>
    <author>
      <name>Yongkyu Cho</name>
    </author>
    <author>
      <name>Young Myoung Ko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10479-019-03511-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10479-019-03511-9" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Annals of Operations Research, 293 (2020), 27-55</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.01611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.01611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.02287v1</id>
    <updated>2018-11-06T11:10:38Z</updated>
    <published>2018-11-06T11:10:38Z</published>
    <title>Defining Big Data Analytics Benchmarks for Next Generation
  Supercomputers</title>
    <summary>  The design and construction of high performance computing (HPC) systems
relies on exhaustive performance analysis and benchmarking. Traditionally this
activity has been geared exclusively towards simulation scientists, who,
unsurprisingly, have been the primary customers of HPC for decades. However,
there is a large and growing volume of data science work that requires these
large scale resources, and as such the calls for inclusion and investments in
data for HPC have been increasing. So when designing a next generation HPC
platform, it is necessary to have HPC-amenable big data analytics benchmarks.
In this paper, we propose a set of big data analytics benchmarks and sample
codes designed for testing the capabilities of current and next generation
supercomputers.
</summary>
    <author>
      <name>Drew Schmidt</name>
    </author>
    <author>
      <name>Junqi Yin</name>
    </author>
    <author>
      <name>Michael Matheson</name>
    </author>
    <author>
      <name>Bronson Messer</name>
    </author>
    <author>
      <name>Mallikarjun Shankar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.02287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.02287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.05239v2</id>
    <updated>2019-04-17T09:12:23Z</updated>
    <published>2018-11-13T12:07:04Z</published>
    <title>Global attraction of ODE-based mean field models with hyperexponential
  job sizes</title>
    <summary>  Mean field modeling is a popular approach to assess the performance of large
scale computer systems. The evolution of many mean field models is
characterized by a set of ordinary differential equations that have a unique
fixed point. In order to prove that this unique fixed point corresponds to the
limit of the stationary measures of the finite systems, the unique fixed point
must be a global attractor. While global attraction was established for various
systems in case of exponential job sizes, it is often unclear whether these
proof techniques can be generalized to non-exponential job sizes. In this paper
we show how simple monotonicity arguments can be used to prove global
attraction for a broad class of ordinary differential equations that capture
the evolution of mean field models with hyperexponential job sizes. This class
includes both existing as well as previously unstudied load balancing schemes
and can be used for systems with either finite or infinite buffers. The main
novelty of the approach exists in using a Coxian representation for the
hyperexponential job sizes and a partial order that is stronger than the
componentwise partial order used in the exponential case.
</summary>
    <author>
      <name>Benny Van Houdt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper was accepted at ACM Sigmetrics 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.05239v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.05239v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.09736v2</id>
    <updated>2019-11-23T22:34:29Z</updated>
    <published>2018-11-24T01:17:51Z</published>
    <title>Accelerating Reduction and Scan Using Tensor Core Units</title>
    <summary>  Driven by deep learning, there has been a surge of specialized processors for
matrix multiplication, referred to as TensorCore Units (TCUs). These TCUs are
capable of performing matrix multiplications on small matrices (usually 4x4 or
16x16) to accelerate the convolutional and recurrent neural networks in deep
learning workloads. In this paper we leverage NVIDIA's TCU to express both
reduction and scan with matrix multiplication and show the benefits -- in terms
of program simplicity, efficiency, and performance. Our algorithm exercises the
NVIDIA TCUs which would otherwise be idle, achieves 89%-98% of peak memory copy
bandwidth, and is orders of magnitude faster (up to 100x for reduction and 3x
for scan) than state-of-the-art methods for small segment sizes -- common in
machine learning and scientific applications. Our algorithm achieves this while
decreasing the power consumption by up to 22% for reduction and16%for scan.
</summary>
    <author>
      <name>Abdul Dakkak</name>
    </author>
    <author>
      <name>Cheng Li</name>
    </author>
    <author>
      <name>Isaac Gelado</name>
    </author>
    <author>
      <name>Jinjun Xiong</name>
    </author>
    <author>
      <name>Wen-mei Hwu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3330345.3331057</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3330345.3331057" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACM International Conference on Supercomputing
  (ICS '19)</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.09736v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.09736v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.10220v1</id>
    <updated>2018-11-26T07:49:13Z</updated>
    <published>2018-11-26T07:49:13Z</published>
    <title>Evaluation of Intel Memory Drive Technology Performance for Scientific
  Applications</title>
    <summary>  In this paper, we present benchmark data for Intel Memory Drive Technology
(IMDT), which is a new generation of Software-defined Memory (SDM) based on
Intel ScaleMP collaboration and using 3D XPointTM based Intel Solid-State
Drives (SSDs) called Optane. We studied IMDT performance for synthetic
benchmarks, scientific kernels, and applications. We chose these benchmarks to
represent different patterns for computation and accessing data on disks and
memory. To put performance of IMDT in comparison, we used two memory
configurations: hybrid IMDT DDR4/Optane and DDR4 only systems. The performance
was measured as a percentage of used memory and analyzed in detail. We found
that for some applications DDR4/Optane hybrid configuration outperforms DDR4
setup by up to 20%.
</summary>
    <author>
      <name>Vladimir Mironov</name>
    </author>
    <author>
      <name>Andrey Kudryavtsev</name>
    </author>
    <author>
      <name>Yuri Alexeev</name>
    </author>
    <author>
      <name>Alexander Moskovsky</name>
    </author>
    <author>
      <name>Igor Kulikov</name>
    </author>
    <author>
      <name>Igor Chernykh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3286475.3286479</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3286475.3286479" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the Workshop on Memory Centric High Performance
  Computing (MCHPC'18). ACM, New York, NY, USA, 14-21, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.10220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.10220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.11475v1</id>
    <updated>2018-11-28T10:07:34Z</updated>
    <published>2018-11-28T10:07:34Z</published>
    <title>The L-CSC cluster: Optimizing power efficiency to become the greenest
  supercomputer in the world in the Green500 list of November 2014</title>
    <summary>  The L-CSC (Lattice Computer for Scientific Computing) is a general purpose
compute cluster built with commodity hardware installed at GSI. Its main
operational purpose is Lattice QCD (LQCD) calculations for physics simulations.
Quantum Chromo Dynamics (QCD) is the physical theory describing the strong
force, one of the four known fundamental interactions in the universe. L-CSC
leverages a multi-GPU design accommodating the huge demand of LQCD for memory
bandwidth. In recent years, heterogeneous clusters with accelerators such as
GPUs have become more and more powerful while supercomputers in general have
shown enormous increases in power consumption making electricity costs and
cooling a significant factor in the total cost of ownership. Using mainly GPUs
for processing, L-CSC is very power-efficient, and its architecture was
optimized to provide the greatest possible power efficiency. This paper
presents the cluster design as well as optimizations to improve the power
efficiency. It examines the power measurements performed for the Green500 list
of the most power-efficient supercomputers in the world which led to the number
1 position as the greenest supercomputer in November 2014.
</summary>
    <author>
      <name>David Rohr</name>
    </author>
    <author>
      <name>Gvozden Neskovic</name>
    </author>
    <author>
      <name>Volker Lindenstruth</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14529/jsfi150304</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14529/jsfi150304" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Supercomputing frontiers and innovations, vol. 2 , no. 3, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.11475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.11475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.03393v3</id>
    <updated>2020-12-02T18:02:31Z</updated>
    <published>2019-01-04T21:16:56Z</published>
    <title>Star sampling with and without replacement</title>
    <summary>  Star sampling (SS) is a random sampling procedure on a graph wherein each
sample consists of a randomly selected vertex (the star center) and its one-hop
neighbors (the star endpoints). We consider the use of star sampling to find
any member of an arbitrary target set of vertices in a graph, where the figure
of merit (cost) is either the expected number of samples (unit cost) or the
expected number of star centers plus star endpoints (linear cost) until a
vertex in the target set is encountered, either as a star center or as a star
point. We analyze this performance measure on three related star sampling
paradigms: SS with replacement (SSR), SS without center replacement (SSC), and
SS without star replacement (SSS). We derive exact and approximate expressions
for the expected unit and linear costs of SSR, SSC, and SSS on Erdos-Renyi (ER)
graphs. Our results show there is i) little difference in unit cost, but ii)
significant difference in linear cost, across the three paradigms. Although our
results are derived for ER graphs, experiments on "real-world" graphs suggest
our performance expressions are reasonably accurate for non-ER graphs.
</summary>
    <author>
      <name>Jonathan Stokes</name>
    </author>
    <author>
      <name>Steven Weber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Superseded by arXiv:1910.00431</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.03393v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.03393v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.08627v1</id>
    <updated>2019-01-23T14:49:52Z</updated>
    <published>2019-01-23T14:49:52Z</published>
    <title>High order concentrated non-negative matrix-exponential functions</title>
    <summary>  Highly concentrated functions play an important role in many research fields
including control system analysis and physics, and they turned out to be the
key idea behind inverse Laplace transform methods as well.
  This paper uses the matrix-exponential family of functions to create highly
concentrated functions, whose squared coefficient of variation (SCV) is very
low. In the field of stochastic modeling, matrix-exponential functions have
been used for decades. They have many advantages: they are easy to manipulate,
always non-negative, and integrals involving matrix-exponential functions often
have closed-form solutions. For the time being there is no symbolic
construction available to obtain the most concentrated matrix-exponential
functions, and the numerical optimization-based approach has many pitfalls,
too.
  In this paper, we present a numerical optimization-based procedure to
construct highly concentrated matrix-exponential functions. To make the
objective function explicit and easy to evaluate we introduce and use a new
representation called hyper-trigonometric representation. This representation
makes it possible to achieve very low SCV.
</summary>
    <author>
      <name>Gabor Horvath</name>
    </author>
    <author>
      <name>Illes Horvath</name>
    </author>
    <author>
      <name>Miklos Telek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for publication</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.08627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.08627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.09842v2</id>
    <updated>2019-12-31T02:41:36Z</updated>
    <published>2019-01-28T17:36:06Z</published>
    <title>Overbooking Microservices in the Cloud</title>
    <summary>  We consider the problem of scheduling serverless-computing instances such as
Amazon Lambda functions, or scheduling microservices within (privately held)
virtual machines (VMs). Instead of a quota per tenant/customer, we assume
demand for Lambda functions is modulated by token-bucket mechanisms per tenant.
Such quotas are due to, e.g., limited resources (as in a fog/edge-cloud
context) or to prevent excessive unauthorized invocation of numerous instances
by malware. Based on an upper bound on the stationary number of active "Lambda
servers" considering the execution-time distribution of Lambda functions, we
describe an approach that the cloud could use to overbook Lambda functions for
improved utilization of IT resources. An earlier bound for a single service
tier is extended to multiple service tiers. For the context of scheduling
microservices in a private setting, the framework could be used to determine
the required VM resources for a token-bucket constrained workload stream.
Finally, we note that the looser Markov inequality may be useful in settings
where the job service times are dependent.
</summary>
    <author>
      <name>George Kesidis</name>
    </author>
    <link href="http://arxiv.org/abs/1901.09842v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.09842v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.04566v1</id>
    <updated>2019-02-12T06:55:35Z</updated>
    <published>2019-02-12T06:55:35Z</published>
    <title>Finite Horizon Throughput Maximization for a Wirelessly Powered Device
  over a Time Varying Channel</title>
    <summary>  In this work, we consider an energy harvesting device (EHD) served by an
access point with a single antenna that is used for both wireless power
transfer (WPT) and data transfer. The objective is to maximize the expected
throughput of the EHD over a finite horizon when the channel state information
is only available causally. The EHD is energized by WPT for a certain duration,
which is subject to optimization, and then, EHD transmits its information bits
to the AP until the end of the time horizon by employing optimal dynamic power
allocation. The joint optimization problem is modeled as a dynamic programming
problem. Based on the characteristic of the problem, we prove that a time
dependent threshold type structure exists for the optimal WPT duration, and we
obtain closed form solution to the dynamic power allocation in the uplink
period.
</summary>
    <author>
      <name>Mehdi Salehi Heydar Abad</name>
    </author>
    <author>
      <name>Ozgur Ercetin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1804.01834</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.04566v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.04566v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.04568v1</id>
    <updated>2019-02-12T06:55:53Z</updated>
    <published>2019-02-12T06:55:53Z</published>
    <title>Wireless energy and information transfer in networks with hybrid ARQ</title>
    <summary>  In this paper, we consider a class of wireless powered communication devices
using hybrid automatic repeat request (HARQ) protocol to ensure reliable
communications. In particular, we analyze the trade-off between accumulating
mutual information and harvesting RF energy at the receiver of a point-to-point
link over a time-varying independent and identically distributed (i.i.d.)
channel. The transmitter is assumed to have a constant energy source while the
receiver relies, solely, on the RF energy harvested from the received signal.
At each time slot, the incoming RF signal is split between information
accumulation and energy accumulation with the objective of minimizing the
expected number of re-transmissions. A major finding of this work is that the
optimal policy minimizing the expected number of re-transmissions utilizes the
incoming RF signal to either exclusively harvest energy or to accumulate mutual
information. This finding enables achieving an optimal solution in feasible
time by converting a two dimensional uncountable state Markov decision process
(MDP) with continuous action space into a countable state MDP with binary
decision space.
</summary>
    <author>
      <name>Mehdi Salehi Heydar Abad</name>
    </author>
    <author>
      <name>Ozgur Ercetin</name>
    </author>
    <author>
      <name>Tamer Elbatt</name>
    </author>
    <author>
      <name>Mohammed Nafie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1711.02878</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2018 IEEE Wireless Communications and Networking Conference (WCNC)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1902.04568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.04568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.04890v1</id>
    <updated>2019-02-12T06:55:41Z</updated>
    <published>2019-02-12T06:55:41Z</published>
    <title>Energy harvesting wireless networks with correlated energy sources</title>
    <summary>  This work considers a system with two energy harvesting (EH) nodes
transmitting to a common destination over a random access channel. The amount
of harvested energy is assumed to be random and independent over time, but
correlated among the nodes possibly with respect to their relative position. A
threshold-based transmission policy is developed for the maximization of the
expected aggregate network throughput. Assuming that there is no a priori
channel state or EH information available to the nodes, the aggregate network
throughput is obtained. The optimal thresholds are determined for two
practically important special cases: i) at any time only one of the sensors
harvests energy due to, for example, physical separation of the nodes; ii) the
nodes are spatially close, and at any time, either both nodes or none of them
harvests energy.
</summary>
    <author>
      <name>Mehdi Salehi Heydar Abad</name>
    </author>
    <author>
      <name>Deniz Gunduz</name>
    </author>
    <author>
      <name>Ozgur Ercetin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2016 IEEE Wireless Communications and Networking Conference</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1902.04890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.04890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.00771v1</id>
    <updated>2019-03-02T21:36:35Z</updated>
    <published>2019-03-02T21:36:35Z</published>
    <title>Reliable Access to Massive Restricted Texts: Experience-based Evaluation</title>
    <summary>  Libraries are seeing growing numbers of digitized textual corpora that
frequently come with restrictions on their content. Computational analysis
corpora that are large, while of interest to scholars, can be cumbersome
because of the combination of size, granularity of access, and access
restrictions. Efficient management of such a collection for general access
especially under failures depends on the primary storage system. In this paper,
we identify the requirements of managing for computational analysis a massive
text corpus and use it as basis to evaluate candidate storage solutions. The
study based on the 5.9 billion page collection of the HathiTrust digital
library. Our findings led to the choice of Cassandra 3.x for the primary back
end store, which is currently in deployment in the HathiTrust Research Center.
</summary>
    <author>
      <name>Zong Peng</name>
    </author>
    <author>
      <name>Beth Plale</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">a preprint version of submission to CCPE special issue</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.00771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.00771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.09346v2</id>
    <updated>2020-11-20T20:27:47Z</updated>
    <published>2019-03-22T03:42:48Z</published>
    <title>heSRPT: Optimal Parallel Scheduling of Jobs With Known Sizes</title>
    <summary>  When parallelizing a set of jobs across many servers, one must balance a
trade-off between granting priority to short jobs and maintaining the overall
efficiency of the system. When the goal is to minimize the mean flow time of a
set of jobs, it is usually the case that one wants to complete short jobs
before long jobs. However, since jobs usually cannot be parallelized with
perfect efficiency, granting strict priority to the short jobs can result in
very low system efficiency which in turn hurts the mean flow time across jobs.
In this paper, we derive the optimal policy for allocating servers to jobs at
every moment in time in order to minimize mean flow time across jobs. We assume
that jobs follow a sublinear, concave speedup function, and hence jobs
experience diminishing returns from being allocated additional servers. We show
that the optimal policy, heSRPT, will complete jobs according to their size
order, but maintains overall system efficiency by allocating some servers to
each job at every moment in time. We compare heSRPT with state-of-the-art
allocation policies from the literature and show that heSRPT outperforms its
competitors by at least 30%, and often by much more.
</summary>
    <author>
      <name>Benjamin Berg</name>
    </author>
    <author>
      <name>Rein Vesilo</name>
    </author>
    <author>
      <name>Mor Harchol-Balter</name>
    </author>
    <link href="http://arxiv.org/abs/1903.09346v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.09346v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.10171v1</id>
    <updated>2019-03-25T08:22:27Z</updated>
    <published>2019-03-25T08:22:27Z</published>
    <title>Effect of payload size on goodput when message segmentations occur for
  wireless networks: Case of packet corruptions recovered by stop-and-wait
  protocol</title>
    <summary>  This paper investigates the effect of payload size on goodput for wireless
networks where packets created from a message through a segmentation function
are lost due to bit errors and they are recovered by a stop-and-wait protocol.
To achieve this, we derive the exact analytical form of goodput using the
analytical form of a packet-size distribution, given a message-size
distribution and a payload size. In previous work, the packet sizes are assumed
to be constant, which are payload size plus header size, although actual
segmented packets are not constant in size. Hence, this constant packet-size
assumption may be not justified for goodput analysis. From numerical results,
we show that the constant packet-size assumption is not justified under low
bit-error rates. Furthermore, we indicate that the curves of goodput are
concave in payload size under high bit-error rates. In addition, we show that
the larger mean bit-error burst length yields less concave curves of goodput.
</summary>
    <author>
      <name>Takashi Ikegawa</name>
    </author>
    <link href="http://arxiv.org/abs/1903.10171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.10171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.05347v1</id>
    <updated>2019-04-10T17:58:23Z</updated>
    <published>2019-04-10T17:58:23Z</published>
    <title>Cross-Platform Performance Portability Using Highly Parametrized SYCL
  Kernels</title>
    <summary>  Over recent years heterogeneous systems have become more prevalent across HPC
systems, with over 100 supercomputers in the TOP500 incorporating GPUs or other
accelerators. These hardware platforms have different performance
characteristics and optimization requirements. In order to make the most of
multiple accelerators a developer has to provide implementations of their
algorithms tuned for each device. Hardware vendors provide libraries targeting
their devices specifically, which provide good performance but frequently have
different API designs, hampering portability.
  The SYCL programming model allows users to write heterogeneous programs using
completely standard C++, and so developers have access to the power of C++
templates when developing compute kernels. In this paper we show that by
writing highly parameterized kernels for matrix multiplies and convolutions we
achieve performance competitive with vendor implementations across different
architectures. Furthermore, tuning for new devices amounts to choosing the
combinations of kernel parameters that perform best on the hardware.
</summary>
    <author>
      <name>John Lawson</name>
    </author>
    <author>
      <name>Mehdi Goli</name>
    </author>
    <author>
      <name>Duncan McBain</name>
    </author>
    <author>
      <name>Daniel Soutar</name>
    </author>
    <author>
      <name>Louis Sugy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.05347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.05615v1</id>
    <updated>2019-04-11T10:32:02Z</updated>
    <published>2019-04-11T10:32:02Z</published>
    <title>A Processor-Sharing model for the Performance of Virtualized Network
  Functions</title>
    <summary>  The parallel execution of requests in a Cloud Computing platform, as for
Virtualized Network Functions, is modeled by an $M^{[X]}/M/1$ Processor-Sharing
(PS) system, where each request is seen as a batch of unit jobs. The
performance of such paralleled system can then be measured by the quantiles of
the batch sojourn time distribution. In this paper, we address the evaluation
of this distribution for the $M^{[X]}/M/1$-PS queue with batch arrivals and
geometrically distributed batch size. General results on the residual busy
period (after a tagged batch arrival time) and the number of unit jobs served
during this residual busy period are first derived. This enables us to provide
an approximation for the distribution tail of the batch sojourn time whose
accuracy is confirmed by simulation.
</summary>
    <author>
      <name>Fabrice Guillemin</name>
    </author>
    <author>
      <name>Veronica Quintuna Rodriguez</name>
    </author>
    <author>
      <name>Alain Simonian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for publication</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.05615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.05654v1</id>
    <updated>2019-04-11T12:11:13Z</updated>
    <published>2019-04-11T12:11:13Z</published>
    <title>On the sojourn of an arbitrary customer in an $M/M/1$ Processor Sharing
  Queue</title>
    <summary>  In this paper, we consider the number of both arrivals and departures seen by
a tagged customer while in service in a classical $M/M/1$ processor sharing
queue. By exploiting the underlying orthogonal structure of this queuing system
revealed in an earlier study, we compute the distributions of these two
quantities and prove that they are equal in distribution. We moreover derive
the asymptotic behavior of this common distribution. The knowledge of the
number of departures seen by a tagged customer allows us to test the validity
of an approximation, which consists of assuming that the tagged customer is
randomly served among those customers in the residual busy period of the queue
following the arrival of the tagged customer. A numerical evidence shows that
this approximation is reasonable for moderate values of the number of
departures, given that the asymptotic behaviors of the distributions are very
different even if the exponential decay rates are equal.
</summary>
    <author>
      <name>Fabrice Guillemin</name>
    </author>
    <author>
      <name>Veronica Quintuna Rodriguez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for publication</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.05654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.05924v2</id>
    <updated>2020-03-07T21:30:00Z</updated>
    <published>2019-04-11T18:55:55Z</published>
    <title>The distribution of age-of-information performance measures for message
  processing systems</title>
    <summary>  The idea behind the recently introduced "age of information" performance
measure of a networked message processing system is that it indicates our
knowledge regarding the "freshness" of the most recent piece of information
that can be used as a criterion for real-time control. In this foundational
paper, we examine two such measures, one that has been extensively studied in
the recent literature and a new one that could be more relevant from the point
of view of the processor. Considering these measures as stochastic processes in
a stationary environment (defined by the arrival processes, message processing
times and admission controls in bufferless systems), we characterize their
distributions using the Palm inversion formula. Under renewal assumptions we
derive explicit solutions for their Laplace transforms and show some
interesting decomposition properties. Previous work has mostly focused on
computation of expectations in very particular cases. We argue that using
bufferless or very small buffer systems is best and support this by simulation.
We also pose some open problems including assessment of enqueueing policies
that may be better in cases where one wishes to minimize more general
functionals of the age of information measures.
</summary>
    <author>
      <name>George Kesidis</name>
    </author>
    <author>
      <name>Takis Konstantopoulos</name>
    </author>
    <author>
      <name>Michael Zazanis</name>
    </author>
    <link href="http://arxiv.org/abs/1904.05924v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05924v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.06480v3</id>
    <updated>2021-12-23T07:34:31Z</updated>
    <published>2019-04-13T04:40:12Z</published>
    <title>Dynamic scheduling in a partially fluid, partially lossy queueing system</title>
    <summary>  We consider a single server queueing system with two classes of jobs: eager
jobs with small sizes that require service to begin almost immediately upon
arrival, and tolerant jobs with larger sizes that can wait for service. While
blocking probability is the relevant performance metric for the eager class,
the tolerant class seeks to minimize its mean sojourn time. In this paper, we
discuss the performance of each class under dynamic scheduling policies, where
the scheduling of both classes depends on the instantaneous state of the
system. This analysis is carried out under a certain fluid limit, where the
arrival rate and service rate of the eager class are scaled to infinity,
holding the offered load constant. Our performance characterizations reveal a
(dynamic) pseudo-conservation law that ties the performance of both the classes
to the standalone blocking probabilities of the eager class. Further, the
performance is robust to other specifics of the scheduling policies. We also
characterize the Pareto frontier of the achievable region of performance
vectors under the same fluid limit, and identify a (two-parameter) class of
Pareto-complete scheduling policies.
</summary>
    <author>
      <name>Kiran Chaudhary</name>
    </author>
    <author>
      <name>Veeraruna Kavitha</name>
    </author>
    <author>
      <name>Jayakrishnan Nair</name>
    </author>
    <link href="http://arxiv.org/abs/1904.06480v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.06480v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.06825v1</id>
    <updated>2019-04-15T03:13:13Z</updated>
    <published>2019-04-15T03:13:13Z</published>
    <title>Performance Models for Data Transfers: A Case Study with Molecular
  Chemistry Kernels</title>
    <summary>  With increasing complexity of hardwares, systems with different memory nodes
are ubiquitous in High Performance Computing (HPC). It is paramount to develop
strategies to overlap the data transfers between memory nodes with computations
in order to exploit the full potential of these systems. In this article, we
consider the problem of deciding the order of data transfers between two memory
nodes for a set of independent tasks with the objective to minimize the
makespan. We prove that with limited memory capacity, obtaining the optimal
order of data transfers is a NP-complete problem. We propose several heuristics
for this problem and provide details about their favorable situations. We
present an analysis of our heuristics on traces, obtained by running 2
molecular chemistry kernels, namely, Hartree-Fock (HF) and Coupled Cluster
Single Double (CCSD) on 10 nodes of an HPC system. Our results show that some
of our heuristics achieve significant overlap for moderate memory capacities
and are very close to the lower bound of makespan.
</summary>
    <author>
      <name>Suraj Kumar</name>
    </author>
    <author>
      <name>Lionel Eyraud-Dubois</name>
    </author>
    <author>
      <name>Sriram Krishnamoorthy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3337821.3337921</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3337821.3337921" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">https://www.hpcs.cs.tsukuba.ac.jp/icpp2019/</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.06825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.06825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.03439v1</id>
    <updated>2019-05-09T04:21:09Z</updated>
    <published>2019-05-09T04:21:09Z</published>
    <title>Load Balancing Guardrails: Keeping Your Heavy Traffic on the Road to Low
  Response Times</title>
    <summary>  Load balancing systems, comprising a central dispatcher and a scheduling
policy at each server, are widely used in practice, and their response time has
been extensively studied in the theoretical literature. While much is known
about the scenario where the scheduling at the servers is
First-Come-First-Served (FCFS), to minimize mean response time we must use
Shortest-Remaining-Processing-Time (SRPT) scheduling at the servers. Much less
is known about dispatching polices when SRPT scheduling is used. Unfortunately,
traditional dispatching policies that are used in practice in systems with FCFS
servers often have poor performance in systems with SRPT servers. In this
paper, we devise a simple fix that can be applied to any dispatching policy.
This fix, called guardrails, ensures that the dispatching policy yields optimal
mean response time under heavy traffic when used in a system with SRPT servers.
Any dispatching policy, when augmented with guardrails, becomes heavy-traffic
optimal. Our results yield the first analytical bounds on mean response time
for load balancing systems with SRPT scheduling at the servers.
</summary>
    <author>
      <name>Isaac Grosof</name>
    </author>
    <author>
      <name>Ziv Scully</name>
    </author>
    <author>
      <name>Mor Harchol-Balter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages. To appear in ACM SIGMETRICS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.03439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.03439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.07641v2</id>
    <updated>2020-07-23T15:39:14Z</updated>
    <published>2019-05-18T20:16:02Z</published>
    <title>On a caching system with object sharing</title>
    <summary>  We consider a content-caching system thatis shared by a number of proxies.
The cache could belocated in an edge-cloud datacenter and the proxies couldeach
serve a large population of mobile end-users. Eachproxy operates its own
LRU-list of a certain capacity inthe shared cache. The length of objects
simultaneouslyappearing in plural LRU-lists is equally divided amongthem,i.e.,
object sharing among the LRUs. We provide a "working-set" approximation for
this system to quicklyestimate the cache-hit probabilities under such
objectsharing, which can be used to facilitate admission control.Also, a way to
reduce ripple evictions,i.e.,setrequestoverhead, is suggested. We give
numerical results for ourMemCacheD with Object Sharing (MCD-OS) prototype.
</summary>
    <author>
      <name>George Kesidis</name>
    </author>
    <author>
      <name>Nader Alfares</name>
    </author>
    <author>
      <name>Xi Li</name>
    </author>
    <author>
      <name>Bhuvan Urgaonkar</name>
    </author>
    <author>
      <name>Mahmut Kandemir</name>
    </author>
    <author>
      <name>Takis Konstantopoulos</name>
    </author>
    <link href="http://arxiv.org/abs/1905.07641v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.07641v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.08764v2</id>
    <updated>2019-10-01T20:59:22Z</updated>
    <published>2019-05-21T17:33:19Z</published>
    <title>Performance Analysis of Deep Learning Workloads on Leading-edge Systems</title>
    <summary>  This work examines the performance of leading-edge systems designed for
machine learning computing, including the NVIDIA DGX-2, Amazon Web Services
(AWS) P3, IBM Power System Accelerated Compute Server AC922, and a
consumer-grade Exxact TensorEX TS4 GPU server. Representative deep learning
workloads from the fields of computer vision and natural language processing
are the focus of the analysis. Performance analysis is performed along with a
number of important dimensions. Performance of the communication interconnects
and large and high-throughput deep learning models are considered. Different
potential use models for the systems as standalone and in the cloud also are
examined. The effect of various optimization of the deep learning models and
system configurations is included in the analysis.
</summary>
    <author>
      <name>Yihui Ren</name>
    </author>
    <author>
      <name>Shinjae Yoo</name>
    </author>
    <author>
      <name>Adolfy Hoisie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.08764v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08764v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.8; C.1.2; I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.11012v1</id>
    <updated>2019-05-27T07:25:52Z</updated>
    <published>2019-05-27T07:25:52Z</published>
    <title>The Impact of GPU DVFS on the Energy and Performance of Deep Learning:
  an Empirical Study</title>
    <summary>  Over the past years, great progress has been made in improving the computing
power of general-purpose graphics processing units (GPGPUs), which facilitates
the prosperity of deep neural networks (DNNs) in multiple fields like computer
vision and natural language processing. A typical DNN training process
repeatedly updates tens of millions of parameters, which not only requires huge
computing resources but also consumes significant energy. In order to train
DNNs in a more energy-efficient way, we empirically investigate the impact of
GPU Dynamic Voltage and Frequency Scaling (DVFS) on the energy consumption and
performance of deep learning. Our experiments cover a wide range of GPU
architectures, DVFS settings, and DNN configurations. We observe that, compared
to the default core frequency settings of three tested GPUs, the optimal core
frequency can help conserve 8.7%$\sim$23.1% energy consumption for different
DNN training cases. Regarding the inference, the benefits vary from
19.6%$\sim$26.4%. Our findings suggest that GPU DVFS has great potentials to
help develop energy efficient DNN training/inference schemes.
</summary>
    <author>
      <name>Zhenheng Tang</name>
    </author>
    <author>
      <name>Yuxin Wang</name>
    </author>
    <author>
      <name>Qiang Wang</name>
    </author>
    <author>
      <name>Xiaowen Chu</name>
    </author>
    <link href="http://arxiv.org/abs/1905.11012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.11012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.11707v1</id>
    <updated>2019-05-28T09:41:17Z</updated>
    <published>2019-05-28T09:41:17Z</published>
    <title>Function-as-a-Service Benchmarking Framework</title>
    <summary>  Cloud Service Providers deliver their products in form of 'as-a-Service',
which are typically categorized by the level of abstraction. This approach
hides the implementation details and shows only functionality to the user.
However, the problem is that it is hard to measure the performance of Cloud
services, because they behave like black boxes. Especially with
Function-as-a-Service it is even more difficult because it completely hides
server and infrastructure management from users by design. Cloud Service
Prodivers usually restrict the maximum size of code, memory and runtime of
Cloud Functions. Nevertheless, users need clarification if more ressources are
needed to deliver services in high quality. In this regard, we present the
architectural design of a new Function-as-a-Service benchmarking tool, which
allows users to evaluate the performance of Cloud Functions. Furthermore, the
capabilities of the framework are tested on an isolated platform with a
specific workload. The results show that users are able to get insights into
Function-as-a-Service environments. This, in turn, allows users to identify
factors which may slow down or speed up the performance of Cloud Functions.
</summary>
    <author>
      <name>Roland Pellegrini</name>
    </author>
    <author>
      <name>Igor Ivkic</name>
    </author>
    <author>
      <name>Markus Tauber</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5220/0007757304790487</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5220/0007757304790487" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">https://www.scitepress.org/PublicationsDetail.aspx?ID=Df3810DssWw=&amp;t=1</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.11707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.11707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.02894v1</id>
    <updated>2019-07-05T15:30:04Z</updated>
    <published>2019-07-05T15:30:04Z</published>
    <title>RegDem: Increasing GPU Performance via Shared Memory Register Spilling</title>
    <summary>  GPU utilization, measured as occupancy, is limited by the parallel threads'
combined usage of on-chip resources, such as registers and the
programmer-managed shared memory. Higher resource demand means lower effective
parallel thread count, and therefore lower program performance. Our
investigation found that registers are often the occupancy limiters.
  The de-facto nvcc compiler-based approach spills excessive registers to the
off-chip memory, ignoring the shared memory and leaving the on-chip resources
underutilized. To mitigate the register demand, this paper presents a binary
translation technique, called RegDem, that spills excessive registers to the
underutilized shared memory by transforming the GPU assembly code (SASS). Most
GPU programs do not fully use shared memory, thus allowing RegDem to use it for
register spilling. The higher occupancy achieved by RegDem outweighs the
slightly higher cost of accessing shared memory instead of placing data in
registers. The paper also presents a compile-time performance predictor that
models instructions stalls to choose the best version from a set of program
variants. Cumulatively, these techniques outperform the nvcc compiler with a 9%
geometric mean, the highest observed being 18%.
</summary>
    <author>
      <name>Putt Sakdhnagool</name>
    </author>
    <author>
      <name>Amit Sabne</name>
    </author>
    <author>
      <name>Rudolf Eigenmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.02894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.02894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.08302v1</id>
    <updated>2019-07-18T21:51:29Z</updated>
    <published>2019-07-18T21:51:29Z</published>
    <title>Quantitative Impact Evaluation of an Abstraction Layer for Data Stream
  Processing Systems</title>
    <summary>  With the demand to process ever-growing data volumes, a variety of new data
stream processing frameworks have been developed. Moving an implementation from
one such system to another, e.g., for performance reasons, requires adapting
existing applications to new interfaces. Apache Beam addresses these high
substitution costs by providing an abstraction layer that enables executing
programs on any of the supported streaming frameworks. In this paper, we
present a novel benchmark architecture for comparing the performance impact of
using Apache Beam on three streaming frameworks: Apache Spark Streaming, Apache
Flink, and Apache Apex. We find significant performance penalties when using
Apache Beam for application development in the surveyed systems. Overall, usage
of Apache Beam for the examined streaming applications caused a high variance
of query execution times with a slowdown of up to a factor of 58 compared to
queries developed without the abstraction layer. All developed benchmark
artifacts are publicly available to ensure reproducible results.
</summary>
    <author>
      <name>Guenter Hesse</name>
    </author>
    <author>
      <name>Christoph Matthies</name>
    </author>
    <author>
      <name>Kelvin Glass</name>
    </author>
    <author>
      <name>Johannes Huegle</name>
    </author>
    <author>
      <name>Matthias Uflacker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICDCS.2019.00137</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICDCS.2019.00137" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2019 International Conference on Distributed Computing Systems
  (ICDCS), pp. 1381-1392</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1907.08302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.08302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.09049v2</id>
    <updated>2020-05-05T13:21:43Z</updated>
    <published>2019-07-21T23:03:42Z</published>
    <title>Multiple Server SRPT with speed scaling is competitive</title>
    <summary>  Can the popular shortest remaining processing time (SRPT) algorithm achieve a
constant competitive ratio on multiple servers when server speeds are
adjustable (speed scaling) with respect to the flow time plus energy
consumption metric? This question has remained open for a while, where a
negative result in the absence of speed scaling is well known. The main result
of this paper is to show that multi-server SRPT can be constant competitive,
with a competitive ratio that only depends on the power-usage function of the
servers, but not on the number of jobs/servers or the job sizes (unlike when
speed scaling is not allowed). When all job sizes are unity, we show that
round-robin routing is optimal and can achieve the same competitive ratio as
the best known algorithm for the single server problem. Finally, we show that a
class of greedy dispatch policies, including policies that route to the least
loaded or the shortest queue, do not admit a constant competitive ratio. When
job arrivals are stochastic, with Poisson arrivals and i.i.d. job sizes, we
show that random routing and a simple gated-static speed scaling algorithm
achieves a constant competitive ratio.
</summary>
    <author>
      <name>Rahul Vaze</name>
    </author>
    <author>
      <name>Jayakrishnan Nair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEEE/ACM Transactions on Networking</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.09049v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.09049v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.11603v1</id>
    <updated>2019-07-26T14:47:10Z</updated>
    <published>2019-07-26T14:47:10Z</published>
    <title>Anonymity Mixes as (Partial) Assembly Queues: Modeling and Analysis</title>
    <summary>  Anonymity platforms route the traffic over a network of special routers that
are known as mixes and implement various traffic disruption techniques to hide
the communicating users' identities. Batch mixes in particular anonymize
communicating peers by allowing message exchange to take place only after a
sufficient number of messages (a batch) accumulate, thus introducing delay. We
introduce a queueing model for batch mix and study its delay properties. Our
analysis shows that delay of a batch mix grows quickly as the batch size gets
close to the number of senders connected to the mix. We then propose a
randomized batch mixing strategy and show that it achieves much better delay
scaling in terms of the batch size. However, randomization is shown to reduce
the anonymity preserving capabilities of the mix. We also observe that queueing
models are particularly useful to study anonymity metrics that are more
practically relevant such as the time-to-deanonymize metric.
</summary>
    <author>
      <name>Mehmet Fatih Aktas</name>
    </author>
    <author>
      <name>Emina Soljanin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Information Theory Workshop, 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.11603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.11603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.12666v1</id>
    <updated>2019-07-29T21:52:06Z</updated>
    <published>2019-07-29T21:52:06Z</published>
    <title>Modeling Shared Cache Performance of OpenMP Programs using Reuse
  Distance</title>
    <summary>  Performance modeling of parallel applications on multicore computers remains
a challenge in computational co-design due to the complex design of multicore
processors including private and shared memory hierarchies. We present a
Scalable Analytical Shared Memory Model to predict the performance of parallel
applications that runs on a multicore computer and shares the same level of
cache in the hierarchy. This model uses a computationally efficient,
probabilistic method to predict the reuse distance profiles, where reuse
distance is a hardware architecture-independent measure of the patterns of
virtual memory accesses. It relies on a stochastic, static basic block-level
analysis of reuse profiles measured from the memory traces of applications ran
sequentially on small instances rather than using a multi-threaded trace. The
results indicate that the hit-rate predictions on the shared cache are
accurate.
</summary>
    <author>
      <name>Atanu Barai</name>
    </author>
    <author>
      <name>Gopinath Chennupati</name>
    </author>
    <author>
      <name>Nandakishore Santhi</name>
    </author>
    <author>
      <name>Abdel-Hameed A. Badawy</name>
    </author>
    <author>
      <name>Stephan Eidenbenz</name>
    </author>
    <link href="http://arxiv.org/abs/1907.12666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.12666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01924v1</id>
    <updated>2019-08-06T01:34:51Z</updated>
    <published>2019-08-06T01:34:51Z</published>
    <title>Edge AIBench: Towards Comprehensive End-to-end Edge Computing
  Benchmarking</title>
    <summary>  In edge computing scenarios, the distribution of data and collaboration of
workloads on different layers are serious concerns for performance, privacy,
and security issues. So for edge computing benchmarking, we must take an
end-to-end view, considering all three layers: client-side devices, edge
computing layer, and cloud servers. Unfortunately, the previous work ignores
this most important point. This paper presents the BenchCouncil's coordinated e
ort on edge AI benchmarks, named Edge AIBench. In total, Edge AIBench models
four typical application scenarios: ICU Patient Monitor, Surveillance Camera,
Smart Home, and Autonomous Vehicle with the focus on data distribution and
workload collaboration on three layers. Edge AIBench is a part of the
open-source AIBench project, publicly available from
http://www.benchcouncil.org/AIBench/index.html. We also build an edge computing
testbed with a federated learning framework to resolve performance, privacy,
and security issues.
</summary>
    <author>
      <name>Tianshu Hao</name>
    </author>
    <author>
      <name>Yunyou Huang</name>
    </author>
    <author>
      <name>Xu Wen</name>
    </author>
    <author>
      <name>Wanling Gao</name>
    </author>
    <author>
      <name>Fan Zhang</name>
    </author>
    <author>
      <name>Chen Zheng</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Hainan Ye</name>
    </author>
    <author>
      <name>Kai Hwang</name>
    </author>
    <author>
      <name>Zujie Ren</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <link href="http://arxiv.org/abs/1908.01924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.02415v2</id>
    <updated>2019-10-04T19:11:51Z</updated>
    <published>2019-08-07T01:42:14Z</published>
    <title>Redundancy Scheduling in Systems with Bi-Modal Job Service Time
  Distribution</title>
    <summary>  Queuing systems with redundant requests have drawn great attention because of
their promise to reduce the job completion time and variability. Despite a
large body of work on the topic, we are still far from fully understanding the
benefits of redundancy in practice. We here take one step towards practical
systems by studying queuing systems with bi-modal job service time
distribution. Such distributions have been observed in practice, as can be seen
in, e.g., Google cluster traces. We develop an analogy to a classical urns and
balls problem, and use it to study the queuing time performance of two
non-adaptive classical scheduling policies: random and round-robin. We
introduce new performance indicators in the analogous model, and argue that
they are good predictors of the queuing time in non-adaptive scheduling
policies. We then propose a non-adaptive scheduling policy that is based on
combinatorial designs, and show that it has better performance indicators.
Simulations confirm that the proposed scheduling policy, as the performance
indicators suggest, reduces the queuing times compared to random and
round-robin scheduling.
</summary>
    <author>
      <name>Amir Behrouzi-Far</name>
    </author>
    <author>
      <name>Emina Soljanin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Allerton 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.02415v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02415v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.02702v1</id>
    <updated>2019-08-07T16:19:15Z</updated>
    <published>2019-08-07T16:19:15Z</published>
    <title>Performance Comparison for Neuroscience Application Benchmarks</title>
    <summary>  Researchers within the Human Brain Project and related projects have in the
last couple of years expanded their needs for high-performance computing
infrastructures. The needs arise from a diverse set of science challenges that
range from large-scale simulations of brain models to processing of
extreme-scale experimental data sets. The ICEI project, which is in the process
of creating a distributed infrastructure optimised for brain research, started
to build-up a set of benchmarks that reflect the diversity of applications in
this field. In this paper we analyse the performance of some selected
benchmarks on an IBM POWER8 and Intel Skylake based systems with and without
GPUs.
</summary>
    <author>
      <name>Andreas Herten</name>
    </author>
    <author>
      <name>Thorsten Hater</name>
    </author>
    <author>
      <name>Wouter Klijn</name>
    </author>
    <author>
      <name>Dirk Pleiter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-34356-9_31</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-34356-9_31" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at ISC19 Conference Workshop IWOPH (International Workshop
  on OpenPOWER for HPC)</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.02702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.08123v4</id>
    <updated>2020-03-23T20:39:38Z</updated>
    <published>2019-08-21T21:35:03Z</published>
    <title>Computing System Congestion Management Using Exponential Smoothing
  Forecasting</title>
    <summary>  An overloaded computer must finish what it starts and not start what will
fail or hang. A congestion management algorithm the author developed, and
Siemens Corporation patented for telecom products, effectively manages traffic
overload with its unique formulation of Exponential Smoothing forecasting.
Siemens filed for exclusive rights to this technique in 2003 and obtained US
patent US7301903B2 in 2007 with this author, an employee at the time of the
filing, the sole inventor. A computer program, written in C language, which
exercises the methodology is listed at the end of this document and available
on GitHub.
</summary>
    <author>
      <name>James F Brady</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 figures, 20 pages including computer program listing v2 - clarified
  some notation v3 - added C program GitHub location V4 - minor wording cleanup</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.08123v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.08123v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.11338v1</id>
    <updated>2019-08-29T16:42:52Z</updated>
    <published>2019-08-29T16:42:52Z</published>
    <title>TapirXLA: Embedding Fork-Join Parallelism into the XLA Compiler in
  TensorFlow Using Tapir</title>
    <summary>  This work introduces TapirXLA, a replacement for TensorFlow's XLA compiler
that embeds recursive fork-join parallelism into XLA's low-level representation
of code. Machine-learning applications rely on efficient parallel processing to
achieve performance, and they employ a variety of technologies to improve
performance, including compiler technology. But compilers in machine-learning
frameworks lack a deep understanding of parallelism, causing them to lose
performance by missing optimizations on parallel computation. This work studies
how Tapir, a compiler intermediate representation (IR) that embeds parallelism
into a mainstream compiler IR, can be incorporated into a compiler for machine
learning to remedy this problem. TapirXLA modifies the XLA compiler in
TensorFlow to employ the Tapir/LLVM compiler to optimize low-level parallel
computation. TapirXLA encodes the parallelism within high-level TensorFlow
operations using Tapir's representation of fork-join parallelism. TapirXLA also
exposes to the compiler implementations of linear-algebra library routines
whose parallel operations are encoded using Tapir's representation. We compared
the performance of TensorFlow using TapirXLA against TensorFlow using an
unmodified XLA compiler. On four neural-network benchmarks, TapirXLA speeds up
the parallel running time of the network by a geometric-mean multiplicative
factor of 30% to 100%, across four CPU architectures.
</summary>
    <author>
      <name>Tao B. Schardl</name>
    </author>
    <author>
      <name>Siddharth Samsi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HPEC43674.2020.9286232</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HPEC43674.2020.9286232" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE HPEC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.11338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.11338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.00214v2</id>
    <updated>2019-10-21T13:39:14Z</updated>
    <published>2019-10-01T06:18:27Z</published>
    <title>Automatic Throughput and Critical Path Analysis of x86 and ARM Assembly
  Kernels</title>
    <summary>  Useful models of loop kernel runtimes on out-of-order architectures require
an analysis of the in-core performance behavior of instructions and their
dependencies. While an instruction throughput prediction sets a lower bound to
the kernel runtime, the critical path defines an upper bound. Such predictions
are an essential part of analytic (i.e., white-box) performance models like the
Roofline and Execution-Cache-Memory (ECM) models. They enable a better
understanding of the performance-relevant interactions between hardware
architecture and loop code. The Open Source Architecture Code Analyzer (OSACA)
is a static analysis tool for predicting the execution time of sequential
loops. It previously supported only x86 (Intel and AMD) architectures and
simple, optimistic full-throughput execution. We have heavily extended OSACA to
support ARM instructions and critical path prediction including the detection
of loop-carried dependencies, which turns it into a versatile
cross-architecture modeling tool. We show runtime predictions for code on Intel
Cascade Lake, AMD Zen, and Marvell ThunderX2 micro-architectures based on
machine models from available documentation and semi-automatic benchmarking.
The predictions are compared with actual measurements.
</summary>
    <author>
      <name>Jan Laukemann</name>
    </author>
    <author>
      <name>Julian Hammer</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/PMBS49563.2019.00006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/PMBS49563.2019.00006" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.00214v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.00214v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.00651v1</id>
    <updated>2019-09-30T07:28:47Z</updated>
    <published>2019-09-30T07:28:47Z</published>
    <title>Memory Centric Characterization and Analysis of SPEC CPU2017 Suite</title>
    <summary>  In this paper we provide a comprehensive, memory-centric characterization of
the SPEC CPU2017 benchmark suite, using a number of mechanisms including
dynamic binary instrumentation, measurements on native hardware using hardware
performance counters and OS based tools.
  We present a number of results including working set sizes, memory capacity
consumption and, memory bandwidth utilization of various workloads. Our
experiments reveal that the SPEC CPU2017 workloads are surprisingly memory
intensive, with approximately 50% of all dynamic instructions being memory
intensive ones. We also show that there is a large variation in the memory
footprint and bandwidth utilization profiles of the entire suite, with some
benchmarks using as much as 16 GB of main memory and up to 2.3 GB/s of memory
bandwidth.
  We also perform instruction execution and distribution analysis of the suite
and find that the average instruction count for SPEC CPU2017 workloads is an
order of magnitude higher than SPEC CPU2006 ones. In addition, we also find
that FP benchmarks of the SPEC 2017 suite have higher compute requirements: on
average, FP workloads execute three times the number of compute operations as
compared to INT workloads.
</summary>
    <author>
      <name>Sarabjeet Singh</name>
    </author>
    <author>
      <name>Manu Awasthi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3297663.3310311</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3297663.3310311" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 133 figures, A short version of this work has been
  published at "Proceedings of the 2019 ACM/SPEC International Conference on
  Performance Engineering"</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.00651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.00651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02516v1</id>
    <updated>2019-10-06T20:21:16Z</updated>
    <published>2019-10-06T20:21:16Z</published>
    <title>Optimising energy and overhead for large parameter space simulations</title>
    <summary>  Many systems require optimisation over multiple objectives, where objectives
are characteristics of the system such as energy consumed or increase in time
to perform the work. Optimisation is performed by selecting the `best' set of
input parameters to elicit the desired objectives. However, the parameter
search space can often be far larger than can be searched in a reasonable time.
Additionally, the objectives are often mutually exclusive -- leading to a
decision being made as to which objective is more important or optimising over
a combination of the objectives. This work is an application of a Genetic
Algorithm to identify the Pareto frontier for finding the optimal parameter
sets for all combinations of objectives. A Pareto frontier can be used to
identify the sets of optimal parameters for which each is the `best' for a
given combination of objectives -- thus allowing decisions to be made with full
knowledge. We demonstrate this approach for the HTC-Sim simulation system in
the case where a Reinforcement Learning scheduler is tuned for the two
objectives of energy consumption and task overhead. Demonstrating that this
approach can reduce the energy consumed by ~36% over previously published work
without significantly increasing the overhead.
</summary>
    <author>
      <name>Alexander J. M. Kell</name>
    </author>
    <author>
      <name>Matthew Forshaw</name>
    </author>
    <author>
      <name>A. Stephen McGough</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for IGSC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.02516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.06458v1</id>
    <updated>2019-10-14T23:05:34Z</updated>
    <published>2019-10-14T23:05:34Z</published>
    <title>TCD-NPE: A Re-configurable and Efficient Neural Processing Engine,
  Powered by Novel Temporal-Carry-deferring MACs</title>
    <summary>  In this paper, we first propose the design of Temporal-Carry-deferring MAC
(TCD-MAC) and illustrate how our proposed solution can gain significant energy
and performance benefit when utilized to process a stream of input data. We
then propose using the TCD-MAC to build a reconfigurable, high speed, and low
power Neural Processing Engine (TCD-NPE). We, further, propose a novel
scheduler that lists the sequence of needed processing events to process an MLP
model in the least number of computational rounds in our proposed TCD-NPE. We
illustrate that our proposed TCD-NPE significantly outperform similar neural
processing solutions that use conventional MACs in terms of both energy
consumption and execution time.
</summary>
    <author>
      <name>Ali Mirzaeian</name>
    </author>
    <author>
      <name>Houman Homayoun</name>
    </author>
    <author>
      <name>Avesta Sasan</name>
    </author>
    <link href="http://arxiv.org/abs/1910.06458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.06663v1</id>
    <updated>2019-10-15T11:31:36Z</updated>
    <published>2019-10-15T11:31:36Z</published>
    <title>AI Benchmark: All About Deep Learning on Smartphones in 2019</title>
    <summary>  The performance of mobile AI accelerators has been evolving rapidly in the
past two years, nearly doubling with each new generation of SoCs. The current
4th generation of mobile NPUs is already approaching the results of
CUDA-compatible Nvidia graphics cards presented not long ago, which together
with the increased capabilities of mobile deep learning frameworks makes it
possible to run complex and deep AI models on mobile devices. In this paper, we
evaluate the performance and compare the results of all chipsets from Qualcomm,
HiSilicon, Samsung, MediaTek and Unisoc that are providing hardware
acceleration for AI inference. We also discuss the recent changes in the
Android ML pipeline and provide an overview of the deployment of deep learning
models on mobile devices. All numerical results provided in this paper can be
found and are regularly updated on the official project website:
http://ai-benchmark.com.
</summary>
    <author>
      <name>Andrey Ignatov</name>
    </author>
    <author>
      <name>Radu Timofte</name>
    </author>
    <author>
      <name>Andrei Kulik</name>
    </author>
    <author>
      <name>Seungsoo Yang</name>
    </author>
    <author>
      <name>Ke Wang</name>
    </author>
    <author>
      <name>Felix Baum</name>
    </author>
    <author>
      <name>Max Wu</name>
    </author>
    <author>
      <name>Lirong Xu</name>
    </author>
    <author>
      <name>Luc Van Gool</name>
    </author>
    <link href="http://arxiv.org/abs/1910.06663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.07312v1</id>
    <updated>2019-10-16T12:41:13Z</updated>
    <published>2019-10-16T12:41:13Z</published>
    <title>Throughput and Delay Analysis of Slotted Aloha with Batch Service</title>
    <summary>  In this paper, we study the throughput and delay performances of the slotted
Aloha with batch service, which has wide applications in random access
networks. Different from the classical slotted Aloha, each node in the slotted
Aloha with batch service can transmit up to M packets once it succeeds in
channel competition. The throughput is substantially improved because up to M
packets jointly undertake the overhead due to contention. In an innovative
vacation model developed in this paper, we consider each batch of data
transmission as a busy period of each node, and the process between two
successive busy periods as a vacation period. We then formulate the number of
arrivals during a vacation period in a renewal-type equation, which
characterizes the dependency between busy periods and vacation periods. Based
on this formulation, we derive the mean waiting time of a packet and the
bounded delay region for the slotted Aloha with batch service. Our results
indicate the throughput and delay performances are substantially improved with
the increase of batch sizeM, and the bounded delay region is enlarged
accordingly. As M goes to infinity, we find the saturated throughput can
approach 100% of channel capacity, and the system remains stable irrespective
of the population size and transmission probability.
</summary>
    <author>
      <name>Huanhuan Huang</name>
    </author>
    <author>
      <name>Tong Ye</name>
    </author>
    <author>
      <name>Tony T. Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1910.07312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.07377v1</id>
    <updated>2019-10-16T14:36:30Z</updated>
    <published>2019-10-16T14:36:30Z</published>
    <title>Kriptosare.gen, a dockerized Bitcoin testbed: analysis of server
  performance</title>
    <summary>  Bitcoin is a peer-to-peer distributed cryptocurrency system, that keeps all
transaction history in a public ledger known as blockchain. The Bitcoin network
is implicitly pseudoanonymous and its nodes are controlled by independent
entities making network analysis difficult. This calls for the development of a
fully controlled testing environment.
  This paper presents Kriptosare.gen, a dockerized automatized Bitcoin testbed,
for deploying full-scale custom Bitcoin networks. The testbed is deployed in a
single machine executing four different experiments, each one with different
network configuration. We perform a cost analysis to investigate how the
resources are related with network parameters and provide experimental data
quantifying the amount of computational resources needed to run the different
types of simulations. Obtained results demonstrate that it is possible to run
the testbed with a configuration similar to a real Bitcoin system.
</summary>
    <author>
      <name>Francesco Zola</name>
    </author>
    <author>
      <name>Cristina Pérez-Solá</name>
    </author>
    <author>
      <name>Jon Egaña Zubia</name>
    </author>
    <author>
      <name>Maria Eguimendia</name>
    </author>
    <author>
      <name>Jordi Herrera-Joancomartí</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/NTMS.2019.8763809</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/NTMS.2019.8763809" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures, 1 table, presented during the 10th IFIP
  International Conference on New Technologies, Mobility and Security (NTMS)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2019 10th IFIP International Conference on New Technologies,
  Mobility and Security (NTMS) (pp. 1-5). IEEE</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.07377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.12894v1</id>
    <updated>2019-10-28T18:02:32Z</updated>
    <published>2019-10-28T18:02:32Z</published>
    <title>Please come back later: Benefiting from deferrals in service systems</title>
    <summary>  The performance evaluation of loss service systems, where customers who
cannot be served upon arrival get dropped, has a long history going back to the
classical Erlang B model. In this paper, we consider the performance benefits
arising from the possibility of deferring customers who cannot be served upon
arrival. Specifically, we consider an Erlang B type loss system where the
system operator can, subject to certain constraints, ask a customer arriving
when all servers are busy, to come back at a specified time in the future. If
the system is still fully loaded when the deferred customer returns, she gets
dropped for good. For such a system, we ask: How should the system operator
determine the rearrival times of the deferred customers based on the state of
the system (which includes those customers already deferred and yet to arrive)?
How does one quantify the performance benefit of such a deferral policy? Our
contributions are as follows. We propose a simple state-dependent policy for
determining the rearrival times of deferred customers. For this policy, we
characterize the long run fraction of customers dropped. We also analyse a
relaxation where the deferral times are bounded in expectation. Via extensive
numerical evaluations, we demonstrate the superiority of the proposed
state-dependent policies over naive state-independent deferral policies.
</summary>
    <author>
      <name>Anmol Kagrecha</name>
    </author>
    <author>
      <name>Jayakrishnan Nair</name>
    </author>
    <link href="http://arxiv.org/abs/1910.12894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.12894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.14496v1</id>
    <updated>2019-10-31T14:33:30Z</updated>
    <published>2019-10-31T14:33:30Z</published>
    <title>Direct N-body application on low-power and energy-efficient parallel
  architectures</title>
    <summary>  The aim of this work is to quantitatively evaluate the impact of computation
on the energy consumption on ARM MPSoC platforms, exploiting CPUs, embedded
GPUs and FPGAs. One of them possibly represents the future of High Performance
Computing systems: a prototype of an Exascale supercomputer. Performance and
energy measurements are made using a state-of-the-art direct $N$-body code from
the astrophysical domain. We provide a comparison of the time-to-solution and
energy delay product metrics, for different software configurations. We have
shown that FPGA technologies can be used for application kernel acceleration
and are emerging as a promising alternative to "traditional" technologies for
HPC, which purely focus on peak-performance than on power-efficiency.
</summary>
    <author>
      <name>D. Goz</name>
    </author>
    <author>
      <name>G. Ieronymakis</name>
    </author>
    <author>
      <name>V. Papaefstathiou</name>
    </author>
    <author>
      <name>N. Dimou</name>
    </author>
    <author>
      <name>S. Bertocco</name>
    </author>
    <author>
      <name>A. Ragagnin</name>
    </author>
    <author>
      <name>L. Tornatore</name>
    </author>
    <author>
      <name>G. Taffoni</name>
    </author>
    <author>
      <name>I. Coretti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figure, 2 tables; The final publication will be available
  at IOS Press</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.14496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.14496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.14546v1</id>
    <updated>2019-10-31T15:47:35Z</updated>
    <published>2019-10-31T15:47:35Z</published>
    <title>Debian Package usage profiler for Debian based Systems</title>
    <summary>  The embedded devices of today due to their CPU, RAM capabilities can run
various Linux distributions but in most cases they are different from general
purpose distributions as they are usually lighter and specific to the needs of
that particular system. In this project, we share the problems associated in
adopting a fully heavy-weight Debian based system like Ubuntu in
embedded/automotive platforms and provide solutions to optimize them to
identify unused/redundant content in the system. This helps developer to reduce
the hefty general purpose distribution to an application specific distribution.
The solution involves collecting usage data in the system in a non-invasive
manner (to avoid any drop in performance) to suggest users the redundant,
unused parts of the system that can be safely removed without impacting the
system functionality.
</summary>
    <author>
      <name>Bharath Honnesara Sreenivasa</name>
    </author>
    <author>
      <name>Ajay Rajan</name>
    </author>
    <link href="http://arxiv.org/abs/1910.14546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.14546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.00119v2</id>
    <updated>2020-05-28T12:33:53Z</updated>
    <published>2019-10-31T21:40:42Z</published>
    <title>ALERT: Accurate Learning for Energy and Timeliness</title>
    <summary>  An increasing number of software applications incorporate runtime Deep Neural
Networks (DNNs) to process sensor data and return inference results to humans.
Effective deployment of DNNs in these interactive scenarios requires meeting
latency and accuracy constraints while minimizing energy, a problem exacerbated
by common system dynamics. Prior approaches handle dynamics through either (1)
system-oblivious DNN adaptation, which adjusts DNN latency/accuracy tradeoffs,
or (2) application-oblivious system adaptation, which adjusts resources to
change latency/energy tradeoffs. In contrast, this paper improves on the
state-of-the-art by coordinating application- and system-level adaptation.
ALERT, our runtime scheduler, uses a probabilistic model to detect
environmental volatility and then simultaneously select both a DNN and a system
resource configuration to meet latency, accuracy, and energy constraints. We
evaluate ALERT on CPU and GPU platforms for image and speech tasks in dynamic
environments. ALERT's holistic approach achieves more than 13% energy
reduction, and 27% error reduction over prior approaches that adapt solely at
the application or system level. Furthermore, ALERT incurs only 3% more energy
consumption and 2% higher DNN-inference error than an oracle scheme with
perfect application and system knowledge.
</summary>
    <author>
      <name>Chengcheng Wan</name>
    </author>
    <author>
      <name>Muhammad Santriaji</name>
    </author>
    <author>
      <name>Eri Rogers</name>
    </author>
    <author>
      <name>Henry Hoffmann</name>
    </author>
    <author>
      <name>Michael Maire</name>
    </author>
    <author>
      <name>Shan Lu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3485730.3493446</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3485730.3493446" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">USENIX ATC, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.00119v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00119v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.02987v1</id>
    <updated>2019-11-08T02:13:21Z</updated>
    <published>2019-11-08T02:13:21Z</published>
    <title>The Pitfall of Evaluating Performance on Emerging AI Accelerators</title>
    <summary>  In recent years, domain-specific hardware has brought significant performance
improvements in deep learning (DL). Both industry and academia only focus on
throughput when evaluating these AI accelerators, which usually are custom
ASICs deployed in datacenter to speed up the inference phase of DL workloads.
Pursuing higher hardware throughput such as OPS (Operation Per Second) using
various optimizations seems to be their main design target. However, they
ignore the importance of accuracy in the DL nature. Motivated by this, this
paper argue that a single throughput metric can not comprehensively reflect the
real-world performance of AI accelerators. To reveal this pitfall, we evaluates
several frequently-used optimizations on a typical AI accelerator and
quantifies their impact on accuracy and throughout under representative DL
inference workloads. Based on our experimental results, we find that some
optimizations cause significant loss on accuracy in some workloads, although it
can improves the throughout. Furthermore, our results show the importance of
end-to-end evaluation in DL.
</summary>
    <author>
      <name>Zihan Jiang</name>
    </author>
    <author>
      <name>Jiansong Li</name>
    </author>
    <author>
      <name>Jiangfeng Zhan</name>
    </author>
    <link href="http://arxiv.org/abs/1911.02987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.02987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03282v2</id>
    <updated>2020-03-06T23:24:28Z</updated>
    <published>2019-11-08T14:31:18Z</published>
    <title>nanoBench: A Low-Overhead Tool for Running Microbenchmarks on x86
  Systems</title>
    <summary>  We present nanoBench, a tool for evaluating small microbenchmarks using
hardware performance counters on Intel and AMD x86 systems. Most existing tools
and libraries are intended to either benchmark entire programs, or program
segments in the context of their execution within a larger program. In
contrast, nanoBench is specifically designed to evaluate small, isolated pieces
of code. Such code is common in microbenchmark-based hardware analysis
techniques.
  Unlike previous tools, nanoBench can execute microbenchmarks directly in
kernel space. This allows to benchmark privileged instructions, and it enables
more accurate measurements. The reading of the performance counters is
implemented with minimal overhead avoiding functions calls and branches. As a
consequence, nanoBench is precise enough to measure individual memory accesses.
  We illustrate the utility of nanoBench at the hand of two case studies.
First, we briefly discuss how nanoBench has been used to determine the latency,
throughput, and port usage of more than 13,000 instruction variants on recent
x86 processors. Second, we show how to generate microbenchmarks to precisely
characterize the cache architectures of eleven Intel Core microarchitectures.
This includes the most comprehensive analysis of the employed cache replacement
policies to date.
</summary>
    <author>
      <name>Andreas Abel</name>
    </author>
    <author>
      <name>Jan Reineke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISPASS48437.2020.00014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISPASS48437.2020.00014" rel="related"/>
    <link href="http://arxiv.org/abs/1911.03282v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03282v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.07449v4</id>
    <updated>2019-12-13T00:46:38Z</updated>
    <published>2019-11-18T06:42:15Z</published>
    <title>Understanding Open Source Serverless Platforms: Design Considerations
  and Performance</title>
    <summary>  Serverless computing is increasingly popular because of the promise of lower
cost and the convenience it provides to users who do not need to focus on
server management. This has resulted in the availability of a number of
proprietary and open-source serverless solutions. We seek to understand how the
performance of serverless computing depends on a number of design issues using
several popular open-source serverless platforms. We identify the
idiosyncrasies affecting performance (throughput and latency) for different
open-source serverless platforms. Further, we observe that just having either
resource-based (CPU and memory) or workload-based (request per second (RPS) or
concurrent requests) auto-scaling is inadequate to address the needs of the
serverless platforms.
</summary>
    <author>
      <name>Junfeng Li</name>
    </author>
    <author>
      <name>Sameer G. Kulkarni</name>
    </author>
    <author>
      <name>K. K. Ramakrishnan</name>
    </author>
    <author>
      <name>Dan Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3366623.3368139</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3366623.3368139" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 5th International Workshop on Serverless
  Computing, Pages 37-42, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.07449v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.07449v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.11642v1</id>
    <updated>2019-11-26T15:33:56Z</updated>
    <published>2019-11-26T15:33:56Z</published>
    <title>System Performance with varying L1 Instruction and Data Cache Sizes: An
  Empirical Analysis</title>
    <summary>  In this project, we investigate the fluctuations in performance caused by
changing the Instruction (I-cache) size and the Data (D-cache) size in the L1
cache. We employ the Gem5 framework to simulate a system with varying
specifications on a single host machine. We utilize the FreqMine benchmark
available under the PARSEC suite as the workload program to benchmark our
simulated system. The Out-order CPU (O3) with Ruby memory model was simulated
in a Full-System X86 environment with Linux OS. The chosen metrics deal with
Hit Rate, Misses, Memory Latency, Instruction Rate, and Bus Traffic within the
system. Performance observed by varying L1 size within a certain range of
values was used to compute Confidence Interval based statistics for relevant
metrics. Our expectations, corresponding experimental observations, and
discrepancies are also discussed in this report.
</summary>
    <author>
      <name>Ramya Akula</name>
    </author>
    <author>
      <name>Kartik Jain</name>
    </author>
    <author>
      <name>Deep Jigar Kotecha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Figures and 3 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.11642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.11852v1</id>
    <updated>2019-11-09T13:17:12Z</updated>
    <published>2019-11-09T13:17:12Z</published>
    <title>Rule Designs for Optimal Online Game Matchmaking</title>
    <summary>  Online games are the most popular form of entertainment among youngsters as
well as elders. Recognized as e-Sports, they may become an official part of the
Olympic Games by 2020. However, a long waiting time for matchmaking will
largely affect players' experiences. We examine different matchmaking
mechanisms for 2v2 games. By casting the mechanisms into a queueing theoretic
framework, we decompose the rule design process into a sequence of decision
making problems, and derive the optimal mechanism with minimum expected waiting
time. We further the result by exploring additional static as well as dynamic
rule designs' impacts. In the static setting, we consider the game allows
players to choose sides before the battle. In the dynamic setting, we consider
the game offers multiple zones for players of different skill levels. In both
settings, we examine the value of choice-free players. Closed form expressions
for the expected waiting time in different settings illuminate the guidelines
for online game rule designs.
</summary>
    <author>
      <name>Mingkuan Xu</name>
    </author>
    <author>
      <name>Yang Yu</name>
    </author>
    <author>
      <name>Chenye Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1911.11852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.12877v1</id>
    <updated>2019-11-28T22:07:22Z</updated>
    <published>2019-11-28T22:07:22Z</published>
    <title>GraphZero: Breaking Symmetry for Efficient Graph Mining</title>
    <summary>  Graph mining for structural patterns is a fundamental task in many
applications. Compilation-based graph mining systems, represented by AutoMine,
generate specialized algorithms for the provided patterns and substantially
outperform other systems. However, the generated code causes substantial
computation redundancy and the compilation process incurs too much overhead to
be used online, both due to the inherent symmetry in the structural patterns.
  In this paper, we propose an optimizing compiler, GraphZero, to completely
address these limitations through symmetry breaking based on group theory.
GraphZero implements three novel techniques. First, its schedule explorer
efficiently prunes the schedule space without missing any high-performance
schedule. Second, it automatically generates and enforces a set of restrictions
to eliminate computation redundancy. Third, it generalizes orientation, a
surprisingly effective optimization that was mainly used for clique patterns,
to apply to arbitrary patterns. Evaluated on multiple graph mining applications
and complex patterns with 7 real-world graph datasets, GraphZero demonstrates
up to 40X performance improvement and up to 197X reduction on schedule
generation overhead over AutoMine.
</summary>
    <author>
      <name>Daniel Mawhirter</name>
    </author>
    <author>
      <name>Sam Reinehr</name>
    </author>
    <author>
      <name>Connor Holmes</name>
    </author>
    <author>
      <name>Tongping Liu</name>
    </author>
    <author>
      <name>Bo Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1911.12877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.12877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.13027v2</id>
    <updated>2020-08-28T12:32:34Z</updated>
    <published>2019-11-29T09:58:23Z</published>
    <title>Using performance analysis tools for parallel-in-time integrators --
  Does my time-parallel code do what I think it does?</title>
    <summary>  While many ideas and proofs of concept for parallel-in-time integration
methods exists, the number of large-scale, accessible time-parallel codes is
rather small. This is often due to the apparent or subtle complexity of the
algorithms and the many pitfalls awaiting developers of parallel numerical
software. One example of such a time-parallel code is pySDC, which implements,
among others, the parallel full approximation scheme in space and time
(PFASST). Inspired by nonlinear multigrid ideas, PFASST allows to integrate
multiple time-steps simultaneously using a space-time hierarchy of spectral
deferred corrections. In this paper we demonstrate the application of
performance analysis tools to the PFASST implementation pySDC. Tracing the path
we took for this work, we highlight the obstacles encountered, describe
remedies and explain the sometimes surprising findings made possible by the
tools. Although focusing only on a single implementation of a particular
parallel-in-time integrator, we hope that our results and in particular the way
we obtained them are a blueprint for other time-parallel codes.
</summary>
    <author>
      <name>Robert Speck</name>
    </author>
    <author>
      <name>Michael Knobloch</name>
    </author>
    <author>
      <name>Sebastian Lührs</name>
    </author>
    <author>
      <name>Andreas Gocht</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 15 figures, CVS Proceedings of the 9th PinT Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.13027v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.13027v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.00572v2</id>
    <updated>2019-12-03T06:37:43Z</updated>
    <published>2019-12-02T03:40:27Z</published>
    <title>BenchCouncil's View on Benchmarking AI and Other Emerging Workloads</title>
    <summary>  This paper outlines BenchCouncil's view on the challenges, rules, and vision
of benchmarking modern workloads like Big Data, AI or machine learning, and
Internet Services. We conclude the challenges of benchmarking modern workloads
as FIDSS (Fragmented, Isolated, Dynamic, Service-based, and Stochastic), and
propose the PRDAERS benchmarking rules that the benchmarks should be specified
in a paper-and-pencil manner, relevant, diverse, containing different levels of
abstractions, specifying the evaluation metrics and methodology, repeatable,
and scaleable. We believe proposing simple but elegant abstractions that help
achieve both efficiency and general-purpose is the final target of benchmarking
in future, which may be not pressing. In the light of this vision, we shortly
discuss BenchCouncil's related projects.
</summary>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Wanling Gao</name>
    </author>
    <author>
      <name>Rui Ren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.00572v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.00572v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.03348v2</id>
    <updated>2019-12-31T16:40:33Z</updated>
    <published>2019-12-06T21:38:55Z</published>
    <title>Scheduling in the Presence of Data Intensive Compute Jobs</title>
    <summary>  We study the performance of non-adaptive scheduling policies in computing
systems with multiple servers. Compute jobs are mostly regular, with modest
service requirements. However, there are sporadic data intensive jobs, whose
expected service time is much higher than that of the regular jobs. Forthis
model, we are interested in the effect of scheduling policieson the average
time a job spends in the system. To this end, we introduce two performance
indicators in a simplified, only-arrival system. We believe that these
performance indicators are good predictors of the relative performance of the
policies in the queuing system, which is supported by simulations results.
</summary>
    <author>
      <name>Amir Behrouzi-Far</name>
    </author>
    <author>
      <name>Emina Soljanin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in IEEE BigData 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.03348v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.03348v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.06322v3</id>
    <updated>2021-01-12T02:03:16Z</updated>
    <published>2019-12-13T04:39:16Z</published>
    <title>Queueing Analysis of GPU-Based Inference Servers with Dynamic Batching:
  A Closed-Form Characterization</title>
    <summary>  GPU-accelerated computing is a key technology to realize high-speed inference
servers using deep neural networks (DNNs). An important characteristic of
GPU-based inference is that the computational efficiency, in terms of the
processing speed and energy consumption, drastically increases by processing
multiple jobs together in a batch. In this paper, we formulate GPU-based
inference servers as a batch service queueing model with batch-size dependent
processing times. We first show that the energy efficiency of the server
monotonically increases with the arrival rate of inference jobs, which suggests
that it is energy-efficient to operate the inference server under a utilization
level as high as possible within a latency requirement of inference jobs. We
then derive a closed-form upper bound for the mean latency, which provides a
simple characterization of the latency performance. Through simulation and
numerical experiments, we show that the exact value of the mean latency is well
approximated by this upper bound. We further compare this upper bound with the
latency curve measured in real implementation of GPU-based inference servers
and we show that the real performance curve is well explained by the derived
simple formula.
</summary>
    <author>
      <name>Yoshiaki Inoue</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.peva.2020.102183</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.peva.2020.102183" rel="related"/>
    <link href="http://arxiv.org/abs/1912.06322v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.06322v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.09256v1</id>
    <updated>2019-12-19T15:05:30Z</updated>
    <published>2019-12-19T15:05:30Z</published>
    <title>Is Big Data Performance Reproducible in Modern Cloud Networks?</title>
    <summary>  Performance variability has been acknowledged as a problem for over a decade
by cloud practitioners and performance engineers. Yet, our survey of top
systems conferences reveals that the research community regularly disregards
variability when running experiments in the cloud. Focusing on networks, we
assess the impact of variability on cloud-based big-data workloads by gathering
traces from mainstream commercial clouds and private research clouds. Our data
collection consists of millions of datapoints gathered while transferring over
9 petabytes of data. We characterize the network variability present in our
data and show that, even though commercial cloud providers implement mechanisms
for quality-of-service enforcement, variability still occurs, and is even
exacerbated by such mechanisms and service provider policies. We show how
big-data workloads suffer from significant slowdowns and lack predictability
and replicability, even when state-of-the-art experimentation techniques are
used. We provide guidelines for practitioners to reduce the volatility of big
data performance, making experiments more repeatable.
</summary>
    <author>
      <name>Alexandru Uta</name>
    </author>
    <author>
      <name>Alexandru Custura</name>
    </author>
    <author>
      <name>Dmitry Duplyakin</name>
    </author>
    <author>
      <name>Ivo Jimenez</name>
    </author>
    <author>
      <name>Jan Rellermeyer</name>
    </author>
    <author>
      <name>Carlos Maltzahn</name>
    </author>
    <author>
      <name>Robert Ricci</name>
    </author>
    <author>
      <name>Alexandru Iosup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages paper, 3 pages references</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.09256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.09256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.09870v2</id>
    <updated>2023-03-03T09:59:54Z</updated>
    <published>2019-12-20T15:11:30Z</published>
    <title>QoS-aware energy-efficient workload routing and server speed control
  policy in data centers: a robust queueing theoretic approach</title>
    <summary>  Operating cloud service infrastructures requires high energy efficiency while
ensuring a satisfactory service level. Motivated by data centers, we consider a
workload routing and server speed control policy applicable to the system
operating under fluctuating demands. Dynamic control algorithms are generally
more energy-efficient than static ones. However, they often require frequent
information exchanges between routers and servers, making the data centers'
management hesitate to deploy these algorithms. This study presents a static
routing and server speed control policy that could achieve energy efficiency
similar to a dynamic algorithm and eliminate the necessity of frequent
communication among resources. We take a robust queueing theoretic approach to
response time constraints for the quality of service (QoS) conditions. Each
server is modeled as a G/G/1 processor sharing queue, and the concept of
uncertainty sets defines the domain of stochastic primitives. We derive an
approximative upper bound of sojourn times from uncertainty sets and develop an
approximative sojourn time quantile estimation method for QoS. Numerical
experiments confirm the proposed static policy offers competitive solutions
compared with the dynamic algorithm.
</summary>
    <author>
      <name>Seung Min Baik</name>
    </author>
    <author>
      <name>Young Myoung Ko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/24725854.2023.2183531</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/24725854.2023.2183531" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IISE Transactions, 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1912.09870v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.09870v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.11310v4</id>
    <updated>2020-09-28T07:22:17Z</updated>
    <published>2019-12-24T12:15:59Z</published>
    <title>Deadline-aware Scheduling for Maximizing Information Freshness in
  Industrial Cyber-Physical System</title>
    <summary>  Age of Information is an interesting metric that captures the freshness of
information in the underlying applications. It is a combination of both packets
inter-arrival time and packet transmission delay. In recent times, advanced
real-time systems rely on this metric for delivering status updates as timely
as possible. This paper aims to accomplish optimal transmission scheduling
policy to maintain the information freshness of real-time updates in the
industrial cyber-physical systems. Here the coexistence of both cyber and
physical units and their individual requirements to provide the quality of
service is one of the critical challenges to handle. A greedy scheduling policy
called deadline-aware highest latency first has been proposed for this purpose.
This paper also gives the analytical proof of its optimality, and finally, the
claim is validated by comparing the performance of our algorithm with other
scheduling policies by extensive simulations.
</summary>
    <author>
      <name>Devarpita Sinha</name>
    </author>
    <author>
      <name>Rajarshi Roy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JSEN.2020.3014368</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JSEN.2020.3014368" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in IEEE Sensors Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.11310v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11310v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.04989v4</id>
    <updated>2020-06-16T07:21:06Z</updated>
    <published>2020-02-12T13:44:41Z</published>
    <title>Eigenvector Component Calculation Speedup over NumPy for
  High-Performance Computing</title>
    <summary>  Applications related to artificial intelligence, machine learning, and system
identification simulations essentially use eigenvectors. Calculating
eigenvectors for very large matrices using conventional methods is
compute-intensive and renders the applications slow. Recently,
Eigenvector-Eigenvalue Identity formula promising significant speedup was
identified. We study the algorithmic implementation of the formula against the
existing state-of-the-art algorithms and their implementations to evaluate the
performance gains. We provide a first of its kind systematic study of the
implementation of the formula. We demonstrate further improvements using
high-performance computing concepts over native NumPy eigenvector
implementation which uses LAPACK and BLAS.
</summary>
    <author>
      <name>Shrey Dabhi</name>
    </author>
    <author>
      <name>Manojkumar Parmar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at 8th International Conference on Recent Trends in
  Computing (ICRTC 2020), to be published in Springer Lecture Notes in Networks
  and Systems (LNNS)</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.04989v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.04989v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.06018v1</id>
    <updated>2020-02-13T08:00:56Z</updated>
    <published>2020-02-13T08:00:56Z</published>
    <title>A Prompt Report on the Performance of Intel Optane DC Persistent Memory
  Module</title>
    <summary>  In this prompt report, we present the basic performance evaluation of Intel
Optane Data Center Persistent Memory Module (Optane DCPMM), which is the first
commercially-available, byte-addressable non-volatile memory modules released
in April 2019. Since at the moment of writing only a few reports on its
performance were published, this letter is intended to complement other
performance studies. Through experiments using our own measurement tools, we
obtained that the latency of random read-only access was approximately 374 ns.
That of random writeback-involving access was 391 ns. The bandwidths of
read-only and writeback-involving access for interleaved memory modules were
approximately 38 GB/s and 3 GB/s, respectively.
</summary>
    <author>
      <name>Takahiro Hirofuchi</name>
    </author>
    <author>
      <name>Ryousei Takano</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1587/transinf.2019EDL8141</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1587/transinf.2019EDL8141" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEICE Transactions on Information and Systems, 2020.
  arXiv admin note: substantial text overlap with arXiv:1907.12014</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.06018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.07062v1</id>
    <updated>2020-02-14T17:13:13Z</updated>
    <published>2020-02-14T17:13:13Z</published>
    <title>An optimal scheduling architecture for accelerating batch algorithms on
  Neural Network processor architectures</title>
    <summary>  In neural network topologies, algorithms are running on batches of data
tensors. The batches of data are typically scheduled onto the computing cores
which execute in parallel. For the algorithms running on batches of data, an
optimal batch scheduling architecture is very much needed by suitably utilizing
hardware resources - thereby resulting in significant reduction training and
inference time. In this paper, we propose to accelerate the batch algorithms
for neural networks through a scheduling architecture enabling optimal compute
power utilization. The proposed optimal scheduling architecture can be built
into HW or can be implemented in SW alone which can be leveraged for
accelerating batch algorithms. The results demonstrate that the proposed
architecture speeds up the batch algorithms compared to the previous solutions.
The proposed idea applies to any HPC architecture meant for neural networks.
</summary>
    <author>
      <name>Phani Kumar Nyshadham</name>
    </author>
    <author>
      <name>Mohit Sinha</name>
    </author>
    <author>
      <name>Biswajit Mishra</name>
    </author>
    <author>
      <name>H S Vijay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, page 7 contains the proposed example</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.07062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.07062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.08316v2</id>
    <updated>2020-04-17T16:36:08Z</updated>
    <published>2020-02-17T20:49:12Z</published>
    <title>Re-evaluating scaling methods for distributed parallel systems</title>
    <summary>  The paper explains why Amdahl's Law shall be interpreted specifically for
distributed parallel systems and why it generated so many debates, discussions,
and abuses. We set up a general model and list many of the terms affecting
parallel processing. We scrutinize the validity of neglecting certain terms in
different approximations, with special emphasis on the famous scaling laws of
parallel processing. We clarify that when using the right interpretation of
terms, Amdahl's Law is the governing law of all kinds of parallel processing.
Amdahl's Law describes among others the history of supercomputing, the inherent
performance limitation of the different kinds of parallel processing and it is
the basic Law of the 'modern computing' paradigm, that the computing systems
working under extreme computing conditions are desperately needed.
</summary>
    <author>
      <name>János Végh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:2001.01266</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.08316v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.08316v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.08908v1</id>
    <updated>2020-02-20T17:52:53Z</updated>
    <published>2020-02-20T17:52:53Z</published>
    <title>Asymptotically Optimal Load Balancing in Large-scale Heterogeneous
  Systems with Multiple Dispatchers</title>
    <summary>  We consider the load balancing problem in large-scale heterogeneous systems
with multiple dispatchers. We introduce a general framework called
Local-Estimation-Driven (LED). Under this framework, each dispatcher keeps
local (possibly outdated) estimates of queue lengths for all the servers, and
the dispatching decision is made purely based on these local estimates. The
local estimates are updated via infrequent communications between dispatchers
and servers. We derive sufficient conditions for LED policies to achieve
throughput optimality and delay optimality in heavy-traffic, respectively.
These conditions directly imply delay optimality for many previous local-memory
based policies in heavy traffic. Moreover, the results enable us to design new
delay optimal policies for heterogeneous systems with multiple dispatchers.
Finally, the heavy-traffic delay optimality of the LED framework directly
resolves a recent open problem on how to design optimal load balancing schemes
using delayed information.
</summary>
    <author>
      <name>Xingyu Zhou</name>
    </author>
    <author>
      <name>Ness Shroff</name>
    </author>
    <author>
      <name>Adam Wierman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.08908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.08908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10788v1</id>
    <updated>2020-02-25T10:56:47Z</updated>
    <published>2020-02-25T10:56:47Z</published>
    <title>Learning Queuing Networks by Recurrent Neural Networks</title>
    <summary>  It is well known that building analytical performance models in practice is
difficult because it requires a considerable degree of proficiency in the
underlying mathematics. In this paper, we propose a machine-learning approach
to derive performance models from data. We focus on queuing networks, and
crucially exploit a deterministic approximation of their average dynamics in
terms of a compact system of ordinary differential equations. We encode these
equations into a recurrent neural network whose weights can be directly related
to model parameters. This allows for an interpretable structure of the neural
network, which can be trained from system measurements to yield a white-box
parameterized model that can be used for prediction purposes such as what-if
analyses and capacity planning. Using synthetic models as well as a real case
study of a load-balancing system, we show the effectiveness of our technique in
yielding models with high predictive power.
</summary>
    <author>
      <name>Giulio Garbi</name>
    </author>
    <author>
      <name>Emilio Incerto</name>
    </author>
    <author>
      <name>Mirco Tribastone</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3358960.3379134</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3358960.3379134" rel="related"/>
    <link href="http://arxiv.org/abs/2002.10788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.6; C.4; D.2.0; D.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.12798v1</id>
    <updated>2020-02-27T05:06:19Z</updated>
    <published>2020-02-27T05:06:19Z</published>
    <title>Optimizing Memory-Access Patterns for Deep Learning Accelerators</title>
    <summary>  Deep learning (DL) workloads are moving towards accelerators for faster
processing and lower cost. Modern DL accelerators are good at handling the
large-scale multiply-accumulate operations that dominate DL workloads; however,
it is challenging to make full use of the compute power of an accelerator since
the data must be properly staged in a software-managed scratchpad memory.
Failing to do so can result in significant performance loss. This paper
proposes a systematic approach which leverages the polyhedral model to analyze
all operators of a DL model together to minimize the number of memory accesses.
Experiments show that our approach can substantially reduce the impact of
memory accesses required by common neural-network models on a homegrown AWS
machine-learning inference chip named Inferentia, which is available through
Amazon EC2 Inf1 instances.
</summary>
    <author>
      <name>Hongbin Zheng</name>
    </author>
    <author>
      <name>Sejong Oh</name>
    </author>
    <author>
      <name>Huiqing Wang</name>
    </author>
    <author>
      <name>Preston Briggs</name>
    </author>
    <author>
      <name>Jiading Gai</name>
    </author>
    <author>
      <name>Animesh Jain</name>
    </author>
    <author>
      <name>Yizhi Liu</name>
    </author>
    <author>
      <name>Rich Heaton</name>
    </author>
    <author>
      <name>Randy Huang</name>
    </author>
    <author>
      <name>Yida Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract for a poster presented at C4ML workshop 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.12798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.12798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.04821v4</id>
    <updated>2021-01-29T21:21:37Z</updated>
    <published>2020-03-10T15:58:12Z</published>
    <title>Benchmarking TinyML Systems: Challenges and Direction</title>
    <summary>  Recent advancements in ultra-low-power machine learning (TinyML) hardware
promises to unlock an entirely new class of smart applications. However,
continued progress is limited by the lack of a widely accepted benchmark for
these systems. Benchmarking allows us to measure and thereby systematically
compare, evaluate, and improve the performance of systems and is therefore
fundamental to a field reaching maturity. In this position paper, we present
the current landscape of TinyML and discuss the challenges and direction
towards developing a fair and useful hardware benchmark for TinyML workloads.
Furthermore, we present our four benchmarks and discuss our selection
methodology. Our viewpoints reflect the collective thoughts of the TinyMLPerf
working group that is comprised of over 30 organizations.
</summary>
    <author>
      <name>Colby R. Banbury</name>
    </author>
    <author>
      <name>Vijay Janapa Reddi</name>
    </author>
    <author>
      <name>Max Lam</name>
    </author>
    <author>
      <name>William Fu</name>
    </author>
    <author>
      <name>Amin Fazel</name>
    </author>
    <author>
      <name>Jeremy Holleman</name>
    </author>
    <author>
      <name>Xinyuan Huang</name>
    </author>
    <author>
      <name>Robert Hurtado</name>
    </author>
    <author>
      <name>David Kanter</name>
    </author>
    <author>
      <name>Anton Lokhmotov</name>
    </author>
    <author>
      <name>David Patterson</name>
    </author>
    <author>
      <name>Danilo Pau</name>
    </author>
    <author>
      <name>Jae-sun Seo</name>
    </author>
    <author>
      <name>Jeff Sieracki</name>
    </author>
    <author>
      <name>Urmish Thakker</name>
    </author>
    <author>
      <name>Marian Verhelst</name>
    </author>
    <author>
      <name>Poonam Yadav</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.04821v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.04821v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.06452v3</id>
    <updated>2021-03-03T14:06:36Z</updated>
    <published>2020-03-13T19:19:45Z</published>
    <title>How Fast Can We Insert? An Empirical Performance Evaluation of Apache
  Kafka</title>
    <summary>  Message brokers see widespread adoption in modern IT landscapes, with Apache
Kafka being one of the most employed platforms. These systems feature
well-defined APIs for use and configuration and present flexible solutions for
various data storage scenarios. Their ability to scale horizontally enables
users to adapt to growing data volumes and changing environments. However, one
of the main challenges concerning message brokers is the danger of them
becoming a bottleneck within an IT architecture. To prevent this, knowledge
about the amount of data a message broker using a specific configuration can
handle needs to be available. In this paper, we propose a monitoring
architecture for message brokers and similar Java Virtual Machine-based
systems. We present a comprehensive performance analysis of the popular Apache
Kafka platform using our approach. As part of the benchmark, we study selected
data ingestion scenarios with respect to their maximum data ingestion rates.
The results show that we can achieve an ingestion rate of about 420,000
messages/second on the used commodity hardware and with the developed data
sender tool.
</summary>
    <author>
      <name>Guenter Hesse</name>
    </author>
    <author>
      <name>Christoph Matthies</name>
    </author>
    <author>
      <name>Matthias Uflacker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICPADS51040.2020.00089</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICPADS51040.2020.00089" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Parallel and Distributed Systems
  (ICPADS) 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.06452v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06452v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.14087v2</id>
    <updated>2020-04-29T08:48:02Z</updated>
    <published>2020-03-31T10:49:41Z</published>
    <title>Static vs accumulating priorities in healthcare queues under heavy loads</title>
    <summary>  Amid unprecedented times caused by COVID-19, healthcare systems all over the
world are strained to the limits of, or even beyond, capacity. A similar event
is experienced by some healthcare systems regularly, due to for instance
seasonal spikes in the number of patients. We model this as a queueing system
in heavy traffic (where the arrival rate is approaching the service rate from
below) or in overload (where the arrival rate exceeds the service rate). In
both cases we assume that customers (patients) may have different priorities
and we consider two popular service disciplines: static priorities and
accumulating priorities. It has been shown that the latter allows for patients
of all classes to be seen in a timely manner as long as the system is stable.
We demonstrate however that if accumulating priorities are used in the heavy
traffic or overload regime, then all patients, including those with the highest
priority, will experience very long waiting times. If on the other hand static
priorities are applied, then one can ensure that the highest-priority patients
will be seen in a timely manner even in overloaded systems.
</summary>
    <author>
      <name>Binyamin Oz</name>
    </author>
    <author>
      <name>Seva Shneer</name>
    </author>
    <author>
      <name>Ilze Ziedins</name>
    </author>
    <link href="http://arxiv.org/abs/2003.14087v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.14087v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.00991v1</id>
    <updated>2020-04-01T12:55:11Z</updated>
    <published>2020-04-01T12:55:11Z</published>
    <title>Computational Performance of a Germline Variant Calling Pipeline for
  Next Generation Sequencing</title>
    <summary>  With the booming of next generation sequencing technology and its
implementation in clinical practice and life science research, the need for
faster and more efficient data analysis methods becomes pressing in the field
of sequencing. Here we report on the evaluation of an optimized germline
mutation calling pipeline, HummingBird, by assessing its performance against
the widely accepted BWA-GATK pipeline. We found that the HummingBird pipeline
can significantly reduce the running time of the primary data analysis for
whole genome sequencing and whole exome sequencing while without significantly
sacrificing the variant calling accuracy. Thus, we conclude that expansion of
such software usage will help to improve the primary data analysis efficiency
for next generation sequencing.
</summary>
    <author>
      <name>Jie Liu</name>
    </author>
    <author>
      <name>Xiaotian Wu</name>
    </author>
    <author>
      <name>Kai Zhang</name>
    </author>
    <author>
      <name>Bing Liu</name>
    </author>
    <author>
      <name>Renyi Bao</name>
    </author>
    <author>
      <name>Xiao Chen</name>
    </author>
    <author>
      <name>Yiran Cai</name>
    </author>
    <author>
      <name>Yiming Shen</name>
    </author>
    <author>
      <name>Xinjun He</name>
    </author>
    <author>
      <name>Jun Yan</name>
    </author>
    <author>
      <name>Weixing Ji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.00991v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.00991v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF, q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; D.4.8; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.03276v3</id>
    <updated>2020-07-19T16:35:24Z</updated>
    <published>2020-04-07T11:21:47Z</published>
    <title>Function-as-a-Service Performance Evaluation: A Multivocal Literature
  Review</title>
    <summary>  Function-as-a-Service (FaaS) is one form of the serverless cloud computing
paradigm and is defined through FaaS platforms (e.g., AWS Lambda) executing
event-triggered code snippets (i.e., functions). Many studies that empirically
evaluate the performance of such FaaS platforms have started to appear but we
are currently lacking a comprehensive understanding of the overall domain. To
address this gap, we conducted a multivocal literature review (MLR) covering
112 studies from academic (51) and grey (61) literature. We find that existing
work mainly studies the AWS Lambda platform and focuses on micro-benchmarks
using simple functions to measure CPU speed and FaaS platform overhead (i.e.,
container cold starts). Further, we discover a mismatch between academic and
industrial sources on tested platform configurations, find that function
triggers remain insufficiently studied, and identify HTTP API gateways and
cloud storages as the most used external service integrations. Following
existing guidelines on experimentation in cloud systems, we discover many flaws
threatening the reproducibility of experiments presented in the surveyed
studies. We conclude with a discussion of gaps in literature and highlight
methodological suggestions that may serve to improve future FaaS performance
evaluation studies.
</summary>
    <author>
      <name>Joel Scheuner</name>
    </author>
    <author>
      <name>Philipp Leitner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jss.2020.110708</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jss.2020.110708" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">improvements including postprint updates</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.03276v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.03276v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.03695v1</id>
    <updated>2020-04-07T20:39:28Z</updated>
    <published>2020-04-07T20:39:28Z</published>
    <title>Offsite Autotuning Approach -- Performance Model Driven Autotuning
  Applied to Parallel Explicit ODE Methods</title>
    <summary>  Autotuning techniques are a promising approach to minimize the otherwise
tedious manual effort of optimizing scientific applications for a specific
target platform. Ideally, an autotuning approach is capable of reliably
identifying the most efficient implementation variant(s) for a new target
system or new characteristics of the input by applying suitable program
transformations and analytic models. In this work, we introduce Offsite, an
offline autotuning approach which automates this selection process at
installation time by rating implementation variants based on an analytic
performance model without requiring time-consuming runtime experiments. From
abstract multilevel YAML description languages, Offsite automatically derives
optimized, platform-specific and problem-specific code of possible
implementation variants and applies the performance model to these
implementation variants.
  We apply Offsite to parallel numerical methods for ordinary differential
equations (ODEs). In particular, we investigate tuning a specific class of
explicit ODE solvers (PIRK methods) for various initial value problems (IVPs)
on shared-memory systems. Our experiments demonstrate that Offsite is able to
reliably identify a set of the most efficient implementation variants for given
test configurations (ODE solver, IVP, platform) and is capable of effectively
handling important autotuning scenarios.
</summary>
    <author>
      <name>Johannes Seiferth</name>
    </author>
    <author>
      <name>Matthias Korch</name>
    </author>
    <author>
      <name>Thomas Rauber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted preprint version; 19 pages; 5 figures; 4 tables; 7 listings</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.03695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.03695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.05628v1</id>
    <updated>2020-04-12T15:01:04Z</updated>
    <published>2020-04-12T15:01:04Z</published>
    <title>GAPP: A Fast Profiler for Detecting Serialization Bottlenecks in
  Parallel Linux Applications</title>
    <summary>  We present a parallel profiling tool, GAPP, that identifies serialization
bottlenecks in parallel Linux applications arising from load imbalance or
contention for shared resources . It works by tracing kernel context switch
events using kernel probes managed by the extended Berkeley Packet Filter
(eBPF) framework. The overhead is thus extremely low (an average 4% run time
overhead for the applications explored), the tool requires no program
instrumentation and works for a variety of serialization bottlenecks. We
evaluate GAPP using the Parsec3.0 benchmark suite and two large open-source
projects: MySQL and Nektar++ (a spectral/hp element framework). We show that
GAPP is able to reveal a wide range of bottleneck-related performance issues,
for example arising from synchronization primitives, busy-wait loops, memory
operations, thread imbalance and resource contention.
</summary>
    <author>
      <name>Reena Nair</name>
    </author>
    <author>
      <name>Tony Field</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3358960.3379136</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3358960.3379136" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.05628v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.05628v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.07519v1</id>
    <updated>2020-04-16T08:23:45Z</updated>
    <published>2020-04-16T08:23:45Z</published>
    <title>Refined Mean Field Analysis of the Gossip Shuffle Protocol -- extended
  version --</title>
    <summary>  Gossip protocols form the basis of many smart collective adaptive systems.
They are a class of fully decentralised, simple but robust protocols for the
distribution of information throughout large scale networks with hundreds or
thousands of nodes. Mean field analysis methods have made it possible to
approximate and analyse performance aspects of such large scale protocols in an
efficient way. Taking the gossip shuffle protocol as a benchmark, we evaluate a
recently developed refined mean field approach. We illustrate the gain in
accuracy this can provide for the analysis of medium size models analysing two
key performance measures. We also show that refined mean field analysis
requires special attention to correctly capture the coordination aspects of the
gossip shuffle protocol.
</summary>
    <author>
      <name>Nicolas Gast</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA, France</arxiv:affiliation>
    </author>
    <author>
      <name>Diego Latella</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CNR-ISTI, Italy</arxiv:affiliation>
    </author>
    <author>
      <name>Mieke Massink</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CNR-ISTI, Italy</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is an extended version of a short paper accepted for the
  LNCS proceedings of COORDINATION 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.07519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.07519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.11847v2</id>
    <updated>2021-12-23T02:24:44Z</updated>
    <published>2020-04-24T16:42:42Z</published>
    <title>Age of Information for Single Buffer Systems with Vacation Server</title>
    <summary>  In this research, we study the information freshness in M/G/1 queueing system
with a single buffer and the server taking multiple vacations. This system has
wide applications in communication systems. We aim to evaluate the information
freshness in this system with both i.i.d. and non-i.i.d. vacations under three
different scheduling policies, namely Conventional Buffer System (CBS), Buffer
Relaxation System (BRS), and Conventional Buffer System with Preemption in
Service (CBS-P). For the systems with i.i.d. vacations, we derive the
closed-form expressions of information freshness metrics such as the expected
Age of Information (AoI), the expected Peak Age of Information (PAoI), and the
variance of peak age under each policy. For systems with non-i.i.d. vacations,
we use the polling system as an example and provide the closed-form expression
of its PAoI under each policy. We explore the conditions under which one of
these policies has advantages over the others for each information freshness
metric. We further perform numerical studies to validate our results and
develop insights.
</summary>
    <author>
      <name>Jin Xu</name>
    </author>
    <author>
      <name>I-Hong Hou</name>
    </author>
    <author>
      <name>Natarajan Gautam</name>
    </author>
    <link href="http://arxiv.org/abs/2004.11847v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.11847v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.05836v1</id>
    <updated>2020-05-12T14:56:05Z</updated>
    <published>2020-05-12T14:56:05Z</published>
    <title>Demonstrating 100 Gbps in and out of the public Clouds</title>
    <summary>  There is increased awareness and recognition that public Cloud providers do
provide capabilities not found elsewhere, with elasticity being a major driver.
The value of elastic scaling is however tightly coupled to the capabilities of
the networks that connect all involved resources, both in the public Clouds and
at the various research institutions. This paper presents results of
measurements involving file transfers inside public Cloud providers, fetching
data from on-prem resources into public Cloud instances and fetching data from
public Cloud storage into on-prem nodes. The networking of the three major
Cloud providers, namely Amazon Web Services, Microsoft Azure and the Google
Cloud Platform, has been benchmarked. The on-prem nodes were managed by either
the Pacific Research Platform or located at the University of Wisconsin -
Madison. The observed sustained throughput was of the order of 100 Gbps in all
the tests moving data in and out of the public Clouds and throughput reaching
into the Tbps range for data movements inside the public Cloud providers
themselves. All the tests used HTTP as the transfer protocol.
</summary>
    <author>
      <name>Igor Sfiligoi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3311790.3399612</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3311790.3399612" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.05836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.05836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.06410v1</id>
    <updated>2020-05-13T16:36:25Z</updated>
    <published>2020-05-13T16:36:25Z</published>
    <title>High Performance and Portable Convolution Operators for ARM-based
  Multicore Processors</title>
    <summary>  The considerable impact of Convolutional Neural Networks on many Artificial
Intelligence tasks has led to the development of various high performance
algorithms for the convolution operator present in this type of networks. One
of these approaches leverages the \imcol transform followed by a general matrix
multiplication (GEMM) in order to take advantage of the highly optimized
realizations of the GEMM kernel in many linear algebra libraries. The main
problems of this approach are 1) the large memory workspace required to host
the intermediate matrices generated by the IM2COL transform; and 2) the time to
perform the IM2COL transform, which is not negligible for complex neural
networks. This paper presents a portable high performance convolution algorithm
based on the BLIS realization of the GEMM kernel that avoids the use of the
intermediate memory by taking advantage of the BLIS structure. In addition, the
proposed algorithm eliminates the cost of the explicit IM2COL transform, while
maintaining the portability and performance of the underlying realization of
GEMM in BLIS.
</summary>
    <author>
      <name>Pablo San Juan</name>
    </author>
    <author>
      <name>Adrián Castelló</name>
    </author>
    <author>
      <name>Manuel F. Dolz</name>
    </author>
    <author>
      <name>Pedro Alonso-Jordá</name>
    </author>
    <author>
      <name>Enrique S. Quintana-Ortí</name>
    </author>
    <link href="http://arxiv.org/abs/2005.06410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.06410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8; C.4; I.2; I.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.13788v3</id>
    <updated>2021-04-15T06:50:45Z</updated>
    <published>2020-05-28T05:54:30Z</published>
    <title>Age of Information in an Overtake-Free Network of Quasi-Reversible
  Queues</title>
    <summary>  We show how to calculate the Age of Information in an overtake-free network
of quasi-reversible queues, with exponential exogenous interarrivals of
multiple classes of update packets and exponential service times at all nodes.
Results are provided for any number of M/M/1 First-Come-First-Served (FCFS)
queues in tandem, and for a network with two classes of update packets,
entering through different queues in the network and exiting through the same
queue. The main takeaway is that in a network with different classes of update
packets, individual classes roughly preserve the ages they would achieve if
they were alone in the network, except when shared queues become saturated, in
which case the ages increase considerably.
</summary>
    <author>
      <name>Ioannis Koukoutsidis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MASCOTS50786.2020.9285958</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MASCOTS50786.2020.9285958" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrigendum to the MASCOTS 2020 paper. The following changes have
  been made: a) Re-calculation of the AoI for multiple classes to take full
  account of the presence of other-class packets b) correction of erroneous
  statement about the equality of the AoI right bound to the peak AoI.
  Clarifications on the extensibility of the analysis to other quasi-reversible
  queues are also provided</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">28th International Symposium on Modeling, Analysis, and Simulation
  of Computer and Telecommunication Systems (MASCOTS 2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.13788v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.13788v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.00515v1</id>
    <updated>2020-05-31T13:09:31Z</updated>
    <published>2020-05-31T13:09:31Z</published>
    <title>Staffing for many-server systems facing non-standard arrival processes</title>
    <summary>  Arrival processes to service systems often display (i) larger than
anticipated fluctuations, (ii) a time-varying rate, and (iii) temporal
correlation. Motivated by this, we introduce a specific non-homogeneous Poisson
process that incorporates these three features. The resulting arrival process
is fed into an infinite-server system, which is then used as a proxy for its
many-server counterpart. This leads to a staffing rule based on the square-root
staffing principle that acknowledges the three features. After a slight
rearrangement of servers over the time slots, we succeed to stabilize system
performance even under highly varying and strongly correlated conditions. We
fit the arrival stream model to real data from an emergency department and
demonstrate (by simulation) the performance of the novel staffing rule.
</summary>
    <author>
      <name>M. Heemskerk</name>
    </author>
    <author>
      <name>M. Mandjes</name>
    </author>
    <author>
      <name>B. Mathijsen</name>
    </author>
    <link href="http://arxiv.org/abs/2006.00515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.00515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60K25, 90B22" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.03404v1</id>
    <updated>2020-06-05T12:33:05Z</updated>
    <published>2020-06-05T12:33:05Z</published>
    <title>Joint performance analysis of ages of information in a multi-source
  pushout server</title>
    <summary>  Age of information (AoI) has been widely accepted as a measure quantifying
freshness of status information in real-time status update systems. In many of
such systems, multiple sources share a limited network resource and therefore
the AoIs defined for the individual sources should be correlated with each
other. However, there are not found any results studying the correlation of two
or more AoIs in a status update system with multiple sources. In this work, we
consider a multi-source system sharing a common service facility and provide a
framework to investigate joint performance of the multiple AoIs. We then apply
our framework to a simple pushout server with multiple sources and derive a
closed-form formula of the joint Laplace transform of the AoIs in the case with
independent M/G inputs. We further show some properties of the correlation
coefficient of AoIs in the two-source system.
</summary>
    <author>
      <name>Yukang Jiang</name>
    </author>
    <author>
      <name>Naoto Miyoshi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22pages, 2 figures (2 subfigures in each)</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.03404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.03404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G55, 60K30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; C.4; H.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.05503v1</id>
    <updated>2020-06-07T07:50:59Z</updated>
    <published>2020-06-07T07:50:59Z</published>
    <title>Stochastic Automata Network for Performance Evaluation of Heterogeneous
  SoC Communication</title>
    <summary>  To meet ever increasing demand for performance of emerging System-on-Chip
(SoC) applications, designer employ techniques for concurrent communication
between components. Hence communication architecture becomes complex and major
performance bottleneck. An early performance evaluation of communication
architecture is the key to reduce design time, time-to-market and consequently
cost of the system. Moreover, it helps to optimize system performance by
selecting appropriate communication architecture. However, performance model of
concurrent communication is complex to describe and hard to solve. In this
paper, we propose methodology for performance evaluation of bus based
communication architectures, modeling for which is based on modular Stochastic
Automata Network (SAN). We employ Generalized Semi Markov Process (GSMP) model
for each module of the SAN that emulates dynamic behavior of a Processing
Element (PE) of an SoC architecture. The proposed modeling approach provides an
early estimation of performance parameters viz. memory bandwidth, average queue
length at memory and average waiting time seen by a processing element; while
we provide parameters viz. number of processing elements, the mean computation
time of processing elements and the first and second moments of connection time
between processing elements and memories, as input to the model.
</summary>
    <author>
      <name>Ulhas Deshmukh</name>
    </author>
    <author>
      <name>Vineet Sahula</name>
    </author>
    <link href="http://arxiv.org/abs/2006.05503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.05503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.15463v1</id>
    <updated>2020-06-27T22:44:52Z</updated>
    <published>2020-06-27T22:44:52Z</published>
    <title>Queues with Small Advice</title>
    <summary>  Motivated by recent work on scheduling with predicted job sizes, we consider
the performance of scheduling algorithms with minimal advice, namely a single
bit. Besides demonstrating the power of very limited advice, such schemes are
quite natural. In the prediction setting, one bit of advice can be used to
model a simple prediction as to whether a job is "large" or "small"; that is,
whether a job is above or below a given threshold. Further, one-bit advice
schemes can correspond to mechanisms that tell whether to put a job at the
front or the back for the queue, a limitation which may be useful in many
implementation settings. Finally, queues with a single bit of advice have a
simple enough state that they can be analyzed in the limiting mean-field
analysis framework for the power of two choices. Our work follows in the path
of recent work by showing that even small amounts of even possibly inaccurate
information can greatly improve scheduling performance.
</summary>
    <author>
      <name>Michael Mitzenmacher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, draft version, to be submitted, subject to cahnge</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.15463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.15463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.16368v1</id>
    <updated>2020-06-29T20:45:52Z</updated>
    <published>2020-06-29T20:45:52Z</published>
    <title>Probabilistic Bounds on the End-to-End Delay of Service Function Chains
  using Deep MDN</title>
    <summary>  Ensuring the conformance of a service system's end-to-end delay to service
level agreement (SLA) constraints is a challenging task that requires
statistical measures beyond the average delay. In this paper, we study the
real-time prediction of the end-to-end delay distribution in systems with
composite services such as service function chains. In order to have a general
framework, we use queueing theory to model service systems, while also adopting
a statistical learning approach to avoid the limitations of queueing-theoretic
methods such as stationarity assumptions or other approximations that are often
used to make the analysis mathematically tractable. Specifically, we use deep
mixture density networks (MDN) to predict the end-to-end distribution of the
delay given the network's state. As a result, our method is sufficiently
general to be applied in different contexts and applications. Our evaluations
show a good match between the learned distributions and the simulations, which
suggest that the proposed method is a good candidate for providing
probabilistic bounds on the end-to-end delay of more complex systems where
simulations or theoretical methods are not applicable.
</summary>
    <author>
      <name>Majid Raeis</name>
    </author>
    <author>
      <name>Ali Tizghadam</name>
    </author>
    <author>
      <name>Alberto Leon-Garcia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, to be presented at IEEE PIMRC 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.16368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.16368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.01222v1</id>
    <updated>2020-07-02T16:11:03Z</updated>
    <published>2020-07-02T16:11:03Z</published>
    <title>COCOA: Cold Start Aware Capacity Planning for Function-as-a-Service
  Platforms</title>
    <summary>  Function-as-a-Service (FaaS) is increasingly popular in the software industry
due to the implied cost-savings in event-driven workloads and its synergy with
DevOps. To size an on-premise FaaS platform, it is important to estimate the
required CPU and memory capacity to serve the expected loads. Given the
service-level agreements, it is however challenging to take the cold start
issue into account during the sizing process. We have investigated the
similarity of this problem with the hit rate improvement problem in TTL caches
and concluded that solutions for TTL cache, although potentially applicable,
lead to over-provisioning in FaaS. Thus, we propose a novel approach, COCOA, to
solve this issue. COCOA uses a queueing-based approach to assess the effect of
cold starts on FaaS response times. It also considers different memory
consumption values depending on whether the function is idle or in execution.
Using an event-driven FaaS simulator, FaasSim, we have developed, we show that
COCOA can reduce over-provisioning by over 70% in some workloads, while
satisfying the service-level agreements.
</summary>
    <author>
      <name>Alim Ul Gias</name>
    </author>
    <author>
      <name>Giuliano Casale</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.01222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.01222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.01463v2</id>
    <updated>2021-03-01T06:49:22Z</updated>
    <published>2020-07-03T02:31:09Z</published>
    <title>Flexibility in an asymmetric system with prolonged service time at
  non-dedicated servers</title>
    <summary>  The prolonged service time at non-dedicated servers has been observed in [1].
Motivated by such real problems, we propose a stylized model which
characterizes the feature of the prolonged service time at non-dedicated
servers in an asymmetric system. We study the independent system, the full
flexibility system and the partial flexibility system when the occupation rate
of the system, the degree of the prolonged service time and the degree of the
asymmetry are allowed to change. We show that under certain circumstances, the
partial flexibility scheme outperforms the full flexibility system and the
independent system in such a model. Our results also provide instructions on
how to introduce flexibility when the service time at non-dedicated servers is
prolonged in an asymmetric system.
</summary>
    <author>
      <name>Yanting Chen</name>
    </author>
    <author>
      <name>Jingui Xie</name>
    </author>
    <author>
      <name>Taozeng Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.01463v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.01463v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.11650v1</id>
    <updated>2020-07-22T20:05:01Z</updated>
    <published>2020-07-22T20:05:01Z</published>
    <title>Discrete-time Queueing Model of Age of Information with Multiple
  Information Sources</title>
    <summary>  Information freshness in IoT-based status update systems has recently been
studied through the Age of Information (AoI) and Peak AoI (PAoI) performance
metrics. In this paper, we study a discrete-time server arising in multi-source
IoT systems which accepts incoming information packets from multiple
information sources so as to be forwarded to a remote monitor for status update
purposes. Under the assumption of Bernoulli information packet arrivals and a
common geometric service time distribution across all the sources, we
numerically obtain the exact per-source distributions of AoI and PAoI in
matrix-geometric form for three different queueing disciplines: i)
Non-Preemptive Bufferless (NPB) ii) Preemptive Bufferless (PB) iii)
Non-Preemptive Single Buffer with Replacement (NPSBR). The proposed numerical
algorithm employs the theory of Discrete-Time Markov Chains (DTMC) of
Quasi-Birth-Death (QBD) type and is matrix analytical, i.e, the algorithm is
based on numerically stable and efficient vector-matrix operations.Numerical
examples are provided to validate the accuracy and effectiveness of the
proposed queueing model. We also present a numerical example on the optimum
choice of the Bernoulli parameters in a practical IoT system with two sources
with diverse AoI requirements.
</summary>
    <author>
      <name>Nail Akar</name>
    </author>
    <author>
      <name>Ozancan Dogan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JIOT.2021.3053768</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JIOT.2021.3053768" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Internet of Things Journal 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2007.11650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.11650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.15314v1</id>
    <updated>2020-07-30T08:51:33Z</updated>
    <published>2020-07-30T08:51:33Z</published>
    <title>Delay and Price Differentiation in Cloud Computing: A Service Model,
  Supporting Architectures, and Performance</title>
    <summary>  Many cloud service providers (CSPs) provide on-demand service at a price with
a small delay. We propose a QoS-differentiated model where multiple SLAs
deliver both on-demand service for latency-critical users and delayed services
for delay-tolerant users at lower prices. Two architectures are considered to
fulfill SLAs. The first is based on priority queues. The second simply
separates servers into multiple modules, each for one SLA. As an ecosystem, we
show that the proposed framework is dominant-strategy incentive compatible.
Although the first architecture appears more prevalent in the literature, we
prove the superiority of the second architecture, under which we further
leverage queueing theory to determine the optimal SLA delays and prices.
Finally, the viability of the proposed framework is validated through numerical
comparison with the on-demand service and it exhibits a revenue improvement in
excess of 200%. Our results can help CSPs design optimal delay-differentiated
services and choose appropriate serving architectures.
</summary>
    <author>
      <name>Xiaohu Wu</name>
    </author>
    <author>
      <name>Francesco De Pellegrini</name>
    </author>
    <author>
      <name>Giuliano Casale</name>
    </author>
    <link href="http://arxiv.org/abs/2007.15314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.15314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.06613v2</id>
    <updated>2021-10-30T02:59:33Z</updated>
    <published>2020-12-11T19:40:19Z</published>
    <title>Beyond Scaling: Calculable Error Bounds of the Power-of-Two-Choices
  Mean-Field Model in Heavy-Traffic</title>
    <summary>  This paper provides a recipe for deriving calculable approximation errors of
mean-field models in heavy-traffic with the focus on the well-known load
balancing algorithm -- power-of-two-choices (Po2). The recipe combines Stein's
method for linearized mean-field models and State Space Concentration (SSC)
based on geometric tail bounds. In particular, we divide the state space into
two regions, a neighborhood near the mean-field equilibrium and the complement
of that. We first use a tail bound to show that the steady-state probability
being outside the neighborhood is small. Then, we use a linearized mean-field
model and Stein's method to characterize the generator difference, which
provides the dominant term of the approximation error. From the dominant term,
we are able to obtain an asymptotically-tight bound and a nonasymptotic upper
bound, both are calculable bounds, not order-wise scaling results like most
results in the literature. Finally, we compared the theoretical bounds with
numerical evaluations to show the effectiveness of our results. We note that
the simulation results show that both bounds are valid even for small size
systems such as a system with only ten servers.
</summary>
    <author>
      <name>Fnu Hairi</name>
    </author>
    <author>
      <name>Xin Liu</name>
    </author>
    <author>
      <name>Lei Ying</name>
    </author>
    <link href="http://arxiv.org/abs/2012.06613v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.06613v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.07224v1</id>
    <updated>2020-12-14T02:51:47Z</updated>
    <published>2020-12-14T02:51:47Z</published>
    <title>Traffic Rate Network Tomography with Higher-Order Cumulants</title>
    <summary>  Network tomography aims at estimating source-destination traffic rates from
link traffic measurements. This inverse problem was formulated by Vardi in 1996
for Poisson traffic over networks operating under deterministic as well as
random routing regimes. In this paper we expand Vardi's second-order moment
matching rate estimation approach to higher-order cumulant matching with the
goal of increasing the column rank of the mapping and consequently improving
the rate estimation accuracy. We develop a systematic set of linear cumulant
matching equations and express them compactly in terms of the Khatri-Rao
product. Both least squares estimation and iterative minimum I-divergence
estimation are considered. We develop an upper bound on the mean squared error
(MSE) in least squares rate estimation from empirical cumulants. We demonstrate
for the NSFnet that supplementing Vardi's approach with third-order empirical
cumulant reduces its averaged normalized MSE relative to the theoretical
minimum of the second-order moment matching approach by about 12%-18%. This
minimum MSE is obtained when Vardi's second-order moment matching approach is
based on the theoretical rather than the empirical moments.
</summary>
    <author>
      <name>Hanoch Lev-Ari</name>
    </author>
    <author>
      <name>Yariv Ephraim</name>
    </author>
    <author>
      <name>Brian L. Mark</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.07224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.07224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62P30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.09064v1</id>
    <updated>2020-12-16T16:34:39Z</updated>
    <published>2020-12-16T16:34:39Z</published>
    <title>Exponential Convergence Rate for the Asymptotic Optimality of Whittle
  Index Policy</title>
    <summary>  We evaluate the performance of Whittle index policy for restless Markovian
bandits, when the number of bandits grows. It is proven in [30] that this
performance is asymptotically optimal if the bandits are indexable and the
associated deterministic system has a global attractor fixed point. In this
paper we show that, under the same conditions, the convergence rate is
exponential in the number of bandits, unless the fixed point is singular (to be
defined later). Our proof is based on the nature of the deterministic equation
governing the stochastic system: We show that it is a piecewise affine
continuous dynamical system inside the simplex of the empirical measure of the
bandits. Using simulations and numerical solvers, we also investigate the cases
where the conditions for the exponential rate theorem are violated, notably
when attracting limit cycles appear, or when the fixed point is singular. We
illustrate our theorem on a Markovian fading channel model, which has been well
studied in the literature. Finally, we extend our synchronous model results to
the asynchronous model.
</summary>
    <author>
      <name>Nicolas Gast</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">POLARIS</arxiv:affiliation>
    </author>
    <author>
      <name>Bruno Gaujal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">POLARIS</arxiv:affiliation>
    </author>
    <author>
      <name>Chen Yan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">POLARIS</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2012.09064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.09064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.07129v1</id>
    <updated>2021-01-18T15:44:05Z</updated>
    <published>2021-01-18T15:44:05Z</published>
    <title>Verifiable Failure Localization in Smart Grid under Cyber-Physical
  Attacks</title>
    <summary>  Cyber-physical attacks impose a significant threat to the smart grid, as the
cyber attack makes it difficult to identify the actual damage caused by the
physical attack. To defend against such attacks, various inference-based
solutions have been proposed to estimate the states of grid elements (e.g.,
transmission lines) from measurements outside the attacked area, out of which a
few have provided theoretical conditions for guaranteed accuracy. However,
these conditions are usually based on the ground truth states and thus not
verifiable in practice. To solve this problem, we develop (i) verifiable
conditions that can be tested based on only observable information, and (ii)
efficient algorithms for verifying the states of links (i.e., transmission
lines) within the attacked area based on these conditions. Our numerical
evaluations based on the Polish power grid and IEEE 300-bus system demonstrate
that the proposed algorithms are highly successful in verifying the states of
truly failed links, and can thus greatly help in prioritizing repairs during
the recovery process.
</summary>
    <author>
      <name>Yudi Huang</name>
    </author>
    <author>
      <name>Ting He</name>
    </author>
    <author>
      <name>Nilanjan Ray Chaudhuri</name>
    </author>
    <author>
      <name>Thomas La Porta</name>
    </author>
    <link href="http://arxiv.org/abs/2101.07129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.07129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.02916v1</id>
    <updated>2021-03-04T09:43:13Z</updated>
    <published>2021-03-04T09:43:13Z</published>
    <title>Consensus in Blockchain Systems with Low Network Throughput: A
  Systematic Mapping Study</title>
    <summary>  Blockchain technologies originate from cryptocurrencies. Thus, most
blockchain technologies assume an environment with a fast and stable network.
However, in some blockchain-based systems, e.g., supply chain management (SCM)
systems, some Internet of Things (IOT) nodes can only rely on the low-quality
network sometimes to achieve consensus. Thus, it is critical to understand the
applicability of existing consensus algorithms in such environments. We
performed a systematic mapping study to evaluate and compare existing consensus
mechanisms' capability to provide integrity and security with varying network
properties. Our study identified 25 state-of-the-art consensus algorithms from
published and preprint literature. We categorized and compared the consensus
algorithms qualitatively based on established performance and integrity metrics
and well-known blockchain security issues. Results show that consensus
algorithms rely on the synchronous network for correctness cannot provide the
expected integrity. Such consensus algorithms may also be vulnerable to
distributed-denial-of-service (DDOS) and routing attacks, given limited network
throughput. Conversely, asynchronous consensus algorithms, e.g.,
Honey-BadgerBFT, are deemed more robust against many of these attacks and may
provide high integrity in asynchrony events.
</summary>
    <author>
      <name>Henrik Knudsen</name>
    </author>
    <author>
      <name>Jakob Svennevik Notland</name>
    </author>
    <author>
      <name>Peter Halland Haro</name>
    </author>
    <author>
      <name>Truls Bakkejord Ræder</name>
    </author>
    <author>
      <name>Jingyue Li</name>
    </author>
    <link href="http://arxiv.org/abs/2103.02916v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.02916v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.03222v1</id>
    <updated>2021-02-09T09:52:01Z</updated>
    <published>2021-02-09T09:52:01Z</published>
    <title>Modified Erlang loss system for cognitive wireless networks</title>
    <summary>  This paper considers a modified Erlang loss system for cognitive wireless
networks and related applications. A primary user has preemptive priority over
secondary users and the primary customer is lost if upon arrival all the
channels are used by other primary users. Secondary users cognitively use idle
channels and they can wait at an infinite buffer in cases idle channels are not
available upon arrival or they are interrupted by primary users. We obtain
explicit stability condition for the cases where arrival processes of primary
users and secondary users follow Poisson processes and their service times
follow two distinct arbitrary distributions. The stability condition is
insensitive to the service time distributions and implies the maximal
throughout of secondary users. For a special case of exponential service time
distributions, we analyze in depth to show the effect of parameters on the
delay performance and the mean number of interruptions of secondary users. Our
simulations for distributions rather than exponential reveal that the mean
number of terminations for secondary users is less sensitive to the service
time distribution of primary users.
</summary>
    <author>
      <name>E. V. Morozov</name>
    </author>
    <author>
      <name>S. S. Rogozin</name>
    </author>
    <author>
      <name>H. Q. Nguyen</name>
    </author>
    <author>
      <name>T. Phung-Duc</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Journal of Mathematical Sciences</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.03222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.03222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.06775v1</id>
    <updated>2021-03-11T16:35:39Z</updated>
    <published>2021-03-11T16:35:39Z</published>
    <title>ESPBench: The Enterprise Stream Processing Benchmark</title>
    <summary>  Growing data volumes and velocities in fields such as Industry 4.0 or the
Internet of Things have led to the increased popularity of data stream
processing systems. Enterprises can leverage these developments by enriching
their core business data and analyses with up-to-date streaming data. Comparing
streaming architectures for these complex use cases is challenging, as existing
benchmarks do not cover them. ESPBench is a new enterprise stream processing
benchmark that fills this gap. We present its architecture, the benchmarking
process, and the query workload. We employ ESPBench on three state-of-the-art
stream processing systems, Apache Spark, Apache Flink, and Hazelcast Jet, using
provided query implementations developed with Apache Beam. Our results
highlight the need for the provided ESPBench toolkit that supports benchmark
execution, as it enables query result validation and objective latency
measures.
</summary>
    <author>
      <name>Guenter Hesse</name>
    </author>
    <author>
      <name>Christoph Matthies</name>
    </author>
    <author>
      <name>Michael Perscheid</name>
    </author>
    <author>
      <name>Matthias Uflacker</name>
    </author>
    <author>
      <name>Hasso Plattner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3427921.3450242</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3427921.3450242" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as full research paper at ACM/SPEC International Conference
  on Performance Engineering 2021 (ICPE 21)</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.06775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.06775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.10942v1</id>
    <updated>2021-03-17T09:04:56Z</updated>
    <published>2021-03-17T09:04:56Z</published>
    <title>A Survey of Stability Results for Redundancy Systems</title>
    <summary>  Redundancy mechanisms consist in sending several copies of a same job to a
subset of servers. It constitutes one of the most promising ways to exploit
diversity in multiservers applications. However, its pros and cons are still
not sufficiently understood in the context of realistic models with generic
statistical properties of service-times distributions and correlation
structures of copies. We aim at giving a survey of recent results concerning
the stability-arguably the first benchmark of performance-of systems with
cancel-oncompletion redundancy. We also point out open questions and
conjectures.
</summary>
    <author>
      <name>Elene Anton</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT, Toulouse INP</arxiv:affiliation>
    </author>
    <author>
      <name>Urtzi Ayesta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT, Toulouse INP, UPV/EHU</arxiv:affiliation>
    </author>
    <author>
      <name>Matthieu Jonckheere</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT, Toulouse INP</arxiv:affiliation>
    </author>
    <author>
      <name>Ina Maria Verloop</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT, Toulouse INP</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">A.B. Piunovskiy and Y. Zhang (eds.), Modern Trends in Controlled
  Stochastic Processes : Theory and Applications, Volume III, Springer, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2103.10942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.10942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.13042v1</id>
    <updated>2021-03-24T08:09:59Z</updated>
    <published>2021-03-24T08:09:59Z</published>
    <title>Accelerating Sparse Approximate Matrix Multiplication on GPUs</title>
    <summary>  Although the matrix multiplication plays a vital role in computational linear
algebra, there are few efficient solutions for matrix multiplication of the
near-sparse matrices. The Sparse Approximate Matrix Multiply (SpAMM) is one of
the algorithms to fill the performance gap neglected by traditional
optimizations for dense/sparse matrix multiplication. However, existing SpAMM
algorithms fail to exploit the performance potential of GPUs for acceleration.
In this paper, we present cuSpAMM, the first parallel SpAMM algorithm optimized
for multiple GPUs. Several performance optimizations have been proposed,
including algorithm re-design to adapt to the thread parallelism, blocking
strategies for memory access optimization, and the acceleration with the tensor
core. In addition, we scale cuSpAMM to run on multiple GPUs with an effective
load balance scheme. We evaluate cuSpAMM on both synthesized and real-world
datasets on multiple GPUs. The experiment results show that cuSpAMM achieves
significant performance speedup compared to vendor optimized cuBLAS and
cuSPARSE libraries.
</summary>
    <author>
      <name>Xiaoyan Liu</name>
    </author>
    <author>
      <name>Yi Liu</name>
    </author>
    <author>
      <name>Ming Dun</name>
    </author>
    <author>
      <name>Bohong Yin</name>
    </author>
    <author>
      <name>Hailong Yang</name>
    </author>
    <author>
      <name>Zhongzhi Luan</name>
    </author>
    <author>
      <name>Depei Qian</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11227-022-04334-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11227-022-04334-5" rel="related"/>
    <link href="http://arxiv.org/abs/2103.13042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.13042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.14404v2</id>
    <updated>2021-03-30T05:33:14Z</updated>
    <published>2021-03-26T11:22:56Z</published>
    <title>ReaDmE: Read-Rate Based Dynamic Execution Scheduling for Intermittent
  RF-Powered Devices</title>
    <summary>  This paper presents a method for remotely and dynamically determining the
execution schedule of long-running tasks on intermittently powered devices such
as computational RFID. Our objective is to prevent brown-out events caused by
sudden power-loss due to the intermittent nature of the powering channel. We
formulate, validate and demonstrate that the read-rate measured from an RFID
reader (number of successful interrogations per second) can provide an adequate
means of estimating the powering channel condition for passively powered CRFID
devices. This method is attractive because it can be implemented without
imposing an added burden on the device or requiring additional hardware. We
further propose ReaDmE, a dynamic execution scheduling scheme to mitigate
brownout events to support long-run execution of complex tasks, such as
cryptographic algorithms, on CRFID. Experimental results demonstrate that the
ReaDmE method can improve CRFID's long-run execution success rate by 20% at the
critical operational range or reduce time overhead by up to 23% compared to
previous execution scheduling methods.
</summary>
    <author>
      <name>Yang Su</name>
    </author>
    <author>
      <name>Damith C. Ranasinghe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEEE RFID 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.14404v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.14404v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.00258v1</id>
    <updated>2021-04-01T05:30:03Z</updated>
    <published>2021-04-01T05:30:03Z</published>
    <title>Pinpointing the Memory Behaviors of DNN Training</title>
    <summary>  The training of deep neural networks (DNNs) is usually memory-hungry due to
the limited device memory capacity of DNN accelerators. Characterizing the
memory behaviors of DNN training is critical to optimize the device memory
pressures. In this work, we pinpoint the memory behaviors of each device memory
block of GPU during training by instrumenting the memory allocators of the
runtime system. Our results show that the memory access patterns of device
memory blocks are stable and follow an iterative fashion. These observations
are useful for the future optimization of memory-efficient training from the
perspective of raw memory access patterns.
</summary>
    <author>
      <name>Jiansong Li</name>
    </author>
    <author>
      <name>Xiao Dong</name>
    </author>
    <author>
      <name>Guangli Li</name>
    </author>
    <author>
      <name>Peng Zhao</name>
    </author>
    <author>
      <name>Xueying Wang</name>
    </author>
    <author>
      <name>Xiaobing Chen</name>
    </author>
    <author>
      <name>Xianzhi Yu</name>
    </author>
    <author>
      <name>Yongxin Yang</name>
    </author>
    <author>
      <name>Zihan Jiang</name>
    </author>
    <author>
      <name>Wei Cao</name>
    </author>
    <author>
      <name>Lei Liu</name>
    </author>
    <author>
      <name>Xiaobing Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ISPASS'21 poster</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.00258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.00258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.03388v1</id>
    <updated>2021-04-07T20:49:36Z</updated>
    <published>2021-04-07T20:49:36Z</published>
    <title>DJXPerf: Identifying Memory Inefficiencies via Object-centric Profiling
  for Java</title>
    <summary>  Java is the "go-to" programming language choice for developing scalable
enterprise cloud applications. In such systems, even a few percent CPU time
savings can offer a significant competitive advantage and cost saving. Although
performance tools abound in Java, those that focus on the data locality in the
memory hierarchy are rare.
  In this paper, we present DJXPerf, a lightweight, object-centric memory
profiler for Java, which associates memory-hierarchy performance metrics (e.g.,
cache/TLB misses) with Java objects. DJXPerf uses statistical sampling of
hardware performance monitoring counters to attribute metrics to not only
source code locations but also Java objects. DJXPerf presents Java object
allocation contexts combined with their usage contexts and presents them
ordered by the poor locality behaviors. DJXPerf's performance measurement,
object attribution, and presentation techniques guide optimizing object
allocation, layout, and access patterns. DJXPerf incurs only ~8% runtime
overhead and ~5% memory overhead on average, requiring no modifications to
hardware, OS, Java virtual machine, or application source code, which makes it
attractive to use in production. Guided by DJXPerf, we study and optimize a
number of Java and Scala programs, including well-known benchmarks and
real-world applications, and demonstrate significant speedups.
</summary>
    <author>
      <name>Bolun Li</name>
    </author>
    <author>
      <name>Pengfei Su</name>
    </author>
    <author>
      <name>Milind Chabbi</name>
    </author>
    <author>
      <name>Shuyin Jiao</name>
    </author>
    <author>
      <name>Xu Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages (including 2-page reference), 5 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.03388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.03388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05102v1</id>
    <updated>2021-04-11T20:49:55Z</updated>
    <published>2021-04-11T20:49:55Z</published>
    <title>PPT-Multicore: Performance Prediction of OpenMP applications using Reuse
  Profiles and Analytical Modeling</title>
    <summary>  We present PPT-Multicore, an analytical model embedded in the Performance
Prediction Toolkit (PPT) to predict parallel application performance running on
a multicore processor. PPT-Multicore builds upon our previous work towards a
multicore cache model. We extract LLVM basic block labeled memory trace using
an architecture-independent LLVM-based instrumentation tool only once in an
application's lifetime. The model uses the memory trace and other parameters
from an instrumented sequentially executed binary. We use a probabilistic and
computationally efficient reuse profile to predict the cache hit rates and
runtimes of OpenMP programs' parallel sections. We model Intel's Broadwell,
Haswell, and AMD's Zen2 architectures and validate our framework using
different applications from PolyBench and PARSEC benchmark suites. The results
show that PPT-Multicore can predict cache hit rates with an overall average
error rate of 1.23% while predicting the runtime with an error rate of 9.08%.
</summary>
    <author>
      <name>Atanu Barai</name>
    </author>
    <author>
      <name>Yehia Arafa</name>
    </author>
    <author>
      <name>Abdel-Hameed Badawy</name>
    </author>
    <author>
      <name>Gopinath Chennupati</name>
    </author>
    <author>
      <name>Nandakishore Santhi</name>
    </author>
    <author>
      <name>Stephan Eidenbenz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11227-021-03949-4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11227-021-03949-4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2103.10635. J Supercomput
  (2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.05102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05829v1</id>
    <updated>2021-04-12T21:32:01Z</updated>
    <published>2021-04-12T21:32:01Z</published>
    <title>NekRS, a GPU-Accelerated Spectral Element Navier-Stokes Solver</title>
    <summary>  The development of NekRS, a GPU-oriented thermal-fluids simulation code based
on the spectral element method (SEM) is described. For performance portability,
the code is based on the open concurrent compute abstraction and leverages
scalable developments in the SEM code Nek5000 and in libParanumal, which is a
library of high-performance kernels for high-order discretizations and
PDE-based miniapps. Critical performance sections of the Navier-Stokes time
advancement are addressed. Performance results on several platforms are
presented, including scaling to 27,648 V100s on OLCF Summit, for calculations
of up to 60B gridpoints.
</summary>
    <author>
      <name>Paul Fischer</name>
    </author>
    <author>
      <name>Stefan Kerkemeier</name>
    </author>
    <author>
      <name>Misun Min</name>
    </author>
    <author>
      <name>Yu-Hsiang Lan</name>
    </author>
    <author>
      <name>Malachi Phillips</name>
    </author>
    <author>
      <name>Thilina Rathnayake</name>
    </author>
    <author>
      <name>Elia Merzari</name>
    </author>
    <author>
      <name>Ananias Tomboulides</name>
    </author>
    <author>
      <name>Ali Karakus</name>
    </author>
    <author>
      <name>Noel Chalmers</name>
    </author>
    <author>
      <name>Tim Warburton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.05829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="35-04" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.0; F.2; G.2; G.4; I.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.08050v3</id>
    <updated>2021-10-17T15:19:12Z</updated>
    <published>2021-04-16T11:53:10Z</published>
    <title>Age of information without service preemption</title>
    <summary>  When designing a message transmission system, from the point of view of
making sure that the information transmitted is as fresh as possible, two rules
of thumb seem reasonable: use small buffers and adopt a last-in-first-out
policy. In this paper, we measure freshness of information using the "age of
information" performance measure. Considering it as a stochastic process
operating in a stationary regime, we compute not just the first moment but the
whole marginal distribution of the age of information (something important in
applications) for two well-performing systems. In neither case do we allow for
preemption of the message being processed because this may be difficult to
implement in practice. We assume that the arrival process is Poisson and that
the messages have independent sizes (service times) with common distribution.
We use Palm and Markov-renewal theory to derive explicit results for Laplace
transforms. In particular, this approach can be used to analyze more complex
last-in-first-out systems with larger buffer sizes.
</summary>
    <author>
      <name>George Kesidis</name>
    </author>
    <author>
      <name>Takis Konstantopoulos</name>
    </author>
    <author>
      <name>Michael A. Zazanis</name>
    </author>
    <link href="http://arxiv.org/abs/2104.08050v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08050v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.09273v1</id>
    <updated>2021-04-19T13:16:46Z</updated>
    <published>2021-04-19T13:16:46Z</published>
    <title>Asymptotic analysis of the sojourn time of a batch in an $M^{[X]}/M/1$
  Processor Sharing Queue</title>
    <summary>  In this paper, we exploit results obtained in an earlier study for the
Laplace transform of the sojourn time $\Omega$ of an entire batch in the
$M^{[X]}/M/1$ Processor Sharing (PS) queue in order to derive the asymptotic
behavior of the complementary probability distribution function of this random
variable, namely the behavior of $P(\Omega&gt;x)$ when $x$ tends to infinity. We
precisely show that up to a multiplying factor, the behavior of $P(\Omega&gt;x)$
for large $x$ is of the same order of magnitude as $P(\omega&gt;x)$, where
$\omega$ is the sojourn time of an arbitrary job is the system. From a
practical point of view, this means that if a system has to be dimensioned to
guarantee processing time for jobs then the system can also guarantee
processing times for entire batches by introducing a marginal amount of
processing capacity.
</summary>
    <author>
      <name>Fabrice Guillemin</name>
    </author>
    <author>
      <name>Alain Simonian</name>
    </author>
    <author>
      <name>Ridha Nasri</name>
    </author>
    <author>
      <name>Veronica Quintuna Rodriguez</name>
    </author>
    <link href="http://arxiv.org/abs/2104.09273v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.09273v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.11393v3</id>
    <updated>2021-10-17T12:06:59Z</updated>
    <published>2021-04-23T03:04:41Z</published>
    <title>Age of information distribution under dynamic service preemption</title>
    <summary>  Age of Information (AoI) has emerged as an important quality-of-service
measure for applications that prioritize delivery of the freshest information,
e.g., virtual or augmented reality over mobile devices and wireless sensor
networks used in the control of cyber-physical systems. We derive the Laplace
transform of the stationary AoI for the M/GI/1/2 system with a "dynamic"
service preemption and pushout policy depending on the existing service time of
the in-service message. Thus, our system generalizes both the static M/GI/1/2
queue-pushout system without service preemption and the M/GI/1/1 bufferless
system with service preemption - two systems considered to provide very good
AoI performance. Based on our analysis, for a service-time distribution that is
a mixture of deterministic and exponential, we numerically show that the
dynamic policy has lower mean AoI than that of these two static policies and
also that of the well studied M/GI/1/1 blocking system.
</summary>
    <author>
      <name>George Kesidis</name>
    </author>
    <author>
      <name>Takis Konstantopoulos</name>
    </author>
    <author>
      <name>Michael A. Zazanis</name>
    </author>
    <link href="http://arxiv.org/abs/2104.11393v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.11393v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.01792v1</id>
    <updated>2021-05-04T23:10:43Z</updated>
    <published>2021-05-04T23:10:43Z</published>
    <title>Aggregate Cyber-Risk Management in the IoT Age: Cautionary Statistics
  for (Re)Insurers and Likes</title>
    <summary>  In this paper, we provide (i) a rigorous general theory to elicit conditions
on (tail-dependent) heavy-tailed cyber-risk distributions under which a risk
management firm might find it (non)sustainable to provide aggregate cyber-risk
coverage services for smart societies, and (ii)a real-data driven numerical
study to validate claims made in theory assuming boundedly rational cyber-risk
managers, alongside providing ideas to boost markets that aggregate dependent
cyber-risks with heavy-tails.To the best of our knowledge, this is the only
complete general theory till date on the feasibility of aggregate cyber-risk
management.
</summary>
    <author>
      <name>Ranjan Pal</name>
    </author>
    <author>
      <name>Ziyuan Huang</name>
    </author>
    <author>
      <name>Xinlong Yin</name>
    </author>
    <author>
      <name>Sergey Lototsky</name>
    </author>
    <author>
      <name>Swades De</name>
    </author>
    <author>
      <name>Sasu Tarkoma</name>
    </author>
    <author>
      <name>Mingyan Liu</name>
    </author>
    <author>
      <name>Jon Crowcroft</name>
    </author>
    <author>
      <name>Nishanth Sastry</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JIOT.2020.3039254</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JIOT.2020.3039254" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">incrementally updated version to version in IEEE Internet of Things
  Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.01792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.01792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.RM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.01492v1</id>
    <updated>2021-06-02T22:18:43Z</updated>
    <published>2021-06-02T22:18:43Z</published>
    <title>Nudge: Stochastically Improving upon FCFS</title>
    <summary>  The First-Come First-Served (FCFS) scheduling policy is the most popular
scheduling algorithm used in practice. Furthermore, its usage is theoretically
validated: for light-tailed job size distributions, FCFS has weakly optimal
asymptotic tail of response time. But what if we don't just care about the
asymptotic tail? What if we also care about the 99th percentile of response
time, or the fraction of jobs that complete in under one second? Is FCFS still
best? Outside of the asymptotic regime, only loose bounds on the tail of FCFS
are known, and optimality is completely open.
  In this paper, we introduce a new policy, Nudge, which is the first policy to
provably stochastically improve upon FCFS. We prove that Nudge simultaneously
improves upon FCFS at every point along the tail, for light-tailed job size
distributions. As a result, Nudge outperforms FCFS for every moment and every
percentile of response time. Moreover, Nudge provides a multiplicative
improvement over FCFS in the asymptotic tail. This resolves a long-standing
open problem by showing that, counter to previous conjecture, FCFS is not
strongly asymptotically optimal.
</summary>
    <author>
      <name>Isaac Grosof</name>
    </author>
    <author>
      <name>Kunhe Yang</name>
    </author>
    <author>
      <name>Ziv Scully</name>
    </author>
    <author>
      <name>Mor Harchol-Balter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3410220.3460102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3410220.3460102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 4 figures. To appear in SIGMETRICS 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.01492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.01492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.01470v2</id>
    <updated>2021-10-25T08:57:08Z</updated>
    <published>2021-08-02T12:33:59Z</published>
    <title>FIRESTARTER 2: Dynamic Code Generation for Processor Stress Tests</title>
    <summary>  Processor stress tests target to maximize processor power consumption by
executing highly demanding workloads. They are typically used to test the
cooling and electrical infrastructure of compute nodes or larger systems in
labs or data centers. While multiple of these tools already exists, they have
to be re-evaluated and updated regularly to match the developments in computer
architecture. This paper presents the first major update of FIRESTARTER, an
Open Source tool specifically designed to create near-peak power consumption.
The main new features concern the online generation of workloads and automatic
self-tuning for specific hardware configurations. We further apply these new
features on an AMD Rome system and demonstrate the optimization process. Our
analysis shows how accesses to the different levels of the memory hierarchy
contribute to the overall power consumption. Finally, we demonstrate how the
auto-tuning algorithm can cope with different processor configurations and how
these influence the effectiveness of the created workload.
</summary>
    <author>
      <name>Robert Schöne</name>
    </author>
    <author>
      <name>Markus Schmidl</name>
    </author>
    <author>
      <name>Mario Bielert</name>
    </author>
    <author>
      <name>Daniel Hackenberg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/Cluster48925.2021.00084</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/Cluster48925.2021.00084" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">supported in part by the German Research Foundation (DFG) within the
  CRC 912 - HAEC</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.01470v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.01470v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.09534v2</id>
    <updated>2023-08-12T03:15:37Z</updated>
    <published>2021-08-21T16:05:42Z</published>
    <title>Theoretical Analysis and Evaluation of NoCs with Weighted Round-Robin
  Arbitration</title>
    <summary>  Fast and accurate performance analysis techniques are essential in early
design space exploration and pre-silicon evaluations, including software
eco-system development. In particular, on-chip communication continues to play
an increasingly important role as the many-core processors scale up. This paper
presents the first performance analysis technique that targets networks-on-chip
(NoCs) that employ weighted round-robin (WRR) arbitration. Besides fairness,
WRR arbitration provides flexibility in allocating bandwidth proportionally to
the importance of the traffic classes, unlike basic round-robin and
priority-based arbitration. The proposed approach first estimates the effective
service time of the packets in the queue due to WRR arbitration. Then, it uses
the effective service time to compute the average waiting time of the packets.
Next, we incorporate a decomposition technique to extend the analytical model
to handle NoC of any size. The proposed approach achieves less than 5% error
while executing real applications and 10% error under challenging synthetic
traffic with different burstiness levels.
</summary>
    <author>
      <name>Sumit K. Mandal</name>
    </author>
    <author>
      <name>Jie Tong</name>
    </author>
    <author>
      <name>Raid Ayoub</name>
    </author>
    <author>
      <name>Michael Kishinevsky</name>
    </author>
    <author>
      <name>Ahmed Abousamra</name>
    </author>
    <author>
      <name>Umit Y. Ogras</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted in International Conference on Computer Aided
  Design (ICCAD), 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.09534v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.09534v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.04536v1</id>
    <updated>2021-09-09T19:58:40Z</updated>
    <published>2021-09-09T19:58:40Z</published>
    <title>Performance Analysis of CP2K Code for Ab Initio Molecular Dynamics</title>
    <summary>  Using a realistic molecular catalyst system, we conduct scaling studies of ab
initio molecular dynamics simulations using the CP2K code on both Intel Xeon
CPU and NVIDIA V100 GPU architectures. We explore using process placement and
affinity to gain additional performance improvements. We also use statistical
methods to understand performance changes in spite of the variability in
runtime for each molecular dynamics timestep. We found ideal conditions for CPU
runs included at least four MPI ranks per node, bound evenly across each
socket, and fully utilizing processing cores with one OpenMP thread per core,
no benefit was shown from reserving cores for the system. The CPU-only
simulations scaled at 70% or more of the ideal scaling up to 10 compute nodes,
after which the returns began to diminish more quickly. Simulations on a single
40-core node with two NVIDIA V100 GPUs for acceleration achieved over 3.7x
speedup compared to the fastest single 36-core node CPU-only version, and
showed 13% speedup over the fastest time we achieved across five CPU-only
nodes.
</summary>
    <author>
      <name>Dewi Yokelson</name>
    </author>
    <author>
      <name>Nikolay V. Tkachenko</name>
    </author>
    <author>
      <name>Robert Robey</name>
    </author>
    <author>
      <name>Ying Wai Li</name>
    </author>
    <author>
      <name>Pavel A. Dub</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.04536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.04536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.04621v1</id>
    <updated>2021-09-10T02:10:45Z</updated>
    <published>2021-09-10T02:10:45Z</published>
    <title>An Effective Early Multi-core System Shared Cache Design Method Based on
  Reuse-distance Analysis</title>
    <summary>  In this paper, we proposed an effective and efficient multi-core shared-cache
design optimization approach based on reuse-distance analysis of the data
traces of target applications. Since data traces are independent of system
hardware architectures, a designer can easily compute the best cache design at
the early system design phase using our approach. We devise a very efficient
and yet accurate method to derive the aggregated reuse-distance histograms of
concurrent applications for accurate cache performance analysis and
optimization. Essentially, the actual shared-cache contention results of
concurrent applications are embedded in the aggregated reuse-distance
histograms and therefore the approach is very effective. The experimental
results show that the average error rate of shared-cache miss-count estimations
of our approach is less than 2.4%. Using a simple scanning search method, one
can easily determine the true optimal cache configurations at the early system
design phase.
</summary>
    <author>
      <name>Hsin-Yu Ho</name>
    </author>
    <author>
      <name>Ren-Song Tsay</name>
    </author>
    <link href="http://arxiv.org/abs/2109.04621v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.04621v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.12317v1</id>
    <updated>2021-09-25T08:50:56Z</updated>
    <published>2021-09-25T08:50:56Z</published>
    <title>A fluid reservoir model for the Age of Information through
  energy-harvesting transmitters</title>
    <summary>  We apply a fluid-reservoir model to study the Age-of-Information (AoI) of
update packets through energy-harvesting transmitters. The model is closer to
how energy is stored and depleted in reality, and can reveal the system
behavior for different settings of packet arrival rates, service rates, and
energy charging and depletion rates. We present detailed results for both
finite and infinite transmitter buffers and an infinite energy reservoir, and
some indicative results for a finite reservoir. The results are derived for the
mean AoI in the case of an infinite transmitter buffer and an infinite
reservoir, and for the mean peak AoI for the remaining cases. The results show
that, similar to a system without energy constraints, the transmitter buffer
should be kept to a minimum in order to avoid queueing delays and maintain
freshness of updates. Furthermore, a high update packet rate is only helpful in
energy-rich regimes, whereas in energy-poor regimes more frequent updates
deplete the energy reservoir and result in higher AoI values.
</summary>
    <author>
      <name>Ioannis Z. Koukoutsidis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2021 International Symposium on Performance Evaluation of Computer
  and Telecommunication Systems (SPECTS 2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2109.12317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.12317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.12663v3</id>
    <updated>2022-06-12T18:48:40Z</updated>
    <published>2021-09-26T17:52:50Z</published>
    <title>WCFS: A new framework for analyzing multiserver systems</title>
    <summary>  Multiserver queueing systems are found at the core of a wide variety of
practical systems. Many important multiserver models have a
previously-unexplained similarity: identical mean response time behavior is
empirically observed in the heavy traffic limit. We explain this similarity for
the first time.
  We do so by introducing the work-conserving finite-skip (WCFS) framework,
which encompasses a broad class of important models. This class includes the
heterogeneous M/G/k, the limited processor sharing policy for the M/G/1, the
threshold parallelism model, and the multiserver-job model under a novel
scheduling algorithm.
  We prove that for all WCFS models, scaled mean response time $E[T](1-\rho)$
converges to the same value, $E[S^2]/(2E[S])$, in the heavy-traffic limit,
which is also the heavy traffic limit for the M/G/1/FCFS. Moreover, we prove
additively tight bounds on mean response time for the WCFS class, which hold
for all load $\rho$. For each of the four models mentioned above, our bounds
are the first known bounds on mean response time.
</summary>
    <author>
      <name>Isaac Grosof</name>
    </author>
    <author>
      <name>Mor Harchol-Balter</name>
    </author>
    <author>
      <name>Alan Scheller-Wolf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages. Under submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.12663v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.12663v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.14156v1</id>
    <updated>2021-09-29T02:54:26Z</updated>
    <published>2021-09-29T02:54:26Z</published>
    <title>Labor-right Protecting Dispatch of Meal Delivery Platforms</title>
    <summary>  The boom in the meal delivery industry brings growing concern about the labor
rights of riders. Current dispatch policies of meal-delivery platforms focus
mainly on satisfying consumers or minimizing the number of riders for cost
savings. There are few discussions on improving the working conditions of
riders by algorithm design. The lack of concerns on labor rights in mechanism
and dispatch design has resulted in a very large time waste for riders and
their risky driving. In this research, we propose a queuing-model-based
framework to discuss optimal dispatch policy with the goal of labor rights
protection. We apply our framework to develop an algorithm minimizing the
waiting time of food delivery riders with guaranteed user experience. Our
framework also allows us to manifest the value of restaurants' data about their
offline-order numbers on improving the benefits of riders.
</summary>
    <author>
      <name>Wentao Weng</name>
    </author>
    <author>
      <name>Yang Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.14156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.14156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.06326v3</id>
    <updated>2024-01-28T22:48:07Z</updated>
    <published>2021-10-12T20:23:01Z</published>
    <title>When Does the Gittins Policy Have Asymptotically Optimal Response Time
  Tail?</title>
    <summary>  We consider scheduling in the M/G/1 queue with unknown job sizes. It is known
that the Gittins policy minimizes mean response time in this setting. However,
the behavior of the tail of response time under Gittins is poorly understood,
even in the large-response-time limit. Characterizing Gittins's asymptotic tail
behavior is important because if Gittins has optimal tail asymptotics, then it
simultaneously provides optimal mean response time and good tail performance.
  In this work, we give the first comprehensive account of Gittins's asymptotic
tail behavior. For heavy-tailed job sizes, we find that Gittins always has
asymptotically optimal tail. The story for light-tailed job sizes is less
clear-cut: Gittins's tail can be optimal, pessimal, or in between. To remedy
this, we show that a modification of Gittins avoids pessimal tail behavior
while achieving near-optimal mean response time.
</summary>
    <author>
      <name>Ziv Scully</name>
    </author>
    <author>
      <name>Lucas van Kreveld</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Operations Research</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.06326v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.06326v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.09526v1</id>
    <updated>2021-10-18T16:50:28Z</updated>
    <published>2021-10-18T16:50:28Z</published>
    <title>Infinite Servers Queue Systems Busy Period Time Length Distribution and
  Parameters Study through Computational Simulation</title>
    <summary>  A FORTRAN program to simulate the operation of infinite servers queues is
presented in this work. Poisson arrivals processes are considered but not only.
For many parameters of interest in queuing systems study or application, either
there are not theoretical results or, existing, they are mathematically
intractable what makes their utility doubtful. In this case a possible issue is
to use simulation methods in order to get more useful results. Indeed, using
simulation, some experiences may be performed and the respective results used
to conjecture about certain queue systems interesting quantities. In this paper
this procedure is followed to learn something more about quantities of interest
for those infinite servers queue systems, in particular about busy period
parameters and probability distributions.
</summary>
    <author>
      <name>Manuel Alberto M. Ferreira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages and 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.09526v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.09526v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.11579v1</id>
    <updated>2021-10-22T04:15:19Z</updated>
    <published>2021-10-22T04:15:19Z</published>
    <title>How to Schedule Near-Optimally under Real-World Constraints</title>
    <summary>  Scheduling is a critical part of practical computer systems, and scheduling
has also been extensively studied from a theoretical perspective.
Unfortunately, there is a gap between theory and practice, as the optimal
scheduling policies presented by theory can be difficult or impossible to
perfectly implement in practice. In this work, we use recent breakthroughs in
queueing theory to begin to bridge this gap. We show how to translate
theoretically optimal policies -- which provably minimize mean response time
(a.k.a. latency) -- into near-optimal policies that are easily implemented in
practical settings. Specifically, we handle the following real-world
constraints:
  - We show how to schedule in systems where job sizes (a.k.a. running time)
are unknown, or only partially known. We do so using simple policies that
achieve performance very close to the much more complicated theoretically
optimal policies.
  - We show how to schedule in systems that have only a limited number of
priority levels available. We show how to adapt theoretically optimal policies
to this constrained setting and determine how many levels we need for
near-optimal performance.
  - We show how to schedule in systems where job preemption can only happen at
specific checkpoints. Adding checkpoints allows for smarter scheduling, but
each checkpoint incurs time overhead. We give a rule of thumb that
near-optimally balances this tradeoff.
</summary>
    <author>
      <name>Ziv Scully</name>
    </author>
    <author>
      <name>Mor Harchol-Balter</name>
    </author>
    <link href="http://arxiv.org/abs/2110.11579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.11579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.11719v1</id>
    <updated>2021-10-22T11:44:24Z</updated>
    <published>2021-10-22T11:44:24Z</published>
    <title>Experience with PCIe streaming on FPGA for high throughput ML
  inferencing</title>
    <summary>  Achieving maximum possible rate of inferencing with minimum hardware
resources plays a major role in reducing enterprise operational costs. In this
paper we explore use of PCIe streaming on FPGA based platforms to achieve high
throughput. PCIe streaming is a unique capability available on FPGA that
eliminates the need for memory copy overheads. We have presented our results
for inferences on a gradient boosted trees model, for online retail
recommendations. We compare the results achieved with the popular library
implementations on GPU and the CPU platforms and observe that the PCIe
streaming enabled FPGA implementation achieves the best overall measured
performance. We also measure power consumption across all platforms and find
that the PCIe streaming on FPGA platform achieves the 25x and 12x better energy
efficiency than an implementation on CPU and GPU platforms, respectively. We
discuss the conditions that need to be met, in order to achieve this kind of
acceleration on the FPGA. Further, we analyze the run time statistics on GPU
and FPGA and identify opportunities to enhance performance on both the
platforms.
</summary>
    <author>
      <name>Piyush Manavar</name>
    </author>
    <author>
      <name>Manoj Nambiar</name>
    </author>
    <author>
      <name>Nupur Sumeet</name>
    </author>
    <author>
      <name>Rekha Singhal</name>
    </author>
    <author>
      <name>Sharod Choudhary</name>
    </author>
    <author>
      <name>Amey Pandit</name>
    </author>
    <link href="http://arxiv.org/abs/2110.11719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.11719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.06981v1</id>
    <updated>2021-12-13T19:31:19Z</updated>
    <published>2021-12-13T19:31:19Z</published>
    <title>Public Release and Validation of SPEC CPU2017 PinPoints</title>
    <summary>  Phase-based statistical sampling methods such as SimPoints have proven to be
effective at dramatically reducing the long time for architectural simulators
to run large workloads such as SPEC CPU2017. However, generating and validating
them is a long and tenuous process. While checkpoints of program phases, or
"pinballs", of SPEC CPU2017 have been collected by other researchers and shared
with the research community, they are outdated and produce errors when used
with the latest versions of the Sniper architectural simulator. To facilitate
our own research as well as contribute to the community, we collect and
validate our own pinballs for the SPEC CPU2017 SPECspeed suite and release them
to the public domain. In this work we document our methodology, the hardware
and software details of the collection process, and our validation results. In
terms of CPI, our pinballs have an average error rate of 12% when compared with
the native whole-program benchmark execution.
</summary>
    <author>
      <name>Haiyang Han</name>
    </author>
    <author>
      <name>Nikos Hardavellas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.06981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.06981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.0; C.4; I.6.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.10037v1</id>
    <updated>2021-12-19T01:16:15Z</updated>
    <published>2021-12-19T01:16:15Z</published>
    <title>FSpGEMM: An OpenCL-based HPC Framework for Accelerating General Sparse
  Matrix-Matrix Multiplication on FPGAs</title>
    <summary>  General sparse matrix-matrix multiplication (SpGEMM) is an integral part of
many scientific computing, high-performance computing (HPC), and graph analytic
applications. This paper presents a new compressed sparse vector (CSV) format
for representing sparse matrices and FSpGEMM, an OpenCL-based HPC framework for
accelerating general sparse matrix-matrix multiplication on FPGAs. The proposed
FSpGEMM framework includes an FPGA kernel implementing a throughput-optimized
hardware architecture based on Gustavson's algorithm and a host program
implementing pre-processing functions for converting input matrices to the CSV
format tailored for the proposed architecture. FSpGEMM utilizes a new buffering
scheme tailored to Gustavson's algorithm. We compare FSpGEMM implemented on an
Intel Arria 10 GX FPGA development board with Intel Math Kernel Library (MKL)
implemented on an Intel Xeon E5-2637 CPU and cuSPARSE on an NVIDIA GTX TITAN X
GPU, respectively, for multiplying a set of sparse matrices selected from
SuiteSparse Matrix Collection. The experiment results show that the proposed
FSpGEMM solution achieves on average 4.9x and 1.7x higher performance with
31.9x and 13.1x lower energy consumption per SpGEMM computation than the CPU
and GPU implementations, respectively.
</summary>
    <author>
      <name>Erfan Bank Tavakoli</name>
    </author>
    <author>
      <name>Michael Riera</name>
    </author>
    <author>
      <name>Masudul Hassan Quraishi</name>
    </author>
    <author>
      <name>Fengbo Ren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.10037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.10037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.11277v1</id>
    <updated>2021-12-21T15:04:32Z</updated>
    <published>2021-12-21T15:04:32Z</published>
    <title>Porting a benchmark with a classic workload to blockchain: TPC-C on
  Hyperledger Fabric</title>
    <summary>  Many cross-organization cooperation applications of blockchain-based
distributed ledger technologies (DLT) do not aim at innovation at the
cooperation pattern level: essentially the same ''business'' is conducted by
the parties, but this time without a central party to be trusted with
bookkeeping. The migration to DLT is expected to have a negative performance
impact, but some DLTs, such as Hyperledger Fabric, are accepted to be much
better suited performance-wise to such use cases than others. However, with the
somewhat surprising, but ongoing absence of application-level performance
benchmarks for DLTs, cross-DLT comparison for "classic" workloads and the
evaluation of the performance impact of "blockchainification" is still
ill-supported. We present the design and Hyperledger Caliper-based open
implementation of a full port of the classic TPC-C benchmark to Hyperledger
Fabric, complete with a structured approach for transforming the original
database schema to a smart contract data model. Initial measurements about the
workload characteristics that will affect the design of large-scale performance
evaluations are also included.
</summary>
    <author>
      <name>Attila Klenik</name>
    </author>
    <author>
      <name>Imre Kocsis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3477314.3507006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3477314.3507006" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.11277v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.11277v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.11767v1</id>
    <updated>2021-12-22T10:08:58Z</updated>
    <published>2021-12-22T10:08:58Z</published>
    <title>Supporting RISC-V Performance Counters through Performance analysis
  tools for Linux (Perf)</title>
    <summary>  Increased attention to RISC-V in Cloud, Data Center, Automotive and
Networking applications, has been fueling the move of RISC-V to the
high-performance computing scenario. However, lack of powerful performance
monitoring tools will result in poorly optimized applications and,
consequently, a limited computing performance. While the RISC-V ISA already
defines a hardware performance monitor (HPM), current software gives limited
support for monitoring performance. In this paper we introduce extensions and
modifications to the Performance analysis tools for Linux(perf/perf_events),
Linux kernel, and OpenSBI, aiming to achieve full support for the RISC-V
performance monitoring specification. Preliminary testing and evaluation was
carried out in Linux 5.7 running on a FPGA booted CVA6 CPU, formerly named
Ariane, showing a monitoring overhead of 0.283%.
</summary>
    <author>
      <name>Joao Mario Domingos</name>
    </author>
    <author>
      <name>Pedro Tomas</name>
    </author>
    <author>
      <name>Leonel Sousa</name>
    </author>
    <link href="http://arxiv.org/abs/2112.11767v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.11767v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.12667v1</id>
    <updated>2021-12-23T16:08:38Z</updated>
    <published>2021-12-23T16:08:38Z</published>
    <title>Using Silent Writes in Low-Power Traffic-Aware ECC</title>
    <summary>  Using Error Detection Code (EDC) and Error Correction Code (ECC) is a
noteworthy way to increase cache memories robustness against soft errors. EDC
enables detecting errors in cache memory while ECC is used to correct erroneous
cache blocks. ECCs are often costly as they impose considerable area and energy
overhead on cache memory. Reducing this overhead has been the subject of many
studies. In particular, a previous study has suggested mapping ECC to the main
memory at the expense of high cache traffic and energy. A major source of this
excessive traffic and energy is the high frequency of cache writes. In this
work, we show that a significant portion of cache writes are silent, i.e., they
write the same data already existing. We build on this observation and
introduce Traffic-aware ECC (or simply TCC). TCC detects silent writes by an
efficient mechanism. Once such writes are detected updating their ECC is
avoided effectively reducing L2 cache traffic and access frequency. Using our
solution, we reduce L2 cache access frequency by 8% while maintaining
performance. We reduce L2 cache dynamic and overall cache energy by up to 32%
and 8%, respectively. Furthermore, TCC reduces L2 cache miss rate by 3%.
</summary>
    <author>
      <name>Mostafa Kishani</name>
    </author>
    <author>
      <name>Amirali Baniasadi</name>
    </author>
    <author>
      <name>Hossein Pedram</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-24154-3_19</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-24154-3_19" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PATMOS, pp. 180-192. Springer, Berlin, Heidelberg, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2112.12667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.12667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.00410v1</id>
    <updated>2022-02-28T07:26:44Z</updated>
    <published>2022-02-28T07:26:44Z</published>
    <title>Markovian Analysis of Coordination Strategies in Tandem Polling Queues
  with Setups</title>
    <summary>  We analyze a network of tandem polling queues with two stations operating
under synchronized polling (SP) and out-of-sync polling (OP) strategies, and
with nonzero setups. We conduct an exact analysis using a decomposition
approach to compare the performance in terms of throughput and mean waiting
times to investigate when one strategy might be preferred over the other. We
also numerically investigate the condition for network stability operating
under the two strategies and show that polling network is unstable when there
is bottleneck at downstream stations. We find that the SP strategy outperforms
the OP strategy in case of product and station symmetric networks while under
certain settings of product and station asymmetry, OP strategy outperforms the
SP strategy.
</summary>
    <author>
      <name>Ravi Suman</name>
    </author>
    <author>
      <name>Ananth Krishnamurthy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2202.10045</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.00410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.00410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.02530v2</id>
    <updated>2022-03-17T20:07:34Z</updated>
    <published>2022-03-04T19:17:00Z</published>
    <title>Machine Learning for CUDA+MPI Design Rules</title>
    <summary>  We present a new strategy for automatically exploring the design space of key
CUDA+MPI programs and providing design rules that discriminate slow from fast
implementations. In such programs, the order of operations (e.g., GPU kernels,
MPI communication) and assignment of operations to resources (e.g., GPU
streams) makes the space of possible designs enormous. Systems experts have the
task of redesigning and reoptimizing these programs to effectively utilize each
new platform. This work provides a prototype tool to reduce that burden.
  In our approach, a directed acyclic graph of CUDA and MPI operations defines
the design space for the program. Monte-Carlo tree search discovers regions of
the design space that have large impact on the program's performance. A
sequence-to-vector transformation defines features for each explored
implementation, and each implementation is assigned a class label according to
its relative performance. A decision tree is trained on the features and labels
to produce design rules for each class; these rules can be used by systems
experts to guide their implementations. We demonstrate our strategy using a key
kernel from scientific computing -- sparse-matrix vector multiplication -- on a
platform with multiple MPI ranks and GPU streams.
</summary>
    <author>
      <name>Carl Pearson</name>
    </author>
    <author>
      <name>Aurya Javeed</name>
    </author>
    <author>
      <name>Karen Devine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.02530v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.02530v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.08943v2</id>
    <updated>2022-03-19T19:27:47Z</updated>
    <published>2022-03-16T21:07:28Z</published>
    <title>CachePerf: A Unified Cache Miss Classifier via Hybrid Hardware Sampling</title>
    <summary>  The cache plays a key role in determining the performance of applications, no
matter for sequential or concurrent programs on homogeneous and heterogeneous
architecture. Fixing cache misses requires to understand the origin and the
type of cache misses. However, this remains to be an unresolved issue even
after decades of research. This paper proposes a unified profiling
tool--CachePerf--that could correctly identify different types of cache misses,
differentiate allocator-induced issues from those of applications, and exclude
minor issues without much performance impact. The core idea behind CachePerf is
a hybrid sampling scheme: it employs the PMU-based coarse-grained sampling to
select very few susceptible instructions (with frequent cache misses) and then
employs the breakpoint-based fine-grained sampling to collect the memory access
pattern of these instructions. Based on our evaluation, CachePerf only imposes
14% performance overhead and 19% memory overhead (for applications with large
footprints), while identifying the types of cache misses correctly. CachePerf
detected 9 previous-unknown bugs. Fixing the reported bugs achieves from 3% to
3788% performance speedup. CachePerf will be an indispensable complementary to
existing profilers due to its effectiveness and low overhead.
</summary>
    <author>
      <name>Jin Zhou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Jiaxun</arxiv:affiliation>
    </author>
    <author>
      <name> Steven</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Jiaxun</arxiv:affiliation>
    </author>
    <author>
      <name> Tang</name>
    </author>
    <author>
      <name>Hanmei Yang</name>
    </author>
    <author>
      <name>Tongping Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3489048.3526954</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3489048.3526954" rel="related"/>
    <link href="http://arxiv.org/abs/2203.08943v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.08943v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.13969v1</id>
    <updated>2022-03-26T02:23:33Z</updated>
    <published>2022-03-26T02:23:33Z</published>
    <title>Preventing Outages under Coordinated Cyber-Physical Attack with Secured
  PMUs</title>
    <summary>  Due to the potentially severe consequences of coordinated cyber-physical
attacks (CCPA), the design of defenses has gained significant attention. A
popular approach is to eliminate the existence of attacks by either securing
existing sensors or deploying secured PMUs. In this work, we improve this
approach by lowering the defense target from eliminating attacks to preventing
outages and reducing the required number of PMUs. To this end, we formulate the
problem of PMU Placement for Outage Prevention (PPOP) under DC power flow model
as a tri-level non-linear optimization problem and transform it into a bi-level
mixed-integer linear programming (MILP) problem. Then, we propose an
alternating optimization framework to solve PPOP by iteratively adding
constraints, for which we develop two constraint generation algorithms. In
addition, for large-scale grids, we propose a polynomial-time heuristic
algorithm to obtain suboptimal solutions. Next, we extend our solution to
achieve the defense goal under AC power flow model. Finally, we evaluate our
algorithm on IEEE 30-bus, 57-bus, 118-bus, and 300-bus systems, which
demonstrates the potential of the proposed approach in greatly reducing the
required number of PMUs.
</summary>
    <author>
      <name>Yudi Huang</name>
    </author>
    <author>
      <name>Ting He</name>
    </author>
    <author>
      <name>Nilanjan Ray Chaudhuri</name>
    </author>
    <author>
      <name>Thomas La Porta</name>
    </author>
    <link href="http://arxiv.org/abs/2203.13969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.13969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.03394v2</id>
    <updated>2022-05-25T21:23:47Z</updated>
    <published>2022-04-07T12:29:55Z</published>
    <title>Towards Comparing Performance of Algorithms in Hardware and Software</title>
    <summary>  In this paper, we report on a preliminary investigation of the potential
performance gain of programs implemented in field-programmable gate arrays
(FPGAs) using a high-level language Chisel compared to ordinary high-level
software implementations executed on general-purpose computers and small and
cheap computers. FPGAs inherently support parallel evaluations, while
sequential computers do not. For this preliminary investigation, we have chosen
a highly parallelizable program as a case study to show an upper bound of
performance gain. The purpose is to demonstrate whether or not programming
FPGAs has the potential for performance optimizations of ordinary programs. We
have developed and evaluated Conway's Game of Life for an FPGA, a small and
cheap computer Raspberry Pi 4, and a MacBook Pro Laptop. We have compared the
performance of programs over different input sizes to decide the relative
increase in runtime.
</summary>
    <author>
      <name>Maja H. Kirkeby</name>
    </author>
    <author>
      <name>Martin Schoeberl</name>
    </author>
    <link href="http://arxiv.org/abs/2204.03394v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.03394v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.04643v1</id>
    <updated>2022-04-10T10:19:31Z</updated>
    <published>2022-04-10T10:19:31Z</published>
    <title>A Palm Calculus Approach to the Distribution of the Age of Information</title>
    <summary>  A key metric to express the timeliness of status updates in latency-sensitive
networked systems is the age of information (AoI), i.e., the time elapsed since
the generation of the last received informative status message. This metric
allows studying a number of applications including updates of sensory and
control information in cyber-physical systems and vehicular networks as well
as, job and resource allocation in cloud clusters. State-of-the-art approaches
to analyzing the AoI rely on queueing models that are composed of one or many
queuing systems endowed with service order, e.g., FIFO, LIFO, or
last-generated-first-out order. A major difficulty arising in these analysis
methods is capturing the AoI under message reordering when the delivery is
non-preemptive and non-FIFO, i.e., when messages can overtake each other and
the reception of informative messages may obsolete some messages that are
underway. In this paper, we derive an exact formulation for the distribution of
AoI in non-preemptive, non-FIFO systems where the main ingredients of our
analysis are Palm calculus and time inversion. Owing to the rationality of the
Laplace-Stieltjes transforms that are used in our approach, we obtain
computable exact expressions for the distribution of AoI.
</summary>
    <author>
      <name>Amr Rizk</name>
    </author>
    <author>
      <name>Jean-Yves Le Boudec</name>
    </author>
    <link href="http://arxiv.org/abs/2204.04643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.04643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.3015v1</id>
    <updated>2007-05-21T19:00:25Z</updated>
    <published>2007-05-21T19:00:25Z</published>
    <title>An Extensible Timing Infrastructure for Adaptive Large-scale
  Applications</title>
    <summary>  Real-time access to accurate and reliable timing information is necessary to
profile scientific applications, and crucial as simulations become increasingly
complex, adaptive, and large-scale. The Cactus Framework provides flexible and
extensible capabilities for timing information through a well designed
infrastructure and timing API. Applications built with Cactus automatically
gain access to built-in timers, such as gettimeofday and getrusage,
system-specific hardware clocks, and high-level interfaces such as PAPI. We
describe the Cactus timer interface, its motivation, and its implementation. We
then demonstrate how this timing information can be used by an example
scientific application to profile itself, and to dynamically adapt itself to a
changing environment at run time.
</summary>
    <author>
      <name>Dylan Stark</name>
    </author>
    <author>
      <name>Gabrielle Allen</name>
    </author>
    <author>
      <name>Tom Goodale</name>
    </author>
    <author>
      <name>Thomas Radke</name>
    </author>
    <author>
      <name>Erik Schnetter</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Roman Wyrzykowski et al., editors, Parallel Processing and
  Applied Mathematics (PPAM), 2007, Gdansk, Poland, volume 4967 of Lecture
  Notes in Computer Science (LNCS), pages 1170-1179. Springer, 2007.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.3015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.3015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.1041v1</id>
    <updated>2011-12-05T19:57:44Z</updated>
    <published>2011-12-05T19:57:44Z</published>
    <title>Stabilization of Branching Queueing Networks</title>
    <summary>  Queueing networks are gaining attraction for the performance analysis of
parallel computer systems. A Jackson network is a set of interconnected
servers, where the completion of a job at server i may result in the creation
of a new job for server j. We propose to extend Jackson networks by "branching"
and by "control" features. Both extensions are new and substantially expand the
modelling power of Jackson networks. On the other hand, the extensions raise
computational questions, particularly concerning the stability of the networks,
i.e, the ergodicity of the underlying Markov chain. We show for our extended
model that it is decidable in polynomial time if there exists a controller that
achieves stability. Moreover, if such a controller exists, one can efficiently
compute a static randomized controller which stabilizes the network in a very
strong sense; in particular, all moments of the queue sizes are finite.
</summary>
    <author>
      <name>Tomáš Brázdil</name>
    </author>
    <author>
      <name>Stefan Kiefer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">technical report for a STACS'12 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.1041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.1041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.2822v1</id>
    <updated>2011-12-13T08:50:47Z</updated>
    <published>2011-12-13T08:50:47Z</published>
    <title>A Temporal Approach to Stochastic Network Calculus</title>
    <summary>  Stochastic network calculus is a newly developed theory for stochastic
service guarantee analysis of computer networks. In the current stochastic
network calculus literature, its fundamental models are based on the cumulative
amount of traffic or cumulative amount of service. However, there are network
scenarios where direct application of such models is difficult. This paper
presents a temporal approach to stochastic network calculus. The key idea is to
develop models and derive results from the time perspective. Particularly, we
define traffic models and service models based on the cumulative packet
inter-arrival time and the cumulative packet service time, respectively.
Relations among these models as well as with the existing models in the
literature are established. In addition, we prove the basic properties of the
proposed models, such as delay bound and backlog bound, output
characterization, concatenation property and superposition property. These
results form a temporal stochastic network calculus and compliment the existing
results.
</summary>
    <author>
      <name>Jing Xie</name>
    </author>
    <author>
      <name>Yuming Jiang</name>
    </author>
    <author>
      <name>Min Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">45 pages. An early version of this paper has been presented at 17th
  Annual Meeting of the IEEE/ACM International Symposium on Modelling, Analysis
  and Simulation of Computer and Telecommunication Systems. This version has
  been submitted to a journal and is waiting for being reviewed</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.2822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.2822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.5975v1</id>
    <updated>2012-10-10T18:02:33Z</updated>
    <published>2012-10-10T18:02:33Z</published>
    <title>Solid State Disk Object-Based Storage with Trim Commands</title>
    <summary>  This paper presents a model of NAND flash SSD utilization and write
amplification when the ATA/ATAPI SSD Trim command is incorporated into
object-based storage under a variety of user workloads, including a uniform
random workload with objects of fixed size and a uniform random workload with
objects of varying sizes. We first summarize the existing models for write
amplification in SSDs for workloads with and without the Trim command, then
propose an alteration of the models that utilizes a framework of object-based
storage. The utilization of objects and pages in the SSD is derived, with the
analytic results compared to simulation. Finally, the effect of objects on
write amplification and its computation is discussed along with a potential
application to optimization of SSD usage through object storage metadata
servers that allocate object classes of distinct object size.
</summary>
    <author>
      <name>Tasha Frankie</name>
    </author>
    <author>
      <name>Gordon Hughes</name>
    </author>
    <author>
      <name>Ken Kreutz-Delgado</name>
    </author>
    <link href="http://arxiv.org/abs/1210.5975v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.5975v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.6122v1</id>
    <updated>2012-10-23T04:09:30Z</updated>
    <published>2012-10-23T04:09:30Z</published>
    <title>Performance Evaluation: Ball-Tree and KD-Tree in the Context of MST</title>
    <summary>  Now a days many algorithms are invented or being inventing to find the
solution for Euclidean Minimum Spanning Tree, EMST, problem, as its
applicability is increasing in much wide range of fields containing spatial or
spatio temporal data viz. astronomy which consists of millions of spatial data.
To solve this problem, we are presenting a technique by adopting the dual tree
algorithm for finding efficient EMST and experimented on a variety of real time
and synthetic datasets. This paper presents the observed experimental
observations and the efficiency of the dual tree framework, in the context of
kdtree and ball tree on spatial datasets of different dimensions.
</summary>
    <author>
      <name>Hazarath Munaga</name>
    </author>
    <author>
      <name>Venkata Jarugumalli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-32573-1_38</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-32573-1_38" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">http://link.springer.com/chapter/10.1007%2F978-3-642-32573-1_38?LI=true 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.6122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.6122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.8176v1</id>
    <updated>2012-10-30T21:23:57Z</updated>
    <published>2012-10-30T21:23:57Z</published>
    <title>Eigenvalue-based Cyclostationary Spectrum Sensing Using Multiple
  Antennas</title>
    <summary>  In this paper, we propose a signal-selective spectrum sensing method for
cognitive radio networks and specifically targeted for receivers with
multiple-antenna capability. This method is used for detecting the presence or
absence of primary users based on the eigenvalues of the cyclic covariance
matrix of received signals. In particular, the cyclic correlation significance
test is used to detect a specific signal-of-interest by exploiting knowledge of
its cyclic frequencies. The analytical threshold for achieving constant false
alarm rate using this detection method is presented, verified through
simulations, and shown to be independent of both the number of samples used and
the noise variance, effectively eliminating the dependence on accurate noise
estimation. The proposed method is also shown, through numerical simulations,
to outperform existing multiple-antenna cyclostationary-based spectrum sensing
algorithms under a quasi-static Rayleigh fading channel, in both spatially
correlated and uncorrelated noise environments. The algorithm also has
significantly lower computational complexity than these other approaches.
</summary>
    <author>
      <name>Paulo Urriza</name>
    </author>
    <author>
      <name>Eric Rebeiz</name>
    </author>
    <author>
      <name>Danijela Cabric</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/GLOCOM.2012.6503326</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/GLOCOM.2012.6503326" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures, accepted to IEEE GLOBECOM 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.8176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.8176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.8421v2</id>
    <updated>2014-05-08T21:31:29Z</updated>
    <published>2012-10-31T18:04:47Z</published>
    <title>Distribution of the Number of Retransmissions of Bounded Documents</title>
    <summary>  Retransmission-based failure recovery represents a primary approach in
existing communication networks that guarantees data delivery in the presence
of channel failures. Recent work has shown that, when data sizes have infinite
support, retransmissions can cause long (-tailed) delays even if all traffic
and network characteristics are light-tailed. In this paper we investigate the
practically important case of bounded data units 0 &lt;= L_b &lt;= b under the
condition that the hazard functions of the distributions of data sizes and
channel statistics are proportional. To this end, we provide an explicit and
uniform characterization of the entire body of the retransmission distribution
Pr[N_b &gt; n] in both n and b. Our main discovery is that this distribution can
be represented as the product of a power law and Gamma distribution. This
rigorous approximation clearly demonstrates the coupling of a power law
distribution, dominating the main body, and the Gamma distribution, determining
the exponential tail. Our results are validated via simulation experiments and
can be useful for designing retransmission-based systems with the required
performance characteristics. From a broader perspective, this study applies to
any other system, e.g., computing, where restart mechanisms are employed after
a job processing failure.
</summary>
    <author>
      <name>Predrag R. Jelenković</name>
    </author>
    <author>
      <name>Evangelia D. Skiani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.8421v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.8421v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.02603v2</id>
    <updated>2015-08-27T23:44:06Z</updated>
    <published>2015-03-09T18:26:55Z</published>
    <title>An asymptotically optimal policy and state-space collapse for the
  multi-class shared queue</title>
    <summary>  We consider a multi-class G/G/1 queue with a finite shared buffer. There is
task admission and server scheduling control which aims to minimize the cost
which consists of holding and rejection components. We construct a policy that
is asymptotically optimal in the heavy traffic limit. The policy stems from
solution to Harrison-Taksar (HT) free boundary problem and is expressed by a
single free boundary point. We show that the HT problem solution translated
into the queuelength processes follows a specific {\it triangular} form. This
form implies the queuelength control policy which is different from the known
$c\mu$ priority rule and has a novel structure.
  We exemplify that the probabilistic methods we exploit can be successfully
applied to solving scheduling and admission problems in cloud computing.
</summary>
    <author>
      <name>Mark Shifrin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1412.6775</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.02603v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.02603v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.07241v1</id>
    <updated>2015-03-25T00:10:50Z</updated>
    <published>2015-03-25T00:10:50Z</published>
    <title>GraphMat: High performance graph analytics made productive</title>
    <summary>  Given the growing importance of large-scale graph analytics, there is a need
to improve the performance of graph analysis frameworks without compromising on
productivity. GraphMat is our solution to bridge this gap between a
user-friendly graph analytics framework and native, hand-optimized code.
GraphMat functions by taking vertex programs and mapping them to high
performance sparse matrix operations in the backend. We get the productivity
benefits of a vertex programming framework without sacrificing performance.
GraphMat is in C++, and we have been able to write a diverse set of graph
algorithms in this framework with the same effort compared to other vertex
programming frameworks. GraphMat performs 1.2-7X faster than high performance
frameworks such as GraphLab, CombBLAS and Galois. It achieves better multicore
scalability (13-15X on 24 cores) than other frameworks and is 1.2X off native,
hand-optimized code on a variety of different graph algorithms. Since GraphMat
performance depends mainly on a few scalable and well-understood sparse matrix
operations, GraphMatcan naturally benefit from the trend of increasing
parallelism on future hardware.
</summary>
    <author>
      <name>Narayanan Sundaram</name>
    </author>
    <author>
      <name>Nadathur Rajagopalan Satish</name>
    </author>
    <author>
      <name>Md Mostofa Ali Patwary</name>
    </author>
    <author>
      <name>Subramanya R Dulloor</name>
    </author>
    <author>
      <name>Satya Gautam Vadlamudi</name>
    </author>
    <author>
      <name>Dipankar Das</name>
    </author>
    <author>
      <name>Pradeep Dubey</name>
    </author>
    <link href="http://arxiv.org/abs/1503.07241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.07241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.07931v1</id>
    <updated>2015-03-27T00:08:13Z</updated>
    <published>2015-03-27T00:08:13Z</published>
    <title>Are Markov Models Effective for Storage Reliability Modelling?</title>
    <summary>  Continuous Time Markov Chains (CTMC) have been used extensively to model
reliability of storage systems. While the exponentially distributed sojourn
time of Markov models is widely known to be unrealistic (and it is necessary to
consider Weibull-type models for components such as disks), recent work has
also highlighted some additional infirmities with the CTMC model, such as the
ability to handle repair times. Due to the memoryless property of these models,
any failure or repair of one component resets the "clock" to zero with any
partial repair or aging in some other subsystem forgotten. It has therefore
been argued that simulation is the only accurate technique available for
modelling the reliability of a storage system with multiple components.
  We show how both the above problematic aspects can be handled when we
consider a careful set of approximations in a detailed model of the system. A
detailed model has many states, and the transitions between them and the
current state captures the "memory" of the various components. We model a
non-exponential distribution using a sum of exponential distributions, along
with the use of a CTMC solver in a probabilistic model checking tool that has
support for reducing large state spaces. Furthermore, it is possible to get
results close to what is obtained through simulation and at much lower cost.
</summary>
    <author>
      <name>Prasenjit Karmakar</name>
    </author>
    <author>
      <name>K. Gopinath</name>
    </author>
    <link href="http://arxiv.org/abs/1503.07931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.07931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.08548v2</id>
    <updated>2017-03-10T07:36:38Z</updated>
    <published>2015-03-30T06:21:38Z</published>
    <title>Hitting Times in Markov Chains with Restart and their Application to
  Network Centrality</title>
    <summary>  Motivated by applications in telecommunications, computer scienceand physics,
we consider a discrete-time Markov process withrestart. At each step the
process eitherwith a positive probability restarts from a given distribution,
orwith the complementary probability continues according to a Markovtransition
kernel. The main contribution of the present work is thatwe obtain an explicit
expression for the expectation of the hittingtime (to a given target set) of
the process with restart.The formula is convenient when considering the problem
of optimizationof the expected hitting time with respect to the restart
probability.We illustrate our results with two examplesin uncountable and
countable state spaces andwith an application to network centrality.
</summary>
    <author>
      <name>Konstantin Avrachenkov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NEO</arxiv:affiliation>
    </author>
    <author>
      <name>Alexey Piunovskiy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">imagine</arxiv:affiliation>
    </author>
    <author>
      <name>Yi Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">imagine</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1503.08548v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.08548v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01845v1</id>
    <updated>2017-11-06T11:43:47Z</updated>
    <published>2017-11-06T11:43:47Z</published>
    <title>Comparison of Parallelisation Approaches, Languages, and Compilers for
  Unstructured Mesh Algorithms on GPUs</title>
    <summary>  Efficiently exploiting GPUs is increasingly essential in scientific
computing, as many current and upcoming supercomputers are built using them. To
facilitate this, there are a number of programming approaches, such as CUDA,
OpenACC and OpenMP 4, supporting different programming languages (mainly C/C++
and Fortran). There are also several compiler suites (clang, nvcc, PGI, XL)
each supporting different combinations of languages. In this study, we take a
detailed look at some of the currently available options, and carry out a
comprehensive analysis and comparison using computational loops and
applications from the domain of unstructured mesh computations. Beyond runtimes
and performance metrics (GB/s), we explore factors that influence performance
such as register counts, occupancy, usage of different memory types,
instruction counts, and algorithmic differences. Results of this work show how
clang's CUDA compiler frequently outperform NVIDIA's nvcc, performance issues
with directive-based approaches on complex kernels, and OpenMP 4 support
maturing in clang and XL; currently around 10% slower than CUDA.
</summary>
    <author>
      <name>G. D. Balogh</name>
    </author>
    <author>
      <name>I. Z. Reguly</name>
    </author>
    <author>
      <name>G. R. Mudalige</name>
    </author>
    <link href="http://arxiv.org/abs/1711.01845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05487v1</id>
    <updated>2017-11-15T10:22:53Z</updated>
    <published>2017-11-15T10:22:53Z</published>
    <title>Performance Analysis and Optimization of Sparse Matrix-Vector
  Multiplication on Modern Multi- and Many-Core Processors</title>
    <summary>  This paper presents a low-overhead optimizer for the ubiquitous sparse
matrix-vector multiplication (SpMV) kernel. Architectural diversity among
different processors together with structural diversity among different sparse
matrices lead to bottleneck diversity. This justifies an SpMV optimizer that is
both matrix- and architecture-adaptive through runtime specialization. To this
direction, we present an approach that first identifies the performance
bottlenecks of SpMV for a given sparse matrix on the target platform either
through profiling or by matrix property inspection, and then selects suitable
optimizations to tackle those bottlenecks. Our optimization pool is based on
the widely used Compressed Sparse Row (CSR) sparse matrix storage format and
has low preprocessing overheads, making our overall approach practical even in
cases where fast decision making and optimization setup is required. We
evaluate our optimizer on three x86-based computing platforms and demonstrate
that it is able to distinguish and appropriately optimize SpMV for the majority
of matrices in a representative test suite, leading to significant speedups
over the CSR and Inspector-Executor CSR SpMV kernels available in the latest
release of the Intel MKL library.
</summary>
    <author>
      <name>Athena Elafrou</name>
    </author>
    <author>
      <name>Georgios Goumas</name>
    </author>
    <author>
      <name>Nektarios Koziris</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICPP.2017.38</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICPP.2017.38" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures, ICPP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.05487v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05487v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11468v1</id>
    <updated>2017-11-30T15:45:56Z</updated>
    <published>2017-11-30T15:45:56Z</published>
    <title>Lattice Boltzmann Benchmark Kernels as a Testbed for Performance
  Analysis</title>
    <summary>  Lattice Boltzmann methods (LBM) are an important part of current
computational fluid dynamics (CFD). They allow easy implementations and
boundary handling. However, competitive time to solution not only depends on
the choice of a reasonable method, but also on an efficient implementation on
modern hardware. Hence, performance optimization has a long history in the
lattice Boltzmann community. A variety of options exists regarding the
implementation with direct impact on the solver performance. Experimenting and
evaluating each option often is hard as the kernel itself is typically embedded
in a larger code base. With our suite of lattice Boltzmann kernels we provide
the infrastructure for such endeavors. Already included are several kernels
ranging from simple to fully optimized implementations. Although these kernels
are not fully functional CFD solvers, they are equipped with a solid
verification method. The kernels may act as an reference for performance
comparisons and as a blue print for optimization strategies. In this paper we
give an overview of already available kernels, establish a performance model
for each kernel, and show a comparison of implementations and recent
architectures.
</summary>
    <author>
      <name>Markus Wittmann</name>
    </author>
    <author>
      <name>Viktor Haag</name>
    </author>
    <author>
      <name>Thomas Zeiser</name>
    </author>
    <author>
      <name>Harald Köstler</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.compfluid.2018.03.030</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.compfluid.2018.03.030" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">preprint, submitted to Computer &amp; Fluids Special Issue DSFD2017</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers &amp; Fluids, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.11468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04215v1</id>
    <updated>2018-05-11T01:15:29Z</updated>
    <published>2018-05-11T01:15:29Z</published>
    <title>ProCal: A Low-Cost and Programmable Calibration Tool for IoT Devices</title>
    <summary>  Calibration is an important step towards building reliable IoT systems. For
example, accurate sensor reading requires ADC calibration, and power monitoring
chips must be calibrated before being used for measuring the energy consumption
of IoT devices. In this paper, we present ProCal, a low-cost, accurate, and
scalable power calibration tool. ProCal is a programmable platform which
provides dynamic voltage and current output for calibration. The basic idea is
to use a digital potentiometer connected to a parallel resistor network
controlled through digital switches. The resistance and output frequency of
ProCal is controlled by a software communicating with the board through the SPI
interface. Our design provides a simple synchronization mechanism which
prevents the need for accurate time synchronization. We present mathematical
modeling and validation of the tool by incorporating the concept of Fibonacci
sequence. Our extensive experimental studies show that this tool can
significantly improve measurement accuracy. For example, for ATMega2560, the
ADC error reduces from 0.2% to 0.01%. ProCal not only costs less than 2\% of
the current commercial solutions, it is also highly accurate by being able to
provide extensive range of current and voltage values.
</summary>
    <author>
      <name>Chia-Chi Li</name>
    </author>
    <author>
      <name>Behnam Dezfouli</name>
    </author>
    <link href="http://arxiv.org/abs/1805.04215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04303v1</id>
    <updated>2018-05-11T10:03:36Z</updated>
    <published>2018-05-11T10:03:36Z</published>
    <title>Enabling Cross-Event Optimization in Discrete-Event Simulation Through
  Compile-Time Event Batching</title>
    <summary>  A discrete-event simulation (DES) involves the execution of a sequence of
event handlers dynamically scheduled at runtime. As a consequence, a priori
knowledge of the control flow of the overall simulation program is limited. In
particular, powerful optimizations supported by modern compilers can only be
applied on the scope of individual event handlers, which frequently involve
only a few lines of code. We propose a method that extends the scope for
compiler optimizations in discrete-event simulations by generating batches of
multiple events that are subjected to compiler optimizations as contiguous
procedures. A runtime mechanism executes suitable batches at negligible
overhead. Our method does not require any compiler extensions and introduces
only minor additional effort during model development. The feasibility and
potential performance gains of the approach are illustrated on the example of
an idealized proof-ofconcept model. We believe that the applicability of the
approach extends to general event-driven programs.
</summary>
    <author>
      <name>Marc Leinweber</name>
    </author>
    <author>
      <name>Hannes Hartenstein</name>
    </author>
    <author>
      <name>Philipp Andelfinger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5445/IR/1000082690</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5445/IR/1000082690" rel="related"/>
    <link href="http://arxiv.org/abs/1805.04303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06865v2</id>
    <updated>2018-11-12T05:00:55Z</updated>
    <published>2018-05-17T17:06:21Z</published>
    <title>Optimal Scheduling and Exact Response Time Analysis for Multistage Jobs</title>
    <summary>  Scheduling to minimize mean response time in an M/G/1 queue is a classic
problem. The problem is usually addressed in one of two scenarios. In the
perfect-information scenario, the scheduler knows each job's exact size, or
service requirement. In the zero-information scenario, the scheduler knows only
each job's size distribution. The well-known shortest remaining processing time
(SRPT) policy is optimal in the perfect-information scenario, and the more
complex Gittins policy is optimal in the zero-information scenario.
  In real systems the scheduler often has partial but incomplete information
about each job's size. We introduce a new job model, that of multistage jobs,
to capture this partial-information scenario. A multistage job consists of a
sequence of stages, where both the sequence of stages and stage sizes are
unknown, but the scheduler always knows which stage of a job is in progress. We
give an optimal algorithm for scheduling multistage jobs in an M/G/1 queue and
an exact response time analysis of our algorithm.
</summary>
    <author>
      <name>Ziv Scully</name>
    </author>
    <author>
      <name>Mor Harchol-Balter</name>
    </author>
    <author>
      <name>Alan Scheller-Wolf</name>
    </author>
    <link href="http://arxiv.org/abs/1805.06865v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06865v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07686v1</id>
    <updated>2018-05-20T01:07:03Z</updated>
    <published>2018-05-20T01:07:03Z</published>
    <title>SRPT for Multiserver Systems</title>
    <summary>  The Shortest Remaining Processing Time (SRPT) scheduling policy and its
variants have been extensively studied in both theoretical and practical
settings. While beautiful results are known for single-server SRPT, much less
is known for multiserver SRPT. In particular, stochastic analysis of the M/G/k
under multiserver SRPT is entirely open. Intuition suggests that multiserver
SRPT should be optimal or near-optimal for minimizing mean response time.
However, the only known analysis of multiserver SRPT is in the worst-case
adversarial setting, where SRPT can be far from optimal. In this paper, we give
the first stochastic analysis bounding mean response time of the M/G/k under
multiserver SRPT. Using our response time bound, we show that multiserver SRPT
has asymptotically optimal mean response time in the heavy-traffic limit. The
key to our bounds is a strategic combination of stochastic and worst-case
techniques. Beyond SRPT, we prove similar response time bounds and optimality
results for several other multiserver scheduling policies.
</summary>
    <author>
      <name>Isaac Grosof</name>
    </author>
    <author>
      <name>Ziv Scully</name>
    </author>
    <author>
      <name>Mor Harchol-Balter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages. Submitted to IFIP Performance 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.07686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09641v1</id>
    <updated>2018-05-23T14:46:19Z</updated>
    <published>2018-05-23T14:46:19Z</published>
    <title>Infinite-server queueing model with MAPkGk Markov arrival streams,
  random volume of customers in random environment subject to catastrophe</title>
    <summary>  In this paper the infinite server queue model in semi-Markov random
environment with k Markov arrival streams, random resources of customers, and
catastrophes is considered. After catastrophes occur, all customers in the
model are flashed out and the system jumps into recovery station. After the
recovery time the model works from the empty state. The transient and
stationary joint distributions of numbers of different types of customers in
the model at moment t, numbers of different types of served in some interval
customers, volume of accumulated resources in the model at moment t, and total
volume of served resources in an interval for the model without catastrophes
are found. The transient and stationary joint distributions of numbers of
different types of customers in the model at moment t, and volume of
accumulated resources in the model at moment t and their moments for the model
with catastrophes are obtained. All results are obtained using Danzig
collective marks method and renewal theory methods.
</summary>
    <author>
      <name>Khanik Kerobyan</name>
    </author>
    <author>
      <name>Ruben Kerobyan</name>
    </author>
    <author>
      <name>Koffi Enakoutsa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.09641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11681v3</id>
    <updated>2018-08-23T05:15:30Z</updated>
    <published>2018-05-29T19:43:01Z</published>
    <title>Maximizing Service Reward for Queues with Deadlines</title>
    <summary>  In this paper we consider a real time queuing system with rewards and
deadlines. We assume that packet processing time is known upon arrival, as is
the case in communication networks. This assumption allows us to demonstrate
that the well known Earliest-Deadline-First policy performance can be improved.
We then propose a scheduling policy that provides excellent results for packets
with rewards and deadlines. We prove that the policy is optimal under
deterministic service time and binomial reward distribution. In the more
general case we prove that the policy processes the maximal number of packets
while collecting rewards higher than the expected reward. We present simulation
results that show its high performance in more generic cases compared to the
most commonly used scheduling policies.
</summary>
    <author>
      <name>Li-on Raviv</name>
    </author>
    <author>
      <name>Amir Leshem</name>
    </author>
    <link href="http://arxiv.org/abs/1805.11681v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11681v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00912v2</id>
    <updated>2018-10-10T11:28:46Z</updated>
    <published>2018-09-04T12:05:29Z</published>
    <title>Automated Instruction Stream Throughput Prediction for Intel and AMD
  Microarchitectures</title>
    <summary>  An accurate prediction of scheduling and execution of instruction streams is
a necessary prerequisite for predicting the in-core performance behavior of
throughput-bound loop kernels on out-of-order processor architectures. Such
predictions are an indispensable component of analytical performance models,
such as the Roofline and the Execution-Cache-Memory (ECM) model, and allow a
deep understanding of the performance-relevant interactions between hardware
architecture and loop code. We present the Open Source Architecture Code
Analyzer (OSACA), a static analysis tool for predicting the execution time of
sequential loops comprising x86 instructions under the assumption of an
infinite first-level cache and perfect out-of-order scheduling. We show the
process of building a machine model from available documentation and
semi-automatic benchmarking, and carry it out for the latest Intel Skylake and
AMD Zen micro-architectures. To validate the constructed models, we apply them
to several assembly kernels and compare runtime predictions with actual
measurements. Finally we give an outlook on how the method may be generalized
to new architectures.
</summary>
    <author>
      <name>Jan Laukemann</name>
    </author>
    <author>
      <name>Julian Hammer</name>
    </author>
    <author>
      <name>Johannes Hofmann</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/PMBS.2018.8641578</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/PMBS.2018.8641578" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.00912v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00912v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.07851v1</id>
    <updated>2018-09-20T20:55:15Z</updated>
    <published>2018-09-20T20:55:15Z</published>
    <title>FFT Convolutions are Faster than Winograd on Modern CPUs, Here is Why</title>
    <summary>  Winograd-based convolution has quickly gained traction as a preferred
approach to implement convolutional neural networks (ConvNet) on various
hardware platforms because it requires fewer floating point operations than
FFT-based or direct convolutions.
  This paper compares three highly optimized implementations (regular FFT--,
Gauss--FFT--, and Winograd--based convolutions) on modern multi-- and
many--core CPUs. Although all three implementations employed the same
optimizations for modern CPUs, our experimental results with two popular
ConvNets (VGG and AlexNet) show that the FFT--based implementations generally
outperform the Winograd--based approach, contrary to the popular belief.
  To understand the results, we use a Roofline performance model to analyze the
three implementations in detail, by looking at each of their computation phases
and by considering not only the number of floating point operations, but also
the memory bandwidth and the cache sizes. The performance analysis explains
why, and under what conditions, the FFT--based implementations outperform the
Winograd--based one, on modern CPUs.
</summary>
    <author>
      <name>Aleksandar Zlateski</name>
    </author>
    <author>
      <name>Zhen Jia</name>
    </author>
    <author>
      <name>Kai Li</name>
    </author>
    <author>
      <name>Fredo Durand</name>
    </author>
    <link href="http://arxiv.org/abs/1809.07851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.07851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.08311v1</id>
    <updated>2018-09-18T20:25:44Z</updated>
    <published>2018-09-18T20:25:44Z</published>
    <title>SCOPE: C3SR Systems Characterization and Benchmarking Framework</title>
    <summary>  This report presents the design of the Scope infrastructure for extensible
and portable benchmarking. Improvements in high- performance computing systems
rely on coordination across different levels of system abstraction. Developing
and defining accurate performance measurements is necessary at all levels of
the system hierarchy, and should be as accessible as possible to developers
with different backgrounds. The Scope project aims to lower the barrier to
entry for developing performance benchmarks by providing a software
architecture that allows benchmarks to be developed independently, by providing
useful C/C++ abstractions and utilities, and by providing a Python package for
generating publication-quality plots of resulting measurements.
</summary>
    <author>
      <name>Carl Pearson</name>
    </author>
    <author>
      <name>Abdul Dakkak</name>
    </author>
    <author>
      <name>Cheng Li</name>
    </author>
    <author>
      <name>Sarah Hashash</name>
    </author>
    <author>
      <name>Jinjun Xiong</name>
    </author>
    <author>
      <name>Wen-mei Hwu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, draft</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.08311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.08311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.10572v2</id>
    <updated>2019-12-12T15:39:59Z</updated>
    <published>2018-09-27T15:25:02Z</published>
    <title>Scalar Arithmetic Multiple Data: Customizable Precision for Deep Neural
  Networks</title>
    <summary>  Quantization of weights and activations in Deep Neural Networks (DNNs) is a
powerful technique for network compression, and has enjoyed significant
attention and success. However, much of the inference-time benefit of
quantization is accessible only through the use of customized hardware
accelerators or by providing an FPGA implementation of quantized arithmetic.
  Building on prior work, we show how to construct arbitrary bit-precise signed
and unsigned integer operations using a software technique which logically
\emph{embeds} a vector architecture with custom bit-width lanes in universally
available fixed-width scalar arithmetic.
  We evaluate our approach on a high-end Intel Haswell processor, and an
embedded ARM processor. Our approach yields very fast implementations of
bit-precise custom DNN operations, which often match or exceed the performance
of operations quantized to the sizes supported in native arithmetic. At the
strongest level of quantization, our approach yields a maximum speedup of
$\thicksim6\times$ on the Intel platform, and $\thicksim10\times$ on the ARM
platform versus quantization to native 8-bit integers.
</summary>
    <author>
      <name>Andrew Anderson</name>
    </author>
    <author>
      <name>David Gregg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ARITH.2019.00018</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ARITH.2019.00018" rel="related"/>
    <link href="http://arxiv.org/abs/1809.10572v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.10572v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.10596v1</id>
    <updated>2018-09-21T15:58:12Z</updated>
    <published>2018-09-21T15:58:12Z</published>
    <title>Predicting the confirmation time of Bitcoin transactions</title>
    <summary>  We study the probabilistic distribution of the confirmation time of Bitcoin
transactions, conditional on the current memory pool (i.e., the queue of
transactions awaiting confirmation). The results of this paper are particularly
interesting for users that want to make a Bitcoin transaction during
`heavy-traffic situations', when the transaction demand exceeds the block
capacity. In such situations, Bitcoin users tend to bid up the transaction
fees, in order to gain priority over other users that pay a lower fee. We argue
that the time until a Bitcoin transaction is confirmed can be modelled as a
particular stochastic fluid queueing process (to be precise: a
Cram\'er-Lundberg process). We approximate the queueing process in two
different ways. The first approach leads to a lower bound on the confirmation
probability, which becomes increasingly tight as traffic decreases. The second
approach relies on a diffusion approximation with a continuity correction,
which becomes increasingly accurate as traffic intensifies. The accuracy of the
approximations under different traffic loads are evaluated in a simulation
study.
</summary>
    <author>
      <name>David Koops</name>
    </author>
    <link href="http://arxiv.org/abs/1809.10596v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.10596v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.07561v1</id>
    <updated>2018-12-18T18:48:26Z</updated>
    <published>2018-12-18T18:48:26Z</published>
    <title>A Preliminary Study of Neural Network-based Approximation for HPC
  Applications</title>
    <summary>  Machine learning, as a tool to learn and model complicated (non)linear
relationships between input and output data sets, has shown preliminary success
in some HPC problems. Using machine learning, scientists are able to augment
existing simulations by improving accuracy and significantly reducing
latencies. Our ongoing research work is to create a general framework to apply
neural network-based models to HPC applications. In particular, we want to use
the neural network to approximate and replace code regions within the HPC
application to improve performance (i.e., reducing the execution time) of the
HPC application. In this paper, we present our preliminary study and results.
Using two applications (the Newton-Raphson method and the Lennard-Jones (LJ)
potential in LAMMP) for our case study, we achieve up to 2.7x and 2.46x
speedup, respectively.
</summary>
    <author>
      <name>Wenqian Dong</name>
    </author>
    <author>
      <name>Anzheng Guolu</name>
    </author>
    <author>
      <name>Dong Li</name>
    </author>
    <link href="http://arxiv.org/abs/1812.07561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.07561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.04624v2</id>
    <updated>2019-08-02T07:51:44Z</updated>
    <published>2019-06-11T14:36:03Z</published>
    <title>ROOT I/O compression algorithms and their performance impact within Run
  3</title>
    <summary>  The LHCs Run3 will push the envelope on data-intensive workflows and, since
at the lowest level this data is managed using the ROOT software framework,
preparations for managing this data are starting already. At the beginning of
LHC Run 1, all ROOT data was compressed with the ZLIB algorithm; since then,
ROOT has added support for additional algorithms such as LZMA and LZ4, each
with unique strengths. This work must continue as industry introduces new
techniques - ROOT can benefit saving disk space or reducing the I/O and
bandwidth for online and offline needs of experiments by introducing better
compression algorithms. In addition to alternate algorithms, we have been
exploring alternate techniques to improve parallelism and apply
pre-conditioners to the serialized data.
  We have performed a survey of the performance of the new compression
techniques. Our survey includes various use cases of data compression of ROOT
files provided by different LHC experiments. We also provide insight into
solutions applied to resolve bottlenecks in compression algorithms, resulting
in improved ROOT performance.
</summary>
    <author>
      <name>Oksana Shadura</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Nebraska-Lincoln</arxiv:affiliation>
    </author>
    <author>
      <name>Brian Paul Bockelman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Nebraska-Lincoln</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-6596/1525/1/012049</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-6596/1525/1/012049" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to proceedings of ACAT 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.04624v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.04624v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.07312v2</id>
    <updated>2020-10-05T22:23:12Z</updated>
    <published>2019-06-18T00:06:46Z</published>
    <title>MultiCloud Resource Management using Apache Mesos for Planned
  Integration with Apache Airavata</title>
    <summary>  We discuss initial results and our planned approach for incorporating Apache
Mesos based resource management that will enable design and development of
scheduling strategies for Apache Airavata jobs so that they can be launched on
multiple clouds, wherein several VMs do not have Public IP addresses. We
present initial work and next steps on the design of a meta-scheduler using
Apache Mesos. Apache Mesos presents a unified view of resources available
across several clouds and clusters. Our meta-scheduler can potentially examine
and identify the cases where multiple small jobs have been submitted by the
same scientists and then redirect job from the same community account or user
to different clusters. Our approach uses a NAT firewall to make nodes/VMs,
without a Public IP, visible to Mesos for the unified view.
</summary>
    <author>
      <name>Pankaj Saha</name>
    </author>
    <author>
      <name>Madhusudhan Govindaraju</name>
    </author>
    <author>
      <name>Suresh Marru</name>
    </author>
    <author>
      <name>Marlon Pierce</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.6084/m9.figshare.4491629.v2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.6084/m9.figshare.4491629.v2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Gateways 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.07312v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.07312v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.08138v2</id>
    <updated>2019-07-02T10:08:33Z</updated>
    <published>2019-06-19T15:08:06Z</published>
    <title>Collecting and Presenting Reproducible Intranode Stencil Performance:
  INSPECT</title>
    <summary>  Stencil algorithms have been receiving considerable interest in HPC research
for decades. The techniques used to approach multi-core stencil performance
modeling and engineering span basic runtime measurements, elaborate performance
models, detailed hardware counter analysis, and thorough scaling behavior
evaluation. Due to the plurality of approaches and stencil patterns, we set out
to develop a generalizable methodology for reproducible measurements
accompanied by state-of-the-art performance models. Our open-source toolchain,
and collected results are publicly available in the "Intranode Stencil
Performance Evaluation Collection" (INSPECT). We present the underlying
methodologies, models and tools involved in gathering and documenting the
performance behavior of a collection of typical stencil patterns across
multiple architectures and hardware configuration options. Our aim is to endow
performance-aware application developers with reproducible baseline performance
data and validated models to initiate a well-defined process of performance
assessment and optimization.
</summary>
    <author>
      <name>Julian Hornich</name>
    </author>
    <author>
      <name>Julian Hammer</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Thomas Gruber</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14529/jsfi190301</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14529/jsfi190301" rel="related"/>
    <link href="http://arxiv.org/abs/1906.08138v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.08138v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.10037v1</id>
    <updated>2019-06-24T15:53:47Z</updated>
    <published>2019-06-24T15:53:47Z</published>
    <title>Platform Independent Software Analysis for Near Memory Computing</title>
    <summary>  Near-memory Computing (NMC) promises improved performance for the
applications that can exploit the features of emerging memory technologies such
as 3D-stacked memory. However, it is not trivial to find such applications and
specialized tools are needed to identify them. In this paper, we present
PISA-NMC, which extends a state-of-the-art hardware agnostic profiling tool
with metrics concerning memory and parallelism, which are relevant for NMC. The
metrics include memory entropy, spatial locality, data-level, and
basic-block-level parallelism. By profiling a set of representative
applications and correlating the metrics with the application's performance on
a simulated NMC system, we verify the importance of those metrics. Finally, we
demonstrate which metrics are useful in identifying applications suitable for
NMC architectures.
</summary>
    <author>
      <name>Stefano Corda</name>
    </author>
    <author>
      <name>Gagandeep Singh</name>
    </author>
    <author>
      <name>Ahsan Javed Awan</name>
    </author>
    <author>
      <name>Roel Jordans</name>
    </author>
    <author>
      <name>Henk Corporaal</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Euromicro Conference on Digital System Design (DSD) 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.10037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.10081v1</id>
    <updated>2019-06-24T16:59:07Z</updated>
    <published>2019-06-24T16:59:07Z</published>
    <title>EasyCrash: Exploring Non-Volatility of Non-Volatile Memory for High
  Performance Computing Under Failures</title>
    <summary>  Emerging non-volatile memory (NVM) is promising for building future HPC.
Leveraging the non-volatility of NVM as main memory, we can restart the
application using data objects remaining on NVM when the application crashes.
This paper explores this solution to handle HPC under failures, based on the
observation that many HPC applications have good enough intrinsic fault
tolerance. To improve the possibility of successful recomputation with correct
outcomes and ignorable performance loss, we introduce EasyCrash, a framework to
decide how to selectively persist application data objects during application
execution. Our evaluation shows that EasyCrash transforms 54% of crashes that
cannot correctly recompute into the correct computation while incurring a
negligible performance overhead (1.5% on average). Using EasyCrash and
application intrinsic fault tolerance, 82% of crashes can successfully
recompute. When EasyCrash is used with a traditional checkpoint scheme, it
enables up to 24% improvement (15% on average) in system efficiency.
</summary>
    <author>
      <name>Jie Ren</name>
    </author>
    <author>
      <name>Kai Wu</name>
    </author>
    <author>
      <name>Dong Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.10081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.10347v2</id>
    <updated>2020-08-27T07:10:39Z</updated>
    <published>2019-06-25T06:54:40Z</published>
    <title>ALTIS: Modernizing GPGPU Benchmarking</title>
    <summary>  This paper presents Altis, a benchmark suite for modern GPGPU computing.
Previous benchmark suites such as Rodinia and SHOC have served the research
community well, but were developed years ago when hardware was more limited,
software supported fewer features, and production hardware-accelerated
workloads were scarce. Since that time, GPU compute density and memory capacity
has grown exponentially, programmability features such as unified memory,
demand paging, and HyperQ have matured, and new workloads such as deep neural
networks (DNNs), graph analytics, and crypto-currencies have emerged in
production environments, stressing the hardware and software in ways that
previous benchmarks did not anticipate. Drawing inspiration from Rodinia and
SHOC, Altis is a benchmark suite designed for modern GPU architectures and
modern GPU runtimes, representing a diverse set of application domains. By
adopting and extending applications from Rodinia and SHOC, adding new
applications, and focusing on CUDA platforms, Altis better represents modern
GPGPU workloads to enable support GPGPU research in both architecture and
system software.
</summary>
    <author>
      <name>Bodun Hu</name>
    </author>
    <author>
      <name>Christopher J. Rossbach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISPASS 2020. Project: https://github.com/utcs-scea/altis</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.10347v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10347v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.11204v1</id>
    <updated>2019-06-26T16:47:44Z</updated>
    <published>2019-06-26T16:47:44Z</published>
    <title>Stress-SGX: Load and Stress your Enclaves for Fun and Profit</title>
    <summary>  The latest generation of Intel processors supports Software Guard Extensions
(SGX), a set of instructions that implements a Trusted Execution Environment
(TEE) right inside the CPU, by means of so-called enclaves. This paper presents
Stress-SGX, an easy-to-use stress-test tool to evaluate the performance of
SGX-enabled nodes. We build on top of the popular Stress-NG tool, while only
keeping the workload injectors (stressors) that are meaningful in the SGX
context. We report on several insights and lessons learned about porting legacy
code to run inside an SGX enclave, as well as the limitations introduced by
this process. Finally, we use Stress-SGX to conduct a study comparing the
performance of different SGX-enabled machines.
</summary>
    <author>
      <name>Sébastien Vaucher</name>
    </author>
    <author>
      <name>Valerio Schiavoni</name>
    </author>
    <author>
      <name>Pascal Felber</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-05529-5_24</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-05529-5_24" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">European Commission Project: LEGaTO - Low Energy Toolset for
  Heterogeneous Computing (EC-H2020-780681)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">in Networked Systems, Springer International Publishing, 2019, pp.
  358-363</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.11204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.11204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.12066v1</id>
    <updated>2019-06-28T07:25:24Z</updated>
    <published>2019-06-28T07:25:24Z</published>
    <title>Pinpointing Performance Inefficiencies in Java</title>
    <summary>  Many performance inefficiencies such as inappropriate choice of algorithms or
data structures, developers' inattention to performance, and missed compiler
optimizations show up as wasteful memory operations. Wasteful memory operations
are those that produce/consume data to/from memory that may have been avoided.
We present, JXPerf, a lightweight performance analysis tool for pinpointing
wasteful memory operations in Java programs. Traditional byte-code
instrumentation for such analysis (1) introduces prohibitive overheads and (2)
misses inefficiencies in machine code generation. JXPerf overcomes both of
these problems. JXPerf uses hardware performance monitoring units to sample
memory locations accessed by a program and uses hardware debug registers to
monitor subsequent accesses to the same memory. The result is a lightweight
measurement at machine-code level with attribution of inefficiencies to their
provenance: machine and source code within full calling contexts. JXPerf
introduces only 7% runtime overhead and 7% memory overhead making it useful in
production. Guided by JXPerf, we optimize several Java applications by
improving code generation and choosing superior data structures and algorithms,
which yield significant speedups.
</summary>
    <author>
      <name>Pengfei Su</name>
    </author>
    <author>
      <name>Qingsen Wang</name>
    </author>
    <author>
      <name>Milind Chabbi</name>
    </author>
    <author>
      <name>Xu Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a full-version of our ESEC/FSE'2019 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.12066v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.12066v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.02053v1</id>
    <updated>2019-09-04T18:50:16Z</updated>
    <published>2019-09-04T18:50:16Z</published>
    <title>ModiPick: SLA-aware Accuracy Optimization For Mobile Deep Inference</title>
    <summary>  Mobile applications are increasingly leveraging complex deep learning models
to deliver features, e.g., image recognition, that require high prediction
accuracy. Such models can be both computation and memory-intensive, even for
newer mobile devices, and are therefore commonly hosted in powerful remote
servers. However, current cloud-based inference services employ static model
selection approach that can be suboptimal for satisfying application SLAs
(service level agreements), as they fail to account for inherent dynamic mobile
environment.
  We introduce a cloud-based technique called ModiPick that dynamically selects
the most appropriate model for each inference request, and adapts its selection
to match different SLAs and execution time budgets that are caused by variable
mobile environments. The key idea of ModiPick is to make inference speed and
accuracy trade-offs at runtime with a pool of managed deep learning models. As
such, ModiPick masks unpredictable inference time budgets and therefore meets
SLA targets, while improving accuracy within mobile network constraints. We
evaluate ModiPick through experiments based on prototype systems and through
simulations. We show that ModiPick achieves comparable inference accuracy to a
greedy approach while improving SLA adherence by up to 88.5%.
</summary>
    <author>
      <name>Samuel S. Ogden</name>
    </author>
    <author>
      <name>Tian Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages (13 with citations), 9 figures. Expansion of work done for
  PhD research qualifier presentation</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.02053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.08999v1</id>
    <updated>2019-09-19T13:53:55Z</updated>
    <published>2019-09-19T13:53:55Z</published>
    <title>Branch prediction related Optimizations for Multithreaded Processors</title>
    <summary>  Major chip manufacturers have all introduced Multithreaded processors. These
processors are used for running a variety of workloads. Efficient resource
utilization is an important design aspect in such processors. Depending on the
workload, mis-speculated execution can severely impact resource utilization and
power utilization. In general, compared to a uniprocessor, a multithreaded
processor may have better tolerance towards mis-speculation. However there can
still be phases where even a multi-threaded processor performance may get
impacted by branch induced mis-speculation. In this paper I propose monitoring
the branch predictor behavior of various hardware threads running on the
multi-threaded processor and use that information as a feedback to the thread
arbiter/picker which schedules the next thread to fetch instructions from. If I
find that a particular thread is going through a phase where it is consistently
mis-predicting its branches and its average branch misprediction stall is above
a specific threshold then I temporarily reduce the priority for picking that
thread. I do a qualitative comparison of various solutions to the problem of
resource inefficiency caused due to mis-speculated branches in multithreaded
processors. This work can be extended by doing a quantitative evaluation.
</summary>
    <author>
      <name>Murthy Durbhakula</name>
    </author>
    <link href="http://arxiv.org/abs/1909.08999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.08999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.01653v1</id>
    <updated>2020-01-06T16:35:46Z</updated>
    <published>2020-01-06T16:35:46Z</published>
    <title>A Fast Analytical Model of Fully Associative Caches</title>
    <summary>  While the cost of computation is an easy to understand local property, the
cost of data movement on cached architectures depends on global state, does not
compose, and is hard to predict. As a result, programmers often fail to
consider the cost of data movement. Existing cache models and simulators
provide the missing information but are computationally expensive. We present a
lightweight cache model for fully associative caches with least recently used
(LRU) replacement policy that gives fast and accurate results. We count the
cache misses without explicit enumeration of all memory accesses by using
symbolic counting techniques twice: 1) to derive the stack distance for each
memory access and 2) to count the memory accesses with stack distance larger
than the cache size. While this technique seems infeasible in theory, due to
non-linearities after the first round of counting, we show that the counting
problems are sufficiently linear in practice. Our cache model often computes
the results within seconds and contrary to simulation the execution time is
mostly problem size independent. Our evaluation measures modeling errors below
0.6% on real hardware. By providing accurate data placement information we
enable memory hierarchy aware software development.
</summary>
    <author>
      <name>Tobias Gysi</name>
    </author>
    <author>
      <name>Tobias Grosser</name>
    </author>
    <author>
      <name>Laurin Brandner</name>
    </author>
    <author>
      <name>Torsten Hoefler</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3314221.3314606</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3314221.3314606" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 16 figures, PLDI19</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.01653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.02530v2</id>
    <updated>2020-06-03T20:28:07Z</updated>
    <published>2020-01-07T18:58:16Z</published>
    <title>On Competitive Analysis for Polling Systems</title>
    <summary>  Polling systems have been widely studied, however most of these studies focus
on polling systems with renewal processes for arrivals and random variables for
service times. There is a need driven by practical applications to study
polling systems with arbitrary arrivals (not restricted to time-varying or in
batches) and revealed service time upon a job's arrival. To address that need,
our work considers a polling system with generic setting and for the first time
provides the worst-case analysis for online scheduling policies in this system.
We provide conditions for the existence of constant competitive ratios, and
competitive lower bounds for general scheduling policies in polling systems.
Our work also bridges the queueing and scheduling communities by proving the
competitive ratios for several well-studied policies in the queueing
literature, such as cyclic policies with exhaustive, gated or l-limited service
disciplines for polling systems.
</summary>
    <author>
      <name>Jin Xu</name>
    </author>
    <author>
      <name>Natarajan Gautam</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/nav.21926</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/nav.21926" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Naval Research Logistics. 2020; 67: 404-419</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2001.02530v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.02530v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.07938v1</id>
    <updated>2020-01-22T10:04:36Z</updated>
    <published>2020-01-22T10:04:36Z</published>
    <title>Automatically Harnessing Sparse Acceleration</title>
    <summary>  Sparse linear algebra is central to many scientific programs, yet compilers
fail to optimize it well. High-performance libraries are available, but
adoption costs are significant. Moreover, libraries tie programs into
vendor-specific software and hardware ecosystems, creating non-portable code.
  In this paper, we develop a new approach based on our specification Language
for implementers of Linear Algebra Computations (LiLAC). Rather than requiring
the application developer to (re)write every program for a given library, the
burden is shifted to a one-off description by the library implementer. The
LiLAC-enabled compiler uses this to insert appropriate library routines without
source code changes.
  LiLAC provides automatic data marshaling, maintaining state between calls and
minimizing data transfers. Appropriate places for library insertion are
detected in compiler intermediate representation, independent of source
languages.
  We evaluated on large-scale scientific applications written in FORTRAN;
standard C/C++ and FORTRAN benchmarks; and C++ graph analytics kernels. Across
heterogeneous platforms, applications and data sets we show speedups of
1.1$\times$ to over 10$\times$ without user intervention.
</summary>
    <author>
      <name>Philip Ginsbach</name>
    </author>
    <author>
      <name>Bruce Collie</name>
    </author>
    <author>
      <name>Michael F. P. O'Boyle</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3377555.3377893</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3377555.3377893" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CC 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.07938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.07938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.10381v2</id>
    <updated>2020-05-05T16:36:25Z</updated>
    <published>2020-01-22T07:35:09Z</published>
    <title>Detecting State Transitions of a Markov Source: Sampling Frequency and
  Age Trade-off</title>
    <summary>  We consider a finite-state Discrete-Time Markov Chain (DTMC) source that can
be sampled for detecting the events when the DTMC transits to a new state. Our
goal is to study the trade-off between sampling frequency and staleness in
detecting the events. We argue that, for the problem at hand, using Age of
Information (AoI) for quantifying the staleness of a sample is conservative and
therefore, introduce \textit{age penalty} for this purpose. We study two
optimization problems: minimize average age penalty subject to an average
sampling frequency constraint, and minimize average sampling frequency subject
to an average age penalty constraint; both are Constrained Markov Decision
Problems. We solve them using linear programming approach and compute Markov
policies that are optimal among all causal policies. Our numerical results
demonstrate that the computed Markov policies not only outperform optimal
periodic sampling policies, but also achieve sampling frequencies close to or
lower than that of an optimal clairvoyant (non-causal) sampling policy, if a
small age penalty is allowed.
</summary>
    <author>
      <name>Jaya Prakash Champati</name>
    </author>
    <author>
      <name>Mikael Skoglund</name>
    </author>
    <author>
      <name>James Gross</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, published in IEEE INFOCOM AoI Workshop 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.10381v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.10381v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.01040v2</id>
    <updated>2021-03-18T04:49:15Z</updated>
    <published>2020-08-03T17:24:52Z</published>
    <title>A Learned Performance Model for Tensor Processing Units</title>
    <summary>  Accurate hardware performance models are critical to efficient code
generation. They can be used by compilers to make heuristic decisions, by
superoptimizers as a minimization objective, or by autotuners to find an
optimal configuration for a specific program. However, they are difficult to
develop because contemporary processors are complex, and the recent
proliferation of deep learning accelerators has increased the development
burden. We demonstrate a method of learning performance models from a corpus of
tensor computation graph programs for Tensor Processing Unit (TPU) instances.
We show that our learned model outperforms a heavily-optimized analytical
performance model on two tasks -- tile-size selection and operator fusion --
and that it helps an autotuner discover faster programs in a setting where
access to TPUs is limited or expensive.
</summary>
    <author>
      <name>Samuel J. Kaufman</name>
    </author>
    <author>
      <name>Phitchaya Mangpo Phothilimthana</name>
    </author>
    <author>
      <name>Yanqi Zhou</name>
    </author>
    <author>
      <name>Charith Mendis</name>
    </author>
    <author>
      <name>Sudip Roy</name>
    </author>
    <author>
      <name>Amit Sabne</name>
    </author>
    <author>
      <name>Mike Burrows</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A version will appear in the Proceedings of the 4th MLSys Conference,
  San Jose, CA, USA, 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.01040v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.01040v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.03904v3</id>
    <updated>2020-11-09T00:21:09Z</updated>
    <published>2020-08-10T05:17:54Z</published>
    <title>Performance Analysis of Priority-Aware NoCs with Deflection Routing
  under Traffic Congestion</title>
    <summary>  Priority-aware networks-on-chip (NoCs) are used in industry to achieve
predictable latency under different workload conditions. These NoCs incorporate
deflection routing to minimize queuing resources within routers and achieve low
latency during low traffic load. However, deflected packets can exacerbate
congestion during high traffic load since they consume the NoC bandwidth.
State-of-the-art analytical models for priority-aware NoCs ignore deflected
traffic despite its significant latency impact during congestion. This paper
proposes a novel analytical approach to estimate end-to-end latency of
priority-aware NoCs with deflection routing under bursty and heavy traffic
scenarios. Experimental evaluations show that the proposed technique
outperforms alternative approaches and estimates the average latency for real
applications with less than 8% error compared to cycle-accurate simulations.
</summary>
    <author>
      <name>Sumit K. Mandal</name>
    </author>
    <author>
      <name>Anish Krishnakumar</name>
    </author>
    <author>
      <name>Raid Ayoub</name>
    </author>
    <author>
      <name>Michael Kishinevsky</name>
    </author>
    <author>
      <name>Umit Y. Ogras</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article is in the Proceedings of ICCAD 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.03904v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.03904v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.08830v1</id>
    <updated>2020-08-20T08:11:16Z</updated>
    <published>2020-08-20T08:11:16Z</published>
    <title>Optimal Load Balancing in Bipartite Graphs</title>
    <summary>  Applications in cloud platforms motivate the study of efficient load
balancing under job-server constraints and server heterogeneity. In this paper,
we study load balancing on a bipartite graph where left nodes correspond to job
types and right nodes correspond to servers, with each edge indicating that a
job type can be served by a server. Thus edges represent locality constraints,
i.e., each job can only be served at servers which contained certain data
and/or machine learning (ML) models. Servers in this system can have
heterogeneous service rates. In this setting, we investigate the performance of
two policies named Join-the-Fastest-of-the-Shortest-Queue (JFSQ) and
Join-the-Fastest-of-the-Idle-Queue (JFIQ), which are simple variants of
Join-the-Shortest-Queue and Join-the-Idle-Queue, where ties are broken in favor
of the fastest servers. Under a "well-connected" graph condition, we show that
JFSQ and JFIQ are asymptotically optimal in the mean response time when the
number of servers goes to infinity. In addition to asymptotic optimality, we
also obtain upper bounds on the mean response time for finite-size systems. We
further show that the well-connectedness condition can be satisfied by a random
bipartite graph construction with relatively sparse connectivity.
</summary>
    <author>
      <name>Wentao Weng</name>
    </author>
    <author>
      <name>Xingyu Zhou</name>
    </author>
    <author>
      <name>R. Srikant</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.08830v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.08830v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.13145v1</id>
    <updated>2020-08-30T11:44:37Z</updated>
    <published>2020-08-30T11:44:37Z</published>
    <title>Performance portability through machine learning guided kernel selection
  in SYCL libraries</title>
    <summary>  Automatically tuning parallel compute kernels allows libraries and frameworks
to achieve performance on a wide range of hardware, however these techniques
are typically focused on finding optimal kernel parameters for particular input
sizes and parameters. General purpose compute libraries must be able to cater
to all inputs and parameters provided by a user, and so these techniques are of
limited use. Additionally, parallel programming frameworks such as SYCL require
that the kernels be deployed in a binary format embedded within the library. As
such it is impractical to deploy a large number of possible kernel
configurations without inflating the library size.
  Machine learning methods can be used to mitigate against both of these
problems and provide performance for general purpose routines with a limited
number of kernel configurations. We show that unsupervised clustering methods
can be used to select a subset of the possible kernels that should be deployed
and that simple classification methods can be trained to select from these
kernels at runtime to give good performance. As these techniques are fully
automated, relying only on benchmark data, the tuning process for new hardware
or problems does not require any developer effort or expertise.
</summary>
    <author>
      <name>John Lawson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.13145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.13145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.04061v2</id>
    <updated>2020-11-24T17:05:59Z</updated>
    <published>2020-09-09T01:35:08Z</published>
    <title>GPA: A GPU Performance Advisor Based on Instruction Sampling</title>
    <summary>  Developing efficient GPU kernels can be difficult because of the complexity
of GPU architectures and programming models. Existing performance tools only
provide coarse-grained suggestions at the kernel level, if any. In this paper,
we describe GPA, a performance advisor for NVIDIA GPUs that suggests potential
code optimization opportunities at a hierarchy of levels, including individual
lines, loops, and functions. To relieve users of the burden of interpreting
performance counters and analyzing bottlenecks, GPA uses data flow analysis to
approximately attribute measured instruction stalls to their root causes and
uses information about a program's structure and the GPU to match inefficiency
patterns with suggestions for optimization. To quantify each suggestion's
potential benefits, we developed PC sampling-based performance models to
estimate its speedup. Our experiments with benchmarks and applications show
that GPA provides an insightful report to guide performance optimization. Using
GPA, we obtained speedups on a Volta V100 GPU ranging from 1.01$\times$ to
3.53$\times$, with a geometric mean of 1.22$\times$.
</summary>
    <author>
      <name>Keren Zhou</name>
    </author>
    <author>
      <name>Xiaozhu Meng</name>
    </author>
    <author>
      <name>Ryuichi Sai</name>
    </author>
    <author>
      <name>John Mellor-Crummey</name>
    </author>
    <link href="http://arxiv.org/abs/2009.04061v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.04061v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.07400v1</id>
    <updated>2020-09-16T00:29:13Z</updated>
    <published>2020-09-16T00:29:13Z</published>
    <title>tinyMD: A Portable and Scalable Implementation for Pairwise Interactions
  Simulations</title>
    <summary>  This paper investigates the suitability of the AnyDSL partial evaluation
framework to implement tinyMD: an efficient, scalable, and portable simulation
of pairwise interactions among particles. We compare tinyMD with the miniMD
proxy application that scales very well on parallel supercomputers. We discuss
the differences between both implementations and contrast miniMD's performance
for single-node CPU and GPU targets, as well as its scalability on SuperMUC-NG
and Piz Daint supercomputers. Additionaly, we demonstrate tinyMD's flexibility
by coupling it with the waLBerla multi-physics framework. This allow us to
execute tinyMD simulations using the load-balancing mechanism implemented in
waLBerla.
</summary>
    <author>
      <name>Rafael Ravedutti L. Machado</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Chair for System Simulation at University of Erlangen-Nürnberg</arxiv:affiliation>
    </author>
    <author>
      <name>Jonas Schmitt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Chair for System Simulation at University of Erlangen-Nürnberg</arxiv:affiliation>
    </author>
    <author>
      <name>Sebastian Eibl</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Chair for System Simulation at University of Erlangen-Nürnberg</arxiv:affiliation>
    </author>
    <author>
      <name>Jan Eitzinger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Regional Computer Center Erlangen at University of Erlangen-Nürnberg</arxiv:affiliation>
    </author>
    <author>
      <name>Roland Leißa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Saarland Informatics Campus at Saarland University</arxiv:affiliation>
    </author>
    <author>
      <name>Sebastian Hack</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Saarland Informatics Campus at Saarland University</arxiv:affiliation>
    </author>
    <author>
      <name>Arsène Pérard-Gayot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Saarland Informatics Campus at Saarland University</arxiv:affiliation>
    </author>
    <author>
      <name>Richard Membarth</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Saarland Informatics Campus at Saarland University</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">German Research Center for Artificial Intelligence at Saarland Informatics Campus</arxiv:affiliation>
    </author>
    <author>
      <name>Harald Köstler</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Chair for System Simulation at University of Erlangen-Nürnberg</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 8 figures, submitted to Journal of Computational Science</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.07400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.07400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.2, D.1.3, D.3.3, J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.07935v3</id>
    <updated>2021-09-29T13:31:20Z</updated>
    <published>2020-09-16T21:08:27Z</published>
    <title>Towards an Objective Metric for the Performance of Exact Triangle Count</title>
    <summary>  The performance of graph algorithms is often measured in terms of the number
of traversed edges per second (TEPS). However, this performance metric is
inadequate for a graph operation such as exact triangle counting. In triangle
counting, execution times on graphs with a similar number of edges can be
distinctly different as demonstrated by results from the past Graph Challenge
entries. We discuss the need for an objective performance metric for graph
operations and the desired characteristics of such a metric such that it more
accurately captures the interactions between the amount of work performed and
the capabilities of the hardware on which the code is executed. Using exact
triangle counting as an example, we derive a metric that captures how certain
techniques employed in many implementations improve performance. We demonstrate
that our proposed metric can be used to evaluate and compare multiple
approaches for triangle counting, using a SIMD approach as a case study against
a scalar baseline.
</summary>
    <author>
      <name>Mark P. Blanco</name>
    </author>
    <author>
      <name>Scott McMillan</name>
    </author>
    <author>
      <name>Tze Meng Low</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HPEC43674.2020.9286188</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HPEC43674.2020.9286188" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages, 2020 IEEE High Performance Extreme Computing
  Conference(HPEC)</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.07935v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.07935v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.09433v1</id>
    <updated>2020-09-20T14:07:53Z</updated>
    <published>2020-09-20T14:07:53Z</published>
    <title>On the Throughput Optimization in Large-Scale Batch-Processing Systems</title>
    <summary>  We analyze a data-processing system with $n$ clients producing jobs which are
processed in \textit{batches} by $m$ parallel servers; the system throughput
critically depends on the batch size and a corresponding sub-additive speedup
function. In practice, throughput optimization relies on numerical searches for
the optimal batch size, a process that can take up to multiple days in existing
commercial systems. In this paper, we model the system in terms of a closed
queueing network; a standard Markovian analysis yields the optimal throughput
in $\omega\left(n^4\right)$ time. Our main contribution is a mean-field model
of the system for the regime where the system size is large. We show that the
mean-field model has a unique, globally attractive stationary point which can
be found in closed form and which characterizes the asymptotic throughput of
the system as a function of the batch size. Using this expression we find the
\textit{asymptotically} optimal throughput in $O(1)$ time. Numerical settings
from a large commercial system reveal that this asymptotic optimum is accurate
in practical finite regimes.
</summary>
    <author>
      <name>Sounak Kar</name>
    </author>
    <author>
      <name>Robin Rehrmann</name>
    </author>
    <author>
      <name>Arpan Mukhopadhyay</name>
    </author>
    <author>
      <name>Bastian Alt</name>
    </author>
    <author>
      <name>Florin Ciucu</name>
    </author>
    <author>
      <name>Heinz Koeppl</name>
    </author>
    <author>
      <name>Carsten Binnig</name>
    </author>
    <author>
      <name>Amr Rizk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.09433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.11067v1</id>
    <updated>2020-09-23T11:40:38Z</updated>
    <published>2020-09-23T11:40:38Z</published>
    <title>Correlation Coefficient Analysis of the Age of Information in
  Multi-Source Systems</title>
    <summary>  This paper studies the age of information (AoI) on an information updating
system such that multiple sources share one server to process packets of
updated information. In such systems, packets from different sources compete
for the server, and thus they may suffer from being interrupted, being
backlogged, and becoming stale. Therefore, in order to grasp structures of such
systems, it is crucially important to study a metric indicating a correlation
of different sources. In this paper, we aim to analyze the correlation of AoIs
on a single-server queueing system with multiple sources. As our contribution,
we provide the closed-form expression of the correlation coefficient of the
AoIs. To this end, we first derive the Laplace-Stieltjes transform of the
stationary distribution of each AoI for the multiple sources. Some nontrivial
properties on the systems are revealed from our analysis results.
</summary>
    <author>
      <name>Yukang Jiang</name>
    </author>
    <author>
      <name>Kiichi Tokuyama</name>
    </author>
    <author>
      <name>Yuichiro Wada</name>
    </author>
    <author>
      <name>Moeko Yajima</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.11067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; H.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.11806v1</id>
    <updated>2020-09-24T16:49:24Z</updated>
    <published>2020-09-24T16:49:24Z</published>
    <title>Investigating Applications on the A64FX</title>
    <summary>  The A64FX processor from Fujitsu, being designed for computational simulation
and machine learning applications, has the potential for unprecedented
performance in HPC systems. In this paper, we evaluate the A64FX by
benchmarking against a range of production HPC platforms that cover a number of
processor technologies. We investigate the performance of complex scientific
applications across multiple nodes, as well as single node and mini-kernel
benchmarks. This paper finds that the performance of the A64FX processor across
our chosen benchmarks often significantly exceeds other platforms, even without
specific application optimisations for the processor instruction set or
hardware. However, this is not true for all the benchmarks we have undertaken.
Furthermore, the specific configuration of applications can have an impact on
the runtime and performance experienced.
</summary>
    <author>
      <name>Adrian Jackson</name>
    </author>
    <author>
      <name>Michèle Weiland</name>
    </author>
    <author>
      <name>Nick Brown</name>
    </author>
    <author>
      <name>Andrew Turner</name>
    </author>
    <author>
      <name>Mark Parsons</name>
    </author>
    <link href="http://arxiv.org/abs/2009.11806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.13903v1</id>
    <updated>2020-09-29T09:52:59Z</updated>
    <published>2020-09-29T09:52:59Z</published>
    <title>Performance Modeling of Streaming Kernels and Sparse Matrix-Vector
  Multiplication on A64FX</title>
    <summary>  The A64FX CPU powers the current number one supercomputer on the Top500 list.
Although it is a traditional cache-based multicore processor, its peak
performance and memory bandwidth rival accelerator devices. Generating
efficient code for such a new architecture requires a good understanding of its
performance features. Using these features, we construct the
Execution-Cache-Memory (ECM) performance model for the A64FX processor in the
FX700 supercomputer and validate it using streaming loops. We also identify
architectural peculiarities and derive optimization hints. Applying the ECM
model to sparse matrix-vector multiplication (SpMV), we motivate why the CRS
matrix storage format is inappropriate and how the SELL-C-sigma format with
suitable code optimizations can achieve bandwidth saturation for SpMV.
</summary>
    <author>
      <name>Christie L. Alappat</name>
    </author>
    <author>
      <name>Jan Laukemann</name>
    </author>
    <author>
      <name>Thomas Gruber</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <author>
      <name>Nils Meyer</name>
    </author>
    <author>
      <name>Tilo Wettig</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/PMBS51919.2020.00006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/PMBS51919.2020.00006" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.13903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.13903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.00631v1</id>
    <updated>2020-10-01T11:59:00Z</updated>
    <published>2020-10-01T11:59:00Z</published>
    <title>Stability for Two-class Multiserver-job Systems</title>
    <summary>  Multiserver-job systems, where jobs require concurrent service at many
servers, occur widely in practice. Much is known in the dropping setting, where
jobs are immediately discarded if they require more servers than are currently
available. However, very little is known in the more practical setting where
jobs queue instead.
  In this paper, we derive a closed-form analytical expression for the
stability region of a two-class (non-dropping) multiserver-job system where
each class of jobs requires a distinct number of servers and requires a
distinct exponential distribution of service time, and jobs are served in
first-come-first-served (FCFS) order. This is the first result of any kind for
an FCFS multiserver-job system where the classes have distinct service
distributions. Our work is based on a technique that leverages the idea of a
"saturated" system, in which an unlimited number of jobs are always available.
  Our analytical formula provides insight into the behavior of FCFS
multiserver-job systems, highlighting the huge wastage (idle servers while jobs
are in the queue) that can occur, as well as the nonmonotonic effects of the
service rates on wastage.
</summary>
    <author>
      <name>Isaac Grosof</name>
    </author>
    <author>
      <name>Mor Harchol-Balter</name>
    </author>
    <author>
      <name>Alan Scheller-Wolf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.00631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.00631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.07226v3</id>
    <updated>2021-09-14T16:03:33Z</updated>
    <published>2020-10-14T16:40:23Z</published>
    <title>Discriminating Equivalent Algorithms via Relative Performance</title>
    <summary>  In scientific computing, it is common that a mathematical expression can be
computed by many different algorithms (sometimes over hundreds), each
identifying a specific sequence of library calls. Although mathematically
equivalent, those algorithms might exhibit significant differences in terms of
performance. However in practice, due to fluctuations, there is not one
algorithm that consistently performs noticeably better than the rest. For this
reason, with this work we aim to identify not the one best algorithm, but the
subset of algorithms that are reliably faster than the rest. To this end,
instead of using the usual approach of quantifying the performance of an
algorithm in absolute terms, we present a measurement-based clustering approach
to sort the algorithms into equivalence (or performance) classes using
pair-wise comparisons. We show that this approach, based on relative
performance, leads to robust identification of the fastest algorithms even
under noisy system conditions. Furthermore, it enables the development of
practical machine learning models for automatic algorithm selection.
</summary>
    <author>
      <name>Aravind Sankaran</name>
    </author>
    <author>
      <name>Paolo Bientinesi</name>
    </author>
    <link href="http://arxiv.org/abs/2010.07226v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.07226v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.03685v1</id>
    <updated>2020-11-07T04:23:07Z</updated>
    <published>2020-11-07T04:23:07Z</published>
    <title>An approach to define Very High Capacity Networks with improved quality
  at an affordable cost</title>
    <summary>  This paper aims to propose one possible approach in the setting of VHCNs
(Very High Capacity Networks) performance targets that should be capable of
promoting efficient investments for operators and, at the same time, improving
the benefits for end-users. To this aim, we suggest relying on some specific
KPIs (Key Performance Indicators), especially throughput - i.e., the bandwidth
as perceived by the customer - valid at the application layer, instead of the
physical layer data-rate. In this regard, the paper underlines that the
bandwidth perceived is strictly linked to the latency. The most important
implication is that some of the most demanding services envisaged for the
future (e.g., mobile virtual and augmented reality, tactile internet) cannot be
met by merely increasing the low-level protocol data-rate. Therefore, for the
VHCNs reducing latency through Edge Cloud Computing (ECC) is a mandatory
pre-requisite.
</summary>
    <author>
      <name>Giovanni Santella</name>
    </author>
    <author>
      <name>Francesco Vatalaro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.03685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.03685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.07160v1</id>
    <updated>2020-11-13T22:55:15Z</updated>
    <published>2020-11-13T22:55:15Z</published>
    <title>Phoebe: Reuse-Aware Online Caching with Reinforcement Learning for
  Emerging Storage Models</title>
    <summary>  With data durability, high access speed, low power efficiency and byte
addressability, NVMe and SSD, which are acknowledged representatives of
emerging storage technologies, have been applied broadly in many areas.
However, one key issue with high-performance adoption of these technologies is
how to properly define intelligent cache layers such that the performance gap
between emerging technologies and main memory can be well bridged. To this end,
we propose Phoebe, a reuse-aware reinforcement learning framework for the
optimal online caching that is applicable for a wide range of emerging storage
models. By continuous interacting with the cache environment and the data
stream, Phoebe is capable to extract critical temporal data dependency and
relative positional information from a single trace, becoming ever smarter over
time. To reduce training overhead during online learning, we utilize periodical
training to amortize costs. Phoebe is evaluated on a set of Microsoft cloud
storage workloads. Experiment results show that Phoebe is able to close the gap
of cache miss rate from LRU and a state-of-the-art online learning based cache
policy to the Belady's optimal policy by 70.3% and 52.6%, respectively.
</summary>
    <author>
      <name>Nan Wu</name>
    </author>
    <author>
      <name>Pengcheng Li</name>
    </author>
    <link href="http://arxiv.org/abs/2011.07160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.07160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.07401v2</id>
    <updated>2022-04-07T17:48:09Z</updated>
    <published>2020-11-14T22:12:27Z</published>
    <title>RL-QN: A Reinforcement Learning Framework for Optimal Control of
  Queueing Systems</title>
    <summary>  With the rapid advance of information technology, network systems have become
increasingly complex and hence the underlying system dynamics are often unknown
or difficult to characterize. Finding a good network control policy is of
significant importance to achieve desirable network performance (e.g., high
throughput or low delay). In this work, we consider using model-based
reinforcement learning (RL) to learn the optimal control policy for queueing
networks so that the average job delay (or equivalently the average queue
backlog) is minimized. Traditional approaches in RL, however, cannot handle the
unbounded state spaces of the network control problem. To overcome this
difficulty, we propose a new algorithm, called Reinforcement Learning for
Queueing Networks (RL-QN), which applies model-based RL methods over a finite
subset of the state space, while applying a known stabilizing policy for the
rest of the states. We establish that the average queue backlog under RL-QN
with an appropriately constructed subset can be arbitrarily close to the
optimal result. We evaluate RL-QN in dynamic server allocation, routing and
switching problems. Simulation results show that RL-QN minimizes the average
queue backlog effectively.
</summary>
    <author>
      <name>Bai Liu</name>
    </author>
    <author>
      <name>Qiaomin Xie</name>
    </author>
    <author>
      <name>Eytan Modiano</name>
    </author>
    <link href="http://arxiv.org/abs/2011.07401v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.07401v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.07968v3</id>
    <updated>2020-11-19T06:35:46Z</updated>
    <published>2020-11-16T14:00:12Z</published>
    <title>Tools for modelling and simulating the Smart Grid</title>
    <summary>  The Smart Grid (SG) is a Cyber-Physical System (CPS) considered a critical
infrastructure divided into cyber (software) and physical (hardware)
counterparts that complement each other. It is responsible for timely power
provision wrapped by Information and Communication Technologies (ICT) for
handling bi-directional energy flows in electric power grids. Enacting control
and performance over the massive infrastructure of the SG requires convenient
analysis methods. Modelling and simulation (M&amp;S) is a performance evaluation
technique used to study virtually any system by testing designs and
artificially creating 'what-if' scenarios for system reasoning and advanced
analysis. M&amp;S avoids stressing the actual physical infrastructure and systems
in production by addressing the problem in a purely computational perspective.
Present work compiles a non-exhaustive list of tools for M&amp;S of interest when
tackling SG capabilities. Our contribution is to delineate available options
for modellers when considering power systems in combination with ICT. We also
show the auxiliary tools and details of most relevant solutions pointing out
major features and combinations over the years.
</summary>
    <author>
      <name>Ricardo M. Czekster</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 1 table, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.07968v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.07968v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.03751v1</id>
    <updated>2021-02-07T09:22:55Z</updated>
    <published>2021-02-07T09:22:55Z</published>
    <title>DV-DVFS: Merging Data Variety and DVFS Technique to Manage the Energy
  Consumption of Big Data Processing</title>
    <summary>  Data variety is one of the most important features of Big Data. Data variety
is the result of aggregating data from multiple sources and uneven distribution
of data. This feature of Big Data causes high variation in the consumption of
processing resources such as CPU consumption. This issue has been overlooked in
previous works. To overcome the mentioned problem, in the present work, we used
Dynamic Voltage and Frequency Scaling (DVFS) to reduce the energy consumption
of computation. To this goal, we consider two types of deadlines as our
constraint. Before applying the DVFS technique to computer nodes, we estimate
the processing time and the frequency needed to meet the deadline. In the
evaluation phase, we have used a set of data sets and applications. The
experimental results show that our proposed approach surpasses the other
scenarios in processing real datasets. Based on the experimental results in
this paper, DV-DVFS can achieve up to 15% improvement in energy consumption.
</summary>
    <author>
      <name>Hossein Ahmadvand</name>
    </author>
    <author>
      <name>Fouzhan Foroutan</name>
    </author>
    <author>
      <name>Mahmood Fathy</name>
    </author>
    <link href="http://arxiv.org/abs/2102.03751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.03751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.05204v1</id>
    <updated>2021-02-10T01:07:41Z</updated>
    <published>2021-02-10T01:07:41Z</published>
    <title>NumaPerf: Predictive and Full NUMA Profiling</title>
    <summary>  Parallel applications are extremely challenging to achieve the optimal
performance on the NUMA architecture, which necessitates the assistance of
profiling tools. However, existing NUMA-profiling tools share some similar
shortcomings, such as portability, effectiveness, and helpfulness issues. This
paper proposes a novel profiling tool - NumaPerf - that overcomes these issues.
NumaPerf aims to identify potential performance issues for any NUMA
architecture, instead of only on the current hardware. To achieve this,
NumaPerf focuses on memory sharing patterns between threads, instead of real
remote accesses. NumaPerf further detects potential thread migrations and load
imbalance issues that could significantly affect the performance but are
omitted by existing profilers. NumaPerf also separates cache coherence issues
that may require different fix strategies. Based on our extensive evaluation,
NumaPerf is able to identify more performance issues than any existing tool,
while fixing these bugs leads to up to 5.94x performance speedup.
</summary>
    <author>
      <name>Xin Zhao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Massachusetts Amherst</arxiv:affiliation>
    </author>
    <author>
      <name>Jin Zhou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Massachusetts Amherst</arxiv:affiliation>
    </author>
    <author>
      <name>Hui Guan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Massachusetts Amherst</arxiv:affiliation>
    </author>
    <author>
      <name>Wei Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Texas at San Antonio</arxiv:affiliation>
    </author>
    <author>
      <name>Xu Liu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">North Carolina State University</arxiv:affiliation>
    </author>
    <author>
      <name>Tongping Liu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Massachusetts Amherst</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2102.05204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.05204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.07731v2</id>
    <updated>2023-01-03T21:46:00Z</updated>
    <published>2021-02-15T18:30:43Z</published>
    <title>An In-Depth Investigation of the Performance Characteristics of
  Hyperledger Fabric</title>
    <summary>  Private permissioned blockchains are deployed in ever greater numbers to
facilitate cross-organizational processes in various industries, particularly
in supply chain management. One popular example of this trend is Hyperledger
Fabric. Compared to public permissionless blockchains, it promises improved
performance and provides certain features that address key requirements of
enterprises. However, also permissioned blockchains are still not as scalable
as centralized systems, and due to the scarcity of theoretical results and
empirical data, their real-world performance cannot be predicted with the
necessary precision. We intend to address this issue by conducting an in-depth
performance analysis of Hyperledger Fabric. The paper presents a detailed
compilation of various performance characteristics using an enhanced version of
the Distributed Ledger Performance Scan (DLPS). Researchers and practitioners
alike can use the various performance properties identified and discussed as
guidelines to better configure and implement their Hyperledger Fabric network.
Likewise, they are encouraged to use the DLPS framework to conduct their
measurements.
</summary>
    <author>
      <name>Tobias Guggenberger</name>
    </author>
    <author>
      <name>Johannes Sedlmeir</name>
    </author>
    <author>
      <name>Gilbert Fridgen</name>
    </author>
    <author>
      <name>André Luckow</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cie.2022.108716</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cie.2022.108716" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers &amp; Industrial Engineering (2022), Volume 173, 108716</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2102.07731v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.07731v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4; J.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.12740v2</id>
    <updated>2021-03-03T09:37:21Z</updated>
    <published>2021-02-25T09:10:27Z</published>
    <title>Performance Comparison for Scientific Computations on the Edge via
  Relative Performance</title>
    <summary>  In a typical Internet-of-Things setting that involves scientific
applications, a target computation can be evaluated in many different ways
depending on the split of computations among various devices. On the one hand,
different implementations (or algorithms)--equivalent from a mathematical
perspective--might exhibit significant difference in terms of performance. On
the other hand, some of the implementations are likely to show similar
performance characteristics. In this paper, we focus on analyzing the
performance of a given set of algorithms by clustering them into performance
classes. To this end, we use a measurement-based approach to evaluate and score
algorithms based on pair-wise comparisons; we refer to this approach
as"Relative performance analysis". Each comparison yields one of three
outcomes: one algorithm can be "better", "worse", or "equivalent" to another;
those algorithms evaluating to have equivalent performance are merged into the
same performance class. We show that our clustering methodology facilitates
algorithm selection with respect to more than one metric; for instance, from
the subset of equivalently fast algorithms, one could then select an algorithm
that consumes the least energy on a certain device.
</summary>
    <author>
      <name>Aravind Sankaran</name>
    </author>
    <author>
      <name>Paolo Bientinesi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IPDPSW52791.2021.00132</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IPDPSW52791.2021.00132" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2021 IEEE International Parallel and Distributed Processing
  Symposium Workshops (IPDPSW)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2102.12740v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.12740v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.12848v1</id>
    <updated>2021-02-25T13:40:17Z</updated>
    <published>2021-02-25T13:40:17Z</published>
    <title>HPC AI500: Representative, Repeatable and Simple HPC AI Benchmarking</title>
    <summary>  Recent years witness a trend of applying large-scale distributed deep
learning algorithms (HPC AI) in both business and scientific computing areas,
whose goal is to speed up the training time to achieve a state-of-the-art
quality. The HPC AI benchmarks accelerate the process. Unfortunately,
benchmarking HPC AI systems at scale raises serious challenges. This paper
presents a representative, repeatable and simple HPC AI benchmarking
methodology. Among the seventeen AI workloads of AIBench Training -- by far the
most comprehensive AI Training benchmarks suite -- we choose two representative
and repeatable AI workloads. The selected HPC AI benchmarks include both
business and scientific computing: Image Classification and Extreme Weather
Analytics. To rank HPC AI systems, we present a new metric named Valid FLOPS,
emphasizing both throughput performance and a target quality. The
specification, source code, datasets, and HPC AI500 ranking numbers are
publicly available from \url{https://www.benchcouncil.org/HPCAI500/}.
</summary>
    <author>
      <name>Zihan Jiang</name>
    </author>
    <author>
      <name>Wanling Gao</name>
    </author>
    <author>
      <name>Fei Tang</name>
    </author>
    <author>
      <name>Xingwang Xiong</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Chuanxin Lan</name>
    </author>
    <author>
      <name>Chunjie Luo</name>
    </author>
    <author>
      <name>Hongxiao Li</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2007.00279</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.12848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.12848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.10299v2</id>
    <updated>2021-12-06T11:21:04Z</updated>
    <published>2021-07-21T18:22:37Z</published>
    <title>Dynamic RF Combining for Multi-Antenna Ambient Energy Harvesting</title>
    <summary>  Ambient radio frequency (RF) energy harvesting (EH) technology is key to
realize self-sustainable, always-on, low-power, massive Internet of Things
networks. Typically, rigid (non-adaptable to channel fluctuations)
multi-antenna receive architectures are proposed to support reliable EH
operation. Herein, we introduce a dynamic RF combining architecture for ambient
RF EH use cases, and exemplify the attainable performance gains via three
simple phase shifts' exploration mechanisms, namely, brute force (BF),
sequential testing (ST) and codebook based (CB). Among the proposed mechanisms,
BF demands the highest power consumption, while CB requires the
highest-resolution phase shifters, thus tipping the scales in favor of ST.
Finally, we show that the performance gains of ST over a rigid RF combining
scheme increase with the number of receive antennas and energy transmitters'
deployment density.
</summary>
    <author>
      <name>Onel Luis Alcaraz López</name>
    </author>
    <author>
      <name>Bruno Clerckx</name>
    </author>
    <author>
      <name>Matti Latva-aho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pags, 5 figs, accepted for publication in IEEE Wireless
  Communications Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.10299v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.10299v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.03905v1</id>
    <updated>2022-01-11T12:24:13Z</updated>
    <published>2022-01-11T12:24:13Z</published>
    <title>Performance of Load Balancers with Bounded Maximum Queue Length in case
  of Non-Exponential Job Sizes</title>
    <summary>  In large-scale distributed systems, balancing the load in an efficient way is
crucial in order to achieve low latency. Recently, some load balancing policies
have been suggested which are able to achieve a bounded maximum queue length in
the large-scale limit. However, these policies have thus far only been studied
in case of exponential job sizes. As job sizes are more variable in real
systems, we investigate how the performance of these policies (and in
particular the value of these bounds) is impacted by the job size distribution.
  We present a unified analysis which can be used to compute the bound on the
queue length in case of phase-type distributed job sizes for four load
balancing policies. We find that in most cases, the bound on the maximum queue
length can be expressed in closed form. In addition, we obtain job size
(in)dependent bounds on the expected response time.
  Our methodology relies on the use of the cavity process. That is, we
conjecture that the cavity process captures the behaviour of the real system as
the system size grows large. For each policy, we illustrate the accuracy of the
cavity process by means of simulation.
</summary>
    <author>
      <name>Tim Hellemans</name>
    </author>
    <author>
      <name>Grzegorz Kielanski</name>
    </author>
    <author>
      <name>Benny Van Houdt</name>
    </author>
    <link href="http://arxiv.org/abs/2201.03905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.03905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.05884v1</id>
    <updated>2022-01-15T16:32:51Z</updated>
    <published>2022-01-15T16:32:51Z</published>
    <title>Calipers: A Criticality-aware Framework for Modeling Processor
  Performance</title>
    <summary>  Computer architecture design space is vast and complex. Tools are needed to
explore new ideas and gain insights quickly, with low efforts and at a desired
accuracy. We propose Calipers, a criticality-based framework to model key
abstractions of complex architectures and a program's execution using dynamic
event-dependence graphs. By applying graph algorithms, Calipers can track
instruction and event dependencies, compute critical paths, and analyze
architecture bottlenecks. By manipulating the graph, Calipers enables
architects to investigate a wide range of Instruction Set Architecture (ISA)
and microarchitecture design choices/"what-if" scenarios during both early- and
late-stage design space exploration without recompiling and rerunning the
program. Calipers can model in-order and out-of-order microarchitectures,
structural hazards, and different types of ISAs, and can evaluate multiple
ideas in a single run. Modeling algorithms are described in detail.
  We apply Calipers to explore and gain insights in complex microarchitectural
and ISA ideas for RISC and EDGE processors, at lower effort than cycle-accurate
simulators and with comparable accuracy. For example, among a variety of
investigations presented in the paper, experiments show that targeting only a
fraction of critical loads can help realize most benefits of value prediction.
</summary>
    <author>
      <name>Hossein Golestani</name>
    </author>
    <author>
      <name>Rathijit Sen</name>
    </author>
    <author>
      <name>Vinson Young</name>
    </author>
    <author>
      <name>Gagan Gupta</name>
    </author>
    <link href="http://arxiv.org/abs/2201.05884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.05884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.11575v2</id>
    <updated>2022-12-04T17:29:36Z</updated>
    <published>2022-02-23T15:50:09Z</published>
    <title>Shisha: Online scheduling of CNN pipelines on heterogeneous
  architectures</title>
    <summary>  Chiplets have become a common methodology in modern chip design. Chiplets
improve yield and enable heterogeneity at the level of cores, memory subsystem
and the interconnect. Convolutional Neural Networks (CNNs) have high
computational, bandwidth and memory capacity requirements owing to the
increasingly large amount of weights. Thus to exploit chiplet-based
architectures, CNNs must be optimized in terms of scheduling and workload
distribution among computing resources. We propose Shisha, an online approach
to generate and schedule parallel CNN pipelines on chiplet architectures.
Shisha targets heterogeneity in compute performance and memory bandwidth and
tunes the pipeline schedule through a fast online exploration technique. We
compare Shisha with Simulated Annealing, Hill Climbing and Pipe-Search. On
average, the convergence time is improved by ~35x in Shisha compared to other
exploration algorithms. Despite the quick exploration, Shisha's solution is
often better than that of other heuristic exploration algorithms.
</summary>
    <author>
      <name>Pirah Noor Soomro</name>
    </author>
    <author>
      <name>Mustafa Abduljabbar</name>
    </author>
    <author>
      <name>Jeronimo Castrillon</name>
    </author>
    <author>
      <name>Miquel Pericàs</name>
    </author>
    <link href="http://arxiv.org/abs/2202.11575v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.11575v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.04575v1</id>
    <updated>2022-05-09T21:47:08Z</updated>
    <published>2022-05-09T21:47:08Z</published>
    <title>JCSP: Joint Caching and Service Placement for Edge Computing Systems</title>
    <summary>  With constrained resources, what, where, and how to cache at the edge is one
of the key challenges for edge computing systems. The cached items include not
only the application data contents but also the local caching of edge services
that handle incoming requests. However, current systems separate the contents
and services without considering the latency interplay of caching and queueing.
Therefore, in this paper, we propose a novel class of stochastic models that
enable the optimization of content caching and service placement decisions
jointly. We first explain how to apply layered queueing networks (LQNs) models
for edge service placement and show that combining this with genetic algorithms
provides higher accuracy in resource allocation than an established baseline.
Next, we extend LQNs with caching components to establish a joint modeling
method for content caching and service placement (JCSP) and present analytical
methods to analyze the resulting model. Finally, we simulate real-world Azure
traces to evaluate the JCSP method and find that JCSP achieves up to 35%
improvement in response time and 500MB reduction in memory usage than baseline
heuristics for edge caching resource allocation.
</summary>
    <author>
      <name>Yicheng Gao</name>
    </author>
    <author>
      <name>Giuliano Casale</name>
    </author>
    <link href="http://arxiv.org/abs/2205.04575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.04575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.11753v1</id>
    <updated>2022-05-24T03:26:42Z</updated>
    <published>2022-05-24T03:26:42Z</published>
    <title>Efficient LSM-Tree Key-Value Data Management on Hybrid SSD/HDD Zoned
  Storage</title>
    <summary>  Zoned storage devices, such as zoned namespace (ZNS) solid-state drives
(SSDs) and host-managed shingled magnetic recording (HM-SMR) hard-disk drives
(HDDs), expose interfaces for host-level applications to support fine-grained,
high-performance storage management. Combining ZNS SSDs and HM-SMR HDDs into a
unified hybrid storage system is a natural direction to scale zoned storage at
low cost, yet how to effectively incorporate zoned storage awareness into
hybrid storage is a non-trivial issue. We make a case for key-value (KV) stores
based on log-structured merge trees (LSM-trees) as host-level applications, and
present HHZS, a middleware system that bridges an LSM-tree KV store with hybrid
zoned storage devices based on hints. HHZS leverages hints issued by the
flushing, compaction, and caching operations of the LSM-tree KV store to manage
KV objects in placement, migration, and caching in hybrid ZNS SSD and HM-SMR
HDD zoned storage. Experiments show that our HHZS prototype, when running on
real ZNS SSD and HM-SMR HDD devices, achieves the highest throughput compared
with all baselines under various settings.
</summary>
    <author>
      <name>Jinhong Li</name>
    </author>
    <author>
      <name>Qiuping Wang</name>
    </author>
    <author>
      <name>Patrick P. C. Lee</name>
    </author>
    <link href="http://arxiv.org/abs/2205.11753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.11753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.10164v1</id>
    <updated>2022-06-21T07:43:46Z</updated>
    <published>2022-06-21T07:43:46Z</published>
    <title>Efficient scheduling in redundancy systems with general service times</title>
    <summary>  We characterize the impact of scheduling policies on the mean response time
in nested systems with cancel-on-complete redundancy. We consider not only
redundancy-oblivious policies, such as FCFS and ROS, but also redundancy-aware
policies of the form $\Pi_1-\Pi_2$, where $\Pi_1$ discriminates among job
classes (e.g., least-redundant-first (LRF), most-redundant-first (MRF)) and
$\Pi_2$ discriminates among jobs of the same class. Assuming that jobs have
independent and identically distributed (i.i.d.) copies, we prove the
following: (i) When jobs have exponential service times, LRF policies
outperform any other policy. (ii) When service times are New-Worse-than-Used,
MRF-FCFS outperforms LRF-FCFS as the variability of the service time grows
infinitely large. (iii) When service times are New-Better-than-Used, LRF-ROS
(resp. MRF-ROS) outperforms LRF-FCFS (resp. MRF-FCFS) in a two-server system.
Statement (iii) also holds when job sizes follow a general distribution and
have identical copies (all the copies of a job have the same size). Moreover,
we show via simulation that, for a large class of redundancy systems,
redundancy-aware policies can considerably improve the mean response time
compared to redundancy-oblivious policies. We also explore the effect of
redundancy on the stability region.
</summary>
    <author>
      <name>Elene Anton</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TU/e</arxiv:affiliation>
    </author>
    <author>
      <name>Rhonda Righter</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Ina Maria Verloop</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2206.10164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.10164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.11566v1</id>
    <updated>2022-06-23T09:26:31Z</updated>
    <published>2022-06-23T09:26:31Z</published>
    <title>INTERPLAY: An Intelligent Model for Predicting Performance Degradation
  due to Multi-cache Way-disabling</title>
    <summary>  Modern and future processors need to remain functionally correct in the
presence of permanent faults to sustain scaling benefits and limit field
returns. This paper presents a combined analytical and microarchitectural
simulation-based framework called INTERPLAY, that can rapidly predict, at
design-time, the performance degradation expected from a processor employing
way-disabling to handle permanent faults in caches while in-the-field. The
proposed model can predict a program's performance with an accuracy of up to
98.40% for a processor with a two-level cache hierarchy, when multiple caches
suffer from faults and need to disable one or more of their ways. INTERPLAY is
9.2x faster than an exhaustive simulation approach since it only needs the
training simulation runs for the single-cache way-disabling configurations to
predict the performance for any multi-cache way-disabling configuration.
</summary>
    <author>
      <name>Panagiota Nikolaou</name>
    </author>
    <author>
      <name>Yiannakis Sazeides</name>
    </author>
    <author>
      <name>Maria K. Michael</name>
    </author>
    <link href="http://arxiv.org/abs/2206.11566v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.11566v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.01730v2</id>
    <updated>2022-08-04T16:40:14Z</updated>
    <published>2022-07-04T21:45:33Z</published>
    <title>Service Modeling and Delay Analysis of Packet Delivery over a Wireless
  Link</title>
    <summary>  For delay analysis of packet delivery over a wireless link, several novel
ideas are introduced. One is to construct an equivalent $G/G/1$ non-lossy
queueing model to ease the analysis, enabled by exploiting empirical models of
packet error rate, packet service time and packet loss rate obtained from
measurement. The second is to exploit a classical queueing theory result to
approximate the mean delay. For estimating the delay distribution, the newly
developed stochastic network calculus (SNC) theory is made use of, forming the
third idea. To enable this SNC based analysis, a stochastic service curve
characterization of the link is introduced, relying on a packet service time
model obtained from the empirical models. The focused link is a 802.15.4
wireless link. Extensive experimental investigation under a wide range of
settings was conducted. The proposed ideas are validated with the experiment
results. The validation confirms that the proposed approaches, integrating both
empirical and analytical modes, are effective for service modeling and delay
analysis. This suggests an integrated approach, now found previously, for
quantitative understanding of the delay performance of packet delivery over a
wireless link.
</summary>
    <author>
      <name>Yan Zhang</name>
    </author>
    <author>
      <name>Yuming Jiang</name>
    </author>
    <author>
      <name>Songwei Fu</name>
    </author>
    <link href="http://arxiv.org/abs/2207.01730v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.01730v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.06515v1</id>
    <updated>2022-07-13T20:30:41Z</updated>
    <published>2022-07-13T20:30:41Z</published>
    <title>Automated Cause Analysis of Latency Outliers Using System-Level
  Dependency Graphs</title>
    <summary>  Detecting performance issues and identifying their root causes in the runtime
is a challenging task. Typically, developers use methods such as logging and
tracing to identify bottlenecks. These solutions are, however, not ideal as
they are time-consuming and require manual effort. In this paper, we propose a
method to automate the task of detecting latency outliers using system-level
traces and then comparing them to identify the root cause(s). Our method makes
use of dependency graphs to show internal interactions between threads and
system resources. With these graphs, one can pinpoint where performance issues
occur. However, a single trace can be composed of a large number of requests,
each generating one graph. To automate the task of identifying outliers within
the dataset, we use machine learning density-based models and statistical
calculations such as -score. Our evaluation shows an accuracy greater than 97 %
on outlier detection, making them appropriate for in-production servers and
industry-level use cases.
</summary>
    <author>
      <name>Sneh Patel</name>
    </author>
    <author>
      <name>Brendan Park</name>
    </author>
    <author>
      <name>Naser Ezzati-Jivan</name>
    </author>
    <author>
      <name>Quentin Fournier</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/QRS54544.2021.00054</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/QRS54544.2021.00054" rel="related"/>
    <link href="http://arxiv.org/abs/2207.06515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.06515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.12900v2</id>
    <updated>2022-08-04T07:25:48Z</updated>
    <published>2022-07-26T13:49:08Z</published>
    <title>Perun: Performance Version System</title>
    <summary>  In this paper, we present Perun: an open-source tool suite for
profiling-based performance analysis. At its core, Perun maintains links
between project versions and the corresponding stored performance profiles,
which are then leveraged for automated detection of performance changes in new
project versions. The Perun tool suite further includes multiple profilers (and
is designed such that further profilers can be easily added), a performance
fuzz-tester for workload generation, methods for deriving performance models,
and numerous visualization methods. We demonstrate how Perun can help
developers to analyze their program performance on two examples: detection and
localization of a performance degradation and generation of inputs forcing
performance issues to show up.
</summary>
    <author>
      <name>Tomáš Fiedor</name>
    </author>
    <author>
      <name>Jiří Pavela</name>
    </author>
    <author>
      <name>Adam Rogalewicz</name>
    </author>
    <author>
      <name>Tomáš Vojnar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a manuscript accepted to ICSME'22 Tool Demo track</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.12900v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.12900v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.00682v1</id>
    <updated>2022-08-01T08:30:25Z</updated>
    <published>2022-08-01T08:30:25Z</published>
    <title>Eficiency of REST and gRPC realizing communication tasks in
  microservice-based ecosystems</title>
    <summary>  The aim of this contribution is to analyse practical aspects of the use of
REST APIs and gRPC to realize communication tasks in applications in
microservice-based ecosystems. On the basis of performed experiments, classes
of communication tasks, for which given technology performs data transfer more
efficiently, have been established. This, in turn, allows formulation of
criteria for the selection of appropriate communication methods for
communication tasks to be performed in an application using microservices-based
architecture.
</summary>
    <author>
      <name>Marek Bolanowski</name>
    </author>
    <author>
      <name>Kamil Żak</name>
    </author>
    <author>
      <name>Andrzej Paszkiewicz</name>
    </author>
    <author>
      <name>Maria Ganzha</name>
    </author>
    <author>
      <name>Marcin Paprzycki</name>
    </author>
    <author>
      <name>Piotr Sowiński</name>
    </author>
    <author>
      <name>Ignacio Lacalle</name>
    </author>
    <author>
      <name>Carlos E. Palau</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/FAIA220242</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/FAIA220242" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for the conference -- The 21st International Conference on
  Intelligent Software Methodologies, Tools, and Techniques (SOMET 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.00682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.00682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4; C.4; D.4.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.10198v2</id>
    <updated>2023-03-29T08:47:40Z</updated>
    <published>2022-08-22T10:46:30Z</published>
    <title>Markovian queues with Poisson control</title>
    <summary>  We investigate Markovian queues that are examined by a controller at random
times determined by a Poisson process. Upon examination, the controller sets
the service speed to be equal to the minimum of the current number of customers
in the queue and a certain maximum service speed; this service speed prevails
until the next examination time. We study the resulting two-dimensional Markov
process of queue length and server speed, in particular two regimes with time
scale separation, specifically for infinitely frequent and infinitely long
examination times. In the intermediate regime the analysis proves to be
extremely challenging. To gain further insight into the model dynamics we then
analyse two variants of the model in which the controller is just an observer
and does not change the speed of the server.
</summary>
    <author>
      <name>R. Núñez-Queija</name>
    </author>
    <author>
      <name>B. J. Prabhu</name>
    </author>
    <author>
      <name>J. A. C. Resing</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.10198v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.10198v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.09001v1</id>
    <updated>2022-09-19T13:26:03Z</updated>
    <published>2022-09-19T13:26:03Z</published>
    <title>Exploring the Effects of Multicast Communication on DDS Performance</title>
    <summary>  The Data Distribution Service (DDS) is an Object Management Group (OMG)
standard for high-performance and real-time systems. DDS is a data-centric
middleware based on the publish-subscribe communication pattern and is used in
many mission-critical, or even safety-critical, systems such as air traffic
control and robot operating system (ROS2).
  This research aims at identifying how the usage of multicast affects the
performance of DDS communication for varying numbers of participants
(publishers and subscribers). The results show that DDS configured for
multicast communication can exhibit worse performance under a high load (a
greater number of participants) than DDS configured for unicast communication.
This counter-intuitive result reinforces the need for researchers and
practitioners to be clear about the details of how multicast communication
operates on the network.
</summary>
    <author>
      <name>Kaleem Peeroo</name>
    </author>
    <author>
      <name>Peter Popov</name>
    </author>
    <author>
      <name>Vladimir Stankovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 tables, 3 figures, and presented at EDCC 2022 (European
  Dependable Computing Conference). Editor: Ib\'eria Medeiros. 18th European
  Dependable Computing Conference (EDCC 2022), September 12-15, 2022, Zaragoza,
  Spain. Student Forum Proceedings - EDCC 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.09001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.09001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.13531v1</id>
    <updated>2022-09-14T12:53:21Z</updated>
    <published>2022-09-14T12:53:21Z</published>
    <title>Analysis of Reinforcement Learning for determining task replication in
  workflows</title>
    <summary>  Executing workflows on volunteer computing resources where individual tasks
may be forced to relinquish their resource for the resource's primary use leads
to unpredictability and often significantly increases execution time. Task
replication is one approach that can ameliorate this challenge. This comes at
the expense of a potentially significant increase in system load and energy
consumption. We propose the use of Reinforcement Learning (RL) such that a
system may `learn' the `best' number of replicas to run to increase the number
of workflows which complete promptly whilst minimising the additional workload
on the system when replicas are not beneficial. We show, through simulation,
that we can save 34% of the energy consumption using RL compared to a fixed
number of replicas with only a 4% decrease in workflows achieving a pre-defined
overhead bound.
</summary>
    <author>
      <name>Andrew Stephen McGough</name>
    </author>
    <author>
      <name>Matthew Forshaw</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at EPEW 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.13531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.13531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.13812v2</id>
    <updated>2022-11-30T17:44:22Z</updated>
    <published>2022-09-28T03:39:31Z</published>
    <title>Universal Policy Tracking: Scheduling for Wireless Networks with Delayed
  State Observation</title>
    <summary>  Numerous scheduling algorithms have been proposed to optimize various
performance metrics like throughput, delay and utility in wireless networks.
However, these algorithms often require instantaneous access to network state
information, which is not always available. While network stability can
sometimes be achieved with delayed state information, other performance metrics
such as latency may degrade. Thus, instead of simply stabilizing the system,
our goal is to design a framework that can mimic arbitrary scheduling
algorithms with performance guarantees. A naive approach is to make decisions
directly with delayed information, but we show that such methods may lead to
poor performance. Instead, we propose the Universal Tracking (UT) algorithm
that can mimic the actions of arbitrary scheduling algorithms under observation
delay. We rigorously show that the performance gap between UT and the
scheduling algorithm being tracked is bounded by constants. Our numerical
experiments show that UT significantly outperforms the naive approach in
various applications.
</summary>
    <author>
      <name>Bai Liu</name>
    </author>
    <author>
      <name>Eytan Modiano</name>
    </author>
    <link href="http://arxiv.org/abs/2209.13812v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.13812v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.02620v1</id>
    <updated>2022-10-06T00:46:06Z</updated>
    <published>2022-10-06T00:46:06Z</published>
    <title>Inference Latency Prediction at the Edge</title>
    <summary>  With the growing workload of inference tasks on mobile devices,
state-of-the-art neural architectures (NAs) are typically designed through
Neural Architecture Search (NAS) to identify NAs with good tradeoffs between
accuracy and efficiency (e.g., latency). Since measuring the latency of a huge
set of candidate architectures during NAS is not scalable, approaches are
needed for predicting end-to-end inference latency on mobile devices. Such
predictions are challenging due to hardware heterogeneity, optimizations
applied by ML frameworks, and the diversity of neural architectures. Motivated
by these challenges, in this paper, we first quantitatively assess
characteristics of neural architectures and mobile devices that have
significant effects on inference latency. Based on this assessment, we propose
a latency prediction framework which addresses these challenges by developing
operation-wise latency predictors, under a variety of settings and a number of
hardware devices, with multi-core CPUs and GPUs, achieving high accuracy in
end-to-end latency prediction, as shown by our comprehensive evaluations. To
illustrate that our approach does not require expensive data collection, we
also show that accurate predictions can be achieved on real-world NAs using
only small amounts of profiling data.
</summary>
    <author>
      <name>Zhuojin Li</name>
    </author>
    <author>
      <name>Marco Paolieri</name>
    </author>
    <author>
      <name>Leana Golubchik</name>
    </author>
    <link href="http://arxiv.org/abs/2210.02620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.02620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.09691v1</id>
    <updated>2022-10-18T09:03:32Z</updated>
    <published>2022-10-18T09:03:32Z</published>
    <title>OpenStack and Google Cloud performance comparison in Infrastructure as a
  Service model</title>
    <summary>  Cloud computing is becoming common, and the choice of proper infrastructure
is essential. One of main issues is choosing between private and public clound,
between commercial and non-commercial solutions. This paper aims to compare the
parameters of OpenStack and Google Cloud systems. Both systems deliver a
computing cloud service, enabling the user to use the infrastructure as a
service (IaaS) model. We developed the pipeline using the Python programming
language and its libraries, which enable communication with the aforementioned
clouds. We measured various parameters of instances and task execution:
instance launch and deletion times, and their dependence on the number of
launched instances. Moreover, we used benchmark algorithms to check the
instance performance. We analysed the results and the factors that contributed
to them and provided conclusions, recommendations, and suggestions for further
research based on the gathered data.
</summary>
    <author>
      <name>Michał Łątkowski</name>
    </author>
    <author>
      <name>Robert Nowak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.09691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.09691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.10184v3</id>
    <updated>2023-08-29T14:36:29Z</updated>
    <published>2022-10-18T22:12:29Z</published>
    <title>Application Performance Modeling via Tensor Completion</title>
    <summary>  Performance tuning, software/hardware co-design, and job scheduling are among
the many tasks that rely on models to predict application performance. We
propose and evaluate low-rank tensor decomposition for modeling application
performance. We discretize the input and configuration domains of an
application using regular grids. Application execution times mapped within
grid-cells are averaged and represented by tensor elements. We show that
low-rank canonical-polyadic (CP) tensor decomposition is effective in
approximating these tensors. We further show that this decomposition enables
accurate extrapolation of unobserved regions of an application's parameter
space. We then employ tensor completion to optimize a CP decomposition given a
sparse set of observed execution times. We consider alternative
piecewise/grid-based models and supervised learning models for six applications
and demonstrate that CP decomposition optimized using tensor completion offers
higher prediction accuracy and memory-efficiency for high-dimensional
performance modeling.
</summary>
    <author>
      <name>Edward Hutter</name>
    </author>
    <author>
      <name>Edgar Solomonik</name>
    </author>
    <link href="http://arxiv.org/abs/2210.10184v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.10184v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.02762v1</id>
    <updated>2022-11-04T21:56:41Z</updated>
    <published>2022-11-04T21:56:41Z</published>
    <title>Optimal Scheduling in the Multiserver-job Model under Heavy Traffic</title>
    <summary>  Multiserver-job systems, where jobs require concurrent service at many
servers, occur widely in practice. Essentially all of the theoretical work on
multiserver-job systems focuses on maximizing utilization, with almost nothing
known about mean response time. In simpler settings, such as various known-size
single-server-job settings, minimizing mean response time is merely a matter of
prioritizing small jobs. However, for the multiserver-job system, prioritizing
small jobs is not enough, because we must also ensure servers are not
unnecessarily left idle. Thus, minimizing mean response time requires
prioritizing small jobs while simultaneously maximizing throughput. Our
question is how to achieve these joint objectives.
  We devise the ServerFilling-SRPT scheduling policy, which is the first policy
to minimize mean response time in the multiserver-job model in the heavy
traffic limit. In addition to proving this heavy-traffic result, we present
empirical evidence that ServerFilling-SRPT outperforms all existing scheduling
policies for all loads, with improvements by orders of magnitude at higher
loads.
  Because ServerFilling-SRPT requires knowing job sizes, we also define the
ServerFilling-Gittins policy, which is optimal when sizes are unknown or
partially known.
</summary>
    <author>
      <name>Isaac Grosof</name>
    </author>
    <author>
      <name>Ziv Scully</name>
    </author>
    <author>
      <name>Mor Harchol-Balter</name>
    </author>
    <author>
      <name>Alan Scheller-Wolf</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3570612</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3570612" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, to appear in ACM SIGMETRICS 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.02762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.02762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.05657v2</id>
    <updated>2024-01-18T14:38:40Z</updated>
    <published>2022-11-10T15:36:31Z</published>
    <title>Stochastic Network Calculus with Localized Application of Martingales</title>
    <summary>  Stochastic Network Calculus is a probabilistic method to compute performance
bounds in networks, such as end-to-end delays. It relies on the analysis of
stochastic processes using formalism of (Deterministic) Network Calculus.
However, unlike the deterministic theory, the computed bounds are usually very
loose compared to the simulation. This is mainly due to the intensive use of
the Boole's inequality. On the other hand, analyses based on martingales can
achieve tight bounds, but until now, they have not been applied to sequences of
servers. In this paper, we improve the accuracy of Stochastic Network Calculus
by combining this martingale analysis with a recent Stochastic Network Calculus
results based on the Pay-Multiplexing-Only-Once property, well-known from the
Deterministic Network calculus. We exhibit a non-trivial class of networks that
can benefit from this analysis and compare our bounds with simulation.
</summary>
    <author>
      <name>Anne Bouillard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.05657v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.05657v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.15907v1</id>
    <updated>2022-11-29T03:51:40Z</updated>
    <published>2022-11-29T03:51:40Z</published>
    <title>Performance Evaluation, Optimization and Dynamic Decision in Blockchain
  Systems: A Recent Overview</title>
    <summary>  With rapid development of blockchain technology as well as integration of
various application areas, performance evaluation, performance optimization,
and dynamic decision in blockchain systems are playing an increasingly
important role in developing new blockchain technology. This paper provides a
recent systematic overview of this class of research, and especially,
developing mathematical modeling and basic theory of blockchain systems.
Important examples include (a) performance evaluation: Markov processes,
queuing theory, Markov reward processes, random walks, fluid and diffusion
approximations, and martingale theory; (b) performance optimization: Linear
programming, nonlinear programming, integer programming, and multi-objective
programming; (c) optimal control and dynamic decision: Markov decision
processes, and stochastic optimal control; and (d) artificial intelligence:
Machine learning, deep reinforcement learning, and federated learning. So far,
a little research has focused on these research lines. We believe that the
basic theory with mathematical methods, algorithms and simulations of
blockchain systems discussed in this paper will strongly support future
development and continuous innovation of blockchain technology.
</summary>
    <author>
      <name>Quan-Lin Li</name>
    </author>
    <author>
      <name>Yan-Xia Chang</name>
    </author>
    <author>
      <name>Qing Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.15907v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.15907v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90B22, 60J28" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4; H.3.5; E.2; E.3; D.4.6; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.12443v1</id>
    <updated>2022-12-22T11:18:07Z</updated>
    <published>2022-12-22T11:18:07Z</published>
    <title>Comparison of Three Job Mapping Algorithms for Supercomputer Resource
  Managers</title>
    <summary>  Performance of supercomputer depends on the quality of resource manager, one
of its functions is assignment of jobs to the nodes of clusters or MPP
computers. Parts of parallel programs interact with each other with different
intensity, and mapping of program to supercomputer nodes influence efficiency
of the run. At each program run graph representing application program is to be
mapped onto graph of nodes representing a subset of computer system. The both
graphs are not known beforehand, hence the mapping must be done in reasonable
time while scheduling resources. Three mapping algorithms were explored:
parallel versions of simulated annealing, genetic and composite algorithms. A
set of experimental runs with different algorithms parameters was performed,
comparison of mapping quality and runtime was made, and suggestions on
applicability of algorithms for resource managers were provided.
</summary>
    <author>
      <name>A. V. Baranov</name>
    </author>
    <author>
      <name>E. A. Kiselev</name>
    </author>
    <author>
      <name>B. M. Shabanov</name>
    </author>
    <author>
      <name>A. A. Sorokin</name>
    </author>
    <author>
      <name>P. N. Telegin</name>
    </author>
    <link href="http://arxiv.org/abs/2212.12443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.12443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.01095v3</id>
    <updated>2023-11-27T08:23:48Z</updated>
    <published>2023-01-03T13:41:14Z</published>
    <title>Database management system performance comparisons: A systematic
  literature review</title>
    <summary>  Efficiency has been a pivotal aspect of the software industry since its
inception, as a system that serves the end-user fast, and the service provider
cost-efficiently benefits all parties. A database management system (DBMS) is
an integral part of effectively all software systems, and therefore it is
logical that different studies have compared the performance of different DBMSs
in hopes of finding the most efficient one. This study systematically
synthesizes the results and approaches of studies that compare DBMS performance
and provides recommendations for industry and research. The results show that
performance is usually tested in a way that does not reflect real-world use
cases, and that tests are typically reported in insufficient detail for
replication or for drawing conclusions from the stated results.
</summary>
    <author>
      <name>Toni Taipalus</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jss.2023.111872</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jss.2023.111872" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Systems and Software, Volume 208, Article 111872,
  February 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2301.01095v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.01095v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2; B.8.0; H.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.09262v2</id>
    <updated>2023-04-17T20:06:38Z</updated>
    <published>2023-01-23T04:24:26Z</published>
    <title>AttMEMO : Accelerating Transformers with Memoization on Big Memory
  Systems</title>
    <summary>  Transformer models gain popularity because of their superior inference
accuracy and inference throughput. However, the transformer is
computation-intensive, causing a long inference time. The existing works on
transformer inference acceleration have limitations caused by either the
modification of transformer architectures or the need of specialized hardware.
In this paper, we identify the opportunities of using memoization to accelerate
the self-attention mechanism in transformers without the above limitations.
Built upon a unique observation that there is rich similarity in attention
computation across inference sequences, we build a memoization database that
leverages the emerging big memory system. We introduce a novel embedding
technique to find semantically similar inputs to identify computation
similarity. We also introduce a series of techniques such as memory mapping and
selective memoization to avoid memory copy and unnecessary overhead. We enable
22% inference-latency reduction on average (up to 68%) with negligible loss in
inference accuracy.
</summary>
    <author>
      <name>Yuan Feng</name>
    </author>
    <author>
      <name>Hyeran Jeon</name>
    </author>
    <author>
      <name>Filip Blagojevic</name>
    </author>
    <author>
      <name>Cyril Guyot</name>
    </author>
    <author>
      <name>Qing Li</name>
    </author>
    <author>
      <name>Dong Li</name>
    </author>
    <link href="http://arxiv.org/abs/2301.09262v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.09262v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.06836v3</id>
    <updated>2024-04-18T04:05:15Z</updated>
    <published>2023-02-14T05:20:51Z</published>
    <title>COMET: Neural Cost Model Explanation Framework</title>
    <summary>  Cost models predict the cost of executing given assembly code basic blocks on
a specific microarchitecture. Recently, neural cost models have been shown to
be fairly accurate and easy to construct. They can replace heavily engineered
analytical cost models used in mainstream compiler workflows. However, their
black-box nature discourages their adoption. In this work, we develop the first
framework, COMET, for generating faithful, generalizable, and intuitive
explanations for neural cost models. We generate and compare COMET's
explanations for the popular neural cost model, Ithemal against those for an
accurate CPU simulation-based cost model, uiCA. Our empirical findings show an
inverse correlation between the prediction errors of Ithemal and uiCA and the
granularity of basic block features in COMET's explanations for them, thus
indicating potential reasons for the higher error of Ithemal with respect to
uiCA.
</summary>
    <author>
      <name>Isha Chaudhary</name>
    </author>
    <author>
      <name>Alex Renda</name>
    </author>
    <author>
      <name>Charith Mendis</name>
    </author>
    <author>
      <name>Gagandeep Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 5th MLSys Conference, Santa Clara, CA, USA, 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.06836v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.06836v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.08251v2</id>
    <updated>2024-04-11T22:37:42Z</updated>
    <published>2023-02-16T12:14:42Z</published>
    <title>Updates on the Low-Level Abstraction of Memory Access</title>
    <summary>  Choosing the best memory layout for each hardware architecture is
increasingly important as more and more programs become memory bound. For
portable codes that run across heterogeneous hardware architectures, the choice
of the memory layout for data structures is ideally decoupled from the rest of
a program. The low-level abstraction of memory access (LLAMA) is a C++ library
that provides a zero-runtime-overhead abstraction layer, underneath which
memory mappings can be freely exchanged to customize data layouts, memory
access and access instrumentation, focusing on multidimensional arrays of
nested, structured data.
  After its scientific debut, several improvements and extensions have been
added to LLAMA. This includes compile-time array extents for
zero-memory-overhead views, support for computations during memory access, new
mappings for bit-packing, switching types, byte-splitting, memory access
instrumentation, and explicit SIMD support. This contribution provides an
overview of recent developments in the LLAMA library.
</summary>
    <author>
      <name>Bernhard Manfred Gruber</name>
    </author>
    <link href="http://arxiv.org/abs/2302.08251v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.08251v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.09468v2</id>
    <updated>2023-05-01T18:43:34Z</updated>
    <published>2023-02-19T03:49:02Z</published>
    <title>Rethinking Memory Profiling and Migration for Multi-Tiered Large Memory
  Systems</title>
    <summary>  Multi-tiered large memory systems call for rethinking of memory profiling and
migration because of the unique problems unseen in the traditional memory
systems with smaller capacity and fewer tiers. We develop MTM, an
application-transparent page management system based on three principles: (1)
connecting the control of profiling overhead with the profiling mechanism for
high-quality profiling; (2) building a universal page migration policy on the
complex multi-tiered memory for high performance; and (3) introducing huge page
awareness. We evaluate MTM using common big-data applications with realistic
working sets (hundreds of GB to 1 TB). MTM outperforms seven state-of-the-art
solutions by up to 42% (17% on average)
</summary>
    <author>
      <name>Jie Ren</name>
    </author>
    <author>
      <name>Dong Xu</name>
    </author>
    <author>
      <name>Ivy Peng</name>
    </author>
    <author>
      <name>Junhee Ryu</name>
    </author>
    <author>
      <name>Kwangsik Shin</name>
    </author>
    <author>
      <name>Daewoo Kim</name>
    </author>
    <author>
      <name>Dong Li</name>
    </author>
    <link href="http://arxiv.org/abs/2302.09468v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.09468v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.12954v1</id>
    <updated>2023-02-25T01:43:23Z</updated>
    <published>2023-02-25T01:43:23Z</published>
    <title>WPC: Whole-picture Workload Characterization</title>
    <summary>  This article raises an important and challenging workload characterization
issue: can we uncover each critical component across the stacks contributing
what percentages to any specific bottleneck? The typical critical components
include languages, programming frameworks, runtime environments, instruction
set architectures (ISA), operating systems (OS), and microarchitecture.
Tackling this issue could help propose a systematic methodology to guide the
software and hardware co-design and critical component optimizations. We
propose a whole-picture workload characterization (WPC) methodology to answer
the above issue. In essence, WPC is an iterative ORFE loop consisting of four
steps: Observation, Reference, Fusion, and Exploration. WPC observes different
level data (observation), fuses and normalizes the performance data (fusion)
with respect to the well-designed standard reference workloads suite
(reference), and explores the software and hardware co-design space
(exploration) to investigate the impacts of critical components across the
stacks. We build and open-source the WPC tool. Our evaluations confirm WPC can
quantitatively reveal the contributions of the language, framework, runtime
environment, ISA, OS, and microarchitecture to the primary pipeline efficiency.
</summary>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Kaiyong Yang</name>
    </author>
    <author>
      <name>Chenxi Wang</name>
    </author>
    <author>
      <name>Wanling Gao</name>
    </author>
    <author>
      <name>Chunjie Luo</name>
    </author>
    <author>
      <name>Fan Zhang</name>
    </author>
    <author>
      <name>Zhongxin Ge</name>
    </author>
    <author>
      <name>Li Zhang</name>
    </author>
    <author>
      <name>Guoxin Kang</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <link href="http://arxiv.org/abs/2302.12954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.12954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.05919v1</id>
    <updated>2023-01-17T03:12:35Z</updated>
    <published>2023-01-17T03:12:35Z</published>
    <title>eBPF-based Working Set Size Estimation in Memory Management</title>
    <summary>  Working set size estimation (WSS) is of great significance to improve the
efficiency of program executing and memory arrangement in modern operating
systems. Previous work proposed several methods to estimate WSS, including
self-balloning, Zballoning and so on. However, these methods which are based on
virtual machine usually cause a large overhead. Thus, using those methods to
estimate WSS is impractical. In this paper, we propose a novel framework to
efficiently estimate WSS with eBPF (extended Berkeley Packet Filter), a
cutting-edge technology which monitors and filters data by being attached to
the kernel. With an eBPF program pinned into the kernel, we get the times of
page fault and other information of memory allocation. Moreover, we collect WSS
via vanilla tool to train a predictive model to complete estimation work with
LightGBM, a useful tool which performs well on generating decision trees over
continuous value. The experimental results illustrate that our framework can
estimate WSS precisely with 98.5\% reduction in overhead compared to
traditional methods.
</summary>
    <author>
      <name>Zhilu Lian</name>
    </author>
    <author>
      <name>Yangzi Li</name>
    </author>
    <author>
      <name>Zhixiang Chen</name>
    </author>
    <author>
      <name>Shiwen Shan</name>
    </author>
    <author>
      <name>Baoxin Han</name>
    </author>
    <author>
      <name>Yuxin Su</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICSS55994.2022.00036</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICSS55994.2022.00036" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.05919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.05919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.06073v2</id>
    <updated>2023-10-24T13:55:22Z</updated>
    <published>2023-03-09T17:19:19Z</published>
    <title>I Tag, You Tag, Everybody Tags!</title>
    <summary>  Location tags are designed to track personal belongings. Nevertheless, there
has been anecdotal evidence that location tags are also misused to stalk
people. Tracking is achieved locally, e.g., via Bluetooth with a paired phone,
and remotely, by piggybacking on location-reporting devices which come into
proximity of a tag. This paper studies the performance of the two most popular
location tags (Apple's AirTag and Samsung's SmartTag) through controlled
experiments - with a known large distribution of location-reporting devices -
as well as in-the-wild experiments - with no control on the number and kind of
reporting devices encountered, thus emulating real-life use-cases. We find that
both tags achieve similar performance, e.g., they are located 55% of the times
in about 10 minutes within a 100 m radius. It follows that real time stalking
to a precise location via location tags is impractical, even when both tags are
concurrently deployed which achieves comparable accuracy in half the time.
Nevertheless, half of a victim's exact movements can be backtracked accurately
(10m error) with just a one-hour delay, which is still perilous information in
the possession of a stalker.
</summary>
    <author>
      <name>Hazem Ibrahim</name>
    </author>
    <author>
      <name>Rohail Asim</name>
    </author>
    <author>
      <name>Matteo Varvello</name>
    </author>
    <author>
      <name>Yasir Zaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.06073v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.06073v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.10844v2</id>
    <updated>2023-04-20T04:10:16Z</updated>
    <published>2023-03-20T03:31:35Z</published>
    <title>Analyzing the Performance of the Inter-Blockchain Communication Protocol</title>
    <summary>  With the increasing demand for communication between blockchains, improving
the performance of cross-chain communication protocols becomes an emerging
challenge. We take a first step towards analyzing the limitations of
cross-chain communication protocols by comprehensively evaluating Cosmos
Network's Inter-Blockchain Communication Protocol. To achieve our goal we
introduce a novel framework to guide empirical evaluations of cross-chain
communication protocols. We implement an instance of our framework as a tool to
evaluate the IBC protocol. Our findings highlight several challenges, such as
high transaction confirmation latency, bottlenecks in the blockchain's RPC
implementation and concurrency issues that hinder the scalability of the
cross-chain message relayer. We also demonstrate how to reduce the time
required to complete cross-chain transfers by up to 70% when submitting large
amounts of transfers. Finally, we discuss challenges faced during deployment
with the objective of contributing to the development and advancement of
cross-chain communication.
</summary>
    <author>
      <name>Joao Otavio Chervinski</name>
    </author>
    <author>
      <name>Diego Kreutz</name>
    </author>
    <author>
      <name>Xiwei Xu</name>
    </author>
    <author>
      <name>Jiangshan Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the 53rd IEEE/IFIP DSN 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.10844v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.10844v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.15763v1</id>
    <updated>2023-03-28T06:44:32Z</updated>
    <published>2023-03-28T06:44:32Z</published>
    <title>Characterizing the Performance of Emerging Deep Learning, Graph, and
  High Performance Computing Workloads Under Interference</title>
    <summary>  Throughput-oriented computing via co-running multiple applications in the
same machine has been widely adopted to achieve high hardware utilization and
energy saving on modern supercomputers and data centers. However, efficiently
co-running applications raises new design challenges, mainly because
applications with diverse requirements can stress out shared hardware resources
(IO, Network and Cache) at various levels. The disparities in resource usage
can result in interference, which in turn can lead to unpredictable co-running
behaviors. To better understand application interference, prior work provided
detailed execution characterization. However, these characterization approaches
either emphasize on traditional benchmarks or fall into a single application
domain. To address this issue, we study 25 up-to-date applications and
benchmarks from various application domains and form 625 consolidation pairs to
thoroughly analyze the execution interference caused by application co-running.
Moreover, we leverage mini-benchmarks and real applications to pinpoint the
provenance of co-running interference in both hardware and software aspects.
</summary>
    <author>
      <name>Hao Xu</name>
    </author>
    <author>
      <name>Shuang Song</name>
    </author>
    <author>
      <name>Ze Mao</name>
    </author>
    <link href="http://arxiv.org/abs/2303.15763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.15763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.13110v3</id>
    <updated>2024-04-08T23:17:13Z</updated>
    <published>2023-04-25T19:31:31Z</published>
    <title>Analysis and Mitigation of Shared Resource Contention on Heterogeneous
  Multicore: An Industrial Case Study</title>
    <summary>  In this paper, we present a solution to the industrial challenge put forth by
ARM in 2022. We systematically analyze the effect of shared resource contention
to an augmented reality head-up display (AR-HUD) case-study application of the
industrial challenge on a heterogeneous multicore platform, NVIDIA Jetson Nano.
We configure the AR-HUD application such that it can process incoming image
frames in real-time at 20Hz on the platform. We use Microarchitectural
Denial-of-Service (DoS) attacks as aggressor workloads of the challenge and
show that they can dramatically impact the latency and accuracy of the AR-HUD
application. This results in significant deviations of the estimated
trajectories from known ground truths, despite our best effort to mitigate
their influence by using cache partitioning and real-time scheduling of the
AR-HUD application. To address the challenge, we propose RT-Gang++, a
partitioned real-time gang scheduling framework with last-level cache (LLC) and
integrated GPU bandwidth throttling capabilities. By applying RT-Gang++, we are
able to achieve desired level of performance of the AR-HUD application even in
the presence of fully loaded aggressor tasks.
</summary>
    <author>
      <name>Michael Bechtel</name>
    </author>
    <author>
      <name>Heechul Yun</name>
    </author>
    <link href="http://arxiv.org/abs/2304.13110v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.13110v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.09266v1</id>
    <updated>2023-05-16T08:18:54Z</updated>
    <published>2023-05-16T08:18:54Z</published>
    <title>Case Study for Running Memory-Bound Kernels on RISC-V CPUs</title>
    <summary>  The emergence of a new, open, and free instruction set architecture, RISC-V,
has heralded a new era in microprocessor architectures. Starting with
low-power, low-performance prototypes, the RISC-V community has a good chance
of moving towards fully functional high-end microprocessors suitable for
high-performance computing. Achieving progress in this direction requires
comprehensive development of the software environment, namely operating
systems, compilers, mathematical libraries, and approaches to performance
analysis and optimization. In this paper, we analyze the performance of two
available RISC-V devices when executing three memory-bound applications: a
widely used STREAM benchmark, an in-place dense matrix transposition algorithm,
and a Gaussian Blur algorithm. We show that, compared to x86 and ARM CPUs,
RISC-V devices are still expected to be inferior in terms of computation time
but are very good in resource utilization. We also demonstrate that
well-developed memory optimization techniques for x86 CPUs improve the
performance on RISC-V CPUs. Overall, the paper shows the potential of RISC-V as
an alternative architecture for high-performance computing.
</summary>
    <author>
      <name>Valentin Volokitin</name>
    </author>
    <author>
      <name>Evgeny Kozinov</name>
    </author>
    <author>
      <name>Valentina Kustikova</name>
    </author>
    <author>
      <name>Alexey Liniov</name>
    </author>
    <author>
      <name>Iosif Meyerov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-031-41673-6_5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-031-41673-6_5" rel="related"/>
    <link href="http://arxiv.org/abs/2305.09266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.09266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.15116v1</id>
    <updated>2023-05-24T13:10:25Z</updated>
    <published>2023-05-24T13:10:25Z</published>
    <title>Model-Based Performance Analysis of the HyTeG Finite Element Framework</title>
    <summary>  In this work, we present how code generation techniques significantly improve
the performance of the computational kernels in the HyTeG software framework.
This HPC framework combines the performance and memory advantages of
matrix-free multigrid solvers with the flexibility of unstructured meshes. The
pystencils code generation toolbox is used to replace the original abstract C++
kernels with highly optimized loop nests. The performance of one of those
kernels (the matrix-vector multiplication) is thoroughly analyzed using the
Execution-Cache-Memory (ECM) performance model. We validate these predictions
by measurements on the SuperMUC-NG supercomputer. The experiments show that the
performance mostly matches the predictions. In cases where the prediction does
not match, we discuss the discrepancies. Additionally, we conduct a node-level
scaling study which shows the expected behavior for a memory-bound compute
kernel.
</summary>
    <author>
      <name>Dominik Thönnes</name>
    </author>
    <author>
      <name>Ulrich Rüde</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3592979.3593422</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3592979.3593422" rel="related"/>
    <link href="http://arxiv.org/abs/2305.15116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.15116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.05701v1</id>
    <updated>2023-06-09T06:41:57Z</updated>
    <published>2023-06-09T06:41:57Z</published>
    <title>CAWL: A Cache-aware Write Performance Model of Linux Systems</title>
    <summary>  The performance of data intensive applications is often dominated by their
input/output (I/O) operations but the I/O stack of systems is complex and
severely depends on system specific settings and hardware components. This
situation makes generic performance optimisation challenging and costly for
developers as they would have to run their application on a large variety of
systems to evaluate their improvements. Here, simulation frameworks can help
reducing the experimental overhead but they typically handle the topic of I/O
rather coarse-grained, which leads to significant inaccuracies in performance
predictions. Here, we propose a more accurate model of the write performance of
Linux-based systems that takes different I/O methods and levels (via system
calls, library calls, direct or indirect, etc.), the page cache, background
writing, and the I/O throttling capabilities of the Linux kernel into account.
With our model, we reduce, for example, the relative prediction error compared
to a standard I/O model included in SimGrid for a random I/O scenario from 67 %
down to 10 % relative error against real measurements of the simulated
workload. In other scenarios the differences are even more pronounced.
</summary>
    <author>
      <name>Masoud Gholami</name>
    </author>
    <author>
      <name>Florian Schintke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 9 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.05701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.05701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.07888v2</id>
    <updated>2023-10-03T12:27:53Z</updated>
    <published>2023-06-13T16:28:37Z</published>
    <title>CAMEO: A Causal Transfer Learning Approach for Performance Optimization
  of Configurable Computer Systems</title>
    <summary>  Modern computer systems are highly configurable, with hundreds of
configuration options that interact, resulting in an enormous configuration
space. As a result, optimizing performance goals (e.g., latency) in such
systems is challenging due to frequent uncertainties in their environments
(e.g., workload fluctuations). Recently, transfer learning has been applied to
address this problem by reusing knowledge from configuration measurements from
the source environments, where it is cheaper to intervene than the target
environment, where any intervention is costly or impossible. Recent empirical
research showed that statistical models can perform poorly when the deployment
environment changes because the behavior of certain variables in the models can
change dramatically from source to target. To address this issue, we propose
CAMEO, a method that identifies invariant causal predictors under environmental
changes, allowing the optimization process to operate in a reduced search
space, leading to faster optimization of system performance. We demonstrate
significant performance improvements over state-of-the-art optimization methods
in MLperf deep learning systems, a video analytics pipeline, and a database
system.
</summary>
    <author>
      <name>Md Shahriar Iqbal</name>
    </author>
    <author>
      <name>Ziyuan Zhong</name>
    </author>
    <author>
      <name>Iftakhar Ahmad</name>
    </author>
    <author>
      <name>Baishakhi Ray</name>
    </author>
    <author>
      <name>Pooyan Jamshidi</name>
    </author>
    <link href="http://arxiv.org/abs/2306.07888v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.07888v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.08367v2</id>
    <updated>2024-01-24T11:05:45Z</updated>
    <published>2023-06-14T08:56:48Z</published>
    <title>Accelerating Machine Learning Queries with Linear Algebra Query
  Processing</title>
    <summary>  The rapid growth of large-scale machine learning (ML) models has led numerous
commercial companies to utilize ML models for generating predictive results to
help business decision-making. As two primary components in traditional
predictive pipelines, data processing, and model predictions often operate in
separate execution environments, leading to redundant engineering and
computations. Additionally, the diverging mathematical foundations of data
processing and machine learning hinder cross-optimizations by combining these
two components, thereby overlooking potential opportunities to expedite
predictive pipelines.
  In this paper, we propose an operator fusing method based on GPU-accelerated
linear algebraic evaluation of relational queries. Our method leverages linear
algebra computation properties to merge operators in machine learning
predictions and data processing, significantly accelerating predictive
pipelines by up to 317x. We perform a complexity analysis to deliver
quantitative insights into the advantages of operator fusion, considering
various data and model dimensions. Furthermore, we extensively evaluate matrix
multiplication query processing utilizing the widely-used Star Schema
Benchmark. Through comprehensive evaluations, we demonstrate the effectiveness
and potential of our approach in improving the efficiency of data processing
and machine learning workloads on modern hardware.
</summary>
    <author>
      <name>Wenbo Sun</name>
    </author>
    <author>
      <name>Asterios Katsifodimos</name>
    </author>
    <author>
      <name>Rihan Hai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3603719.3603726</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3603719.3603726" rel="related"/>
    <link href="http://arxiv.org/abs/2306.08367v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.08367v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.07138v1</id>
    <updated>2023-07-14T03:30:32Z</updated>
    <published>2023-07-14T03:30:32Z</published>
    <title>Reconfigurable Intelligent Surface Assisted Free Space Optical
  Information and Power Transfer</title>
    <summary>  Free space optical (FSO) transmission has emerged as a key candidate
technology for 6G to expand new spectrum and improve network capacity due to
its advantages of large bandwidth, low electromagnetic interference, and high
energy efficiency. Resonant beam operating in the infrared band utilizes
spatially separated laser cavities to enable safe and mobile high-power energy
and high-rate information transmission but is limited by line-of-sight (LOS)
channel. In this paper, we propose a reconfigurable intelligent surface (RIS)
assisted resonant beam simultaneous wireless information and power transfer
(SWIPT) system and establish an optical field propagation model to analyze the
channel state information (CSI), in which LOS obstruction can be detected
sensitively and non-line-of-sight (NLOS) transmission can be realized by
changing the phased of resonant beam in RIS. Numerical results demonstrate
that, apart from the transmission distance, the NLOS performance depends on
both the horizontal and vertical positions of RIS. The maximum NLOS energy
efficiency can achieve 55% within a transfer distance of 10m, a translation
distance of $\pm$4mm, and rotation angle of $\pm$50{\deg}.
</summary>
    <author>
      <name>Wen Fang</name>
    </author>
    <author>
      <name>Wen Chen</name>
    </author>
    <author>
      <name>Qingqing Wu</name>
    </author>
    <author>
      <name>Kunlun Wang</name>
    </author>
    <author>
      <name>Shunqing Zhang</name>
    </author>
    <author>
      <name>Qingwen Liu</name>
    </author>
    <author>
      <name>Jun Li</name>
    </author>
    <link href="http://arxiv.org/abs/2307.07138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.07138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.08614v1</id>
    <updated>2023-07-17T16:30:19Z</updated>
    <published>2023-07-17T16:30:19Z</published>
    <title>Splitter Orderings for Probabilistic Bisimulation</title>
    <summary>  Model checking has been proposed as a formal verification approach for
analyzing computer-based and cyber-physical systems. The state space explosion
problem is the main obstacle for applying this approach for sophisticated
systems. Bisimulation minimization is a prominent method for reducing the
number of states in a labeled transition system and is used to alleviate the
challenges of the state space explosion problem. For systems with stochastic
behaviors, probabilistic bisimulation is used to reduce a given model to its
minimized equivalent one. In recent years, several techniques have been
proposed to reduce the time complexity of the iterative methods for computing
probabilistic bisimulation of stochastic systems with nondeterministic
behaviors. In this paper, we propose several techniques to accelerate iterative
processes to partition the state space of a given probabilistic model to its
bisimulation classes. The first technique applies two ordering heuristics for
choosing splitter blocks. The second technique uses hash tables to reduce the
running time and the average time complexity of the standard iterative method.
The proposed approaches are implemented and run on several conventional case
studies and reduce the running time by one order of magnitude on average.
</summary>
    <author>
      <name>Mohammadsadegh Mohagheghi</name>
    </author>
    <author>
      <name>Khayyam Salehi</name>
    </author>
    <link href="http://arxiv.org/abs/2307.08614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.08614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.14860v1</id>
    <updated>2023-07-27T13:45:41Z</updated>
    <published>2023-07-27T13:45:41Z</published>
    <title>Quantum Computer Simulations at Warp Speed: Assessing the Impact of GPU
  Acceleration</title>
    <summary>  Quantum computer simulators are crucial for the development of quantum
computing. In this work, we investigate the suitability and performance impact
of GPU and multi-GPU systems on a widely used simulation tool - the state
vector simulator Qiskit Aer. In particular, we evaluate the performance of both
Qiskit's default Nvidia Thrust backend and the recent Nvidia cuQuantum backend
on Nvidia A100 GPUs. We provide a benchmark suite of representative quantum
applications for characterization. For simulations with a large number of
qubits, the two GPU backends can provide up to 14x speedup over the CPU
backend, with Nvidia cuQuantum providing further 1.5-3x speedup over the
default Thrust backend. Our evaluation on a single GPU identifies the most
important functions in Nvidia Thrust and cuQuantum for different quantum
applications and their compute and memory bottlenecks. We also evaluate the
gate fusion and cache-blocking optimizations on different quantum applications.
Finally, we evaluate large-number qubit quantum applications on multi-GPU and
identify data movement between host and GPU as the limiting factor for the
performance.
</summary>
    <author>
      <name>Jennifer Faj</name>
    </author>
    <author>
      <name>Ivy Peng</name>
    </author>
    <author>
      <name>Jacob Wahlgren</name>
    </author>
    <author>
      <name>Stefano Markidis</name>
    </author>
    <link href="http://arxiv.org/abs/2307.14860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.14860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.14921v1</id>
    <updated>2023-07-27T15:03:13Z</updated>
    <published>2023-07-27T15:03:13Z</published>
    <title>Benchmarking Performance of Deep Learning Model for Material
  Segmentation on Two HPC Systems</title>
    <summary>  Performance Benchmarking of HPC systems is an ongoing effort that seeks to
provide information that will allow for increased performance and improve the
job schedulers that manage these systems. We develop a benchmarking tool that
utilizes machine learning models and gathers performance data on
GPU-accelerated nodes while they perform material segmentation analysis. The
benchmark uses a ML model that has been converted from Caffe to PyTorch using
the MMdnn toolkit and the MINC-2500 dataset. Performance data is gathered on
two ERDC DSRC systems, Onyx and Vulcanite. The data reveals that while
Vulcanite has faster model times in a large number of benchmarks, and it is
also more subject to some environmental factors that can cause performances
slower than Onyx. In contrast the model times from Onyx are consistent across
benchmarks.
</summary>
    <author>
      <name>Warren R. Williams</name>
    </author>
    <author>
      <name>S. Ross Glandon</name>
    </author>
    <author>
      <name>Luke L. Morris</name>
    </author>
    <author>
      <name>Jing-Ru C. Cheng</name>
    </author>
    <link href="http://arxiv.org/abs/2307.14921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.14921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.16604v1</id>
    <updated>2023-07-31T12:13:03Z</updated>
    <published>2023-07-31T12:13:03Z</published>
    <title>Shirac: A linear algebra for event-based system modeling</title>
    <summary>  Digital signal theory is an extension of the analysis of continuous signals.
This extension is provided by discretization and sampling. The sampling of
signals can be mathematically described by a series of Dirac impulses and is
well known. Properties of the Dirac impulse, such as sampling, are derived in
distribution theory. The theory generalizes differential calculus to functions
that are not differentiable in the classical sense such as the Heaviside step
function. Therefore, distribution theory allows one to adopt analog analysis
concepts to digital signals. In this report, we extend the concept of Dirac
combs, a series of Dirac impulses as known from signal theory, to performance
analysis of computers. The goal is to connect methods from electrical
engineering or physics to different models of computation such as graphs, and
network as well as real-time calculus.
</summary>
    <author>
      <name>Iwan Feras Fattohi</name>
    </author>
    <author>
      <name>Christian Prehofer</name>
    </author>
    <author>
      <name>Frank Slomka</name>
    </author>
    <link href="http://arxiv.org/abs/2307.16604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.16604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.03678v1</id>
    <updated>2023-08-07T15:52:00Z</updated>
    <published>2023-08-07T15:52:00Z</published>
    <title>Evaluation of ARM CPUs for IceCube available through Google Kubernetes
  Engine</title>
    <summary>  The IceCube experiment has substantial simulation needs and is in continuous
search for the most cost-effective ways to satisfy them. The most CPU-intensive
part relies on CORSIKA, a cosmic ray air shower simulation. Historically,
IceCube relied exclusively on x86-based CPUs, like Intel Xeon and AMD EPYC, but
recently server-class ARM-based CPUs are also becoming available, both on-prem
and in the cloud. In this paper we present our experience in running a sample
CORSIKA simulation on both ARM and x86 CPUs available through Google Kubernetes
Engine (GKE). We used the production binaries for the x86 instances, but had to
build the binaries for ARM instances from source code, which turned out to be
mostly painless. Our benchmarks show that ARM-based CPUs in GKE were not only
the most cost-effective but were also the fastest in absolute terms in all the
tested configurations. While the advantage is not drastic, about 20% in
cost-effectiveness and less than 10% in absolute terms, it is still large
enough to warrant an investment in ARM support for IceCube.
</summary>
    <author>
      <name>Igor Sfiligoi</name>
    </author>
    <author>
      <name>David Schultz</name>
    </author>
    <author>
      <name>Benedikt Riedel</name>
    </author>
    <author>
      <name>Frank Würthwein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,3 tables, Submitted to proceedings of CHEP23</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.03678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.03678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.07402v2</id>
    <updated>2023-09-18T14:25:59Z</updated>
    <published>2023-08-14T18:43:45Z</published>
    <title>Energy Efficiency of Quantum Statevector Simulation at Scale</title>
    <summary>  Classical simulations are essential for the development of quantum computing,
and their exponential scaling can easily fill any modern supercomputer. In this
paper we consider the performance and energy consumption of large Quantum
Fourier Transform (QFT) simulations run on ARCHER2, the UK's National
Supercomputing Service, with QuEST toolkit. We take into account CPU clock
frequency and node memory size, and use cache-blocking to rearrange the
circuit, which minimises communications. We find that using 2.00GHz instead of
2.25GHz can save as much as 25% of energy at 5% increase in runtime. Higher
node memory also has the potential to be more efficient, and cost the user
fewer CUs, but at higher runtime penalty. Finally, we present a cache-blocking
QFT circuit, which halves the required communication. All our optimisations
combined result in 40% faster simulations and 35% energy savings in 44 qubit
simulations on 4,096 ARCHER2 nodes.
</summary>
    <author>
      <name>Jakub Adamski</name>
    </author>
    <author>
      <name>James Peter Richings</name>
    </author>
    <author>
      <name>Oliver Thomson Brown</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3624062.3624270</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3624062.3624270" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures. Accepted to Sustainable Supercomputing workshop
  at SC23</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.07402v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.07402v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.09385v1</id>
    <updated>2023-08-18T08:33:32Z</updated>
    <published>2023-08-18T08:33:32Z</published>
    <title>Optimization of Resources to Minimize Power Dissipation in 5G Wireless
  Networks</title>
    <summary>  In today's modern communications, with evolution of various applications, the
demand of data rate is increasing exponentially at the cost of huge consumption
of available resources. It has been recorded that the communication networks
dissipate nearly 1\% of the world-wide total power consumption, results in
millions of tons of CO2 emission due to their production and thereby causes
various environmental health hazards. The optimal utilization of available
resources that can balance the present coexisting problem without any
compromise on the high throughput demand, paves the way for the next generation
green 5G wireless networks. In this chapter, we study the minimization of total
power consumption while satisfying the desired coverage of the user equipments
(UEs) to provide the minimum throughput over the network. In this regard, the
deployment of base stations (BSs), their number, and transmit power are
optimized in two scenarios (i) when the UEs are large in 5G wireless network
and (ii) when moderate UEs are distributed over the field.
</summary>
    <author>
      <name>Jyotsna Rani</name>
    </author>
    <author>
      <name>Ganesh Prasad</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-981-15-6390-4_10</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-981-15-6390-4_10" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">5G and Beyond Wireless Systems, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2308.09385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.09385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.04259v1</id>
    <updated>2023-09-08T11:01:05Z</updated>
    <published>2023-09-08T11:01:05Z</published>
    <title>C++ Design Patterns for Low-latency Applications Including
  High-frequency Trading</title>
    <summary>  This work aims to bridge the existing knowledge gap in the optimisation of
latency-critical code, specifically focusing on high-frequency trading (HFT)
systems. The research culminates in three main contributions: the creation of a
Low-Latency Programming Repository, the optimisation of a market-neutral
statistical arbitrage pairs trading strategy, and the implementation of the
Disruptor pattern in C++. The repository serves as a practical guide and is
enriched with rigorous statistical benchmarking, while the trading strategy
optimisation led to substantial improvements in speed and profitability. The
Disruptor pattern showcased significant performance enhancement over
traditional queuing methods. Evaluation metrics include speed, cache
utilisation, and statistical significance, among others. Techniques like Cache
Warming and Constexpr showed the most significant gains in latency reduction.
Future directions involve expanding the repository, testing the optimised
trading algorithm in a live trading environment, and integrating the Disruptor
pattern with the trading algorithm for comprehensive system benchmarking. The
work is oriented towards academics and industry practitioners seeking to
improve performance in latency-sensitive applications.
</summary>
    <author>
      <name>Paul Bilokon</name>
    </author>
    <author>
      <name>Burak Gunduz</name>
    </author>
    <link href="http://arxiv.org/abs/2309.04259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.04259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.TR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.05373v3</id>
    <updated>2023-09-14T07:18:56Z</updated>
    <published>2023-09-11T10:48:58Z</published>
    <title>SPEChpc 2021 Benchmarks on Ice Lake and Sapphire Rapids Infiniband
  Clusters: A Performance and Energy Case Study</title>
    <summary>  In this work, fundamental performance, power, and energy characteristics of
the full SPEChpc 2021 benchmark suite are assessed on two different clusters
based on Intel Ice Lake and Sapphire Rapids CPUs using the MPI-only codes'
variants. We use memory bandwidth, data volume, and scalability metrics in
order to categorize the benchmarks and pinpoint relevant performance and
scalability bottlenecks on the node and cluster levels. Common patterns such as
memory bandwidth limitation, dominating communication and synchronization
overhead, MPI serialization, superlinear scaling, and alignment issues could be
identified, in isolation or in combination, showing that SPEChpc 2021 is
representative of many HPC workloads. Power dissipation and energy measurements
indicate that the modern Intel server CPUs have such a high idle power level
that race-to-idle is the paramount strategy for energy to solution and
energy-delay product minimization. On the chip level, only memory-bound code
shows a clear advantage of Sapphire Rapids compared to Ice Lake in terms of
energy to solution.
</summary>
    <author>
      <name>Ayesha Afzal</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3624062.3624197</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3624062.3624197" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures; corrected links to system docs</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.05373v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.05373v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.09084v1</id>
    <updated>2023-09-16T19:37:53Z</updated>
    <published>2023-09-16T19:37:53Z</published>
    <title>Comparative evaluation of bandwidth-bound applications on the Intel Xeon
  CPU MAX Series</title>
    <summary>  In this paper we explore the performance of Intel Xeon MAX CPU Series,
representing the most significant new variation upon the classical CPU
architecture since the Intel Xeon Phi Processor. Given the availability of a
large on-package high-bandwidth memory, the bandwidth-to-compute ratio has
significantly shifted compared to other CPUs on the market. Since a large
fraction of HPC workloads are sensitive to the available bandwidth, we explore
how this architecture performs on a selection of HPC proxies and applications
that are mostly sensitive to bandwidth, and how it compares to the previous 3rd
generation Intel Xeon Scalable processors (codenamed Ice Lake) and an AMD EPYC
7003 Series Processor with 3D V-Cache Technology (codenamed Milan-X). We
explore performance with different parallel implementations (MPI, MPI+OpenMP,
MPI+SYCL), compiled with different compilers and flags, and executed with or
without hyperthreading. We show how performance bottlenecks are shifted from
bandwidth to communication latencies for some applications, and demonstrate
speedups compared to the previous generation between 2.0x-4.3x.
</summary>
    <author>
      <name>Istvan Z Reguly</name>
    </author>
    <link href="http://arxiv.org/abs/2309.09084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.09084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.10075v1</id>
    <updated>2023-09-18T18:42:56Z</updated>
    <published>2023-09-18T18:42:56Z</published>
    <title>Evaluating the performance portability of SYCL across CPUs and GPUs on
  bandwidth-bound applications</title>
    <summary>  In this paper, we evaluate the portability of the SYCL programming model on
some of the latest CPUs and GPUs from a wide range of vendors, utilizing the
two main compilers: DPC++ and hipSYCL/OpenSYCL. Both compilers currently
support GPUs from all three major vendors; we evaluate performance on the
Intel(R) Data Center GPU Max 1100, the NVIDIA A100 GPU, and the AMD MI250X GPU.
Support on CPUs currently is less established, with DPC++ only supporting x86
CPUs through OpenCL, however, OpenSYCL does have an OpenMP backend capable of
targeting all modern CPUs; we benchmark the Intel Xeon Platinum 8360Y Processor
(Ice Lake), the AMD EPYC 9V33X (Genoa-X), and the Ampere Altra platforms. We
study a range of primarily bandwidth-bound applications implemented using the
OPS and OP2 DSLs, evaluate different formulations in SYCL, and contrast their
performance to "native" programming approaches where available
(CUDA/HIP/OpenMP). On GPU architectures SCYL on average even slightly
outperforms native approaches, while on CPUs it falls behind - highlighting a
continued need for improving CPU performance. While SYCL does not solve all the
challenges of performance portability (e.g. needing different algorithms on
different hardware), it does provide a single programming model and ecosystem
to target most current HPC architectures productively.
</summary>
    <author>
      <name>Istvan Z Reguly</name>
    </author>
    <link href="http://arxiv.org/abs/2309.10075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.10075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.12864v1</id>
    <updated>2023-09-22T13:38:25Z</updated>
    <published>2023-09-22T13:38:25Z</published>
    <title>The Importance of Worst-Case Memory Contention Analysis for
  Heterogeneous SoCs</title>
    <summary>  Memory interference may heavily inflate task execution times in Heterogeneous
Systems-on-Chips (HeSoCs). Knowing worst-case interference is consequently
fundamental for supporting the correct execution of time-sensitive
applications. In most of the literature, worst-case interference is assumed to
be generated by, and therefore is estimated through read-intensive synthetic
workloads with no caching. Yet these workloads do not always generate
worst-case interference. This is the consequence of the general results
reported in this work. By testing on multiple architectures, we determined that
the highest interference generation traffic pattern is actually hardware
dependant, and that making assumptions could lead to a severe underestimation
of the worst-case (in our case, of more than 9x).
</summary>
    <author>
      <name>Lorenzo Carletti</name>
    </author>
    <author>
      <name>Gianluca Brilli</name>
    </author>
    <author>
      <name>Alessandro Capotondi</name>
    </author>
    <author>
      <name>Paolo Valente</name>
    </author>
    <author>
      <name>Andrea Marongiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for presentation at the CPS workshop 2023
  (http://www.cpsschool.eu/cps-workshop)</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.12864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.12864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.14856v1</id>
    <updated>2023-09-26T11:33:16Z</updated>
    <published>2023-09-26T11:33:16Z</published>
    <title>PTPerf: On the performance evaluation of Tor Pluggable Transports</title>
    <summary>  Tor, one of the most popular censorship circumvention systems, faces regular
blocking attempts by censors. Thus, to facilitate access, it relies on
"pluggable transports" (PTs) that disguise Tor's traffic and make it hard for
the adversary to block Tor. However, these are not yet well studied and
compared for the performance they provide to the users. Thus, we conduct a
first comparative performance evaluation of a total of 12 PTs -- the ones
currently supported by the Tor project and those that can be integrated in the
future.
  Our results reveal multiple facets of the PT ecosystem. (1) PTs' download
time significantly varies even under similar network conditions. (2) All PTs
are not equally reliable. Thus, clients who regularly suffer censorship may
falsely believe that such PTs are blocked. (3) PT performance depends on the
underlying communication primitive. (4) PTs performance significantly depends
on the website access method (browser or command-line). Surprisingly, for some
PTs, website access time was even less than vanilla Tor.
  Based on our findings from more than 1.25M measurements, we provide
recommendations about selecting PTs and believe that our study can facilitate
access for users who face censorship.
</summary>
    <author>
      <name>Zeya Umayya</name>
    </author>
    <author>
      <name>Dhruv Malik</name>
    </author>
    <author>
      <name>Devashish Gosain</name>
    </author>
    <author>
      <name>Piyush Kumar Sharma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.14856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.14856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.15172v1</id>
    <updated>2023-09-26T18:13:34Z</updated>
    <published>2023-09-26T18:13:34Z</published>
    <title>Unbalanced Job Approximation using Taylor Series Expansion and Review of
  Performance Bounds</title>
    <summary>  Unbalanced Job Approximation - UJA is a family of low-cost formulas to obtain
the throughput of Queueing Networks - QNs with fixed rate servers using Taylor
series expansion of job loadings with respect to the mean loading. UJA with one
term yields the same throughput as optimistic Balanced Job Bound - BJB, which
at some point exceeds the maximum asymptotic throughput. The accuracy of the
estimated throughput increases with more terms in the Taylor series. UJA can be
used in parametric studies by reducing the cost of solving large QNs by
aggregating stations into a single Flow-Equivalent Service Center - FESCs
defined by its throughput characteristic. While UJA has been extended to two
classes it may be applied to more classes by job class aggregation. BJB has
been extended to QNs with delay servers and multiple jobs classes by Eager and
Sevcik, throughput bounds by Eager and Sevcik, Kriz, Proportional Bound - PB
and Prop. Approximation Bound - PAM by Hsieh and Lam and Geometric Bound - GB
by Casale et al. are reviewed.
</summary>
    <author>
      <name>Alexander Thomasian</name>
    </author>
    <link href="http://arxiv.org/abs/2309.15172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.15172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.00788v1</id>
    <updated>2023-10-01T20:55:13Z</updated>
    <published>2023-10-01T20:55:13Z</published>
    <title>Web Image Formats: Assessment of Their Real-World-Usage and Performance
  across Popular Web Browsers</title>
    <summary>  In 2023, images on the web make up 41% of transmitted data, significantly
impacting the performance of web apps. Fortunately, image formats like WEBP and
AVIF could offer advanced compression and faster page loading, but may face
performance disparities across browsers. Therefore, we conducted performance
evaluations on five major browsers - Chrome, Edge, Safari, Opera, and Firefox -
while comparing four image formats. The results indicate that the newer formats
exhibited notable performance enhancements across all browsers, leading to
shorter loading times. Compared to the compressed JPEG format, WEBP and AVIF
improved the Page Load Time by 21% and 15%, respectively. However, web scraping
revealed that JPEG and PNG still dominate web image choices, with WEBP at 4% as
the most used new format. Through the web scraping and web performance
evaluation, this research serves to (1) explore image format preferences in web
applications and analyze distribution and characteristics across
frequently-visited sites in 2023 and (2) assess the performance impact of
distinct web image formats on application load times across popular web
browsers.
</summary>
    <author>
      <name>Benedikt Dornauer</name>
    </author>
    <author>
      <name>Michael Felderer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint: Product-Focused Software Process Improvement 24th
  International Conference, PROFES 2023, Dornbirn, Austria , Dezember 10-13,
  2023, Proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.00788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.00788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.06178v1</id>
    <updated>2023-10-09T22:06:35Z</updated>
    <published>2023-10-09T22:06:35Z</published>
    <title>Look-Up mAI GeMM: Increasing AI GeMMs Performance by Nearly 2.5x via
  msGeMM</title>
    <summary>  AI models are increasing in size and recent advancement in the community has
shown that unlike HPC applications where double precision datatype are
required, lower-precision datatypes such as fp8 or int4 are sufficient to bring
the same model quality both for training and inference. Following these trends,
GPU vendors such as NVIDIA and AMD have added hardware support for fp16, fp8
and int8 GeMM operations with an exceptional performance via Tensor Cores.
However, this paper proposes a new algorithm called msGeMM which shows that AI
models with low-precision datatypes can run with ~2.5x fewer multiplication and
add instructions. Efficient implementation of this algorithm requires special
CUDA cores with the ability to add elements from a small look-up table at the
rate of Tensor Cores.
</summary>
    <author>
      <name>Saeed Maleki</name>
    </author>
    <link href="http://arxiv.org/abs/2310.06178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.06178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.06939v1</id>
    <updated>2023-10-10T18:50:27Z</updated>
    <published>2023-10-10T18:50:27Z</published>
    <title>On the Role of Font Formats in Building Efficient Web Applications</title>
    <summary>  The success of a web application is closely linked to its performance, which
positively impacts user satisfaction and contributes to energy-saving efforts.
Among the various optimization techniques, one specific subject focuses on
improving the utilization of web fonts. This study investigates the impact of
different font formats on client-side resource consumption, such as CPU,
memory, load time, and energy. In a controlled experiment, we evaluate
performance metrics using the four font formats: OTF, TTF, WOFF, and WOFF2. The
results of the study show that there are significant differences between all
pair-wise format comparisons regarding all performance metrics. Overall, WOFF2
performs best, except in terms of memory allocation. Through the study and
examination of literature, this research contributes (1) an overview of
methodologies to enhance web performance through font utilization, (2) a
specific exploration of the four prevalent font formats in an experimental
setup, and (3) practical recommendations for scientific professionals and
practitioners.
</summary>
    <author>
      <name>Benedikt Dornauer</name>
    </author>
    <author>
      <name>Wolfgang Vigl</name>
    </author>
    <author>
      <name>Michael Felderer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint: Product-Focused Software Process Improvement 24th
  International Conference, PROFES 2023, Dornbirn, Austria, December 10-13,
  2023, Proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.06939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.06939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.08246v1</id>
    <updated>2023-10-12T11:50:02Z</updated>
    <published>2023-10-12T11:50:02Z</published>
    <title>Collaborative Precoding Design for Adjacent Integrated Sensing and
  Communication Base Stations</title>
    <summary>  Integrated sensing and communication (ISAC) base stations can provide
communication and wide range sensing information for vehicles via downlink (DL)
transmission, thus enhancing vehicle driving safety. One major challenge for
realizing high performance communication and sensing is how to deal with the DL
mutual interference among adjacent ISAC base stations, which includes not only
communication related interference, but also radar sensing related
interference. In this paper, we establish a DL mutual interference model of
adjacent ISAC base stations, and analyze the relationship for mutual
interference channels between communications and radar sensing. To improve the
sensing and communication performance, we propose a collaborative precoding
design for coordinated adjacent base stations to mitigate the mutual
interference under the transmit power constraint and constant modulus
constraint, which is formulated as a non-convex optimization problem. We first
relax the problem into a convex programming by omitting the rank constraint,
and propose a joint optimization algorithm to solve the problem. We furthermore
propose a sequential optimization algorithm, which divides the collaborative
precoding design problem into four subproblems and finds the optimum via a
gradient descent algorithm. Finally, we evaluate the collaborative precoding
design algorithms by considering sensing and communication performance via
numerical results.
</summary>
    <author>
      <name>Wangjun Jiang</name>
    </author>
    <author>
      <name>Zhiqing Wei</name>
    </author>
    <author>
      <name>Fan Liu</name>
    </author>
    <author>
      <name>Zhiyong Feng</name>
    </author>
    <author>
      <name>Ping Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.08246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.08246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.13212v1</id>
    <updated>2023-10-20T01:13:49Z</updated>
    <published>2023-10-20T01:13:49Z</published>
    <title>Facile: Fast, Accurate, and Interpretable Basic-Block Throughput
  Prediction</title>
    <summary>  Basic-block throughput models such as uiCA, IACA, GRANITE, Ithemal, llvm-mca,
OSACA, or CQA guide optimizing compilers and help performance engineers
identify and eliminate bottlenecks. For this purpose, basic-block throughput
models should ideally be fast, accurate, and interpretable.
  Recent advances have significantly improved accuracy: uiCA, the
state-of-the-art model, achieves an error of about 1% relative to measurements
across a wide range of microarchitectures. The computational efficiency of
throughput models, which is equally important for widespread adoption,
especially in compilers, has so far received little attention.
  In this paper, we introduce Facile, an analytical throughput model that is
fast, accurate, and interpretable. Facile analyzes different potential
bottlenecks independently and analytically. Due to its compositional nature,
Facile's predictions directly pinpoint the bottlenecks. We evaluate Facile on a
wide range of microarchitectures and show that it is almost two orders of
magnitude faster than existing models while achieving state-of-the-art
accuracy.
</summary>
    <author>
      <name>Andreas Abel</name>
    </author>
    <author>
      <name>Shrey Sharma</name>
    </author>
    <author>
      <name>Jan Reineke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IISWC59245.2023.00023</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IISWC59245.2023.00023" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Symposium on Workload Characterization, IISWC
  2023, Ghent, Belgium, October 1-3, 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2310.13212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.13212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.13513v2</id>
    <updated>2023-10-27T02:56:54Z</updated>
    <published>2023-10-20T13:57:28Z</published>
    <title>Exploring the Potential of Flexible 8-bit Format: Design and Algorithm</title>
    <summary>  Neural network quantization is widely used to reduce model inference
complexity in real-world deployments. However, traditional integer quantization
suffers from accuracy degradation when adapting to various dynamic ranges.
Recent research has focused on a new 8-bit format, FP8, with hardware support
for both training and inference of neural networks but lacks guidance for
hardware design. In this paper, we analyze the benefits of using FP8
quantization and provide a comprehensive comparison of FP8 with INT
quantization. Then we propose a flexible mixed-precision quantization framework
that supports various number systems, enabling optimal selection of the most
appropriate quantization format for different neural network architectures.
Experimental results demonstrate that our proposed framework achieves
competitive performance compared to full precision on various tasks, including
image classification, object detection, segmentation, and natural language
understanding. Our work furnishes critical insights into the tangible benefits
and feasibility of employing FP8 quantization, paving the way for heightened
neural network efficiency in tangible scenarios. Our code is available in the
supplementary material.
</summary>
    <author>
      <name>Zhuoyi Zhang</name>
    </author>
    <author>
      <name>Yunchen Zhang</name>
    </author>
    <author>
      <name>Gonglei Shi</name>
    </author>
    <author>
      <name>Yu Shen</name>
    </author>
    <author>
      <name>Ruihao Gong</name>
    </author>
    <author>
      <name>Xiaoxu Xia</name>
    </author>
    <author>
      <name>Qi Zhang</name>
    </author>
    <author>
      <name>Lewei Lu</name>
    </author>
    <author>
      <name>Xianglong Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2310.13513v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.13513v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.13637v2</id>
    <updated>2023-12-13T22:40:21Z</updated>
    <published>2023-10-20T16:37:03Z</published>
    <title>Application Performance Benchmarks for Quantum Computers</title>
    <summary>  Current technological advancements of quantum computers highlight the need
for application-driven, practical and well-defined methods of benchmarking
their performance. As the existing NISQ device's quality of two-qubit gate
errors rate is even around one percent and the number of qubits is still
limited to a few or several dozen, naturally, we need to propose rather small
algorithms instances taken from key promising application areas, such as
quantum chemistry, combinatorial optimisation or machine learning. While many
techniques for assessing the performance of logical components, such as gate
fidelity and qubit coherence exist, it is still challenging to extrapolate
those values onto the performance of different quantum algorithms and
subroutines. This work aims to introduce a series of initial quantum
application benchmarks together with a methodology of execution for measuring
the performance and fidelity of the results. The proposed suite refers to
several variational algorithms widely used on available NISQ devices but also
includes examples of quantum circuits designed for a fault-tolerant quantum
computer.
</summary>
    <author>
      <name>Krzysztof Kurowski</name>
    </author>
    <author>
      <name>Piotr Rydlichowski</name>
    </author>
    <author>
      <name>Konrad Wojciechowski</name>
    </author>
    <author>
      <name>Tomasz Pecyna</name>
    </author>
    <author>
      <name>Mateusz Slysz</name>
    </author>
    <link href="http://arxiv.org/abs/2310.13637v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.13637v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.15560v1</id>
    <updated>2023-10-24T07:02:55Z</updated>
    <published>2023-10-24T07:02:55Z</published>
    <title>Modeling and Design of the Communication Sensing and Control Coupled
  Closed-Loop Industrial System</title>
    <summary>  With the advent of 5G era, factories are transitioning towards wireless
networks to break free from the limitations of wired networks. In 5G-enabled
factories, unmanned automatic devices such as automated guided vehicles and
robotic arms complete production tasks cooperatively through the periodic
control loops. In such loops, the sensing data is generated by sensors, and
transmitted to the control center through uplink wireless communications. The
corresponding control commands are generated and sent back to the devices
through downlink wireless communications. Since wireless communications,
sensing and control are tightly coupled, there are big challenges on the
modeling and design of such closed-loop systems. In particular, existing
theoretical tools of these functionalities have different modelings and
underlying assumptions, which make it difficult for them to collaborate with
each other. Therefore, in this paper, an analytical closed-loop model is
proposed, where the performances and resources of communication, sensing and
control are deeply related. To achieve the optimal control performance, a
co-design of communication resource allocation and control method is proposed,
inspired by the model predictive control algorithm. Numerical results are
provided to demonstrate the relationships between the resources and control
performances.
</summary>
    <author>
      <name>Zeyang Meng</name>
    </author>
    <author>
      <name>Dingyou Ma</name>
    </author>
    <author>
      <name>Shengfeng Wang</name>
    </author>
    <author>
      <name>Zhiqing Wei</name>
    </author>
    <author>
      <name>Zhiyong Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, received by GlobeCom 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.15560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.15560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="93C55, 94A99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.16122v1</id>
    <updated>2023-10-24T18:52:24Z</updated>
    <published>2023-10-24T18:52:24Z</published>
    <title>A Performance-Portable SYCL Implementation of CRK-HACC for Exascale</title>
    <summary>  The first generation of exascale systems will include a variety of machine
architectures, featuring GPUs from multiple vendors. As a result, many
developers are interested in adopting portable programming models to avoid
maintaining multiple versions of their code. It is necessary to document
experiences with such programming models to assist developers in understanding
the advantages and disadvantages of different approaches.
  To this end, this paper evaluates the performance portability of a SYCL
implementation of a large-scale cosmology application (CRK-HACC) running on
GPUs from three different vendors: AMD, Intel, and NVIDIA. We detail the
process of migrating the original code from CUDA to SYCL and show that
specializing kernels for specific targets can greatly improve performance
portability without significantly impacting programmer productivity. The SYCL
version of CRK-HACC achieves a performance portability of 0.96 with a code
divergence of almost 0, demonstrating that SYCL is a viable programming model
for performance-portable applications.
</summary>
    <author>
      <name>Esteban M. Rangel</name>
    </author>
    <author>
      <name>S. John Pennycook</name>
    </author>
    <author>
      <name>Adrian Pope</name>
    </author>
    <author>
      <name>Nicholas Frontiere</name>
    </author>
    <author>
      <name>Zhiqiang Ma</name>
    </author>
    <author>
      <name>Varsha Madananth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 13 figures, 2023 International Workshop on Performance,
  Portability &amp; Productivity in HPC</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.16122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.16122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.7; D.2.8; D.1.3; J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.07602v1</id>
    <updated>2023-11-11T02:33:10Z</updated>
    <published>2023-11-11T02:33:10Z</published>
    <title>Cache Optimization and Performance Modeling of Batched, Small, and
  Rectangular Matrix Multiplication on Intel, AMD, and Fujitsu Processors</title>
    <summary>  Factorization and multiplication of dense matrices and tensors are critical,
yet extremely expensive pieces of the scientific toolbox. Careful use of low
rank approximation can drastically reduce the computation and memory
requirements of these operations. In addition to a lower arithmetic complexity,
such methods can, by their structure, be designed to efficiently exploit modern
hardware architectures. The majority of existing work relies on batched BLAS
libraries to handle the computation of many small dense matrices. We show that
through careful analysis of the cache utilization, register accumulation using
SIMD registers and a redesign of the implementation, one can achieve
significantly higher throughput for these types of batched low-rank matrices
across a large range of block and batch sizes. We test our algorithm on 3 CPUs
using diverse ISAs -- the Fujitsu A64FX using ARM SVE, the Intel Xeon 6148
using AVX-512 and AMD EPYC 7502 using AVX-2, and show that our new batching
methodology is able to obtain more than twice the throughput of vendor
optimized libraries for all CPU architectures and problem sizes.
</summary>
    <author>
      <name>Sameer Deshmukh</name>
    </author>
    <author>
      <name>Rio Yokota</name>
    </author>
    <author>
      <name>George Bosilca</name>
    </author>
    <link href="http://arxiv.org/abs/2311.07602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.07602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.14650v5</id>
    <updated>2024-06-28T21:05:20Z</updated>
    <published>2023-11-24T18:32:34Z</published>
    <title>GVEL: Fast Graph Loading in Edgelist and Compressed Sparse Row (CSR)
  formats</title>
    <summary>  Efficient IO techniques are crucial in high-performance graph processing
frameworks like Gunrock and Hornet, as fast graph loading can help minimize
processing time and reduce system/cloud usage charges. This research study
presents approaches for efficiently reading an Edgelist from a text file and
converting it to a Compressed Sparse Row (CSR) representation. On a server with
dual 16-core Intel Xeon Gold 6226R processors and Seagate Exos 10e2400 HDDs,
our approach, which we term as GVEL, outperforms Hornet, Gunrock, and PIGO by
significant margins in CSR reading, exhibiting an average speedup of 78x, 112x,
and 1.8x, respectively. For Edgelist reading, GVEL is 2.6x faster than PIGO on
average, and achieves a Edgelist read rate of 1.9 billion edges/s. For every
doubling of threads, GVEL improves performance at an average rate of 1.9x and
1.7x for reading Edgelist and reading CSR respectively.
</summary>
    <author>
      <name>Subhajit Sahu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 9 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.14650v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.14650v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.03113v1</id>
    <updated>2023-12-05T20:17:38Z</updated>
    <published>2023-12-05T20:17:38Z</published>
    <title>GPU Graph Processing on CXL-Based Microsecond-Latency External Memory</title>
    <summary>  In GPU graph analytics, the use of external memory such as the host DRAM and
solid-state drives is a cost-effective approach to processing large graphs
beyond the capacity of the GPU onboard memory. This paper studies the use of
Compute Express Link (CXL) memory as alternative external memory for GPU graph
processing in order to see if this emerging memory expansion technology enables
graph processing that is as fast as using the host DRAM. Through analysis and
evaluation using FPGA prototypes, we show that representative GPU graph
traversal algorithms involving fine-grained random access can tolerate an
external memory latency of up to a few microseconds introduced by the CXL
interface as well as by the underlying memory devices. This insight indicates
that microsecond-latency flash memory may be used as CXL memory devices to
realize even more cost-effective GPU graph processing while still achieving
performance close to using the host DRAM.
</summary>
    <author>
      <name>Shintaro Sano</name>
    </author>
    <author>
      <name>Yosuke Bando</name>
    </author>
    <author>
      <name>Kazuhiro Hiwada</name>
    </author>
    <author>
      <name>Hirotsugu Kajihara</name>
    </author>
    <author>
      <name>Tomoya Suzuki</name>
    </author>
    <author>
      <name>Yu Nakanishi</name>
    </author>
    <author>
      <name>Daisuke Taki</name>
    </author>
    <author>
      <name>Akiyuki Kaneko</name>
    </author>
    <author>
      <name>Tatsuo Shiozawa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3624062.3624173</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3624062.3624173" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the SC '23 Workshops of The International
  Conference on High Performance Computing, Network, Storage, and Analysis
  (SC-W '23), pp. 962-972, November 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2312.03113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.03113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.05779v1</id>
    <updated>2023-12-10T05:55:56Z</updated>
    <published>2023-12-10T05:55:56Z</published>
    <title>Autotuning by Changing Directives and Number of Threads in OpenMP using
  ppOpen-AT</title>
    <summary>  Recently, computers have diversified architectures. To achieve high numerical
calculation software performance, it is necessary to tune the software
according to the target computer architecture. However, code optimization for
each environment is difficult unless it is performed by a specialist who knows
computer architectures well. By applying autotuning (AT), the tuning effort can
be reduced. Optimized implementation by AT that enhances computer performance
can be used even by non-experts. In this research, we propose a technique for
AT for programs using open multi-processing (OpenMP). We propose an AT method
using an AT language that changes the OpenMP optimized loop and dynamically
changes the number of threads in OpenMP according to computational kernels.
Performance evaluation was performed using the Fujitsu PRIMEHPC FX100, which is
a K-computer type supercomputer installed at the Information Technology Center,
Nagoya University. As a result, we found there was a performance increase of
1.801 times that of the original code in a plasma turbulence analysis.
</summary>
    <author>
      <name>Toma Sakurai</name>
    </author>
    <author>
      <name>Satoshi Ohshima</name>
    </author>
    <author>
      <name>Takahiro Katagiri</name>
    </author>
    <author>
      <name>Toru Nagai</name>
    </author>
    <link href="http://arxiv.org/abs/2312.05779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.05779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.02431v2</id>
    <updated>2024-01-09T08:44:50Z</updated>
    <published>2023-11-14T07:55:56Z</published>
    <title>Execution time budget assignment for mixed criticality systems</title>
    <summary>  In this paper we propose to quantify execution time variability of programs
using statistical dispersion parameters. We show how the execution time
variability can be exploited in mixed criticality real-time systems. We propose
a heuristic to compute the execution time budget to be allocated to each low
criticality real-time task according to its execution time variability. We show
using experiments and simulations that the proposed heuristic reduces the
probability of exceeding the allocated budget compared to algorithms which do
not take into account the execution time variability parameter.
</summary>
    <author>
      <name>Mohamed Amine Khelassi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIGM</arxiv:affiliation>
    </author>
    <author>
      <name>Yasmina Abdeddaïm</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIGM</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">10th International Workshop on Mixed Criticality Systems at the
  Real Time Systems Symposium (RTSS 2023), IEEE, Dec 2023, Taipei, Taiwan</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2401.02431v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.02431v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.11934v1</id>
    <updated>2024-01-22T13:25:08Z</updated>
    <published>2024-01-22T13:25:08Z</published>
    <title>Systematic Performance Evaluation Framework for LEO Mega-Constellation
  Satellite Networks</title>
    <summary>  Low Earth orbit (LEO) mega-constellation satellite networks have shown great
potential to extend the coverage capability of conventional terrestrial
networks. How to systematically define, quantify, and assess the technical
performance of LEO mega-constellation satellite networks remains an open issue.
In this paper, we propose a comprehensive key performance indicator (KPI)
framework for mega-constellation based LEO satellite networks. An efficient LEO
constellation oriented performance evaluation methodology is then carefully
designed by resorting to the concept of interfering area and spherical
geographic cell. We have carried out rigorous system-level simulations and
provided numerical results to assess the KPI framework. It can be observed that
the achieved area traffic capacity of the reference LEO constellation is around
4 Kbps/km2, with service availability ranging from 0.36 to 0.39. Besides, the
average access success probability and handover failure rate is approximate to
96% and 10%, respectively, in the nearest satellite association scheme.
</summary>
    <author>
      <name>Yu Wang</name>
    </author>
    <author>
      <name>Chuili Kong</name>
    </author>
    <author>
      <name>Xian Meng</name>
    </author>
    <author>
      <name>Hejia Luo</name>
    </author>
    <author>
      <name>Ke-Xin Li</name>
    </author>
    <author>
      <name>Jun Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures, accepted by IEEE ICC2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.11934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.11934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.16492v1</id>
    <updated>2024-01-29T19:06:08Z</updated>
    <published>2024-01-29T19:06:08Z</published>
    <title>GPU Cluster Scheduling for Network-Sensitive Deep Learning</title>
    <summary>  We propose a novel GPU-cluster scheduler for distributed DL (DDL) workloads
that enables proximity based consolidation of GPU resources based on the DDL
jobs' sensitivities to the anticipated communication-network delays. Our
scheduler consists of three major components: (i) a classical delay scheduling
algorithm to facilitate job placement and consolidation; (ii) a
network-sensitive job preemption strategy; and (iii) an "auto-tuner" mechanism
to optimize delay timers for effective delay scheduling. Additionally, to
enable a cost-effective methodology for large-scale experiments, we develop a
data-driven DDL cluster simulation platform. Employing the simulation platform
we compare against several state-of-the-art alternatives on real-world workload
traces to demonstrate the benefits of our design. Our scheduler can provide
improvement of up to 69% in end-to-end Makespan for training all jobs compared
to the prevailing consolidation-based scheduling methods, while reducing the
average job completion time by up to 83% and minimizing the communication
overheads by up to 98% under congested networking conditions.
</summary>
    <author>
      <name>Aakash Sharma</name>
    </author>
    <author>
      <name>Vivek M. Bhasi</name>
    </author>
    <author>
      <name>Sonali Singh</name>
    </author>
    <author>
      <name>George Kesidis</name>
    </author>
    <author>
      <name>Mahmut T. Kandemir</name>
    </author>
    <author>
      <name>Chita R. Das</name>
    </author>
    <link href="http://arxiv.org/abs/2401.16492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.16492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.05483v1</id>
    <updated>2024-02-08T08:25:01Z</updated>
    <published>2024-02-08T08:25:01Z</published>
    <title>Reconsidering the performance of DEVS modeling and simulation
  environments using the DEVStone benchmark</title>
    <summary>  The Discrete Event System Specification formalism (DEVS), which supports
hierarchical and modular model composition, has been widely used to understand,
analyze and develop a variety of systems. DEVS has been implemented in various
languages and platforms over the years. The DEVStone benchmark was conceived to
generate a set of models with varied structure and behavior, and to automate
the evaluation of the performance of DEVS-based simulators. However, DEVStone
is still in a preliminar phase and more model analysis is required. In this
paper, we revisit DEVStone introducing new equations to compute the number of
events triggered. We also introduce a new benchmark, called HOmem, designed as
an alternative version of HOmod, with similar CPU and memory requirements, but
with an easier implementation and analytically more manageable. Finally, we
compare both the performance and memory footprint of five different DEVS
simulators in two different hardware platforms.
</summary>
    <author>
      <name>José L. Risco-Martín</name>
    </author>
    <author>
      <name>Saurabh Mittal</name>
    </author>
    <author>
      <name>Juan Carlos Fabero</name>
    </author>
    <author>
      <name>Marina Zapater</name>
    </author>
    <author>
      <name>Román Hermida</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1177/0037549717690447</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1177/0037549717690447" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SIMULATION, 93(6), 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2402.05483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.05483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.14567v1</id>
    <updated>2024-02-22T14:16:12Z</updated>
    <published>2024-02-22T14:16:12Z</published>
    <title>CesASMe and Staticdeps: static detection of memory-carried dependencies
  for code analyzers</title>
    <summary>  A variety of code analyzers, such as IACA, uiCA, llvm-mca or Ithemal, strive
to statically predict the throughput of a computation kernel. Each analyzer is
based on its own simplified CPU model reasoning at the scale of a basic block.
Facing this diversity, evaluating their strengths and weaknesses is important
to guide both their usage and their enhancement.
  We present CesASMe, a fully-tooled solution to evaluate code analyzers on
C-level benchmarks composed of a benchmark derivation procedure that feeds an
evaluation harness. We conclude that memory-carried data dependencies are a
major source of imprecision for these tools. We tackle this issue with
staticdeps, a static analyzer extracting memory-carried data dependencies,
including across loop iterations, from an assembly basic block. We integrate
its output to uiCA, a state-of-the-art code analyzer, to evaluate staticdeps'
impact on a code analyzer's precision through CesASMe.
</summary>
    <author>
      <name>Théophile Bastian</name>
    </author>
    <author>
      <name>Hugo Pompougnac</name>
    </author>
    <author>
      <name>Alban Dutilleul</name>
    </author>
    <author>
      <name>Fabrice Rastello</name>
    </author>
    <link href="http://arxiv.org/abs/2402.14567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.14567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.15773v1</id>
    <updated>2024-02-24T09:15:45Z</updated>
    <published>2024-02-24T09:15:45Z</published>
    <title>Performance bottlenecks detection through microarchitectural sensitivity</title>
    <summary>  Modern Out-of-Order (OoO) CPUs are complex systems with many components
interleaved in non-trivial ways. Pinpointing performance bottlenecks and
understanding the underlying causes of program performance issues are critical
tasks to make the most of hardware resources.
  We provide an in-depth overview of performance bottlenecks in recent OoO
microarchitectures and describe the difficulties of detecting them. Techniques
that measure resources utilization can offer a good understanding of a
program's execution, but, due to the constraints inherent to Performance
Monitoring Units (PMU) of CPUs, do not provide the relevant metrics for each
use case.
  Another approach is to rely on a performance model to simulate the CPU
behavior. Such a model makes it possible to implement any new
microarchitecture-related metric. Within this framework, we advocate for
implementing modeled resources as parameters that can be varied at will to
reveal performance bottlenecks. This allows a generalization of bottleneck
analysis that we call sensitivity analysis.
  We present Gus, a novel performance analysis tool that combines the
advantages of sensitivity analysis and dynamic binary instrumentation within a
resource-centric CPU model. We evaluate the impact of sensitivity on bottleneck
analysis over a set of high-performance computing kernels.
</summary>
    <author>
      <name>Hugo Pompougnac</name>
    </author>
    <author>
      <name>Alban Dutilleul</name>
    </author>
    <author>
      <name>Christophe Guillon</name>
    </author>
    <author>
      <name>Nicolas Derumigny</name>
    </author>
    <author>
      <name>Fabrice Rastello</name>
    </author>
    <link href="http://arxiv.org/abs/2402.15773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.15773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.01164v1</id>
    <updated>2024-03-02T10:37:21Z</updated>
    <published>2024-03-02T10:37:21Z</published>
    <title>HeteGen: Heterogeneous Parallel Inference for Large Language Models on
  Resource-Constrained Devices</title>
    <summary>  In recent times, the emergence of Large Language Models (LLMs) has resulted
in increasingly larger model size, posing challenges for inference on
low-resource devices. Prior approaches have explored offloading to facilitate
low-memory inference but often suffer from efficiency due to I/O bottlenecks.
To achieve low-latency LLMs inference on resource-constrained devices, we
introduce HeteGen, a novel approach that presents a principled framework for
heterogeneous parallel computing using CPUs and GPUs. Based on this framework,
HeteGen further employs heterogeneous parallel computing and asynchronous
overlap for LLMs to mitigate I/O bottlenecks. Our experiments demonstrate a
substantial improvement in inference speed, surpassing state-of-the-art methods
by over 317% at most.
</summary>
    <author>
      <name>Xuanlei Zhao</name>
    </author>
    <author>
      <name>Bin Jia</name>
    </author>
    <author>
      <name>Haotian Zhou</name>
    </author>
    <author>
      <name>Ziming Liu</name>
    </author>
    <author>
      <name>Shenggan Cheng</name>
    </author>
    <author>
      <name>Yang You</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MLSys 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.01164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.01164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.01579v1</id>
    <updated>2024-03-03T18:03:20Z</updated>
    <published>2024-03-03T18:03:20Z</published>
    <title>A Continuous Benchmarking Infrastructure for High-Performance Computing
  Applications</title>
    <summary>  For scientific software, especially those used for large-scale simulations,
achieving good performance and efficiently using the available hardware
resources is essential. It is important to regularly perform benchmarks to
ensure the efficient use of hardware and software when systems are changing and
the software evolves. However, this can become quickly very tedious when many
options for parameters, solvers, and hardware architectures are available. We
present a continuous benchmarking strategy that automates benchmarking new code
changes on high-performance computing clusters. This makes it possible to track
how each code change affects the performance and how it evolves.
</summary>
    <author>
      <name>Christoph Alt</name>
    </author>
    <author>
      <name>Martin Lanser</name>
    </author>
    <author>
      <name>Jonas Plewinski</name>
    </author>
    <author>
      <name>Atin Janki</name>
    </author>
    <author>
      <name>Axel Klawonn</name>
    </author>
    <author>
      <name>Harald Köstler</name>
    </author>
    <author>
      <name>Michael Selzer</name>
    </author>
    <author>
      <name>Ulrich Rüde</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/17445760.2024.2360190</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/17445760.2024.2360190" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Parallel, Emergent &amp; Distributed Systems,
  2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2403.01579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.01579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.16063v1</id>
    <updated>2024-03-24T08:25:42Z</updated>
    <published>2024-03-24T08:25:42Z</published>
    <title>Explainable Port Mapping Inference with Sparse Performance Counters for
  AMD's Zen Architectures</title>
    <summary>  Performance models are instrumental for optimizing performance-sensitive
code. When modeling the use of functional units of out-of-order x86-64 CPUs,
data availability varies by the manufacturer: Instruction-to-port mappings for
Intel's processors are available, whereas information for AMD's designs are
lacking. The reason for this disparity is that standard techniques to infer
exact port mappings require hardware performance counters that AMD does not
provide.
  In this work, we modify the port mapping inference algorithm of the widely
used uops.info project to not rely on Intel's performance counters. The
modifications are based on a formal port mapping model with a
counter-example-guided algorithm powered by an SMT solver. We investigate in
how far AMD's processors comply with this model and where unexpected
performance characteristics prevent an accurate port mapping. Our results
provide valuable insights for creators of CPU performance models as well as for
software developers who want to achieve peak performance on recent AMD CPUs.
</summary>
    <author>
      <name>Fabian Ritter</name>
    </author>
    <author>
      <name>Sebastian Hack</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ASPLOS 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.16063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.16063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.16745v1</id>
    <updated>2024-03-25T13:19:09Z</updated>
    <published>2024-03-25T13:19:09Z</published>
    <title>Multilevel Modeling as a Methodology for the Simulation of Human
  Mobility</title>
    <summary>  Multilevel modeling is increasingly relevant in the context of modelling and
simulation since it leads to several potential benefits, such as software reuse
and integration, the split of semantically separated levels into sub-models,
the possibility to employ different levels of detail, and the potential for
parallel execution. The coupling that inevitably exists between the sub-models,
however, implies the need for maintaining consistency between the various
components, more so when different simulation paradigms are employed (e.g.,
sequential vs parallel, discrete vs continuous). In this paper we argue that
multilevel modelling is well suited for the simulation of human mobility, since
it naturally leads to the decomposition of the model into two layers, the
"micro" and "macro" layer, where individual entities (micro) and long-range
interactions (macro) are described. In this paper we investigate the challenges
of multilevel modeling, and describe some preliminary results using prototype
implementations of multilayer simulators in the context of epidemic diffusion
and vehicle pollution.
</summary>
    <author>
      <name>Luca Serena</name>
    </author>
    <author>
      <name>Moreno Marzolla</name>
    </author>
    <author>
      <name>Gabriele D'Angelo</name>
    </author>
    <author>
      <name>Stefano Ferretti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DS-RT55542.2022.9932080</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DS-RT55542.2022.9932080" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">proc. 2022 IEEE/ACM 26th International Symposium on Distributed
  Simulation and Real-Time Applications (DS-RT'22), Al\`es, France, September
  26-28, 2022, pp. 49-56</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2403.16745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.16745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.00039v1</id>
    <updated>2024-03-24T02:45:34Z</updated>
    <published>2024-03-24T02:45:34Z</published>
    <title>MicroHD: An Accuracy-Driven Optimization of Hyperdimensional Computing
  Algorithms for TinyML systems</title>
    <summary>  Hyperdimensional computing (HDC) is emerging as a promising AI approach that
can effectively target TinyML applications thanks to its lightweight computing
and memory requirements. Previous works on HDC showed that limiting the
standard 10k dimensions of the hyperdimensional space to much lower values is
possible, reducing even more HDC resource requirements. Similarly, other
studies demonstrated that binary values can be used as elements of the
generated hypervectors, leading to significant efficiency gains at the cost of
some degree of accuracy degradation. Nevertheless, current optimization
attempts do not concurrently co-optimize HDC hyper-parameters, and accuracy
degradation is not directly controlled, resulting in sub-optimal HDC models
providing several applications with unacceptable output qualities. In this
work, we propose MicroHD, a novel accuracy-driven HDC optimization approach
that iteratively tunes HDC hyper-parameters, reducing memory and computing
requirements while ensuring user-defined accuracy levels. The proposed method
can be applied to HDC implementations using different encoding functions,
demonstrates good scalability for larger HDC workloads, and achieves
compression and efficiency gains up to 200x when compared to baseline
implementations for accuracy degradations lower than 1%.
</summary>
    <author>
      <name>Flavio Ponzina</name>
    </author>
    <author>
      <name>Tajana Rosing</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a full paper by the tinyML Research Symposium 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.00039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.00039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.01473v2</id>
    <updated>2024-06-24T22:30:15Z</updated>
    <published>2024-04-01T20:49:56Z</published>
    <title>gpu_tracker: Python package for tracking and profiling GPU utilization
  in both desktop and high-performance computing environments</title>
    <summary>  Determining the maximum usage of random-access memory (RAM) on both the
motherboard and on a graphical processing unit (GPU) over the lifetime of a
computing task can be extremely useful for troubleshooting points of failure as
well as optimizing memory utilization, especially within a high-performance
computing (HPC) setting. While there are tools for tracking compute time and
RAM, including by job management tools themselves, tracking of GPU usage, to
our knowledge, does not currently have sufficient solutions. We present
gpu_tracker, a Python package that tracks the computational resource usage of a
task while running in the background, including the real compute time that the
task takes to complete, its maximum RAM usage, and the maximum GPU RAM usage,
specifically for Nvidia GPUs. We demonstrate that gpu_tracker can seamlessly
track computational resource usage with minimal overhead, both within desktop
and HPC execution environments.
</summary>
    <author>
      <name>Erik D. Huckvale</name>
    </author>
    <author>
      <name>Hunter N. B. Moseley</name>
    </author>
    <link href="http://arxiv.org/abs/2404.01473v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.01473v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.14926v2</id>
    <updated>2024-04-29T11:15:50Z</updated>
    <published>2024-04-23T11:11:20Z</published>
    <title>Towards self-optimization of publish/subscribe IoT systems using
  continuous performance monitoring</title>
    <summary>  Today, more and more embedded devices are being connected through a network,
generally Internet, offering users different services. This concept refers to
Internet of Things (IoT), bringing information and control capabilities in many
fields like medicine, smart homes, home security, etc. Main drawbacks of IoT
environment are its dependency on Internet connectivity and need continuous
devices power. These dependencies may affect system performances, namely
request processing response times. In this context, we propose in this paper a
continuous performance monitoring methodology, applied on IoT systems based on
Publish/subscribe communication model. Our approach assesses performances using
Stochastic Petri net modeling, and self-optimizes whenever poor performances
are detected. Our approach relies on a Stochastic Petri nets modelling and
analysis to assess performances. We target improving performances, in
particular response times, by online modification of influencing factors.
</summary>
    <author>
      <name>Mohammed Djahafi</name>
    </author>
    <author>
      <name>Nabila Salmi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/csit.2024.140502</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/csit.2024.140502" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures. Conference: 5th International Conference on
  Networks, Blockchain and Internet of Things (NBIoT 2024). Published by
  Computer Science Conference Proceedings in Computer Science &amp; Information
  Technology (CS &amp; IT)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Science &amp; Information Technology (CS &amp; IT) 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2404.14926v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.14926v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.15752v2</id>
    <updated>2024-07-27T11:48:34Z</updated>
    <published>2024-04-24T09:14:17Z</published>
    <title>Performance Evaluation of CMOS Annealing with Support Vector Machine</title>
    <summary>  In this paper, support vector machine (SVM) performance was assessed
utilizing a quantum-inspired complementary metal-oxide semiconductor (CMOS)
annealer. The primary focus during performance evaluation was the accuracy rate
in binary classification problems. A comparative analysis was conducted between
SVM running on a CPU (classical computation) and executed on a quantum-inspired
annealer. The performance outcome was evaluated using a CMOS annealing machine,
thereby obtaining an accuracy rate of 93.7% for linearly separable problems,
92.7% for non-linearly separable problem 1, and 97.6% for non-linearly
separable problem 2. These results reveal that a CMOS annealing machine can
achieve an accuracy rate that closely rivals that of classical computation.
</summary>
    <author>
      <name>Ryoga Fukuhara</name>
    </author>
    <author>
      <name>Makoto Morishita</name>
    </author>
    <author>
      <name>Takahiro Katagiri</name>
    </author>
    <author>
      <name>Masatoshi Kawai</name>
    </author>
    <author>
      <name>Toru Nagai</name>
    </author>
    <author>
      <name>Tetsuya Hoshino</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MCSoC64144.2024.00094</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MCSoC64144.2024.00094" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixed some errors</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2024 IEEE 17th International Symposium on Embedded
  Multicore/Many-core Systems-on-Chip (MCSoC)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2404.15752v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.15752v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.04102v4</id>
    <updated>2024-10-01T16:23:26Z</updated>
    <published>2024-05-07T08:16:00Z</published>
    <title>Analysis of Markovian Arrivals and Service with Applications to
  Intermittent Overload</title>
    <summary>  In many important real-world queueing settings, arrival and service rates
fluctuate over time. We consider the MAMS system, where the arrival and service
rates each vary according to an arbitrary finite-state Markov chain, allowing
intermittent overload to be modeled. This model has been extensively studied,
and we derive results matching those found in the literature via a somewhat
novel framework.
  We derive a characterization of mean queue length in the MAMS system, with
explicit bounds for all arrival and service chains at all loads, using our new
framework. Our bounds are tight in heavy traffic. We prove even stronger bounds
for the important special case of two-level arrivals with intermittent
overload.
  Our framework is based around the concepts of relative arrivals and relative
completions, which have previously been used in studying the MAMS system, under
different names. These quantities allow us to tractably capture the transient
correlational effect of the arrival and service processes on the mean queue
length.
</summary>
    <author>
      <name>Isaac Grosof</name>
    </author>
    <author>
      <name>Yige Hong</name>
    </author>
    <author>
      <name>Mor Harchol-Balter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report: not intended for publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.04102v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.04102v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.11927v1</id>
    <updated>2024-05-20T10:06:19Z</updated>
    <published>2024-05-20T10:06:19Z</published>
    <title>Response time in a pair of processor sharing queues with
  Join-the-Shortest-Queue scheduling</title>
    <summary>  Join-the-Shortest-Queue (JSQ) is the scheduling policy of choice for many
network providers, cloud servers and traffic management systems, where
individual queues are served under processor sharing (PS) queueing discipline.
A numerical solution for the response time distribution in two parallel PS
queues with JSQ scheduling is derived for the first time. Using the generating
function method, two partial differential equations (PDEs) are obtained
corresponding to conditional response times, where the conditioning is on a
particular traced task joining the first or the second queue. These PDEs are
functional equations that contain partial generating functions and their
partial derivatives, and therefore cannot be solved by commonly used
techniques. We are able to solve these PDEs numerically with good accuracy and
perform the deconditioning with respect to the queue-length probabilities by
evaluating a certain complex integral. Numerical results for the density and
the first four moments compare well against regenerative simulation with
500,000 regeneration cycles.
</summary>
    <author>
      <name>Julianna Bor</name>
    </author>
    <author>
      <name>Peter G Harrison</name>
    </author>
    <link href="http://arxiv.org/abs/2405.11927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.11927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.14209v3</id>
    <updated>2025-02-16T05:12:51Z</updated>
    <published>2024-05-23T06:06:32Z</published>
    <title>Exploring and Evaluating Real-world CXL: Use Cases and System Adoption</title>
    <summary>  Compute eXpress Link (CXL) is emerging as a promising memory interface
technology. However, its performance characteristics remain largely unclear due
to the limited availability of production hardware. Key questions include: What
are the use cases for the CXL memory? What are the impacts of the CXL memory on
application performance? How to use the CXL memory in combination with existing
memory components? In this work, we study the performance of three genuine CXL
memory-expansion cards from different vendors. We characterize the basic
performance of the CXL memory, study how HPC applications and large language
models (LLM) can benefit from the CXL memory, and study the interplay between
memory tiering and page interleaving. We also propose a novel data object-level
interleaving policy to match the interleaving policy with memory access
patterns. Our findings reveal the challenges and opportunities of using the CXL
memory.
</summary>
    <author>
      <name>Xi Wang</name>
    </author>
    <author>
      <name>Jie Liu</name>
    </author>
    <author>
      <name>Jianbo Wu</name>
    </author>
    <author>
      <name>Shuangyan Yang</name>
    </author>
    <author>
      <name>Jie Ren</name>
    </author>
    <author>
      <name>Bhanu Shankar</name>
    </author>
    <author>
      <name>Dong Li</name>
    </author>
    <link href="http://arxiv.org/abs/2405.14209v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.14209v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.15645v1</id>
    <updated>2024-05-24T15:34:25Z</updated>
    <published>2024-05-24T15:34:25Z</published>
    <title>An Online Probabilistic Distributed Tracing System</title>
    <summary>  Distributed tracing has become a fundamental tool for diagnosing performance
issues in the cloud by recording causally ordered, end-to-end workflows of
request executions. However, tracing in production workloads can introduce
significant overheads due to the extensive instrumentation needed for
identifying performance variations. This paper addresses the trade-off between
the cost of tracing and the utility of the "spans" within that trace through
Astraea, an online probabilistic distributed tracing system. Astraea is based
on our technique that combines online Bayesian learning and multi-armed bandit
frameworks. This formulation enables Astraea to effectively steer tracing
towards the useful instrumentation needed for accurate performance diagnosis.
Astraea localizes performance variations using only 10-28% of available
instrumentation, markedly reducing tracing overhead, storage, compute costs,
and trace analysis time.
</summary>
    <author>
      <name>M. Toslali</name>
    </author>
    <author>
      <name>S. Qasim</name>
    </author>
    <author>
      <name>S. Parthasarathy</name>
    </author>
    <author>
      <name>F. A. Oliveira</name>
    </author>
    <author>
      <name>H. Huang</name>
    </author>
    <author>
      <name>G. Stringhini</name>
    </author>
    <author>
      <name>Z. Liu</name>
    </author>
    <author>
      <name>A. K. Coskun</name>
    </author>
    <link href="http://arxiv.org/abs/2405.15645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.15645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.16502v1</id>
    <updated>2024-05-26T09:40:01Z</updated>
    <published>2024-05-26T09:40:01Z</published>
    <title>AmBC-NOMA-Aided Short-Packet Communication for High Mobility V2X
  Transmissions</title>
    <summary>  In this paper, we investigate the performance of ambient backscatter
communication non-orthogonal multiple access (AmBC-NOMA)-assisted short packet
communication for high-mobility vehicle-to-everything transmissions. In the
proposed system, a roadside unit (RSU) transmits a superimposed signal to a
typical NOMA user pair. Simultaneously, the backscatter device (BD) transmits
its own signal towards the user pair by reflecting and modulating the RSU's
superimposed signals. Due to vehicles' mobility, we consider realistic
assumptions of time-selective fading and channel estimation errors. Theoretical
expressions for the average block error rates (BLERs) of both users are
derived. Furthermore, analysis and insights on transmit signal-to-noise ratio,
vehicles' mobility, imperfect channel estimation, the reflection efficiency at
the BD, and blocklength are provided. Numerical results validate the
theoretical findings and reveal that the AmBC-NOMA system outperforms its
orthogonal multiple access counterpart in terms of BLER performance.
</summary>
    <author>
      <name>Xinyue Pei</name>
    </author>
    <author>
      <name>Xingwei Wang</name>
    </author>
    <author>
      <name>Yingyang Chen</name>
    </author>
    <author>
      <name>Tingrui Pei</name>
    </author>
    <author>
      <name>Miaowen Wen</name>
    </author>
    <link href="http://arxiv.org/abs/2405.16502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.16502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.17650v1</id>
    <updated>2024-05-27T20:48:33Z</updated>
    <published>2024-05-27T20:48:33Z</published>
    <title>An Analysis of Performance Bottlenecks in MRI Pre-Processing</title>
    <summary>  Magnetic Resonance Image (MRI) pre-processing is a critical step for
neuroimaging analysis. However, the computational cost of MRI pre-processing
pipelines is a major bottleneck for large cohort studies and some clinical
applications. While High-Performance Computing (HPC) and, more recently, Deep
Learning have been adopted to accelerate the computations, these techniques
require costly hardware and are not accessible to all researchers. Therefore,
it is important to understand the performance bottlenecks of MRI pre-processing
pipelines to improve their performance. Using Intel VTune profiler, we
characterized the bottlenecks of several commonly used MRI-preprocessing
pipelines from the ANTs, FSL, and FreeSurfer toolboxes. We found that few
functions contributed to most of the CPU time, and that linear interpolation
was the largest contributor. Data access was also a substantial bottleneck. We
identified a bug in the ITK library that impacts the performance of ANTs
pipeline in single-precision and a potential issue with the OpenMP scaling in
FreeSurfer recon-all. Our results provide a reference for future efforts to
optimize MRI pre-processing pipelines.
</summary>
    <author>
      <name>Mathieu Dugré</name>
    </author>
    <author>
      <name>Yohan Chatelain</name>
    </author>
    <author>
      <name>Tristan Glatard</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/gigascience/giae098</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/gigascience/giae098" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures, 3 tables, 1 listing</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.17650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.17650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.01133v4</id>
    <updated>2024-06-14T09:12:21Z</updated>
    <published>2024-06-03T09:25:14Z</published>
    <title>Impact of Generative AI (Large Language Models) on the PRA model
  construction and maintenance, observations</title>
    <summary>  The rapid development of Large Language Models (LLMs) and Generative
Pre-Trained Transformers(GPTs) in the field of Generative Artificial
Intelligence (AI) can significantly impact task automation in themodern
economy. We anticipate that the PRA field will inevitably be affected by this
technology. Thus, themain goal of this paper is to engage the risk assessment
community into a discussion of benefits anddrawbacks of this technology for
PRA. We make a preliminary analysis of possible application of LLM
inProbabilistic Risk Assessment (PRA) modeling context referring to the ongoing
experience in softwareengineering field. We explore potential application
scenarios and the necessary conditions for controlledLLM usage in PRA modeling
(whether static or dynamic). Additionally, we consider the potential impact
ofthis technology on PRA modeling tools.
</summary>
    <author>
      <name>Valentin Rychkov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">EDF R\&amp;D</arxiv:affiliation>
    </author>
    <author>
      <name>Claudia Picoco</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">EDF R\&amp;D</arxiv:affiliation>
    </author>
    <author>
      <name>Emilie Caleca</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">EDF R\&amp;D</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2406.01133v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.01133v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.12797v1</id>
    <updated>2024-06-20T21:36:00Z</updated>
    <published>2024-06-20T21:36:00Z</published>
    <title>CEBench: A Benchmarking Toolkit for the Cost-Effectiveness of LLM
  Pipelines</title>
    <summary>  Online Large Language Model (LLM) services such as ChatGPT and Claude 3 have
transformed business operations and academic research by effortlessly enabling
new opportunities. However, due to data-sharing restrictions, sectors such as
healthcare and finance prefer to deploy local LLM applications using costly
hardware resources. This scenario requires a balance between the effectiveness
advantages of LLMs and significant financial burdens. Additionally, the rapid
evolution of models increases the frequency and redundancy of benchmarking
efforts. Existing benchmarking toolkits, which typically focus on
effectiveness, often overlook economic considerations, making their findings
less applicable to practical scenarios. To address these challenges, we
introduce CEBench, an open-source toolkit specifically designed for
multi-objective benchmarking that focuses on the critical trade-offs between
expenditure and effectiveness required for LLM deployments. CEBench allows for
easy modifications through configuration files, enabling stakeholders to
effectively assess and optimize these trade-offs. This strategic capability
supports crucial decision-making processes aimed at maximizing effectiveness
while minimizing cost impacts. By streamlining the evaluation process and
emphasizing cost-effectiveness, CEBench seeks to facilitate the development of
economically viable AI solutions across various industries and research fields.
The code and demonstration are available in
\url{https://github.com/amademicnoboday12/CEBench}.
</summary>
    <author>
      <name>Wenbo Sun</name>
    </author>
    <author>
      <name>Jiaqi Wang</name>
    </author>
    <author>
      <name>Qiming Guo</name>
    </author>
    <author>
      <name>Ziyu Li</name>
    </author>
    <author>
      <name>Wenlu Wang</name>
    </author>
    <author>
      <name>Rihan Hai</name>
    </author>
    <link href="http://arxiv.org/abs/2407.12797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.12797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.13096v1</id>
    <updated>2024-07-18T02:08:48Z</updated>
    <published>2024-07-18T02:08:48Z</published>
    <title>DSO: A GPU Energy Efficiency Optimizer by Fusing Dynamic and Static
  Information</title>
    <summary>  Increased reliance on graphics processing units (GPUs) for high-intensity
computing tasks raises challenges regarding energy consumption. To address this
issue, dynamic voltage and frequency scaling (DVFS) has emerged as a promising
technique for conserving energy while maintaining the quality of service (QoS)
of GPU applications. However, existing solutions using DVFS are hindered by
inefficiency or inaccuracy as they depend either on dynamic or static
information respectively, which prevents them from being adopted to practical
power management schemes. To this end, we propose a novel energy efficiency
optimizer, called DSO, to explore a light weight solution that leverages both
dynamic and static information to model and optimize the GPU energy efficiency.
DSO firstly proposes a novel theoretical energy efficiency model which reflects
the DVFS roofline phenomenon and considers the tradeoff between performance and
energy. Then it applies machine learning techniques to predict the parameters
of the above model with both GPU kernel runtime metrics and static code
features. Experiments on modern DVFS-enabled GPUs indicate that DSO can enhance
energy efficiency by 19% whilst maintaining performance within a 5% loss
margin.
</summary>
    <author>
      <name>Qiang Wang</name>
    </author>
    <author>
      <name>Laiyi Li</name>
    </author>
    <author>
      <name>Weile Luo</name>
    </author>
    <author>
      <name>Yijia Zhang</name>
    </author>
    <author>
      <name>Bingqiang Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2407.13096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.13096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.01805v1</id>
    <updated>2024-08-03T15:12:54Z</updated>
    <published>2024-08-03T15:12:54Z</published>
    <title>Billion-files File Systems (BfFS): A Comparison</title>
    <summary>  As the volume of data being produced is increasing at an exponential rate
that needs to be processed quickly, it is reasonable that the data needs to be
available very close to the compute devices to reduce transfer latency. Due to
this need, local filesystems are getting close attention to understand their
inner workings, performance, and more importantly their limitations. This study
analyzes few popular Linux filesystems: EXT4, XFS, BtrFS, ZFS, and F2FS by
creating, storing, and then reading back one billion files from the local
filesystem. The study also captured and analyzed read/write throughput, storage
blocks usage, disk space utilization and overheads, and other metrics useful
for system designers and integrators. Furthermore, the study explored other
side effects such as filesystem performance degradation during and after these
large numbers of files and folders are created.
</summary>
    <author>
      <name>Sohail Shaikh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper is 9 pages. Source code is open and uploaded to Git, along with
  an Excel spreadsheet used for analysis</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.01805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.01805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.05251v1</id>
    <updated>2024-08-08T20:20:36Z</updated>
    <published>2024-08-08T20:20:36Z</published>
    <title>Columbo: Low Level End-to-End System Traces through Modular Full-System
  Simulation</title>
    <summary>  Fully understanding performance is a growing challenge when building
next-generation cloud systems. Often these systems build on next-generation
hardware, and evaluation in realistic physical testbeds is out of reach. Even
when physical testbeds are available, visibility into essential system aspects
is a challenge in modern systems where system performance depends on often
sub-$\mu s$ interactions between HW and SW components. Existing tools such as
performance counters, logging, and distributed tracing provide aggregate or
sampled information, but remain insufficient for understanding individual
requests in-depth. In this paper, we explore a fundamentally different approach
to enable in-depth understanding of cloud system behavior at the software and
hardware level, with (almost) arbitrarily fine-grained visibility. Our proposal
is to run cloud systems in detailed full-system simulations, configure the
simulators to collect detailed events without affecting the system, and finally
assemble these events into end-to-end system traces that can be analyzed by
existing distributed tracing tools.
</summary>
    <author>
      <name>Jakob Görgen</name>
    </author>
    <author>
      <name>Vaastav Anand</name>
    </author>
    <author>
      <name>Hejing Li</name>
    </author>
    <author>
      <name>Jialin Li</name>
    </author>
    <author>
      <name>Antoine Kaufmann</name>
    </author>
    <link href="http://arxiv.org/abs/2408.05251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.05251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.06579v1</id>
    <updated>2024-08-13T02:37:06Z</updated>
    <published>2024-08-13T02:37:06Z</published>
    <title>Understanding Power Consumption Metric on Heterogeneous Memory Systems</title>
    <summary>  Contemporary memory systems contain a variety of memory types, each
possessing distinct characteristics. This trend empowers applications to opt
for memory types aligning with developer's desired behavior. As a result,
developers gain flexibility to tailor their applications to specific needs,
factoring in attributes like latency, bandwidth, and power consumption. Our
research centers on the aspect of power consumption within memory systems. We
introduce an approach that equips developers with comprehensive insights into
the power consumption of individual memory types. Additionally, we propose an
ordered hierarchy of memory types. Through this methodology, developers can
make informed decisions for efficient memory usage aligned with their unique
requirements.
</summary>
    <author>
      <name>Andrès Rubio Proaño</name>
    </author>
    <author>
      <name>Kento Sato</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICPADS60453.2023.00408</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICPADS60453.2023.00408" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICPADS 2023</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2023 IEEE 29th International Conference on Parallel and
  Distributed Systems (ICPADS), Ocean Flower Island, China, 2023, pp. 2859-2862</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2408.06579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.06579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.07378v2</id>
    <updated>2024-09-24T09:26:45Z</updated>
    <published>2024-08-14T08:54:29Z</published>
    <title>Inspection of I/O Operations from System Call Traces using
  Directly-Follows-Graph</title>
    <summary>  We aim to identify the differences in Input/Output(I/O) behavior between
multiple user programs through the inspection of system calls (i.e., requests
made to the operating system). A typical program issues a large number of I/O
requests to the operating system, thereby making the process of inspection
challenging. In this paper, we address this challenge by presenting a
methodology to synthesize I/O system call traces into a specific type of
directed graph, known as the Directly-Follows-Graph (DFG). Based on the DFG, we
present a technique to compare the traces from multiple programs or different
configurations of the same program, such that it is possible to identify the
differences in the I/O behavior. We apply our methodology to the IOR benchmark,
and compare the contentions for file accesses when the benchmark is run with
different options for file output and software interface.
</summary>
    <author>
      <name>Aravind Sankaran</name>
    </author>
    <author>
      <name>Ilya Zhukov</name>
    </author>
    <author>
      <name>Wolfgang Frings</name>
    </author>
    <author>
      <name>Paolo Bientinesi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SCW63240.2024.00196</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SCW63240.2024.00196" rel="related"/>
    <link href="http://arxiv.org/abs/2408.07378v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.07378v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.11173v1</id>
    <updated>2024-08-20T20:11:33Z</updated>
    <published>2024-08-20T20:11:33Z</published>
    <title>Delegation with Trust&lt;T&gt;: A Scalable, Type- and Memory-Safe Alternative
  to Locks</title>
    <summary>  We present Trust&lt;T&gt;, a general, type- and memory-safe alternative to locking
in concurrent programs. Instead of synchronizing multi-threaded access to an
object of type T with a lock, the programmer may place the object in a
Trust&lt;T&gt;. The object is then no longer directly accessible. Instead a
designated thread, the object's trustee, is responsible for applying any
requested operations to the object, as requested via the Trust&lt;T&gt; API. Locking
is often said to offer a limited throughput per lock. Trust&lt;T&gt; is based on
delegation, a message-passing technique which does not suffer this per-lock
limitation. Instead, per-object throughput is limited by the capacity of the
object's trustee, which is typically considerably higher. Our evaluation shows
Trust&lt;T&gt; consistently and considerably outperforming locking where lock
contention exists, with up to 22x higher throughput in microbenchmarks, and
5-9x for a home grown key-value store, as well as memcached, in situations with
high lock contention. Moreover, Trust&lt;T&gt; is competitive with locks even in the
absence of lock contention.
</summary>
    <author>
      <name>Noaman Ahmad</name>
    </author>
    <author>
      <name>Ben Baenen</name>
    </author>
    <author>
      <name>Chen Chen</name>
    </author>
    <author>
      <name>Jakob Eriksson</name>
    </author>
    <link href="http://arxiv.org/abs/2408.11173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.11173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.16551v1</id>
    <updated>2024-08-29T14:14:37Z</updated>
    <published>2024-08-29T14:14:37Z</published>
    <title>TINA: Acceleration of Non-NN Signal Processing Algorithms Using NN
  Accelerators</title>
    <summary>  This paper introduces TINA, a novel framework for implementing non Neural
Network (NN) signal processing algorithms on NN accelerators such as GPUs, TPUs
or FPGAs. The key to this approach is the concept of mapping mathematical and
logic functions as a series of convolutional and fully connected layers. By
mapping functions into such a small substack of NN layers, it becomes possible
to execute non-NN algorithms on NN hardware (HW) accelerators efficiently, as
well as to ensure the portability of TINA implementations to any platform that
supports such NN accelerators. Results show that TINA is highly competitive
compared to alternative frameworks, specifically for complex functions with
iterations. For a Polyphase Filter Bank use case TINA shows GPU speedups of up
to 80x vs a CPU baseline with NumPy compared to 8x speedup achieved by
alternative frameworks. The framework is open source and publicly available at
https://github.com/ChristiaanBoe/TINA.
</summary>
    <author>
      <name>Christiaan Boerkamp</name>
    </author>
    <author>
      <name>Steven van der Vlugt</name>
    </author>
    <author>
      <name>Zaid Al-Ars</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint for MLSP 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.16551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.16551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.00854v3</id>
    <updated>2024-10-04T19:36:01Z</updated>
    <published>2024-09-01T22:06:58Z</published>
    <title>Scaler: Efficient and Effective Cross Flow Analysis</title>
    <summary>  Performance analysis is challenging as different components (e.g.,different
libraries, and applications) of a complex system can interact with each other.
However, few existing tools focus on understanding such interactions. To bridge
this gap, we propose a novel analysis method "Cross Flow Analysis (XFA)" that
monitors the interactions/flows across these components. We also built the
Scaler profiler that provides a holistic view of the time spent on each
component (e.g., library or application) and every API inside each component.
This paper proposes multiple new techniques, such as Universal Shadow Table,
and Relation-Aware Data Folding. These techniques enable Scaler to achieve low
runtime overhead, low memory overhead, and high profiling accuracy. Based on
our extensive experimental results, Scaler detects multiple unknown performance
issues inside widely-used applications, and therefore will be a useful
complement to existing work.
  The reproduction package including the source code, benchmarks, and
evaluation scripts, can be found at https://doi.org/10.5281/zenodo.13336658.
</summary>
    <author>
      <name> Steven</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Jiaxun</arxiv:affiliation>
    </author>
    <author>
      <name> Tang</name>
    </author>
    <author>
      <name>Mingcan Xiang</name>
    </author>
    <author>
      <name>Yang Wang</name>
    </author>
    <author>
      <name>Bo Wu</name>
    </author>
    <author>
      <name>Jianjun Chen</name>
    </author>
    <author>
      <name>Tongping Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3691620.3695473</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3691620.3695473" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper has been accepted by ASE'24
  https://conf.researchr.org/details/ase-2024/ase-2024-research/73/Scaler-Efficient-and-Effective-Cross-Flow-Analysis</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.00854v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.00854v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.08075v1</id>
    <updated>2024-09-12T14:28:54Z</updated>
    <published>2024-09-12T14:28:54Z</published>
    <title>Computational Algorithms for the Product Form Solution of Closed Queuing
  Networks with Finite Buffers and Skip-Over Policy</title>
    <summary>  Closed queuing networks with finite capacity buffers and skip-over policies
are fundamental models in the performance evaluation of computer and
communication systems. This technical report presents the details of
computational algorithms to derive the key performance metrics for such
networks. The primary focus is on the efficient computation of the
normalization constant, which is critical for determining the steady-state
probabilities of the network states under investigation. A convolution
algorithm is proposed, which paves the way for the computation of key
performance indices, such as queue length distribution and throughput,
accommodating the intricacies introduced by finite capacity constraints and
skip-over mechanisms. Finally, an extension of the traditional Mean Value
Analysis algorithm addressing numerical stability is provided. The approaches
discussed here allow make the investigation of large-scale networks feasible
and enable the development of robust implementations of these techniques for
practical use.
</summary>
    <author>
      <name>Gianfranco Balbo</name>
    </author>
    <author>
      <name>Andrea Marin</name>
    </author>
    <author>
      <name>Diletta Olliaro</name>
    </author>
    <author>
      <name>Matteo Sereno</name>
    </author>
    <link href="http://arxiv.org/abs/2409.08075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.08075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.08108v1</id>
    <updated>2024-09-12T15:00:58Z</updated>
    <published>2024-09-12T15:00:58Z</published>
    <title>Microarchitectural comparison and in-core modeling of state-of-the-art
  CPUs: Grace, Sapphire Rapids, and Genoa</title>
    <summary>  With Nvidia's release of the Grace Superchip, all three big semiconductor
companies in HPC (AMD, Intel, Nvidia) are currently competing in the race for
the best CPU. In this work we analyze the performance of these state-of-the-art
CPUs and create an accurate in-core performance model for their
microarchitectures Zen 4, Golden Cove, and Neoverse V2, extending the Open
Source Architecture Code Analyzer (OSACA) tool and comparing it with LLVM-MCA.
Starting from the peculiarities and up- and downsides of a single core, we
extend our comparison by a variety of microbenchmarks and the capabilities of a
full node. The "write-allocate (WA) evasion" feature, which can automatically
reduce the memory traffic caused by write misses, receives special attention;
we show that the Grace Superchip has a next-to-optimal implementation of WA
evasion, and that the only way to avoid write allocates on Zen 4 is the
explicit use of non-temporal stores.
</summary>
    <author>
      <name>Jan Laukemann</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.08108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.08108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.13639v1</id>
    <updated>2024-09-20T16:45:51Z</updated>
    <published>2024-09-20T16:45:51Z</published>
    <title>RAVE: RISC-V Analyzer of Vector Executions, a QEMU tracing plugin</title>
    <summary>  Simulators are crucial during the development of a chip, like the RISC-V
accelerator designed in the European Processor Initiative project. In this
paper, we showcase the limitations of the current simulation solutions in the
project and propose using QEMU with RAVE, a plugin we implement and describe in
this document. This methodology can rapidly simulate and analyze applications
running on the v1.0 and v0.7.1 RISC-V V-extension. Our plugin reports the
vector and scalar instructions alongside useful information such as the
vector-length being used, the single-element-width, and the register usage,
among other vectorization metrics. We provide an API used from the simulated
Application to control the RAVE plugin and the capability to generate
vectorization traces that can be analyzed using Paraver. Finally, we
demonstrate the efficiency of our solution between different evaluated machines
and against other simulation methods used in the European Processor Accelerator
(EPAC) project.
</summary>
    <author>
      <name>Pablo Vizcaino</name>
    </author>
    <author>
      <name>Filippo Mantovani</name>
    </author>
    <author>
      <name>Jesus Labarta</name>
    </author>
    <author>
      <name>Roger Ferrer</name>
    </author>
    <link href="http://arxiv.org/abs/2409.13639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.13639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.14887v3</id>
    <updated>2025-01-07T09:55:57Z</updated>
    <published>2024-09-23T10:35:57Z</published>
    <title>Deploying Open-Source Large Language Models: A performance Analysis</title>
    <summary>  Since the release of ChatGPT in November 2022, large language models (LLMs)
have seen considerable success, including in the open-source community, with
many open-weight models available. However, the requirements to deploy such a
service are often unknown and difficult to evaluate in advance. To facilitate
this process, we conducted numerous tests at the Centre Inria de l'Universit\'e
de Bordeaux. In this article, we propose a comparison of the performance of
several models of different sizes (mainly Mistral and LLaMa) depending on the
available GPUs, using vLLM, a Python library designed to optimize the inference
of these models. Our results provide valuable information for private and
public groups wishing to deploy LLMs, allowing them to evaluate the performance
of different models based on their available hardware. This study thus
contributes to facilitating the adoption and use of these large language models
in various application domains.
</summary>
    <author>
      <name>Yannis Bendi-Ouis</name>
    </author>
    <author>
      <name>Dan Dutartre</name>
    </author>
    <author>
      <name>Xavier Hinaut</name>
    </author>
    <link href="http://arxiv.org/abs/2409.14887v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.14887v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.15468v1</id>
    <updated>2024-09-23T18:48:12Z</updated>
    <published>2024-09-23T18:48:12Z</published>
    <title>FRSZ2 for In-Register Block Compression Inside GMRES on GPUs</title>
    <summary>  The performance of the GMRES iterative solver on GPUs is limited by the GPU
main memory bandwidth. Compressed Basis GMRES outperforms GMRES by storing the
Krylov basis in low precision, thereby reducing the memory access. An open
question is whether compression techniques that are more sophisticated than
casting to low precision can enable large runtime savings while preserving the
accuracy of the final results. This paper presents the lightweight in-register
compressor FRSZ2 that can decompress at the bandwidth speed of a modern NVIDIA
H100 GPU. In an experimental evaluation, we demonstrate using FRSZ2 instead of
low precision for compression of the Krylov basis can bring larger runtime
benefits without impacting final accuracy.
</summary>
    <author>
      <name>Thomas Grützmacher</name>
    </author>
    <author>
      <name>Robert Underwood</name>
    </author>
    <author>
      <name>Sheng Di</name>
    </author>
    <author>
      <name>Franck Cappello</name>
    </author>
    <author>
      <name>Hartwig Anzt</name>
    </author>
    <link href="http://arxiv.org/abs/2409.15468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.15468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.07737v1</id>
    <updated>2024-10-10T09:15:14Z</updated>
    <published>2024-10-10T09:15:14Z</published>
    <title>Plug-and-Play Performance Estimation for LLM Services without Relying on
  Labeled Data</title>
    <summary>  Large Language Model (LLM) services exhibit impressive capability on
unlearned tasks leveraging only a few examples by in-context learning (ICL).
However, the success of ICL varies depending on the task and context, leading
to heterogeneous service quality. Directly estimating the performance of LLM
services at each invocation can be laborious, especially requiring abundant
labeled data or internal information within the LLM. This paper introduces a
novel method to estimate the performance of LLM services across different tasks
and contexts, which can be "plug-and-play" utilizing only a few unlabeled
samples like ICL. Our findings suggest that the negative log-likelihood and
perplexity derived from LLM service invocation can function as effective and
significant features. Based on these features, we utilize four distinct
meta-models to estimate the performance of LLM services. Our proposed method is
compared against unlabeled estimation baselines across multiple LLM services
and tasks. And it is experimentally applied to two scenarios, demonstrating its
effectiveness in the selection and further optimization of LLM services.
</summary>
    <author>
      <name>Can Wang</name>
    </author>
    <author>
      <name>Dianbo Sui</name>
    </author>
    <author>
      <name>Hongliang Sun</name>
    </author>
    <author>
      <name>Hao Ding</name>
    </author>
    <author>
      <name>Bolin Zhang</name>
    </author>
    <author>
      <name>Zhiying Tu</name>
    </author>
    <link href="http://arxiv.org/abs/2410.07737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.07737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.11260v1</id>
    <updated>2024-10-15T04:35:49Z</updated>
    <published>2024-10-15T04:35:49Z</published>
    <title>A Zoned Storage Optimized Flash Cache on ZNS SSDs</title>
    <summary>  Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block
interface penalties of flash-based SSDs. It is a good opportunity for flash
cache to address cache throughput and write amplification (WA) issues by fully
controlling data allocation and garbage collection via zone-based interfaces.
However, there are several critical challenges that need to be addressed
including zone-interface compatibility, data management of large zone size, and
a better tradeoff between throughput, cache hit ratio, and WA.
  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache
on ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs
with low mapping and operational overhead, and 2) a novel zCache Engine with
cross-layer optimizations to resolve the throughput regression and WA issues of
garbage collection, which consists of delayed data eviction with virtual
over-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,
and a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that
Z-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and
almost no WA compared to CacheLib with compatible regular SSDs, demonstrating
benefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X
throughput and 92% WA reduction compared with F2FS-based scheme.
</summary>
    <author>
      <name>Chongzhuo Yang</name>
    </author>
    <author>
      <name>Chang Guo</name>
    </author>
    <author>
      <name>Ming Zhao</name>
    </author>
    <author>
      <name>Zhichao Cao</name>
    </author>
    <link href="http://arxiv.org/abs/2410.11260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.11260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.15853v1</id>
    <updated>2024-10-21T10:29:04Z</updated>
    <published>2024-10-21T10:29:04Z</published>
    <title>ADS Performance Revisited</title>
    <summary>  Real-time measurements are important for in-depth control of manufacturing
processes, which, for modern AI methods, need integration with high-level
languages. In our last SSP paper we investigated the performance of a Python
and a Java-JNA based approach to integrate the Beckhoff ADS protocol for
real-time edge communication into an Industry 4.0 platform. There, we have
shown that while Java outperforms Python, both solutions do not meet the
desired goal of 1-20kHz depending on the task. However, we are are still
lacking an explanation for this result as well as an analysis of alternatives.
For the first topic, we show in this paper that 1) exchanging Java-JNA with
Java-JNI in this setting does not further improve the performance 2) a C++
program realizing the same behavior in a more direct integration does not
perform better and 3) profiling shows that the majority of the execution is
spend in ADS. For the second topic, we show that alternative uses of the ADS
library allow for better performance.
</summary>
    <author>
      <name>Alexander Weber</name>
    </author>
    <author>
      <name>Holger Eichelberger</name>
    </author>
    <author>
      <name>Jobst Hildebrand</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Three pages about ADS integration into Java</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.15853v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.15853v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.21677v1</id>
    <updated>2024-10-29T02:55:55Z</updated>
    <published>2024-10-29T02:55:55Z</published>
    <title>Two Criteria for Performance Analysis of Optimization Algorithms</title>
    <summary>  Performance analysis is crucial in optimization research, especially when
addressing black-box problems through nature-inspired algorithms. Current
practices often rely heavily on statistical methods, which can lead to various
logical paradoxes. To address this challenge, this paper introduces two
criteria to ensure that performance analysis is unaffected by irrelevant
factors. The first is the isomorphism criterion, which asserts that performance
evaluation should remain unaffected by the modeling approach. The second is the
IIA criterion,stating that comparisons between two algorithms should not be
influenced by irrelevant third-party algorithms. Additionally, we conduct a
comprehensive examination of the underlying causes of these paradoxes, identify
conditions for checking the criteria, and propose ideas to tackle these issues.
The criteria presented offer a framework for researchers to critically assess
the performance metrics or ranking methods, ultimately aiming to enhance the
rigor of evaluation metrics and ranking methods.
</summary>
    <author>
      <name>Yunpeng Jing</name>
    </author>
    <author>
      <name>HaiLin Liu</name>
    </author>
    <author>
      <name>Qunfeng Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2410.21677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.21677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.23537v1</id>
    <updated>2024-10-31T00:58:11Z</updated>
    <published>2024-10-31T00:58:11Z</published>
    <title>ALISE: Accelerating Large Language Model Serving with Speculative
  Scheduling</title>
    <summary>  Large Language Models (LLMs) represent a revolutionary advancement in the
contemporary landscape of artificial general intelligence (AGI). As exemplified
by ChatGPT, LLM-based applications necessitate minimal response latency and
maximal throughput for inference serving. However, due to the unpredictability
of LLM execution, the first-come-first-serve (FCFS) scheduling policy employed
by current LLM serving systems suffers from head-of-line (HoL) blocking issues
and long job response times.
  In this paper, we propose a new efficient LLM inference serving framework,
named ALISE. The key design paradigm of ALISE is to leverage a novel
speculative scheduler by estimating the execution time for each job and
exploiting such prior knowledge to assign appropriate job priority orders, thus
minimizing potential queuing delays for heterogeneous workloads. Furthermore,
to mitigate the memory overhead of the intermediate key-value (KV) cache, we
employ a priority-based adaptive memory management protocol and
quantization-based compression techniques. Evaluations demonstrate that in
comparison to the state-of-the-art solution vLLM, ALISE improves the throughput
of inference serving by up to 1.8x and 2.1x under the same latency constraint
on the Alpaca and ShareGPT datasets, respectively.
</summary>
    <author>
      <name>Youpeng Zhao</name>
    </author>
    <author>
      <name>Jun Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCAD 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.23537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.23537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.23762v1</id>
    <updated>2024-10-31T09:27:08Z</updated>
    <published>2024-10-31T09:27:08Z</published>
    <title>Efficient Performance Analysis of Modular Rewritable Petri Nets</title>
    <summary>  Petri Nets (PN) are extensively used as a robust formalism to model
concurrent and distributed systems; however, they encounter difficulties in
accurately modeling adaptive systems. To address this issue, we defined
rewritable PT nets (RwPT) using Maude, a declarative language that ensures
consistent rewriting logic semantics. Recently, we proposed a modular approach
that employs algebraic operators to build extensive RwPT models. This
methodology uses composite node labeling to maintain hierarchical organization
through net rewrites and has been shown to be effective. Once stochastic
parameters are integrated into the formalism, we introduce an automated
procedure to derive a lumped CTMC from the quotient graph generated by a
modular RwPT model. To demonstrate the effectiveness of our method, we present
a fault-tolerant manufacturing system as a case study.
</summary>
    <author>
      <name>Lorenzo Capra</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universitá degli Studi di Milano</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Gribaudo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Politecnico di Milano</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.410.5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.410.5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings FROM 2024, arXiv:2410.23020</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 410, 2024, pp. 69-83</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2410.23762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.23762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.02814v1</id>
    <updated>2024-11-05T05:22:14Z</updated>
    <published>2024-11-05T05:22:14Z</published>
    <title>The Hitchhiker's Guide to Programming and Optimizing CXL-Based
  Heterogeneous Systems</title>
    <summary>  We present a thorough analysis of the use of CXL-based heterogeneous systems.
We built a cluster of server systems that combines different vendor's CPUs and
various types of CXL devices. We further developed a heterogeneous memory
benchmark suite, Heimdall, to profile the performance of such heterogeneous
systems. By leveraging Heimdall, we unveiled the detailed architecture design
in these systems, drew observations on optimizing performance for workloads,
and pointed out directions for future development of CXL-based heterogeneous
systems.
</summary>
    <author>
      <name>Zixuan Wang</name>
    </author>
    <author>
      <name>Suyash Mahar</name>
    </author>
    <author>
      <name>Luyi Li</name>
    </author>
    <author>
      <name>Jangseon Park</name>
    </author>
    <author>
      <name>Jinpyo Kim</name>
    </author>
    <author>
      <name>Theodore Michailidis</name>
    </author>
    <author>
      <name>Yue Pan</name>
    </author>
    <author>
      <name>Tajana Rosing</name>
    </author>
    <author>
      <name>Dean Tullsen</name>
    </author>
    <author>
      <name>Steven Swanson</name>
    </author>
    <author>
      <name>Kyung Chang Ryoo</name>
    </author>
    <author>
      <name>Sungjoo Park</name>
    </author>
    <author>
      <name>Jishen Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/2411.02814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.02814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.05491v1</id>
    <updated>2024-11-08T11:43:40Z</updated>
    <published>2024-11-08T11:43:40Z</published>
    <title>Overhead Measurement Noise in Different Runtime Environments</title>
    <summary>  In order to detect performance changes, measurements are performed with the
same execution environment. In cloud environments, the noise from different
processes running on the same cluster nodes might change measurement results
and thereby make performance changes hard to measure.
  The benchmark MooBench determines the overhead of different observability
tools and is executed continuously. In this study, we compare the suitability
of different execution environments to benchmark the observability overhead
using MooBench. To do so, we compare the execution times and standard deviation
of MooBench in a cloud execution environment to three bare-metal execution
environments. We find that bare metal servers have lower runtime and standard
deviation for multi-threaded MooBench execution. Nevertheless, we see that
performance changes up to 4.41% are detectable by GitHub actions, as long as
only sequential workloads are examined.
</summary>
    <author>
      <name>David Georg Reichelt</name>
    </author>
    <author>
      <name>Reiner Jung</name>
    </author>
    <author>
      <name>André van Hoorn</name>
    </author>
    <link href="http://arxiv.org/abs/2411.05491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.05491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.7; D.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.07447v3</id>
    <updated>2025-04-16T16:58:32Z</updated>
    <published>2024-11-12T00:10:34Z</published>
    <title>Optimizing LLM Inference for Database Systems: Cost-Aware Scheduling for
  Concurrent Requests</title>
    <summary>  LLMs are increasingly used inside database systems and in database
applications for better complexity management and decision-making, where LLM
inferences require significant GPU costs. LLM inference systems, however, are
slow compared to database systems, limiting the expansion of the use of LLMs
inside database systems. This paper first analyzes the LLM inference
performance and focuses on a data management issue in LLM inference. We reveal
that the root of the problem is the lack of an adequate resource cost model and
optimization strategy when executing multiple concurrent inference requests. We
adapt classic database multi-query optimization techniques by introducing cost
models for concurrent inference requests and new scheduling strategies to
optimize the use of memory resources by concurrent requests, thereby
substantially improving performance.
</summary>
    <author>
      <name>Kyoungmin Kim</name>
    </author>
    <author>
      <name>Kijae Hong</name>
    </author>
    <author>
      <name>Caglar Gulcehre</name>
    </author>
    <author>
      <name>Anastasia Ailamaki</name>
    </author>
    <link href="http://arxiv.org/abs/2411.07447v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.07447v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.07622v1</id>
    <updated>2024-11-12T08:09:45Z</updated>
    <published>2024-11-12T08:09:45Z</published>
    <title>A Performance Analysis of BFT Consensus for Blockchains</title>
    <summary>  Distributed ledgers are common in the industry. Some of them can use
blockchains as their underlying infrastructure. A blockchain requires
participants to agree on its contents. This can be achieved via a consensus
protocol, and several BFT (Byzantine Fault Tolerant) protocols have been
proposed for this purpose. How do these protocols differ in performance? And
how is this difference affected by the communication network? Moreover, such a
protocol would need a timer to ensure progress, but how should the timer be
set?
  This paper presents an analytical model to address these and related issues
in the case of crash faults. Specifically, it focuses on two consensus
protocols (Istanbul BFT and HotStuff) and two network topologies (Folded-Clos
and Dragonfly). The model provides closed-form expressions for analyzing how
the timer value and number of participants, faults and switches affect the
consensus time. The formulas and analyses are validated with simulations. The
conclusion offers some tips for analytical modeling of such protocols.
</summary>
    <author>
      <name>J. D. Chan</name>
    </author>
    <author>
      <name>Y. C. Tay</name>
    </author>
    <author>
      <name>Brian R. Z. Yen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.07622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.07622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.07687v1</id>
    <updated>2024-11-12T10:10:40Z</updated>
    <published>2024-11-12T10:10:40Z</published>
    <title>OSCAR-P and aMLLibrary: Profiling and Predicting the Performance of
  FaaS-based Applications in Computing Continua</title>
    <summary>  This paper proposes an automated framework for efficient application
profiling and training of Machine Learning (ML) performance models, composed of
two parts: OSCAR-P and aMLLibrary. OSCAR-P is an auto-profiling tool designed
to automatically test serverless application workflows running on multiple
hardware and node combinations in cloud and edge environments. OSCAR-P obtains
relevant profiling information on the execution time of the individual
application components. These data are later used by aMLLibrary to train
ML-based performance models. This makes it possible to predict the performance
of applications on unseen configurations. We test our framework on clusters
with different architectures (x86 and arm64) and workloads, considering
multi-component use-case applications. This extensive experimental campaign
proves the efficiency of OSCAR-P and aMLLibrary, significantly reducing the
time needed for the application profiling, data collection, and data
processing. The preliminary results obtained on the ML performance models
accuracy show a Mean Absolute Percentage Error lower than 30% in all the
considered scenarios.
</summary>
    <author>
      <name>Roberto Sala</name>
    </author>
    <author>
      <name>Bruno Guindani</name>
    </author>
    <author>
      <name>Enrico Galimberti</name>
    </author>
    <author>
      <name>Federica Filippini</name>
    </author>
    <author>
      <name>Hamta Sedghani</name>
    </author>
    <author>
      <name>Danilo Ardagna</name>
    </author>
    <author>
      <name>Sebastián Risco</name>
    </author>
    <author>
      <name>Germán Moltó</name>
    </author>
    <author>
      <name>Miguel Caballer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jss.2024.112282</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jss.2024.112282" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted on Journal of Systems and Software</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.07687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.07687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.08494v1</id>
    <updated>2024-11-13T10:28:10Z</updated>
    <published>2024-11-13T10:28:10Z</published>
    <title>Achieving Consistent and Comparable CPU Evaluation Outcomes</title>
    <summary>  The SPEC CPU2017 benchmark suite is an industry standard for accessing CPU
performance. It adheres strictly to some workload and system configurations -
arbitrary specificity - while leaving other system configurations undefined -
arbitrary ambiguity. This article reveals: (1) Arbitrary specificity proves not
meaningful, obscuring many scenarios, as evidenced by significant performance
variations, a 74.49x performance difference observed on the same CPU. (2)
Arbitrary ambiguity is unfair as it fails to establish the same configurations
for comparing different CPUs.
  We propose an innovative CPU evaluation methodology. It considers all
workload and system configurations valid and mandates each configuration to be
well-defined to avoid arbitrary specificity and ambiguity. To reduce the
evaluation cost, a sampling approach is proposed to select a subset of the
configurations. To expose CPU performance under different scenarios, it treats
all outcomes under each configuration as equally important. Finally, it
utilizes confidence level and confidence interval to report the outcomes to
avoid bias.
</summary>
    <author>
      <name>Chenxi Wang</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Wanling Gao</name>
    </author>
    <author>
      <name>Yikang Yang</name>
    </author>
    <author>
      <name>Yutong Zhou</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <link href="http://arxiv.org/abs/2411.08494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.08494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.12692v2</id>
    <updated>2025-01-24T05:12:22Z</updated>
    <published>2024-11-19T17:59:12Z</published>
    <title>SparseInfer: Training-free Prediction of Activation Sparsity for Fast
  LLM Inference</title>
    <summary>  Leveraging sparsity is crucial for optimizing large language model inference.
however, modern LLMs employing SiLU as their activation function exhibit
minimal activation sparsity. Recent research has proposed replacing SiLU with
ReLU to induce significant activation sparsity and showed no downstream task
accuracy degradation through fine tuning. However, taking full advantage of it
required training a predictor to estimate this sparsity. In this paper, we
introduce SparseInfer, a simple, light weight, and training free predictor for
activation sparsity of ReLU field LLMs, in which activation sparsity is
predicted by comparing only the sign bits of inputs and weights. To compensate
for possible prediction inaccuracy, an adaptive tuning of the predictor's
conservativeness is enabled, which can also serve as a control knob for
optimizing LLM inference. The proposed method achieves approximately faster
inference speed over the state of the art, with negligible accuracy loss of
within 1%p.
</summary>
    <author>
      <name>Jiho Shin</name>
    </author>
    <author>
      <name>Hoeseok Yang</name>
    </author>
    <author>
      <name>Youngmin Yi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.12692v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.12692v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.15399v1</id>
    <updated>2024-11-23T00:51:09Z</updated>
    <published>2024-11-23T00:51:09Z</published>
    <title>Less is More: Optimizing Function Calling for LLM Execution on Edge
  Devices</title>
    <summary>  The advanced function-calling capabilities of foundation models open up new
possibilities for deploying agents to perform complex API tasks. However,
managing large amounts of data and interacting with numerous APIs makes
function calling hardware-intensive and costly, especially on edge devices.
Current Large Language Models (LLMs) struggle with function calling at the edge
because they cannot handle complex inputs or manage multiple tools effectively.
This results in low task-completion accuracy, increased delays, and higher
power consumption. In this work, we introduce Less-is-More, a novel
fine-tuning-free function-calling scheme for dynamic tool selection. Our
approach is based on the key insight that selectively reducing the number of
tools available to LLMs significantly improves their function-calling
performance, execution time, and power efficiency on edge devices. Experimental
results with state-of-the-art LLMs on edge hardware show agentic success rate
improvements, with execution time reduced by up to 70% and power consumption by
up to 40%.
</summary>
    <author>
      <name>Varatheepan Paramanayakam</name>
    </author>
    <author>
      <name>Andreas Karatzas</name>
    </author>
    <author>
      <name>Iraklis Anagnostopoulos</name>
    </author>
    <author>
      <name>Dimitrios Stamoulis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at DATE 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.15399v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.15399v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.16152v2</id>
    <updated>2024-12-25T07:28:08Z</updated>
    <published>2024-11-25T07:22:20Z</published>
    <title>Optimizing Winograd Convolution on ARMv8 processors</title>
    <summary>  As Convolutional Neural Networks (CNNs) gain prominence in deep learning,
algorithms like Winograd Convolution have been introduced to enhance
computational efficiency. However, existing implementations often face
challenges such as high transformation overhead, suboptimal computation
efficiency, and reduced parallel performance in some layers. We propose a fused
Winograd Convolution algorithm optimized for ARMv8 CPUs, integrating input
transformation, filter transformation, computation, and output transformation
into a single pipeline. By maintaining consecutive memory access and using a
custom z-shaped data layout, our approach fully utilizes an optimized GEMM
micro-kernel with a ping-pong technique. Additionally, we introduce a
multi-dimensional parallel strategy that adapts to convolutional layer scales.
To maximize performance, we manually optimize each kernel in AArch64 assembly
and carefully tune blocking parameters. Experimental results show speedups of
up to 4.74x, 4.10x, 4.72x, and 10.57x over NCNN, NNPACK, FastConv, and ACL on
the Kunpeng 920 platform using multiple threads, with respective gains of
3.85x, 2.81x, 4.20x, and 7.80x on the AWS Graviton2, and 3.32x, 3.68x, 8.00x,
and 9.28x on the Phytium 2000+.
</summary>
    <author>
      <name>Haoyuan Gui</name>
    </author>
    <author>
      <name>Xiaoyu Zhang</name>
    </author>
    <author>
      <name>Chong Zhang</name>
    </author>
    <author>
      <name>Zitong Su</name>
    </author>
    <author>
      <name>Huiyuan Li</name>
    </author>
    <link href="http://arxiv.org/abs/2411.16152v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.16152v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.18873v1</id>
    <updated>2024-11-28T02:51:54Z</updated>
    <published>2024-11-28T02:51:54Z</published>
    <title>Automating Energy-Efficient GPU Kernel Generation: A Fast Search-Based
  Compilation Approach</title>
    <summary>  Deep Neural Networks (DNNs) have revolutionized various fields, but their
deployment on GPUs often leads to significant energy consumption. Unlike
existing methods for reducing GPU energy consumption, which are either
hardware-inflexible or limited by workload constraints, this paper addresses
the problem at the GPU kernel level. We propose a novel search-based
compilation method to generate energy-efficient GPU kernels by incorporating
energy efficiency into the search process. To accelerate the energy evaluation
process, we develop an accurate energy cost model based on high-level kernel
features. Furthermore, we introduce a dynamic updating strategy for the energy
cost model, reducing the need for on-device energy measurements and
accelerating the search process. Our evaluation demonstrates that the proposed
approach can generate GPU kernels with up to 21.69% reduced energy consumption
while maintaining low latency.
</summary>
    <author>
      <name>Yijia Zhang</name>
    </author>
    <author>
      <name>Zhihong Gou</name>
    </author>
    <author>
      <name>Shijie Cao</name>
    </author>
    <author>
      <name>Weigang Feng</name>
    </author>
    <author>
      <name>Sicheng Zhang</name>
    </author>
    <author>
      <name>Guohao Dai</name>
    </author>
    <author>
      <name>Ningyi Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2411.18873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.18873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.01852v1</id>
    <updated>2024-11-29T10:21:12Z</updated>
    <published>2024-11-29T10:21:12Z</published>
    <title>Communication efficient application of sequences of planar rotations to
  a matrix</title>
    <summary>  We present an efficient algorithm for the application of sequences of planar
rotations to a matrix. Applying such sequences efficiently is important in many
numerical linear algebra algorithms for eigenvalues. Our algorithm is novel in
three main ways. First, we introduce a new kernel that is optimized for
register reuse in a novel way. Second, we introduce a blocking and packing
scheme that improves the cache efficiency of the algorithm. Finally, we
thoroughly analyze the memory operations of the algorithm which leads to
important theoretical insights and makes it easier to select good parameters.
Numerical experiments show that our algorithm outperforms the state-of-the-art
and achieves a flop rate close to the theoretical peak on modern hardware.
</summary>
    <author>
      <name>Thijs Steel</name>
    </author>
    <author>
      <name>Julien Langou</name>
    </author>
    <link href="http://arxiv.org/abs/2412.01852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.01852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65F15, 65Y05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.07015v1</id>
    <updated>2024-12-09T21:48:26Z</updated>
    <published>2024-12-09T21:48:26Z</published>
    <title>Accurate Performance Modeling And Uncertainty Analysis of Lossy
  Compression in Scientific Applications</title>
    <summary>  Scientific applications typically generate large volumes of floating-point
data, making lossy compression one of the most effective methods for data
reduction, thereby lowering storage requirements and improving performance in
large-scale applications. However, variations in compression time can
significantly impact overall performance improvement, due to inaccurate
scheduling, workload imbalances, etc. Existing approaches rely on empirical
methods to predict the compression performance, which often lack
interpretability and suffer from limitations in accuracy and generalizability.
In this paper, we propose surrogate models for predicting the compression time
of prediction-based lossy compression and provide a detailed analysis of the
factors influencing time variability with uncertainty analysis. Our evaluation
shows that our solution can accuratly predict the compression time with 5%
average error across six scientific datasets. It also provides accurate 95%
confidence interval, which is essential for time-sensitive scheduling and
applications.
</summary>
    <author>
      <name>Youyuan Liu</name>
    </author>
    <author>
      <name>Taolue Yang</name>
    </author>
    <author>
      <name>Sian Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.07015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.07015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.16716v1</id>
    <updated>2024-12-21T17:57:21Z</updated>
    <published>2024-12-21T17:57:21Z</published>
    <title>Resource Allocation Influence on Application Performance in Sliced
  Testbeds</title>
    <summary>  Modern network architectures have shaped market segments, governments, and
communities with intelligent and pervasive applications. Ongoing digital
transformation through technologies such as softwarization, network slicing,
and AI drives this evolution, along with research into Beyond 5G (B5G) and 6G
architectures. Network slices require seamless management, observability, and
intelligent-native resource allocation, considering user satisfaction, cost
efficiency, security, and energy. Slicing orchestration architectures have been
extensively studied to accommodate these requirements, particularly in resource
allocation for network slices. This study explored the observability of
resource allocation regarding network slice performance in two nationwide
testbeds. We examined their allocation effects on slicing connectivity latency
using a partial factorial experimental method with Central Processing Unit
(CPU) and memory combinations. The results reveal different resource impacts
across the testbeds, indicating a non-uniform influence on the CPU and memory
within the same network slice.
</summary>
    <author>
      <name>Rodrigo Moreira</name>
    </author>
    <author>
      <name>Larissa F. Rodrigues Moreira</name>
    </author>
    <author>
      <name>Tereza C. Carvalho</name>
    </author>
    <author>
      <name>Flávio de Oliveira Silva</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5753/wpeif.2024.2095</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5753/wpeif.2024.2095" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper already published at Anais do XV Workshop de Pesquisa
  Experimental da Internet do Futuro (WPEIF)</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.16716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.16716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.18960v1</id>
    <updated>2024-12-25T18:36:21Z</updated>
    <published>2024-12-25T18:36:21Z</published>
    <title>XRFlux: Virtual Reality Benchmark for Edge Caching Systems</title>
    <summary>  We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality
(VR) delivery systems using edge-cloud caching. As VR applications and systems
progress, the need to meet strict latency and Quality of Experience (QoE)
requirements is increasingly evident. In the context of VR, traditional cloud
architectures (e.g., remote AWS S3 for content delivery) often struggle to meet
these demands, especially for users of the same application in different
locations. With edge computing, resources are brought closer to users in
efforts to reduce latency and improve QoEs. However, VR's dynamic nature, with
changing fields of view (FoVs) and user synchronization requirements, creates
various challenges for edge caching. We address the lack of suitable benchmarks
and propose a framework that simulates multiuser VR scenarios while logging
users' interaction with objects within their actual and predicted FoVs. The
benchmark's activity log can then be played back through an edge cache to
assess the resulting QoEs. This tool fills a gap by supporting research in the
optimization of edge caching (and other edge-cloud functions) for VR streaming.
</summary>
    <author>
      <name>Nader Alfares</name>
    </author>
    <author>
      <name>George Kesidis</name>
    </author>
    <link href="http://arxiv.org/abs/2412.18960v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.18960v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.19051v1</id>
    <updated>2024-12-26T04:13:52Z</updated>
    <published>2024-12-26T04:13:52Z</published>
    <title>Performance Characterization and Optimizations of Traditional ML
  Applications</title>
    <summary>  Even in the era of Deep Learning based methods, traditional machine learning
methods with large data sets continue to attract significant attention.
However, we find an apparent lack of a detailed performance characterization of
these methods in the context of large training datasets. In this work, we study
the system's behavior of a number of traditional ML methods as implemented in
popular free software libraries/modules to identify critical performance
bottlenecks experienced by these applications. The performance characterization
study reveals several interesting insights on the performance of these
applications. Then we evaluate the performance benefits of applying some
well-known optimizations at the levels of caches and the main memory. More
specifically, we test the usefulness of optimizations such as (i) software
prefetching to improve cache performance and (ii) data layout and computation
reordering optimizations to improve locality in DRAM accesses. These
optimizations are implemented as modifications to the well-known scikit-learn
library, and hence can be easily leveraged by application programmers. We
evaluate the impact of the proposed optimizations using a combination of
simulation and execution on a real system. The software prefetching
optimization results in performance benefits varying from 5.2%-27.1% on
different ML applications while the data layout and computation reordering
approaches yield 6.16%-28.0% performance improvement.
</summary>
    <author>
      <name>Harsh Kumar</name>
    </author>
    <author>
      <name>R. Govindarajan</name>
    </author>
    <link href="http://arxiv.org/abs/2412.19051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.19051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.01057v1</id>
    <updated>2025-01-02T04:59:32Z</updated>
    <published>2025-01-02T04:59:32Z</published>
    <title>HPC Application Parameter Autotuning on Edge Devices: A Bandit Learning
  Approach</title>
    <summary>  The growing necessity for enhanced processing capabilities in edge devices
with limited resources has led us to develop effective methods for improving
high-performance computing (HPC) applications. In this paper, we introduce LASP
(Lightweight Autotuning of Scientific Application Parameters), a novel strategy
designed to address the parameter search space challenge in edge devices. Our
strategy employs a multi-armed bandit (MAB) technique focused on online
exploration and exploitation. Notably, LASP takes a dynamic approach, adapting
seamlessly to changing environments. We tested LASP with four HPC applications:
Lulesh, Kripke, Clomp, and Hypre. Its lightweight nature makes it particularly
well-suited for resource-constrained edge devices. By employing the MAB
framework to efficiently navigate the search space, we achieved significant
performance improvements while adhering to the stringent computational limits
of edge devices. Our experimental results demonstrate the effectiveness of LASP
in optimizing parameter search on edge devices.
</summary>
    <author>
      <name>Abrar Hossain</name>
    </author>
    <author>
      <name>Abdel-Hameed A. Badawy</name>
    </author>
    <author>
      <name>Mohammad A. Islam</name>
    </author>
    <author>
      <name>Tapasya Patki</name>
    </author>
    <author>
      <name>Kishwar Ahmed</name>
    </author>
    <link href="http://arxiv.org/abs/2501.01057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.01057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.02483v1</id>
    <updated>2025-01-05T09:12:37Z</updated>
    <published>2025-01-05T09:12:37Z</published>
    <title>sTiles: An Accelerated Computational Framework for Sparse Factorizations
  of Structured Matrices</title>
    <summary>  This paper introduces sTiles, a GPU-accelerated framework for factorizing
sparse structured symmetric matrices. By leveraging tile algorithms for
fine-grained computations, sTiles uses a structure-aware task execution flow to
handle challenging arrowhead sparse matrices with variable bandwidths, common
in scientific and engineering fields. It minimizes fill-in during Cholesky
factorization using permutation techniques and employs a static scheduler to
manage tasks on shared-memory systems with GPU accelerators. sTiles balances
tile size and parallelism, where larger tiles enhance algorithmic intensity but
increase floating-point operations and memory usage, while parallelism is
constrained by the arrowhead structure. To expose more parallelism, a
left-looking Cholesky variant breaks sequential dependencies in trailing
submatrix updates via tree reductions. Evaluations show sTiles achieves
speedups of up to 8.41X, 9.34X, 5.07X, and 11.08X compared to CHOLMOD, SymPACK,
MUMPS, and PARDISO, respectively, and a 5X speedup compared to a 32-core AMD
EPYC CPU on an NVIDIA A100 GPU. Our generic software framework imports
well-established concepts from dense matrix computations but they all require
customizations in their deployments on hybrid architectures to best handle
factorizations of sparse matrices with arrowhead structures.
</summary>
    <author>
      <name>Esmail Abdul Fattah</name>
    </author>
    <author>
      <name>Hatem Ltaief</name>
    </author>
    <author>
      <name>Havard Rue</name>
    </author>
    <author>
      <name>David Keyes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.02483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.02483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.03427v1</id>
    <updated>2025-01-06T23:15:26Z</updated>
    <published>2025-01-06T23:15:26Z</published>
    <title>Boosting Cross-Architectural Emulation Performance by Foregoing the
  Intermediate Representation Model</title>
    <summary>  As more applications utilize virtualization and emulation to run
mission-critical tasks, the performance requirements of emulated and
virtualized platforms continue to rise. Hardware virtualization is not
universally available for all systems, and is incapable of emulating CPU
architectures, requiring software emulation to be used. QEMU, the premier
cross-architecture emulator for Linux and some BSD systems, currently uses
dynamic binary translation (DBT) through intermediate representations using its
Tiny Code Generator (TCG) model. While using intermediate representations of
translated code allows QEMU to quickly add new host and guest architectures, it
creates additional steps in the emulation pipeline which decrease performance.
We construct a proof of concept emulator to demonstrate the slowdown caused by
the usage of intermediate representations in TCG; this emulator performed up to
35x faster than QEMU with TCG, indicating substantial room for improvement in
QEMU's design. We propose an expansion of QEMU's two-tier engine system (Linux
KVM versus TCG) to include a middle tier using direct binary translation for
commonly paired architectures such as RISC-V, x86, and ARM. This approach
provides a slidable trade-off between development effort and performance
depending on the needs of end users.
</summary>
    <author>
      <name>Amy Iris Parker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures. Submitted to the 5th International Conference on
  Electrical, Computer and Energy Technologies</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.03427v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.03427v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.8; D.4.7; C.1.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.05811v1</id>
    <updated>2025-01-10T09:27:15Z</updated>
    <published>2025-01-10T09:27:15Z</published>
    <title>MLKAPS: Machine Learning and Adaptive Sampling for HPC Kernel
  Auto-tuning</title>
    <summary>  Many High-Performance Computing (HPC) libraries rely on decision trees to
select the best kernel hyperparameters at runtime,depending on the input and
environment. However, finding optimized configurations for each input and
environment is challengingand requires significant manual effort and
computational resources. This paper presents MLKAPS, a tool that automates this
task usingmachine learning and adaptive sampling techniques. MLKAPS generates
decision trees that tune HPC kernels' design parameters toachieve efficient
performance for any user input. MLKAPS scales to large input and design spaces,
outperforming similar state-of-the-artauto-tuning tools in tuning time and mean
speedup. We demonstrate the benefits of MLKAPS on the highly optimized Intel
MKLdgetrf LU kernel and show that MLKAPS finds blindspots in the manual tuning
of HPC experts. It improves over 85% of the inputswith a geomean speedup of
x1.30. On the Intel MKL dgeqrf QR kernel, MLKAPS improves performance on 85% of
the inputs with ageomean speedup of x1.18.
</summary>
    <author>
      <name>Mathys Jam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LI-PaRAD, UVSQ</arxiv:affiliation>
    </author>
    <author>
      <name>Eric Petit</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LI-PaRAD, UVSQ</arxiv:affiliation>
    </author>
    <author>
      <name>Pablo de Oliveira Castro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LI-PaRAD, UVSQ</arxiv:affiliation>
    </author>
    <author>
      <name>David Defour</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAMPS, UPVD</arxiv:affiliation>
    </author>
    <author>
      <name>Greg Henry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LI-PaRAD, UVSQ</arxiv:affiliation>
    </author>
    <author>
      <name>William Jalby</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LI-PaRAD, UVSQ</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2501.05811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.05811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13382v1</id>
    <updated>2025-01-23T04:50:05Z</updated>
    <published>2025-01-23T04:50:05Z</published>
    <title>Accelerating Gaussian beam tracing method with dynamic parallelism on
  graphics processing units</title>
    <summary>  This study presents a reconstruction of the Gaussian Beam Tracing solution
using CUDA, with a particular focus on the utilisation of GPU acceleration as a
means of overcoming the performance limitations of traditional CPU algorithms
in complex acoustic simulations. The algorithm is implemented and optimised on
the NVIDIA RTX A6000 GPU, resulting in a notable enhancement in the performance
of the Gaussian Beam Summation (GBS) process. In particular, the
GPU-accelerated GBS algorithm demonstrated a significant enhancement in
performance, reaching up to 790 times faster in city enviroment and 188 times
faster in open plane enviroment compared to the original CPU-based program. To
address the challenges of acceleration, the study introduce innovative
solutions for handling irregular loops and GPU memory limitations, ensuring the
efficient processing of large quantities of rays beyond the GPU's
single-process capacity. Furthermore, this work established performance
evaluation strategies crucial for analysing and reconstructing similar
algorithms. Additionally, the study explored future directions for further
accelerating the algorithm, laying the groundwork for ongoing improvements.
</summary>
    <author>
      <name>Zhang Sheng</name>
    </author>
    <author>
      <name>Lishu Duan</name>
    </author>
    <author>
      <name>Hanbo Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.13553v1</id>
    <updated>2025-01-23T10:56:47Z</updated>
    <published>2025-01-23T10:56:47Z</published>
    <title>Compiler Support for Speculation in Decoupled Access/Execute
  Architectures</title>
    <summary>  Irregular codes are bottlenecked by memory and communication latency.
Decoupled access/execute (DAE) is a common technique to tackle this problem. It
relies on the compiler to separate memory address generation from the rest of
the program, however, such a separation is not always possible due to control
and data dependencies between the access and execute slices, resulting in a
loss of decoupling.
  In this paper, we present compiler support for speculation in DAE
architectures that preserves decoupling in the face of control dependencies. We
speculate memory requests in the access slice and poison mis-speculations in
the execute slice without the need for replays or synchronization. Our
transformation works on arbitrary, reducible control flow and is proven to
preserve sequential consistency. We show that our approach applies to a wide
range of architectural work on CPU/GPU prefetchers, CGRAs, and accelerators,
enabling DAE on a wider range of codes than before.
</summary>
    <author>
      <name>Robert Szafarczyk</name>
    </author>
    <author>
      <name>Syed Waqar Nabi</name>
    </author>
    <author>
      <name>Wim Vanderbauwhede</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3708493.3712695</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3708493.3712695" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Proceedings of the 34th ACM SIGPLAN International
  Conference on Compiler Construction (CC 2025)</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.13553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.14925v2</id>
    <updated>2025-01-28T17:52:16Z</updated>
    <published>2025-01-24T21:31:11Z</published>
    <title>Profiling Apple Silicon Performance for ML Training</title>
    <summary>  Apple Silicon has attracted much attention for its performance and role in
machine learning (ML) training. Unlike NVIDIA GPUs, which have traditionally
dominated ML training, Apple Silicon has a significant difference in memory
architecture. It uses Unified Memory, which integrates CPU and GPU memory
instead of separate CPU memory and GPU VRAM. However, it is difficult to tell
whether Unified Memory means more performance benefits.
  This paper investigates the performance differences by training several large
language model (LLM) workloads end-to-end under different memory scenarios. The
results show a significant performance gap between Apple Silicon and NVIDIA
GPUs. This paper attributes this gap to system-level factors such as page
faults, power consumption, and kernel launch time. In addition, the performance
difference of basic linear algebra subprograms (BLAS) on the NVIDIA GPUs and
Apple Silicon chips is analyzed to further explain the observed gap.
</summary>
    <author>
      <name>Dahua Feng</name>
    </author>
    <author>
      <name>Zhiming Xu</name>
    </author>
    <author>
      <name>Rongxiang Wang</name>
    </author>
    <author>
      <name>Felix Xiaozhu Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2501.14925v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.14925v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.08804v2</id>
    <updated>2025-03-12T06:28:45Z</updated>
    <published>2025-02-12T21:39:22Z</published>
    <title>Novel Lower Bounds on M/G/k Scheduling</title>
    <summary>  In queueing systems, effective scheduling algorithms are essential for
optimizing performance in a wide range of modern applications. While the theory
of optimal M/G/1 scheduling for mean response time is well established, many
modern queueing systems operate with multiple servers. Recently, optimal
scheduling for the M/G/k queue has been explored in the heavy traffic limit,
but much remains unknown about optimal scheduling in the intermediate regime.
  In this paper, we give the first framework for proving nontrivial lower
bounds on the mean response time of the M/G/k system under arbitrary scheduling
policies. These bounds significantly improve upon previous naive lower bounds,
particularly for moderate loads. Key to our approach is a new variable-speed
queue, which we call the Increasing Speed Queue, which more accurately captures
the work completion behavior of multiserver systems. To analyze the expected
work of this Increasing Speed Queue, we develop the DiffeDrift method, a novel
manner of employing the drift method/BAR approach, by developing test functions
via the solutions to a differential equation.
</summary>
    <author>
      <name>Ziyuan Wang</name>
    </author>
    <author>
      <name>Isaac Grosof</name>
    </author>
    <link href="http://arxiv.org/abs/2502.08804v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.08804v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.08995v1</id>
    <updated>2025-02-13T06:14:59Z</updated>
    <published>2025-02-13T06:14:59Z</published>
    <title>PixLift: Accelerating Web Browsing via AI Upscaling</title>
    <summary>  Accessing the internet in regions with expensive data plans and limited
connectivity poses significant challenges, restricting information access and
economic growth. Images, as a major contributor to webpage sizes, exacerbate
this issue, despite advances in compression formats like WebP and AVIF. The
continued growth of complex and curated web content, coupled with suboptimal
optimization practices in many regions, has prevented meaningful reductions in
web page sizes. This paper introduces PixLift, a novel solution to reduce
webpage sizes by downscaling their images during transmission and leveraging AI
models on user devices to upscale them. By trading computational resources for
bandwidth, PixLift enables more affordable and inclusive web access. We address
key challenges, including the feasibility of scaled image requests on popular
websites, the implementation of PixLift as a browser extension, and its impact
on user experience. Through the analysis of 71.4k webpages, evaluations of
three mainstream upscaling models, and a user study, we demonstrate PixLift's
ability to significantly reduce data usage without compromising image quality,
fostering a more equitable internet.
</summary>
    <author>
      <name>Yonas Atinafu</name>
    </author>
    <author>
      <name>Sarthak Malla</name>
    </author>
    <author>
      <name>HyunSeok Daniel Jang</name>
    </author>
    <author>
      <name>Nouar Aldahoul</name>
    </author>
    <author>
      <name>Matteo Varvello</name>
    </author>
    <author>
      <name>Yasir Zaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.08995v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.08995v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.11347v1</id>
    <updated>2025-02-17T01:55:38Z</updated>
    <published>2025-02-17T01:55:38Z</published>
    <title>Evaluating the Performance of the DeepSeek Model in Confidential
  Computing Environment</title>
    <summary>  The increasing adoption of Large Language Models (LLMs) in cloud environments
raises critical security concerns, particularly regarding model confidentiality
and data privacy. Confidential computing, enabled by Trusted Execution
Environments (TEEs), offers a promising solution to mitigate these risks.
However, existing TEE implementations, primarily CPU-based, struggle to
efficiently support the resource-intensive nature of LLM inference and
training. In this work, we present the first evaluation of the DeepSeek model
within a TEE-enabled confidential computing environment, specifically utilizing
Intel Trust Domain Extensions (TDX). Our study benchmarks DeepSeek's
performance across CPU-only, CPU-GPU hybrid, and TEE-based implementations. For
smaller parameter sets, such as DeepSeek-R1-1.5B, the TDX implementation
outperforms the CPU version in executing computations within a secure
environment. It highlights the potential for efficiently deploying LLM models
on resource-constrained systems while ensuring security. The overall GPU-to-CPU
performance ratio averages 12 across different model sizes, with smaller models
exhibiting a lower ratio. Additionally, we provide foundational insights and
guidance on optimizing CPU-GPU confidential computing solutions for scalable
and secure AI deployments. Our findings contribute to the advancement of
privacy-preserving AI, paving the way for efficient and secure LLM inference in
confidential computing environments.
</summary>
    <author>
      <name>Ben Dong</name>
    </author>
    <author>
      <name>Qian Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2502.11347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.11347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.11906v1</id>
    <updated>2025-02-17T15:26:41Z</updated>
    <published>2025-02-17T15:26:41Z</published>
    <title>Comparison of Vectorization Capabilities of Different Compilers for X86
  and ARM CPUs</title>
    <summary>  Most modern processors contain vector units that simultaneously perform the
same arithmetic operation over multiple sets of operands. The ability of
compilers to automatically vectorize code is critical to effectively using
these units. Understanding this capability is important for anyone writing
compute-intensive, high-performance, and portable code. We tested the ability
of several compilers to vectorize code on x86 and ARM. We used the TSVC2 suite,
with modifications that made it more representative of real-world code. On x86,
GCC reported 54% of the loops in the suite as having been vectorized, ICX
reported 50%, and Clang, 46%. On ARM, GCC reported 56% of the loops as having
been vectorized, ACFL reported 54%, and Clang, 47%. We found that the
vectorized code did not always outperform the unvectorized code. In some cases,
given two very similar vectorizable loops, a compiler would vectorize one but
not the other. We also report cases where a compiler vectorized a loop on only
one of the two platforms. Based on our experiments, we cannot definitively say
if any one compiler is significantly better than the others at vectorizing code
on any given platform.
</summary>
    <author>
      <name>Nazmus Sakib</name>
    </author>
    <author>
      <name>Tarun Prabhu</name>
    </author>
    <author>
      <name>Nandakishore Santhi</name>
    </author>
    <author>
      <name>John Shalf</name>
    </author>
    <author>
      <name>Abdel-Hameed A. Badawy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE HPEC 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.11906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.11906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.20072v1</id>
    <updated>2025-02-27T13:29:40Z</updated>
    <published>2025-02-27T13:29:40Z</published>
    <title>A high-performance and portable implementation of the SISSO method for
  CPUs and GPUs</title>
    <summary>  SISSO (sure-independence screening and sparsifying operator) is an artificial
intelligence (AI) method based on symbolic regression and compressed sensing
widely used in materials science research. SISSO++ is its C++ implementation
that employs MPI and OpenMP for parallelization, rendering it well-suited for
high-performance computing (HPC) environments. As heterogeneous hardware
becomes mainstream in the HPC and AI fields, we chose to port the SISSO++ code
to GPUs using the Kokkos performance-portable library. Kokkos allows us to
maintain a single codebase for both Nvidia and AMD GPUs, significantly reducing
the maintenance effort. In this work, we summarize the necessary code changes
we did to achieve hardware and performance portability. This is accompanied by
performance benchmarks on Nvidia and AMD GPUs. We demonstrate the speedups
obtained from using GPUs across the three most time-consuming parts of our
code.
</summary>
    <author>
      <name>Sebastian Eibl</name>
    </author>
    <author>
      <name>Yi Yao</name>
    </author>
    <author>
      <name>Matthias Scheffler</name>
    </author>
    <author>
      <name>Markus Rampp</name>
    </author>
    <author>
      <name>Luca M. Ghiringhelli</name>
    </author>
    <author>
      <name>Thomas A. R. Purcell</name>
    </author>
    <link href="http://arxiv.org/abs/2502.20072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.20072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.00408v1</id>
    <updated>2025-03-01T09:02:30Z</updated>
    <published>2025-03-01T09:02:30Z</published>
    <title>A Microbenchmark Framework for Performance Evaluation of OpenMP Target
  Offloading</title>
    <summary>  We present a framework based on Catch2 to evaluate performance of OpenMP's
target offload model via micro-benchmarks. The compilers supporting OpenMP's
target offload model for heterogeneous architectures are currently undergoing
rapid development. These developments influence performance of various complex
applications in different ways. This framework can be employed to track the
impact of compiler upgrades and compare their performance with the native
programming models. We use the framework to benchmark performance of a few
commonly used operations on leadership class supercomputers such as Perlmutter
at National Energy Research Scientific Computing (NERSC) Center and Frontier at
Oak Ridge Leadership Computing Facility (OLCF). Such a framework will be useful
for compiler developers to gain insights into the overall impact of many small
changes, as well as for users to decide which compilers and versions are
expected to yield best performance for their applications.
</summary>
    <author>
      <name>Mohammad Atif</name>
    </author>
    <author>
      <name>Tianle Wang</name>
    </author>
    <author>
      <name>Zhihua Dong</name>
    </author>
    <author>
      <name>Charles Leggett</name>
    </author>
    <author>
      <name>Meifeng Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2503.00408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.00408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.02504v1</id>
    <updated>2025-03-04T11:15:47Z</updated>
    <published>2025-03-04T11:15:47Z</published>
    <title>Energy efficiency of cache eviction algorithms for Zipf distributed
  objects</title>
    <summary>  This paper presents a summary analysis of the Least Frequently Used (LFU) and
Perfect Least Frequently Used (PLFU) cache eviction algorithms on real data,
transferred on Content Delivery Nettworks (CDNs), as well as on Zipf
distributed samples. In light of the growing emphasis on energy efficiency in
CDNs in recent years due to rising energy costs, this paper considers and
discusses the total CPU time required to run a cache algorithm. The total CPU
time represents a novel metric for evaluating cache performance, and it is
contrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a
new algorithm with an admission policy and the eviction strategy that of PLFU
is presented. The results demonstrate that it is a simple and straightforward
algorithm to implement and offers high CHR and low CPU time.
</summary>
    <author>
      <name>Emese Sziklay</name>
    </author>
    <author>
      <name>Tamás Jursonovics</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures, ICRIC 2023, Volume 2</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.02504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.02504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.02982v1</id>
    <updated>2025-03-04T20:15:56Z</updated>
    <published>2025-03-04T20:15:56Z</published>
    <title>Heavy-traffic Optimality of Skip-the-Longest-Queues in Heterogeneous
  Parallel Service Systems</title>
    <summary>  We consider a discrete-time parallel service system consisting of $n$
heterogeneous single server queues with infinite capacity. Jobs arrive to the
system as an i.i.d. process with rate proportional to $n$, and must be
immediately dispatched in the time slot that they arrive. The dispatcher is
assumed to be able to exchange messages with the servers to obtain their queue
lengths and make dispatching decisions, introducing an undesirable
communication overhead.
  In this setting, we propose a ultra-low communication overhead load balancing
policy dubbed $k$-Skip-the-$d$-Longest-Queues ($k$-SLQ-$d$), where queue
lengths are only observed every $k(n-d)$ time slots and, between observations,
incoming jobs are sent to a queue that is not one of the $d$ longest ones at
the time that the queues were last observed. For this policy, we establish
conditions on $d$ for it to be throughput optimal and we show that, under that
condition, it is asymptotically delay-optimal in heavy-traffic for arbitrarily
low communication overheads (i.e., for arbitrarily large $k$).
</summary>
    <author>
      <name>Yishun Luo</name>
    </author>
    <author>
      <name>Martin Zubeldia</name>
    </author>
    <link href="http://arxiv.org/abs/2503.02982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.02982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.04193v1</id>
    <updated>2025-03-06T08:16:08Z</updated>
    <published>2025-03-06T08:16:08Z</published>
    <title>Towards Multi-dimensional Elasticity for Pervasive Stream Processing
  Services</title>
    <summary>  This paper proposes a hierarchical solution to scale streaming services
across quality and resource dimensions. Modern scenarios, like smart cities,
heavily rely on the continuous processing of IoT data to provide real-time
services and meet application targets (Service Level Objectives -- SLOs). While
the tendency is to process data at nearby Edge devices, this creates a
bottleneck because resources can only be provisioned up to a limited capacity.
To improve elasticity in Edge environments, we propose to scale services in
multiple dimensions -- either resources or, alternatively, the service quality.
We rely on a two-layer architecture where (1) local, service-specific agents
ensure SLO fulfillment through multi-dimensional elasticity strategies; if no
more resources can be allocated, (2) a higher-level agent optimizes global SLO
fulfillment by swapping resources. The experimental results show promising
outcomes, outperforming regular vertical autoscalers, when operating under
tight resource constraints.
</summary>
    <author>
      <name>Boris Sedlak</name>
    </author>
    <author>
      <name>Andrea Morichetta</name>
    </author>
    <author>
      <name>Philipp Raith</name>
    </author>
    <author>
      <name>Víctor Casamayor Pujol</name>
    </author>
    <author>
      <name>Schahram Dustdar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at Percom 2025 as Work in Progress (WIP)</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.04193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.04193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.11244v1</id>
    <updated>2025-03-14T09:52:30Z</updated>
    <published>2025-03-14T09:52:30Z</published>
    <title>LLMPerf: GPU Performance Modeling meets Large Language Models</title>
    <summary>  Performance modeling, a pivotal domain in program cost analysis, currently
relies on manually crafted models constrained by various program and hardware
limitations, especially in the intricate landscape of GPGPU. Meanwhile, Large
Language Models (LLMs) have demonstrated their effectiveness in addressing
diverse programming challenges. Our work establishes a connection between LLMs
and performance modeling, employing the LLM as a performance estimator. Through
experimental exploration with carefully designed large-scale OpenCL datasets,
we highlight the potential capability as well as the main difficulties of using
LLMs in handling performance modeling tasks for OpenCL device source programs.
As the first study for this line of work, our LLM-based performance model
achieves a mean absolute percentage error of $24.25\%$ for a large-scale
generated validation set. On a set of publicly available OpenCL programs, our
model achieves a mean absolute percentage error of $46.1\%$.
</summary>
    <author>
      <name>Khoi N. M. Nguyen</name>
    </author>
    <author>
      <name>Hoang Duy Nguyen Do</name>
    </author>
    <author>
      <name>Huyen Thao Le</name>
    </author>
    <author>
      <name>Thanh Tuan Dao</name>
    </author>
    <link href="http://arxiv.org/abs/2503.11244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.11244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.13679v1</id>
    <updated>2025-03-17T19:32:26Z</updated>
    <published>2025-03-17T19:32:26Z</published>
    <title>PrETi: Predicting Execution Time in Early Stage with LLVM and Machine
  Learning</title>
    <summary>  We introduce preti, a novel framework for predicting software execution time
during the early stages of development. preti leverages an LLVM-based
simulation environment to extract timing-related runtime information, such as
the count of executed LLVM IR instructions. This information, combined with
historical execution time data, is utilized to train machine learning models
for accurate time prediction. To further enhance prediction accuracy, our
approach incorporates simulations of cache accesses and branch prediction. The
evaluations on public benchmarks demonstrate that preti achieves an average
Absolute Percentage Error (APE) of 11.98\%, surpassing state-of-the-art
methods. These results underscore the effectiveness and efficiency of preti as
a robust solution for early-stage timing analysis.
</summary>
    <author>
      <name>Risheng Xu</name>
    </author>
    <author>
      <name>Philipp Sieweck</name>
    </author>
    <author>
      <name>Hermann von Hasseln</name>
    </author>
    <author>
      <name>Dirk Nowotka</name>
    </author>
    <link href="http://arxiv.org/abs/2503.13679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.13679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.14781v1</id>
    <updated>2025-03-18T23:15:02Z</updated>
    <published>2025-03-18T23:15:02Z</published>
    <title>Fake Runs, Real Fixes -- Analyzing xPU Performance Through Simulation</title>
    <summary>  As models become larger, ML accelerators are a scarce resource whose
performance must be continually optimized to improve efficiency. Existing
performance analysis tools are coarse grained, and fail to capture model
performance at the machine-code level. In addition, these tools often do not
provide specific recommendations for optimizations. We present xPU-Shark, a
fine-grained methodology for analyzing ML models at the machine-code level that
provides actionable optimization suggestions. Our core insight is to use a
hardware-level simulator, an artifact of the hardware design process that we
can re-purpose for performance analysis. xPU-Shark captures traces from
production deployments running on accelerators and replays them in a modified
microarchitecture simulator to gain low-level insights into the model's
performance. We implement xPU-Shark for our in-house accelerator and used it to
analyze the performance of several of our production LLMs, revealing several
previously-unknown microarchitecture inefficiencies. Leveraging these insights,
we optimize a common communication collective by up to 15% and reduce token
generation latency by up to 4.1%.
</summary>
    <author>
      <name>Ioannis Zarkadas</name>
    </author>
    <author>
      <name>Amanda Tomlinson</name>
    </author>
    <author>
      <name>Asaf Cidon</name>
    </author>
    <author>
      <name>Baris Kasikci</name>
    </author>
    <author>
      <name>Ofir Weisse</name>
    </author>
    <link href="http://arxiv.org/abs/2503.14781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.14781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16332v1</id>
    <updated>2025-03-20T16:55:05Z</updated>
    <published>2025-03-20T16:55:05Z</published>
    <title>A Dataset of Performance Measurements and Alerts from Mozilla (Data
  Artifact)</title>
    <summary>  Performance regressions in software systems can lead to significant financial
losses and degraded user satisfaction, making their early detection and
mitigation critical. Despite the importance of practices that capture
performance regressions early, there is a lack of publicly available datasets
that comprehensively capture real-world performance measurements,
expert-validated alerts, and associated metadata such as bugs and testing
conditions.
  To address this gap, we introduce a unique dataset to support various
research studies in performance engineering, anomaly detection, and machine
learning. This dataset was collected from Mozilla Firefox's performance testing
infrastructure and comprises 5,655 performance time series, 17,989 performance
alerts, and detailed annotations of resulting bugs collected from May 2023 to
May 2024. By publishing this dataset, we provide researchers with an invaluable
resource for studying performance trends, developing novel change point
detection methods, and advancing performance regression analysis across diverse
platforms and testing environments. The dataset is available at
https://doi.org/10.5281/zenodo.14642238
</summary>
    <author>
      <name>Mohamed Bilel Besbes</name>
    </author>
    <author>
      <name>Diego Elias Costa</name>
    </author>
    <author>
      <name>Suhaib Mujahid</name>
    </author>
    <author>
      <name>Gregory Mierzwinski</name>
    </author>
    <author>
      <name>Marco Castelluccio</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3680256.3721973</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3680256.3721973" rel="related"/>
    <link href="http://arxiv.org/abs/2503.16332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.17783v1</id>
    <updated>2025-03-22T14:28:29Z</updated>
    <published>2025-03-22T14:28:29Z</published>
    <title>Energy-Aware LLMs: A step towards sustainable AI for downstream
  applications</title>
    <summary>  Advanced Large Language Models (LLMs) have revolutionized various fields,
including communication networks, sparking an innovation wave that has led to
new applications and services, and significantly enhanced solution schemes.
Despite all these impressive developments, most LLMs typically require huge
computational resources, resulting in terribly high energy consumption. Thus,
this research study proposes an end-to-end pipeline that investigates the
trade-off between energy efficiency and model performance for an LLM during
fault ticket analysis in communication networks. It further evaluates the
pipeline performance using two real-world datasets for the tasks of root cause
analysis and response feedback in a communication network. Our results show
that an appropriate combination of quantization and pruning techniques is able
to reduce energy consumption while significantly improving model performance.
</summary>
    <author>
      <name>Nguyen Phuc Tran</name>
    </author>
    <author>
      <name>Brigitte Jaumard</name>
    </author>
    <author>
      <name>Oscar Delgado</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to V. International Conference on
  Electrical, Computer and Energy Technologies (ICECET 2025) for possible
  publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.17783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.17783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.17893v1</id>
    <updated>2025-03-23T00:55:31Z</updated>
    <published>2025-03-23T00:55:31Z</published>
    <title>Modeling Utilization to Identify Shared-Memory Atomic Bottlenecks</title>
    <summary>  Performance analysis is critical for GPU programs with data-dependent
behavior, but models like Roofline are not very useful for them and
interpreting raw performance counters is tedious. In this work, we present an
analytical model for shared memory atomics (\emph{fetch-and-op} and
\emph{compare-and-swap} instructions on NVIDIA Volta and Ampere GPU) that
allows users to immediately determine if shared memory atomic operations are a
bottleneck for a program's execution. Our model is based on modeling the
architecture as a single-server queuing model whose inputs are performance
counters. It captures load-dependent behavior such as pipelining, parallelism,
and different access patterns. We embody this model in a tool that uses CUDA
hardware counters as parameters to predict the utilization of the shared-memory
atomic unit. To the best of our knowledge, no existing profiling tool or model
provides this capability for shared-memory atomic operations. We used the model
to compare two histogram kernels that use shared-memory atomics. Although
nearly identical, their performance can be different by up to 30\%. Our tool
correctly identifies a bottleneck shift from shared-memory atomic unit as the
cause of this discrepancy.
</summary>
    <author>
      <name>Rongcui Dong</name>
    </author>
    <author>
      <name>Sreepathi Pai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">GPGPU 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.17893v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.17893v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.20074v2</id>
    <updated>2025-03-27T17:16:44Z</updated>
    <published>2025-03-25T21:20:11Z</published>
    <title>Adaptive Orchestration for Large-Scale Inference on Heterogeneous
  Accelerator Systems Balancing Cost, Performance, and Resilience</title>
    <summary>  The surge in generative AI workloads has created a need for scalable
inference systems that can flexibly harness both GPUs and specialized
accelerators while containing operational costs. This paper proposes a
hardware-agnostic control loop that adaptively allocates requests across
heterogeneous accelerators based on real-time cost and capacity signals. The
approach sustains low latency and high throughput by dynamically shifting
between cost-optimized and capacity-optimized modes, ensuring the most
efficient use of expensive compute resources under fluctuating availability.
Evaluated using the Stable Diffusion model, the framework consistently meets
latency targets, automatically redirects traffic during capacity shortfalls,
and capitalizes on lower-cost accelerators when possible. These results
highlight how a feedback-driven deployment strategy, spanning the entire
software and hardware stack, can help organizations efficiently scale
generative AI workloads while maintaining resilience in the face of limited
accelerator capacity.
</summary>
    <author>
      <name>Yahav Biran</name>
    </author>
    <author>
      <name>Imry Kissos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.20074v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.20074v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.21832v1</id>
    <updated>2025-03-26T22:42:53Z</updated>
    <published>2025-03-26T22:42:53Z</published>
    <title>Assembly line balancing considering stochastic task times and production
  defects</title>
    <summary>  In this paper, we address the inherent limitations in traditional assembly
line balancing, specifically the assumptions that task times are constant and
no defective outputs occur. These assumptions often do not hold in practical
scenarios, leading to inefficiencies. To address these challenges, we introduce
a framework utilizing an "adjusted processing time" approach based on the
distributional information of both processing times and defect occurrences. We
validate our framework through the analysis of two case studies from existing
literature, demonstrating its robustness and adaptability. Our framework is
characterized by its simplicity, both in understanding and implementation,
marking a substantial advancement in the field. It presents a viable and
efficient solution for industries seeking to enhance operational efficiency
through improved resource allocation.
</summary>
    <author>
      <name>Gazi Nazia Nur</name>
    </author>
    <author>
      <name>Mohammad Ahnaf Sadat</name>
    </author>
    <author>
      <name>Basit Mahmud Shahriar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper was peer-reviewed and accepted for inclusion in a
  conference (IUT-ICCET 2024) proceeding, but the conference was postponed
  indefinitely and did not take place</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.21832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.21832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62P30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.10; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.00002v1</id>
    <updated>2025-03-10T16:27:17Z</updated>
    <published>2025-03-10T16:27:17Z</published>
    <title>Are We There Yet? A Measurement Study of Efficiency for LLM Applications
  on Mobile Devices</title>
    <summary>  Recent advancements in large language models (LLMs) have prompted interest in
deploying these models on mobile devices to enable new applications without
relying on cloud connectivity. However, the efficiency constraints of deploying
LLMs on resource-limited devices present significant challenges. In this paper,
we conduct a comprehensive measurement study to evaluate the efficiency
tradeoffs between mobile-based, edge-based, and cloud-based deployments for LLM
applications. We implement AutoLife-Lite, a simplified LLM-based application
that analyzes smartphone sensor data to infer user location and activity
contexts. Our experiments reveal that: (1) Only small-size LLMs (&lt;4B
parameters) can run successfully on powerful mobile devices, though they
exhibit quality limitations compared to larger models; (2) Model compression is
effective in lower the hardware requirement, but may lead to significant
performance degradation; (3) The latency to run LLMs on mobile devices with
meaningful output is significant (&gt;30 seconds), while cloud services
demonstrate better time efficiency (&lt;10 seconds); (4) Edge deployments offer
intermediate tradeoffs between latency and model capabilities, with different
results on CPU-based and GPU-based settings. These findings provide valuable
insights for system designers on the current limitations and future directions
for on-device LLM applications.
</summary>
    <author>
      <name>Xiao Yan</name>
    </author>
    <author>
      <name>Yi Ding</name>
    </author>
    <link href="http://arxiv.org/abs/2504.00002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.00002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.01052v1</id>
    <updated>2025-04-01T12:30:18Z</updated>
    <published>2025-04-01T12:30:18Z</published>
    <title>Analyzing homogenous and heterogeneous multi-server queues via neural
  networks</title>
    <summary>  In this paper, we use a machine learning approach to predict the stationary
distributions of the number of customers in a single-staiton multi server
system. We consider two systems, the first is $c$ homogeneous servers, namely
the $GI/GI/c$ queue. The second is a two-heterogeneous server system, namely
the $GI/GI_i/2$ queue. We train a neural network for these queueing models,
using the first four inter-arrival and service time moments. We demonstrate
empirically that using the fifth moment and beyond does not increase accuracy.
  Compared to existing methods, we show that in terms of the stationary
distribution and the mean value of the number of customers in a $GI/GI/c$
queue, we are state-of-the-art. Further, we are the only ones to predict the
stationary distribution of the number of customers in the system in a
$GI/GI_i/2$ queue. We conduct a thorough performance evaluation to assert that
our model is accurate. In most cases, we demonstrate that our error is less
than 5\%. Finally, we show that making inferences is very fast, where 5000
inferences can be made in parallel within a fraction of a second.
</summary>
    <author>
      <name>Eliran Sherzer</name>
    </author>
    <link href="http://arxiv.org/abs/2504.01052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.01052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07042v2</id>
    <updated>2025-04-28T08:35:30Z</updated>
    <published>2025-04-09T17:00:05Z</published>
    <title>Towards a Higher Roofline for Matrix-Vector Multiplication in
  Matrix-Free HOSFEM</title>
    <summary>  The high-order/spectral finite element method (HOSFEM) is a widely used
numerical method for solving PDEs, with its performance primarily relying on
axhelm, a matrix-free kernel for element-local matrix-vector multiplications.
In axhelm, geometric factors account for over half of memory access but
minimally contribute to computational workload. This imbalance significantly
constrains the performance roofline, indicating that further optimization of
tensor contraction, the core computation in axhelm, yields only minimal
improvements. To overcome this bottleneck, we propose a low-cost on-the-fly
recalculation of geometric factors for trilinear elements, thereby unlocking
substantial potential for optimizing tensor contraction. The proposed approach
is implemented in Nekbone, a standard HOSFEM benchmark. With optimizations such
as merging scalar factors, partial recalculation, Tensor Core acceleration, and
constant memory utilization, performance reaches 85%-100% of the higher
roofline. The optimized kernels achieve speedups of 1.74x-4.10x on NVIDIA A100
and 1.99x-3.77x on DCU K100. This leads to a 1.12x-1.40x speedup for Nekbone.
</summary>
    <author>
      <name>Zijian Cao</name>
    </author>
    <author>
      <name>Qiao Sun</name>
    </author>
    <author>
      <name>Tiangong Zhang</name>
    </author>
    <author>
      <name>Huiyuan Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 10 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.07042v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07042v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.10622v1</id>
    <updated>2025-04-14T18:22:15Z</updated>
    <published>2025-04-14T18:22:15Z</published>
    <title>Improving Upon the generalized c-mu rule: a Whittle approach</title>
    <summary>  Scheduling a stream of jobs whose holding cost changes over time is a classic
and practical problem. Specifically, each job is associated with a holding cost
(penalty), where a job's instantaneous holding cost is some increasing function
of its class and current age (the time it has spent in the system since its
arrival). The goal is to schedule the jobs to minimize the time-average total
holding cost across all jobs.
  The seminal paper on this problem, by Van Mieghem in 1995, introduced the
generalized c-mu rule for scheduling jobs. Since then, this problem has
attracted significant interest but remains challenging due to the absence of a
finite-dimensional state space formulation. Consequently, subsequent works
focus on more tractable versions of this problem.
  This paper returns to the original problem, deriving a heuristic that
empirically improves upon the generalized c-mu rule and all existing
heuristics. Our approach is to first translate the holding cost minimization
problem to a novel Restless Multi-Armed Bandit (R-MAB) problem with a finite
number of arms. Based on our R-MAB, we derive a novel Whittle Index policy,
which is both elegant and intuitive.
</summary>
    <author>
      <name>Zhouzi Li</name>
    </author>
    <author>
      <name>Keerthana Gurushankar</name>
    </author>
    <author>
      <name>Mor Harchol-Balter</name>
    </author>
    <author>
      <name>Alan Scheller-Wolf</name>
    </author>
    <link href="http://arxiv.org/abs/2504.10622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.10622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.10996v1</id>
    <updated>2025-04-15T09:14:18Z</updated>
    <published>2025-04-15T09:14:18Z</published>
    <title>Denoising Application Performance Models with Noise-Resilient Priors</title>
    <summary>  When scaling parallel codes to larger machines, performance models help
identify potential bottlenecks. Since analytically designing these mathematical
representations is usually challenging, empirical models based on performance
measurements offer a practical alternative. Yet, measurements on HPC systems
are typically affected by noise, leading to potentially misleading model
predictions. To reduce the influence of noise, we introduce
application-specific dynamic priors into the modeling process, which we derive
from noise-resilient measurements of computational effort and knowledge of
typical algorithms used in communication routines. These priors then narrow the
search space for our performance models, excluding complexity classes that
reflect noise rather than performance. Our approach keeps the models much
closer to theoretical expectations and significantly improves their predictive
power. Finally, it cuts experimental costs in half by minimizing the number of
repeated measurements.
</summary>
    <author>
      <name>Gustavo de Morais</name>
    </author>
    <author>
      <name>Alexander Geiß</name>
    </author>
    <author>
      <name>Alexandru Calotoiu</name>
    </author>
    <author>
      <name>Gregor Corbin</name>
    </author>
    <author>
      <name>Ahmad Tarraf</name>
    </author>
    <author>
      <name>Torsten Hoefler</name>
    </author>
    <author>
      <name>Bernd Mohr</name>
    </author>
    <author>
      <name>Felix Wolf</name>
    </author>
    <link href="http://arxiv.org/abs/2504.10996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.10996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.16469v1</id>
    <updated>2025-04-23T07:29:32Z</updated>
    <published>2025-04-23T07:29:32Z</published>
    <title>Closed-form analysis of Multi-RIS Reflected Signals in RIS-Aided
  Networks Using Stochastic Geometry</title>
    <summary>  Reconfigurable intelligent surfaces (RISs) enhance wireless communication by
creating engineered signal reflection paths in addition to direct links. This
work presents a stochastic geometry framework using point processes (PPs) to
model multiple randomly deployed RISs conditioned on their associated base
station (BS) locations. By characterizing aggregated reflections from multiple
RISs using the Laplace transform, we analytically assess the performance impact
of RIS-reflected signals by integrating this characterization into
well-established stochastic geometry frameworks. Specifically, we derive
closed-form expressions for the Laplace transform of the reflected signal power
in several deployment scenarios. These analytical results facilitate
performance evaluation of RIS-enabled enhancements. Numerical simulations
validate that optimal RIS placement favors proximity to BSs or user equipment
(UEs), and further quantify the impact of reflected interference, various
fading assumptions, and diverse spatial deployment strategies. Importantly, our
analytical approach shows superior computational efficiency compared to Monte
Carlo simulations.
</summary>
    <author>
      <name>Guodong Sun</name>
    </author>
    <author>
      <name>Francois Baccelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for the SpaSWiN 2025 workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.16469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.16469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.19069v1</id>
    <updated>2025-04-27T01:31:58Z</updated>
    <published>2025-04-27T01:31:58Z</published>
    <title>Performance Analysis of OpenVPN on a Consumer Grade Router</title>
    <summary>  Virtual Private Networks (VPNs) offer an alternative solution using Internet
Protocol (IP) tunnels to create secure, encrypted communication between
geographically distant networks using a common shared medium such as the
Internet. They use tunneling to establish end-to-end connectivity. OpenVPN is a
cross-platform, secure, highly configurable VPN solution. Security in OpenVPN
is handled by the OpenSSL cryptographic library which provides strong security
over a Secure Socket Layer (SSL) using standard algorithms such as Advanced
Encryption Standard (AES), Blowfish, or Triple DES (3DES). The Linksys WRT54GL
router is a consumer-grade router made by Linksys, a division of Cisco Systems,
capable of running under Linux. The Linux-based DD-WRT open-source router
firmware can run OpenVPN on the Linksys WRT54GL router. For this case study,
the performance of OpenVPN is measured and analyzed using a $2^{k-p}$
fractional factorial design for 5 minus 1 factors where $k=5$ and $p=1$. The
results show that the throughput is mainly limited by the encryption cipher
used, and that the round-trip time (RTT) is mostly dependent on the transport
protocol selected.
</summary>
    <author>
      <name>Michael J. Hall</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Washington University in St. Louis</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures. Survey paper for CSE 567M (Computer Systems
  Analysis), Washington University in St. Louis, Nov. 24, 2008. Original
  version available at www.cse.wustl.edu/~jain/cse567-08/ftp/ovpn/index.html</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.19069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.19069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; C.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.19171v1</id>
    <updated>2025-04-27T09:26:15Z</updated>
    <published>2025-04-27T09:26:15Z</published>
    <title>GPU-Accelerated Parallel Selected Inversion for Structured Matrices
  Using sTiles</title>
    <summary>  Selected inversion is essential for applications such as Bayesian inference,
electronic structure calculations, and inverse covariance estimation, where
computing only specific elements of large sparse matrix inverses significantly
reduces computational and memory overhead. We present an efficient
implementation of a two-phase parallel algorithm for computing selected
elements of the inverse of a sparse symmetric matrix A, which can be expressed
as A = LL^T through sparse Cholesky factorization. Our approach leverages a
tile-based structure, focusing on selected dense tiles to optimize
computational efficiency and parallelism. While the focus is on arrowhead
matrices, the method can be extended to handle general structured matrices.
Performance evaluations on a dual-socket 26-core Intel Xeon CPU server
demonstrate that sTiles outperforms state-of-the-art direct solvers such as
Panua-PARDISO, achieving up to 13X speedup on large-scale structured matrices.
Additionally, our GPU implementation using an NVIDIA A100 GPU demonstrates
substantial acceleration over its CPU counterpart, achieving up to 5X speedup
for large, high-bandwidth matrices with high computational intensity. These
results underscore the robustness and versatility of sTiles, validating its
effectiveness across various densities and problem configurations.
</summary>
    <author>
      <name>Esmail Abdul Fattah</name>
    </author>
    <author>
      <name>Hatem Ltaief</name>
    </author>
    <author>
      <name>Havard Rue</name>
    </author>
    <author>
      <name>David Keyes</name>
    </author>
    <link href="http://arxiv.org/abs/2504.19171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.19171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.20348v2</id>
    <updated>2025-05-02T19:16:14Z</updated>
    <published>2025-04-29T01:37:08Z</published>
    <title>CarbonCall: Sustainability-Aware Function Calling for Large Language
  Models on Edge Devices</title>
    <summary>  Large Language Models (LLMs) enable real-time function calling in edge AI
systems but introduce significant computational overhead, leading to high power
consumption and carbon emissions. Existing methods optimize for performance
while neglecting sustainability, making them inefficient for energy-constrained
environments. We introduce CarbonCall, a sustainability-aware function-calling
framework that integrates dynamic tool selection, carbon-aware execution, and
quantized LLM adaptation. CarbonCall adjusts power thresholds based on
real-time carbon intensity forecasts and switches between model variants to
sustain high tokens-per-second throughput under power constraints. Experiments
on an NVIDIA Jetson AGX Orin show that CarbonCall reduces carbon emissions by
up to 52%, power consumption by 30%, and execution time by 30%, while
maintaining high efficiency.
</summary>
    <author>
      <name>Varatheepan Paramanayakam</name>
    </author>
    <author>
      <name>Andreas Karatzas</name>
    </author>
    <author>
      <name>Iraklis Anagnostopoulos</name>
    </author>
    <author>
      <name>Dimitrios Stamoulis</name>
    </author>
    <link href="http://arxiv.org/abs/2504.20348v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.20348v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.02734v1</id>
    <updated>2025-05-05T15:36:31Z</updated>
    <published>2025-05-05T15:36:31Z</published>
    <title>Automotive Middleware Performance: Comparison of FastDDS, Zenoh and
  vSomeIP</title>
    <summary>  In this study, we evaluate the performance of current automotive
communication middlewares under various operating conditions. Specifically, we
examine FastDDS, a widely used open-source middleware, the newly developed
Zenoh middleware, and vSomeIP, COVESAs open-source implementation of SOME/IP.
Our objective is to identify the best performing middleware for specific
operating conditions. To ensure accessibility, we first provide a concise
overview of middleware technologies and their fundamental principles. We then
introduce our testing methodology designed to systematically assess middleware
performance metrics such as scaling performance, end-to-end latency, and
discovery times across multiple message types, network topologies, and
configurations. Finally, we compare the resulting performance data and present
our results in nine findings. Our evaluation code and the resulting data will
be made publicly available upon acceptance.
</summary>
    <author>
      <name>David Philipp Klüner</name>
    </author>
    <author>
      <name>Lucas Hegerath</name>
    </author>
    <author>
      <name>Amin Dieter Hatib</name>
    </author>
    <author>
      <name>Stefan Kowalewski</name>
    </author>
    <author>
      <name>Bassam Alrifaee</name>
    </author>
    <author>
      <name>Alexandru Kampmann</name>
    </author>
    <link href="http://arxiv.org/abs/2505.02734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.02734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.03398v1</id>
    <updated>2025-05-06T10:26:57Z</updated>
    <published>2025-05-06T10:26:57Z</published>
    <title>Benchmark-based Study of CPU/GPU Power-Related Features through JAX and
  TensorFlow</title>
    <summary>  Power management has become a crucial focus in the modern computing
landscape, considering that {\em energy} is increasingly recognized as a
critical resource. This increased the importance of all topics related to {\em
energy-aware computing}. This paper presents an experimental study of three
prevalent power management techniques that are {\em power limitation, frequency
limitation}, and {\em ACPI/P-State governor modes} (OS states related to power
consumption). Through a benchmark approach with a set of six computing kernels,
we investigate {\em power/performance} trade-off with various hardware units
and software frameworks (mainly TensorFlow and JAX). Our experimental results
show that {\em frequency limitation} is the most effective technique to improve
{\em Energy-Delay Product (EDP)}, which is a convolution of energy and running
time. We also observe that running at the highest frequency compared to a
reduced one could lead to a reduction of factor $\frac{1}{10}$ in EDP. Another
noticeable fact is that frequency management shows a consistent behavior with
different CPUs, whereas opposite effects sometimes occur between TensorFlow
(TF) and JAX with the same power management settings.
</summary>
    <author>
      <name>Roblex Nana Tchakoute</name>
    </author>
    <author>
      <name>Claude Tadonki</name>
    </author>
    <author>
      <name>Petr Dokladal</name>
    </author>
    <author>
      <name>Youssef Mesri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.03398v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.03398v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.06085v2</id>
    <updated>2025-05-15T13:07:31Z</updated>
    <published>2025-05-09T14:29:37Z</published>
    <title>Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities</title>
    <summary>  The increasing demand for generative AI as Large Language Models (LLMs)
services has driven the need for specialized hardware architectures that
optimize computational efficiency and energy consumption. This paper evaluates
the performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic
linear algebra kernels at reduced numerical precision, a fundamental operation
in LLM computations. We present a detailed characterization of Grayskull's
execution model, gridsize, matrix dimensions, data formats, and numerical
precision impact computational efficiency. Furthermore, we compare Grayskull's
performance against state-of-the-art architectures with tensor acceleration,
including Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).
Whilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a
competitive trade-off between power consumption and computational throughput,
reaching a peak of 1.55 TFLOPs/Watt with BF16.
</summary>
    <author>
      <name>Hiari Pizzini Cavagna</name>
    </author>
    <author>
      <name>Daniele Cesarini</name>
    </author>
    <author>
      <name>Andrea Bartolini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the Computational Aspects of Deep Learning Workshop at
  ISC High Performance 2025. To appear in the ISC High Performance 2025
  Workshop Proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.06085v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.06085v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.10567v1</id>
    <updated>2025-01-14T17:01:20Z</updated>
    <published>2025-01-14T17:01:20Z</published>
    <title>M|D|$\infty$ Queue Busy Period and Busy Cycle Distributions
  Computational Calculus</title>
    <summary>  Given the busy period and busy cycle major importance in queuing systems, it
is crucial the knowledge of the respective distribution functions that is what
allows the calculation of the important probabilities. For the M|G|$\infty$
queue system, there are no round form formulae for those distribution
functions. But, for the M|D|$\infty$ queue, due the fact that its busy period
and busy cycle have both Laplace transform expression round forms, what does
not happen for any other M|G|$\infty$ queue system, with an algorithm created
by Platzman, Ammons and Bartholdi III, that allows the tail probabilities
computation since the correspondent Laplace transform in round form is known,
those distribution functions calculations are possible. Here, we will implement
the algorithm through a FORTRAN program.
</summary>
    <author>
      <name>Manuel Alberto M. Ferreira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.12988/imf.2014.311249</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.12988/imf.2014.311249" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages an no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.10567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.10567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.12885v1</id>
    <updated>2025-05-19T09:15:48Z</updated>
    <published>2025-05-19T09:15:48Z</published>
    <title>Effects of the Auto-Correlation of Delays on the Age of Information: A
  Gaussian Process Framework</title>
    <summary>  The age of information (AoI) has been studied actively in recent years as a
performance measure for systems that require real-time performance, such as
remote monitoring systems via communication networks. The theoretical analysis
of the AoI is usually formulated based on explicit system modeling, such as a
single-server queueing model. However, in general, the behavior of large-scale
systems such as communication networks is complex, and it is usually difficult
to express the delay using simple queueing models. In this paper, we consider a
framework in which the sequence of delays is composed from a non-negative
continuous-time stochastic process, called a virtual delay process, as a new
modeling approach for the theoretical analysis of the AoI. Under such a
framework, we derive an expression for the transient probability distribution
of the AoI and further apply the theory of stochastic orders to prove that the
high dependence of the sequence of delays leads to the degradation of AoI
performance. We further consider a special case in which the sequence of delays
is generated from a stationary Gaussian process, and we discuss the sensitivity
of the AoI to second-order statistics of the delay process through numerical
experiments.
</summary>
    <author>
      <name>Atsushi Inoie</name>
    </author>
    <author>
      <name>Yoshiaki Inoue</name>
    </author>
    <link href="http://arxiv.org/abs/2505.12885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.12885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.14022v1</id>
    <updated>2025-05-20T07:25:23Z</updated>
    <published>2025-05-20T07:25:23Z</published>
    <title>Towards Efficient Multi-Scale Deformable Attention on NPU</title>
    <summary>  Multi-scale deformable attention (MSDA) is a flexible and powerful feature
extraction mechanism for visual tasks, but its random-access grid sampling
strategy poses significant optimization challenges, especially on
domain-specific accelerators such as NPUs. In this work, we present a co-design
approach that systematically rethinks memory access and computation strategies
for MSDA on the Ascend NPU architecture. With this co-design approach, our
implementation supports both efficient forward and backward computation, is
fully adapted for training workloads, and incorporates a suite of
hardware-aware optimizations. Extensive experiments show that our solution
achieves up to $5.9\times$ (forward), $8.9\times$ (backward), and $7.3\times$
(end-to-end training) speedup over the grid sample-based baseline, and
$1.9\times$, $2.4\times$, and $2.0\times$ acceleration over the latest vendor
library, respectively.
</summary>
    <author>
      <name>Chenghuan Huang</name>
    </author>
    <author>
      <name>Zhigeng Xu</name>
    </author>
    <author>
      <name>Chong Sun</name>
    </author>
    <author>
      <name>Chen Li</name>
    </author>
    <author>
      <name>Ziyang Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.14022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.14022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.15595v1</id>
    <updated>2025-05-21T14:50:09Z</updated>
    <published>2025-05-21T14:50:09Z</published>
    <title>A Methodology to Evaluate Strategies Predicting Rankings on Unseen
  Domains</title>
    <summary>  Frequently, multiple entities (methods, algorithms, procedures, solutions,
etc.) can be developed for a common task and applied across various domains
that differ in the distribution of scenarios encountered. For example, in
computer vision, the input data provided to image analysis methods depend on
the type of sensor used, its location, and the scene content. However, a
crucial difficulty remains: can we predict which entities will perform best in
a new domain based on assessments on known domains, without having to carry out
new and costly evaluations? This paper presents an original methodology to
address this question, in a leave-one-domain-out fashion, for various
application-specific preferences. We illustrate its use with 30 strategies to
predict the rankings of 40 entities (unsupervised background subtraction
methods) on 53 domains (videos).
</summary>
    <author>
      <name>Sébastien Piérard</name>
    </author>
    <author>
      <name>Adrien Deliège</name>
    </author>
    <author>
      <name>Anaïs Halin</name>
    </author>
    <author>
      <name>Marc Van Droogenbroeck</name>
    </author>
    <link href="http://arxiv.org/abs/2505.15595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.15595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809004v1</id>
    <updated>1998-09-02T00:49:25Z</updated>
    <published>1998-09-02T00:49:25Z</published>
    <title>Performance / Price Sort</title>
    <summary>  NTsort is an external sort on WindowsNT 5.0. It has minimal functionality but
excellent price performance. In particular, running on mail-order hardware it
can sort 1.5 GB for a penny. For commercially available sorts, Postman Sort
from Robert Ramey Software Development has elapsed time performance comparable
to NTsort, while using less processor time. It can sort 1.27 GB for a penny
(12.7 million records.) These sorts set new price-performance records. This
paper documents this and proposes that the PennySort benchmark be revised to
Performance/Price sort: a simple GB/$ sort metric based on a two-pass external
sort.
</summary>
    <author>
      <name>Jim Gray</name>
    </author>
    <author>
      <name>Joshua Coates</name>
    </author>
    <author>
      <name>Chris Nyberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Original word file at:
  http://research.microsoft.com/~gray/PennySort.doc</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9809004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.5;H.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9810010v2</id>
    <updated>1998-11-02T21:39:47Z</updated>
    <published>1998-10-09T20:29:43Z</published>
    <title>C++ Templates as Partial Evaluation</title>
    <summary>  This paper explores the relationship between C++ templates and partial
evaluation. Templates were designed to support generic programming, but
unintentionally provided the ability to perform compile-time computations and
code generation. These features are completely accidental, and as a result
their syntax is awkward. By recasting these features in terms of partial
evaluation, a much simpler syntax can be achieved. C++ may be regarded as a
two-level language in which types are first-class values. Template
instantiation resembles an offline partial evaluator. This paper describes
preliminary work toward a single mechanism based on Partial Evaluation which
unifies generic programming, compile-time computation and code generation. The
language Catat is introduced to illustrate these ideas.
</summary>
    <author>
      <name>Todd L. Veldhuizen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9810010v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9810010v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.3.2; D.3.3; D.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9904016v1</id>
    <updated>1999-04-22T15:47:22Z</updated>
    <published>1999-04-22T15:47:22Z</published>
    <title>Brittle System Analysis</title>
    <summary>  The goal of this paper is to define and analyze systems which exhibit brittle
behavior. This behavior is characterized by a sudden and steep decline in
performance as the system approaches the limits of tolerance. This can be due
to input parameters which exceed a specified input, or environmental conditions
which exceed specified operating boundaries. An analogy is made between brittle
commmunication systems in particular and materials science.
</summary>
    <author>
      <name>Stephen F. Bush</name>
    </author>
    <author>
      <name>John Hershey</name>
    </author>
    <author>
      <name>Kirby Vosburgh</name>
    </author>
    <link href="http://arxiv.org/abs/cs/9904016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9904016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2;C.4;B.8;F.2;H.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0209029v1</id>
    <updated>2002-09-25T08:50:18Z</updated>
    <published>2002-09-25T08:50:18Z</published>
    <title>A generalization of Amdahl's law and relative conditions of parallelism</title>
    <summary>  In this work I present a generalization of Amdahl's law on the limits of a
parallel implementation with many processors. In particular I establish some
mathematical relations involving the number of processors and the dimension of
the treated problem, and with these conditions I define, on the ground of the
reachable speedup, some classes of parallelism for the implementations. I also
derive a condition for obtaining superlinear speedup. The used mathematical
technics are those of differential calculus. I describe some examples from
classical problems offered by the specialized literature on the subject.
</summary>
    <author>
      <name>Gianluca Argentini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0209029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0209029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.2; D.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0309018v1</id>
    <updated>2003-09-11T18:37:09Z</updated>
    <published>2003-09-11T18:37:09Z</published>
    <title>Using Propagation for Solving Complex Arithmetic Constraints</title>
    <summary>  Solving a system of nonlinear inequalities is an important problem for which
conventional numerical analysis has no satisfactory method. With a
box-consistency algorithm one can compute a cover for the solution set to
arbitrarily close approximation. Because of difficulties in the use of
propagation for complex arithmetic expressions, box consistency is computed
with interval arithmetic. In this paper we present theorems that support a
simple modification of propagation that allows complex arithmetic expressions
to be handled efficiently. The version of box consistency that is obtained in
this way is stronger than when interval arithmetic is used.
</summary>
    <author>
      <name>M. H. van Emden</name>
    </author>
    <author>
      <name>B. Moa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0309018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0309018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8; G.1.5;G.1.6;I.2.9;I.3.1;C.1.4;D.2.4;F.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0407062v1</id>
    <updated>2004-07-28T14:07:58Z</updated>
    <published>2004-07-28T14:07:58Z</published>
    <title>Performance Analysis of the Globus Toolkit Monitoring and Discovery
  Service, MDS2</title>
    <summary>  Monitoring and information services form a key component of a distributed
system, or Grid. A quantitative study of such services can aid in understanding
the performance limitations, advise in the deployment of the monitoring system,
and help evaluate future development work. To this end, we examined the
performance of the Globus Toolkit(reg. trdmrk) Monitoring and Discovery Service
(MDS2) by instrumenting its main services using NetLogger. Our study shows a
strong advantage to caching or prefetching the data, as well as the need to
have primary components at well-connected sites.
</summary>
    <author>
      <name>Xuehai Zhang</name>
    </author>
    <author>
      <name>Jennifer M. Schopf</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0407062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0407062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.4; H.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0501073v1</id>
    <updated>2005-01-25T13:28:38Z</updated>
    <published>2005-01-25T13:28:38Z</published>
    <title>Optimal Union-Find in Constraint Handling Rules</title>
    <summary>  Constraint Handling Rules (CHR) is a committed-choice rule-based language
that was originally intended for writing constraint solvers. In this paper we
show that it is also possible to write the classic union-find algorithm and
variants in CHR. The programs neither compromise in declarativeness nor
efficiency. We study the time complexity of our programs: they match the
almost-linear complexity of the best known imperative implementations. This
fact is illustrated with experimental results.
</summary>
    <author>
      <name>Tom Schrijvers</name>
    </author>
    <author>
      <name>Thom Fruehwirth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures, to appear in Theory and Practice of Logic
  Programming (TPLP)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0501073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0501073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502003v1</id>
    <updated>2005-02-01T12:23:26Z</updated>
    <published>2005-02-01T12:23:26Z</published>
    <title>Shawn: A new approach to simulating wireless sensor networks</title>
    <summary>  We consider the simulation of wireless sensor networks (WSN) using a new
approach. We present Shawn, an open-source discrete-event simulator that has
considerable differences to all other existing simulators. Shawn is very
powerful in simulating large scale networks with an abstract point of view. It
is, to the best of our knowledge, the first simulator to support generic
high-level algorithms as well as distributed protocols on exactly the same
underlying networks.
</summary>
    <author>
      <name>Alexander Kroeller</name>
    </author>
    <author>
      <name>Dennis Pfisterer</name>
    </author>
    <author>
      <name>Carsten Buschmann</name>
    </author>
    <author>
      <name>Sandor P. Fekete</name>
    </author>
    <author>
      <name>Stefan Fischer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures, 2 tables, Latex, to appear in Design, Analysis,
  and Simulation of Distributed Systems 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0502003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.7, D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502009v1</id>
    <updated>2005-02-02T03:26:40Z</updated>
    <published>2005-02-02T03:26:40Z</published>
    <title>Performance Considerations for Gigabyte per Second Transcontinental
  Disk-to-Disk File Transfers</title>
    <summary>  Moving data from CERN to Pasadena at a gigabyte per second using the next
generation Internet requires good networking and good disk IO. Ten Gbps
Ethernet and OC192 links are in place, so now it is simply a matter of
programming. This report describes our preliminary work and measurements in
configuring the disk subsystem for this effort. Using 24 SATA disks at each
endpoint we are able to locally read and write an NTFS volume is striped across
24 disks at 1.2 GBps. A 32-disk stripe delivers 1.7 GBps. Experiments on higher
performance and higher-capacity systems deliver up to 3.5 GBps.
</summary>
    <author>
      <name>Peter Kukol</name>
    </author>
    <author>
      <name>Jim Gray</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0502009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0602061v1</id>
    <updated>2006-02-16T22:33:04Z</updated>
    <published>2006-02-16T22:33:04Z</published>
    <title>The Computational and Storage Potential of Volunteer Computing</title>
    <summary>  "Volunteer computing" uses Internet-connected computers, volunteered by their
owners, as a source of computing power and storage. This paper studies the
potential capacity of volunteer computing. We analyzed measurements of over
330,000 hosts participating in a volunteer computing project. These
measurements include processing power, memory, disk space, network throughput,
host availability, user-specified limits on resource usage, and host churn. We
show that volunteer computing can support applications that are significantly
more data-intensive, or have larger memory and storage requirements, than those
in current projects.
</summary>
    <author>
      <name>David P. Anderson</name>
    </author>
    <author>
      <name>Gilles Fedak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. To appear in CCGrid 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0602061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0602061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605030v1</id>
    <updated>2006-05-08T08:04:20Z</updated>
    <published>2006-05-08T08:04:20Z</published>
    <title>A Delay Analysis of Maximal Matching Switching with Speedup</title>
    <summary>  In this paper we analyze the average queue backlog in a combined input-output
queued switch using a maximal size matching scheduling algorithm. We compare
this average backlog to the average backlog achieved by an optimal switch. We
model the cell arrival process as independent and identically distributed
between time slots and uniformly distributed among input and output ports. For
switches with many input and output ports, the backlog associated with maximal
size matching with speedup 3 is no more than 10/3 times the backlog associated
with an optimal switch. Moreover, this performance ratio rapidly approaches 2
as speedup increases.
</summary>
    <author>
      <name>Randy Cogill</name>
    </author>
    <author>
      <name>Sanjay Lall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures. Submitted to the 2006 IEEE Conference on
  Decision and Control</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0605030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612048v1</id>
    <updated>2006-12-08T00:44:26Z</updated>
    <published>2006-12-08T00:44:26Z</published>
    <title>Queue Model of Leaf Degree Keeping Process in Gnutella Network</title>
    <summary>  Leaf degree keeping process of Gnutella is discussed in this paper. Queue
system based on rules of Gnutella protocol are introduced to modeling this
process. The leaf degree distributions resulted from the queue system and from
our real measurement are compared. The well match of those distributions reveal
that the leaf degree distribution in Gnutella network should not be power law
or power law like as reported before. It is more likely a distribution driven
by certain queue process specified by the protocol.
</summary>
    <author>
      <name>chunxi li</name>
    </author>
    <author>
      <name>changjia chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0612048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701195v1</id>
    <updated>2007-01-30T17:14:52Z</updated>
    <published>2007-01-30T17:14:52Z</published>
    <title>An Abstract Monte-Carlo Method for the Analysis of Probabilistic
  Programs</title>
    <summary>  We introduce a new method, combination of random testing and abstract
interpretation, for the analysis of programs featuring both probabilistic and
non-probabilistic nondeterminism. After introducing "ordinary" testing, we show
how to combine testing and abstract interpretation and give formulas linking
the precision of the results to the number of iterations. We then discuss
complexity and optimization issues and end with some experimental results.
</summary>
    <author>
      <name>David Monniaux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIENS</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/360204.360211</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/360204.360211" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">POPL: Annual Symposium on Principles of Programming Languages
  (2001) 93 - 101</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0701195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702062v1</id>
    <updated>2007-02-11T09:52:12Z</updated>
    <published>2007-02-11T09:52:12Z</published>
    <title>Noise Limited Computational Speed</title>
    <summary>  In modern transistor based logic gates, the impact of noise on computation
has become increasingly relevant since the voltage scaling strategy, aimed at
decreasing the dissipated power, has increased the probability of error due to
the reduced switching threshold voltages. In this paper we discuss the role of
noise in a two state model that mimic the dynamics of standard logic gates and
show that the presence of the noise sets a fundamental limit to the computing
speed. An optimal idle time interval that minimizes the error probability, is
derived.
</summary>
    <author>
      <name>Luca Gammaitoni</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.2817968</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.2817968" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">L. Gammaitoni, Applied Physics Letters, 11/2007, Volume 91, p.3,
  (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0702062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.1.3; B.7.0; B.8.1; B.8.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/physics/0405154v1</id>
    <updated>2004-05-29T10:05:51Z</updated>
    <published>2004-05-29T10:05:51Z</published>
    <title>The ATLAS Tile Calorimeter Test Beam Monitoring Program</title>
    <summary>  During 2003 test beam session for ATLAS Tile Calorimeter a monitoring program
has been developed to ease the setup of correct running condition and the
assessment of data quality. The program has been built using the Online
Software services provided by the ATLAS Online Software group. The first part
of this note contains a brief overview of these services followed by the full
description of Tile Calorimeter monitoring program architecture and features.
Performances and future upgrades are discussed in the final part of this note.
</summary>
    <author>
      <name>Paolo Adragna</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Pisa and Istituto Nazionale di Fisica Nucleare, Sezione di Pisa</arxiv:affiliation>
    </author>
    <author>
      <name>Andrea Dotti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Pisa and Istituto Nazionale di Fisica Nucleare, Sezione di Pisa</arxiv:affiliation>
    </author>
    <author>
      <name>Chiara Roda</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Pisa and Istituto Nazionale di Fisica Nucleare, Sezione di Pisa</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 8 figures, ATLAS TILECAL Note</arxiv:comment>
    <link href="http://arxiv.org/abs/physics/0405154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0405154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.1455v1</id>
    <updated>2007-10-08T01:56:20Z</updated>
    <published>2007-10-08T01:56:20Z</published>
    <title>Superrecursive Features of Interactive Computation</title>
    <summary>  Functioning and interaction of distributed devices and concurrent algorithms
are analyzed in the context of the theory of algorithms. Our main concern here
is how and under what conditions algorithmic interactive devices can be more
powerful than the recursive models of computation, such as Turing machines.
Realization of such a higher computing power makes these systems
superrecursive. We find here five sources for superrecursiveness in
interaction. In addition, we prove that when all of these sources are excluded,
the algorithmic interactive system in question is able to perform only
recursive computations. These results provide computer scientists with
necessary and sufficient conditions for achieving superrecursiveness by
algorithmic interactive devices.
</summary>
    <author>
      <name>Mark Burgin</name>
    </author>
    <link href="http://arxiv.org/abs/0710.1455v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.1455v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; F.1.2; I.2.11; C.1.4; C.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4486v1</id>
    <updated>2007-10-24T14:48:39Z</updated>
    <published>2007-10-24T14:48:39Z</published>
    <title>Non-linear estimation is easy</title>
    <summary>  Non-linear state estimation and some related topics, like parametric
estimation, fault diagnosis, and perturbation attenuation, are tackled here via
a new methodology in numerical differentiation. The corresponding basic system
theoretic definitions and properties are presented within the framework of
differential algebra, which permits to handle system variables and their
derivatives of any order. Several academic examples and their computer
simulations, with on-line estimations, are illustrating our viewpoint.
</summary>
    <author>
      <name>Michel Fliess</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Cédric Join</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs, CRAN</arxiv:affiliation>
    </author>
    <author>
      <name>Hebertt Sira-Ramirez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1504/IJMIC.2008.020996</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1504/IJMIC.2008.020996" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Int. J. Modelling Identification and Control 4, 1 (2008) 12-27</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.2302v2</id>
    <updated>2008-01-28T16:18:50Z</updated>
    <published>2007-12-14T08:14:20Z</published>
    <title>Data access optimizations for highly threaded multi-core CPUs with
  multiple memory controllers</title>
    <summary>  Processor and system architectures that feature multiple memory controllers
are prone to show bottlenecks and erratic performance numbers on codes with
regular access patterns. Although such effects are well known in the form of
cache thrashing and aliasing conflicts, they become more severe when memory
access is involved. Using the new Sun UltraSPARC T2 processor as a prototypical
multi-core design, we analyze performance patterns in low-level and application
benchmarks and show ways to circumvent bottlenecks by careful data layout and
padding.
</summary>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Thomas Zeiser</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures. Accepted for Workshop on Large-Scale Parallel
  Processing 2008. Revised and extended version</arxiv:comment>
    <link href="http://arxiv.org/abs/0712.2302v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.2302v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.1177v1</id>
    <updated>2008-09-06T15:06:53Z</updated>
    <published>2008-09-06T15:06:53Z</published>
    <title>Amdahl's and Gustafson-Barsis laws revisited</title>
    <summary>  The paper presents a simple derivation of the Gustafson-Barsis law from the
Amdahl's law. In the computer literature these two laws describing the speedup
limits of parallel applications are derived separately. It is shown, that
treating the time of the execution of the sequential part of the application as
a constant, in few lines the Gustafson-Barsis law can be obtained from the
Amdahl's law and that the popular claim, that Gustafson-Barsis law overthrows
Amdahl's law is a mistake.
</summary>
    <author>
      <name>Andrzej Karbowski</name>
    </author>
    <link href="http://arxiv.org/abs/0809.1177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.1177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.1571v1</id>
    <updated>2008-10-09T04:42:27Z</updated>
    <published>2008-10-09T04:42:27Z</published>
    <title>An Analytical Model of Information Dissemination for a Gossip-based
  Protocol</title>
    <summary>  We develop an analytical model of information dissemination for a gossiping
protocol that combines both pull and push approaches. With this model we
analyse how fast an item is replicated through a network, and how fast the item
spreads in the network, and how fast the item covers the network. We also
determine the optimal size of the exchange buffer, to obtain fast replication.
Our results are confirmed by large-scale simulation experiments.
</summary>
    <author>
      <name>Rena Bakhshi</name>
    </author>
    <author>
      <name>Daniela Gavidia</name>
    </author>
    <author>
      <name>Wan Fokkink</name>
    </author>
    <author>
      <name>Maarten van Steen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.comnet.2009.03.017</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.comnet.2009.03.017" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 8 figures, technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.1571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.1571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.2736v1</id>
    <updated>2009-02-16T14:26:36Z</updated>
    <published>2009-02-16T14:26:36Z</published>
    <title>Random Fruits on the Zielonka Tree</title>
    <summary>  Stochastic games are a natural model for the synthesis of controllers
confronted to adversarial and/or random actions. In particular,
$\omega$-regular games of infinite length can represent reactive systems which
are not expected to reach a correct state, but rather to handle a continuous
stream of events. One critical resource in such applications is the memory used
by the controller. In this paper, we study the amount of memory that can be
saved through the use of randomisation in strategies, and present matching
upper and lower bounds for stochastic Muller games.
</summary>
    <author>
      <name>Florian Horn</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">26th International Symposium on Theoretical Aspects of Computer
  Science - STACS 2009 (2009) 541-552</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0902.2736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.2736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.4822v1</id>
    <updated>2009-02-27T12:47:22Z</updated>
    <published>2009-02-27T12:47:22Z</published>
    <title>Lightweight Task Analysis for Cache-Aware Scheduling on Heterogeneous
  Clusters</title>
    <summary>  We present a novel characterization of how a program stresses cache. This
characterization permits fast performance prediction in order to simulate and
assist task scheduling on heterogeneous clusters. It is based on the estimation
of stack distance probability distributions. The analysis requires the
observation of a very small subset of memory accesses, and yields a reasonable
to very accurate prediction in constant time.
</summary>
    <author>
      <name>Xavier Grehant</name>
    </author>
    <author>
      <name>Sverre Jarp</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper was originally published in: ISBN #: 1-60132-084-1 (a
  two-volume set) Proceedings of the 2008 International Conference on Parallel
  and Distributed Processing Techniques and Applications (PDPTA'08) Editors:
  Hamid R. Arabnia and Youngsong Mun</arxiv:comment>
    <link href="http://arxiv.org/abs/0902.4822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.4822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.1738v3</id>
    <updated>2010-07-28T22:28:53Z</updated>
    <published>2009-05-11T23:57:58Z</published>
    <title>Information Ranking and Power Laws on Trees</title>
    <summary>  We study the situations when the solution to a weighted stochastic recursion
has a power law tail. To this end, we develop two complementary approaches, the
first one extends Goldie's (1991) implicit renewal theorem to cover recursions
on trees; and the second one is based on a direct sample path large deviations
analysis of weighted recursive random sums. We believe that these methods may
be of independent interest in the analysis of more general weighted branching
processes as well as in the analysis of algorithms.
</summary>
    <author>
      <name>Predrag R. Jelenkovic</name>
    </author>
    <author>
      <name>Mariana Olvera-Cravioto</name>
    </author>
    <link href="http://arxiv.org/abs/0905.1738v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.1738v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60H25, 60J80" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.3710v1</id>
    <updated>2009-07-21T17:35:44Z</updated>
    <published>2009-07-21T17:35:44Z</published>
    <title>Throughput metrics and packet delay in TCP/IP networks</title>
    <summary>  In the paper the method for estimation of throughput metrics like available
bandwidth and end-t-end capacity is supposed. This method is based on
measurement of network delay $D_i$ for packets of different sizes $W_i$. The
simple expression for available bandwidth $B_{av} =(W_2-W_1)/(D_2-D_1)$ is
substantiated. The number of experiments on matching of the results received
new and traditional methods is spent. The received results testify to
possibility of application of new model.
</summary>
    <author>
      <name>A. V. Sukhov</name>
    </author>
    <author>
      <name>T. G. Sultanov</name>
    </author>
    <author>
      <name>M. V. Strizhov</name>
    </author>
    <author>
      <name>A. P. Platonov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">RIPE 59, Lissabon, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.3710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.3710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.3; C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.4934v1</id>
    <updated>2009-09-27T14:20:46Z</updated>
    <published>2009-09-27T14:20:46Z</published>
    <title>Characteristics of multithreading models for high-performance IO driven
  network applications</title>
    <summary>  In a technological landscape that is quickly moving toward dense multi-CPU
and multi-core computer systems, where using multithreading is an increasingly
popular application design decision, it is important to choose a proper model
for distributing tasks across multiple threads that will result in the best
efficiency for the application and the system as a whole. The work described in
this paper creates, implements and evaluates various models of distributing
tasks to CPU threads and investigates their characteristics for use in modern
high-performance network servers. The results presented here comprise a roadmap
of models for building multithreaded server applications for modern server
hardware and Unix-like operating systems.
</summary>
    <author>
      <name>Ivan Voras</name>
    </author>
    <author>
      <name>Mario Zagar</name>
    </author>
    <link href="http://arxiv.org/abs/0909.4934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.4934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.3; D.2.11; D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.2582v1</id>
    <updated>2009-10-14T12:18:28Z</updated>
    <published>2009-10-14T12:18:28Z</published>
    <title>Scalable Distributed-Memory External Sorting</title>
    <summary>  We engineer algorithms for sorting huge data sets on massively parallel
machines. The algorithms are based on the multiway merging paradigm. We first
outline an algorithm whose I/O requirement is close to a lower bound. Thus, in
contrast to naive implementations of multiway merging and all other approaches
known to us, the algorithm works with just two passes over the data even for
the largest conceivable inputs. A second algorithm reduces communication
overhead and uses more conventional specifications of the result at the cost of
slightly increased I/O requirements. An implementation wins the well known
sorting benchmark in several categories and by a large margin over its
competitors.
</summary>
    <author>
      <name>Mirko Rahn</name>
    </author>
    <author>
      <name>Peter Sanders</name>
    </author>
    <author>
      <name>Johannes Singler</name>
    </author>
    <link href="http://arxiv.org/abs/0910.2582v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.2582v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; E.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.1504v1</id>
    <updated>2009-11-08T09:59:15Z</updated>
    <published>2009-11-08T09:59:15Z</published>
    <title>Throughput Limits of IEEE 802.11 and IEEE 802.15.3</title>
    <summary>  IEEE 802.11 and IEEE 802.15.3 are wireless standards originally designed for
wireless local area network (WLAN) and wireless personal area network (WPAN).
This paper studies MAC throughput analysis of both standards. We present a
comparative analysis of both standards in terms of MAC throughput and bandwidth
efficiency. Numerical results show that the performance of IEEE 802.15.3
transcends IEEE 802.11 in all cases.
</summary>
    <author>
      <name>Sana Ullah</name>
    </author>
    <author>
      <name>Yingji Zhong</name>
    </author>
    <author>
      <name>S. M. Riazul Islam</name>
    </author>
    <author>
      <name>Ahasanun Nessa</name>
    </author>
    <author>
      <name>Kyung Sup Kwak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/WiCom.2008.731</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/WiCom.2008.731" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures, 1 table, 4th International Conference on Wireless
  Communications, Networking and Mobile Computing, 2008. WiCOM '08</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.1504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.1504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.4033v1</id>
    <updated>2009-11-20T12:56:49Z</updated>
    <published>2009-11-20T12:56:49Z</published>
    <title>Extending Firewall Session Table to Accelerate NAT, QoS Classification
  and Routing</title>
    <summary>  security and QoS are the two most precious objectives for network systems to
be attained. Unfortunately, they are in conflict, while QoS tries to minimize
processing delay, strong security protection requires more processing time and
cause packet delay. This article is a step towards resolving this conflict by
extending the firewall session table to accelerate NAT, QoS classification, and
routing processing time while providing the same level of security protection.
Index Terms ? stateful packet filtering; firewall; session/state table; QoS;
NAT; Routing.
</summary>
    <author>
      <name>Mahmoud Mostafa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Anas Abou El Kalam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Christian Fraboul</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">19th International Conference on Computer Theory and Applications
  (ICCTA 2009), Alexandria : Egypt (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.4033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.4033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.1897v1</id>
    <updated>2009-12-10T12:01:37Z</updated>
    <published>2009-12-10T12:01:37Z</published>
    <title>Adaptive Scheduling of Data Paths using Uppaal Tiga</title>
    <summary>  We apply Uppaal Tiga to automatically compute adaptive scheduling strategies
for an industrial case study dealing with a state-of-the-art image processing
pipeline of a printer. As far as we know, this is the first application of
timed automata technology to an industrial scheduling problem with uncertainty
in job arrivals.
</summary>
    <author>
      <name>Israa AlAttili</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Radboud University Nijmegen</arxiv:affiliation>
    </author>
    <author>
      <name>Fred Houben</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Radboud University Nijmegen</arxiv:affiliation>
    </author>
    <author>
      <name>Georgeta Igna</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Radboud University Nijmegen</arxiv:affiliation>
    </author>
    <author>
      <name>Steffen Michels</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Radboud University Nijmegen</arxiv:affiliation>
    </author>
    <author>
      <name>Feng Zhu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Radboud University Nijmegen</arxiv:affiliation>
    </author>
    <author>
      <name>Frits Vaandrager</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Radboud University Nijmegen</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.13.1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.13.1" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 13, 2009, pp. 1-11</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.1897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.1897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.1899v1</id>
    <updated>2009-12-10T01:51:14Z</updated>
    <published>2009-12-10T01:51:14Z</published>
    <title>Markovian Testing Equivalence and Exponentially Timed Internal Actions</title>
    <summary>  In the theory of testing for Markovian processes developed so far,
exponentially timed internal actions are not admitted within processes. When
present, these actions cannot be abstracted away, because their execution takes
a nonzero amount of time and hence can be observed. On the other hand, they
must be carefully taken into account, in order not to equate processes that are
distinguishable from a timing viewpoint. In this paper, we recast the
definition of Markovian testing equivalence in the framework of a Markovian
process calculus including exponentially timed internal actions. Then, we show
that the resulting behavioral equivalence is a congruence, has a sound and
complete axiomatization, has a modal logic characterization, and can be decided
in polynomial time.
</summary>
    <author>
      <name>Marco Bernardo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.13.2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.13.2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 13, 2009, pp. 13-25</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.1899v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.1899v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.1902v1</id>
    <updated>2009-12-10T01:52:07Z</updated>
    <published>2009-12-10T01:52:07Z</published>
    <title>Strong, Weak and Branching Bisimulation for Transition Systems and
  Markov Reward Chains: A Unifying Matrix Approach</title>
    <summary>  We first study labeled transition systems with explicit successful
termination. We establish the notions of strong, weak, and branching
bisimulation in terms of boolean matrix theory, introducing thus a novel and
powerful algebraic apparatus. Next we consider Markov reward chains which are
standardly presented in real matrix theory. By interpreting the obtained matrix
conditions for bisimulations in this setting, we automatically obtain the
definitions of strong, weak, and branching bisimulation for Markov reward
chains. The obtained strong and weak bisimulations are shown to coincide with
some existing notions, while the obtained branching bisimulation is new, but
its usefulness is questionable.
</summary>
    <author>
      <name>Nikola Trčka</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Eindhoven University of Technology</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.13.5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.13.5" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 13, 2009, pp. 55-65</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.1902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.1902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.1; D.2.1; D.2.4; F.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.2128v1</id>
    <updated>2009-12-10T23:40:35Z</updated>
    <published>2009-12-10T23:40:35Z</published>
    <title>Proceedings First Workshop on Quantitative Formal Methods: Theory and
  Applications</title>
    <summary>  This volume contains the papers presented at the 1st workshop on Quantitative
Formal Methods: Theory and Applications, which was held in Eindhoven on 3
November 2009 as part of the International Symposium on Formal Methods 2009.
This volume contains the final versions of all contributions accepted for
presentation at the workshop.
</summary>
    <author>
      <name>Suzana Andova</name>
    </author>
    <author>
      <name>Annabelle McIver</name>
    </author>
    <author>
      <name>Pedro D'Argenio</name>
    </author>
    <author>
      <name>Pieter Cuijpers</name>
    </author>
    <author>
      <name>Jasen Markovski</name>
    </author>
    <author>
      <name>Caroll Morgan</name>
    </author>
    <author>
      <name>Manuel Núñez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.13</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.13" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 13, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.2128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.2128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.1; D.2.4; D.3.1; F.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.2931v3</id>
    <updated>2010-11-25T17:35:23Z</updated>
    <published>2010-01-17T22:14:42Z</published>
    <title>Towards Transactional Load over XtreemFS</title>
    <summary>  We propose using trace-based assessment of the performance of distributed
file systems (DFS) under transactional IO load. The assessment includes
simulations and experiments using the IO traces. Our experiments suggest that
DFS, and specifically XtreemFS have a good potential to support transactional
IO load in distributed environments: they demonstrate good performance, high
availability and scalability, while at the same time opening the way to TCO
reduction.
</summary>
    <author>
      <name>Roman Talyansky</name>
    </author>
    <author>
      <name>Bernd Scheuermann</name>
    </author>
    <author>
      <name>Bjorn Kolbeck</name>
    </author>
    <author>
      <name>Jan Stender</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper is withdrawn by the author due to affiliation incorrectness</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.2931v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.2931v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.4108v2</id>
    <updated>2010-02-25T02:01:14Z</updated>
    <published>2010-01-23T00:22:33Z</published>
    <title>A Multi-Stage CUDA Kernel for Floyd-Warshall</title>
    <summary>  We present a new implementation of the Floyd-Warshall All-Pairs Shortest
Paths algorithm on CUDA. Our algorithm runs approximately 5 times faster than
the previously best reported algorithm. In order to achieve this speedup, we
applied a new technique to reduce usage of on-chip shared memory and allow the
CUDA scheduler to more effectively hide instruction latency.
</summary>
    <author>
      <name>Ben Lund</name>
    </author>
    <author>
      <name>Justin W Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.4108v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.4108v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.0134v1</id>
    <updated>2010-01-31T15:36:36Z</updated>
    <published>2010-01-31T15:36:36Z</published>
    <title>Constraint solvers: An empirical evaluation of design decisions</title>
    <summary>  This paper presents an evaluation of the design decisions made in four
state-of-the-art constraint solvers; Choco, ECLiPSe, Gecode, and Minion. To
assess the impact of design decisions, instances of the five problem classes
n-Queens, Golomb Ruler, Magic Square, Social Golfers, and Balanced Incomplete
Block Design are modelled and solved with each solver. The results of the
experiments are not meant to give an indication of the performance of a solver,
but rather investigate what influence the choice of algorithms and data
structures has.
  The analysis of the impact of the design decisions focuses on the different
ways of memory management, behaviour with increasing problem size, and
specialised algorithms for specific types of variables. It also briefly
considers other, less significant decisions.
</summary>
    <author>
      <name>Lars Kotthoff</name>
    </author>
    <link href="http://arxiv.org/abs/1002.0134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.0134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.2898v1</id>
    <updated>2010-05-17T12:08:14Z</updated>
    <published>2010-05-17T12:08:14Z</published>
    <title>Saturation Throughput - Delay Analysis of IEEE 802.11 DCF in Fading
  Channel</title>
    <summary>  In this paper, we analytically analyzed the impact of an error-prone channel
over all performance measures in a trafficsaturated IEEE 802.11 WLAN. We
calculated station's transmission probability by using the modified Markov
chain model of the backoff window size that considers the frame-error rates and
maximal allowable number of retransmission attempts. The frame error rate has a
significant impact over theoretical throughput, mean frame delay, and discard
probability. The peak throughput of a WLAN is insensitive of the maximal number
of retransmissions. Discard probabilities are insensitive to the station access
method, Basic or RTS/CTS.
</summary>
    <author>
      <name>Zoran Hadzi-Velkov</name>
    </author>
    <author>
      <name>Boris Spasenovski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the IEEE International Conference on Communications
  2003 (ICC 2003), 6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.2898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.2898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.2183v1</id>
    <updated>2010-06-11T02:10:58Z</updated>
    <published>2010-06-11T02:10:58Z</published>
    <title>Highly Parallel Sparse Matrix-Matrix Multiplication</title>
    <summary>  Generalized sparse matrix-matrix multiplication is a key primitive for many
high performance graph algorithms as well as some linear solvers such as
multigrid. We present the first parallel algorithms that achieve increasing
speedups for an unbounded number of processors. Our algorithms are based on
two-dimensional block distribution of sparse matrices where serial sections use
a novel hypersparse kernel for scalability. We give a state-of-the-art MPI
implementation of one of our algorithms. Our experiments show scaling up to
thousands of processors on a variety of test scenarios.
</summary>
    <author>
      <name>Aydın Buluç</name>
    </author>
    <author>
      <name>John R. Gilbert</name>
    </author>
    <link href="http://arxiv.org/abs/1006.2183v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.2183v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.3595v1</id>
    <updated>2010-11-16T07:17:53Z</updated>
    <published>2010-11-16T07:17:53Z</published>
    <title>Optimizing real-time RDF data streams</title>
    <summary>  The Resource Description Framework (RDF) provides a common data model for the
integration of "real-time" social and sensor data streams with the Web and with
each other. While there exist numerous protocols and data formats for
exchanging dynamic RDF data, or RDF updates, these options should be examined
carefully in order to enable a Semantic Web equivalent of the high-throughput,
low-latency streams of typical Web 2.0, multimedia, and gaming applications.
This paper contains a brief survey of RDF update formats and a high-level
discussion of both TCP and UDP-based transport protocols for updates. Its main
contribution is the experimental evaluation of a UDP-based architecture which
serves as a real-world example of a high-performance RDF streaming application
in an Internet-scale distributed environment.
</summary>
    <author>
      <name>Joshua Shinavier</name>
    </author>
    <link href="http://arxiv.org/abs/1011.3595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.3595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.6223v3</id>
    <updated>2011-09-27T14:14:21Z</updated>
    <published>2010-11-29T13:24:11Z</published>
    <title>Just-In-Time compilation of OCaml byte-code</title>
    <summary>  This paper presents various improvements that were applied to OCamlJIT2, a
Just-In-Time compiler for the OCaml byte-code virtual machine. OCamlJIT2
currently runs on various Unix-like systems with x86 or x86-64 processors. The
improvements, including the new x86 port, are described in detail, and
performance measures are given, including a direct comparison of OCamlJIT2 to
OCamlJIT.
</summary>
    <author>
      <name>Benedikt Meurer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 6 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.6223v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.6223v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.3; D.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.1237v1</id>
    <updated>2011-01-06T15:47:59Z</updated>
    <published>2011-01-06T15:47:59Z</published>
    <title>Statistical Analysis of Link Scheduling on Long Paths</title>
    <summary>  We study how the choice of packet scheduling algorithms influences end-to-end
performance on long network paths. Taking a network calculus approach, we
consider both deterministic and statistical performance metrics. A key enabling
contribution for our analysis is a significantly sharpened method for computing
a statistical bound for the service given to a flow by the network as a whole.
For a suitably parsimonious traffic model we develop closed-form expressions
for end-to-end delays, backlog, and output burstiness. The deterministic
versions of our bounds yield optimal bounds on end-to-end backlog and output
burstiness for some schedulers, and are highly accurate for end-to-end delay
bounds.
</summary>
    <author>
      <name>Yashar Ghiassi-Farrokhfal</name>
    </author>
    <author>
      <name>Jorg Liebeherr</name>
    </author>
    <author>
      <name>Almut Burchard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.1237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.1237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.2499v2</id>
    <updated>2012-10-08T15:04:54Z</updated>
    <published>2011-04-13T14:12:15Z</published>
    <title>OpenCL/OpenGL approach for studying active Brownian motion</title>
    <summary>  This work presents a methodology for studying active Brownian dynamics on
ratchet potentials using interoperating OpenCL and OpenGL frameworks.
Programing details along with optimization issues are discussed, followed by a
com- parison of performance on different devices. Time of visualization using
OpenGL sharing buffer with OpenCL has been tested against another technique
which, while using OpenGL, does not share memory buffer with OpenCL. Both
methods have been compared with visualizing data to an external software -
gnuplot. OpenCL/OpenGL interoperating method has been found the most
appropriate to visualize any large set of data for which calculation itself is
not very long.
</summary>
    <author>
      <name>Michał Żabicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.2499v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.2499v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.1536v1</id>
    <updated>2011-07-07T23:42:26Z</updated>
    <published>2011-07-07T23:42:26Z</published>
    <title>The M/M/Infinity Service System with Ranked Servers in Heavy Traffic</title>
    <summary>  We consider an M/M/Infinity service system in which an arriving customer is
served by the first idle server in an infinite sequence S_1, S_2, ... of
servers. We determine the first two terms in the asymptotic expansions of the
moments of L as lambda tends to infinity, where L is the index of the server
S_L serving a newly arriving customer in equilibrium, and lambda is the ratio
of the arrival rate to the service rate. The leading terms of the moments show
that L/lambda tends to a uniform distribution on [0,1].
</summary>
    <author>
      <name>Patrick Eschenfeldt</name>
    </author>
    <author>
      <name>Ben Gross</name>
    </author>
    <author>
      <name>Nicholas Pippenger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">i+6 pp</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.1536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.1536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60K26, 90B22" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.1421v1</id>
    <updated>2011-09-07T11:20:46Z</updated>
    <published>2011-09-07T11:20:46Z</published>
    <title>Profiling parallel Mercury programs with ThreadScope</title>
    <summary>  The behavior of parallel programs is even harder to understand than the
behavior of sequential programs. Parallel programs may suffer from any of the
performance problems affecting sequential programs, as well as from several
problems unique to parallel systems. Many of these problems are quite hard (or
even practically impossible) to diagnose without help from specialized tools.
We present a proposal for a tool for profiling the parallel execution of
Mercury programs, a proposal whose implementation we have already started. This
tool is an adaptation and extension of the ThreadScope profiler that was first
built to help programmers visualize the execution of parallel Haskell programs.
</summary>
    <author>
      <name>Paul Bone</name>
    </author>
    <author>
      <name>Zoltan Somogyi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21st Workshop on Logic-based methods in Programming Environments.
  Lexington, Kentucky, July 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.1421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.1421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.1029v2</id>
    <updated>2011-10-27T16:09:44Z</updated>
    <published>2011-10-05T16:05:41Z</published>
    <title>Towards a native toplevel for the OCaml language</title>
    <summary>  This paper presents the current state of our work on an interactive toplevel
for the OCaml language based on the optimizing native code compiler and
runtime. Our native toplevel is up to 100 times faster than the default OCaml
toplevel, which is based on the byte code compiler and interpreter. It uses
Just-In-Time techniques to compile toplevel phrases to native code at runtime,
and currently works with various Unix-like systems running on x86 or x86-64
processors.
</summary>
    <author>
      <name>Marcell Fischbach</name>
    </author>
    <author>
      <name>Benedikt Meurer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures, technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.1029v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.1029v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.3; D.3.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.5710v1</id>
    <updated>2011-11-24T10:13:13Z</updated>
    <published>2011-11-24T10:13:13Z</published>
    <title>On Mean Field Convergence and Stationary Regime</title>
    <summary>  Assume that a family of stochastic processes on some Polish space $E$
converges to a deterministic process; the convergence is in distribution (hence
in probability) at every fixed point in time. This assumption holds for a large
family of processes, among which many mean field interaction models and is
weaker than previously assumed. We show that any limit point of an invariant
probability of the stochastic process is an invariant probability of the
deterministic process. The results are valid in discrete and in continuous
time.
</summary>
    <author>
      <name>Michel Benaim</name>
    </author>
    <author>
      <name>Jean-Yves Le Boudec</name>
    </author>
    <link href="http://arxiv.org/abs/1111.5710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.5710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.1598v1</id>
    <updated>2012-04-07T06:24:53Z</updated>
    <published>2012-04-07T06:24:53Z</published>
    <title>Improving Seek Time for Column Store Using MMH Algorithm</title>
    <summary>  Hash based search has, proven excellence on large data warehouses stored in
column store. Data distribution has significant impact on hash based search. To
reduce impact of data distribution, we have proposed Memory Managed Hash (MMH)
algorithm that uses shift XOR group for Queries and Transactions in column
store. Our experiments show that MMH improves read and write throughput by 22%
for TPC-H distribution.
</summary>
    <author>
      <name>Tejaswini Apte</name>
    </author>
    <author>
      <name>Dr. Maya Ingle</name>
    </author>
    <author>
      <name>Dr. A. K. Goyal</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">(IJACSA) International Journal of Advanced Computer Science and
  Applications Vol. 3, No.2, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.1598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.1598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.5281v1</id>
    <updated>2012-04-24T06:59:37Z</updated>
    <published>2012-04-24T06:59:37Z</published>
    <title>Stochastic Analysis of Mean Interference for RTS/CTS Mechanism</title>
    <summary>  The RTS/CTS handshake mechanism in WLAN is studied using stochastic geometry.
The effect of RTS/CTS is treated as a thinning procedure for a spatially
distributed point process that models the potential transceivers in a WLAN, and
the resulting concurrent transmission processes are described. Exact formulas
for the intensity of the concurrent transmission processes and the mean
interference experienced by a typical receiver are established. The analysis
yields useful results for understanding how the design parameters of RTS/CTS
affect the network interference.
</summary>
    <author>
      <name>Yi Zhong</name>
    </author>
    <author>
      <name>Wenyi Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">technical report, in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.5281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.5281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.2543v1</id>
    <updated>2012-08-13T10:48:39Z</updated>
    <published>2012-08-13T10:48:39Z</published>
    <title>Doing More for Less -- Cache-Aware Parallel Contraction Hierarchies
  Preprocessing</title>
    <summary>  Contraction Hierarchies is a successful speedup-technique to Dijkstra's
seminal shortest path algorithm that has a convenient trade-off between
preprocessing and query times. We investigate a shared-memory parallel
implementation that uses $O(n+m)$ space for storing the graph and O(1) space
for each core during preprocessing. The presented data structures and
algorithms consequently exploits cache locality and thus exhibit competitive
preprocessing times. The presented implementation is especially suitable for
preprocessing graphs of planet-wide scale in practice. Also, our experiments
show that optimal data structures in the PRAM model can be beaten in practice
by exploiting memory cache hierarchies.
</summary>
    <author>
      <name>Dennis Luxen</name>
    </author>
    <author>
      <name>Dennis Schieferdecker</name>
    </author>
    <link href="http://arxiv.org/abs/1208.2543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.2543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4560v1</id>
    <updated>2012-09-20T15:18:54Z</updated>
    <published>2012-09-20T15:18:54Z</published>
    <title>Distributing an Exact Algorithm for Maximum Clique: maximising the
  costup</title>
    <summary>  We take an existing implementation of an algorithm for the maximum clique
problem and modify it so that we can distribute it over an ad-hoc cluster of
machines. Our goal was to achieve a significant speedup in performance with
minimal development effort, i.e. a maximum costup. We present a simple
modification to a state-of-the-art exact algorithm for maximum clique that
allows us to distribute it across many machines. An empirical study over large
hard benchmarks shows that speedups of an order of magnitude are routine for 25
or more machines.
</summary>
    <author>
      <name>Ciaran McCreesh</name>
    </author>
    <author>
      <name>Patrick Prosser</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/a5040545</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/a5040545" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 Algorithms, 2 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Algorithms 2012, 5(4), 545-587</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.4560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.5374v1</id>
    <updated>2012-12-21T09:52:43Z</updated>
    <published>2012-12-21T09:52:43Z</published>
    <title>A Blind Time-Reversal Detector in the Presence of Channel Correlation</title>
    <summary>  A blind target detector using the time reversal transmission is proposed in
the presence of channel correlation. We calculate the exact moments of the test
statistics involved. The derived moments are used to construct an accurate
approximative Likelihood Ratio Test (LRT) based on multivariate Edgeworth
expansion. Performance gain over an existing detector is observed in scenarios
with channel correlation and relatively strong target signal.
</summary>
    <author>
      <name>Zhong Zheng</name>
    </author>
    <author>
      <name>Lu Wei</name>
    </author>
    <author>
      <name>Jyri Hämäläinen</name>
    </author>
    <author>
      <name>Olav Tirkkonen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2013.2251880</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2013.2251880" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures. Submitted to IEEE Signal Processing Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.5374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.5374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.6640v1</id>
    <updated>2012-12-29T15:29:26Z</updated>
    <published>2012-12-29T15:29:26Z</published>
    <title>Exploring mutexes, the Oracle RDBMS retrial spinlocks</title>
    <summary>  Spinlocks are widely used in database engines for processes synchronization.
KGX mutexes is new retrial spinlocks appeared in contemporary Oracle versions
for submicrosecond synchronization. The mutex contention is frequently observed
in highly concurrent OLTP environments.
  This work explores how Oracle mutexes operate, spin, and sleep. It develops
predictive mathematical model and discusses parameters and statistics related
to mutex performance tuning, as well as results of contention experiments.
</summary>
    <author>
      <name>Andrey Nikolaev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of International Conference on Informatics MEDIAS2012.
  Cyprus, Limassol, May 7--14, 2012. ISBN 978-5-88835-023-2. 12 pages, 15
  figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.6640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.6640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.4773v1</id>
    <updated>2013-02-19T22:59:44Z</updated>
    <published>2013-02-19T22:59:44Z</published>
    <title>Optimal Discriminant Functions Based On Sampled Distribution Distance
  for Modulation Classification</title>
    <summary>  In this letter, we derive the optimal discriminant functions for modulation
classification based on the sampled distribution distance. The proposed method
classifies various candidate constellations using a low complexity approach
based on the distribution distance at specific testpoints along the cumulative
distribution function. This method, based on the Bayesian decision criteria,
asymptotically provides the minimum classification error possible given a set
of testpoints. Testpoint locations are also optimized to improve classification
performance. The method provides significant gains over existing approaches
that also use the distribution of the signal features.
</summary>
    <author>
      <name>Paulo Urriza</name>
    </author>
    <author>
      <name>Eric Rebeiz</name>
    </author>
    <author>
      <name>Danijela Cabric</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LCOMM.2013.082113.131131</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LCOMM.2013.082113.131131" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, submitted to IEEE Communications Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.4773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.4773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.0038v3</id>
    <updated>2015-07-24T21:13:52Z</updated>
    <published>2013-02-28T22:54:52Z</published>
    <title>Approximately Optimal Scheduling of an M/G/1 Queue with Heavy Tails</title>
    <summary>  Distributions with a heavy tail are difficult to estimate. If the design of a
scheduling policy is sensitive to the details of heavy tail distributions of
the service times, an approximately optimal solution is difficult to obtain.
This paper shows that the optimal scheduling of an M/G/1 queue with heavy
tailed service times does not present this difficulty and that an approximately
optimal strategy can be derived by truncating the distributions.
</summary>
    <author>
      <name>Vijay Kamble</name>
    </author>
    <author>
      <name>Jean Walrand</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11134-015-9435-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11134-015-9435-0" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Queueing Systems, Volume 80, Issue 3, pp 261-271, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1303.0038v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.0038v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.0534v1</id>
    <updated>2013-09-02T20:34:09Z</updated>
    <published>2013-09-02T20:34:09Z</published>
    <title>Machines are benchmarked by code, not algorithms</title>
    <summary>  This article highlights how small modifications to either the source code of
a benchmark program or the compilation options may impact its behavior on a
specific machine. It argues that for evaluating machines, benchmark providers
and users be careful to ensure reproducibility of results based on the machine
code actually running on the hardware and not just source code. The article
uses color to grayscale conversion of digital images as a running example.
</summary>
    <author>
      <name>Raphael 'kena' Poss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, 11 figures, 11 listings, 17 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.0534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.0534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.0569v1</id>
    <updated>2013-09-03T01:07:05Z</updated>
    <published>2013-09-03T01:07:05Z</published>
    <title>Product-form solutions for integrated services packet networks and cloud
  computing systems</title>
    <summary>  We iteratively derive the product-form solutions of stationary distributions
of priority multiclass queueing networks with multi-sever stations. The
networks are Markovian with exponential interarrival and service time
distributions. These solutions can be used to conduct performance analysis or
as comparison criteria for approximation and simulation studies of large scale
networks with multi-processor shared-memory switches and cloud computing
systems with parallel-server stations. Numerical comparisons with existing
Brownian approximating model are provided to indicate the effectiveness of our
algorithm.
</summary>
    <author>
      <name>Wanyang Dai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 3 figures, short conference version is reported at MICAI
  2006</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Mathematical Problems in Engineering, Vol. 2014 (Regular Issue),
  Article ID 767651, 16 pages, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1309.0569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.0569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.3809v1</id>
    <updated>2013-10-10T11:02:13Z</updated>
    <published>2013-10-10T11:02:13Z</published>
    <title>Efficient Modular Arithmetic for SIMD Devices</title>
    <summary>  This paper describes several new improvements of modular arithmetic and how
to exploit them in order to gain more efficient implementations of commonly
used algorithms, especially in cryptographic applications. We further present a
new record for modular multiplications per second on a single desktop computer
as well as a new record for the ECM factoring algorithm. This new results allow
building personal computers which can handle more than 3 billion modular
multiplications per second for a 192 bit module at moderate costs using modern
graphic cards.
</summary>
    <author>
      <name>Wilke Trei</name>
    </author>
    <link href="http://arxiv.org/abs/1310.3809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.3809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.5839v1</id>
    <updated>2013-10-22T08:48:03Z</updated>
    <published>2013-10-22T08:48:03Z</published>
    <title>Extreme Scaling of Lattice Quantum Chromodynamics</title>
    <summary>  As the complexity and size of challenges in science and engineering are
continually increasing, it is highly important that applications are able to
scale strongly to very large numbers of cores (&gt;100,000 cores) to enable HPC
systems to be utilised efficiently. This paper presents results of strong
scaling tests performed with an MPI only and a hybrid MPI + OpenMP version of
the Lattice QCD application BQCD on the European Tier-0 system SuperMUC at LRZ.
</summary>
    <author>
      <name>David Brayford</name>
    </author>
    <author>
      <name>Momme Allalen</name>
    </author>
    <author>
      <name>Volker Weinberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, talk given at the "Extreme Scaling on SuperMUC"
  Minisymposium during ParCo 2013, International Conference on Parallel
  Computing, 10-13 September 2013, Munich</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.5839v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.5839v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-lat" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.6173v1</id>
    <updated>2013-10-23T10:27:49Z</updated>
    <published>2013-10-23T10:27:49Z</published>
    <title>Self-Organizing Mobility Robustness Optimization in LTE Networks with
  eICIC</title>
    <summary>  We address the problem of Mobility Robustness Optimization (MRO) and describe
centralized Self Organizing Network (SON) solutions that can optimize
connected-mode mobility Key Performance Indicators (KPIs). Our solution extends
the earlier work of eICIC parameter optimization [7], to heterogeneous networks
with mobility, and outline methods of progressive complexity that optimize the
Retaining/Offloading Bias which are macro/pico views of Cell Individual Offset
parameters. Simulation results under real LTE network deployment assumptions of
a US metropolitan area demonstrate the effects of such solutions on the
mobility KPIs. To our knowledge, this solution is the first that demonstrates
the joint optimization of eICIC and MRO.
</summary>
    <author>
      <name>Carl Weaver</name>
    </author>
    <author>
      <name>Pantelis Monogioudis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.6173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.6173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.0960v2</id>
    <updated>2014-03-25T15:01:23Z</updated>
    <published>2013-11-05T03:42:19Z</published>
    <title>The abstract Cauchy problem for the non-stationary bulk queue M(t)|M[k,
  B]|1</title>
    <summary>  We derived state probability equations describing the queue M(t)|M[k, B]|1
and formulated as an abstract Cauchy problem to investigate by means of the
semi-group theory of bounded linear operators in functional analysis. For the
abstract Cauchy problem of this queue, we determined the eigenfunctions of
maximal operator and showed some properties of the Dirichlet operator.
</summary>
    <author>
      <name>Yong Chol Chon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.0960v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.0960v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.GM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60K20, 60K25, 68M20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.1195v1</id>
    <updated>2013-11-05T20:57:26Z</updated>
    <published>2013-11-05T20:57:26Z</published>
    <title>Performance Modeling of BitTorrent Peer-to-Peer File Sharing Networks</title>
    <summary>  BitTorrent is undoubtedly the most popular P2P file sharing application on
today's Internet. The widespread popularity of BitTorrent has attracted a great
deal of attention from networking researchers who conducted various performance
studies on it. This paper presents a comprehensive survey of analytical
performance modeling techniques for BitTorrent networks. The performance models
examined in this study include deterministic models, Markov chain models, fluid
flow models, and queuing network models. These models evaluate the performance
metrics of BitTorrent networks at different regimes with various realistic
factors considered. Furthermore, a comparative analysis is conducted on those
modeling techniques in the aspects of complexity, accuracy, extensibility, and
scalability.
</summary>
    <author>
      <name>Kunjie Xu</name>
    </author>
    <link href="http://arxiv.org/abs/1311.1195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.1195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.3928v1</id>
    <updated>2013-11-15T17:17:21Z</updated>
    <published>2013-11-15T17:17:21Z</published>
    <title>HS06 Benchmark for an ARM Server</title>
    <summary>  We benchmarked an ARM cortex-A9 based server system with a four-core CPU
running at 1.1 GHz. The system used Ubuntu 12.04 as operating system and the
HEPSPEC 2006 (HS06) benchmarking suite was compiled natively with gcc-4.4 on
the system. The benchmark was run for various settings of the relevant gcc
compiler options. We did not find significant influence from the compiler
options on the benchmark result. The final HS06 benchmark result is 10.4.
</summary>
    <author>
      <name>Stefan Kluth</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-6596/513/6/062025</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-6596/513/6/062025" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">three pages, two figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.3928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.3928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.1513v1</id>
    <updated>2014-01-06T20:11:21Z</updated>
    <published>2014-01-06T20:11:21Z</published>
    <title>On the Stability of Random Multiple Access with Feedback Exploitation
  and Queue Priority</title>
    <summary>  In this paper, we study the stability of two interacting queues under random
multiple access in which the queues leverage the feedback information. We
derive the stability region under random multiple access where one of the two
queues exploits the feedback information and backs off under negative
acknowledgement (NACK) and the other, higher priority, queue will access the
channel with probability one. We characterize the stability region of this
feedback-based random access protocol and prove that this derived stability
region encloses the stability region of the conventional random access (RA)
scheme that does not exploit the feedback information.
</summary>
    <author>
      <name>Karim G. Seddik</name>
    </author>
    <link href="http://arxiv.org/abs/1401.1513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.1513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.4950v1</id>
    <updated>2014-01-20T15:52:03Z</updated>
    <published>2014-01-20T15:52:03Z</published>
    <title>MRRR-based Eigensolvers for Multi-core Processors and Supercomputers</title>
    <summary>  The real symmetric tridiagonal eigenproblem is of outstanding importance in
numerical computations; it arises frequently as part of eigensolvers for
standard and generalized dense Hermitian eigenproblems that are based on a
reduction to tridiagonal form. For its solution, the algorithm of Multiple
Relatively Robust Representations (MRRR or MR3 in short) - introduced in the
late 1990s - is among the fastest methods. To compute k eigenpairs of a real
n-by-n tridiagonal T, MRRR only requires O(kn) arithmetic operations; in
contrast, all the other practical methods require O(k^2 n) or O(n^3) operations
in the worst case. This thesis centers around the performance and accuracy of
MRRR.
</summary>
    <author>
      <name>Matthias Petschow</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">AICES, RWTH Aachen</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.4950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.4950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.5897v1</id>
    <updated>2014-02-21T12:23:19Z</updated>
    <published>2014-02-21T12:23:19Z</published>
    <title>A Study on the Influence of Caching: Sequences of Dense Linear Algebra
  Kernels</title>
    <summary>  It is universally known that caching is critical to attain high- performance
implementations: In many situations, data locality (in space and time) plays a
bigger role than optimizing the (number of) arithmetic floating point
operations. In this paper, we show evidence that at least for linear algebra
algorithms, caching is also a crucial factor for accurate performance modeling
and performance prediction.
</summary>
    <author>
      <name>Elmar Peise</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">AICES, RWTH Aachen</arxiv:affiliation>
    </author>
    <author>
      <name>Paolo Bientinesi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">AICES, RWTH Aachen</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the Ninth International Workshop on Automatic
  Performance Tuning (iWAPT2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.5897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.5897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2071v1</id>
    <updated>2014-06-09T03:50:36Z</updated>
    <published>2014-06-09T03:50:36Z</published>
    <title>Formal and Informal Methods for Multi-Core Design Space Exploration</title>
    <summary>  We propose a tool-supported methodology for design-space exploration for
embedded systems. It provides means to define high-level models of applications
and multi-processor architectures and evaluate the performance of different
deployment (mapping, scheduling) strategies while taking uncertainty into
account. We argue that this extension of the scope of formal verification is
important for the viability of the domain.
</summary>
    <author>
      <name>Jean-Francois Kempf</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">VERIMAG</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Lebeltel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">VERIMAG</arxiv:affiliation>
    </author>
    <author>
      <name>Oded Maler</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">VERIMAG</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.154.6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.154.6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings QAPL 2014, arXiv:1406.1567</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 154, 2014, pp. 78-92</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.2071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.0142v1</id>
    <updated>2014-08-01T12:00:03Z</updated>
    <published>2014-08-01T12:00:03Z</published>
    <title>On open problems in polling systems</title>
    <summary>  In the present paper we address two open problems concerning polling systems,
viz., queueing systems consisting of multiple queues attended by a single
server that visits the queues one at a time. The first open problem deals with
a system consisting of two queues, one of which has gated service, while the
other receives 1-limited service. The second open problem concerns polling
systems with general (renewal) arrivals and deterministic switch-over times
that become infinitely large. We discuss related, known results for both
problems, and the difficulties encountered when trying to solve them.
</summary>
    <author>
      <name>Marko Boon</name>
    </author>
    <author>
      <name>Onno Boxma</name>
    </author>
    <author>
      <name>Erik Winands</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11134-011-9247-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11134-011-9247-9" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Queueing Systems 68, pp. 365-374, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.0142v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.0142v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.0282v1</id>
    <updated>2014-08-01T09:43:15Z</updated>
    <published>2014-08-01T09:43:15Z</published>
    <title>A Polling Model with Multiple Priority Levels</title>
    <summary>  In this paper we consider a single-server cyclic polling system. Between
visits to successive queues, the server is delayed by a random switch-over
time. The order in which customers are served in each queue is determined by a
priority level that is assigned to each customer at his arrival. For this
situation the following service disciplines are considered: gated, exhaustive,
and globally gated. We study the cycle time distribution, the waiting times for
each customer type, the joint queue length distribution of all priority classes
at all queues at polling epochs, and the steady-state marginal queue length
distributions for each customer type.
</summary>
    <author>
      <name>Marko Boon</name>
    </author>
    <author>
      <name>Ivo Adan</name>
    </author>
    <author>
      <name>Onno Boxma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.peva.2010.01.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.peva.2010.01.002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1408.0110</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Performance Evaluation 67, pp. 468-484, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.0282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.0282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.0143v1</id>
    <updated>2014-11-01T17:55:45Z</updated>
    <published>2014-11-01T17:55:45Z</published>
    <title>Estimating the Spatial Reuse with Configuration Models</title>
    <summary>  We propose a new methodology to estimate the spatial reuse of CSMA-like
scheduling. Instead of focusing on spatial configurations of users, we model
the interferences between users as a random graph. Using configuration models
for random graphs, we show how the properties of the medium access mechanism
are captured by some deterministic differential equations, when the size of the
graph gets large. Performance indicators such as the probability of connection
of a given node can then be efficiently computed from these equations. We also
perform simulations to illustrate the results on different types of random
graphs. Even on spatial structures, these estimates get very accurate as soon
as the variance of the interference is not negligible.
</summary>
    <author>
      <name>Paola Bermolen</name>
    </author>
    <author>
      <name>Matthieu Jonckheere</name>
    </author>
    <author>
      <name>Federico Larroca</name>
    </author>
    <author>
      <name>Pascal Moyal</name>
    </author>
    <link href="http://arxiv.org/abs/1411.0143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.0143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00087v1</id>
    <updated>2015-01-31T09:45:29Z</updated>
    <published>2015-01-31T09:45:29Z</published>
    <title>On Optimization of Network-coded Scalable Multimedia Service
  Multicasting</title>
    <summary>  In the near future, the delivery of multimedia multicast services over
next-generation networks is likely to become one of the main pillars of future
cellular networks. In this extended abstract, we address the issue of
efficiently multicasting layered video services by defining a novel
optimization paradigm that is based on an Unequal Error Protection
implementation of Random Linear Network Coding, and aims to ensure target
service coverages by using a limited amount of radio resources.
</summary>
    <author>
      <name>Andrea Tassi</name>
    </author>
    <author>
      <name>Ioannis Chatzigeorgiou</name>
    </author>
    <author>
      <name>Dejan Vukobratović</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of the 6th Systems and Networks Optimization for Wireless
  (SNOW) Workshop 2015, to appear</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.00087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.07967v1</id>
    <updated>2015-04-29T19:01:30Z</updated>
    <published>2015-04-29T19:01:30Z</published>
    <title>Improved repeatability measures for evaluating performance of feature
  detectors</title>
    <summary>  The most frequently employed measure for performance characterisation of
local feature detectors is repeatability, but it has been observed that this
does not necessarily mirror actual performance. Presented are improved
repeatability formulations which correlate much better with the true
performance of feature detectors. Comparative results for several
state-of-the-art feature detectors are presented using these measures; it is
found that Hessian-based detectors are generally superior at identifying
features when images are subject to various geometric and photometric
transformations.
</summary>
    <author>
      <name>Shoaib Ehsan</name>
    </author>
    <author>
      <name>Nadia Kanwal</name>
    </author>
    <author>
      <name>Adrian F. Clark</name>
    </author>
    <author>
      <name>Klaus D. McDonald-Maier</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronics Letters 8th July 2010 Vol. 46 No. 14</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1504.07967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.03648v1</id>
    <updated>2015-07-13T22:54:20Z</updated>
    <published>2015-07-13T22:54:20Z</published>
    <title>Deadline-aware Power Management in Data Centers</title>
    <summary>  We study the dynamic power optimization problem in data centers. We formulate
and solve the following offline problem: in which slot which server has to be
assigned to which job; and in which slot which server has to be switched ON or
OFF so that the total power is optimal for some time horizon. We show that the
offline problem is a new version of generalized assignment problem including
new constraints issuing from deadline characteristics of jobs and difference of
activation energy of servers. We propose an online algorithm that solves the
problem heuristically and compare it to randomized routing.
</summary>
    <author>
      <name>Cengis Hasan</name>
    </author>
    <author>
      <name>Zygmunt J. Haas</name>
    </author>
    <link href="http://arxiv.org/abs/1507.03648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.03648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08187v2</id>
    <updated>2016-02-25T05:31:36Z</updated>
    <published>2015-07-29T15:36:21Z</published>
    <title>Dependability Analysis of Control Systems using SystemC and Statistical
  Model Checking</title>
    <summary>  Stochastic Petri nets are commonly used for modeling distributed systems in
order to study their performance and dependability. This paper proposes a
realization of stochastic Petri nets in SystemC for modeling large embedded
control systems. Then statistical model checking is used to analyze the
dependability of the constructed model. Our verification framework allows users
to express a wide range of useful properties to be verified which is
illustrated through a case study.
</summary>
    <author>
      <name>Van Chan Ngo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ESTASYS</arxiv:affiliation>
    </author>
    <author>
      <name>Axel Legay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ESTASYS</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1507.08187v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08187v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03111v1</id>
    <updated>2015-09-10T11:46:12Z</updated>
    <published>2015-09-10T11:46:12Z</published>
    <title>Integration of RTMFP in the OMNeT++ Simulation Environment</title>
    <summary>  This paper introduces the new Real-Time Media Flow Protocol (RTMFP)
simulation model for the INET framework for the OMNeT++ simulation environment.
RTMFP is a message orientated protocol with a focus on real time peer-to-peer
communication. After Adobe Inc. released the specifications, we were able to
implement the protocol in INET and compare its performance to the similar
Stream Control Transmission Protocol (SCTP) with a focus on congestion control
and flow control mechanisms.
</summary>
    <author>
      <name>Felix Weinrank</name>
    </author>
    <author>
      <name>Michael Tüxen</name>
    </author>
    <author>
      <name>Erwin P. Rathgeb</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in: A. F\"orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.03111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03118v3</id>
    <updated>2017-03-03T08:33:37Z</updated>
    <published>2015-09-10T12:02:51Z</published>
    <title>Execution-Cache-Memory Performance Model: Introduction and Validation</title>
    <summary>  This report serves two purposes: To introduce and validate the
Execution-Cache-Memory (ECM) performance model and to provide a thorough
analysis of current Intel processor architectures with a special emphasis on
Intel Xeon Haswell-EP. The ECM model is a simple analytical performance model
which focuses on basic architectural resources. The architectural analysis and
model predictions are showcased and validated using a set of elementary
microbenchmarks.
</summary>
    <author>
      <name>Johannes Hofmann</name>
    </author>
    <author>
      <name>Jan Eitzinger</name>
    </author>
    <author>
      <name>Dietmar Fey</name>
    </author>
    <link href="http://arxiv.org/abs/1509.03118v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03118v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03127v1</id>
    <updated>2015-09-10T12:45:46Z</updated>
    <published>2015-09-10T12:45:46Z</published>
    <title>Integration of the Packetdrill Testing Tool in INET</title>
    <summary>  Google released in 2013 a script-based tool called packetdrill, which allows
to test transport protocols like UDP and TCP on Linux and BSD-based operating
systems. The scripts defining a test-case allow to inject packets to the
implementation under test, perform operations at the API controlling the
transport protocol and verify the sending of packets, all at specified times.
This paper describes a port of packetdrill to the INET framework for the
OMNeT++ simulation environment providing a simple and powerful method of
testing the transport protocols implemented in INET.
</summary>
    <author>
      <name>Irene Rüngeler</name>
    </author>
    <author>
      <name>Michael Tüxen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in: A. F\"orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.03127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03550v1</id>
    <updated>2015-09-11T15:22:41Z</updated>
    <published>2015-09-11T15:22:41Z</published>
    <title>Skip This Paper - RINASim: Your Recursive InterNetwork Architecture
  Simulator</title>
    <summary>  Recursive InterNetwork Architecture is a clean-slate approach to how to deal
with the current issues of the Internet based on the traditional TCP/IP
networking stack. Instead of using a fixed number of layers with dedicated
functionality, RINA proposes a single generic layer with programmable
functionality that may be recursively stacked. We introduce a brand new
framework for modeling and simulation of RINA that is intended for OMNeT++.
</summary>
    <author>
      <name>Vladimir Vesely</name>
    </author>
    <author>
      <name>Marcel Marek</name>
    </author>
    <author>
      <name>Tomas Hykel</name>
    </author>
    <author>
      <name>Ondrej Rysavy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in: A. F\"orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015, arXiv:1509.03284, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.03550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03553v1</id>
    <updated>2015-09-11T15:26:37Z</updated>
    <published>2015-09-11T15:26:37Z</published>
    <title>Implementation of a Wake-up Radio Cross-Layer Protocol in OMNeT++ /
  MiXiM</title>
    <summary>  This paper presents the DoRa protocol, which is a new cross-layer protocol
for handling the double radio of nodes in wake-up radio scenario. The
implementation details in OMNET++/MiXiM are also given, with a focus on the
implemented MAC layers. The main goal of the DoRa protocol is to reduce energy
consumption in wireless sensor network, by taking full advantage of the passive
wake-up scheme. The performance of the DoRa protocol is then evaluated and
results are compared with B-MAC and IEEE 802.15.4 protocols.
</summary>
    <author>
      <name>Jean Lebreton</name>
    </author>
    <author>
      <name>Nour Murad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in: A. F\"orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015, arXiv:1509.03284, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.03553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.08169v1</id>
    <updated>2015-09-28T01:23:36Z</updated>
    <published>2015-09-28T01:23:36Z</published>
    <title>Proceedings Thirteenth Workshop on Quantitative Aspects of Programming
  Languages and Systems</title>
    <summary>  This volume contains the proceedings of the Thirteenth Workshop on
Quantitative Aspects of Programming Languages and Systems (QAPL 2015), held in
London, UK, on 11 and 12 April, 2015. QAPL 2015 was a satellite event of the
European Joint Conferences on Theory and Practice of Software (ETAPS) focussing
on quantitative aspects of computation. The Program Committee of QAPL 2015
selected 8 regular papers and 2 presentation-only papers. The workshop
programme included two QAPL keynote presentations by Catuscia Palamidessi
(Inria/LIX, France) on "Quantitative Aspects of Privacy and Information Flow,"
and Holger Hermanns (Saarland University, Germany) on "Optimal Continuous Time
Markov Decisions."
</summary>
    <author>
      <name>Nathalie Bertrand</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria Rennes, France</arxiv:affiliation>
    </author>
    <author>
      <name>Mirco Tribastone</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMT - Institute for Advanced Studies Lucca, Italy</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.194</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.194" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 194, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1509.08169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.08169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.08561v1</id>
    <updated>2015-09-29T02:10:30Z</updated>
    <published>2015-09-29T02:10:30Z</published>
    <title>Efficient Checking of Individual Rewards Properties in Markov Population
  Models</title>
    <summary>  In recent years fluid approaches to the analysis of Markov populations models
have been demonstrated to have great pragmatic value. Initially developed to
estimate the behaviour of the system in terms of the expected values of
population counts, the fluid approach has subsequently been extended to more
sophisticated interrogations of models through its embedding within model
checking procedures. In this paper we extend recent work on checking CSL
properties of individual agents within a Markovian population model, to
consider the checking of properties which incorporate rewards.
</summary>
    <author>
      <name>Luca Bortolussi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Trieste</arxiv:affiliation>
    </author>
    <author>
      <name>Jane Hillston</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Edinburgh</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.194.3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.194.3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings QAPL 2015, arXiv:1509.08169</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 194, 2015, pp. 32-47</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1509.08561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.08561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00041v2</id>
    <updated>2016-04-07T17:49:16Z</updated>
    <published>2015-09-30T21:31:42Z</published>
    <title>iotools: High-Performance I/O Tools for R</title>
    <summary>  The iotools package provides a set of tools for Input/Output (I/O) intensive
datasets processing in R (R Core Team, 2014). Efficent parsing methods are
included which minimize copying and avoid the use of intermediate string
representations whenever possible. Functions for applying chunk-wise operations
allow for computing on streaming input as well as arbitrarily large files. We
present a set of example use cases for iotools, as well as extensive benchmarks
comparing comparable functions provided in both core-R as well as other
contributed packages.
</summary>
    <author>
      <name>Taylor Arnold</name>
    </author>
    <author>
      <name>Michael Kane</name>
    </author>
    <author>
      <name>Simon Urbanek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00041v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00041v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="03-04" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.07815v2</id>
    <updated>2017-05-19T13:40:53Z</updated>
    <published>2016-01-28T16:19:39Z</published>
    <title>Convex Optimization of Real Time SoC</title>
    <summary>  Convex optimization methods are employed to optimize a real-time (RT)
system-on-chip (SoC) under a variety of physical resource-driven constraints,
demonstrated on an industry MPEG2 encoder SoC. The power optimization is
compared to conventional performance-optimization framework, showing a factor
of two and a half saving in power. Convex optimization is shown to be very
efficient in a high-level early stage design exploration, guiding computer
architects as to the choice of area, voltage, and frequency of the individual
components of the Chip Multiprocessor (CMP).
</summary>
    <author>
      <name>L. Yavits</name>
    </author>
    <author>
      <name>A. Morad</name>
    </author>
    <author>
      <name>R. Ginosar</name>
    </author>
    <author>
      <name>U. Weiser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.07815v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07815v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.04873v1</id>
    <updated>2016-02-16T00:39:33Z</updated>
    <published>2016-02-16T00:39:33Z</published>
    <title>A Stochastic Performance Model for Pipelined Krylov Methods</title>
    <summary>  Pipelined Krylov methods seek to ameliorate the latency due to inner products
necessary for projection by overlapping it with the computation associated with
sparse matrix-vector multiplication. We clarify a folk theorem that this can
only result in a speedup of $2\times$ over the naive implementation. Examining
many repeated runs, we show that stochastic noise also contributes to the
latency, and we model this using an analytical probability distribution. Our
analysis shows that speedups greater than $2\times$ are possible with these
algorithms.
</summary>
    <author>
      <name>Hannah Morgan</name>
    </author>
    <author>
      <name>Matthew G. Knepley</name>
    </author>
    <author>
      <name>Patrick Sanan</name>
    </author>
    <author>
      <name>L. Ridgway Scott</name>
    </author>
    <link href="http://arxiv.org/abs/1602.04873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.04873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.06763v2</id>
    <updated>2022-04-06T06:32:17Z</updated>
    <published>2016-02-22T13:21:05Z</published>
    <title>Algorithm 979: Recursive Algorithms for Dense Linear Algebra -- The
  ReLAPACK Collection</title>
    <summary>  To exploit both memory locality and the full performance potential of highly
tuned kernels, dense linear algebra libraries such as LAPACK commonly implement
operations as blocked algorithms. However, to achieve next-to-optimal
performance with such algorithms, significant tuning is required. On the other
hand, recursive algorithms are virtually tuning free, and yet attain similar
performance. In this paper, we first analyze and compare blocked and recursive
algorithms in terms of performance, and then introduce ReLAPACK, an open-source
library of recursive algorithms to seamlessly replace most of LAPACK's blocked
algorithms. In many scenarios, ReLAPACK clearly outperforms reference LAPACK,
and even improves upon the performance of optimizes libraries.
</summary>
    <author>
      <name>Elmar Peise</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">AICES, RWTH Aachen</arxiv:affiliation>
    </author>
    <author>
      <name>Paolo Bientinesi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">AICES, RWTH Aachen</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1602.06763v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.06763v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.02955v1</id>
    <updated>2016-03-09T16:40:39Z</updated>
    <published>2016-03-09T16:40:39Z</published>
    <title>A Performance Evaluation of Container Technologies on Internet of Things
  Devices</title>
    <summary>  The use of virtualization technologies in different contexts - such as Cloud
Environments, Internet of Things (IoT), Software Defined Networking (SDN) - has
rapidly increased during the last years. Among these technologies,
container-based solutions own characteristics for deploying distributed and
lightweight applications. This paper presents a performance evaluation of
container technologies on constrained devices, in this case, on Raspberry Pi.
The study shows that, overall, the overhead added by containers is negligible.
</summary>
    <author>
      <name>Roberto Morabito</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/INFCOMW.2016.7562228</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/INFCOMW.2016.7562228" rel="related"/>
    <link href="http://arxiv.org/abs/1603.02955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.02955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03252v1</id>
    <updated>2016-03-10T13:20:05Z</updated>
    <published>2016-03-10T13:20:05Z</published>
    <title>Extension of PRISM by Synthesis of Optimal Timeouts in Fixed-Delay CTMC</title>
    <summary>  We present a practically appealing extension of the probabilistic model
checker PRISM rendering it to handle fixed-delay continuous-time Markov chains
(fdCTMCs) with rewards, the equivalent formalism to the deterministic and
stochastic Petri nets (DSPNs). fdCTMCs allow transitions with fixed-delays (or
timeouts) on top of the traditional transitions with exponential rates. Our
extension supports an evaluation of expected reward until reaching a given set
of target states. The main contribution is that, considering the fixed-delays
as parameters, we implemented a synthesis algorithm that computes the
epsilon-optimal values of the fixed-delays minimizing the expected reward. We
provide a performance evaluation of the synthesis on practical examples.
</summary>
    <author>
      <name>Ľuboš Korenčiak</name>
    </author>
    <author>
      <name>Vojtěch Řehák</name>
    </author>
    <author>
      <name>Adrian Farmadin</name>
    </author>
    <link href="http://arxiv.org/abs/1603.03252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02233v1</id>
    <updated>2016-07-08T05:36:36Z</updated>
    <published>2016-07-08T05:36:36Z</published>
    <title>On Formal Methods for Collective Adaptive System Engineering. {Scalable
  Approximated, Spatial} Analysis Techniques. Extended Abstract</title>
    <summary>  In this extended abstract a view on the role of Formal Methods in System
Engineering is briefly presented. Then two examples of useful analysis
techniques based on solid mathematical theories are discussed as well as the
software tools which have been built for supporting such techniques. The first
technique is Scalable Approximated Population DTMC Model-checking. The second
one is Spatial Model-checking for Closure Spaces. Both techniques have been
developed in the context of the EU funded project QUANTICOL.
</summary>
    <author>
      <name>Diego Latella</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CNR-ISTI</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.217.7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.217.7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings FORECAST 2016, arXiv:1607.02001</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 217, 2016, pp. 53-61</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.02233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00658v1</id>
    <updated>2016-08-02T00:37:24Z</updated>
    <published>2016-08-02T00:37:24Z</published>
    <title>Rate Reduction for State-labelled Markov Chains with Upper Time-bounded
  CSL Requirements</title>
    <summary>  This paper presents algorithms for identifying and reducing a dedicated set
of controllable transition rates of a state-labelled continuous-time Markov
chain model. The purpose of the reduction is to make states to satisfy a given
requirement, specified as a CSL upper time-bounded Until formula. We
distinguish two different cases, depending on the type of probability bound. A
natural partitioning of the state space allows us to develop possible
solutions, leading to simple algorithms for both cases.
</summary>
    <author>
      <name>Bharath Siva Kumar Tati</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UniBw</arxiv:affiliation>
    </author>
    <author>
      <name>Markus Siegle</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UniBw</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.220.7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.220.7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings Cassting'16/SynCoP'16, arXiv:1608.00177</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 220, 2016, pp. 77-89</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.00658v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00658v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.4; G.3; C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04041v1</id>
    <updated>2016-08-14T00:58:41Z</updated>
    <published>2016-08-14T00:58:41Z</published>
    <title>Julia Implementation of the Dynamic Distributed Dimensional Data Model</title>
    <summary>  Julia is a new language for writing data analysis programs that are easy to
implement and run at high performance. Similarly, the Dynamic Distributed
Dimensional Data Model (D4M) aims to clarify data analysis operations while
retaining strong performance. D4M accomplishes these goals through a
composable, unified data model on associative arrays. In this work, we present
an implementation of D4M in Julia and describe how it enables and facilitates
data analysis. Several experiments showcase scalable performance in our new
Julia version as compared to the original Matlab implementation.
</summary>
    <author>
      <name>Alexander Chen</name>
    </author>
    <author>
      <name>Alan Edelman</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Dylan Hutchison</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HPEC.2016.7761626</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HPEC.2016.7761626" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 16 figures, IEEE HPEC 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.08168v1</id>
    <updated>2016-10-26T05:00:16Z</updated>
    <published>2016-10-26T05:00:16Z</published>
    <title>Location Aggregation of Spatial Population CTMC Models</title>
    <summary>  In this paper we focus on spatial Markov population models, describing the
stochastic evolution of populations of agents, explicitly modelling their
spatial distribution, representing space as a discrete, finite graph. More
specifically, we present a heuristic approach to aggregating spatial locations,
which is designed to preserve the dynamical behaviour of the model whilst
reducing the computational cost of analysis. Our approach combines stochastic
approximation ideas (moment closure, linear noise), with computational
statistics (spectral clustering) to obtain an efficient aggregation, which is
experimentally shown to be reasonably accurate on two case studies: an instance
of epidemic spreading and a London bike sharing scenario.
</summary>
    <author>
      <name>Luca Bortolussi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Trieste, CNR-ISTI</arxiv:affiliation>
    </author>
    <author>
      <name>Cheng Feng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Edinburgh</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.227.3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.227.3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings QAPL'16, arXiv:1610.07696</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 227, 2016, pp. 30-43</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.08168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.08168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09170v1</id>
    <updated>2016-11-28T15:16:42Z</updated>
    <published>2016-11-28T15:16:42Z</published>
    <title>DESP-C++: A Discrete-Event Simulation Package for C++</title>
    <summary>  DESP-C++ is a C++ discrete-event random simulation engine that has been
designed to be fast, very easy to use and expand, and valid. DESP-C++ is based
on the resource view. Its complete architecture is presented in detail, as well
as a short " user manual ". The validity of DESP-C++ is demonstrated by the
simulation of three significant models. In each case, the simulation results
obtained with DESP-C++ match those obtained with a validated simulation
software: QNAP2. The versatility of DESP-C++ is also illustrated this way,
since the modelled systems are very different from each other: a simple
production system, the dining philosopher classical deadlock problem, and a
complex object-oriented database management system.
</summary>
    <author>
      <name>Jérôme Darmont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMOS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Software: Practice and Experience, Wiley, 2000, 30 (1), pp.37-60</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.09170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.00456v1</id>
    <updated>2016-12-01T21:00:05Z</updated>
    <published>2016-12-01T21:00:05Z</published>
    <title>Characterising radio telescope software with the Workload
  Characterisation Framework</title>
    <summary>  We present a modular framework, the Workload Characterisation Framework
(WCF), that is developed to reproducibly obtain, store and compare key
characteristics of radio astronomy processing software. As a demonstration, we
discuss the experiences using the framework to characterise a LOFAR calibration
and imaging pipeline.
</summary>
    <author>
      <name>Y. G. Grange</name>
    </author>
    <author>
      <name>R. Lakhoo</name>
    </author>
    <author>
      <name>M. Petschow</name>
    </author>
    <author>
      <name>C. Wu</name>
    </author>
    <author>
      <name>B. Veenboer</name>
    </author>
    <author>
      <name>I. Emsley</name>
    </author>
    <author>
      <name>T. J. Dijkema</name>
    </author>
    <author>
      <name>A. P. Mechev</name>
    </author>
    <author>
      <name>G. Mariani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures; to be published in ADASS XXVI (held October
  16-20, 2016) proceedings. See
  http://www.adass2016.inaf.it/images/posters/grange.pdf for the poster</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2019, ADASS XXVI, ASP Conf. Ser., Vol 521, Eds. M. Molinaro, K.
  Shortridge, &amp; F. Pasian, 683</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1612.00456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.00456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.8; K.6.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.02557v1</id>
    <updated>2016-12-08T08:20:48Z</updated>
    <published>2016-12-08T08:20:48Z</published>
    <title>Sorting Data on Ultra-Large Scale with RADULS. New Incarnation of Radix
  Sort</title>
    <summary>  The paper introduces RADULS, a new parallel sorter based on radix sort
algorithm, intended to organize ultra-large data sets efficiently. For example
4G 16-byte records can be sorted with 16 threads in less than 15 seconds on
Intel Xeon-based workstation. The implementation of RADULS is not only highly
optimized to gain such an excellent performance, but also parallelized in a
cache friendly manner to make the most of modern multicore architectures.
Besides, our parallel scheduler launches a few different procedures at runtime,
according to the current parameters of the execution, for proper workload
management. All experiments show RADULS to be superior to competing algorithms.
</summary>
    <author>
      <name>Marek Kokot</name>
    </author>
    <author>
      <name>Sebastian Deorowicz</name>
    </author>
    <author>
      <name>Agnieszka Debudaj-Grabysz</name>
    </author>
    <link href="http://arxiv.org/abs/1612.02557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.02557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06042v1</id>
    <updated>2017-03-16T12:53:52Z</updated>
    <published>2017-03-16T12:53:52Z</published>
    <title>A Visual Web Tool to Perform What-If Analysis of Optimization Approaches</title>
    <summary>  In Operation Research, practical evaluation is essential to validate the
efficacy of optimization approaches. This paper promotes the usage of
performance profiles as a standard practice to visualize and analyze
experimental results. It introduces a Web tool to construct and export
performance profiles as SVG or HTML files. In addition, the application relies
on a methodology to estimate the benefit of hypothetical solver improvements.
Therefore, the tool allows one to employ what-if analysis to screen possible
research directions, and identify those having the best potential. The approach
is showcased on two Operation Research technologies: Constraint Programming and
Mixed Integer Linear Programming.
</summary>
    <author>
      <name>Sascha Van Cauwelaert</name>
    </author>
    <author>
      <name>Michele Lombardi</name>
    </author>
    <author>
      <name>Pierre Schaus</name>
    </author>
    <link href="http://arxiv.org/abs/1703.06042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00513v1</id>
    <updated>2017-04-03T10:22:32Z</updated>
    <published>2017-04-03T10:22:32Z</published>
    <title>Optimizing Communication by Compression for Multi-GPU Scalable
  Breadth-First Searches</title>
    <summary>  The Breadth First Search (BFS) algorithm is the foundation and building block
of many higher graph-based operations such as spanning trees, shortest paths
and betweenness centrality. The importance of this algorithm increases each day
due to it is a key requirement for many data structures which are becoming
popular nowadays. When the BFS algorithm is parallelized by distributing the
graph between several processors the interconnection network limits the
performance. Hence, improvements on this area may benefit the overall
performance of the algorithm.
  This work presents an alternative compression scheme for communications in
distributed BFS processing. It focuses on BFS processors using General-Purpose
Graphics Processing Units.
</summary>
    <author>
      <name>Julian Romera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Initial version, 105 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P30, 05C50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.4; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07983v1</id>
    <updated>2017-05-22T20:21:09Z</updated>
    <published>2017-05-22T20:21:09Z</published>
    <title>Liquid Cloud Storage</title>
    <summary>  A liquid system provides durable object storage based on spreading
redundantly generated data across a network of hundreds to thousands of
potentially unreliable storage nodes. A liquid system uses a combination of a
large code, lazy repair, and a flow storage organization. We show that a liquid
system can be operated to enable flexible and essentially optimal combinations
of storage durability, storage overhead, repair bandwidth usage, and access
performance.
</summary>
    <author>
      <name>Michael G. Luby</name>
    </author>
    <author>
      <name>Roberto Padovani</name>
    </author>
    <author>
      <name>Thomas J. Richardson</name>
    </author>
    <author>
      <name>Lorenz Minder</name>
    </author>
    <author>
      <name>Pooja Aggarwal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44 pages, 21 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.07983v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07983v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.04254v1</id>
    <updated>2017-07-13T13:51:26Z</updated>
    <published>2017-07-13T13:51:26Z</published>
    <title>Language-based Abstractions for Dynamical Systems</title>
    <summary>  Ordinary differential equations (ODEs) are the primary means to modelling
dynamical systems in many natural and engineering sciences. The number of
equations required to describe a system with high heterogeneity limits our
capability of effectively performing analyses. This has motivated a large body
of research, across many disciplines, into abstraction techniques that provide
smaller ODE systems while preserving the original dynamics in some appropriate
sense. In this paper we give an overview of a recently proposed
computer-science perspective to this problem, where ODE reduction is recast to
finding an appropriate equivalence relation over ODE variables, akin to
classical models of computation based on labelled transition systems.
</summary>
    <author>
      <name>Andrea Vandin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMT School for Advanced Studies Lucca</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.250.2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.250.2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings QAPL 2017, arXiv:1707.03668</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 250, 2017, pp. 15-24</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.04254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.04254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02108v1</id>
    <updated>2017-09-07T07:00:38Z</updated>
    <published>2017-09-07T07:00:38Z</published>
    <title>ParaPlan: A Tool for Parallel Reachability Analysis of Planar Polygonal
  Differential Inclusion Systems</title>
    <summary>  We present the ParaPlan tool which provides the reachability analysis of
planar hybrid systems defined by differential inclusions (SPDI). It uses the
parallelized and optimized version of the algorithm underlying the SPeeDI tool.
The performance comparison demonstrates the speed-up of up to 83 times with
respect to the sequential implementation on various benchmarks. Some of the
benchmarks we used are randomly generated with the novel approach based on the
partitioning of the plane with Voronoi diagrams.
</summary>
    <author>
      <name>Andrei Sandler</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Hertfordshire</arxiv:affiliation>
    </author>
    <author>
      <name>Olga Tveretina</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Hertfordshire</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.256.20</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.256.20" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings GandALF 2017, arXiv:1709.01761</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 256, 2017, pp. 283-296</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.02108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02225v2</id>
    <updated>2017-10-13T06:02:36Z</updated>
    <published>2017-09-07T13:23:27Z</published>
    <title>Enhancing KiWi - Scalable Concurrent Key-Value Map</title>
    <summary>  We take a relatively fresh wait-free, concurrent sorted map called KiWi, fix
and enhance it. First, we test its linearizability by fuzzing and applying
Wing&amp;Gong [2] linearizability test. After fixing a few bugs in the algorithm
design and its implementation, we enhance it. We design, implement and test two
new linearizable operations sizeLowerBound() and sizeUpperBound(). We further
compose these operations to create more useful operations. Last, we evaluate
the map performance because previous evaluations became obsolete due to our bug
corrections.
</summary>
    <author>
      <name>Assaf Yifrach</name>
    </author>
    <author>
      <name>Niv Gabso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02225v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02225v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00077v1</id>
    <updated>2017-09-29T20:14:47Z</updated>
    <published>2017-09-29T20:14:47Z</published>
    <title>Efficient Pattern Matching in Python</title>
    <summary>  Pattern matching is a powerful tool for symbolic computations. Applications
include term rewriting systems, as well as the manipulation of symbolic
expressions, abstract syntax trees, and XML and JSON data. It also allows for
an intuitive description of algorithms in the form of rewrite rules. We present
the open source Python module MatchPy, which offers functionality and
expressiveness similar to the pattern matching in Mathematica. In particular,
it includes syntactic pattern matching, as well as matching for commutative
and/or associative functions, sequence variables, and matching with
constraints. MatchPy uses new and improved algorithms to efficiently find
matches for large pattern sets by exploiting similarities between patterns. The
performance of MatchPy is investigated on several real-world problems.
</summary>
    <author>
      <name>Manuel Krebber</name>
    </author>
    <author>
      <name>Henrik Barthels</name>
    </author>
    <author>
      <name>Paolo Bientinesi</name>
    </author>
    <link href="http://arxiv.org/abs/1710.00077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02568v1</id>
    <updated>2017-10-06T19:56:24Z</updated>
    <published>2017-10-06T19:56:24Z</published>
    <title>High-Speed Data Dissemination over Device-to-Device Millimeter-Wave
  Networks for Highway Vehicular Communication</title>
    <summary>  Gigabit-per-second connectivity among vehicles is expected to be a key
enabling technology for sensor information sharing, in turn, resulting in safer
Intelligent Transportation Systems (ITSs). Recently proposed millimeter-wave
(mmWave) systems appear to be the only solution capable of meeting the data
rate demand imposed by future ITS services. In this poster, we assess the
performance of a mmWave device-to-device (D2D) vehicular network by
investigating the impact of system and communication parameters on end-users.
</summary>
    <author>
      <name>Andrea Tassi</name>
    </author>
    <author>
      <name>Robert J. Piechocki</name>
    </author>
    <author>
      <name>Andrew Nix</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEEE VNC 2017, Torino, IT</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.02568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06957v1</id>
    <updated>2017-10-18T22:55:43Z</updated>
    <published>2017-10-18T22:55:43Z</published>
    <title>Network Load Balancing Methods: Experimental Comparisons and Improvement</title>
    <summary>  Load balancing algorithms play critical roles in systems where the workload
has to be distributed across multiple resources, such as cores in
multiprocessor system, computers in distributed computing, and network links.
In this paper, we study and evaluate four load balancing methods: random, round
robin, shortest queue, and shortest queue with stale load information. We build
a simulation model and compare mean delay of the systems for the load balancing
methods. We also provide a method to improve shortest queue with stale load
information load balancing. A performance analysis for the improvement is also
presented in this paper.
</summary>
    <author>
      <name>Shafinaz Islam</name>
    </author>
    <link href="http://arxiv.org/abs/1710.06957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04582v1</id>
    <updated>2018-01-14T16:47:52Z</updated>
    <published>2018-01-14T16:47:52Z</published>
    <title>Distributed dynamic load balancing for task parallel programming</title>
    <summary>  In this paper, we derive and investigate approaches to dynamically load
balance a distributed task parallel application software. The load balancing
strategy is based on task migration. Busy processes export parts of their ready
task queue to idle processes. Idle--busy pairs of processes find each other
through a random search process that succeeds within a few steps with high
probability. We evaluate the load balancing approach for a block Cholesky
factorization implementation and observe a reduction in execution time on the
order of 5\% in the selected test cases.
</summary>
    <author>
      <name>Afshin Zafari</name>
    </author>
    <author>
      <name>Elisabeth Larsson</name>
    </author>
    <link href="http://arxiv.org/abs/1801.04582v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04582v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07832v1</id>
    <updated>2018-02-21T22:11:58Z</updated>
    <published>2018-02-21T22:11:58Z</published>
    <title>Comparative study of finite element methods using the Time-Accuracy-Size
  (TAS) spectrum analysis</title>
    <summary>  We present a performance analysis appropriate for comparing algorithms using
different numerical discretizations. By taking into account the total
time-to-solution, numerical accuracy with respect to an error norm, and the
computation rate, a cost-benefit analysis can be performed to determine which
algorithm and discretization are particularly suited for an application. This
work extends the performance spectrum model in Chang et. al. 2017 for
interpretation of hardware and algorithmic tradeoffs in numerical PDE
simulation. As a proof-of-concept, popular finite element software packages are
used to illustrate this analysis for Poisson's equation.
</summary>
    <author>
      <name>Justin Chang</name>
    </author>
    <author>
      <name>Maurice S. Fabien</name>
    </author>
    <author>
      <name>Matthew G. Knepley</name>
    </author>
    <author>
      <name>Richard T. Mills</name>
    </author>
    <link href="http://arxiv.org/abs/1802.07832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65Y05, 65Y20, 68N99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.11179v1</id>
    <updated>2018-03-29T17:51:18Z</updated>
    <published>2018-03-29T17:51:18Z</published>
    <title>Proof-of-Concept Examples of Performance-Transparent Programming Models</title>
    <summary>  Machine-specific optimizations command the machine to behave in a specific
way. As current programming models largely leave machine details unexposed,
they cannot accommodate direct encoding of such commands. In previous work we
have proposed the design of performance-transparent programming models to
facilitate this use-case; this report contains proof-of-concept examples of
such programming models. We demonstrate how programming model abstractions may
reveal the memory footprint, vector unit utilization and data reuse of an
application, with prediction accuracy ranging from 0 to 25 \%.
</summary>
    <author>
      <name>Benjamin Andreassen Bjørnseth</name>
    </author>
    <author>
      <name>Jan Christian Meyer</name>
    </author>
    <author>
      <name>Lasse Natvig</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Companion report to short-paper "Make Software Harder", to be
  presented at Computing Frontiers 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.11179v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.11179v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00503v1</id>
    <updated>2018-03-22T02:04:47Z</updated>
    <published>2018-03-22T02:04:47Z</published>
    <title>Decoding Superposed LoRa Signals</title>
    <summary>  Long-range low-power wireless communications, such as LoRa, are used in many
IoT and environmental monitoring applications. They typically increase the
communication range to several kilometers, at the cost of reducing the bitrate
to a few bits per seconds. Collisions further reduce the performance of these
communications. In this paper, we propose two algorithms to decode colliding
signals: one algorithm requires the transmitters to be slightly desynchronized,
and the other requires the transmitters to be synchronized. To do so, we use
the timing information to match the correct symbols to the correct
transmitters. We show that our algorithms are able to significantly improve the
overall throughput of LoRa.
</summary>
    <author>
      <name>Nancy El Rachkidy</name>
    </author>
    <author>
      <name>Alexandre Guitton</name>
    </author>
    <author>
      <name>Megumi Kaneko</name>
    </author>
    <link href="http://arxiv.org/abs/1804.00503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03256v1</id>
    <updated>2018-04-09T22:08:13Z</updated>
    <published>2018-04-09T22:08:13Z</published>
    <title>Restructuring expression dags for efficient parallelization</title>
    <summary>  In the field of robust geometric computation it is often necessary to make
exact decisions based on inexact floating-point arithmetic. One common approach
is to store the computation history in an arithmetic expression dag and to
re-evaluate the expression with increasing precision until an exact decision
can be made. We show that exact-decisions number types based on expression dags
can be evaluated faster in practice through parallelization on multiple cores.
We compare the impact of several restructuring methods for the expression dag
on its running time in a parallel environment.
</summary>
    <author>
      <name>Martin Wilhelm</name>
    </author>
    <link href="http://arxiv.org/abs/1804.03256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.01540v1</id>
    <updated>2018-10-02T23:32:32Z</updated>
    <published>2018-10-02T23:32:32Z</published>
    <title>The Effect of Data Marshalling on Computation Offloading Decisions</title>
    <summary>  We conducted an extensive set of experiments with an offloading testbed to
understand the impact that data marshalling techniques have on computation
offloading decisions. We find that the popular JSON format to marshall data
between client and server comes at a significant computational expense compared
to a minimalistic raw data transfer. The computational time is significant in
that it affects computation offloading decisions in a variety of conditions. We
outline some of these conditions.
</summary>
    <author>
      <name>Julio A. Reyes-Munoz</name>
    </author>
    <author>
      <name>Michael McGarry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.01540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.01540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.03488v1</id>
    <updated>2018-10-08T14:23:27Z</updated>
    <published>2018-10-08T14:23:27Z</published>
    <title>A Droplet Approach Based on Raptor Codes for Distributed Computing With
  Straggling Servers</title>
    <summary>  We propose a coded distributed computing scheme based on Raptor codes to
address the straggler problem. In particular, we consider a scheme where each
server computes intermediate values, referred to as droplets, that are either
stored locally or sent over the network. Once enough droplets are collected,
the computation can be completed. Compared to previous schemes in the
literature, our proposed scheme achieves lower computational delay when the
decoding time is taken into account.
</summary>
    <author>
      <name>Albin Severinson</name>
    </author>
    <author>
      <name>Alexandre Graell i Amat</name>
    </author>
    <author>
      <name>Eirik Rosnes</name>
    </author>
    <author>
      <name>Francisco Lazaro</name>
    </author>
    <author>
      <name>Gianluigi Liva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the 2018 International Symposium on Turbo Codes &amp;
  Iterative Information Processing, Hong Kong</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.03488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.03488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.00375v1</id>
    <updated>2018-12-29T17:52:53Z</updated>
    <published>2018-12-29T17:52:53Z</published>
    <title>Computing the $k$-coverage of a wireless network</title>
    <summary>  Coverage is one of the main quality of service of a wirelessnetwork.
$k$-coverage, that is to be covered simultaneously by $k$network nodes, is
synonym of reliability and numerous applicationssuch as multiple site MIMO
features, or handovers. We introduce here anew algorithm for computing the
$k$-coverage of a wirelessnetwork. Our method is based on the observation that
$k$-coverage canbe interpreted as $k$ layers of $1$-coverage, or simply
coverage. Weuse simplicial homology to compute the network's topology and
areduction algorithm to indentify the layers of $1$-coverage. Weprovide figures
and simulation results to illustrate our algorithm.
</summary>
    <author>
      <name>Anaïs Vergne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Decreusefond</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Martins</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Valuetools 2019, Mar 2019, Palma de Mallorca, Spain. 2019. arXiv
  admin note: text overlap with arXiv:1802.08442</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.00375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.00375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.05836v1</id>
    <updated>2019-01-14T17:18:26Z</updated>
    <published>2019-01-14T17:18:26Z</published>
    <title>A Multilevel Approach for the Performance Analysis of Parallel
  Algorithms</title>
    <summary>  We provide a multilevel approach for analysing performances of parallel
algorithms. The main outcome of such approach is that the algorithm is
described by using a set of operators which are related to each other according
to the problem decomposition. Decomposition level determines the granularity of
the algorithm. A set of block matrices (decomposition and execution) highlights
fundamental characteristics of the algorithm, such as inherent parallelism and
sources of overheads.
</summary>
    <author>
      <name>Luisa D'Amore</name>
    </author>
    <author>
      <name>Valeria Mele</name>
    </author>
    <author>
      <name>Diego Romano</name>
    </author>
    <author>
      <name>Giuliano Laccetti</name>
    </author>
    <link href="http://arxiv.org/abs/1901.05836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.09942v2</id>
    <updated>2019-01-30T13:40:45Z</updated>
    <published>2019-01-28T19:02:44Z</published>
    <title>On transaction parallelizability in Ethereum</title>
    <summary>  Ethereum clients execute transactions in a sequential order prescribed by the
consensus protocol. This is a safe and conservative approach to blockchain
transaction processing which forgoes running transactions in parallel even when
doing so would be beneficial and safe, e.g., when there is no intersection in
the sets of accounts that the transactions read or modify. In this work we
study the degree of transaction parallelizability and present results from
three different simulations using real Ethereum transaction data. Our
simulations demonstrate that notable gains are achievable with parallelization,
and suggest that the potential for parallelizability improves as transaction
rates increase.
</summary>
    <author>
      <name>Nadi Sarrar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.09942v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.09942v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.03018v2</id>
    <updated>2019-06-07T22:14:58Z</updated>
    <published>2019-02-08T11:08:31Z</published>
    <title>Deterministic contention management for low latency Cloud RAN over an
  optical ring</title>
    <summary>  The N-GREEN project has for goal to design a low cost optical ring technology
with good performances (throughput, latency...) without using expensive
end-to-end connections. We study the compatibility of such a technology with
the development of the Cloud RAN, a latency critical application which is a
major aspect of 5G deployment. We show that deterministically managing Cloud
RAN traffic minimizes its latency while also improving the latency of the other
traffics.
</summary>
    <author>
      <name>Dominique Barth</name>
    </author>
    <author>
      <name>Maël Guiraud</name>
    </author>
    <author>
      <name>Yann Strozecki</name>
    </author>
    <link href="http://arxiv.org/abs/1902.03018v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03018v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.10982v1</id>
    <updated>2019-03-26T16:11:47Z</updated>
    <published>2019-03-26T16:11:47Z</published>
    <title>Matrix multiplication and universal scalability of the time on the Intel
  Scalable processors</title>
    <summary>  Matrix multiplication is one of the core operations in many areas of
scientific computing. We present the results of the experiments with the matrix
multiplication of the big size comparable with the big size of the onboard
memory, which is 1.5 terabyte in our case. We run experiments on the computing
board with two sockets and with two Intel Xeon Platinum 8164 processors, each
with 26 cores and with multi-threading. The most interesting result of our
study is the observation of the perfect scalability law of the matrix
multiplication, and of the universality of this law.
</summary>
    <author>
      <name>Alexander Russkov</name>
    </author>
    <author>
      <name>Lev Shchur</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-6596/1163/1/012079</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-6596/1163/1/012079" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Physics: Conference Series, volume 1163, 012079, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.10982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.10982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.07813v1</id>
    <updated>2019-04-12T07:58:51Z</updated>
    <published>2019-04-12T07:58:51Z</published>
    <title>Energy Saving Strategy Based on Profiling</title>
    <summary>  Constraints imposed by power consumption and the related costs are one of the
key roadblocks to the design and development of next generation exascale
systems. To mitigate these issues, strategies that reduce the power consumption
of the processor are the need of the hour. Techniques such as Dynamic Voltage
and Frequency Scaling (DVFS) exist which reduce the power consumption of a
processor at runtime but they should be used in such a manner so that their
overhead does not hamper application performance. In this paper, we propose an
energy saving strategy which operates on timeslice basis to apply DVFS under a
user defined performance constraint. Results show energy savings up to 7% when
NAS benchmarks are tested on a laptop platform
</summary>
    <author>
      <name>Milan Yadav</name>
    </author>
    <author>
      <name>Kanak Khanna</name>
    </author>
    <link href="http://arxiv.org/abs/1904.07813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.07813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.08762v1</id>
    <updated>2019-04-18T13:25:52Z</updated>
    <published>2019-04-18T13:25:52Z</published>
    <title>Memory and Parallelism Analysis Using a Platform-Independent Approach</title>
    <summary>  Emerging computing architectures such as near-memory computing (NMC) promise
improved performance for applications by reducing the data movement between CPU
and memory. However, detecting such applications is not a trivial task. In this
ongoing work, we extend the state-of-the-art platform-independent software
analysis tool with NMC related metrics such as memory entropy, spatial
locality, data-level, and basic-block-level parallelism. These metrics help to
identify the applications more suitable for NMC architectures.
</summary>
    <author>
      <name>Stefano Corda</name>
    </author>
    <author>
      <name>Gagandeep Singh</name>
    </author>
    <author>
      <name>Ahsan Javed Awan</name>
    </author>
    <author>
      <name>Roel Jordans</name>
    </author>
    <author>
      <name>Henk Corporaal</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3323439.3323988</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3323439.3323988" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22nd ACM International Workshop on Software and Compilers for
  Embedded Systems (SCOPES '19), May 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.08762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.01135v1</id>
    <updated>2019-05-03T12:10:02Z</updated>
    <published>2019-05-03T12:10:02Z</published>
    <title>On the Impact of Memory Allocation on High-Performance Query Processing</title>
    <summary>  Somewhat surprisingly, the behavior of analytical query engines is crucially
affected by the dynamic memory allocator used. Memory allocators highly
influence performance, scalability, memory efficiency and memory fairness to
other processes. In this work, we provide the first comprehensive experimental
analysis on the impact of memory allocation for high-performance query engines.
We test five state-of-the-art dynamic memory allocators and discuss their
strengths and weaknesses within our DBMS. The right allocator can increase the
performance of TPC-DS (SF 100) by 2.7x on a 4-socket Intel Xeon server.
</summary>
    <author>
      <name>Dominik Durner</name>
    </author>
    <author>
      <name>Viktor Leis</name>
    </author>
    <author>
      <name>Thomas Neumann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3329785.3329918</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3329785.3329918" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">DaMoN 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.01135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.01294v1</id>
    <updated>2019-05-01T23:39:42Z</updated>
    <published>2019-05-01T23:39:42Z</published>
    <title>RedisGraph GraphBLAS Enabled Graph Database</title>
    <summary>  RedisGraph is a Redis module developed by Redis Labs to add graph database
functionality to the Redis database. RedisGraph represents connected data as
adjacency matrices. By representing the data as sparse matrices and employing
the power of GraphBLAS (a highly optimized library for sparse matrix
operations), RedisGraph delivers a fast and efficient way to store, manage and
process graphs. Initial benchmarks indicate that RedisGraph is significantly
faster than comparable graph databases.
</summary>
    <author>
      <name>Pieter Cailliau</name>
    </author>
    <author>
      <name>Tim Davis</name>
    </author>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Roi Lipman</name>
    </author>
    <author>
      <name>Jeffrey Lovitz</name>
    </author>
    <author>
      <name>Keren Ouaknine</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IPDPSW.2019.00054</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IPDPSW.2019.00054" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE IPDPS 2019 GrAPL workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.01294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.02334v3</id>
    <updated>2019-10-31T20:22:49Z</updated>
    <published>2019-05-07T02:43:56Z</published>
    <title>Internet Speed Measurement: Current Challenges and Future
  Recommendations</title>
    <summary>  Government organizations, regulators, consumers, Internet service providers,
and application providers alike all have an interest in measuring user Internet
"speed". Access speeds have increased by an order of magnitude in past years,
with gigabit speeds available to tens of millions of homes. Approaches must
evolve to accurately reflect the changing user experience and network speeds.
This paper offers historical and technical background on current speed testing
methods, highlights their limitations as access network speeds continue to
increase, and offers recommendations for the next generation of Internet
"speed" measurement.
</summary>
    <author>
      <name>Nick Feamster</name>
    </author>
    <author>
      <name>Jason Livingood</name>
    </author>
    <link href="http://arxiv.org/abs/1905.02334v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.02334v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.00050v1</id>
    <updated>2019-06-27T01:46:35Z</updated>
    <published>2019-06-27T01:46:35Z</published>
    <title>State-of-the-Art on Query &amp; Transaction Processing Acceleration</title>
    <summary>  The vast amount of processing power and memory bandwidth provided by modern
Graphics Processing Units (GPUs) make them a platform for data-intensive
applications. The database community identified GPUs as effective co-processors
for data processing. In the past years, there were many approaches to make use
of GPUs at different levels of a database system. In this Internal Technical
Report, based on the [1] and some other research papers, we identify possible
research areas at LIP6 for GPU-accelerated database management systems. We
describe some key properties, typical challenges of GPU-aware database
architectures, and identify major open challenges.
</summary>
    <author>
      <name>Bernd Amann</name>
    </author>
    <author>
      <name>Youry Khmelevsky</name>
    </author>
    <author>
      <name>Gaetan Hains</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.00050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.00050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.02064v5</id>
    <updated>2021-11-24T12:51:43Z</updated>
    <published>2019-07-02T21:04:47Z</published>
    <title>Accelerator-level Parallelism</title>
    <summary>  Future applications demand more performance, but technology advances have
been faltering. A promising approach to further improve computer system
performance under energy constraints is to employ hardware accelerators.
Already today, mobile systems concurrently employ multiple accelerators in what
we call accelerator-level parallelism (ALP). To spread the benefits of ALP more
broadly, we charge computer scientists to develop the science needed to best
achieve the performance and cost goals of ALP hardware and software.
</summary>
    <author>
      <name>Mark D. Hill</name>
    </author>
    <author>
      <name>Vijay Janapa Reddi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, &amp; 7 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.02064v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.02064v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.05560v1</id>
    <updated>2019-07-12T03:21:54Z</updated>
    <published>2019-07-12T03:21:54Z</published>
    <title>Simulating Nonlinear Neutrino Oscillations on Next-Generation Many-Core
  Architectures</title>
    <summary>  In this work an astrophysical simulation code, XFLAT, is developed to study
neutrino oscillations in supernovae. XFLAT is designed to utilize multiple
levels of parallelism through MPI, OpenMP, and SIMD instructions
(vectorization). It can run on both the CPU and the Xeon Phi co-processor, the
latter of which is based on the Intel Many Integrated Core Architecture (MIC).
The performance of XFLAT on configurations and scenarios has been analyzed. In
addition, the impact of I/O and the multi-node configuration on the Xeon
Phi-equipped heterogeneous supercomputers such as Stampede at the Texas
Advanced Computing Center (TACC) was investigated.
</summary>
    <author>
      <name>Vahid Noormofidi</name>
    </author>
    <link href="http://arxiv.org/abs/1907.05560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.05560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.10926v1</id>
    <updated>2019-08-28T19:48:10Z</updated>
    <published>2019-08-28T19:48:10Z</published>
    <title>Performance Analysis of Zippers</title>
    <summary>  A zipper is a powerful technique of representing a purely functional data
structure in a way that allows fast access to a specific element. It is often
used in cases where the imperative data structures would use a mutable pointer.
However, the efficiency of zippers as a replacement for mutable pointers is not
sufficiently explored. We attempt to address this issue by comparing the
performance of zippers and mutable pointers in two common scenarios and three
different languages: C++, C#, and Haskell.
</summary>
    <author>
      <name>Vít Šefl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Part of DECLARE 19 proceedings, 15 pages, 9 listings, 6 figures,
  source files available at https://github.com/vituscze/performance-zippers</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.10926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02351v3</id>
    <updated>2019-11-02T14:22:44Z</updated>
    <published>2019-10-06T01:10:57Z</published>
    <title>Clustering case statements for indirect branch predictors</title>
    <summary>  We present an O(nlogn) algorithm to compile a switch statement into jump
tables. To generate jump tables that can be efficiently predicted by current
hardware branch predictors, we added an upper bound on the number of entries
for each table. This modification of the previously best known algorithm
reduces the complexity from O(n^2) to O(nlogn).
</summary>
    <author>
      <name>Evandro Menezes</name>
    </author>
    <author>
      <name>Sebastian Pop</name>
    </author>
    <author>
      <name>Aditya Kumar</name>
    </author>
    <link href="http://arxiv.org/abs/1910.02351v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02351v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.0; F.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.10234v1</id>
    <updated>2019-10-22T21:14:16Z</updated>
    <published>2019-10-22T21:14:16Z</published>
    <title>The Bitlet Model: Defining a Litmus Test for the Bitwise
  Processing-in-Memory Paradigm</title>
    <summary>  This paper describes an analytical modeling tool called Bitlet that can be
used, in a parameterized fashion, to understand the affinity of workloads to
processing-in-memory (PIM) as opposed to traditional computing. The tool
uncovers interesting trade-offs between operation complexity (cycles required
to perform an operation through PIM) and other key parameters, such as system
memory bandwidth, data transfer size, the extent of data alignment, and
effective memory capacity involved in PIM computations. Despite its simplicity,
the model has already proven useful. In the future, we intend to extend and
refine Bitlet to further increase its utility.
</summary>
    <author>
      <name>Kunal Korgaonkar</name>
    </author>
    <author>
      <name>Ronny Ronen</name>
    </author>
    <author>
      <name>Anupam Chattopadhyay</name>
    </author>
    <author>
      <name>Shahar Kvatinsky</name>
    </author>
    <link href="http://arxiv.org/abs/1910.10234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.12562v2</id>
    <updated>2020-04-06T11:43:43Z</updated>
    <published>2019-10-28T11:21:58Z</published>
    <title>Bounding Mean First Passage Times in Population Continuous-Time Markov
  Chains</title>
    <summary>  We consider the problem of bounding mean first passage times for a class of
continuous-time Markov chains that captures stochastic interactions between
groups of identical agents. The quantitative analysis of such probabilistic
population models is notoriously difficult since typically neither state-based
numerical approaches nor methods based on stochastic sampling give efficient
and accurate results. Here, we propose a technique that extends recently
developed methods using semi-definite programming to determine bounds on mean
first passage times. We further apply the technique to hybrid models and
demonstrate its accuracy and efficiency for some examples from biology.
</summary>
    <author>
      <name>Michael Backenköhler</name>
    </author>
    <author>
      <name>Luca Bortolussi</name>
    </author>
    <author>
      <name>Verena Wolf</name>
    </author>
    <link href="http://arxiv.org/abs/1910.12562v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.12562v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.11661v3</id>
    <updated>2021-08-20T20:25:17Z</updated>
    <published>2019-12-25T12:53:40Z</published>
    <title>Large fork-join queues with nearly deterministic arrival and service
  times</title>
    <summary>  In this paper, we study an $N$ server fork-join queue with nearly
deterministic arrival and service times. Specifically, we present a fluid limit
for the maximum queue length as $N\to\infty$. This fluid limit depends on the
initial number of tasks. In order to prove these results, we develop extreme
value theory and diffusion approximations for the queue lengths.
</summary>
    <author>
      <name>Dennis Schol</name>
    </author>
    <author>
      <name>Maria Vlasiou</name>
    </author>
    <author>
      <name>Bert Zwart</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.11661v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11661v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.10745v1</id>
    <updated>2020-03-24T10:06:06Z</updated>
    <published>2020-03-24T10:06:06Z</published>
    <title>Towards Fine-Grained Billing For Cloud Networking</title>
    <summary>  We revisit multi-tenant network virtualization in data centers, and make the
case for tenant-specific virtual switches. In particular, tenant-specific
virtual switches allow cloud providers to extend fine-grained billing (known,
e.g., from serverless architectures) to the network, accounting not only for
IO, but also CPU or energy. We sketch an architecture and present economical
motivation and recent technological enablers. We also find that virtual
switches today do not offer sufficient multi-tenancy and can introduce
artificial performance bottlenecks, e.g., in load balancers. We conclude by
discussing additional use cases for tentant-specific switches.
</summary>
    <author>
      <name>Kashyap Thimmaraju</name>
    </author>
    <author>
      <name>Stefan Schmid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.10745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.10745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.10506v1</id>
    <updated>2020-04-22T11:41:33Z</updated>
    <published>2020-04-22T11:41:33Z</published>
    <title>A Non-Ideal NOMA-based mmWave D2D Networks with Hardware and CSI
  Imperfections</title>
    <summary>  This letter investigates a non-orthogonal multiple access (NOMA) assisted
millimeter-wave device-to-device (D2D) network practically limited by multiple
interference noises, transceiver hardware impairments, imperfect successive
interference cancellation, and channel state information mismatch. Generalized
outage probability expressions for NOMA-D2D users are deduced and achieved
results, validated by Monte Carlo simulations, are compared with the orthogonal
multiple access to show the superior performance of the proposed network model
</summary>
    <author>
      <name>Leila Tlebaldiyeva</name>
    </author>
    <author>
      <name>Galymzhan Nauryzbayev</name>
    </author>
    <author>
      <name>Sultangali Arzykulov</name>
    </author>
    <author>
      <name>Yerassyl Akhmetkaziyev</name>
    </author>
    <author>
      <name>Mohammad S. Hashmi</name>
    </author>
    <author>
      <name>Ahmed M. Eltawil</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICTC52510.2021.9621035</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICTC52510.2021.9621035" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.10506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.10506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.02683v2</id>
    <updated>2020-10-29T11:48:11Z</updated>
    <published>2020-05-06T09:32:34Z</published>
    <title>Analysis of the Symmetric Join the Shortest Orbit Queue</title>
    <summary>  This work introduces the join the shortest queue policy in the retrial
setting. We consider a Markovian single server retrial system with two infinite
capacity orbits. An arriving job finding the server busy, it is forwarded to
the least loaded orbit. Otherwise, it is forwarded to an orbit randomly.
Orbiting jobs of either type retry to access the server independently. We
investigate the stability condition, the stationary tail decay rate, and obtain
the equilibrium distribution by using the compensation method.
</summary>
    <author>
      <name>Ioannis Dimitriou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.02683v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.02683v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60K25, 90B22, 68M20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.04093v1</id>
    <updated>2020-05-05T14:31:29Z</updated>
    <published>2020-05-05T14:31:29Z</published>
    <title>Importing Relationships into a Running Graph Database Using Parallel
  Processing</title>
    <summary>  Importing relationships into a running graph database using multiple threads
running concurrently is a difficult task, as multiple threads cannot write
information to the same node at the same time. Here we present an algorithm in
which relationships are sorted into bins, then imported such that no two
threads ever access the same node concurrently. When this algorithm was
implemented as a procedure to run on the Neo4j graph database, it reduced the
time to import relationships by up to 69% when 32 threads were used.
</summary>
    <author>
      <name>Joshua Porter</name>
    </author>
    <author>
      <name>Aleks Ontman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, code provided on GitHub
  https://github.com/Lnofeisone/graph-iterateRelationship</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.04093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.04093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.07336v2</id>
    <updated>2020-08-30T18:34:56Z</updated>
    <published>2020-07-14T20:15:36Z</published>
    <title>Layer-Parallel Training with GPU Concurrency of Deep Residual Neural
  Networks via Nonlinear Multigrid</title>
    <summary>  A Multigrid Full Approximation Storage algorithm for solving Deep Residual
Networks is developed to enable neural network parallelized layer-wise training
and concurrent computational kernel execution on GPUs. This work demonstrates a
10.2x speedup over traditional layer-wise model parallelism techniques using
the same number of compute units.
</summary>
    <author>
      <name>Andrew C. Kirby</name>
    </author>
    <author>
      <name>Siddharth Samsi</name>
    </author>
    <author>
      <name>Michael Jones</name>
    </author>
    <author>
      <name>Albert Reuther</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures, 27 citations. Accepted to 2020 IEEE High
  Performance Extreme Computing Conference - Outstanding Paper Award</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.07336v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.07336v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.07163v1</id>
    <updated>2020-12-13T22:00:23Z</updated>
    <published>2020-12-13T22:00:23Z</published>
    <title>Comparing the costs of abstraction for DL frameworks</title>
    <summary>  High level abstractions for implementing, training, and testing Deep Learning
(DL) models abound. Such frameworks function primarily by abstracting away the
implementation details of arbitrary neural architectures, thereby enabling
researchers and engineers to focus on design. In principle, such frameworks
could be "zero-cost abstractions"; in practice, they incur translation and
indirection overheads. We study at which points exactly in the engineering
life-cycle of a DL model the highest costs are paid and whether they can be
mitigated. We train, test, and evaluate a representative DL model using
PyTorch, LibTorch, TorchScript, and cuDNN on representative datasets, comparing
accuracy, execution time and memory efficiency.
</summary>
    <author>
      <name>Maksim Levental</name>
    </author>
    <author>
      <name>Elena Orlova</name>
    </author>
    <link href="http://arxiv.org/abs/2012.07163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.07163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05200v1</id>
    <updated>2021-04-12T04:45:00Z</updated>
    <published>2021-04-12T04:45:00Z</published>
    <title>A Note on the Performance of Algorithms for Solving Linear Diophantine
  Equations in the Naturals</title>
    <summary>  We implement four algorithms for solving linear Diophantine equations in the
naturals: a lexicographic enumeration algorithm, a completion procedure, a
graph-based algorithm, and the Slopes algorithm. As already known, the
lexicographic enumeration algorithm and the completion procedure are slower
than the other two algorithms. We compare in more detail the graph-based
algorithm and the Slopes algorithm. In contrast to previous comparisons, our
work suggests that they are equally fast on small inputs, but the graph-based
algorithm gets much faster as the input grows. We conclude that implementations
of AC-unification algorithms should use the graph-based algorithm for maximum
efficiency.
</summary>
    <author>
      <name>Valeriu Motroi</name>
    </author>
    <author>
      <name>Stefan Ciobaca</name>
    </author>
    <link href="http://arxiv.org/abs/2104.05200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.09433v1</id>
    <updated>2021-04-19T16:27:04Z</updated>
    <published>2021-04-19T16:27:04Z</published>
    <title>A Choreographed Outline Instrumentation Algorithm for Asynchronous
  Components</title>
    <summary>  The runtime analysis of decentralised software requires instrumentation
methods that are scalable, but also minimally invasive. This paper presents a
new algorithm that instruments choreographed outline monitors. Our
instrumentation algorithm scales and reorganises monitors dynamically as the
system executes. We demonstrate the implementability of choreographed outline
instrumentation and compare it to inline instrumentation, subject to rigorous
and comprehensive benchmarking. Our results debunk the general notion that
outline monitoring is necessarily infeasible, and show that our implementation
induces runtime overhead comparable to that of its inline counterpart for many
practical cases.
</summary>
    <author>
      <name>Luca Aceto</name>
    </author>
    <author>
      <name>Duncan Paul Attard</name>
    </author>
    <author>
      <name>Adrian Francalanza</name>
    </author>
    <author>
      <name>Anna Ingólfsdóttir</name>
    </author>
    <link href="http://arxiv.org/abs/2104.09433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.09433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.10030v1</id>
    <updated>2021-04-20T15:09:53Z</updated>
    <published>2021-04-20T15:09:53Z</published>
    <title>Reproducibility Report for the Paper: QN-based Modeling and Analysis of
  Software Performance Antipatterns for Cyber-Physical Systems</title>
    <summary>  The authors have uploaded their artifact to Zenodo, which ensures a long-term
retention of the artifact. The artifact allows to re-run the experiments very
smoothly, and the dependencies are well documented. The process to regenerate
data for the figures and tables in the paper completes, and all results are
reproducible.
  This paper can thus receive the Artifacts Available badge. The software in
the artifact runs correctly with no trouble, and is relevant to the paper, thus
deserving the Artifacts Evaluated -- Functional badge. Given the successful
reproduction of all figures and tables, the Results Reproduced badge can be
assigned.
</summary>
    <author>
      <name>Alessandro Pellegrini</name>
    </author>
    <link href="http://arxiv.org/abs/2104.10030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.10030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.04791v1</id>
    <updated>2021-08-08T17:26:14Z</updated>
    <published>2021-08-08T17:26:14Z</published>
    <title>Improving MATLAB's isprime performance without arbitrary-precision
  arithmetic</title>
    <summary>  MATLAB is a numerical computing platform used by scientists, engineers,
mathematicians, and students which contains many mathematical functions,
including isprime. MATLAB's isprime function determines which elements of an
input array are prime. This research details modular arithmetic techniques, the
Miller-Rabin primality test, vectorized operations, and division-minimizing
strategies which harness the power of MATLAB's capabilities to improve
isprime's performance. The results are typically 5 to 10 times faster for small
integers and many hundreds of times faster for large integers and long arrays.
</summary>
    <author>
      <name>Travis Near</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 tables, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.04791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.04791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.05896v1</id>
    <updated>2021-09-10T02:05:20Z</updated>
    <published>2021-09-10T02:05:20Z</published>
    <title>A Precise Program Phase Identification Method Based on Frequency Domain
  Analysis</title>
    <summary>  In this paper, we present a systematic approach that transforms the program
execution trace into the frequency domain and precisely identifies program
phases. The analyzed results can be embedded into program code to mark the
starting point and execution characteristics, such as CPI (Cycles per
Instruction), of each phase. The so generated information can be applied to
runtime program phase prediction. With the precise program phase information,
more intelligent software and system optimization techniques can be further
explored and developed.
</summary>
    <author>
      <name>Hsuan-Yi Lin</name>
    </author>
    <author>
      <name>Ren-Song Tsay</name>
    </author>
    <link href="http://arxiv.org/abs/2109.05896v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.05896v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.12048v1</id>
    <updated>2021-09-24T16:13:40Z</updated>
    <published>2021-09-24T16:13:40Z</published>
    <title>Deployment and configuration of MEC apps with Simu5G</title>
    <summary>  Multi-access Edge Computing (MEC) is expected to act as the enabler for the
integration of 5G (and future 6G) communication technologies with
cloud-computing-based capabilities at the edge of the network. This will enable
low-latency and context-aware applications for users of such mobile networks.
In this paper we describe the implementation of a MEC model for the Simu5G
simulator and illustrate how to configure the environment to evaluate MEC
applications in both simulation and real-time emulation modes.
</summary>
    <author>
      <name>Alessandro Noferi</name>
    </author>
    <author>
      <name>Giovanni Nardini</name>
    </author>
    <author>
      <name>Giovanni Stea</name>
    </author>
    <author>
      <name>Antonio Virdis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in: M. Marek, G. Nardini, V. Vesely (Eds.), Proceedings of
  the 8th OMNeT++ Community Summit, Virtual Summit, September 8-10, 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.12048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.12048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.12854v1</id>
    <updated>2021-09-27T07:54:02Z</updated>
    <published>2021-09-27T07:54:02Z</published>
    <title>Quality Control Methodology for Simulation Models of Computer Network
  Protocols</title>
    <summary>  This paper summarizes know-how about modeling and simulation of computer
networking protocols we contributed to the OMNeT++ community. We propose a
methodology aiming to set a reliable ground truth for the quality of simulation
models of networking protocols. We demonstrate the application of this
methodology on our EIGRP source code pull-requested to the INET framework.
</summary>
    <author>
      <name>Vladimír Veselý</name>
    </author>
    <author>
      <name>Jan Zavřel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in: M. Marek, G. Nardini, V. Vesely (Eds.), Proceedings of
  the 8th OMNeT++ Community Summit, Virtual Summit, September 8-10, 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.12854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.12854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.15190v1</id>
    <updated>2021-09-30T15:05:54Z</updated>
    <published>2021-09-30T15:05:54Z</published>
    <title>inbaverSim: An OMNeT++ Model Framework for Content Centric Networking</title>
    <summary>  Today's networks are used primarily to move content. To cater to this
requirement Information Centric Networks (ICN) were introduced. One of the main
architectures of ICN is Content Centric Networking (CCN) and its derivative,
Named Data Networking (NDN). CCN is standardized at the Internet Engineering
Task Force (IETF) and is envisaged to replace the current Internet over time.
To evaluate large scale deployments of CCN, a model framework called the
inbaverSim is developed in OMNeT++. This work presents the architecture of this
model framework together with an example evaluation using the model framework.
The code is open source and is available at GitHub.
</summary>
    <author>
      <name>Asanga Udugama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in: M. Marek, G. Nardini, V. Vesely (Eds.), Proceedings of
  the 8th OMNeT++ Community Summit, Virtual Summit, September 8-10, 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.15190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.15190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.10357v1</id>
    <updated>2021-10-20T03:22:38Z</updated>
    <published>2021-10-20T03:22:38Z</published>
    <title>Fast Bitmap Fit: A CPU Cache Line friendly memory allocator for single
  object allocations</title>
    <summary>  Applications making excessive use of single-object based data structures
(such as linked lists, trees, etc...) can see a drop in efficiency over a
period of time due to the randomization of nodes in memory. This slow down is
due to the ineffective use of the CPU's L1/L2 cache. We present a novel
approach for mitigating this by presenting the design of a single-object memory
allocator that preserves memory locality across randomly ordered memory
allocations and deallocations.
</summary>
    <author>
      <name>Dhruv Matani</name>
    </author>
    <author>
      <name>Gaurav Menghani</name>
    </author>
    <link href="http://arxiv.org/abs/2110.10357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.10357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.10572v1</id>
    <updated>2021-11-20T12:24:36Z</updated>
    <published>2021-11-20T12:24:36Z</published>
    <title>Control Analysis of Packet Transmission Algorithms: Study on Fairness
  and Stability</title>
    <summary>  This document is a study of fairness, feedback and stability notions of
different packet transmission algorithms. We start the discussion with defining
two scalable control algorithms namely primal and dual algorithm. We discuss
the dual algorithm model and then understand the fair dual algorithm. Further,
we discuss different notions of fairness under fair dual algorithm those
correspond to TCP and RCP congestion control protocols. Feedback parameters are
analyzed in each of these fairness algorithms and thus their stability is
studied.
</summary>
    <author>
      <name>Lokesh Bommisetty</name>
    </author>
    <link href="http://arxiv.org/abs/2111.10572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.10572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.06617v1</id>
    <updated>2021-12-10T14:24:28Z</updated>
    <published>2021-12-10T14:24:28Z</published>
    <title>(R)SE challenges in HPC</title>
    <summary>  We discuss some specific software engineering challenges in the field of
high-performance computing, and argue that the slow adoption of SE tools and
techniques is at least in part caused by the fact that these do not address the
HPC challenges `out-of-the-box'. By giving some examples of solutions for
designing, testing and benchmarking HPC software, we intend to bring software
engineering and HPC closer together.
</summary>
    <author>
      <name>Jonas Thies</name>
    </author>
    <author>
      <name>Melven Röhrig-Zöllner</name>
    </author>
    <author>
      <name>Achim Basermann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, whitepaper for the RSE-HPC-2021 workshop on the SC'21,
  https://us-rse.org/rse-hpc-2021/</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.06617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.06617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65Y05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.4; D.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.09860v1</id>
    <updated>2021-12-18T06:58:36Z</updated>
    <published>2021-12-18T06:58:36Z</published>
    <title>Morpheme Boundary Detection &amp; Grammatical Feature Prediction for
  Gujarati : Dataset &amp; Model</title>
    <summary>  Developing Natural Language Processing resources for a low resource language
is a challenging but essential task. In this paper, we present a Morphological
Analyzer for Gujarati. We have used a Bi-Directional LSTM based approach to
perform morpheme boundary detection and grammatical feature tagging. We have
created a data set of Gujarati words with lemma and grammatical features. The
Bi-LSTM based model of Morph Analyzer discussed in the paper handles the
language morphology effectively without the knowledge of any hand-crafted
suffix rules. To the best of our knowledge, this is the first dataset and morph
analyzer model for the Gujarati language which performs both grammatical
feature tagging and morpheme boundary detection tasks.
</summary>
    <author>
      <name>Jatayu Baxi</name>
    </author>
    <author>
      <name>Brijesh Bhatt</name>
    </author>
    <link href="http://arxiv.org/abs/2112.09860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.09860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.1723v2</id>
    <updated>2008-07-02T08:39:05Z</updated>
    <published>2008-03-12T08:57:50Z</published>
    <title>Estimation of available bandwidth and measurement infrastructure for
  Russian segment of Internet</title>
    <summary>  In paper the method for estimation of available bandwidth is supposed which
does not demand the advanced utilities. Our method is based on the measurement
of network delay $D$ for packets of different sizes $W$. The simple expression
for available bandwidth $B_{av} =(W_2-W_1)/(D_2-D_1)$ is substantiated. For the
experimental testing the measurement infrastructure for Russian segment of
Internet was installed in framework of RFBR grant 06-07-89074.
</summary>
    <author>
      <name>A. P. Platonov</name>
    </author>
    <author>
      <name>D. I. Sidelnikov</name>
    </author>
    <author>
      <name>M. V. Strizhov</name>
    </author>
    <author>
      <name>A. M. Sukhov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures, submitted to Telecommunication (www.nait.ru, in
  Russian)</arxiv:comment>
    <link href="http://arxiv.org/abs/0803.1723v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.1723v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.5986v1</id>
    <updated>2011-05-30T13:20:43Z</updated>
    <published>2011-05-30T13:20:43Z</published>
    <title>A Modeling Framework for Gossip-based Information Spread</title>
    <summary>  We present an analytical framework for gossip protocols based on the pairwise
information exchange between interacting nodes. This framework allows for
studying the impact of protocol parameters on the performance of the protocol.
Previously, gossip-based information dissemination protocols have been analyzed
under the assumption of perfect, lossless communication channels. We extend our
framework for the analysis of networks with lossy channels. We show how the
presence of message loss, coupled with specific topology configurations,impacts
the expected behavior of the protocol. We validate the obtained models against
simulations for two protocols.
</summary>
    <author>
      <name>Rena Bakhshi</name>
    </author>
    <author>
      <name>Daniela Gavidia</name>
    </author>
    <author>
      <name>Wan Fokkink</name>
    </author>
    <author>
      <name>Maarten van Steen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, including appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.5986v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.5986v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.3513v2</id>
    <updated>2013-07-01T18:59:31Z</updated>
    <published>2013-06-14T20:55:54Z</published>
    <title>A Simple Policy for Multiple Queues with Size-Independent Service Times</title>
    <summary>  We consider a service system with two Poisson arrival queues. A server
chooses which queue to serve at each moment. Once a queue is served, all the
customers will be served within a fixed amount of time. This model is useful in
studying airport shuttling or certain online computing systems. We propose a
simple yet optimal state-independent policy for this problem which is not only
easy to implement, but also performs very well.
</summary>
    <author>
      <name>Yuhang Liu</name>
    </author>
    <author>
      <name>Zizhuo Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1306.3513v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.3513v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.02413v1</id>
    <updated>2015-03-09T09:59:58Z</updated>
    <published>2015-03-09T09:59:58Z</published>
    <title>Stochastic Service Placement</title>
    <summary>  Resource allocation for cloud services is a complex task due to the diversity
of the services and the dynamic workloads. One way to address this is by
overprovisioning which results in high cost due to the unutilized resources. A
much more economical approach, relying on the stochastic nature of the demand,
is to allocate just the right amount of resources and use additional more
expensive mechanisms in case of overflow situations where demand exceeds the
capacity. In this paper we study this approach and show both by comprehensive
analysis for independent normal distributed demands and simulation on synthetic
data that it is significantly better than currently deployed methods.
</summary>
    <author>
      <name>Galia Shabtai</name>
    </author>
    <author>
      <name>Danny Raz</name>
    </author>
    <author>
      <name>Yuval Shavitt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.02413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.02413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01407v2</id>
    <updated>2017-11-07T03:33:29Z</updated>
    <published>2017-11-04T07:11:22Z</published>
    <title>Timing Aware Dummy Metal Fill Methodology</title>
    <summary>  In this paper, we analyzed parasitic coupling capacitance coming from dummy
metal fill and its impact on timing. Based on the modeling, we proposed two
approaches to minimize the timing impact from dummy metal fill. The first
approach applies more spacing between critical nets and metal fill, while the
second approach leverages the shielding effects of reference nets. Experimental
results show consistent improvement compared to traditional metal fill method.
</summary>
    <author>
      <name>Luis Charre</name>
    </author>
    <author>
      <name>Bruno Gravano</name>
    </author>
    <author>
      <name>Rémi Pôssas</name>
    </author>
    <author>
      <name>Chen Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.01407v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01407v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04172v1</id>
    <updated>2017-11-11T17:06:56Z</updated>
    <published>2017-11-11T17:06:56Z</published>
    <title>Depth First Always On Routing Trace Algorithm</title>
    <summary>  In this paper, we discussed current limitation in the
electronic-design-automotation (EDA) tool on tracing the always on routing. We
developed an algorithm to efficiently track the secondary power routing and
accurately estimate the routing quality using approximate voltage drop as the
criteria. The fast check can identify potential hotspot issues without going
through sign-off checks. It helps designers to capture issues at early stages
and fix the issues with less design effort. We also discussed some limitations
to our algorithm.
</summary>
    <author>
      <name>Anthony Kim</name>
    </author>
    <author>
      <name>Sung Hyun Chen</name>
    </author>
    <author>
      <name>Chen Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.04172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07440v1</id>
    <updated>2017-11-20T17:50:54Z</updated>
    <published>2017-11-20T17:50:54Z</published>
    <title>Deep Reinforcement Learning for Multi-Resource Multi-Machine Job
  Scheduling</title>
    <summary>  Minimizing job scheduling time is a fundamental issue in data center networks
that has been extensively studied in recent years. The incoming jobs require
different CPU and memory units, and span different number of time slots. The
traditional solution is to design efficient heuristic algorithms with
performance guarantee under certain assumptions. In this paper, we improve a
recently proposed job scheduling algorithm using deep reinforcement learning
and extend it to multiple server clusters. Our study reveals that deep
reinforcement learning method has the potential to outperform traditional
resource allocation algorithms in a variety of complicated environments.
</summary>
    <author>
      <name>Weijia Chen</name>
    </author>
    <author>
      <name>Yuedong Xu</name>
    </author>
    <author>
      <name>Xiaofeng Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1711.07440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00680v1</id>
    <updated>2018-05-02T08:53:35Z</updated>
    <published>2018-05-02T08:53:35Z</published>
    <title>BUDAMAF: Data Management in Cloud Federations</title>
    <summary>  Data management has always been a multi-domain problem even in the simplest
cases. It involves, quality of service, security, resource management, cost
management, incident identification, disaster avoidance and/or recovery, as
well as many other concerns. In our case, this situation gets ever more
complicated because of the divergent nature of a cloud federation like BASMATI.
In this federation, the BASMATI Unified Data Management Framework (BUDaMaF),
tries to create an automated uniform way of managing all the data transactions,
as well as the data stores themselves, in a polyglot multi-cloud, consisting of
a plethora of different machines and data store systems.
</summary>
    <author>
      <name>Evangelos Psomakelis</name>
    </author>
    <author>
      <name>Konstantinos Tserpes</name>
    </author>
    <author>
      <name>Dimosthenis Anagnostopoulos</name>
    </author>
    <author>
      <name>Theodora Varvarigou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.00680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06693v1</id>
    <updated>2018-05-17T10:49:50Z</updated>
    <published>2018-05-17T10:49:50Z</published>
    <title>Hierarchical Beamforming: Resource Allocation, Fairness and Flow Level
  Performance</title>
    <summary>  We consider hierarchical beamforming in wireless networks. For a given
population of flows, we propose computationally efficient algorithms for fair
rate allocation including proportional fairness and max-min fairness. We next
propose closed-form formulas for flow level performance, for both elastic (with
either proportional fairness and max-min fairness) and streaming traffic. We
further assess the performance of hierarchical beamforming using numerical
experiments. Since the proposed solutions have low complexity compared to
conventional beamforming, our work suggests that hierarchical beamforming is a
promising candidate for the implementation of beamforming in future cellular
networks.
</summary>
    <author>
      <name>Julien Floquet</name>
    </author>
    <author>
      <name>Richard Combes</name>
    </author>
    <author>
      <name>Zwi Altman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.03421v1</id>
    <updated>2018-07-19T12:32:54Z</updated>
    <published>2018-07-19T12:32:54Z</published>
    <title>Performance Evaluation of the Quorum Blockchain Platform</title>
    <summary>  Quorum is a permissioned blockchain platform built from the Ethereum codebase
with adaptations to make it a permissioned consortium platform. It is one of
the key contenders in the permissioned ledger space. Quorum supports
confidentiality and privacy of smart contracts and transactions, and crash and
Byzantine fault tolerant consensus algorithms. In this paper, we characterize
the performance features of Quorum. We study the throughput and latency
characteristics of Quorum with different workloads and consensus algorithms
that it supports. Through a suite of micro-benchmarks, we explore how certain
transaction and smart contract parameters can affect transaction latencies.
</summary>
    <author>
      <name>Arati Baliga</name>
    </author>
    <author>
      <name>I Subhod</name>
    </author>
    <author>
      <name>Pandurang Kamat</name>
    </author>
    <author>
      <name>Siddhartha Chatterjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages including references</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.03421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.03421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.04151v1</id>
    <updated>2018-09-11T20:47:06Z</updated>
    <published>2018-09-11T20:47:06Z</published>
    <title>Feature-Specific Profiling</title>
    <summary>  While high-level languages come with significant readability and
maintainability benefits, their performance remains difficult to predict. For
example, programmers may unknowingly use language features inappropriately,
which cause their programs to run slower than expected. To address this issue,
we introduce feature-specific profiling, a technique that reports performance
costs in terms of linguistic constructs. Feature-specific profilers help
programmers find expensive uses of specific features of their language. We
describe the architecture of a profiler that implements our approach, explain
prototypes of the profiler for two languages with different characteristics and
implementation strategies, and provide empirical evidence for the approach's
general usefulness as a performance debugging tool.
</summary>
    <author>
      <name>Leif Andersen</name>
    </author>
    <author>
      <name>Vincent St-Amour</name>
    </author>
    <author>
      <name>Jan Vitek</name>
    </author>
    <author>
      <name>Matthias Felleisen</name>
    </author>
    <link href="http://arxiv.org/abs/1809.04151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.04151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.04324v1</id>
    <updated>2018-09-12T09:23:42Z</updated>
    <published>2018-09-12T09:23:42Z</published>
    <title>Poster Abstract: LPWA-MAC - a Low Power Wide Area network MAC protocol
  for cyber-physical systems</title>
    <summary>  Low-Power Wide-Area Networks (LPWANs) are being successfully used for the
monitoring of large-scale systems that are delay-tolerant and which have
low-bandwidth requirements. The next step would be instrumenting these for the
control of Cyber-Physical Systems (CPSs) distributed over large areas which
require more bandwidth, bounded delays and higher reliability or at least more
rigorous guarantees therein. This paper presents LPWA-MAC, a novel Low Power
Wide-Area network MAC protocol, that ensures bounded end-to-end delays, high
channel utility and supports many of the different traffic patterns and
data-rates typical of CPS.
</summary>
    <author>
      <name>Laksh Bhatia</name>
    </author>
    <author>
      <name>Ivana Tomic</name>
    </author>
    <author>
      <name>Julie A. McCann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3274783.3275183</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3274783.3275183" rel="related"/>
    <link href="http://arxiv.org/abs/1809.04324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.04324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.03131v1</id>
    <updated>2018-11-20T10:25:56Z</updated>
    <published>2018-11-20T10:25:56Z</published>
    <title>Playing with and against Hedge</title>
    <summary>  Hedge has been proposed as an adaptive scheme, which guides an agent's
decision in resource selection and distribution problems that can be modeled as
a multi-armed bandit full information game. Such problems are encountered in
the areas of computer and communication networks, e.g. network path selection,
load distribution, network interdiction, and also in problems in the area of
transportation. We study Hedge under the assumption that the total loss that
can be suffered by the player in each round is upper bounded. In this paper, we
study the worst performance of Hedge.
</summary>
    <author>
      <name>Miltiades E. Anagnostou</name>
    </author>
    <author>
      <name>Maria A. Lambrou</name>
    </author>
    <link href="http://arxiv.org/abs/1812.03131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.03131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.05737v1</id>
    <updated>2019-06-13T14:59:17Z</updated>
    <published>2019-06-13T14:59:17Z</published>
    <title>A JIT Compiler for Neural Network Inference</title>
    <summary>  This paper describes a C++ library that compiles neural network models at
runtime into machine code that performs inference. This approach in general
promises to achieve the best performance possible since it is able to integrate
statically known properties of the network directly into the code. In our
experiments on the NAO V6 platform, it outperforms existing implementations
significantly on small networks, while being inferior on large networks. The
library was already part of the B-Human code release 2018, but has been
extended since and is now available as a standalone version that can be
integrated into any C++14 code base.
</summary>
    <author>
      <name>Felix Thielke</name>
    </author>
    <author>
      <name>Arne Hasselbring</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-35699-6_36</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-35699-6_36" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, to appear in RoboCup 2019: Robot World Cup XXIII, Springer</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.05737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.00394v1</id>
    <updated>2019-09-01T13:25:38Z</updated>
    <published>2019-09-01T13:25:38Z</published>
    <title>Improving the Effective Utilization of Supercomputer Resources by Adding
  Low-Priority Containerized Jobs</title>
    <summary>  We propose an approach to utilize idle computational resources of
supercomputers. The idea is to maintain an additional queue of low-priority
non-parallel jobs and execute them in containers, using container migration
tools to break the execution down into separate intervals. We propose a
container management system that can maintain this queue and interact with the
supercomputer scheduler. We conducted a series of experiments simulating
supercomputer scheduler and the proposed system. The experiments demonstrate
that the proposed system increases the effective utilization of supercomputer
resources under most of the conditions, in some cases significantly improving
the performance.
</summary>
    <author>
      <name>Julia Dubenskaya</name>
    </author>
    <author>
      <name>Stanislav Polyakov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CEUR Workshop Proceedings. - 2019. - Vol. 2406. - P. 43-53</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1909.00394v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00394v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.02765v2</id>
    <updated>2019-10-03T06:04:24Z</updated>
    <published>2019-09-06T08:36:05Z</published>
    <title>ILP-M Conv: Optimize Convolution Algorithm for Single-Image Convolution
  Neural Network Inference on Mobile GPUs</title>
    <summary>  Convolution neural networks are widely used for mobile applications. However,
GPU convolution algorithms are designed for mini-batch neural network training,
the single-image convolution neural network inference algorithm on mobile GPUs
is not well-studied. After discussing the usage difference and examining the
existing convolution algorithms, we proposed the HNTMP convolution algorithm.
The HNTMP convolution algorithm achieves $14.6 \times$ speedup than the most
popular \textit{im2col} convolution algorithm, and $2.30 \times$ speedup than
the fastest existing convolution algorithm (direct convolution) as far as we
know.
</summary>
    <author>
      <name>Zhuoran Ji</name>
    </author>
    <link href="http://arxiv.org/abs/1909.02765v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02765v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.02852v2</id>
    <updated>2019-09-09T16:56:11Z</updated>
    <published>2019-09-02T19:21:12Z</published>
    <title>Efficient Lock-Free Durable Sets</title>
    <summary>  Non-volatile memory is expected to co-exist or replace DRAM in upcoming
architectures. Durable concurrent data structures for non-volatile memories are
essential building blocks for constructing adequate software for use with these
architectures. In this paper, we propose a new approach for durable concurrent
sets and use this approach to build the most efficient durable hash tables
available today. Evaluation shows a performance improvement factor of up to
3.3x over existing technology.
</summary>
    <author>
      <name>Yoav Zuriel</name>
    </author>
    <author>
      <name>Michal Friedman</name>
    </author>
    <author>
      <name>Gali Sheffi</name>
    </author>
    <author>
      <name>Nachshon Cohen</name>
    </author>
    <author>
      <name>Erez Petrank</name>
    </author>
    <link href="http://arxiv.org/abs/1909.02852v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02852v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.09390v1</id>
    <updated>2019-09-20T09:37:06Z</updated>
    <published>2019-09-20T09:37:06Z</published>
    <title>SPSC: a new execution policy for exploring discrete-time stochastic
  simulations</title>
    <summary>  In this paper, we introduce a new method called SPSC (Simulation,
Partitioning, Selection, Cloning) to estimate efficiently the probability of
possible solutions in stochastic simulations. This method can be applied to any
type of simulation, however it is particularly suitable for multi-agent-based
simulations (MABS). Therefore, its performance is evaluated on a well-known
MABS and compared to the classical approach, i.e., Monte Carlo.
</summary>
    <author>
      <name>Yu-Lin Huang</name>
    </author>
    <author>
      <name>Gildas Morvan</name>
    </author>
    <author>
      <name>Frédéric Pichon</name>
    </author>
    <author>
      <name>David Mercier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in PRIMA 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.09390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.09390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.10562v1</id>
    <updated>2019-09-23T18:29:25Z</updated>
    <published>2019-09-23T18:29:25Z</published>
    <title>AI Matrix: A Deep Learning Benchmark for Alibaba Data Centers</title>
    <summary>  Alibaba has China's largest e-commerce platform. To support its diverse
businesses, Alibaba has its own large-scale data centers providing the
computing foundation for a wide variety of software applications. Among these
applications, deep learning (DL) has been playing an important role in
delivering services like image recognition, objection detection, text
recognition, recommendation, and language processing. To build more efficient
data centers that deliver higher performance for these DL applications, it is
important to understand their computational needs and use that information to
guide the design of future computing infrastructure. An effective way to
achieve this is through benchmarks that can fully represent Alibaba's DL
applications.
</summary>
    <author>
      <name>Wei Zhang</name>
    </author>
    <author>
      <name>Wei Wei</name>
    </author>
    <author>
      <name>Lingjie Xu</name>
    </author>
    <author>
      <name>Lingling Jin</name>
    </author>
    <author>
      <name>Cheng Li</name>
    </author>
    <link href="http://arxiv.org/abs/1909.10562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.10562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.05952v2</id>
    <updated>2022-08-04T15:10:43Z</updated>
    <published>2020-01-16T17:23:07Z</published>
    <title>On Expert Behaviors and Question Types for Efficient Query-Based
  Ontology Fault Localization</title>
    <summary>  We challenge existing query-based ontology fault localization methods wrt.
assumptions they make, criteria they optimize, and interaction means they use.
We find that their efficiency depends largely on the behavior of the
interacting expert, that performed calculations can be inefficient or
imprecise, and that used optimization criteria are often not fully realistic.
As a remedy, we suggest a novel (and simpler) interaction approach which
overcomes all identified problems and, in comprehensive experiments on faulty
real-world ontologies, enables a successful fault localization while requiring
fewer expert interactions in 66 % of the cases, and always at least 80 % less
expert waiting time, compared to existing methods.
</summary>
    <author>
      <name>Patrick Rodler</name>
    </author>
    <link href="http://arxiv.org/abs/2001.05952v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05952v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.06108v1</id>
    <updated>2020-01-16T23:07:11Z</updated>
    <published>2020-01-16T23:07:11Z</published>
    <title>Performance Evaluation of Multiparty Authentication in 5G IIoT
  Environments</title>
    <summary>  With the rapid development of various emerging technologies such as the
Industrial Internet of Things (IIoT), there is a need to secure communications
between such devices. Communication system delays are one of the factors that
adversely affect the performance of an authentication system. 5G networks
enable greater data throughput and lower latency, which presents new
opportunities for the secure authentication of business transactions between
IIoT devices. We evaluate an approach to developing a flexible and secure model
for authenticating IIoT components in dynamic 5G environments.
</summary>
    <author>
      <name>Hussain Al-Aqrabi</name>
    </author>
    <author>
      <name>Phil Lane</name>
    </author>
    <author>
      <name>Richard Hill</name>
    </author>
    <link href="http://arxiv.org/abs/2001.06108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.06108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.01827v1</id>
    <updated>2020-08-04T21:01:30Z</updated>
    <published>2020-08-04T21:01:30Z</published>
    <title>High performance on-demand de-identification of a petabyte-scale medical
  imaging data lake</title>
    <summary>  With the increase in Artificial Intelligence driven approaches, researchers
are requesting unprecedented volumes of medical imaging data which far exceed
the capacity of traditional on-premise client-server approaches for making the
data research analysis-ready. We are making available a flexible solution for
on-demand de-identification that combines the use of mature software
technologies with modern cloud-based distributed computing techniques to enable
faster turnaround in medical imaging research. The solution is part of a
broader platform that supports a secure high performance clinical data science
platform.
</summary>
    <author>
      <name>Joseph Mesterhazy</name>
    </author>
    <author>
      <name>Garrick Olson</name>
    </author>
    <author>
      <name>Somalee Datta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.01827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.01827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.08478v1</id>
    <updated>2020-08-19T14:38:07Z</updated>
    <published>2020-08-19T14:38:07Z</published>
    <title>Evaluating the Performance of NVIDIA's A100 Ampere GPU for Sparse Linear
  Algebra Computations</title>
    <summary>  GPU accelerators have become an important backbone for scientific high
performance computing, and the performance advances obtained from adopting new
GPU hardware are significant. In this paper we take a first look at NVIDIA's
newest server line GPU, the A100 architecture part of the Ampere generation.
Specifically, we assess its performance for sparse linear algebra operations
that form the backbone of many scientific applications and assess the
performance improvements over its predecessor.
</summary>
    <author>
      <name>Yuhsiang Mike Tsai</name>
    </author>
    <author>
      <name>Terry Cojean</name>
    </author>
    <author>
      <name>Hartwig Anzt</name>
    </author>
    <link href="http://arxiv.org/abs/2008.08478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.08478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.02995v2</id>
    <updated>2021-09-09T08:13:37Z</updated>
    <published>2020-09-07T10:23:08Z</published>
    <title>Collaborative Management of Benchmark Instances and their Attributes</title>
    <summary>  Experimental evaluation is an integral part in the design process of
algorithms. Publicly available benchmark instances are widely used to evaluate
methods in SAT solving. For the interpretation of results and the design of
algorithm portfolios their attributes are crucial. Capturing the interrelation
of benchmark instances and their attributes is considerably simplified through
our specification of a benchmark instance identifier. Thus, our tool increases
the availability of both by providing means to manage and retrieve benchmark
instances by their attributes and vice versa. Like this, it facilitates the
design and analysis of SAT experiments and the exchange of results.
</summary>
    <author>
      <name>Markus Iser</name>
    </author>
    <author>
      <name>Luca Springer</name>
    </author>
    <author>
      <name>Carsten Sinz</name>
    </author>
    <link href="http://arxiv.org/abs/2009.02995v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.02995v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.00273v1</id>
    <updated>2021-01-30T16:59:50Z</updated>
    <published>2021-01-30T16:59:50Z</published>
    <title>BAMSim Simulator</title>
    <summary>  Resource allocation is an essential design aspect for current systems and
bandwidth allocation is an essential design aspect in multi-protocol label
switched and OpenFlow/SDN network infrastructures. The bandwidth allocation
models (BAMs) are an alternative to allocate and share bandwidth among network
users. BAMs have an extensive number of parameters that need to be defined and
tuned to achieve an expected network performance. This paper presents the
BAMSim simulator to support the network manager decision process in choosing a
set of BAM configuration parameters for network design or during network
operation.
</summary>
    <author>
      <name>Rafael F. Reale</name>
    </author>
    <author>
      <name>Walter P. neto</name>
    </author>
    <author>
      <name>Joberto S. B. Martins</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.4473102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.4473102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages; 1 figure, International Workshop on ADVANCEs in ICT
  Infrastructures and Services (ADVANCE 2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.00273v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.00273v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.08778v2</id>
    <updated>2021-03-21T15:07:02Z</updated>
    <published>2021-01-25T22:18:48Z</published>
    <title>Large-Scale Benchmarks for the Job Shop Scheduling Problem</title>
    <summary>  This report contains the description of two novel job shop scheduling
benchmarks that resemble instances of real scheduling problem as they appear in
industry. In particular, the aim was to provide large-scale benchmarks (up to 1
million operations) to test the state-of-the-art scheduling solutions on
problems that are closer to what occurs in a real industrial context. The first
benchmark is an extension of the well known Taillard benchmark (1992), while
the second is a collection of scheduling instances with a known-optimum
solution.
</summary>
    <author>
      <name>Giacomo Da Col</name>
    </author>
    <author>
      <name>Erich Teppan</name>
    </author>
    <link href="http://arxiv.org/abs/2102.08778v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.08778v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.11198v1</id>
    <updated>2021-02-22T17:24:08Z</updated>
    <published>2021-02-22T17:24:08Z</published>
    <title>Reading from External Memory</title>
    <summary>  Modern external memory is represented by several device classes. At present,
HDD, SATA SSD and NVMe SSD are widely used. Recently ultra-low latency SSD such
as Intel Optane became available on the market. Each of these types exhibits
it's own pattern for throughput, latency and parallelism. To achieve the
highest performance one has to pick an appropriate I/O interface provided by
the operating system. In this work we present a detailed overview and
evaluation of modern storage reading performance with regard to available Linux
synchronous and asynchronous interfaces. While throughout this work we aim for
the highest throughput we also measure latency and CPU usage. We provide this
report in hope the detailed results could be interesting to both researchers
and practitioners.
</summary>
    <author>
      <name>Ruslan Savchenko</name>
    </author>
    <link href="http://arxiv.org/abs/2102.11198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.11198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.03357v1</id>
    <updated>2021-07-07T17:03:51Z</updated>
    <published>2021-07-07T17:03:51Z</published>
    <title>Performance Evaluation of Mixed-Precision Runge-Kutta Methods</title>
    <summary>  Additive Runge-Kutta methods designed for preserving highly accurate
solutions in mixed-precision computation were proposed and analyzed in [8].
These specially designed methods use reduced precision or the implicit
computations and full precision for the explicit computations. We develop a
FORTRAN code to solve a nonlinear system of ordinary differential equations
using the mixed precision additive Runge-Kutta (MP-ARK) methods on IBM POWER9
and Intel x86\_64 chips. The convergence, accuracy, runtime, and energy
consumption of these methods is explored. We show that these MP-ARK methods
efficiently produce accurate solutions with significant reductions in runtime
(and by extension energy consumption).
</summary>
    <author>
      <name>Ben Burnett</name>
    </author>
    <author>
      <name>Sigal Gottlieb</name>
    </author>
    <author>
      <name>Zachary J. Grant</name>
    </author>
    <author>
      <name>Alfa Heryudono</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE HPEC 2021 submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.03357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.03357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.12550v1</id>
    <updated>2021-07-27T01:57:03Z</updated>
    <published>2021-07-27T01:57:03Z</published>
    <title>Accelerated Multiple Precision Direct Method and Mixed Precision
  Iterative Refinement on Python Programming Environment</title>
    <summary>  Current Python programming environment does not have any reliable and
efficient multiple precision floating-point (MPF) arithmetic except ``mpmath"
and ``gmpy2" packages based on GNU MP(GMP) and MPFR libraries. Although it is
well known that multi-component-type MPF library can be utilized for middle
length precision arithmetic under 200 bits, they are not widely used on Python
environment. In this paper, we describe our accelerated MPF direct method with
AVX2 techniques and its application to mixed precision iterative refinement
combined with mpmath, and demonstrate their efficiency on x86\_64 computational
environments.
</summary>
    <author>
      <name>Tomonori Kouya</name>
    </author>
    <link href="http://arxiv.org/abs/2107.12550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.12550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.08613v1</id>
    <updated>2022-02-17T11:51:48Z</updated>
    <published>2022-02-17T11:51:48Z</published>
    <title>On the evaluation of (meta-)solver approaches</title>
    <summary>  Meta-solver approaches exploits a number of individual solvers to potentially
build a better solver. To assess the performance of meta-solvers, one can
simply adopt the metrics typically used for individual solvers (e.g., runtime
or solution quality), or employ more specific evaluation metrics (e.g., by
measuring how close the meta-solver gets to its virtual best performance). In
this paper, based on some recently published works, we provide an overview of
different performance metrics for evaluating (meta-)solvers, by underlying
their strengths and weaknesses.
</summary>
    <author>
      <name>Roberto Amadini</name>
    </author>
    <author>
      <name>Maurizio Gabbrielli</name>
    </author>
    <author>
      <name>Tong Liu</name>
    </author>
    <author>
      <name>Jacopo Mauro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1613/jair.1.14102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1613/jair.1.14102" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Artificial Intelligence Research. 76 (2023) 705-719</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2202.08613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.08613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.14741v1</id>
    <updated>2022-05-29T19:22:57Z</updated>
    <published>2022-05-29T19:22:57Z</published>
    <title>CP2K on the road to exascale</title>
    <summary>  The CP2K program package, which can be considered as the swiss army knife of
atomistic simulations, is presented with a special emphasis on ab-initio
molecular dynamics using the second-generation Car-Parrinello method. After
outlining current and near-term development efforts with regards to massively
parallel low-scaling post-Hartree-Fock and eigenvalue solvers, novel approaches
on how we plan to take full advantage of future low-precision hardware
architectures are introduced. Our focus here is on combining our submatrix
method with the approximate computing paradigm to address the immanent exascale
era.
</summary>
    <author>
      <name>Thomas D. Kühne</name>
    </author>
    <author>
      <name>Christian Plessl</name>
    </author>
    <author>
      <name>Robert Schade</name>
    </author>
    <author>
      <name>Ole Schütt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.14741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.14741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.15440v1</id>
    <updated>2022-05-30T21:35:57Z</updated>
    <published>2022-05-30T21:35:57Z</published>
    <title>Lithium-Ion Battery Charging Schedule Optimization to Balance Battery
  Usage and Degradation</title>
    <summary>  This work optimizes a lithium-ion battery charging schedule while considering
a joint revenue and battery degradation model. The study extends the work of
Meheswari et. al. to encourage battery usage/charging at optimal intervals
depending on energy cost forecasts. This paper utilizes central difference
Nesterov momentum gradient descent to come to optimal charging strategies and
deal with the non-linearities of the battery degradation model. This
optimization strategy is tested against constant, random varied price forecasts
and a novel Gaussian process cost forecasting model. Contrary to many other
papers regarding battery charging, formulating schedule optimization as a
multivariate optimization problem provides meaningful insight to the inherent
balance between these two competing objectives.
</summary>
    <author>
      <name>Jacob Azoulay</name>
    </author>
    <author>
      <name>Nico Carballal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.15440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.15440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.04490v1</id>
    <updated>2022-06-09T13:21:00Z</updated>
    <published>2022-06-09T13:21:00Z</published>
    <title>Redundancy in Deep Linear Neural Networks</title>
    <summary>  Conventional wisdom states that deep linear neural networks benefit from
expressiveness and optimization advantages over a single linear layer. This
paper suggests that, in practice, the training process of deep linear
fully-connected networks using conventional optimizers is convex in the same
manner as a single linear fully-connected layer. This paper aims to explain
this claim and demonstrate it. Even though convolutional networks are not
aligned with this description, this work aims to attain a new conceptual
understanding of fully-connected linear networks that might shed light on the
possible constraints of convolutional settings and non-linear architectures.
</summary>
    <author>
      <name>Oriel BenShmuel</name>
    </author>
    <link href="http://arxiv.org/abs/2206.04490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.04490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.11522v1</id>
    <updated>2022-06-23T08:10:34Z</updated>
    <published>2022-06-23T08:10:34Z</published>
    <title>Modeling the System-Level Reliability towards a Convergence of
  Communication, Computing and Control</title>
    <summary>  Enabled and driven by modern advances in wireless telecommunication and
artificial intelligence, the convergence of communication, computing, and
control is becoming inevitable in future industrial applications. Analytical
and optimizing frameworks, however, are not yet readily developed for this new
technical trend. In this work we discuss the necessity and typical scenarios of
this convergence, and propose a new approach to model the system-level
reliability across all involved domainss
</summary>
    <author>
      <name>Bin Han</name>
    </author>
    <author>
      <name>Hans D. Schotten</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3850/978-981-18-5184-1_MS-23-199-cd</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3850/978-981-18-5184-1_MS-23-199-cd" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to appear in the 8th International Symposium on Reliability
  and Risk Management (ISRERM 2022)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">in Proceedings of the 8th International Symposium on Reliability
  Engineering and Risk Management, 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2206.11522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.11522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.03559v1</id>
    <updated>2022-08-06T18:26:30Z</updated>
    <published>2022-08-06T18:26:30Z</published>
    <title>Triple Sparsification of Graph Convolutional Networks without
  Sacrificing the Accuracy</title>
    <summary>  Graph Neural Networks (GNNs) are widely used to perform different machine
learning tasks on graphs. As the size of the graphs grows, and the GNNs get
deeper, training and inference time become costly in addition to the memory
requirement. Thus, without sacrificing accuracy, graph sparsification, or model
compression becomes a viable approach for graph learning tasks. A few existing
techniques only study the sparsification of graphs and GNN models. In this
paper, we develop a SparseGCN pipeline to study all possible sparsification in
GNN. We provide a theoretical analysis and empirically show that it can add up
to 11.6\% additional sparsity to the embedding matrix without sacrificing the
accuracy of the commonly used benchmark graph datasets.
</summary>
    <author>
      <name>Md. Khaledur Rahman</name>
    </author>
    <author>
      <name>Ariful Azad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.03559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.03559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.06450v1</id>
    <updated>2022-09-14T06:53:29Z</updated>
    <published>2022-09-14T06:53:29Z</published>
    <title>Performance Evaluation of Parallel Algorithms</title>
    <summary>  Evaluating how well a whole system or set of subsystems performs is one of
the primary objectives of performance testing. We can tell via performance
assessment if the architecture implementation meets the design objectives.
Performance evaluations of several parallel algorithms are compared in this
study. Both theoretical and experimental methods are used in performance
assessment as a subdiscipline in computer science. The parallel method
outperforms its sequential counterpart in terms of throughput. The parallel
algorithm's performance (speedup) is examined, as shown in the result.
</summary>
    <author>
      <name>Donald Ene Vincent Ike Anireh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/23488387/IJCSE-V9I6P102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/23488387/IJCSE-V9I6P102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 Pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.06450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.06450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.05316v1</id>
    <updated>2022-10-11T10:16:59Z</updated>
    <published>2022-10-11T10:16:59Z</published>
    <title>Sizing up the Batteries: Modelling of Energy-Harvesting Sensor Nodes in
  a Delay Tolerant Network</title>
    <summary>  For energy-harvesting sensor nodes, rechargeable batteries play a critical
role in sensing and transmissions. By coupling two simple Markovian queue
models in a delay-tolerant networking setting, we consider the problem of
battery sizing for these sensor nodes to operate effectively: given the
intended energy depletion and overflow probabilities, how to decide the minimal
battery capacity that is required to ensure opportunistic data exchange despite
the inherent intermittency of renewable energy generation.
</summary>
    <author>
      <name>Jeremiah D. Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures. To appear in Festschrift for Professor Martin
  Purvis, University of Otago</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.05316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.05316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.06043v1</id>
    <updated>2022-11-03T03:32:22Z</updated>
    <published>2022-11-03T03:32:22Z</published>
    <title>Introducing Hermes: Executing Clinical Quality Language (CQL) at over 66
  Million Resources per Second (inexpensively)</title>
    <summary>  Clinical Quality Language (CQL) has emerged as a standard for rule
representation in Clinical Decision Support (CDS) and Electronic Clinical
Quality Measurement (eCQM) in healthcare. While open-source reference
implementations and a few commercial engines exist, there is still a market
need for high-performance engines that can execute CQL queries on the scales of
millions of patients. We introduce the \Hermes{} engine as the world's fastest
commercial CQL execution engine.
</summary>
    <author>
      <name>Angelo Kastroulis</name>
    </author>
    <author>
      <name>Paolo Bonfini</name>
    </author>
    <author>
      <name>Anastasios Litsas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 9 figures, 2 appendices</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.06043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.06043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.11878v1</id>
    <updated>2022-12-22T17:13:59Z</updated>
    <published>2022-12-22T17:13:59Z</published>
    <title>Kokkos-Based Implementation of MPCD on Heterogeneous Nodes</title>
    <summary>  The Kokkos based library Cabana, which has been developed in the Co-design
Center for Particle Applications (CoPA), is used for the implementation of
Multi-Particle Collision Dynamics (MPCD), a particle-based description of
hydrodynamic interactions. It allows a performance portable implementation,
which has been used to study the interplay between CPU and GPU usage on a
multi-node system. As a result, we see most advantages in a homogeneous GPU
usage, but we also discuss the extent to heterogeneous applications, using both
CPU and GPU concurrently.
</summary>
    <author>
      <name>Rene Halver</name>
    </author>
    <author>
      <name>Christoph Junghans</name>
    </author>
    <author>
      <name>Godehard Sutmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-031-30445-3_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-031-30445-3_1" rel="related"/>
    <link href="http://arxiv.org/abs/2212.11878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.11878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.01395v2</id>
    <updated>2023-04-22T00:22:24Z</updated>
    <published>2023-01-04T01:03:04Z</published>
    <title>COST of Graph Processing Using Actors</title>
    <summary>  Graph processing is an increasingly important domain of computer science,
with applications in data and network analysis, among others. Target graphs in
these applications are often large, leading to the creation of "big data"
systems designed to provide the scalability needed to analyze these graphs
using parallel processing. However, researchers have shown that while these
systems often provide scalability, they also often introduce overheads that
exceed the benefits they provide, sometimes resulting in lower absolute
performance than even simple serial implementations. This report studies the
viability and performance of the actor model to implement scalable concurrent
programs to perform common graph computations. We show that relatively simple
actor-based implementations outperform both dedicated graph processing systems
and the benchmark serial implementations.
</summary>
    <author>
      <name>Ronak Buch</name>
    </author>
    <link href="http://arxiv.org/abs/2301.01395v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.01395v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.05102v1</id>
    <updated>2023-01-12T15:59:04Z</updated>
    <published>2023-01-12T15:59:04Z</published>
    <title>Improvement of Computational Performance of Evolutionary AutoML in a
  Heterogeneous Environment</title>
    <summary>  Resource-intensive computations are a major factor that limits the
effectiveness of automated machine learning solutions. In the paper, we propose
a modular approach that can be used to increase the quality of evolutionary
optimization for modelling pipelines with a graph-based structure. It consists
of several stages - parallelization, caching and evaluation. Heterogeneous and
remote resources can be involved in the evaluation stage. The conducted
experiments confirm the correctness and effectiveness of the proposed approach.
The implemented algorithms are available as a part of the open-source framework
FEDOT.
</summary>
    <author>
      <name>Nikolay O. Nikitin</name>
    </author>
    <author>
      <name>Sergey Teryoshkin</name>
    </author>
    <author>
      <name>Valerii Pokrovskii</name>
    </author>
    <author>
      <name>Sergey Pakulin</name>
    </author>
    <author>
      <name>Denis Nasonov</name>
    </author>
    <link href="http://arxiv.org/abs/2301.05102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.05102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.01064v1</id>
    <updated>2023-06-01T18:15:05Z</updated>
    <published>2023-06-01T18:15:05Z</published>
    <title>Characterizing the Cloud's Outbound Network Latency: An Experimental and
  Modeling Study</title>
    <summary>  Cloud latency has critical influences on the success of cloud applications.
Therefore, characterizing cloud network performance is crucial for analyzing
and satisfying different latency requirements. By focusing on the cloud's
outbound network latency, this case study on Google App Engine confirms the
necessity of optimizing application deployment. More importantly, our modeling
effort has established a divide-and-conquer framework to address the complexity
in understanding and investigating the cloud latency.
</summary>
    <author>
      <name>Zheng Li</name>
    </author>
    <author>
      <name>Francisco Millar-Bilbao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IEEECloudSummit48914.2020.00034</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IEEECloudSummit48914.2020.00034" rel="related"/>
    <link href="http://arxiv.org/abs/2306.01064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.01064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.03738v1</id>
    <updated>2023-07-07T17:46:08Z</updated>
    <published>2023-07-07T17:46:08Z</published>
    <title>QIGen: Generating Efficient Kernels for Quantized Inference on Large
  Language Models</title>
    <summary>  We present ongoing work on a new automatic code generation approach for
supporting quantized generative inference on LLMs such as LLaMA or OPT on
off-the-shelf CPUs. Our approach is informed by the target architecture and a
performance model, including both hardware characteristics and method-specific
accuracy constraints. Results on CPU-based inference for LLaMA models show that
our approach can lead to high performance and high accuracy, comparing
favorably to the best existing open-source solution. A preliminary
implementation is available at https://github.com/IST-DASLab/QIGen.
</summary>
    <author>
      <name>Tommaso Pegolotti</name>
    </author>
    <author>
      <name>Elias Frantar</name>
    </author>
    <author>
      <name>Dan Alistarh</name>
    </author>
    <author>
      <name>Markus Püschel</name>
    </author>
    <link href="http://arxiv.org/abs/2307.03738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.03738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.04561v1</id>
    <updated>2023-07-10T13:51:00Z</updated>
    <published>2023-07-10T13:51:00Z</published>
    <title>Performance comparison of timing-based anomaly detectors for Controller
  Area Network: a reproducible study</title>
    <summary>  This work presents an experimental evaluation of the detection performance of
eight different algorithms for anomaly detection on the Controller Area Network
(CAN) bus of modern vehicles based on the analysis of the timing or frequency
of CAN messages. This work solves the current limitations of related scientific
literature, that is based on private dataset, lacks of open implementations,
and detailed description of the detection algorithms. These drawback prevent
the reproducibility of published results, and makes it impossible to compare a
novel proposal against related work, thus hindering the advancement of science.
This paper solves these issues by publicly releasing implementations, labeled
datasets and by describing an unbiased experimental comparisons.
</summary>
    <author>
      <name>Francesco Pollicino</name>
    </author>
    <author>
      <name>Dario Stabili</name>
    </author>
    <author>
      <name>Mirco Marchetti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3604913</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3604913" rel="related"/>
    <link href="http://arxiv.org/abs/2307.04561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.04561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.14711v2</id>
    <updated>2023-09-18T17:50:21Z</updated>
    <published>2023-08-28T17:11:41Z</published>
    <title>Fast Feedforward Networks</title>
    <summary>  We break the linear link between the layer size and its inference cost by
introducing the fast feedforward (FFF) architecture, a log-time alternative to
feedforward networks. We demonstrate that FFFs are up to 220x faster than
feedforward networks, up to 6x faster than mixture-of-experts networks, and
exhibit better training properties than mixtures of experts thanks to noiseless
conditional execution. Pushing FFFs to the limit, we show that they can use as
little as 1% of layer neurons for inference in vision transformers while
preserving 94.2% of predictive performance.
</summary>
    <author>
      <name>Peter Belcak</name>
    </author>
    <author>
      <name>Roger Wattenhofer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.14711v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.14711v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.05669v1</id>
    <updated>2023-09-08T07:49:23Z</updated>
    <published>2023-09-08T07:49:23Z</published>
    <title>Implications of Edge Computing for Static Site Generation</title>
    <summary>  Static site generation (SSG) is a common technique in the web development
space to create performant websites that are easy to host. Numerous SSG tools
exist, and the approach has been complemented by newer approaches, such as
Jamstack, that extend its usability. Edge computing represents a new option to
extend the usefulness of SSG further by allowing the creation of dynamic sites
on top of a static backdrop, providing dynamic resources close to the user. In
this paper, we explore the impact of the recent developments in the edge
computing space and consider its implications for SSG.
</summary>
    <author>
      <name>Juho Vepsäläinen</name>
    </author>
    <author>
      <name>Arto Hellas</name>
    </author>
    <author>
      <name>Petri Vuorimaa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures, 1 table, approved for WEBIST 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.05669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.05669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.09359v1</id>
    <updated>2023-09-17T19:50:26Z</updated>
    <published>2023-09-17T19:50:26Z</published>
    <title>Concurrent Deterministic Skiplist and Other Data Structures</title>
    <summary>  Skiplists are used in a variety of applications for storing data subject to
order criteria. In this article we discuss the design, analysis and performance
of a concurrent deterministic skip list on many-core NUMA nodes. We also
evaluate the performance of a concurrent lock-free unbounded queue
implementation and three implementations of multi-writer, multi-reader~(MWMR)
hash tables and compare their performance with equivalent implementations from
Intel's Thread Building Blocks~(TBB) library. We focus on strategies for memory
management that reduce page faults and cache misses for the memory access
patterns in these data structures. This paper proposes hierarchical usage of
concurrent data structures in programs to improve memory latencies by reducing
memory accesses from remote NUMA nodes.
</summary>
    <author>
      <name>Aparna Sasidharan</name>
    </author>
    <link href="http://arxiv.org/abs/2309.09359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.09359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.16381v1</id>
    <updated>2023-09-28T12:29:02Z</updated>
    <published>2023-09-28T12:29:02Z</published>
    <title>Nek5000/RS Performance on Advanced GPU Architectures</title>
    <summary>  We demonstrate NekRS performance results on various advanced GPU
architectures. NekRS is a GPU-accelerated version of Nek5000 that targets high
performance on exascale platforms. It is being developed in DOE's Center of
Efficient Exascale Discretizations, which is one of the co-design centers under
the Exascale Computing Project. In this paper, we consider Frontier, Crusher,
Spock, Polaris, Perlmutter, ThetaGPU, and Summit. Simulations are performed
with 17x17 rod-bundle geometries from small modular reactor applications. We
discuss strong-scaling performance and analysis.
</summary>
    <author>
      <name>Misun Min</name>
    </author>
    <author>
      <name>Yu-Hsiang Lan</name>
    </author>
    <author>
      <name>Paul Fischer</name>
    </author>
    <author>
      <name>Thilina Rathnayake</name>
    </author>
    <author>
      <name>John Holmen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 13 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.16381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.16381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="35-04" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.0; F.2; G.2; G.4; I.6; J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.16630v1</id>
    <updated>2023-10-25T13:28:18Z</updated>
    <published>2023-10-25T13:28:18Z</published>
    <title>A Survey on Experimental Performance Evaluation of Data Distribution
  Service (DDS) Implementations</title>
    <summary>  The Data Distribution Service (DDS) is a widely used communication
specification for real-time mission-critical systems that follow the principles
of publish-subscribe middleware. DDS has an extensive set of quality of service
(QoS) parameters allowing a thorough customisation of the intended
communication. An extensive survey of the performance of the implementations of
this communication middleware is lacking. This paper closes the gap by
surveying the state of the art in performance of various DDS implementations
and identifying any research gaps that exist within this domain.
</summary>
    <author>
      <name>Kaleem Peeroo</name>
    </author>
    <author>
      <name>Peter Popov</name>
    </author>
    <author>
      <name>Vladimir Stankovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages and 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.16630v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.16630v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.04951v2</id>
    <updated>2024-04-09T11:21:14Z</updated>
    <published>2023-11-08T14:08:00Z</published>
    <title>Leveraging Speculative Sampling and KV-Cache Optimizations Together for
  Generative AI using OpenVINO</title>
    <summary>  Inference optimizations are critical for improving user experience and
reducing infrastructure costs and power consumption. In this article, we
illustrate a form of dynamic execution known as speculative sampling to reduce
the overall latency of text generation and compare it with standard
autoregressive sampling. This can be used together with model-based
optimizations (e.g. quantization) to provide an optimized solution. Both
sampling methods make use of KV caching. A Jupyter notebook and some sample
executions are provided.
</summary>
    <author>
      <name>Haim Barad</name>
    </author>
    <author>
      <name>Ekaterina Aidova</name>
    </author>
    <author>
      <name>Yury Gorbachev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code available at
  https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/speculative-sampling</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.04951v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.04951v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.09440v1</id>
    <updated>2023-11-15T23:23:35Z</updated>
    <published>2023-11-15T23:23:35Z</published>
    <title>On the Relevance of Blockchain Evaluations on Bare Metal</title>
    <summary>  In this paper, we present the first bare metal comparison of modern
blockchains, including Algorand, Avalanche, Diem, Ethereum, Quorum and Solana.
This evaluation was conducted with the recent Diablo benchmark suite, a
framework to evaluate the performance of different blockchains on the same
ground. By tuning network delays in our controlled environment we were able to
reproduce performance trends obtained in geo-distributed settings, hence
demonstrating the relevance of bare metal evaluations to better understand
blockchain performance.
</summary>
    <author>
      <name>Andrei Lebedev</name>
    </author>
    <author>
      <name>Vincent Gramoli</name>
    </author>
    <link href="http://arxiv.org/abs/2311.09440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.09440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.15730v1</id>
    <updated>2023-11-27T11:27:38Z</updated>
    <published>2023-11-27T11:27:38Z</published>
    <title>Analytical Queries: A Comprehensive Survey</title>
    <summary>  Modern hardware heterogeneity brings efficiency and performance opportunities
for analytical query processing. In the presence of continuous data volume and
complexity growth, bridging the gap between recent hardware advancements and
the data processing tools ecosystem is paramount for improving the speed of ETL
and model development. In this paper, we present a comprehensive overview of
existing analytical query processing approaches as well as the use and design
of systems that use heterogeneous hardware for the task. We then analyze
state-of-the-art solutions and identify missing pieces. The last two chapters
discuss the identified problems and present our view on how the ecosystem
should evolve.
</summary>
    <author>
      <name>Petr Kurapov</name>
    </author>
    <author>
      <name>Areg Melik-Adamyan</name>
    </author>
    <link href="http://arxiv.org/abs/2311.15730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.15730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.12472v1</id>
    <updated>2023-12-19T10:19:39Z</updated>
    <published>2023-12-19T10:19:39Z</published>
    <title>A Performance Evaluation of a Quantized Large Language Model on Various
  Smartphones</title>
    <summary>  This paper explores the feasibility and performance of on-device large
language model (LLM) inference on various Apple iPhone models. Amidst the rapid
evolution of generative AI, on-device LLMs offer solutions to privacy,
security, and connectivity challenges inherent in cloud-based models.
Leveraging existing literature on running multi-billion parameter LLMs on
resource-limited devices, our study examines the thermal effects and
interaction speeds of a high-performing LLM across different smartphone
generations. We present real-world performance results, providing insights into
on-device inference capabilities.
</summary>
    <author>
      <name>Tolga Çöplü</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Haltia, Inc.</arxiv:affiliation>
    </author>
    <author>
      <name>Marc Loedi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Haltia, Inc.</arxiv:affiliation>
    </author>
    <author>
      <name>Arto Bendiken</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Haltia, Inc.</arxiv:affiliation>
    </author>
    <author>
      <name>Mykhailo Makohin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Haltia, Inc.</arxiv:affiliation>
    </author>
    <author>
      <name>Joshua J. Bouw</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Haltia, Inc.</arxiv:affiliation>
    </author>
    <author>
      <name>Stephen Cobb</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Haltia, Inc.</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.12472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.12472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.03235v1</id>
    <updated>2024-01-06T15:08:45Z</updated>
    <published>2024-01-06T15:08:45Z</published>
    <title>RAID Organizations for Improved Reliability and Performance: A Not
  Entirely Unbiased Tutorial (1st revision)</title>
    <summary>  RAID proposal advocated replacing large disks with arrays of PC disks, but as
the capacity of small disks increased 100-fold in 1990s the production of large
disks was discontinued. Storage dependability is increased via replication or
erasure coding. Cloud storage providers store multiple copies of data obviating
for need for further redundancy. Varitaions of RAID based on local recovery
codes, partial MDS reduce recovery cost. NAND flash Solid State Disks - SSDs
have low latency and high bandwidth, are more reliable, consume less power and
have a lower TCO than Hard Disk Drives, which are more viable for hyperscalers.
</summary>
    <author>
      <name>Alexander Thomasian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ACM Computing Surveys. arXiv admin note: substantial
  text overlap with arXiv:2306.08763</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.03235v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.03235v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.04904v1</id>
    <updated>2024-01-10T03:12:55Z</updated>
    <published>2024-01-10T03:12:55Z</published>
    <title>Scalable Cyclic Schedulers for Age of Information Optimization in
  Large-Scale Status Update Systems</title>
    <summary>  We study cyclic scheduling for generate-at-will (GAW) multi-source status
update systems with heterogeneous service times and packet drop probabilities,
with the aim of minimizing the weighted sum age of information (AoI), called
system AoI, or the weighted sum peak AoI (PAoI), called system PAoI. In
particular, we obtain well-performing cyclic schedulers which can easily scale
to thousands of information sources and which also have low online
implementation complexity. The proposed schedulers are comparatively studied
against existing scheduling algorithms in terms of computational load and
system AoI/PAoI performance, to validate their effectiveness.
</summary>
    <author>
      <name>Nail Akar</name>
    </author>
    <author>
      <name>Sahan Liyanaarachchi</name>
    </author>
    <author>
      <name>Sennur Ulukus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.04904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.04904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.05820v1</id>
    <updated>2024-01-11T10:36:45Z</updated>
    <published>2024-01-11T10:36:45Z</published>
    <title>Implications of Noise in Resistive Memory on Deep Neural Networks for
  Image Classification</title>
    <summary>  Resistive memory is a promising alternative to SRAM, but is also an
inherently unstable device that requires substantial effort to ensure correct
read and write operations. To avoid the associated costs in terms of area, time
and energy, the present work is concerned with exploring how much noise in
memory operations can be tolerated by image classification tasks based on
neural networks. We introduce a special noisy operator that mimics the noise in
an exemplary resistive memory unit, explore the resilience of convolutional
neural networks on the CIFAR-10 classification task, and discuss a couple of
countermeasures to improve this resilience.
</summary>
    <author>
      <name>Yannick Emonds</name>
    </author>
    <author>
      <name>Kai Xi</name>
    </author>
    <author>
      <name>Holger Fröning</name>
    </author>
    <link href="http://arxiv.org/abs/2401.05820v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.05820v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.11698v1</id>
    <updated>2024-02-18T20:01:55Z</updated>
    <published>2024-02-18T20:01:55Z</published>
    <title>5G Cellular -- An Energy Efficiency Perspective</title>
    <summary>  While the 5G technology of cellular communications promises great capacity
and coverage to access information anywhere and anytime, it is feared to have
huge power consumption. Significant research been has been directed towards
solving this problem which exists both on the subscribers side as well as the
operators side. There have been efforts like predicting traffic, modifying the
physical layer etc. towards making the 5G technology more energy efficient. The
aim of this study is to see the technology enablers for 5G from an energy
efficiency perspective. Efforts will be made to point out specific areas in 5G
cellular where improvements or modifications could make 5G cellular more energy
efficient.
</summary>
    <author>
      <name>Deven Panchal</name>
    </author>
    <link href="http://arxiv.org/abs/2402.11698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.11698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.12203v1</id>
    <updated>2024-02-19T15:07:24Z</updated>
    <published>2024-02-19T15:07:24Z</published>
    <title>MPI Implementation Profiling for Better Application Performance</title>
    <summary>  While application profiling has been a mainstay in the HPC community for
years, profiling of MPI and other communication middleware has not received the
same degree of exploration. This paper adds to the discussion of MPI profiling,
contributing two general-purpose profiling methods as well as practical
applications of these methods to an existing implementation. The ability to
detect performance defects in MPI codes using these methods increases the
potential of further research and development in communication optimization.
</summary>
    <author>
      <name>Riley Shipley</name>
    </author>
    <author>
      <name>Garrett Hooten</name>
    </author>
    <author>
      <name>David Boehme</name>
    </author>
    <author>
      <name>Derek Schafer</name>
    </author>
    <author>
      <name>Anthony Skjellum</name>
    </author>
    <author>
      <name>Olga Pearce</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.12203v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.12203v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.15303v1</id>
    <updated>2024-03-22T15:54:08Z</updated>
    <published>2024-03-22T15:54:08Z</published>
    <title>Network Calculus Characterization of Congestion Control for Time-Varying
  Traffic</title>
    <summary>  Models for the dynamics of congestion control generally involve systems of
coupled differential equations. Universally, these models assume that traffic
sources saturate the maximum transmissions allowed by the congestion control
method. This is not suitable for studying congestion control of intermittent
but bursty traffic sources. In this paper, we present a characterization of
congestion control for arbitrary time-varying traffic that applies to
rate-based as well as window-based congestion control. We leverage the
capability of network calculus to precisely describe the input-output
relationship at network elements for arbitrary source traffic. We show that our
characterization can closely track the dynamics of even complex congestion
control algorithms.
</summary>
    <author>
      <name>Harvinder Lehal</name>
    </author>
    <author>
      <name>Natchanon Luangsomboon</name>
    </author>
    <author>
      <name>Jörg Liebeherr</name>
    </author>
    <link href="http://arxiv.org/abs/2403.15303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.15303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.16013v1</id>
    <updated>2024-03-24T04:58:49Z</updated>
    <published>2024-03-24T04:58:49Z</published>
    <title>Performance evaluation of accelerated complex multiple-precision LU
  decomposition</title>
    <summary>  The direct method is one of the most important algorithms for solving linear
systems of equations, with LU decomposition comprising a significant portion of
its computation time. This study explores strategies to accelerate complex LU
decomposition using multiple-precision floating-point arithmetic of the
multiple-component type. Specifically, we explore the potential efficiency
gains using a combination of SIMDization and the 3M method for complex matrix
multiplication. Our benchmark tests compare this approach with the direct
method implementation in MPLAPACK, focusing on computation time and numerical
errors.
</summary>
    <author>
      <name>Tomonori Kouya</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-031-65273-8_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-031-65273-8_1" rel="related"/>
    <link href="http://arxiv.org/abs/2403.16013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.16013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.02276v1</id>
    <updated>2024-04-02T20:00:14Z</updated>
    <published>2024-04-02T20:00:14Z</published>
    <title>Heterogeneous Data Access Model for Concurrency Control and Methods to
  Deal with High Data Contention</title>
    <summary>  OLTP has stringent performance requirements defined by Service Level
Agreements. Transaction response time is used to determine the maximum
throughout in benchmarks. Capacity planning tools for OLTP performance are
based on queueing network models for hardware resources and database lock
contention has a secondary effect on performance. With ever increasing levels
of e-commerce and surges in OLTP traffic we discuss the need for studies of
database workloads to develop more realistic lock/latch contention models.
Predictive formulas to model increased load leading to thrashing for txns with
identical and nonidentical steps are presented. We review concurrency control
methods to reduce the level of lock/data conflicts in high contention
environments.
</summary>
    <author>
      <name>Alexander Thomasian</name>
    </author>
    <link href="http://arxiv.org/abs/2404.02276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.02276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.16970v1</id>
    <updated>2024-04-25T18:45:09Z</updated>
    <published>2024-04-25T18:45:09Z</published>
    <title>CarbonCP: Carbon-Aware DNN Partitioning with Conformal Prediction for
  Sustainable Edge Intelligence</title>
    <summary>  This paper presents a solution to address carbon emission mitigation for
end-to-end edge computing systems, including the computing at battery-powered
edge devices and servers, as well as the communications between them. We design
and implement, CarbonCP, a context-adaptive, carbon-aware, and
uncertainty-aware AI inference framework built upon conformal prediction
theory, which balances operational carbon emissions, end-to-end latency, and
battery consumption of edge devices through DNN partitioning under varying
system processing contexts and carbon intensity. Our experimental results
demonstrate that CarbonCP is effective in substantially reducing operational
carbon emissions, up to 58.8%, while maintaining key user-centric performance
metrics with only 9.9% error rate.
</summary>
    <author>
      <name>Hongyu Ke</name>
    </author>
    <author>
      <name>Wanxin Jin</name>
    </author>
    <author>
      <name>Haoxin Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2404.16970v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.16970v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.16086v1</id>
    <updated>2024-05-25T06:33:43Z</updated>
    <published>2024-05-25T06:33:43Z</published>
    <title>An Experimental Study of Different Aggregation Schemes in
  Semi-Asynchronous Federated Learning</title>
    <summary>  Federated learning is highly valued due to its high-performance computing in
distributed environments while safeguarding data privacy. To address resource
heterogeneity, researchers have proposed a semi-asynchronous federated learning
(SAFL) architecture. However, the performance gap between different aggregation
targets in SAFL remain unexplored.
  In this paper, we systematically compare the performance between two
algorithm modes, FedSGD and FedAvg that correspond to aggregating gradients and
models, respectively. Our results across various task scenarios indicate these
two modes exhibit a substantial performance gap. Specifically, FedSGD achieves
higher accuracy and faster convergence but experiences more severe fluctuates
in accuracy, whereas FedAvg excels in handling straggler issues but converges
slower with reduced accuracy.
</summary>
    <author>
      <name>Yunbo Li</name>
    </author>
    <author>
      <name>Jiaping Gui</name>
    </author>
    <author>
      <name>Yue Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2405.16086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.16086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.10816v1</id>
    <updated>2024-06-16T06:46:25Z</updated>
    <published>2024-06-16T06:46:25Z</published>
    <title>Optimization of Armv9 architecture general large language model
  inference performance based on Llama.cpp</title>
    <summary>  This article optimizes the inference performance of the Qwen-1.8B model by
performing Int8 quantization, vectorizing some operators in llama.cpp, and
modifying the compilation script to improve the compiler optimization level. On
the Yitian 710 experimental platform, the prefill performance is increased by
1.6 times, the decoding performance is increased by 24 times, the memory usage
is reduced to 1/5 of the original, and the accuracy loss is almost negligible.
</summary>
    <author>
      <name>Longhao Chen</name>
    </author>
    <author>
      <name>Yina Zhao</name>
    </author>
    <author>
      <name>Qiangjun Xie</name>
    </author>
    <author>
      <name>Qinghua Sheng</name>
    </author>
    <link href="http://arxiv.org/abs/2406.10816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.10816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.07812v1</id>
    <updated>2024-07-10T16:32:21Z</updated>
    <published>2024-07-10T16:32:21Z</published>
    <title>S2MPJ and CUTEst optimization problems for Matlab, Python and Julia</title>
    <summary>  A new decoder for the SIF test problems of the CUTEst collection is
described, which produces problem files allowing the computation of values and
derivatives of the objective function and constraints of most \cutest\ problems
directly within ``native'' Matlab, Python or Julia, without any additional
installation or interfacing with MEX files or Fortran programs. When used with
Matlab, the new problem files optionally support reduced-precision
computations.
</summary>
    <author>
      <name>Serge Gratton</name>
    </author>
    <author>
      <name>Philippe L. Toint</name>
    </author>
    <link href="http://arxiv.org/abs/2407.07812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.07812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="49N99, 65K05, 65Y20, 68N99, 90C30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.11611v2</id>
    <updated>2024-07-17T10:07:08Z</updated>
    <published>2024-07-16T11:21:30Z</published>
    <title>Estimating the Energy Footprint of Software Systems: a Primer</title>
    <summary>  In Green Software Development, quantifying the energy footprint of a software
system is one of the most basic activities. This documents provides a
high-level overview of how the energy footprint of a software system can be
estimated to support Green Software Development. We introduce basic concepts in
the area, highlight methodological issues that must be accounted for when
conducting experiments, discuss trade-offs associated with different estimation
approaches, and make some practical considerations. This document aims to be a
starting point for researchers who want to begin conducting work in this area.
</summary>
    <author>
      <name>Fernando Castor</name>
    </author>
    <link href="http://arxiv.org/abs/2407.11611v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.11611v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.11880v1</id>
    <updated>2024-08-21T03:05:27Z</updated>
    <published>2024-08-21T03:05:27Z</published>
    <title>RAO-SS: A Prototype of Run-time Auto-tuning Facility for Sparse Direct
  Solvers</title>
    <summary>  In this paper, a run-time auto-tuning method for performance parameters
according to input matrices is proposed. RAO-SS (Run-time Auto-tuning Optimizer
for Sparse Solvers), which is a prototype of auto-tuning software using the
proposed method, is also evaluated. The RAO-SS is implemented with the
Autopilot, which is middle-ware to support run-time auto-tuning with fuzzy
logic function. The target numerical library is the SuperLU, which is a sparse
direct solver for linear equations. The result indicated that: (1) the speedup
factors of 1.2 for average and 3.6 for maximum to default executions were
obtained; (2) the software overhead of the Autopilot can be ignored in RAO-SS.
</summary>
    <author>
      <name>Takahiro Katagiri</name>
    </author>
    <author>
      <name>Yoshinori Ishii</name>
    </author>
    <author>
      <name>Hiroki Honda</name>
    </author>
    <link href="http://arxiv.org/abs/2408.11880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.11880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.11244v1</id>
    <updated>2024-11-18T02:25:46Z</updated>
    <published>2024-11-18T02:25:46Z</published>
    <title>gDist: Efficient Distance Computation between 3D Meshes on GPU</title>
    <summary>  Computing maximum/minimum distances between 3D meshes is crucial for various
applications, i.e., robotics, CAD, VR/AR, etc. In this work, we introduce a
highly parallel algorithm (gDist) optimized for Graphics Processing Units
(GPUs), which is capable of computing the distance between two meshes with over
15 million triangles in less than 0.4 milliseconds (Fig. 1). By testing on
benchmarks with varying characteristics, the algorithm achieves remarkable
speedups over prior CPU-based and GPU-based algorithms on a commodity GPU
(NVIDIA GeForce RTX 4090). Notably, the algorithm consistently maintains
high-speed performance, even in challenging scenarios that pose difficulties
for prior algorithms.
</summary>
    <author>
      <name>Peng Fang</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Ruofeng Tong</name>
    </author>
    <author>
      <name>Hailong Li</name>
    </author>
    <author>
      <name>Min Tang</name>
    </author>
    <link href="http://arxiv.org/abs/2411.11244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.11244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.18308v1</id>
    <updated>2024-11-27T12:58:06Z</updated>
    <published>2024-11-27T12:58:06Z</published>
    <title>CXL-Interference: Analysis and Characterization in Modern Computer
  Systems</title>
    <summary>  Compute Express Link (CXL) is a promising technology that addresses memory
and storage challenges. Despite its advantages, CXL faces performance threats
from external interference when co-existing with current memory and storage
systems. This interference is under-explored in existing research. To address
this, we develop CXL-Interplay, systematically characterizing and analyzing
interference from memory and storage systems. To the best of our knowledge, we
are the first to characterize CXL interference on real CXL hardware. We also
provide reverse-reasoning analysis with performance counters and kernel
functions. In the end, we propose and evaluate mitigating solutions.
</summary>
    <author>
      <name>Shunyu Mao</name>
    </author>
    <author>
      <name>Jiajun Luo</name>
    </author>
    <author>
      <name>Yixin Li</name>
    </author>
    <author>
      <name>Jiapeng Zhou</name>
    </author>
    <author>
      <name>Weidong Zhang</name>
    </author>
    <author>
      <name>Zheng Liu</name>
    </author>
    <author>
      <name>Teng Ma</name>
    </author>
    <author>
      <name>Shuwen Deng</name>
    </author>
    <link href="http://arxiv.org/abs/2411.18308v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.18308v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.19542v1</id>
    <updated>2024-11-29T08:39:24Z</updated>
    <published>2024-11-29T08:39:24Z</published>
    <title>A dynamic parallel method for performance optimization on hybrid CPUs</title>
    <summary>  The AIPC concept is gaining popularity, and more and more hybrid CPUs will be
running AI models on client devices. However, the current AI inference
framework overlooks the imbalanced hardware capability of hybrid CPUs, leading
to low inference performance. To address this issue, we have introduced a
dynamic parallel method for hybrid CPUs, which significantly increases LLM
inference performance by balancing the workload for each core of a hybrid CPU
before the parallel work starts. This method has enabled Neural Speed to
achieve more than 90% (on average) of memory bandwidth on two hybrid Intel
CPUs.
</summary>
    <author>
      <name>Luo Yu</name>
    </author>
    <author>
      <name>Liu Yucheng</name>
    </author>
    <author>
      <name>Shen Haihao</name>
    </author>
    <link href="http://arxiv.org/abs/2411.19542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.19542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.07892v1</id>
    <updated>2024-12-10T19:59:40Z</updated>
    <published>2024-12-10T19:59:40Z</published>
    <title>Overview of Web Application Performance Optimization Techniques</title>
    <summary>  During its thirty years of existence, the World Wide Web has helped to
transform the world and create digital economies. Although it started as a
global information exchange, it has become the most significant available
application platform on top of its initial target. One of the side effects of
this evolution was perhaps suboptimal ways to deliver content over the web,
leading to wasted resources and business through lost conversions. Technically
speaking, there are many ways to improve the performance of web applications.
In this article, we examine the currently available options and the latest
trends related to improving web application performance.
</summary>
    <author>
      <name>Juho Vepsäläinen</name>
    </author>
    <author>
      <name>Arto Hellas</name>
    </author>
    <author>
      <name>Petri Vuorimaa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 1 figure, 4 tables, accepted for LNBIP journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.07892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.07892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.17510v1</id>
    <updated>2024-12-23T12:15:16Z</updated>
    <published>2024-12-23T12:15:16Z</published>
    <title>Performance evaluation of accelerated real and complex
  multiple-precision sparse matrix-vector multiplication</title>
    <summary>  Sparse matrices have recently played a significant and impactful role in
scientific computing, including artificial intelligence-related fields.
According to historical studies on sparse matrix--vector multiplication (SpMV),
Krylov subspace methods are particularly sensitive to the effects of round-off
errors when using floating-point arithmetic. By employing multiple-precision
linear computation, convergence can be stabilized by reducing these round-off
errors. In this paper, we present the performance of our accelerated SpMV using
SIMD instructions, demonstrating its effectiveness through various examples,
including Krylov subspace methods.
</summary>
    <author>
      <name>Tomonori Kouya</name>
    </author>
    <link href="http://arxiv.org/abs/2412.17510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.17510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.05380v1</id>
    <updated>2025-01-09T17:08:01Z</updated>
    <published>2025-01-09T17:08:01Z</published>
    <title>Optimal Scheduling in a Quantum Switch</title>
    <summary>  With a growing number of quantum networks in operation, there is a pressing
need for performance analysis of quantum switching technologies. A quantum
switch establishes, distributes, and maintains entanglements across a network.
In contrast to a classical switching fabric, a quantum switch is a two sided
queueing network. The switch generates Link Level Entanglements (LLEs), which
are then fused to process the networks entanglement requests. Our proof
techniques analyse a two time scale separation phenomenon at the fluid scale
for a general switch topology. This allows us to demonstrate that the optimal
fluid dynamics are given by a scheduling algorithm that solves a certain
average reward Markov Decision Process.
</summary>
    <author>
      <name>Sanidhay Bhambay</name>
    </author>
    <author>
      <name>Thirupathaiah Vasantam</name>
    </author>
    <author>
      <name>Neil Walton</name>
    </author>
    <link href="http://arxiv.org/abs/2501.05380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.05380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.07139v1</id>
    <updated>2025-01-13T08:58:00Z</updated>
    <published>2025-01-13T08:58:00Z</published>
    <title>FlexQuant: Elastic Quantization Framework for Locally Hosted LLM on Edge
  Devices</title>
    <summary>  Deploying LLMs on edge devices presents serious technical challenges. Memory
elasticity is crucial for edge devices with unified memory, where memory is
shared and fluctuates dynamically. Existing solutions suffer from either poor
transition granularity or high storage costs. We propose FlexQuant, a novel
elasticity framework that generates an ensemble of quantized models, providing
an elastic hosting solution with 15x granularity improvement and 10x storage
reduction compared to SoTA methods. FlexQuant works with most quantization
methods and creates a family of trade-off options under various storage limits
through our pruning method. It brings great performance and flexibility to the
edge deployment of LLMs.
</summary>
    <author>
      <name>Yuji Chai</name>
    </author>
    <author>
      <name>Mujin Kwen</name>
    </author>
    <author>
      <name>David Brooks</name>
    </author>
    <author>
      <name>Gu-Yeon Wei</name>
    </author>
    <link href="http://arxiv.org/abs/2501.07139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.07139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.14763v1</id>
    <updated>2024-12-25T21:53:00Z</updated>
    <published>2024-12-25T21:53:00Z</published>
    <title>Intent-driven scheduling of backup jobs</title>
    <summary>  Job scheduling under various constraints to achieve global optimization is a
well-studied problem. However, in scenarios that involve time-dependent
constraints, such as scheduling backup jobs, achieving global optimization may
not always be desirable. This paper presents a framework for scheduling new
backup jobs in the presence of existing job schedules, focusing on satisfying
intent-based constraints without disrupting current schedules. The proposed
method accommodates various scheduling intents and constraints, and its
effectiveness is validated through extensive testing against a variety of
backup scenarios on real-world data from Veritas Netbackup customer policies.
</summary>
    <author>
      <name>Souvik Dutta</name>
    </author>
    <author>
      <name>Suri Brahmaroutu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures, 1 table, 1 algorithm</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.14763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.14763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; E.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.00021v1</id>
    <updated>2025-01-16T08:00:10Z</updated>
    <published>2025-01-16T08:00:10Z</published>
    <title>PixelBrax: Learning Continuous Control from Pixels End-to-End on the GPU</title>
    <summary>  We present PixelBrax, a set of continuous control tasks with pixel
observations. We combine the Brax physics engine with a pure JAX renderer,
allowing reinforcement learning (RL) experiments to run end-to-end on the GPU.
PixelBrax can render observations over thousands of parallel environments and
can run two orders of magnitude faster than existing benchmarks that rely on
CPU-based rendering. Additionally, PixelBrax supports fully reproducible
experiments through its explicit handling of any stochasticity within the
environments and supports color and video distractors for benchmarking
generalization. We open-source PixelBrax alongside JAX implementations of
several RL algorithms at github.com/trevormcinroe/pixelbrax.
</summary>
    <author>
      <name>Trevor McInroe</name>
    </author>
    <author>
      <name>Samuel Garcin</name>
    </author>
    <link href="http://arxiv.org/abs/2502.00021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.00021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.11017v2</id>
    <updated>2025-03-04T10:33:02Z</updated>
    <published>2025-02-16T06:57:32Z</published>
    <title>Scalable Binary CUR Low-Rank Approximation Algorithm</title>
    <summary>  This paper proposes a scalable binary CUR low-rank approximation algorithm
that leverages parallel selection of representative rows and columns within a
deterministic framework. By employing a blockwise adaptive cross approximation
strategy, the algorithm efficiently identifies dominant components in
large-scale matrices, thereby reducing computational costs. Numerical
experiments on $16,384 \times 16,384$ matrices demonstrate a good speed-up,
with execution time decreasing from $12.37$ seconds using $2$ processes to
$1.02$ seconds using $64$ processes. The tests on Hilbert matrices and
synthetic low-rank matrices of different size across various sizes demonstrate
an near-optimal reconstruction accuracy.
</summary>
    <author>
      <name>Bowen Su</name>
    </author>
    <link href="http://arxiv.org/abs/2502.11017v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.11017v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.12087v1</id>
    <updated>2025-04-16T13:48:37Z</updated>
    <published>2025-04-16T13:48:37Z</published>
    <title>Extrae.jl: Julia bindings for the Extrae HPC Profiler</title>
    <summary>  The Julia programming language has gained acceptance within the
High-Performance Computing (HPC) community due to its ability to tackle
two-language problem: Julia code feels as high-level as Python but allows
developers to tune it to C-level performance. But to squeeze every drop of
performance, Julia needs to integrate with advanced performance analysis tools,
also known as profilers. In this work, we present Extrae.jl, a Julia package to
interface with the Extrae profiler.
</summary>
    <author>
      <name>Sergio Sanchez-Ramirez</name>
    </author>
    <author>
      <name>Mosè Giordano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, in review process for the JuliaCon 2024 proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.12087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.12087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006014v1</id>
    <updated>2000-06-08T16:50:24Z</updated>
    <published>2000-06-08T16:50:24Z</published>
    <title>Solaris System Resource Manager: All I Ever Wanted Was My Unfair
  Advantage (And Why You Can't Have It!)</title>
    <summary>  Traditional UNIX time-share schedulers attempt to be fair to all users by
employing a round-robin style algorithm for allocating CPU time. Unfortunately,
a loophole exists whereby the scheduler can be biased in favor of a greedy user
running many short CPU-time processes. This loophole is not a defect but an
intrinsic property of the round-robin scheduler that ensures responsiveness to
the short CPU demands associated with multiple interactive users. A new
generation of UNIX system resource management software constrains the scheduler
to be equitable to all users regardless of the number of processes each may be
running. This "fair-share" scheduling draws on the concept of pro rating
resource "shares" across users and groups and then dynamically adjusting CPU
usage to meet those share proportions. The simple notion of statically
allocating these shares, however, belies the potential consequences for
performance as measured by user response time and service level targets. We
demonstrate this point by modeling several simple share allocation scenarios
and analyzing the corresponding performance effects. A brief comparison of
commercial system resource management implementations from HP, IBM, and SUN is
also given.
</summary>
    <author>
      <name>Neil J. Gunther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, Updated since the 25th Computer Measurement Group
  Conference, Reno NV, Dec.5-10, 1999</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. CMG'99 Conf. p.194-205</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0006014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4;D.4.1;D.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0305054v1</id>
    <updated>2003-05-29T13:29:27Z</updated>
    <published>2003-05-29T13:29:27Z</published>
    <title>A Monitoring System for the BaBar INFN Computing Cluster</title>
    <summary>  Monitoring large clusters is a challenging problem. It is necessary to
observe a large quantity of devices with a reasonably short delay between
consecutive observations. The set of monitored devices may include PCs, network
switches, tape libraries and other equipments. The monitoring activity should
not impact the performances of the system. In this paper we present PerfMC, a
monitoring system for large clusters. PerfMC is driven by an XML configuration
file, and uses the Simple Network Management Protocol (SNMP) for data
collection. SNMP is a standard protocol implemented by many networked
equipments, so the tool can be used to monitor a wide range of devices. System
administrators can display informations on the status of each device by
connecting to a WEB server embedded in PerfMC. The WEB server can produce
graphs showing the value of different monitored quantities as a function of
time; it can also produce arbitrary XML pages by applying XSL Transformations
to an internal XML representation of the cluster's status. XSL Transformations
may be used to produce HTML pages which can be displayed by ordinary WEB
browsers. PerfMC aims at being relatively easy to configure and operate, and
highly efficient. It is currently being used to monitor the Italian
Reprocessing farm for the BaBar experiment, which is made of about 200 dual-CPU
Linux machines.
</summary>
    <author>
      <name>M. Marzolla</name>
    </author>
    <author>
      <name>V. Melloni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 10 pages, LaTeX, 4 eps figures. PSN
  MOET006</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ECONFC0303241:MOET006,2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0305054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0305054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.8.2; C.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0604016v2</id>
    <updated>2006-05-23T05:07:31Z</updated>
    <published>2006-04-06T00:54:44Z</published>
    <title>On Conditional Branches in Optimal Search Trees</title>
    <summary>  Algorithms for efficiently finding optimal alphabetic decision trees -- such
as the Hu-Tucker algorithm -- are well established and commonly used. However,
such algorithms generally assume that the cost per decision is uniform and thus
independent of the outcome of the decision. The few algorithms without this
assumption instead use one cost if the decision outcome is ``less than'' and
another cost otherwise. In practice, neither assumption is accurate for
software optimized for today's microprocessors. Such software generally has one
cost for the more likely decision outcome and a greater cost -- often far
greater -- for the less likely decision outcome. This problem and
generalizations thereof are thus applicable to hard coding static decision tree
instances in software, e.g., for optimizing program bottlenecks or for
compiling switch statements. An O(n^3)-time O(n^2)-space dynamic programming
algorithm can solve this optimal binary decision tree problem, and this
approach has many generalizations that optimize for the behavior of processors
with predictive branch capabilities, both static and dynamic. Solutions to this
formulation are often faster in practice than ``optimal'' decision trees as
formulated in the literature. Different search paradigms can sometimes yield
even better performance.
</summary>
    <author>
      <name>Michael B. Baer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures (with 10 illustrations total), 1 table;
  reformatted with some additional notes</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0604016v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0604016v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.1.4; C.0; C.1.1; D.3.4; E.1; F.2.2; G.3; H.3.3; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611076v2</id>
    <updated>2008-02-23T04:23:32Z</updated>
    <published>2006-11-16T03:14:40Z</published>
    <title>Proportional Fairness in Multi-channel Multi-rate Wireless Networks-Part
  II: The Case of Time-Varying Channels</title>
    <summary>  This is Part II of a two-part paper series that studies the use of the
proportional fairness (PF) utility function as the basis for capacity
allocation and scheduling in multi-channel multi-rate wireless networks. The
contributions of Part II are twofold. (i) First, we extend the problem
formulation, theoretical results, and algorithms to the case of time-varying
channels, where opportunistic capacity allocation and scheduling can be
exploited to improve system performance. We lay down the theoretical foundation
for optimization that "couples" the time-varying characteristic of channels
with the requirements of the underlying applications into one consideration. In
particular, the extent to which opportunistic optimization is possible is not
just a function of how fast the channel characteristics vary, but also a
function of the elasticity of the underlying applications for delayed capacity
allocation. (ii) Second, building upon our theoretical framework and results,
we study subcarrier allocation and scheduling in orthogonal frequency division
multiplexing (OFDM) cellular wireless networks. We introduce the concept of a
W-normalized Doppler frequency to capture the extent to which opportunistic
scheduling can be exploited to achieve throughput-fairness performance gain. We
show that a "look-back PF" scheduling can strike a good balance between system
throughput and fairness while taking the underlying application requirements
into account.
</summary>
    <author>
      <name>Soung Chang Liew</name>
    </author>
    <author>
      <name>Ying Jun Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0611076v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611076v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702135v1</id>
    <updated>2007-02-23T09:44:17Z</updated>
    <published>2007-02-23T09:44:17Z</published>
    <title>High Performance Direct Gravitational N-body Simulations on Graphics
  Processing Units</title>
    <summary>  We present the results of gravitational direct $N$-body simulations using the
commercial graphics processing units (GPU) NVIDIA Quadro FX1400 and GeForce
8800GTX, and compare the results with GRAPE-6Af special purpose hardware. The
force evaluation of the $N$-body problem was implemented in Cg using the GPU
directly to speed-up the calculations. The integration of the equations of
motions were, running on the host computer, implemented in C using the 4th
order predictor-corrector Hermite integrator with block time steps. We find
that for a large number of particles ($N \apgt 10^4$) modern graphics
processing units offer an attractive low cost alternative to GRAPE special
purpose hardware. A modern GPU continues to give a relatively flat scaling with
the number of particles, comparable to that of the GRAPE. Using the same time
step criterion the total energy of the $N$-body system was conserved better
than to one in $10^6$ on the GPU, which is only about an order of magnitude
worse than obtained with GRAPE. For $N\apgt 10^6$ the GeForce 8800GTX was about
20 times faster than the host computer. Though still about an order of
magnitude slower than GRAPE, modern GPU's outperform GRAPE in their low cost,
long mean time between failure and the much larger onboard memory; the
GRAPE-6Af holds at most 256k particles whereas the GeForce 8800GTF can hold 9
million particles in memory.
</summary>
    <author>
      <name>Simon Portegies Zwart</name>
    </author>
    <author>
      <name>Robert Belleman</name>
    </author>
    <author>
      <name>Peter Geldof</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.newast.2007.05.004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.newast.2007.05.004" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to New Astronomy</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0702135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.0730v1</id>
    <updated>2007-04-05T14:47:25Z</updated>
    <published>2007-04-05T14:47:25Z</published>
    <title>Revisiting the Issues On Netflow Sample and Export Performance</title>
    <summary>  The high volume of packets and packet rates of traffic on some router links
makes it exceedingly difficult for routers to examine every packet in order to
keep detailed statistics about the traffic which is traversing the router.
Sampling is commonly applied on routers in order to limit the load incurred by
the collection of information that the router has to undertake when evaluating
flow information for monitoring purposes. The sampling process in nearly all
cases is a deterministic process of choosing 1 in every N packets on a
per-interface basis, and then forming the flow statistics based on the
collected sampled statistics. Even though this sampling may not be significant
for some statistics, such as packet rate, others can be severely distorted.
However, it is important to consider the sampling techniques and their relative
accuracy when applied to different traffic patterns. The main disadvantage of
sampling is the loss of accuracy in the collected trace when compared to the
original traffic stream. To date there has not been a detailed analysis of the
impact of sampling at a router in various traffic profiles and flow criteria.
In this paper, we assess the performance of the sampling process as used in
NetFlow in detail, and we discuss some techniques for the compensation of loss
of monitoring detail.
</summary>
    <author>
      <name>Hamed Haddadi</name>
    </author>
    <author>
      <name>Raul Landa</name>
    </author>
    <author>
      <name>Miguel Rio</name>
    </author>
    <author>
      <name>Saleem Bhatti</name>
    </author>
    <link href="http://arxiv.org/abs/0704.0730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.0730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.0879v1</id>
    <updated>2007-04-06T11:46:49Z</updated>
    <published>2007-04-06T11:46:49Z</published>
    <title>A Hierarchical Approach for Dependability Analysis of a Commercial
  Cache-Based RAID Storage Architecture</title>
    <summary>  We present a hierarchical simulation approach for the dependability analysis
and evaluation of a highly available commercial cache-based RAID storage
system. The archi-tecture is complex and includes several layers of
overlap-ping error detection and recovery mechanisms. Three ab-straction levels
have been developed to model the cache architecture, cache operations, and
error detection and recovery mechanism. The impact of faults and errors
oc-curring in the cache and in the disks is analyzed at each level of the
hierarchy. A simulation submodel is associated with each abstraction level. The
models have been devel-oped using DEPEND, a simulation-based environment for
system-level dependability analysis, which provides facili-ties to inject
faults into a functional behavior model, to simulate error detection and
recovery mechanisms, and to evaluate quantitative measures. Several fault
models are defined for each submodel to simulate cache component failures, disk
failures, transmission errors, and data errors in the cache memory and in the
disks. Some of the parame-ters characterizing fault injection in a given
submodel cor-respond to probabilities evaluated from the simulation of the
lower-level submodel. Based on the proposed method-ology, we evaluate and
analyze 1) the system behavior un-der a real workload and high error rate
(focusing on error bursts), 2) the coverage of the error detection mechanisms
implemented in the system and the error latency distribu-tions, and 3) the
accumulation of errors in the cache and in the disks.
</summary>
    <author>
      <name>Mohamed Kaaniche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS</arxiv:affiliation>
    </author>
    <author>
      <name>Luigi Romano</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UIUC</arxiv:affiliation>
    </author>
    <author>
      <name>Zbigniew Kalbarczyk</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UIUC</arxiv:affiliation>
    </author>
    <author>
      <name>Ravishankar Iyer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UIUC</arxiv:affiliation>
    </author>
    <author>
      <name>Rick Karcich</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">STORAGETEK</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 28th IEEE International Symposium on Fault-Tolerant
  Computing (FTCS-28), Munich (Germany), IEEE Computer Society, June 1998,
  pp.6-15 (1998) 6-15</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0704.0879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.0879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.0789v1</id>
    <updated>2007-10-03T13:46:01Z</updated>
    <published>2007-10-03T13:46:01Z</published>
    <title>Wireless Local Area Networks with Multiple-Packet Reception Capability</title>
    <summary>  Thanks to its simplicity and cost efficiency, wireless local area network
(WLAN) enjoys unique advantages in providing high-speed and low-cost wireless
services in hot spots and indoor environments. Traditional WLAN
medium-access-control (MAC) protocols assume that only one station can transmit
at a time: simultaneous transmissions of more than one station causes the
destruction of all packets involved. By exploiting recent advances in PHY-layer
multiuser detection (MUD) techniques, it is possible for a receiver to receive
multiple packets simultaneously. This paper argues that such multipacket
reception (MPR) capability can greatly enhance the capacity of future WLANs. In
addition, it provides the MAC-layer and PHY-layer designs needed to achieve the
improved capacity. First, to demonstrate MUD/MPR as a powerful
capacity-enhancement technique, we prove a "super-linearity" result, which
states that the system throughput per unit cost increases as the MPR capability
increases. Second, we show that the commonly deployed binary exponential
backoff (BEB) algorithm in today's WLAN MAC may not be optimal in an MPR
system, and that the optimal backoff factor increases with the MPR capability:
the number of packets that can be received simultaneously. Third, based on the
above insights, we design a joint MAC-PHY layer protocol for an IEEE
802.11-like WLAN that incorporates advanced PHY-layer blind detection and MUD
techniques to implement MPR
</summary>
    <author>
      <name>Ying Jun Zhang</name>
    </author>
    <author>
      <name>Peng Xuan Zheng</name>
    </author>
    <author>
      <name>Soung Chang Liew</name>
    </author>
    <link href="http://arxiv.org/abs/0710.0789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.0789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.1968v1</id>
    <updated>2008-05-14T05:00:12Z</updated>
    <published>2008-05-14T05:00:12Z</published>
    <title>Heavy-Tailed Limits for Medium Size Jobs and Comparison Scheduling</title>
    <summary>  We study the conditional sojourn time distributions of processor sharing
(PS), foreground background processor sharing (FBPS) and shortest remaining
processing time first (SRPT) scheduling disciplines on an event where the job
size of a customer arriving in stationarity is smaller than exactly k&gt;=0 out of
the preceding m&gt;=k arrivals. Then, conditioning on the preceding event, the
sojourn time distribution of this newly arriving customer behaves
asymptotically the same as if the customer were served in isolation with a
server of rate (1-\rho)/(k+1) for PS/FBPS, and (1-\rho) for SRPT, respectively,
where \rho is the traffic intensity. Hence, the introduced notion of
conditional limits allows us to distinguish the asymptotic performance of the
studied schedulers by showing that SRPT exhibits considerably better asymptotic
behavior for relatively smaller jobs than PS/FBPS.
  Inspired by the preceding results, we propose an approximation to the SRPT
discipline based on a novel adaptive job grouping mechanism that uses relative
size comparison of a newly arriving job to the preceding m arrivals.
Specifically, if the newly arriving job is smaller than k and larger than m-k
of the previous m jobs, it is routed into class k. Then, the classes of smaller
jobs are served with higher priorities using the static priority scheduling.
The good performance of this mechanism, even for a small number of classes m+1,
is demonstrated using the asymptotic queueing analysis under the heavy-tailed
job requirements. We also discuss refinements of the comparison grouping
mechanism that improve the accuracy of job classification at the expense of a
small additional complexity.
</summary>
    <author>
      <name>Predrag R. Jelenkovic</name>
    </author>
    <author>
      <name>Xiaozhu Kang</name>
    </author>
    <author>
      <name>Jian Tan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0805.1968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.1968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.3897v1</id>
    <updated>2008-05-26T08:58:16Z</updated>
    <published>2008-05-26T08:58:16Z</published>
    <title>SPARK00: A Benchmark Package for the Compiler Evaluation of
  Irregular/Sparse Codes</title>
    <summary>  We propose a set of benchmarks that specifically targets a major cause of
performance degradation in high performance computing platforms: irregular
access patterns. These benchmarks are meant to be used to asses the performance
of optimizing compilers on codes with a varying degree of irregular access. The
irregularity caused by the use of pointers and indirection arrays are a major
challenge for optimizing compilers. Codes containing such patterns are
notoriously hard to optimize but they have a huge impact on the performance of
modern architectures, which are under-utilized when encountering irregular
memory accesses. In this paper, a set of benchmarks is described that
explicitly measures the performance of kernels containing a variety of
different access patterns found in real world applications. By offering a
varying degree of complexity, we provide a platform for measuring the
effectiveness of transformations. The difference in complexity stems from a
difference in traversal patterns, the use of multiple indirections and control
flow statements. The kernels used cover a variety of different access patterns,
namely pointer traversals, indirection arrays, dynamic loop bounds and run-time
dependent if-conditions. The kernels are small enough to be fully understood
which makes this benchmark set very suitable for the evaluation of
restructuring transformations.
</summary>
    <author>
      <name>H. L. A. van der Spek</name>
    </author>
    <author>
      <name>E. M. Bakker</name>
    </author>
    <author>
      <name>H. A. G. Wijshoff</name>
    </author>
    <link href="http://arxiv.org/abs/0805.3897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.3897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0807.0629v1</id>
    <updated>2008-07-03T19:47:11Z</updated>
    <published>2008-07-03T19:47:11Z</published>
    <title>Exact two-terminal reliability of some directed networks</title>
    <summary>  The calculation of network reliability in a probabilistic context has long
been an issue of practical and academic importance. Conventional approaches
(determination of bounds, sums of disjoint products algorithms, Monte Carlo
evaluations, studies of the reliability polynomials, etc.) only provide
approximations when the network's size increases, even when nodes do not fail
and all edges have the same reliability p. We consider here a directed, generic
graph of arbitrary size mimicking real-life long-haul communication networks,
and give the exact, analytical solution for the two-terminal reliability. This
solution involves a product of transfer matrices, in which individual
reliabilities of edges and nodes are taken into account. The special case of
identical edge and node reliabilities (p and rho, respectively) is addressed.
We consider a case study based on a commonly-used configuration, and assess the
influence of the edges being directed (or not) on various measures of network
performance. While the two-terminal reliability, the failure frequency and the
failure rate of the connection are quite similar, the locations of complex
zeros of the two-terminal reliability polynomials exhibit strong differences,
and various structure transitions at specific values of rho. The present work
could be extended to provide a catalog of exactly solvable networks in terms of
reliability, which could be useful as building blocks for new and improved
bounds, as well as benchmarks, in the general case.
</summary>
    <author>
      <name>Christian Tanguy</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 6th International Workshop on the Design of
  Reliable Communication, La Rochelle : France (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0807.0629v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0807.0629v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.0148v1</id>
    <updated>2008-12-31T21:25:32Z</updated>
    <published>2008-12-31T21:25:32Z</published>
    <title>Using constraint programming to resolve the multi-source/multi-site data
  movement paradigm on the Grid</title>
    <summary>  In order to achieve both fast and coordinated data transfer to collaborative
sites as well as to create a distribution of data over multiple sites,
efficient data movement is one of the most essential aspects in distributed
environment. With such capabilities at hand, truly distributed task scheduling
with minimal latencies would be reachable by internationally distributed
collaborations (such as ones in HENP) seeking for scavenging or maximizing on
geographically spread computational resources. But it is often not all clear
(a) how to move data when available from multiple sources or (b) how to move
data to multiple compute resources to achieve an optimal usage of available
resources. We present a method of creating a Constraint Programming (CP) model
consisting of sites, links and their attributes such as bandwidth for grid
network data transfer also considering user tasks as part of the objective
function for an optimal solution. We will explore and explain trade-off between
schedule generation time and divergence from the optimal solution and show how
to improve and render viable the solution's finding time by using search tree
time limit, approximations, restrictions such as symmetry breaking or grouping
similar tasks together, or generating sequence of optimal schedules by
splitting the input problem. Results of data transfer simulation for each case
will also include a well known Peer-2-Peer model, and time taken to generate a
schedule as well as time needed for a schedule execution will be compared to a
CP optimal solution. We will additionally present a possible implementation
aimed to bring a distributed datasets (multiple sources) to a given site in a
minimal time.
</summary>
    <author>
      <name>Michal Zerola</name>
    </author>
    <author>
      <name>Jerome Lauret</name>
    </author>
    <author>
      <name>Roman Bartak</name>
    </author>
    <author>
      <name>Michal Sumbera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages; ACAT 2008 workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/0901.0148v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.0148v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.1035v8</id>
    <updated>2009-07-06T11:46:38Z</updated>
    <published>2009-02-06T09:30:53Z</published>
    <title>Towards a Statistical Methodology to Evaluate Program Speedups and their
  Optimisation Techniques</title>
    <summary>  The community of program optimisation and analysis, code performance
evaluation, parallelisation and optimising compilation has published since many
decades hundreds of research and engineering articles in major conferences and
journals. These articles study efficient algorithms, strategies and techniques
to accelerate programs execution times, or optimise other performance metrics
(MIPS, code size, energy/power, MFLOPS, etc.). Many speedups are published, but
nobody is able to reproduce them exactly. The non-reproducibility of our
research results is a dark point of the art, and we cannot be qualified as {\it
computer scientists} if we do not provide rigorous experimental methodology.
This article provides a first effort towards a correct statistical protocol for
analysing and measuring speedups. As we will see, some common mistakes are done
by the community inside published articles, explaining part of the
non-reproducibility of the results. Our current article is not sufficient by
its own to deliver a complete experimental methodology, further efforts must be
done by the community to decide about a common protocol for our future
experiences. Anyway, our community should take care about the aspect of
reproducibility of the results in the future.
</summary>
    <author>
      <name>Sid Touati</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PRISM</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0902.1035v8" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.1035v8" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.1394v2</id>
    <updated>2010-02-01T17:56:48Z</updated>
    <published>2009-02-09T10:05:48Z</published>
    <title>Fundamental delay bounds in peer-to-peer chunk-based real-time streaming
  systems</title>
    <summary>  This paper addresses the following foundational question: what is the maximum
theoretical delay performance achievable by an overlay peer-to-peer streaming
system where the streamed content is subdivided into chunks? As shown in this
paper, when posed for chunk-based systems, and as a consequence of the
store-and-forward way in which chunks are delivered across the network, this
question has a fundamentally different answer with respect to the case of
systems where the streamed content is distributed through one or more flows
(sub-streams). To circumvent the complexity emerging when directly dealing with
delay, we express performance in term of a convenient metric, called "stream
diffusion metric". We show that it is directly related to the end-to-end
minimum delay achievable in a P2P streaming network. In a homogeneous scenario,
we derive a performance bound for such metric, and we show how this bound
relates to two fundamental parameters: the upload bandwidth available at each
node, and the number of neighbors a node may deliver chunks to. In this bound,
k-step Fibonacci sequences do emerge, and appear to set the fundamental laws
that characterize the optimal operation of chunk-based systems.
</summary>
    <author>
      <name>Giuseppe Bianchi</name>
    </author>
    <author>
      <name>Nicola Blefari Melazzi</name>
    </author>
    <author>
      <name>Lorenzo Bracciale</name>
    </author>
    <author>
      <name>Francesca Lo Piccolo</name>
    </author>
    <author>
      <name>Stefano Salsano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 21st International Teletraffic Congress (ITC 21),
  2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0902.1394v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.1394v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.4481v2</id>
    <updated>2009-02-26T22:34:03Z</updated>
    <published>2009-02-25T22:59:58Z</published>
    <title>Stability of Finite Population ALOHA with Variable Packets</title>
    <summary>  ALOHA is one of the most basic Medium Access Control (MAC) protocols and
represents a foundation for other more sophisticated distributed and
asynchronous MAC protocols, e.g., CSMA. In this paper, unlike in the
traditional work that focused on mean value analysis, we study the
distributional properties of packet transmission delays over an ALOHA channel.
We discover a new phenomenon showing that a basic finite population ALOHA model
with variable size (exponential) packets is characterized by power law
transmission delays, possibly even resulting in zero throughput. These results
are in contrast to the classical work that shows exponential delays and
positive throughput for finite population ALOHA with fixed packets.
Furthermore, we characterize a new stability condition that is entirely derived
from the tail behavior of the packet and backoff distributions that may not be
determined by mean values. The power law effects and the possible instability
might be diminished, or perhaps eliminated, by reducing the variability of
packets. However, we show that even a slotted (synchronized) ALOHA with packets
of constant size can exhibit power law delays when the number of active users
is random. From an engineering perspective, our results imply that the
variability of packet sizes and number of active users need to be taken into
consideration when designing robust MAC protocols, especially for ad-hoc/sensor
networks where other factors, such as link failures and mobility, might further
compound the problem.
</summary>
    <author>
      <name>Predrag R. Jelenkovic</name>
    </author>
    <author>
      <name>Jian Tan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0902.4481v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.4481v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.3966v1</id>
    <updated>2009-06-22T09:33:17Z</updated>
    <published>2009-06-22T09:33:17Z</published>
    <title>Application of non-uniform laxity to EDF for aperiodic tasks to improve
  task utilisation on multicore platforms</title>
    <summary>  This paper proposes a new scheduler applying the concept of non-uniform
laxity to Earliest deadline first (EDF) approach for aperiodic tasks. This
scheduler improves task utilisation (Execution time / deadline) and also
increases the number of tasks that are being scheduled. Laxity is a measure of
the spare time permitted for the task before it misses its deadline, and is
computed using the expression (deadline - (current time + execution time)).
Weight decides the priority of the task and is defined by the expression
(quantum slice time / allocated time)*total core time for the task. Quantum
slice time is the time actually used, allocated time is the time allocated by
the scheduler, and total core time is the time actually reserved by the core
for execution of one quantum of the task. Non-uniform laxity enables scheduling
of tasks that have higher priority before the normal execution of other tasks
and is computed by multiplying the weight of the task with its laxity. The
algorithm presented in the paper has been simulated on Cheddar, a real time
scheduling tool and also on SESC, an architectural simulator for multicore
platforms, for upto 5000 random task sets, and upto 5000 cores. This scheduler
improves task utilisation by 35% and the number of tasks being scheduled by
36%, compared to conventional EDF.
</summary>
    <author>
      <name>K Pradheep Kumar</name>
    </author>
    <author>
      <name>A P Shanthi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, Journal of Computer Science and Information Security</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSIS 2009, June Issue, Vol. 2, No. 1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0906.3966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.3966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.3047v1</id>
    <updated>2009-07-17T11:35:46Z</updated>
    <published>2009-07-17T11:35:46Z</published>
    <title>Performance of Network and Service Monitoring Frameworks</title>
    <summary>  The efficiency and the performance of anagement systems is becoming a hot
research topic within the networks and services management community. This
concern is due to the new challenges of large scale managed systems, where the
management plane is integrated within the functional plane and where management
activities have to carry accurate and up-to-date information. We defined a set
of primary and secondary metrics to measure the performance of a management
approach. Secondary metrics are derived from the primary ones and quantifies
mainly the efficiency, the scalability and the impact of management activities.
To validate our proposals, we have designed and developed a benchmarking
platform dedicated to the measurement of the performance of a JMX manager-agent
based management system. The second part of our work deals with the collection
of measurement data sets from our JMX benchmarking platform. We mainly studied
the effect of both load and the number of agents on the scalability, the impact
of management activities on the user perceived performance of a managed server
and the delays of JMX operations when carrying variables values. Our findings
show that most of these delays follow a Weibull statistical distribution. We
used this statistical model to study the behavior of a monitoring algorithm
proposed in the literature, under heavy tail delays distribution. In this case,
the view of the managed system on the manager side becomes noisy and out of
date.
</summary>
    <author>
      <name>Abdelkader Lahmadi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Andrey</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Festor</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">11th IFIP/IEEE International Symposium on Integrated Network
  Management (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.3047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.3047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.0819v1</id>
    <updated>2009-10-05T18:30:05Z</updated>
    <published>2009-10-05T18:30:05Z</published>
    <title>Performance Evaluation of Wimax Physical Layer under Adaptive Modulation
  Techniques and Communication Channels</title>
    <summary>  Wimax (Worldwide Interoperability for Microwave Access) is a promising
technology which can offer high speed voice, video and data service up to the
customer end. The aim of this paper is the performance evaluation of an Wimax
system under different combinations of digital modulation (BPSK, QPSK, 4 QAM
and 16 QAM) and different communication channels AWGN and fading channels
(Rayleigh and Rician). And the Wimax system incorporates Reed Solomon (RS)
encoder with Convolutional encoder with half and two third rated codes in FEC
channel coding. The simulation results of estimated Bit Error Rate (BER)
displays that the implementation of interleaved RS code (255, 239, 8) with two
third rated Convolutional code under BPSK modulation technique is highly
effective to combat in the Wimax communication system. To complete this
performance analysis in Wimax based systems, a segment of audio signal is used
for analysis. The transmitted audio message is found to have retrieved
effectively under noisy situation.
</summary>
    <author>
      <name>Md. Ashraful Islam</name>
    </author>
    <author>
      <name>Riaz Uddin Mondal</name>
    </author>
    <author>
      <name>Md. Zahid Hasan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 5, No. 1, pp. 111-114, September 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0910.0819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.0819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.1149v1</id>
    <updated>2010-02-05T08:39:11Z</updated>
    <published>2010-02-05T08:39:11Z</published>
    <title>A Performance Study of GA and LSH in Multiprocessor Job Scheduling</title>
    <summary>  Multiprocessor task scheduling is an important and computationally difficult
problem. This paper proposes a comparison study of genetic algorithm and list
scheduling algorithm. Both algorithms are naturally parallelizable but have
heavy data dependencies. Based on experimental results, this paper presents a
detailed analysis of the scalability, advantages and disadvantages of each
algorithm. Multiprocessors have emerged as a powerful computing means for
running real-time applications, especially where a uni-processor system would
not be sufficient enough to execute all the tasks. The high performance and
reliability of multiprocessors have made them a powerful computing resource.
Such computing environment requires an efficient algorithm to determine when
and on which processor a given task should execute. In multiprocessor systems,
an efficient scheduling of a parallel program onto the processors that
minimizes the entire execution time is vital for achieving a high performance.
This scheduling problem is known to be NP- Hard. In multiprocessor scheduling
problem, a given program is to be scheduled in a given multiprocessor system
such that the program's execution time is minimized. The last job must be
completed as early as possible. Genetic algorithm (GA) is one of the widely
used techniques for constrained optimization.
</summary>
    <author>
      <name>S. R. Vijayalakshmi</name>
    </author>
    <author>
      <name>G. Padmavathi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 1, January 2010,
  http://ijcsi.org/articles/A-Performance-Study-of-GA-and-LSH-in-Multiprocessor-Job-Scheduling.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 1, January 2010,
  http://ijcsi.org/articles/A-Performance-Study-of-GA-and-LSH-in-Multiprocessor-Job-Scheduling.php</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.1149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.1149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.3493v2</id>
    <updated>2011-07-12T20:06:58Z</updated>
    <published>2010-02-18T17:30:54Z</published>
    <title>The Missing Piece Syndrome in Peer-to-Peer Communication</title>
    <summary>  Typical protocols for peer-to-peer file sharing over the Internet divide
files to be shared into pieces. New peers strive to obtain a complete
collection of pieces from other peers and from a seed. In this paper we
investigate a problem that can occur if the seeding rate is not large enough.
The problem is that, even if the statistics of the system are symmetric in the
pieces, there can be symmetry breaking, with one piece becoming very rare. If
peers depart after obtaining a complete collection, they can tend to leave
before helping other peers receive the rare piece. Assuming that peers arrive
with no pieces, there is a single seed, random peer contacts are made, random
useful pieces are downloaded, and peers depart upon receiving the complete
file, the system is stable if the seeding rate (in pieces per time unit) is
greater than the arrival rate, and is unstable if the seeding rate is less than
the arrival rate. The result persists for any piece selection policy that
selects from among useful pieces, such as rarest first, and it persists with
the use of network coding.
</summary>
    <author>
      <name>Bruce Hajek</name>
    </author>
    <author>
      <name>Ji Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures in 5 files. An earlier version appeared in the
  2010 IEEE International Symposium on Information Theory</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.3493v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.3493v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4062v1</id>
    <updated>2010-03-22T05:13:11Z</updated>
    <published>2010-03-22T05:13:11Z</published>
    <title>A Rank Based Replacement Policy for Multimedia Server Cache Using
  Zipf-Like Law</title>
    <summary>  The cache replacement algorithm plays an important role in the overall
performance of Proxy-Server system. In this paper we have proposed VoD cache
memory replacement algorithm for a multimedia server system. We propose a Rank
based cache replacement policy to manage the cache space in individual proxy
server cache. Proposed replacement strategy incorporates in a simple way the
most important characteristics of the video and its accesses such as its size,
access frequency, recentness of the last access and the cost incurred while
transferring the requested video from the server to the proxy. We compare our
algorithm with some popular cache replacement algorithm using simulation. The
video objects are ranked based on the access trend by considering the factors
such as size, frequency and cost. Many studies have demonstrated that
Zipf's-like law can govern many features of the VoD and is used to describe the
popularity of the video. In this paper, we have designed a model, which ranks
the video on the basis of its popularity using the Zipf-like law. The video
with higher ranking is named "hot", while the video with lower ranking is named
"cold". The result show that the proposed rank based algorithm improves cache
hit ratio, cache byte ratio and average request latencies compared to other
algorithms. Our experimental results indicate that Rank based cache replacement
algorithm outperforms LRU, LFU and Greedy Dual.
</summary>
    <author>
      <name>T R Gopalakrishnan Nair</name>
    </author>
    <author>
      <name>P Jayarekha</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 2, Issue 3, March 2010,
  https://sites.google.com/site/journalofcomputing/</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.1674v2</id>
    <updated>2011-01-12T03:14:35Z</updated>
    <published>2010-06-08T23:45:56Z</published>
    <title>Seeing Through Black Boxes : Tracking Transactions through Queues under
  Monitoring Resource Constraints</title>
    <summary>  The problem of optimal allocation of monitoring resources for tracking
transactions progressing through a distributed system, modeled as a queueing
network, is considered. Two forms of monitoring information are considered,
viz., locally unique transaction identifiers, and arrival and departure
timestamps of transactions at each processing queue. The timestamps are assumed
available at all the queues but in the absence of identifiers, only enable
imprecise tracking since parallel processing can result in out-of-order
departures. On the other hand, identifiers enable precise tracking but are not
available without proper instrumentation. Given an instrumentation budget, only
a subset of queues can be selected for production of identifiers, while the
remaining queues have to resort to imprecise tracking using timestamps. The
goal is then to optimally allocate the instrumentation budget to maximize the
overall tracking accuracy. The challenge is that the optimal allocation
strategy depends on accuracies of timestamp-based tracking at different queues,
which has complex dependencies on the arrival and service processes, and the
queueing discipline. We propose two simple heuristics for allocation by
predicting the order of timestamp-based tracking accuracies of different
queues. We derive sufficient conditions for these heuristics to achieve
optimality through the notion of stochastic comparison of queues. Simulations
show that our heuristics are close to optimality, even when the parameters
deviate from these conditions.
</summary>
    <author>
      <name>Animashree Anandkumar</name>
    </author>
    <author>
      <name>Ting He</name>
    </author>
    <author>
      <name>Chatschik Bisdikian</name>
    </author>
    <author>
      <name>Dakshi Agrawal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Elsevier Performance Evaluation</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.1674v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.1674v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.4824v1</id>
    <updated>2010-06-24T16:19:05Z</updated>
    <published>2010-06-24T16:19:05Z</published>
    <title>Decentralized Fair Scheduling in Two-Hop Relay-Assisted Cognitive OFDMA
  Systems</title>
    <summary>  In this paper, we consider a two-hop relay-assisted cognitive downlink OFDMA
system (named as secondary system) dynamically accessing a spectrum licensed to
a primary network, thereby improving the efficiency of spectrum usage. A
cluster-based relay-assisted architecture is proposed for the secondary system,
where relay stations are employed for minimizing the interference to the users
in the primary network and achieving fairness for cell-edge users. Based on
this architecture, an asymptotically optimal solution is derived for jointly
controlling data rates, transmission power, and subchannel allocation to
optimize the average weighted sum goodput where the proportional fair
scheduling (PFS) is included as a special case. This solution supports
decentralized implementation, requires small communication overhead, and is
robust against imperfect channel state information at the transmitter (CSIT)
and sensing measurement. The proposed solution achieves significant throughput
gains and better user-fairness compared with the existing designs. Finally, we
derived a simple and asymptotically optimal scheduling solution as well as the
associated closed-form performance under the proportional fair scheduling for a
large number of users. The system throughput is shown to be
$\mathcal{O}\left(N(1-q_p)(1-q_p^N)\ln\ln K_c\right)$, where $K_c$ is the
number of users in one cluster, $N$ is the number of subchannels and $q_p$ is
the active probability of primary users.
</summary>
    <author>
      <name>Rui Wang</name>
    </author>
    <author>
      <name>Vincent K. N. Lau</name>
    </author>
    <author>
      <name>Ying Cui</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JSTSP.2010.2056352</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JSTSP.2010.2056352" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 9 figures, IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL
  PROCESSING</arxiv:comment>
    <link href="http://arxiv.org/abs/1006.4824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.4824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.4523v2</id>
    <updated>2011-12-09T17:45:04Z</updated>
    <published>2011-01-24T13:32:25Z</published>
    <title>Bandwidth sharing networks with priority scaling</title>
    <summary>  In multi-class communication networks, traffic surges due to one class of
users can significantly degrade the performance for other classes. During these
transient periods, it is thus of crucial importance to implement priority
mechanisms that conserve the quality of service experienced by the affected
classes, while ensuring that the temporarily unstable class is not entirely
neglected. In this paper, we examine the complex interaction occurring between
several classes of traffic when classes obtain bandwidth proportionally to
their incoming traffic.
  We characterize the evolution of the network from the moment the initial
surge takes place until the system reaches its equilibrium. Using an
appropriate scaling, we show that the trajectories of the temporarily unstable
class can be described by a differential equation, while those of the stable
classes retain their stochastic nature. A stochastic averaging phenomenon
occurs and the dynamics of the temporarily unstable and the stable classes
continue to influence one another. We further proceed to characterize the
obtained differential equations and the stability region under this scaling for
monotone networks. We illustrate these result on several toy examples and we
finally build a penalization rule using these results for a network integrating
streaming and elastic traffic.
</summary>
    <author>
      <name>Mathieu Feuillet</name>
    </author>
    <author>
      <name>Matthieu Jonckheere</name>
    </author>
    <author>
      <name>Balakrishna J. Prabhu</name>
    </author>
    <link href="http://arxiv.org/abs/1101.4523v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.4523v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.5794v1</id>
    <updated>2011-01-30T17:59:58Z</updated>
    <published>2011-01-30T17:59:58Z</published>
    <title>Scheduling in a random environment: stability and asymptotic optimality</title>
    <summary>  We investigate the scheduling of a common resource between several concurrent
users when the feasible transmission rate of each user varies randomly over
time. Time is slotted and users arrive and depart upon service completion. This
may model for example the flow-level behavior of end-users in a narrowband HDR
wireless channel (CDMA 1xEV-DO). As performance criteria we consider the
stability of the system and the mean delay experienced by the users. Given the
complexity of the problem we investigate the fluid-scaled system, which allows
to obtain important results and insights for the original system: (1) We
characterize for a large class of scheduling policies the stability conditions
and identify a set of maximum stable policies, giving in each time slot
preference to users being in their best possible channel condition. We find in
particular that many opportunistic scheduling policies like Score-Based,
Proportionally Best or Potential Improvement are stable under the maximum
stability conditions, whereas the opportunistic scheduler Relative-Best or the
cmu-rule are not. (2) We show that choosing the right tie-breaking rule is
crucial for the performance (e.g. average delay) as perceived by a user. We
prove that a policy is asymptotically optimal if it is maximum stable and the
tie-breaking rule gives priority to the user with the highest departure
probability. We will refer to such tie-breaking rule as myopic. (3) We derive
the growth rates of the number of users in the system in overload settings
under various policies, which give additional insights on the performance. (4)
We conclude that simple priority-index policies with the myopic tie-breaking
rule, are stable and asymptotically optimal. All our findings are validated
with extensive numerical experiments.
</summary>
    <author>
      <name>U. Ayesta</name>
    </author>
    <author>
      <name>M. Erausquin</name>
    </author>
    <author>
      <name>M. Jonckheere</name>
    </author>
    <author>
      <name>I. M. Verloop</name>
    </author>
    <link href="http://arxiv.org/abs/1101.5794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.5794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.5243v2</id>
    <updated>2011-09-20T15:41:17Z</updated>
    <published>2011-04-27T20:31:11Z</published>
    <title>Pushing the limits for medical image reconstruction on recent standard
  multicore processors</title>
    <summary>  Volume reconstruction by backprojection is the computational bottleneck in
many interventional clinical computed tomography (CT) applications. Today
vendors in this field replace special purpose hardware accelerators by standard
hardware like multicore chips and GPGPUs. Medical imaging algorithms are on the
verge of employing High Performance Computing (HPC) technology, and are
therefore an interesting new candidate for optimization. This paper presents
low-level optimizations for the backprojection algorithm, guided by a thorough
performance analysis on four generations of Intel multicore processors
(Harpertown, Westmere, Westmere EX, and Sandy Bridge).
  We choose the RabbitCT benchmark, a standardized testcase well supported in
industry, to ensure transparent and comparable results. Our aim is to provide
not only the fastest possible implementation but also compare to performance
models and hardware counter data in order to fully understand the results. We
separate the influence of algorithmic optimizations, parallelization, SIMD
vectorization, and microarchitectural issues and pinpoint problems with current
SIMD instruction set extensions on standard CPUs (SSE, AVX). The use of
assembly language is mandatory for best performance. Finally we compare our
results to the best GPGPU implementations available for this open competition
benchmark.
</summary>
    <author>
      <name>Jan Treibig</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Hannes G. Hofmann</name>
    </author>
    <author>
      <name>Joachim Hornegger</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1177/1094342012442424</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1177/1094342012442424" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 9 figures. Revised and extended version</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.5243v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.5243v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.2380v2</id>
    <updated>2011-07-12T18:13:11Z</updated>
    <published>2011-06-13T03:51:59Z</published>
    <title>Mathematical Model for the Optimal Utilization Percentile in M/M/1
  Systems: A Contribution about Knees in Performance Curves</title>
    <summary>  Performance curves of queuing systems can be analyzed by separating them into
three regions: the flat region, the knee region, and the exponential region.
Practical considerations, usually locate the knee region between 70-90% of the
theoretical maximum utilization. However, there is not a clear agreement about
where the boundaries between regions are, and where exactly the utilization
knee is located. An open debate about knees in performance curves was
undertaken at least 20 years ago. This historical debate is mainly divided
between those who claim that a knee in the curve is not a well-defined term in
mathematics, or it is a subjective and not really meaningful concept, and those
who define knees mathematically and consider their relevance and application.
In this paper, we present a mathematical model and analysis for identifying the
three mentioned regions on performance curves for M/M/1 systems; specifically,
we found the knees, or optimal utilization percentiles, at the vertices of the
hyperbolas that relate response time as a function of utilization. Using these
results, we argue that an adaptive and optimal queuing system could be deployed
by keeping load and throughput within the knee region.
</summary>
    <author>
      <name>Francisco A. Gonzalez-Horta</name>
    </author>
    <author>
      <name>Rogerio A. Enriquez-Caldera</name>
    </author>
    <author>
      <name>Juan M. Ramirez-Cortes</name>
    </author>
    <author>
      <name>Jorge Martinez-Carballido</name>
    </author>
    <author>
      <name>Eldamira Buenfil-Alpuche</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures, 1 table, accepted for publication at The Third
  International Conference on Adaptive and Self-Adaptive Systems and
  Applications (ADAPTIVE 2011), September 25-30, 2011 - Rome, Italy. ISBN:
  978-1-61208-011-6</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.2380v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.2380v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.4922v1</id>
    <updated>2011-07-25T12:33:25Z</updated>
    <published>2011-07-25T12:33:25Z</published>
    <title>On the Performance of Space Shift Keying (SSK) Modulation with Imperfect
  Channel Knowledge</title>
    <summary>  In this paper, we study the sensitivity and robustness of Space Shift Keying
(SSK) modulation to imperfect channel knowledge at the receiver. Unlike the
common widespread belief, we show that SSK modulation is more robust to
imperfect channel knowledge than other state-of-the-art transmission
technologies, and only few training pilots are needed to get reliable enough
channel estimates for data detection. More precisely, we focus our attention on
the so-called Time-Orthogonal-Signal-Design (TOSD-) SSK modulation scheme,
which is an improved version of SSK modulation offering transmit-diversity
gains, and provide the following contributions: i) we develop a closed-form
analytical framework to compute the Average Bit Error Probability (ABEP) of a
mismatched detector for TOSD-SSK modulation, which can be used for arbitrary
transmit-antenna, receive-antenna, channel fading, and training pilots; ii) we
perform a comparative study of the performance of TOSD-SSK modulation and the
Alamouti code under the same imperfect channel knowledge, and show that
TOSD-SSK modulation is more robust to channel estimation errors; iii) we point
out that only few pilot pulses are required to get performance very close to
the perfect channel knowledge lower-bound; and iv) we verify that transmit- and
receive-diversity gains of TOSD-SSK modulation are preserved even for a
mismatched receiver.
</summary>
    <author>
      <name>Marco Di Renzo</name>
    </author>
    <author>
      <name>Dario De Leonardis</name>
    </author>
    <author>
      <name>Fabio Graziosi</name>
    </author>
    <author>
      <name>Harald Haas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/GLOCOM.2011.6133562</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/GLOCOM.2011.6133562" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE GLOBECOM 2011 (to appear)</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.4922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.4922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.0187v5</id>
    <updated>2013-11-29T10:50:10Z</updated>
    <published>2011-07-31T16:10:57Z</published>
    <title>Analysis of Buffer Starvation with Application to Objective QoE
  Optimization of Streaming Services</title>
    <summary>  Our purpose in this paper is to characterize buffer starvations for streaming
services. The buffer is modeled as an M/M/1 queue, plus the consideration of
bursty arrivals. When the buffer is empty, the service restarts after a certain
amount of packets are \emph{prefetched}. With this goal, we propose two
approaches to obtain the \emph{exact distribution} of the number of buffer
starvations, one of which is based on \emph{Ballot theorem}, and the other uses
recursive equations. The Ballot theorem approach gives an explicit result. We
extend this approach to the scenario with a constant playback rate using
T\`{a}kacs Ballot theorem. The recursive approach, though not offering an
explicit result, can obtain the distribution of starvations with
non-independent and identically distributed (i.i.d.) arrival process in which
an ON/OFF bursty arrival process is considered in this work. We further compute
the starvation probability as a function of the amount of prefetched packets
for a large number of files via a fluid analysis. Among many potential
applications of starvation analysis, we show how to apply it to optimize the
objective quality of experience (QoE) of media streaming, by exploiting the
tradeoff between startup/rebuffering delay and starvations.
</summary>
    <author>
      <name>Yuedong Xu</name>
    </author>
    <author>
      <name>Eitan Altman</name>
    </author>
    <author>
      <name>Rachid El-Azouzi</name>
    </author>
    <author>
      <name>Majed Haddad</name>
    </author>
    <author>
      <name>Salaheddine Elayoubi</name>
    </author>
    <author>
      <name>Tania Jimenez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2014.2300041</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2014.2300041" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures; IEEE Infocom 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.0187v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.0187v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.5679v1</id>
    <updated>2011-09-26T19:27:21Z</updated>
    <published>2011-09-26T19:27:21Z</published>
    <title>Queries mining for efficient routing in P2P communities</title>
    <summary>  Peer-to-peer (P2P) computing is currently attracting enormous attention. In
P2P systems a very large number of autonomous computing nodes (the peers) pool
together their resources and rely on each other for data and services.
Peer-to-peer (P2P) Data-sharing systems now generate a significant portion of
Internet traffic. Examples include P2P systems for network storage, web
caching, searching and indexing of relevant documents and distributed
network-threat analysis. Requirements for widely distributed information
systems supporting virtual organizations have given rise to a new category of
P2P systems called schema-based. In such systems each peer exposes its own
schema and the main objective is the efficient search across the P2P network by
processing each incoming query without overly consuming bandwidth. The
usability of these systems depends on effective techniques to find and retrieve
data; however, efficient and effective routing of content-based queries is a
challenging problem in P2P networks. This work was attended as an attempt to
motivate the use of mining algorithms and hypergraphs context to develop two
different methods that improve significantly the efficiency of P2P
communications. The proposed query routing methods direct the query to a set of
relevant peers in such way as to avoid network traffic and bandwidth
consumption. We compare the performance of the two proposed methods with the
baseline one and our experimental results prove that our proposed methods
generate impressive levels of performance and scalability.
</summary>
    <author>
      <name>Anis Ismail</name>
    </author>
    <author>
      <name>Mohamed Quafafou</name>
    </author>
    <author>
      <name>Nicolas Durand</name>
    </author>
    <author>
      <name>Gilles Nachouki</name>
    </author>
    <author>
      <name>Mohammad Hajjar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 9 figures. arXiv admin note: substantial text overlap with
  arXiv:1108.1378</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.5679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.5679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.5229v1</id>
    <updated>2012-01-25T10:22:58Z</updated>
    <published>2012-01-25T10:22:58Z</published>
    <title>Cross-entropy optimisation of importance sampling parameters for
  statistical model checking</title>
    <summary>  Statistical model checking avoids the exponential growth of states associated
with probabilistic model checking by estimating properties from multiple
executions of a system and by giving results within confidence bounds. Rare
properties are often very important but pose a particular challenge for
simulation-based approaches, hence a key objective under these circumstances is
to reduce the number and length of simulations necessary to produce a given
level of confidence. Importance sampling is a well-established technique that
achieves this, however to maintain the advantages of statistical model checking
it is necessary to find good importance sampling distributions without
considering the entire state space.
  Motivated by the above, we present a simple algorithm that uses the notion of
cross-entropy to find the optimal parameters for an importance sampling
distribution. In contrast to previous work, our algorithm uses a low
dimensional vector of parameters to define this distribution and thus avoids
the often intractable explicit representation of a transition matrix. We show
that our parametrisation leads to a unique optimum and can produce many orders
of magnitude improvement in simulation efficiency. We demonstrate the efficacy
of our methodology by applying it to models from reliability engineering and
biochemistry.
</summary>
    <author>
      <name>Cyrille Jégourel</name>
    </author>
    <author>
      <name>Axel Legay</name>
    </author>
    <author>
      <name>Sean Sedwards</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 8 figures, LNCS style</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.5229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.5229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.4423v1</id>
    <updated>2012-02-16T03:54:13Z</updated>
    <published>2012-02-16T03:54:13Z</published>
    <title>On the Reliability of RAID Systems: An Argument for More Check Drives</title>
    <summary>  In this paper we address issues of reliability of RAID systems. We focus on
"big data" systems with a large number of drives and advanced error correction
schemes beyond \RAID{6}. Our RAID paradigm is based on Reed-Solomon codes, and
thus we assume that the RAID consists of $N$ data drives and $M$ check drives.
The RAID fails only if the combined number of failed drives and sector errors
exceeds $M$, a property of Reed-Solomon codes.
  We review a number of models considered in the literature and build upon them
to construct models usable for a large number of data and check drives. We
attempt to account for a significant number of factors that affect RAID
reliability, such as drive replacement or lack thereof, mistakes during service
such as replacing the wrong drive, delayed repair, and the finite duration of
RAID reconstruction. We evaluate the impact of sector failures that do not
result in drive replacement.
  The reader who needs to consider large $M$ and $N$ will find applicable
mathematical techniques concisely summarized here, and should be able to apply
them to similar problems. Most methods are based on the theory of continuous
time Markov chains, but we move beyond this framework when we consider the
fixed time to rebuild broken hard drives, which we model using systems of delay
and partial differential equations.
  One universal statement is applicable across various models: increasing the
number of check drives in all cases increases the reliability of the system,
and is vastly superior to other approaches of ensuring reliability such as
mirroring.
</summary>
    <author>
      <name>Sarah Edge Mann</name>
    </author>
    <author>
      <name>Michael Anderson</name>
    </author>
    <author>
      <name>Marek Rychlik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 11 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.4423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.4423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60K10 (Primary) 62N05, 90B25 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.4880v1</id>
    <updated>2012-02-22T11:08:33Z</updated>
    <published>2012-02-22T11:08:33Z</published>
    <title>Performance Evaluation of the Random Replacement Policy for Networks of
  Caches</title>
    <summary>  The overall performance of content distribution networks as well as recently
proposed information-centric networks rely on both memory and bandwidth
capacities. In this framework, the hit ratio is the key performance indicator
which captures the bandwidth / memory tradeoff for a given global
performance.This paper focuses on the estimation of the hit ratio in a network
of caches that employ the Random replacement policy. Assuming that requests are
independent and identically distributed, general expressions of miss
probabilities for a single Random cache are provided as well as exact results
for specific popularity distributions. Moreover, for any Zipf popularity
distribution with exponent $\alpha$ &gt; 1, we obtain asymptotic equivalents for
the miss probability in the case of large cache size. We extend the analysis to
networks of Random caches, when the topology is either a line or a homogeneous
tree. In that case, approximations for miss probabilities across the network
are derived by assuming that miss events at any node occur independently in
time; the obtained results are compared to the same network using the
Least-Recently-Used discipline, already addressed in the literature. We further
analyze the case of a mixed tandem cache network where the two nodes employ
either Random or Least-Recently-Used policies. In all scenarios, asymptotic
formulas and approximations are extensively compared to simulations and shown
to perform very well. Finally, our results enable us to propose recommendations
for cache replacement disciplines in a network dedicated to content
distribution. These results also hold for a cache using the First-In-First-Out
policy.
</summary>
    <author>
      <name>Massimo Gallo</name>
    </author>
    <author>
      <name>Bruno Kauffmann</name>
    </author>
    <author>
      <name>Luca Muscariello</name>
    </author>
    <author>
      <name>Alain Simonian</name>
    </author>
    <author>
      <name>Christian Tanguy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 11 figures, accepted in a poster version at ACM SIGMETRICS
  2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.4880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.4880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.5040v1</id>
    <updated>2012-03-22T16:51:55Z</updated>
    <published>2012-03-22T16:51:55Z</published>
    <title>Minimizing Slowdown in Heterogeneous Size-Aware Dispatching Systems
  (full version)</title>
    <summary>  We consider a system of parallel queues where tasks are assigned (dispatched)
to one of the available servers upon arrival. The dispatching decision is based
on the full state information, i.e., on the sizes of the new and existing jobs.
We are interested in minimizing the so-called mean slowdown criterion
corresponding to the mean of the sojourn time divided by the processing time.
Assuming no new jobs arrive, the shortest-processing-time-product (SPTP)
schedule is known to minimize the slowdown of the existing jobs. The main
contribution of this paper is three-fold: 1) To show the optimality of SPTP
with respect to slowdown in a single server queue under Poisson arrivals; 2) to
derive the so-called size-aware value functions for
M/G/1-FIFO/LIFO/SPTP/SPT/SRPT with general holding costs of which the slowdown
criterion is a special case; and 3) to utilize the value functions to derive
efficient dispatching policies so as to minimize the mean slowdown in a
heterogeneous server system. The derived policies offer a significantly better
performance than e.g., the size-aware-task-assignment with equal load (SITA-E)
and least-work-left (LWL) policies.
</summary>
    <author>
      <name>Esa Hyytiä</name>
    </author>
    <author>
      <name>Samuli Aalto</name>
    </author>
    <author>
      <name>Aleksi Penttinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the full version of a paper with the same title that appears
  in ACM SIGMETRICS 2012, with the inclusion of the appendix. 15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.5040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.5040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.1204v1</id>
    <updated>2012-06-06T12:48:25Z</updated>
    <published>2012-06-06T12:48:25Z</published>
    <title>Uncertainty Analysis of the Adequacy Assessment Model of a Distributed
  Generation System</title>
    <summary>  Due to the inherent aleatory uncertainties in renewable generators, the
reliability/adequacy assessments of distributed generation (DG) systems have
been particularly focused on the probabilistic modeling of random behaviors,
given sufficient informative data. However, another type of uncertainty
(epistemic uncertainty) must be accounted for in the modeling, due to
incomplete knowledge of the phenomena and imprecise evaluation of the related
characteristic parameters. In circumstances of few informative data, this type
of uncertainty calls for alternative methods of representation, propagation,
analysis and interpretation. In this study, we make a first attempt to
identify, model, and jointly propagate aleatory and epistemic uncertainties in
the context of DG systems modeling for adequacy assessment. Probability and
possibility distributions are used to model the aleatory and epistemic
uncertainties, respectively. Evidence theory is used to incorporate the two
uncertainties under a single framework. Based on the plausibility and belief
functions of evidence theory, the hybrid propagation approach is introduced. A
demonstration is given on a DG system adapted from the IEEE 34 nodes
distribution test feeder. Compared to the pure probabilistic approach, it is
shown that the hybrid propagation is capable of explicitly expressing the
imprecision in the knowledge on the DG parameters into the final adequacy
values assessed. It also effectively captures the growth of uncertainties with
higher DG penetration levels.
</summary>
    <author>
      <name>Yanfu Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SSEC, LGI</arxiv:affiliation>
    </author>
    <author>
      <name>Enrico Zio</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SSEC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Renewable Energy 41 (2012) 235-244</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1206.1204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.1204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.1469v1</id>
    <updated>2012-07-05T21:47:33Z</updated>
    <published>2012-07-05T21:47:33Z</published>
    <title>Cramer-Rao Bounds for Joint RSS/DoA-Based Primary-User Localization in
  Cognitive Radio Networks</title>
    <summary>  Knowledge about the location of licensed primary-users (PU) could enable
several key features in cognitive radio (CR) networks including improved
spatio-temporal sensing, intelligent location-aware routing, as well as aiding
spectrum policy enforcement. In this paper we consider the achievable accuracy
of PU localization algorithms that jointly utilize received-signal-strength
(RSS) and direction-of-arrival (DoA) measurements by evaluating the Cramer-Rao
Bound (CRB). Previous works evaluate the CRB for RSS-only and DoA-only
localization algorithms separately and assume DoA estimation error variance is
a fixed constant or rather independent of RSS. We derive the CRB for joint
RSS/DoA-based PU localization algorithms based on the mathematical model of DoA
estimation error variance as a function of RSS, for a given CR placement. The
bound is compared with practical localization algorithms and the impact of
several key parameters, such as number of nodes, number of antennas and
samples, channel shadowing variance and correlation distance, on the achievable
accuracy are thoroughly analyzed and discussed. We also derive the closed-form
asymptotic CRB for uniform random CR placement, and perform theoretical and
numerical studies on the required number of CRs such that the asymptotic CRB
tightly approximates the numerical integration of the CRB for a given
placement.
</summary>
    <author>
      <name>Jun Wang</name>
    </author>
    <author>
      <name>Jianshu Chen</name>
    </author>
    <author>
      <name>Danijela Cabric</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TWC.2013.012513.120966</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TWC.2013.012513.120966" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 11 figures, 1 table, submitted to IEEE Transactions on
  Wireless Communications</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.1469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.1469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.2908v4</id>
    <updated>2014-03-19T13:55:47Z</updated>
    <published>2012-08-14T15:51:43Z</published>
    <title>Exploring performance and power properties of modern multicore chips via
  simple machine models</title>
    <summary>  Modern multicore chips show complex behavior with respect to performance and
power. Starting with the Intel Sandy Bridge processor, it has become possible
to directly measure the power dissipation of a CPU chip and correlate this data
with the performance properties of the running code. Going beyond a simple
bottleneck analysis, we employ the recently published Execution-Cache-Memory
(ECM) model to describe the single- and multi-core performance of streaming
kernels. The model refines the well-known roofline model, since it can predict
the scaling and the saturation behavior of bandwidth-limited loop kernels on a
multicore chip. The saturation point is especially relevant for considerations
of energy consumption. From power dissipation measurements of benchmark
programs with vastly different requirements to the hardware, we derive a
simple, phenomenological power model for the Sandy Bridge processor. Together
with the ECM model, we are able to explain many peculiarities in the
performance and power behavior of multicore processors, and derive guidelines
for energy-efficient execution of parallel programs. Finally, we show that the
ECM and power models can be successfully used to describe the scaling and power
behavior of a lattice-Boltzmann flow solver code.
</summary>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Jan Treibig</name>
    </author>
    <author>
      <name>Johannes Habich</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/cpe.3180</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/cpe.3180" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 10 figures. Typos corrected, DOI added</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.2908v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.2908v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.1811v2</id>
    <updated>2012-10-05T17:33:19Z</updated>
    <published>2012-09-09T16:39:23Z</published>
    <title>Evaluating the SiteStory Transactional Web Archive With the ApacheBench
  Tool</title>
    <summary>  Conventional Web archives are created by periodically crawling a web site and
archiving the responses from the Web server. Although easy to implement and
common deployed, this form of archiving typically misses updates and may not be
suitable for all preservation scenarios, for example a site that is required
(perhaps for records compliance) to keep a copy of all pages it has served. In
contrast, transactional archives work in conjunction with a Web server to
record all pages that have been served. Los Alamos National Laboratory has
developed SiteSory, an open-source transactional archive written in Java
solution that runs on Apache Web servers, provides a Memento compatible access
interface, and WARC file export features. We used the ApacheBench utility on a
pre-release version of to measure response time and content delivery time in
different environments and on different machines. The performance tests were
designed to determine the feasibility of SiteStory as a production-level
solution for high fidelity automatic Web archiving. We found that SiteStory
does not significantly affect content server performance when it is performing
transactional archiving. Content server performance slows from 0.076 seconds to
0.086 seconds per Web page access when the content server is under load, and
from 0.15 seconds to 0.21 seconds when the resource has many embedded and
changing resources.
</summary>
    <author>
      <name>Justin F. Brunelle</name>
    </author>
    <author>
      <name>Michael L. Nelson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.1811v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.1811v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.6630v2</id>
    <updated>2012-10-01T12:36:47Z</updated>
    <published>2012-09-28T19:58:58Z</published>
    <title>Quantum Monte Carlo for large chemical systems: Implementing efficient
  strategies for petascale platforms and beyond</title>
    <summary>  Various strategies to implement efficiently QMC simulations for large
chemical systems are presented. These include: i.) the introduction of an
efficient algorithm to calculate the computationally expensive Slater matrices.
This novel scheme is based on the use of the highly localized character of
atomic Gaussian basis functions (not the molecular orbitals as usually done),
ii.) the possibility of keeping the memory footprint minimal, iii.) the
important enhancement of single-core performance when efficient optimization
tools are employed, and iv.) the definition of a universal, dynamic,
fault-tolerant, and load-balanced computational framework adapted to all kinds
of computational platforms (massively parallel machines, clusters, or
distributed grids). These strategies have been implemented in the QMC=Chem code
developed at Toulouse and illustrated with numerical applications on small
peptides of increasing sizes (158, 434, 1056 and 1731 electrons). Using 10k-80k
computing cores of the Curie machine (GENCI-TGCC-CEA, France) QMC=Chem has been
shown to be capable of running at the petascale level, thus demonstrating that
for this machine a large part of the peak performance can be achieved.
Implementation of large-scale QMC simulations for future exascale platforms
with a comparable level of efficiency is expected to be feasible.
</summary>
    <author>
      <name>Anthony Scemama</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LCPQ</arxiv:affiliation>
    </author>
    <author>
      <name>Michel Caffarel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LCPQ</arxiv:affiliation>
    </author>
    <author>
      <name>Emmanuel Oseret</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ECR</arxiv:affiliation>
    </author>
    <author>
      <name>William Jalby</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ECR, PRISM</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/jcc.23216</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/jcc.23216" rel="related"/>
    <link href="http://arxiv.org/abs/1209.6630v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.6630v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.5736v1</id>
    <updated>2012-11-25T07:53:55Z</updated>
    <published>2012-11-25T07:53:55Z</published>
    <title>Critical Utility Infrastructural Resilience</title>
    <summary>  The paper refers to CRUTIAL, CRitical UTility InfrastructurAL Resilience, a
European project within the research area of Critical Information
Infrastructure Protection, with a specific focus on the infrastructures
operated by power utilities, widely recognized as fundamental to national and
international economy, security and quality of life. Such infrastructures faced
with the recent market deregulations and the multiple interdependencies with
other infrastructures are becoming more and more vulnerable to various threats,
including accidental failures and deliberate sabotage and malicious attacks.
The subject of CRUTIAL research are small scale networked ICT systems used to
control and manage the electric power grid, in which artifacts controlling the
physical process of electricity transportation need to be connected with
corporate and societal applications performing management and maintenance
functionality. The peculiarity of such ICT-supported systems is that they are
related to the power system dynamics and its emergency conditions. Specific
effort need to be devoted by the Electric Power community and by the
Information Technology community to influence the technological progress in
order to allow commercial intelligent electronic devices to be effectively
deployed for the protection of citizens against cyber threats to electric power
management and control systems. A well-founded know-how needs to be built
inside the industrial power sector to allow all the involved stakeholders to
achieve their service objectives without compromising the resilience properties
of the logical and physical assets that support the electric power provision.
</summary>
    <author>
      <name>Giovanna Dondossola</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISTI</arxiv:affiliation>
    </author>
    <author>
      <name>Geert Deconinck</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISTI</arxiv:affiliation>
    </author>
    <author>
      <name>Felicita Di Giandomenico</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISTI</arxiv:affiliation>
    </author>
    <author>
      <name>Susanna Donatelli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS</arxiv:affiliation>
    </author>
    <author>
      <name>Mohamed Kaaniche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS</arxiv:affiliation>
    </author>
    <author>
      <name>Paulo Verissimo</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Workshop on Complex Network and Infrastructure
  Protection (CNIP-06), Rome : Italy (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.5736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.4538v1</id>
    <updated>2013-03-19T10:18:40Z</updated>
    <published>2013-03-19T10:18:40Z</published>
    <title>Optimization of FASTEST-3D for Modern Multicore Systems</title>
    <summary>  FASTEST-3D is an MPI-parallel finite-volume flow solver based on
block-structured meshes that has been developed at the University of
Erlangen-Nuremberg since the early 1990s. It can be used to solve the laminar
or turbulent incompressible Navier-Stokes equations. Up to now its scalability
was strongly limited by a rather rigid communication infrastructure, which led
to a dominance of MPI time already at small process counts.
  This paper describes several optimizations to increase the performance,
scalability, and flexibility of FASTEST-3D. First, a node-level performance
analysis is carried out in order to pinpoint the main bottlenecks and identify
sweet spots for energy-efficient execution. In addition, a single-precision
version of the solver for the linear equation system arising from the
discretization of the governing equations is devised, which significantly
increases the single-core performance. Then the communication mechanisms in
FASTEST-3D are analyzed and a new communication strategy based on non-blocking
calls is implemented. Performance results with the revised version show
significantly increased single-node performance and considerably improved
communication patterns along with much better parallel scalability. In this
context we discuss the concept of "acceptable parallel efficiency" and how it
influences the real gain of the optimizations. Scaling measurements are carried
out on a modern petascale system. The obtained improvements are of major
importance for the use of FASTEST-3D on current high-performance computer
clusters and will help to perform simulations with much higher spatial and
temporal resolution to tackle turbulent flow in technical applications.
</summary>
    <author>
      <name>Christoph Scheit</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Jan Treibig</name>
    </author>
    <author>
      <name>Stefan Becker</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.4538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.4538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.4816v2</id>
    <updated>2013-03-21T03:39:20Z</updated>
    <published>2013-03-20T02:46:43Z</published>
    <title>Stochastic Modeling of Large-Scale Solid-State Storage Systems:
  Analysis, Design Tradeoffs and Optimization</title>
    <summary>  Solid state drives (SSDs) have seen wide deployment in mobiles, desktops, and
data centers due to their high I/O performance and low energy consumption. As
SSDs write data out-of-place, garbage collection (GC) is required to erase and
reclaim space with invalid data. However, GC poses additional writes that
hinder the I/O performance, while SSD blocks can only endure a finite number of
erasures. Thus, there is a performance-durability tradeoff on the design space
of GC. To characterize the optimal tradeoff, this paper formulates an
analytical model that explores the full optimal design space of any GC
algorithm. We first present a stochastic Markov chain model that captures the
I/O dynamics of large-scale SSDs, and adapt the mean-field approach to derive
the asymptotic steady-state performance. We further prove the model convergence
and generalize the model for all types of workload. Inspired by this model, we
propose a randomized greedy algorithm (RGA) that can operate along the optimal
tradeoff curve with a tunable parameter. Using trace-driven simulation on
DiskSim with SSD add-ons, we demonstrate how RGA can be parameterized to
realize the performance-durability tradeoff.
</summary>
    <author>
      <name>Yongkun Li</name>
    </author>
    <author>
      <name>Patrick P. C. Lee</name>
    </author>
    <author>
      <name>John C. S. Lui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, Sigmetrics 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.4816v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.4816v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.1863v1</id>
    <updated>2013-04-06T07:48:02Z</updated>
    <published>2013-04-06T07:48:02Z</published>
    <title>Stochastic Analysis on RAID Reliability for Solid-State Drives</title>
    <summary>  Solid-state drives (SSDs) have been widely deployed in desktops and data
centers. However, SSDs suffer from bit errors, and the bit error rate is time
dependent since it increases as an SSD wears down. Traditional storage systems
mainly use parity-based RAID to provide reliability guarantees by striping
redundancy across multiple devices, but the effectiveness of RAID in SSDs
remains debatable as parity updates aggravate the wearing and bit error rates
of SSDs. In particular, an open problem is that how different parity
distributions over multiple devices, such as the even distribution suggested by
conventional wisdom, or uneven distributions proposed in recent RAID schemes
for SSDs, may influence the reliability of an SSD RAID array. To address this
fundamental problem, we propose the first analytical model to quantify the
reliability dynamics of an SSD RAID array. Specifically, we develop a
"non-homogeneous" continuous time Markov chain model, and derive the transient
reliability solution. We validate our model via trace-driven simulations and
conduct numerical analysis to provide insights into the reliability dynamics of
SSD RAID arrays under different parity distributions and subject to different
bit error rates and array configurations. Designers can use our model to decide
the appropriate parity distribution based on their reliability requirements.
</summary>
    <author>
      <name>Yongkun Li</name>
    </author>
    <author>
      <name>Patrick P. C. Lee</name>
    </author>
    <author>
      <name>John C. S. Lui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.1863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.1863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3804v1</id>
    <updated>2013-04-13T12:39:34Z</updated>
    <published>2013-04-13T12:39:34Z</published>
    <title>Multithreaded Input-Sensitive Profiling</title>
    <summary>  Input-sensitive profiling is a recent performance analysis technique that
makes it possible to estimate the empirical cost function of individual
routines of a program, helping developers understand how performance scales to
larger inputs and pinpoint asymptotic bottlenecks in the code. A current
limitation of input-sensitive profilers is that they specifically target
sequential computations, ignoring any communication between threads. In this
paper we show how to overcome this limitation, extending the range of
applicability of the original approach to multithreaded applications and to
applications that operate on I/O streams. We develop new metrics for
automatically estimating the size of the input given to each routine
activation, addressing input produced by non-deterministic memory stores
performed by other threads as well as by the OS kernel (e.g., in response to
I/O or network operations). We provide real case studies, showing that our
extension allows it to characterize the behavior of complex applications more
precisely than previous approaches. An extensive experimental investigation on
a variety of benchmark suites (including the SPEC OMP2012 and the PARSEC
benchmarks) shows that our Valgrind-based input-sensitive profiler incurs an
overhead comparable to other prominent heavyweight analysis tools, while
collecting significantly more performance points from each profiling session
and correctly characterizing both thread-induced and external input.
</summary>
    <author>
      <name>Emilio Coppa</name>
    </author>
    <author>
      <name>Camil Demetrescu</name>
    </author>
    <author>
      <name>Irene Finocchi</name>
    </author>
    <author>
      <name>Romolo Marotta</name>
    </author>
    <link href="http://arxiv.org/abs/1304.3804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68N30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; D.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7664v3</id>
    <updated>2015-05-22T13:34:40Z</updated>
    <published>2013-04-29T13:59:21Z</published>
    <title>Chip-level and multi-node analysis of energy-optimized lattice-Boltzmann
  CFD simulations</title>
    <summary>  Memory-bound algorithms show complex performance and energy consumption
behavior on multicore processors. We choose the lattice-Boltzmann method (LBM)
on an Intel Sandy Bridge cluster as a prototype scenario to investigate if and
how single-chip performance and power characteristics can be generalized to the
highly parallel case. First we perform an analysis of a sparse-lattice LBM
implementation for complex geometries. Using a single-core performance model,
we predict the intra-chip saturation characteristics and the optimal operating
point in terms of energy to solution as a function of implementation details,
clock frequency, vectorization, and number of active cores per chip. We show
that high single-core performance and a correct choice of the number of active
cores per chip are the essential optimizations for lowest energy to solution at
minimal performance degradation. Then we extrapolate to the MPI-parallel level
and quantify the energy-saving potential of various optimizations and execution
modes, where we find these guidelines to be even more important, especially
when communication overhead is non-negligible. In our setup we could achieve
energy savings of 35% in this case, compared to a naive approach. We also
demonstrate that a simple non-reflective reduction of the clock speed leaves
most of the energy saving potential unused.
</summary>
    <author>
      <name>Markus Wittmann</name>
    </author>
    <author>
      <name>Georg Hager</name>
    </author>
    <author>
      <name>Thomas Zeiser</name>
    </author>
    <author>
      <name>Jan Treibig</name>
    </author>
    <author>
      <name>Gerhard Wellein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/cpe.3489</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/cpe.3489" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 13 figures; post-peer-review version</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.7664v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7664v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.8013v1</id>
    <updated>2013-07-30T15:23:25Z</updated>
    <published>2013-07-30T15:23:25Z</published>
    <title>Characterizing Data Analysis Workloads in Data Centers</title>
    <summary>  As the amount of data explodes rapidly, more and more corporations are using
data centers to make effective decisions and gain a competitive edge. Data
analysis applications play a significant role in data centers, and hence it has
became increasingly important to understand their behaviors in order to further
improve the performance of data center computer systems. In this paper, after
investigating three most important application domains in terms of page views
and daily visitors, we choose eleven representative data analysis workloads and
characterize their micro-architectural characteristics by using hardware
performance counters, in order to understand the impacts and implications of
data analysis workloads on the systems equipped with modern superscalar
out-of-order processors. Our study on the workloads reveals that data analysis
applications share many inherent characteristics, which place them in a
different class from desktop (SPEC CPU2006), HPC (HPCC), and service workloads,
including traditional server workloads (SPECweb2005) and scale-out service
workloads (four among six benchmarks in CloudSuite), and accordingly we give
several recommendations for architecture and system optimizations. On the basis
of our workload characterization work, we released a benchmark suite named
DCBench for typical datacenter workloads, including data analysis and service
workloads, with an open-source license on our project home page on
http://prof.ict.ac.cn/DCBench. We hope that DCBench is helpful for performing
architecture and small-to-medium scale system researches for datacenter
computing.
</summary>
    <author>
      <name>Zhen Jia</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <author>
      <name>Lixin Zhang</name>
    </author>
    <author>
      <name>Chunjie Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 12 figures, IISWC2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.8013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.8013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.3127v1</id>
    <updated>2013-08-12T15:16:14Z</updated>
    <published>2013-08-12T15:16:14Z</published>
    <title>Performance Analysis of Connection Admission Control Scheme in IEEE
  802.16 OFDMA Networks</title>
    <summary>  IEEE 802.16 OFDMA (Orthogonal Frequency Division Multiple Access) technology
has emerged as a promising technology for broadband access in a Wireless
Metropolitan Area Network (WMAN) environment. In this paper, we address the
problem of queueing theoretic performance modeling and analysis of OFDMA under
broad-band wireless networks. We consider a single-cell IEEE 802.16 environment
in which the base station allocates subchannels to the subscriber stations in
its coverage area. The subchannels allocated to a subscriber station are shared
by multiple connections at that subscriber station. To ensure the Quality of
Service (QoS) performances, a Connection Admission Control (CAC) scheme is
considered at a subscriber station. A queueing analytical framework for these
admission control schemes is presented considering OFDMA-based transmission at
the physical layer. Then, based on the queueing model, both the
connection-level and the packet-level performances are studied and compared
with their analogues in the case without CAC. The connection arrival is modeled
by a Poisson process and the packet arrival for a connection by a two-state
Markov Modulated Poisson Process (MMPP). We determine analytically and
numerically different performance parameters, such as connection blocking
probability, average number of ongoing connections, average queue length,
packet dropping probability, queue throughput and average packet delay.
</summary>
    <author>
      <name>Abdelali El Bouchti</name>
    </author>
    <author>
      <name>Said El Kafhali</name>
    </author>
    <author>
      <name>Abdelkrim Haqiq</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, (IJCSIS) International Journal of Computer Science and
  Information Security, Volume 9 No. 3, March 2011. arXiv admin note:
  substantial text overlap with arXiv:1304.2033, arXiv:1203.4339</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.3127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
